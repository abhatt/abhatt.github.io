<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 04, 2020 04:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17119">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/06/04/the-truth/">The Truth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>What is the truth?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/whead/" rel="attachment wp-att-17122"><img width="160" alt="" class="alignright wp-image-17122" src="https://rjlipton.files.wordpress.com/2020/06/whead.jpg?w=160" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Alfred Whitehead was a logician and philosopher, who had a student of some note. The student was Bertrand Russell and together they wrote the famous three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">Principia Mathematica</a>. It took several hundred pages to get to the result that <img src="https://s0.wp.com/latex.php?latex=%7B1%2B1%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1+1=2}" class="latex" title="{1+1=2}" />.</p>
<p><a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/pm/" rel="attachment wp-att-17124"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/06/pm.png?w=300&amp;h=125" class="aligncenter size-medium wp-image-17124" height="125" /></a></p>
<p>
Amazing. </p>
<p>
Today I thought that discussing truth might be an interesting topic.</p>
<p>
Whitehead said: </p>
<blockquote><p><b> </b> <em> There are no whole truths; all truths are half-truths. It is trying to treat them as whole truths that plays the devil. </em>
</p></blockquote>
<p></p><p>
I like this quote. Whitehead was not the best lecturer, however. He gave the prestigious <a href="https://en.wikipedia.org/wiki/Gifford_lectures">Gifford lectures</a> a year after the astronomer Arthur Eddington. As Wikipedia <a href="https://en.wikipedia.org/wiki/Alfred_North_Whitehead">relates</a> quoting Victor Lowe: </p>
<blockquote><p><b> </b> <em> Eddington was a marvellous popular lecturer who had enthralled an audience of 600 for his entire course. The same audience turned up to Whitehead’s first lecture but it was completely unintelligible, not merely to the world at large but to the elect. My father remarked to me afterwards that if he had not known Whitehead well he would have suspected that it was an imposter making it up as he went along … The audience at subsequent lectures was only about half a dozen in all. </em>
</p></blockquote>
<p>Between the pandemic and the unrest in our cities there is debate about what is the “truth”. On cable news—CNN, MSNBC, FOX—one hears statements about the truth. You can also hear statements like “the experts know” or the “model” shows that this is true. Can math shed light on these discussions? What would Whitehead say?</p>
<p>
</p><p></p><h2> What is the Truth? </h2><p></p>
<p></p><p>
Mathematical truth is the one absolute we can count on—right? Math is precise in its own way, but does it yield truth? Not so clear.</p>
<p>
Whitehead’s proof that <img src="https://s0.wp.com/latex.php?latex=%7B1%2B1%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1+1=2}" class="latex" title="{1+1=2}" /> takes 100’s of pages; it may or may not increase your confidence. Here is a short “proof” that <img src="https://s0.wp.com/latex.php?latex=%7B2%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2=1}" class="latex" title="{2=1}" />:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/proof-5/" rel="attachment wp-att-17125"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/06/proof.png?w=300&amp;h=195" class="aligncenter size-medium wp-image-17125" height="195" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Math proofs are only as safe as two elements that are unavoidably social: </p>
<ol>
<li>
The care we use in applying our reasoning; and <p></p>
</li><li>
The care we use in choosing our assumptions.
</li></ol>
<p>
In the above proof snippet, one step divided by <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> which is the source of the error that <img src="https://s0.wp.com/latex.php?latex=%7B2%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2=1}" class="latex" title="{2=1}" />. A more worrisome issue is reasoning from assumptions. Wrong assumptions are a problem.</p>
<p>
</p><p></p><h2> Who are the Experts? </h2><p></p>
<p></p><p>
One definition of <a href="https://en.wikipedia.org/wiki/Expert">expert</a> is: An expert is somebody who has a broad and deep competence in terms of knowledge, skill and experience through practice and education in a particular field. </p>
<p>
More amusing definitions are: </p>
<blockquote><p><b> </b> <em> Mark Twain defined an expert as “an ordinary fellow from another town.” Will Rogers described an expert as “A man fifty miles from home with a briefcase.” Danish scientist and Nobel laureate Niels Bohr defined an expert as “A person that has made every possible mistake within his or her field.” </em>
</p></blockquote>
<p></p><p>
I find the use of the term <i>expert</i> in regard to the pandemic at best puzzling. How can anyone be an expert when the current situation is unique? The last pandemic happened over a hundred years ago. Unfortunately Twain, Rogers, and Bohr are closer to being correct. The situation we find ourselves in today does not lend itself to being an expert. At least in my non-expert opinion.</p>
<p><a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/not/" rel="attachment wp-att-17131"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/06/not.jpg?w=300&amp;h=102" class="aligncenter size-medium wp-image-17131" height="102" /></a></p>
<p></p><p>
Yes there are people, for example, who are experts on various viral agents. But there is more we do not know about this agent that we do know. </p>
<ul>
<li>
Can you get the virus twice? <p></p>
</li><li>
Can children get the virus? <p></p>
</li><li>
Will a vaccine be possible? <p></p>
</li><li>
Are there long-term affects even for those who survive? <p></p>
</li><li>
And so on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" />
</li></ul>
<p>
</p><p></p><h2> Where are the Models? </h2><p></p>
<p></p><p>
Models are created by experts, so you probably guess that I am not bullish on models. There are lots of models, for example, on the projection of how many will be infected, and how many will get seriously sick, and sadly how many will succumb. These models are based on various assumptions about how the virus works. Most of these assumptions are not proved in any sense. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I plan on saying more about truth in the future. Take care.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/06/04/the-truth/"><span class="datestr">at June 04, 2020 04:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/2020/06/04/itc-2020-program-is-out/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/06/04/itc-2020-program-is-out/">ITC 2020 program is out</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Guest post by Benny Applebaum</p>



<p>The<a href="https://itcrypto.github.io/2020main.html" target="_blank" rel="noreferrer noopener"> ITC 2020 program</a> is out, and this newborn looks healthy and strong! The program contains exciting new works in the area of Information-Theoretic Cryptography, confirming the importance of this new venue. </p>



<p>The PC, chaired by Daniel Wichs, also chose an amazing sequence of invited talks by Dachman-Soled, Natarajan, Jafar, Kol, Raz, and Vaikuntanathan, so this can also be a good opportunity to hear about the big recent developments in the area.</p>



<p><br />The conference will be virtual this year. Participation is free but requires registration. We hope to see many of you there</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/06/04/itc-2020-program-is-out/"><span class="datestr">at June 04, 2020 04:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020-2/">211-0051/20-2K Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642</a><br />
Email: c.lioma@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020-2/"><span class="datestr">at June 04, 2020 09:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/">211-0051/20-2K Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a full professorship in Machine Learning. The new professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151901&amp;DepartmentId=18971&amp;MediaId=4642</a><br />
Email: c.lioma@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/04/211-0051-20-2k-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/"><span class="datestr">at June 04, 2020 09:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/04/211-0243-20-2e-associate-professor-of-machine-learning-at-department-of-computer-science-university-of-copenhagen-apply-by-august-9-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/04/211-0243-20-2e-associate-professor-of-machine-learning-at-department-of-computer-science-university-of-copenhagen-apply-by-august-9-2020/">211-0243/20-2E Associate Professor of Machine Learning at Department of Computer Science, University of Copenhagen (apply by August 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Copenhagen is seeking candidates for an associate professorship in Machine Learning. The new associate professor is expected to make a significant contribution to leadership in the Machine Learning field within the Department of Computer Science.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151903&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151903&amp;DepartmentId=18971&amp;MediaId=4642</a><br />
Email: c.lioma@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/04/211-0243-20-2e-associate-professor-of-machine-learning-at-department-of-computer-science-university-of-copenhagen-apply-by-august-9-2020/"><span class="datestr">at June 04, 2020 09:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/04/211-0062-20-2n-tenure-track-assistant-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/04/211-0062-20-2n-tenure-track-assistant-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/">211-0062/20-2N Tenure-track assistant Professor of Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by August 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Copenhagen is seeking candidates for a tenure track assistant professorship in Machine Learning. The tenure track assistant professor’s duties will primarily include research, including obligations with regard to publication/scientific communication and research-based teaching with associated examination obligations within Machine Learning.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151899&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=151899&amp;DepartmentId=18971&amp;MediaId=4642</a><br />
Email: c.lioma@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/04/211-0062-20-2n-tenure-track-assistant-professor-of-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-august-9-2020/"><span class="datestr">at June 04, 2020 08:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4834">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4834">Pooled testing for covid: Guest post by Zeph Landau</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Scott’s foreword:</span></strong> <a href="https://people.eecs.berkeley.edu/~landau/">Zeph Landau</a>, a noted quantum computing theorist at UC Berkeley who’s worked closely with my adviser Umesh Vazirani, recently asked me if he could write a guest post about <a href="https://en.wikipedia.org/wiki/Group_testing">pooled testing</a> for covid—an old idea that, Zeph argues, could play a crucial role in letting universities safely reopen this fall.  Seeing a small chance to do a great good, I readily agreed.</p>



<p>I should confess that I’m more … fatalistic than Zeph.  Not that I’m proud of it: I think that Zeph’s attitude is superior to mine.  But, like, I’m a theoretical computer scientist with zero expertise in medical testing or statistics, and <em>I</em> knew about pooled testing and its WWII origins—so imagine how thoroughly the actual experts must know the idea.  Just like they know all about variolation, and challenge trials, and copper fixtures, and UV light, and vitamin D supplements, and a dozen other possible tools against covid that future historians might ask why we didn’t try more.</p>



<p>As I’ve written before, I think our fundamental problem is not a lack of good ideas.  It’s that, outside of some isolated pockets of progress, our entire civilization no longer has the will (or ability? is there a difference?) to <em>implement</em> good ideas, or even really to try them.  For anything new that requires coordination, today there are just too many stakeholders who need to be brought on board, too many risks that need further study.  So I see Zeph, and anyone like him, as occupying a tragic position, a bit like that of an Aztec advocating the use of the wheel.  “Sure,” the Aztec elders might calmly reply, “wheeled transport is obvious enough that we’ve <em>all</em> considered it, but a moment’s thought reveals why, in our actually existing empire, it would be reckless, costly, and of at most marginal benefit…”</p>



<p>But I hope I’m wrong!  Better, I hope this post is the one that <em>proves</em> me wrong!  So without further ado, here’s…</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Zeph Landau’s Guest Post</span></strong></p>



<p>This post describes how every university could efficiently use modest testing resources to sensibly and extensively reduce the number of COVID-19 cases on their campus this fall.  It is meant as a call to action to the reader — because without a concerted effort to get the right people the necessary information and take immediate consequential action, a far worse alternative will be implemented almost everywhere. It is my sincere hope, that immediately after reading this post, you will take the following steps:</p>



<p>1) Figure out who is part of the reopening committee at your institution.</p>



<p>2) Find the right people and engage with them either as a fellow faculty member or, better yet, through a connection to get them good information about the information posted here.</p>



<p>3) Then stay engaged and keep pushing.  (See below for links to sample documents.)</p>



<p>OK, here we go.</p>



<p><strong>The Problem</strong></p>



<p>How can we safely open a university or college campus such that we ensure that the number of cases does not drastically increase through the newfound interactions between the population?</p>



<p>One obvious, albeit impractical, solution to opening universities is to test everyone, everyday and isolate those that test positive quickly.  Unfortunately, we can’t do that due to costs ($100 per student per day) and availability of tests (on the order of 1000 tests per day at university testing labs).</p>



<p><em>Turns out there is a solution that uses drastically fewer tests and is commensurate in detecting an outbreak. It is called pooled screening which is a variant of pooled testing.</em></p>



<p><strong>The missing piece: early detection surveillance</strong></p>



<p>So how do we detect most contagious people quickly if we don’t have the resources to test everyone regularly?  The answer is by pooled testing — or to be more accurate (I’ll be clear about why this distinction is important later) <strong>pooled screening. </strong>The idea of pooling is old (attributed to Dorfman in the 40’s), simple, and has been used over and over in all kinds of scenarios. Pooled testing works by mixing samples together from a group and then administering a single test to the mixture. The test is designed to be sensitive enough to come up positive whenever <em>at least one</em> underlying sample is positive. Instead of testing each sample individually, you test the mixture, and then only those groups that test positive undergo a second round of testing of each individual sample. The individuals do not need to deliver a second sample; there is more than enough biological material for multiple tests per sample. When prevalence of a disease is low, most pools come up negative and you save a large amount of testing resources and time.  (For those more visually inclined, here is a one minute <a href="https://www.youtube.com/watch?v=pdC6irz_vys&amp;feature=youtu.be" target="_blank" rel="noreferrer noopener">video</a> on pooled testing).</p>



<p>So what would a good early detection surveillance system look like?  Here is a reasonable and doable framework:</p>



<ul><li>Divide the campus population into three groups (call them A, B, and C).</li><li>Collect samples from each group twice a week, (e.g. Group A: M/Th, Group B: Tu/Fri, Group C Wed/Sat).</li><li>Pool test the samples in groups of 16.</li></ul>



<p>What kinds of resources would this use?</p>



<ul><li>For a 10,000 person campus, you’d need about 200 tests per day, 6 days a week.  The universities that have implemented testing labs typically have the capacity to do on the order of 1000 tests a day.</li><li>Assuming a rough cost of $100 a test (which should be an overestimate if they are using their own lab), it would amount to a $12 a student/ per week.  </li></ul>



<p>What would it accomplish?  It would quickly find outbreaks and new cases.  Under a few different assumptions of the time-course of the viral load in a person, <em>the expected time for detecting an infectious person in this scheme is under 3 days. </em>Those cases would then need to be fed into an existing contact tracing and quarantine protocol.  The result: an outbreak suppressed before it had a chance to get going.</p>



<p>So why aren’t we already doing this?  Read on. . .</p>



<p><strong>The fear of false negatives in pooling</strong></p>



<p>The general concern to implementing pooling  for Covid-19 in the US is two-fold. </p>



<ol><li>Without the creation of a better test the dilution effect will make the test less sensitive and in turn produce more false negatives.  </li><li>Even if you could solve the scientific sensitivity issue, navigating the process of getting government approval is a big barrier.</li></ol>



<p>Let’s take each of these concerns in turn.  The first is definitely a concern if the goal is 1:1 medical testing.  If a sample can be barely seen as positive in an individual test, then the risk is that the dilution effect when pooled with others will cause the group test to come out negative — giving a wrong result to the positive individual.  The word for this is “sensitivity”, i.e. if a test has 95% sensitivity it means that it’ll be accurate 95% of the time and produce a false negative 5% of the time.  So how sensitive would a pooled test be where you combined 16 individual samples into 1 and just ran it through an existing 1:1 test?  Lab data suggests it would have at least 70% sensitivity.  For 1:1 testing this is a non-starter, however, <strong>the goal is early detection </strong>of an outbreak, which is different and as we shall see, a 70% sensitivity does fine for this purpose.</p>



<p>Suppose you are doing early detection surveillance and imagine that an outbreak starts.  Imagine 3 people are infected.  Because you are sampling every 3 days, you’ll be getting at least 6 positive samples, and the chances that your 70% screen misses all 6 is tiny.  As soon as it catches one, a contact tracing protocol is initiated and the others will be found.</p>



<p>Another way to formulate what is going on is that you are trading sensitivity for speed (in the form of capacity and cost)—and that is a huge win.  The pooling and more frequent testing gives you that speed versus sensitivity tradeoff.  Sure, Lebron James (a 70% free-throw shooter) won’t make every free throw, but the chance that he misses 6 in a row is tiny.</p>



<p>For some, the above thinking is straightforward.  However, for the medical testing paradigm—where the goal is the most accurate test for an individual using the one sample you have—this point of view is foreign and in many ways almost out of reach.   </p>



<p>OK.  So with the concern of sensitivity laid to rest, what about the second concern?  That the regulations will get in the way.  It turns out that this isn’t an issue though again, it is slightly counterintuitive for those who work in medical testing.  The task is surveillance, and therefore the pooling test is being used as a screen (not a medical test): negative group tests are not reported to the individual as a negative test result.   Positive groups are deconvoluted for individual testing and results returned to the person who is positive individually.  HHS/CLIA has indicated there aren’t regulatory restrictions as long as you don’t return test results due to the pooled test.</p>



<p>It is important to re-emphasize that the above is for pooled screening (where negative results are not returned), which is in contrast to pooled testing (where negative pools are reported as negative test results for each individual).  For pooled testing, which has received a jump of coverage due to its use recently in Wuhan, there are large regulatory hurdles–the CDC is just formulating criteria for clearing those hurdles and the science looks like, for now, that most labs wouldn’t be able to get above pools of size 5 or so.</p>



<p><strong>How do you safely collect so many samples?</strong></p>



<p>A different direction of concern for early detection surveillance is the logistics and feasibility around collecting samples.  To date, the gold standard for sampling is a deep nasal swab that requires a professional to do it, requires PPE equipment, and is not a pleasant experience.  Using this method wouldn’t work logistically on campus.</p>



<p>However, there are other sampling techniques that allow people to self-sample, both in the form of a shallow nasal swab and saliva based techniques.  The stated concern is obvious: there is a worry that these sampling techniques are less sensitive.  There is some evidence that this is not the case (and even the opposite) but regardless, as has been discussed— in early detection surveillance it is OK to take a hit on sensitivity.  The system remains robust because of the frequent testing and the goal of detecting an outbreak, not every individual.</p>



<p>Being able to self-sample removes a huge bottleneck.  The picture is very much simplified.  Students/faculty/staff self-sample on their prescribed days (either in the presence of a medical professional or not depending on the approved protocol) and then drop off their sample at any of various drop-off stations on campus.  Those stations deliver the samples to the testing facility for pooling and testing.</p>



<p><strong>You can help to get this done</strong></p>



<p>Is what I’m describing a new idea?  As far as I can tell, the answer is both no and yes.  Pooled testing is in the news both as a theoretical idea and now as being implemented at some scale—in Israel, in a lab in Nebraska, and most recently in Wuhan.   But using pooling as a screen (not a medical test) within an early detection surveillance system that repeatedly screens everyone is, as far as I know, not in the discussion.</p>



<p>What seems clear is that right now—reopening committees and labs are perhaps aware of the idea of pooling but only as a theoretical idea of a technology that might be coming at some vague time in the future.  They are unaware that in the form of early detection surveillance, it is right in front of them ready to go.  They’d need a matter of weeks to convert a 1:1 lab into a lab that could handle both pooled screening and 1:1 testing (this <a href="https://www.medrxiv.org/content/10.1101/2020.04.17.20069062v2" target="_blank" rel="noreferrer noopener">lab</a> did it, <a href="https://docs.google.com/document/d/1FVp55JAs2heqV5ktyGVVdoE-W_XTKmZgCWBy1SEoo-o/edit?usp=sharing" target="_blank" rel="noreferrer noopener">here</a> is a brief outline of the steps).  In the same timeline, they could develop a system for handling the logistics of sampling large numbers of people.</p>



<p><strong>And that is where each of you come in . . . </strong>  you can help get these ideas to the right people.  It needs to be done quickly because decisions are being made now as to what to do.  The right people are your colleagues—you just have to find out who they are and reach out to them personally.  You can find out who is on the reopening committee, you can track down faculty members in public health and microbiology.  They are often busy and might be skeptical of what an outsider can offer, but keep trying because my experience has been that if you keep at it and follow up, they will listen and be grateful for the information.</p>



<p>Here is a <a href="https://docs.google.com/document/d/1QZHWZUxrMGzYKycZNpsfuJBCqRphGBzMQf4-ws76YrI/edit?usp=sharing" target="_blank" rel="noreferrer noopener">sample letter</a> you could use.</p>



<p>Here is a crowdsourced <a href="https://docs.google.com/spreadsheets/d/15zXZsGh6W2hoejgp4_wgTQzDPp-qHBrvYYcgOoaitZ4/edit?usp=sharing" target="_blank" rel="noreferrer noopener">spreadsheet</a> for potential contact people at various universities.  If your university isn’t yet there, we ask that you enter the info that you find for your university in this <a href="https://forms.gle/7v13r2qfYiGoVRUM7" target="_blank" rel="noreferrer noopener">form</a> which is linked to the above spreadsheet (or enter it directly into the spreadsheet).</p>



<p>If you want to know more or would like to craft your own letter, here are some relevant links:</p>



<p><a href="https://docs.google.com/document/d/1qdOkIVle8fQtDjQB-EZA_OdMrCbeeAiljhhHRFc05jE/edit?usp=sharing" target="_blank" rel="noreferrer noopener">Covid-19 early detection surveillance on a 240 person facility using 5 tests a day</a></p>



<p><a href="https://docs.google.com/document/d/16fQ91xkL2Evpi7kEYl5FvSM4EAIWf5Lq9unmD1XJQsY/edit?usp=sharing" target="_blank" rel="noreferrer noopener">Covid-19 early detection surveillance for a campus of 24,000 using 500 tests a day</a></p>



<p>And here is a <a href="https://docs.google.com/document/d/1ZHwHGZMGFzqqj6up-9dRda__pzttSwy7B8fx5wBk5cg/edit?usp=sharing" target="_blank" rel="noreferrer noopener">simple analysis of the mean time between contagion</a> and detection that an early detection scheme could accomplish.</p>



<p>If anyone wants to follow up with me, I’m happy to do so.  You can reach me at:  zeph dot landau at gmail dot com </p>



<p>Thanks.</p>



<p>Zeph Landau<br />Dept. of Computer Science<br />University of California, Berkeley</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4834"><span class="datestr">at June 04, 2020 07:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02408">Dynamic Longest Common Substring in Polylogarithmic Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charalampopoulos:Panagiotis.html">Panagiotis Charalampopoulos</a>, Paweł Gawrychowski, Karol Pokorski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02408">PDF</a><br /><b>Abstract: </b>The longest common substring problem consists in finding a longest string
that appears as a (contiguous) substring of two input strings. We consider the
dynamic variant of this problem, in which we are to maintain two dynamic
strings $S$ and $T$, each of length at most $n$, that undergo substitutions of
letters, in order to be able to return a longest common substring after each
substitution. Recently, Amir et al. [ESA 2019] presented a solution for this
problem that needs only $\tilde{\mathcal{O}}(n^{2/3})$ time per update. This
brought the challenge of determining whether there exists a faster solution
with polylogarithmic update time, or (as is the case for other dynamic
problems), we should expect a polynomial (conditional) lower bound. We answer
this question by designing a significantly faster algorithm that processes each
substitution in amortized $\log^{\mathcal{O}(1)} n$ time with high probability.
Our solution relies on exploiting the local consistency of the parsing of a
collection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on
maintaining two dynamic trees with labeled bicolored leaves, so that after each
update we can report a pair of nodes, one from each tree, of maximum combined
weight, which have at least one common leaf-descendant of each color. We
complement this with a lower bound of $\Omega(\log n/ \log\log n)$ for the
update time of any polynomial-size data structure that maintains the LCS of two
dynamic strings, and the same lower bound for the update time of any data
structure of size $\tilde{\mathcal{O}}(n)$ that maintains the LCS of a static
and a dynamic string. Both lower bounds hold even allowing amortization and
randomization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02408"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02399">ExKMC: Expanding Explainable $k$-Means Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frost:Nave.html">Nave Frost</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Michal.html">Michal Moshkovitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02399">PDF</a><br /><b>Abstract: </b>Despite the popularity of explainable AI, there is limited work on effective
methods for unsupervised learning. We study algorithms for $k$-means
clustering, focusing on a trade-off between explainability and accuracy.
Following prior work, we use a small decision tree to partition a dataset into
$k$ clusters. This enables us to explain each cluster assignment by a short
sequence of single-feature thresholds. While larger trees produce more accurate
clusterings, they also require more complex explanations. To allow flexibility,
we develop a new explainable $k$-means clustering algorithm, ExKMC, that takes
an additional parameter $k' \geq k$ and outputs a decision tree with $k'$
leaves. We use a new surrogate cost to efficiently expand the tree and to label
the leaves with one of $k$ clusters. We prove that as $k'$ increases, the
surrogate cost is non-increasing, and hence, we trade explainability for
accuracy. Empirically, we validate that ExKMC produces a low cost clustering,
outperforming both standard decision tree methods and other algorithms for
explainable clustering. Implementation of ExKMC available at
https://github.com/navefr/ExKMC.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02399"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02374">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02374">On tensor rank and commuting matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koiran:Pascal.html">Pascal Koiran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02374">PDF</a><br /><b>Abstract: </b>Obtaining superlinear lower bounds on tensor rank is a major open problem in
complexity theory. In this paper we propose a generalization of the approach
used by Strassen in the proof of his $3n/2$ border rank lower bound. Our
approach revolves around a problem on commuting matrices:
</p>
<p>Given matrices $Z_1,...,Z_p$ of size $n$ and an integer $r&gt;n$, are there
commuting matrices $Z'_1,...,Z'_p$ of size $r$ such that every $Z_k$ is
embedded as a submatrix in the top-left corner of $Z'_k$?
</p>
<p>As one of our main results, we show that this question always has a positive
answer for $r$ larger than $rank(T)+n$, where $T$ denotes the tensor with
slices $Z_1,..,Z_p$. Taking the contrapositive, if one can show for some
specific matrices $Z_1,...,Z_p$ and a specific integer $r$ that this question
has a negative answer, this yields the lower bound $rank(T) &gt; r-n$. There is a
little bit of slack in the above $rank(T)+n$ bound, but we also provide a
number of exact characterizations of tensor rank and symmetric rank, for
ordinary and symmetric tensors, over the fields of real and complex numbers.
Each of these characterizations points to a corresponding variation on the
above approach. In order to explain how Strassen's theorem fits within this
framework we also provide a self-contained proof of his lower bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02374"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02249">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02249">Complete Characterization of Incorrect Orthology Assignments in Best Match Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>David Schaller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gei=szlig=:Manuela.html">Manuela Geiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02249">PDF</a><br /><b>Abstract: </b>Genome-scale orthology assignments are usually based on reciprocal best
matches. In the absence of horizontal gene transfer (HGT), every pair of
orthologs forms a reciprocal best match. Incorrect orthology assignments
therefore are always false positives in the Reciprocal Best Match Graph. We
consider duplication/loss scenarios and characterize unambiguous false-positive
(u-fp) orthology assignments, that is, edges in the Best Match Graphs (BMGs)
that cannot correspond to orthologs for any gene tree that explains the BMG. We
characterize u-fp edges in terms of subgraphs of the BMG and show that, given a
BMG, there is a unique "augmented tree" that explains the BMG and identifies
all u-fp edges in terms of overlapping sets of species in certain subtrees. The
augmented tree can be constructed as a refinement of the unique least resolved
tree of the BMG in polynomial time. Removal of the u-fp edges from the
reciprocal best matches results in a unique orthology assignment.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02249"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02219">LCP-Aware Parallel String Sorting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ellert:Jonas.html">Jonas Ellert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Johannes.html">Johannes Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sitchinava:Nodari.html">Nodari Sitchinava</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02219">PDF</a><br /><b>Abstract: </b>When lexicographically sorting strings, it is not always necessary to inspect
all symbols. For example, the lexicographical rank of "europar" amongst the
strings "eureka", "eurasia", and "excells" only depends on its so called
relevant prefix "euro". The distinguishing prefix size $D$ of a set of strings
is the number of symbols that actually need to be inspected to establish the
lexicographical ordering of all strings. Efficient string sorters should be
$D$-aware, i.e. their complexity should depend on $D$ rather than on the total
number $N$ of all symbols in all strings. While there are many $D$-aware
sorters in the sequential setting, there appear to be no such results in the
PRAM model. We propose a framework yielding a $D$-aware modification of any
existing PRAM string sorter. The derived algorithms are work-optimal with
respect to their original counterpart: If the original algorithm requires
$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time
increases only by a small factor that is logarithmic in the length of the
longest relevant prefix. Our framework universally works for deterministic and
randomized algorithms in all variations of the PRAM model, such that future
improvements in ($D$-unaware) parallel string sorting will directly result in
improvements in $D$-aware parallel string sorting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02219"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02134">Computing Palindromic Trees for a Sliding Window and Its Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mieno:Takuya.html">Takuya Mieno</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watanabe:Kiichi.html">Kiichi Watanabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02134">PDF</a><br /><b>Abstract: </b>The palindromic tree (a.k.a. eertree) for a string $S$ of length $n$ is a
tree-like data structure that represents the set of all distinct palindromic
substrings of $S$, using $O(n)$ space [Rubinchik and Shur, 2018]. It is known
that, when $S$ is over an alphabet of size $\sigma$ and is given in an online
manner, then the palindromic tree of $S$ can be constructed in $O(n\log\sigma)$
time with $O(n)$ space. In this paper, we consider the sliding window version
of the problem: For a fixed window length $d$, we propose two algorithms to
maintain the palindromic tree of size $O(d)$ for every sliding window
$S[i..i+d-1]$ over $S$, one running in $O(n\log\sigma')$ time with $O(d)$ space
where $\sigma' \leq d$ is the maximum number of distinct characters in the
windows, and the other running in $O(n + d\sigma)$ time with $d\sigma + O(d)$
space. We also present applications of our algorithms for computing minimal
unique palindromic substrings (MUPS) and for computing minimal absent
palindromic words (MAPW) for a sliding window.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02134"><span class="datestr">at June 04, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02027">Sampling-Based Motion Planning on Manifold Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Englert:Peter.html">Peter Englert</a>, Isabel M. Rayas Fernández, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramachandran:Ragesh_K=.html">Ragesh K. Ramachandran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sukhatme:Gaurav_S=.html">Gaurav S. Sukhatme</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02027">PDF</a><br /><b>Abstract: </b>We address the problem of planning robot motions in constrained configuration
spaces where the constraints change throughout the motion. A novel problem
formulation is introduced that describes a task as a sequence of intersecting
manifolds, which the robot needs to traverse in order to solve the task. We
specify a class of sequential motion planning problems that fulfill a
particular property of the change in the free configuration space when
transitioning between manifolds. For this problem class, a sequential motion
planning algorithm SMP is developed that searches for optimal intersection
points between manifolds by using RRT* in an inner loop with a novel steering
strategy. We provide a theoretical analysis regarding its probabilistic
completeness and demonstrate its performance on kinematic planning problems
where the constraints are represented as geometric primitives. Further, we show
its capabilities on solving multi-robot object transportation tasks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02027"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01975">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01975">Sparsification of Balanced Directed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Yu.html">Yu Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Kevin.html">Kevin Sun</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01975">PDF</a><br /><b>Abstract: </b>Sparsification, where the cut values of an input graph are approximately
preserved by a sparse graph (called a cut sparsifier) or a succinct data
structure (called a cut sketch), has been an influential tool in graph
algorithms. But, this tool is restricted to undirected graphs, because some
directed graphs are known to not admit sparsification. Such examples, however,
are structurally very dissimilar to undirected graphs in that they exhibit
highly unbalanced cuts. This motivates us to ask: can we sparsify a balanced
digraph?
</p>
<p>To make this question concrete, we define balance $\beta$ of a digraph as the
maximum ratio of the cut value in the two directions (Ene et al., STOC 2016).
We show the following results:
</p>
<p>For-All Sparsification: If all cut values need to be simultaneously preserved
(cf. Bencz\'ur and Karger, STOC 1996), then we show that the size of the
sparsifier (or even cut sketch) must scale linearly with $\beta$. The upper
bound is a simple extension of sparsification of undirected graphs (formally
stated recently in Ikeda and Tanigawa (WAOA 2018)), so our main contribution
here is to show a matching lower bound.
</p>
<p>For-Each Sparsification: If each cut value needs to be individually preserved
(Andoni et al., ITCS 2016), then the situation is more interesting. Here, we
give a cut sketch whose size scales with $\sqrt{\beta}$, thereby beating the
linear lower bound above. We also show that this result is tight by exhibiting
a matching lower bound of $\sqrt{\beta}$ on "for-each" cut sketches.
</p>
<p>Our upper bounds work for general weighted graphs, while the lower bounds
even hold for unweighted graphs with no parallel edges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01975"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01944">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01944">Designing Differentially Private Estimators in High Dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhar:Aditya.html">Aditya Dhar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jason.html">Jason Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01944">PDF</a><br /><b>Abstract: </b>We study differentially private mean estimation in a high-dimensional
setting. Existing differential privacy techniques applied to large dimensions
lead to computationally intractable problems or estimators with excessive
privacy loss. Recent work in high-dimensional robust statistics has identified
computationally tractable mean estimation algorithms with asymptotic
dimension-independent error guarantees. We incorporate these results to develop
a strict bound on the global sensitivity of the robust mean estimator. This
yields a computationally tractable algorithm for differentially private mean
estimation in high dimensions with dimension-independent privacy loss. Finally,
we show on synthetic data that our algorithm significantly outperforms classic
differential privacy methods, overcoming barriers to high-dimensional
differential privacy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01944"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01933">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01933">Hierarchical Clustering: a 0.585 Revenue Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alon:Noga.html">Noga Alon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Azar:Yossi.html">Yossi Azar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vainstein:Danny.html">Danny Vainstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01933">PDF</a><br /><b>Abstract: </b>Hierarchical Clustering trees have been widely accepted as a useful form of
clustering data, resulting in a prevalence of adopting fields including
phylogenetics, image analysis, bioinformatics and more. Recently, Dasgupta
(STOC 16') initiated the analysis of these types of algorithms through the
lenses of approximation. Later, the dual problem was considered by Moseley and
Wang (NIPS 17') dubbing it the Revenue goal function. In this problem, given a
nonnegative weight $w_{ij}$ for each pair $i,j \in [n]=\{1,2, \ldots ,n\}$, the
objective is to find a tree $T$ whose set of leaves is $[n]$ that maximizes the
function $\sum_{i&lt;j \in [n]} w_{ij} (n -|T_{ij}|)$, where $|T_{ij}|$ is the
number of leaves in the subtree rooted at the least common ancestor of $i$ and
$j$.
</p>
<p>In our work we consider the revenue goal function and prove the following
results. First, we prove the existence of a bisection (i.e., a tree of depth 2
in which the root has two children, each being a parent of $n/2$ leaves) which
approximates the general optimal tree solution up to a factor of $\frac{1}{2}$
(which is tight). Second, we apply this result in order to prove a
$\frac{2}{3}p$ approximation for the general revenue problem, where $p$ is
defined as the approximation ratio of the Max-Uncut Bisection problem. Since
$p$ is known to be at least 0.8776 (Wu et al., 2015, Austrin et al., 2016), we
get a 0.585 approximation algorithm for the revenue problem. This improves a
sequence of earlier results which culminated in an 0.4246-approximation
guarantee (Ahmadian et al., 2019).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01933"><span class="datestr">at June 04, 2020 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01903">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01903">On a Class of Constrained Synchronization Problems in NP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoffmann:Stefan.html">Stefan Hoffmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01903">PDF</a><br /><b>Abstract: </b>The class of known constraint automata for which the constrained
synchronization problem is in NP all admit a special form. In this work, we
take a closer look at them. We characterize a wider class of constraint
automata that give constrained synchronization problems in NP, which
encompasses all known problems in NP. We call these automata polycyclic
automata. The corresponding language class of polycyclic languages is
introduced. We show various characterizations and closure properties for this
new language class. We then give a criterion for NP-completeness and a
criterion for polynomial time solvability for polycyclic constraint languages.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01903"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/084">TR20-084 |  Rate Amplification and Query-Efficient Distance Amplification for Locally Decodable Codes | 

	Gil Cohen, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In a seminal work, Kopparty et al. (J. ACM 2017) constructed asymptotically good $n$-bit locally decodable codes (LDC) with $2^{\widetilde{O}(\sqrt{\log{n}})}$ queries. A key ingredient in their construction is a distance amplification procedure by Alon et al. (FOCS 1995) which amplifies the distance $\delta$ of a code to a constant at a  $\mathrm{poly}(1/\delta)$ multiplicative cost in query complexity. Given the pivotal role of the AEL distance amplification procedure in the state-of-the-art constructions of LDC as well as LCC and LTC, one is prompt to ask whether the $\mathrm{poly}(1/\delta)$ factor in query complexity can be reduced.

Our first contribution is a significantly improved distance amplification procedure for LDC. The cost is reduced from $\mathrm{poly}(1/\delta)$ to, roughly, the query complexity of a length $1/\delta$ asymptotically good LDC. We derive several applications, one of which allows us to convert a $q$-query LDC with extremely poor distance $\delta = n^{-(1-o(1))}$ to a constant distance LDC with $q^{\mathrm{poly}(\log\log{n})}$ queries. As another example, amplifying distance $\delta = 2^{-(\log{n})^\alpha}$, for any constant $\alpha &lt; 1$, will require $q^{O(\log\log\log{n})}$ queries using our procedure.

Motivated by the fruitfulness of distance amplification,  we investigate the natural question of rate amplification. Our second contribution is identifying a rich and natural class of LDC and devise two (incomparable) rate amplification procedures for it. These, however, deteriorate the distance, at which point a distance amplification procedure is invoked. Combined, the procedures convert any $q$-query LDC in our class, having rate $\rho$ and, say, constant distance, to an asymptotically good LDC with $q^{\mathrm{poly}(1/\rho)}$ queries.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/084"><span class="datestr">at June 03, 2020 01:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6696939180299352624">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html">How to handle grades during the Pandemic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In March many Colleges sent students home and the rest of the semester was online. This was quite disruptive for the students. Schools, quite reasonably, wanted to make it less traumatic for students.<br />
<br />
So what to do about grades? There are two issues. I state the options I have heard.<br />
<br />
<br />
ISSUE ONE  If P/F How to Got About it?<br />
<br />
1) Grade as usual.<br />
<br />
2) Make all classes P/F.<br />
<br />
PRO: Much less pressure on students.<br />
<br />
CON: Might be demoralizing for the good students.<br />
<br />
3) Make all classes P/F but allow students to opt for letter grades BUT they must decide before the last day of class. Hence teachers must post cutoffs before the final is graded<br />
<br />
CON: Complicated and puts (a) teachers in an awkward position of having to post cutoffs before the final, and (b) puts students in an awkward position of having to predict how well they would do.<br />
<br />
CON: A student can blow off a final knowing they will still get a D (passing) in the course.<br />
<br />
PRO: Good students can still get their A's<br />
<br />
CAVEAT: A transcript might look very strange. Say I was looking at a graduate school applicant and I see<br />
<br />
Operating Systems: A<br />
<br />
Theory of Computation: P<br />
<br />
I would likely assume that the Theory course the student got a C. And that might be unfair.<br />
<br />
3) Make all classes P/F but allow students to opt for letter grades AFTER seeing their letter grades. <br />
<br />
PRO: Less complicated an awkard<br />
<br />
PRO: A students blah blah<br />
<br />
CAVEAT above still applies.<br />
<br />
ISSUE TWO If P/F what about a D in the major<br />
<br />
At UMCP COMP SCI (and I expect other depts)<br />
<br />
a D is a passing grade for the University<br />
<br />
but<br />
<br />
a D is not a passing grade for the Major.<br />
<br />
So if a s CS Major gets a D in Discrete Math that does not count for the major--- they have to take it over again.<br />
<br />
But if classes are P/F what do do about that.<br />
<br />
Options<br />
<br />
1) Students have to take classes in their major for a letter grade.<br />
<br />
CON: The whole point of the P/F is to relieve pressure on the students in these hard times.<br />
<br />
PRO: None.<br />
<br />
2) Students taking a course in their major who get a D will still get a P on the transcript but will be told that they have to take the class over again.<br />
<br />
3) Do nothing, but tell the students<br />
<br />
IF you got a D in a course in your major and you are taking a sequel, STUDY HARD OVER THE SUMMER!<br />
<br />
4) Do nothing, but tell the teachers<br />
<br />
Students in the Fall may have a weak background. Just teach the bare minimum of what they need for the major.<br />
<br />
(Could do both 3 and 4)<br />
<br />
SO- what is your school doing and how is it working?</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html"><span class="datestr">at June 03, 2020 04:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01825">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01825">Efficient tree-structured categorical retrieval</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belazzougui:Djamal.html">Djamal Belazzougui</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kucherov:Gregory.html">Gregory Kucherov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01825">PDF</a><br /><b>Abstract: </b>We study a document retrieval problem in the new framework where $D$ text
documents are organized in a {\em category tree} with a pre-defined number $h$
of categories. This situation occurs e.g. with taxomonic trees in biology or
subject classification systems for scientific literature. Given a string
pattern $p$ and a category (level in the category tree), we wish to efficiently
retrieve the $t$ \emph{categorical units} containing this pattern and belonging
to the category. We propose several efficient solutions for this problem. One
of them uses $n(\log\sigma(1+o(1))+\log D+O(h)) + O(\Delta)$ bits of space and
$O(|p|+t)$ query time, where $n$ is the total length of the documents, $\sigma$
the size of the alphabet used in the documents and $\Delta$ is the total number
of nodes in the category tree. Another solution uses
$n(\log\sigma(1+o(1))+O(\log D))+O(\Delta)+O(D\log n)$ bits of space and
$O(|p|+t\log D)$ query time. We finally propose other solutions which are more
space-efficient at the expense of a slight increase in query time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01825"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01598">A Novel Approach to Solve K-Center Problems with Geographical Placement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hillmann:Peter.html">Peter Hillmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlig:Tobias.html">Tobias Uhlig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodosek:Gabi_Dreo.html">Gabi Dreo Rodosek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rose:Oliver.html">Oliver Rose</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01598">PDF</a><br /><b>Abstract: </b>The facility location problem is a well-known challenge in logistics that is
proven to be NP-hard. In this paper we specifically simulate the geographical
placement of facilities to provide adequate service to customers. Determining
reasonable center locations is an important challenge for a management since it
directly effects future service costs. Generally, the objective is to place the
central nodes such that all customers have convenient access to them. We
analyze the problem and compare different placement strategies and evaluate the
number of required centers. We use several existing approaches and propose a
new heuristic for the problem. For our experiments we consider various
scenarios and employ simulation to evaluate the performance of the optimization
algorithms. Our new optimization approach shows a significant improvement. The
presented results are generally applicable to many domains, e.g., the placement
of military bases, the planning of content delivery networks, or the placement
of warehouses.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01598"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01588">Fast Algorithms for Join Operations on Tree Decompositions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rooij:Johan_M=_M=_van.html">Johan M. M. van Rooij</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01588">PDF</a><br /><b>Abstract: </b>Treewidth is a measure of how tree-like a graph is. It has many important
algorithmic applications because many NP-hard problems on general graphs become
tractable when restricted to graphs of bounded treewidth. Algorithms for
problems on graphs of bounded treewidth mostly are dynamic programming
algorithms using the structure of a tree decomposition of the graph. The
bottleneck in the worst-case run time of these algorithms often is the
computations for the so called join nodes in the associated nice tree
decomposition.
</p>
<p>In this paper, we review two different approaches that have appeared in the
literature about computations for the join nodes: one using fast zeta and
M\"obius transforms and one using fast Fourier transforms. We combine these
approaches to obtain new, faster algorithms for a broad class of vertex subset
problems known as the [\sigma,\rho]-domination problems. Our main result is
that we show how to solve [\sigma,\rho]-domination problems in $O(s^{t+2} t n^2
(t\log(s)+\log(n)))$ arithmetic operations. Here, t is the treewidth, s is the
(fixed) number of states required to represent partial solutions of the
specific [\sigma,\rho]-domination problem, and n is the number of vertices in
the graph. This reduces the polynomial factors involved compared to the
previously best time bound (van Rooij, Bodlaender, Rossmanith, ESA 2009) of $O(
s^{t+2} (st)^{2(s-2)} n^3 )$ arithmetic operations. In particular, this removes
the dependence of the degree of the polynomial on the fixed number of
states~$s$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01588"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01570">CNNs on Surfaces using Rotation-Equivariant Features</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiersma:Ruben.html">Ruben Wiersma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eisemann:Elmar.html">Elmar Eisemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hildebrandt:Klaus.html">Klaus Hildebrandt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01570">PDF</a><br /><b>Abstract: </b>This paper is concerned with a fundamental problem in geometric deep learning
that arises in the construction of convolutional neural networks on surfaces.
Due to curvature, the transport of filter kernels on surfaces results in a
rotational ambiguity, which prevents a uniform alignment of these kernels on
the surface. We propose a network architecture for surfaces that consists of
vector-valued, rotation-equivariant features. The equivariance property makes
it possible to locally align features, which were computed in arbitrary
coordinate systems, when aggregating features in a convolution layer. The
resulting network is agnostic to the choices of coordinate systems for the
tangent spaces on the surface. We implement our approach for triangle meshes.
Based on circular harmonic functions, we introduce convolution filters for
meshes that are rotation-equivariant at the discrete level. We evaluate the
resulting networks on shape correspondence and shape classifications tasks and
compare their performance to other approaches.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01570"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01491">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01491">The Fine-Grained Complexity of Andersen's Pointer Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavlogiannis:Andreas.html">Andreas Pavlogiannis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01491">PDF</a><br /><b>Abstract: </b>Pointer analysis is one of the fundamental problems in static program
analysis. Given a set of pointers, the task is to produce a useful
over-approximation of the memory locations that each pointer may point-to at
runtime. The most common formulation is Andersen's Pointer Analysis (APA),
defined as an inclusion-based set of $m$ pointer constraints over a set of $n$
pointers. Existing algorithms solve APA in $O(n^2\cdot m)$ time, while it has
been conjectured that the problem has no truly sub-cubic algorithm, with a
proof so far having remained elusive. It is also well-known that $APA$ can be
solved in $O(n^2)$ time under certain sparsity conditions that hold naturally
in some settings. Besides these simple bounds, the complexity of the problem
has remained poorly understood.
</p>
<p>In this work we draw a rich fine-grained complexity landscape of APA, and
present upper and lower bounds. First, we establish an $O(n^3)$ upper-bound for
general APA, improving over $O(n^2\cdot m)$ as $n=O(m)$. Second, we show that
sparse instances can be solved in $O(n^{3/2})$ time, improving the current
$O(n^2)$ bound. Third, we show that even on-demand APA ("may a specific pointer
$a$ point to a specific location $b$?") has an $\Omega(n^3)$ (combinatorial)
lower bound under standard complexity-theoretic hypotheses. This formally
establishes the long-conjectured "cubic bottleneck" of APA, and shows that our
$O(n^3)$-time algorithm is optimal. Fourth, we show that under mild
restrictions, APA is solvable in $\tilde{O}(n^{\omega})$ time, where
$\omega&lt;2.373$ is the matrix-multiplication coefficient. It is believed that
$\omega=2+o(1)$, in which case this bound becomes quadratic. Fifth, we show
that even under such restrictions, even the on-demand} problem has an
$\Omega(n^2)$ lower bound under standard complexity-theoretic hypotheses, and
hence our algorithm is optimal when $\omega=2+o(1)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01491"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01428">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01428">Zone Theorem for Arrangements in three dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Sanjeev.html">Sanjeev Saxena</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01428">PDF</a><br /><b>Abstract: </b>In this note, a simple description of zone theorem in three dimensions is
given.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01428"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01400">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01400">Approximation Guarantees of Local Search Algorithms via Localizability of Set Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujii:Kaito.html">Kaito Fujii</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01400">PDF</a><br /><b>Abstract: </b>This paper proposes a new framework for providing approximation guarantees of
local search algorithms. Local search is a basic algorithm design technique and
is widely used for various combinatorial optimization problems. To analyze
local search algorithms for set function maximization, we propose a new notion
called localizability of set functions, which measures how effective local
improvement is. Moreover, we provide approximation guarantees of standard local
search algorithms under various combinatorial constraints in terms of
localizability. The main application of our framework is sparse optimization,
for which we show that restricted strong concavity and restricted smoothness of
the objective function imply localizability, and further develop accelerated
versions of local search algorithms. We conduct experiments in sparse
regression and structure learning of graphical models to confirm the practical
efficiency of the proposed local search algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01400"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01256">Walking through Doors is Hard, even without Staircases: Proving PSPACE-hardness via Planar Assemblies of Door Gadgets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ani:Joshua.html">Joshua Ani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bosboom:Jeffrey.html">Jeffrey Bosboom</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, Yevhenii Diomidov, Dylan Hendrickson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01256">PDF</a><br /><b>Abstract: </b>A door gadget has two states and three tunnels that can be traversed by an
agent (player, robot, etc.): the "open" and "close" tunnel sets the gadget's
state to open and closed, respectively, while the "traverse" tunnel can be
traversed if and only if the door is in the open state. We prove that it is
PSPACE-complete to decide whether an agent can move from one location to
another through a planar assembly of such door gadgets, removing the
traditional need for crossover gadgets and thereby simplifying past
PSPACE-hardness proofs of Lemmings and Nintendo games Super Mario Bros., Legend
of Zelda, and Donkey Kong Country. Our result holds in all but one of the
possible local planar embedding of the open, close, and traverse tunnels within
a door gadget; in the one remaining case, we prove NP-hardness.
</p>
<p>We also introduce and analyze a simpler type of door gadget, called the
self-closing door. This gadget has two states and only two tunnels, similar to
the "open" and "traverse" tunnels of doors, except that traversing the traverse
tunnel also closes the door. In a variant called the symmetric self-closing
door, the "open" tunnel can be traversed if and only if the door is closed. We
prove that it is PSPACE-complete to decide whether an agent can move from one
location to another through a planar assembly of either type of self-closing
door. Then we apply this framework to prove new PSPACE-hardness results for
eight different 3D Mario games and Sokobond.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01256"><span class="datestr">at June 03, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01225">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01225">Streaming Coresets for Symmetric Tensor Factorization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Rachit Chhaya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhari:Jayesh.html">Jayesh Choudhari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dasgupta:Anirban.html">Anirban Dasgupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shit:Supratim.html">Supratim Shit</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01225">PDF</a><br /><b>Abstract: </b>Factorizing tensors has recently become an important optimization module in a
number of machine learning pipelines, especially in latent variable models. We
show how to do this efficiently in the streaming setting. Given a set of $n$
vectors, each in $\mathbb{R}^d$, we present algorithms to select a sublinear
number of these vectors as coreset, while guaranteeing that the CP
decomposition of the $p$-moment tensor of the coreset approximates the
corresponding decomposition of the $p$-moment tensor computed from the full
data. We introduce two novel algorithmic techniques: online filtering and
kernelization. Using these two, we present four algorithms that achieve
different tradeoffs of coreset size, update time and working space, beating or
matching various state of the art algorithms. In case of matrices (2-ordered
tensor) our online row sampling algorithm guarantees $(1 \pm \epsilon)$
relative error spectral approximation. We show applications of our algorithms
in learning single topic modeling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01225"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01202">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01202">Negative Instance for the Edge Patrolling Beacon Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abel:Zachary.html">Zachary Abel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akitaya:Hugo_A=.html">Hugo A. Akitaya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Martin_L=.html">Martin L. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hesterberg:Adam.html">Adam Hesterberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Matias.html">Matias Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ku:Jason_S=.html">Jason S. Ku</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01202">PDF</a><br /><b>Abstract: </b>Can an infinite-strength magnetic beacon always ``catch'' an iron ball, when
the beacon is a point required to be remain nonstrictly outside a polygon, and
the ball is a point always moving instantaneously and maximally toward the
beacon subject to staying nonstrictly within the same polygon? Kouhestani and
Rappaport [JCDCG 2017] gave an algorithm for determining whether a
ball-capturing beacon strategy exists, while conjecturing that such a strategy
always exists. We disprove this conjecture by constructing orthogonal and
general-position polygons in which the ball and the beacon can never be united.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01202"><span class="datestr">at June 03, 2020 11:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/">Post-Doctoral Research Fellowship at All Souls College, University of Oxford (apply by September 11, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>All Souls is primarily a research institution with particular strengths in the Humanities and Social and Theoretical Sciences, and with strong connections to public life. It is strongly committed to supporting early career scholars. The Fellowships are intended to offer opportunities for outstanding early career researchers to establish a record of independent research.</p>
<p>Website: <a href="https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars">https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars</a><br />
Email: pdrf.admin@all-souls.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/"><span class="datestr">at June 02, 2020 11:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4830">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4830">The US might die, but P and PSPACE are forever</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned <a href="https://www.preposterousuniverse.com/podcast/2020/06/01/99-scott-aaronson-on-complexity-computation-and-quantum-gravity/">two-hour podcast</a>, where theoretical physicist and longtime friend-of-the-blog <a href="https://www.preposterousuniverse.com/">Sean Carroll</a> interviews yours truly about complexity theory!  Here’s Sean’s description of this historic event:</p>



<blockquote class="wp-block-quote"><p>There are some problems for which it’s very hard to find the answer, but very easy to check the answer if someone gives it to you. At least, we think there are such problems; whether or not they really exist is the famous <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">P vs NP problem</a>, and actually proving it will win you <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">a million dollars</a>. This kind of question falls under the rubric of “computational complexity theory,” which formalizes how hard it is to computationally attack a well-posed problem. Scott Aaronson is one of the world’s leading thinkers in computational complexity, especially the wrinkles that enter once we consider quantum computers as well as classical ones. We talk about how we quantify complexity, and how that relates to ideas as disparate as creativity, knowledge vs. proof, and what all this has to do with black holes and quantum gravity.</p></blockquote>



<p>So, OK, I guess I should also comment on the national disintegration thing.  As someone who was once himself the victim of a crazy police overreaction (albeit, trivial compared to what African-Americans regularly deal with), I was moved by the scenes of police chiefs in several American towns taking off their helmets and joining protesters to cheers.  Not only is that a deeply moral thing to do, but it serves a practical purpose of quickly defusing the protests.  Right now, of course, is an <em>even worse time than usual</em> for chaos in the streets, with a lethal virus still spreading that doesn’t care whether people are congregating for good or for ill.  If rational discussion of policy still matters, I support the current push to end the “qualified immunity” doctrine, end the provision of military training and equipment to police, and generally spur the nation’s police to rein in their psychopath minority.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4830"><span class="datestr">at June 01, 2020 07:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7741">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/">Theory of Machine Learning summer seminar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like to relay <a href="https://twitter.com/red_abebe/status/1267145593991946245">Rediet Abebe’s call</a> to support local organizations. As Rediet says “These problems have been and will be here for a very long time. We’re not solving racism this month.”. –Boaz]</em></p>



<p>For the last year, I have been co organizing a <a href="https://mltheory.org/#talks">theory of machine learning seminar</a> at Harvard. Following the format of our prior Harvard/MSR/MIT theory reading group, these have been extended blackboard talks with plenty of audience interaction.</p>



<p>Following COVID-19, the last three talks in the semester (by Moritz Hardt,  Zico Kolter, and Anima Anandkumar)  were given virtually. Frankly, I was at first unsure whether these seminars can work in the virtual format but was pleasantly surprised. Talks have been very interactive, with plenty of audience participation in the chat channel. In fact, the virtual format has some <em>advantages </em>over physical talks. Sometimes a question will be asked and answered by a co-author over chat, without the speaker needing to interrupt the talk.</p>



<p>Since the seminars were so successful, we decided to continue holding them over the summer. We have an exciting line up of confirmed speakers, and more will come soon. See <a href="https://mltheory.org/#talks">our webpage</a> for more details, which also contains a google calendar and a mailing list you can sign up for to get the Zoom link.</p>



<p>Confirmed speakers so far include:</p>



<ul><li><a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein</a> – June 11</li><li><a href="https://www.neyshabur.net/">Behnam Neyshabur</a> – June 18</li><li><a href="https://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a> – July 9</li><li><a href="https://www.cohennadav.com/">Nadav Cohen</a> – August 6</li><li><a href="https://maithraraghu.com/">Maithra Raghu</a> – date tbd</li><li><a href="https://research.google/people/HanieSedghi/">Hanie Sedghi</a> – date tbd</li></ul>



<p>More should be confirmed soon – join our <a href="https://groups.google.com/a/seas.harvard.edu/forum/#!forum/ml-theory-seminar/join">mailing list</a> to get updates.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/"><span class="datestr">at June 01, 2020 04:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2566">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">Gradient descent for wide two-layer neural networks – I : Global convergence</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Supervised learning methods come in a variety of flavors. While local averaging techniques such as <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">nearest-neighbors</a> or <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a> are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is simple: optimize the (potentially regularized) risk on training data over prediction functions in a pre-defined set of functions.</p>



<p class="justify-text">When the set of a functions is a convex subset of a vector space with a finite-dimensional representation, with standard assumptions, the corresponding optimization problem is convex. This has the benefits of allowing a thorough theoretical understanding of the computational and statistical properties of learning methods, which often come with strong theoretical guarantees, in terms of running time [<a href="https://arxiv.org/pdf/1405.4980">1</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">2</a>, <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">3</a>] or prediction performance on unseen data [<a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">4</a>, <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">5</a>]. In particular, the linear parameterization can be done either explicitly by building a typically large finite set of features, or implicitly through the use of kernel methods and then a series of dedicated algorithms and theories can be leveraged for efficient non-linear predictions [6, <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">7</a>, <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">8</a>].</p>



<p class="justify-text">However, linearly-parameterized sets of functions do not include neural networks, which lead to state-of-the-art performance in most learning tasks in computer vision, natural language processing, speech processing, in particular through the use of deep and convolutional neural networks [<a href="https://www.deeplearningbook.org/">9</a>].</p>



<h2>Two-layer neural networks with “relu” activations</h2>



<p class="justify-text">The goal of this blog post is to provide some understanding of why supervised machine learning work for the simplest form of such models: $$ h(x) = \frac{1}{m} \sum_{i=1}^m a_i ( b_i^\top x)_+ = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\},$$ where the input \(x\) is a vector in \(\mathbb{R}^d\), and \(m\) is the number of hidden neurons. The weights \(a_i \in \mathbb{R}\), \(i=1,\dots,n\), are the <em>output weights</em>, while the weights \(b_i \in \mathbb{R}^d\), \(i=1,\dots,n\), are the <em>input weights</em>. The rectified linear unit (“relu”) [<a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">10</a>] activation is used, and our results will depend heavily on its positive homogeneity (that is, for \(\lambda &gt; 0\), \((\lambda u)_+ = \lambda u_+\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="407" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/nn_single_blog.png" class="wp-image-3829" height="292" />Two-layer neural network in dimension \(d = 6\) with \(m=4\) hidden neurons, and a single output.</figure></div>



<p class="justify-text">Note that this is an idealized and much simplified set-up for deep learning, as there is a single hidden-layer, no convolutions, no pooling, etc. As I will show below, this simple set-up is already complex to understand, and I believe it captures some of the core difficulties associated with non-convexity.</p>



<p class="justify-text">The first question that one may come to after decades of research in learning theory is: <em>why is it so hard to analyze?</em>  </p>



<p class="justify-text">There are at least two major difficulties:</p>



<ul class="justify-text"><li><strong>Non-linearity</strong>: the dependence on the input weights \(b_i\)’s is non-linear because of the activation function, typically leading to non-convex optimization problems.</li><li><strong>Overparameterization</strong>: The number \(m\) of hidden neurons is very large (often so large that the number of parameters \(m(d+1)\) exceeds the number of observations), which is hard in terms of optimization and potentially generalization to unseen data. </li></ul>



<p class="justify-text">In this blog post, we will leverage the overparameterization and take \(m\) tending to infinity (without any dependence on the number of observations), which will allow us to derive theoretical results. We will leverage two key properties of the problem:</p>



<ul class="justify-text"><li><strong>Separability</strong> of the model in \(w_i = (a_i,b_i)\), that is, the prediction function \(h(x)\) is the sum of terms which are independently parameterized, as \(h = \frac{1}{m} \sum_{i=1}^m \Phi(w_i)\), where \(\Phi: \mathbb{R}^p \to \mathcal{F}\), with \(\mathcal{F}\) a space of functions. In our situation, \(p = d+1\) and: $$ \Phi(w)(x) = a (b^\top x)_+. $$ In other words,  there is no parameter sharing among hidden neurons. Unfortunately, this does not generalize to more than a single hidden layer.</li><li><strong>Homogeneity</strong>: the relu activation is positively homogeneous so that as as function of \(w = (a,b) \in \mathbb{R} \times \mathbb{R}^d\), \(\Phi(w)(x) = a (b^\top x)_+\) is positively 2-homogeneous, that is, \(\Phi(\lambda w) = \lambda^2 \Phi(w)\) for \(\lambda &gt; 0\).</li></ul>



<p class="justify-text">In this sequence of two blog posts, following a recent trend in optimization and machine learning theory [<a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">11</a>, <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">12</a>], optimization and statistics cannot be separated and need to be tackled together. I will focus on gradient flows on empirical or expected risks.</p>



<p class="justify-text">In this blog post, I will cover optimization and how over-parameterization leads to global convergence for 2-homogeneous models, a recent result obtained two years ago with <a href="https://lchizat.github.io/">Lénaïc Chizat</a> [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]. This requires tools from optimal transport which I will briefly describe (for more details, see, e.g., [<a href="https://arxiv.org/abs/1803.00567">14</a>]).</p>



<p class="justify-text">Next month, I will focus on generalization capabilities and the several implicit biases associated with gradient descent in this context [<a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">15</a>, <a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<h2>Infinitely wide limit and probability measures</h2>



<p class="justify-text">Following the standard learning set-up, our goal will be to minimize with respect to the prediction function \(h\) the functional \(R\) defined as $$ R(h) = \mathbb{E}_{p(x,y)} \big[ \ell( y, h(x) ) \big],$$ where \(\ell(y,h(x))\) is the loss incurred by outputting \(h(x)\) when \(y\) is the true label. Even within deep learning, this loss is most often convex in its second argument, such as for least-squares or <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">logistic</a> losses. Thus, I will assume that \(R\) is convex.</p>



<p>The expectation can be considered in two scenarios:</p>



<ul class="justify-text"><li><strong>Empirical risk</strong>: this corresponds to the situation where we have observations \((x_j,y_j)\), \(j=1,\dots,n\), coming from some joint distribution on \((x,y) \in \mathbb{R}^d \times \mathbb{R}\). Minimizing \(R\) then may not lead to any guarantee on unseen data unless some explicit or implicit regularization is used. In next blog post, I will consider the implicit regularization effect of gradient-based algorithms.</li><li><strong>Expected risk (or generalization performance)</strong>: The expectation is taken with respect to unseen data, and thus its value (or a gradient) cannot be computed. However, any training observation \((x_j,y_j)\) can lead to an unbiased estimate, and if single pass stochastic gradient is used, our guarantees will be on the expected risk.</li></ul>



<p class="justify-text">The main and very classical idea is to consider the minimization of $$ G(W) = G(w_1,\dots,w_m) = R \Big( \frac{1}{m} \sum_{i=1}^m \Phi(w_i) \Big),$$ and see it as the minimization of $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ with respect to a probability measure \(\mu\), with the equivalence for $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ where \(\delta(w_i)\) is the Dirac measure at \(w_i\). See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="615" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/diracs_measures-1-1024x156.png" class="wp-image-3852" height="93" />Left: discrete probability measure. Right: measure with density.</figure></div>



<p class="justify-text">When \(m\) is large, we can represent any measure in the <a href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak sense</a> (that is, expectations of any continuous and bounded functions can be approximated). The benefits of considering the space of all measures instead of discrete measures have been used already in variety of contexts in machine learning, statistics and signal processing [<a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">17</a>, <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">18</a>, <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">19</a>]. In this blog post, the key benefit is that the set of measures in convex and \(h = \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \) is linear in the measure \(\mu\), so that our optimization problem has become convex.</p>



<p class="justify-text">However, (1) It does not buy much in practice, as the set of probability measures is infinite-dimensional. <a href="https://en.wikipedia.org/wiki/Frank–Wolfe_algorithm">Frank-Wolfe algorithms</a> can be used, but the choice of new neurons is a difficult optimization problem, NP-hard for the threshold activation function [<a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">20</a>], with polynomial potentially high complexity for the relu activation [<a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">21</a>], and (2) this is not what is used in practice, which is (stochastic) gradient descent.</p>



<h2>Finite-dimensional gradient flow</h2>



<p class="justify-text">In this post, I will consider the gradient flow $$\dot{W} = \ – m \nabla G(W),$$ (where \(m\) is added as a normalization factor to allow a well-defined limit when \(m\) tends to infinity). This is still not exactly what is used in practice, but, as explained in <a href="https://francisbach.com/gradient-flows/">last month post</a>, this is a good approximation of gradient descent (if using the empirical risk, then leading to guarantees of global convergence on the empirical risk only), or stochastic gradient descent (if doing a single pass on the data, then leading to guarantees of global convergence on unseen data). This is a non-convex dynamics, with stationary points and local minima, even when \(m\) is large (see, e.g., [<a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">27</a>]).</p>



<p class="justify-text">Two main questions arise: (1) what does the gradient flow dynamics converge to when the number of neurons \(m\) tends to infinity, and (2) can we get any global convergence guarantees for the limiting dynamics?</p>



<h2>Mean-field limit and Wasserstein gradient flows</h2>



<p class="justify-text">When \(m\) tends to infinity, since we want to use the measure representation, we need to understand the effect of performing the gradient flow jointly on \(w_1,\dots,w_m\) on the measure $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ and see if we can take a limit when \(m\) tends to infinity. This process of taking limits is common in physics and often referred to as the “mean-field” limit, and has been considered in a series of recent works [<a href="https://arxiv.org/pdf/1712.05438">22</a>, <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>, <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">23</a>, <a href="https://arxiv.org/pdf/1805.00915">24</a>]. To avoid too much technicality, I will assume that the map \(\Phi\) is sufficiently differentiable, which unfortunately exclude the relu activation; for dedicated results, see [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>].</p>



<p class="justify-text"><strong>Gradient flows on metric spaces.</strong> In order to understand the dynamics in the space of probability measures, we need to take a step backward and realize that gradient flows can be defined for many functions \(f\) on any metric space \(\mathcal{X}\). Indeed, it can be seen as the limit of taking infinitesimal steps of length \(\gamma\), where each new iterate \(x_{k+1}\) (corresponding to the value at time \(k\gamma\)) is defined recursively from \(x_k\) as $$x_{k+1} \in \arg\min_{x \in \mathcal{X}}\  f(x) + \frac{1}{2\gamma} d(x,x_k)^2.$$ As shown in [<a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">25</a>, <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">26</a>], with some form of interpolation, this defines a curve with prescribed values \(x_k\) at each \(\gamma k\), and when the step-size \(\gamma\) goes to zero, this curves “converges” to the gradient flow.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="419" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/euler-1024x520.png" class="wp-image-3960" height="212" />Gradient flow in bold black, with interpolating curve in red from 14 points \(x_0,\dots,x_{13}\).</figure></div>



<p class="justify-text">For the space \(\mathcal{X} = \mathbb{R}^d\) with the Euclidean distance and a continuously differentiable function \(f\), we obtain that $$x_{k+1} = x_k – \gamma f'(x_k) + o(\gamma),$$ and we get the usual gradient flow associated to \(f\), and the scheme above is nothing less than <a href="https://en.wikipedia.org/wiki/Euler_method">Euler discretization</a> that was described <a href="https://francisbach.com/gradient-flows/">last month</a>.</p>



<p class="justify-text"><strong>Vector space gradient flows on probability measures.</strong> Probability measures are a convex subset of measures with finite <a href="https://en.wikipedia.org/wiki/Total_variation#Total_variation_of_probability_measures">total variation</a>, which is equal to the \(\ell_1\)-norm between densities when the two probability measures have densities with respect to the same base measure. It is a normed vector space for which we could derive our first type of gradient flow, which can be seen as a continuous version of Frank-Wolfe algorithm, where atoms are added one by one, until convergence.  </p>



<p class="justify-text">As mentioned above, the fact that atoms are created sequentially seems attractive computationally. However, (1) deciding which one to add is a computationally hard problem, and (2) the flow on measures cannot be approximated by a finite evolving set of “particles” (here hidden neurons each defined by a vector \(w \in \mathbb{R}^{d+1}\)).</p>



<p class="justify-text"><strong>Wasserstein gradient flows on probability measures.</strong> There is another natural distance here, namely the <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> (sometimes called the Kantorovich–Rubinstein distance). In order to remain short, I will only define it between empirical measures $$\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i) \mbox{ and } \nu = \frac{1}{m} \sum_{i=1}^m \delta(v_i)$$ with the same number of points. The squared 2-Wasserstein distance is obtained by minimizing $$\frac{1}{m} \sum_{i=1}^m \| w_j – v_{\sigma(j)} \|_2^2$$ over all permutations \(\sigma: \{1,\dots,m\} \to \{1,\dots,m\}\). See illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="573" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein-2-1024x307.png" class="wp-image-3886" height="171" />Wasserstein distance between two empirical measures: (left) original observations of two empirical measures with \(m = 11\) points, (right) assigning all black points to red points by minimizing the sum of squared distances between assigned points.</figure></div>



<p class="justify-text">This can be extended to any pair of probability measures, and used within gradient flows, it has a very natural decoupling property: if \(\mu\) is fixed, and \(\nu\) is within a small distance of \(\mu\) in Wasserstein distance, then the optimal permutation above will always be the same, that is, locally, the Wasserstein distance is a sum of squared Euclidean distances. Then, the Wasserstein gradient flow will lead to \(m\) independent local regular Euclidean gradient flows, which interact through the gradient term as: $$ \dot{w}_i = \ –  \nabla \Phi(w_i)  \nabla R\Big(\int_{\mathbb{R}^p} \Phi d\mu \Big),$$ where the Jacobian \(\nabla \Phi(w_i)\) is a linear operator from \(\mathcal{F}\) to \(\mathbb{R}^p\), and \( \nabla R: \mathbb{R}^p \to \mathcal{F}\) the gradient operator of \(R\). Since \(\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i)\), the dynamics of each particle interacts through the gradient of \(R\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="456" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein_flows-1024x508.png" class="wp-image-3890" height="225" />Gradient flow for \(m=7\) interacting particles.</figure></div>



<p class="justify-text">The intuitive reasoning above is behind the formal result for the function $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ that the limit of the Euclidean gradient flow on each particle when \(m\) tends to infinity, is exactly the Wasserstein gradient flow of \(F\). While I have proposed an intuitive explanation, this can be made more formal in particular through the use of partial differential equations on the density of the measure [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>] (see also nice <a href="http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf">slides</a> from Katy Craig on Wasserstein gradient flows).</p>



<p class="justify-text"><strong>Stationary points.</strong> Since \(R\) is assumed convex over the convex set of probability measures, all local minima of \(R\) are global, and we should expect the gradient flow to converge to global optimum from any initial measure. This is true for the gradient flow associated with the total variation metric. However this is not true for the Wasserstein gradient flow, for which stationary points which are not global minimizers exist (given that for discrete measures this corresponds to classical backpropagation, this is well known to anybody who has ever trained a neural network). Note that there exists a notion of convexity for Wasserstein gradient flows, namely <a href="https://en.wikipedia.org/wiki/Geodesic_convexity">geodesic convexity</a>, but the function \(F\) is not geodesically convex in general.</p>



<h2>Global convergence</h2>



<p class="justify-text">We can now describe the main result from our recent work with Lénaïc [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]: under assumptions described below, for the function \(F\) defined above, if the Wasserstein gradient flow converges to a measure, this measure has to be a global minimum of \(F\) (note that we cannot prove it is always convergent).</p>



<p class="justify-text">On top of technical regularity assumptions that I will not describe here, we need two crucial broad assumptions:</p>



<ul class="justify-text"><li><strong>Homogeneity</strong> of the function \(\Phi: \mathbb{R}^{d+1} \to \mathcal{F}\). We need a condition of this form, since if \(R\) is a linear function, then \(F(\mu)\) is of the form \(F(\mu) = \int_{\mathbb{R}^p} \psi(w)d\mu(w)\) with \(\psi(w) = R(\Phi(w))\), and the Wasserstein gradient flow will converge to a weighted some of Diracs at all local minimizers of \(\psi\), which is typically not a global minimizer.</li><li><strong>Initialization with positive mass in all directions</strong>. That is, \(w_i\)’s are uniformly distributed on the sphere or Gaussian, which is the de facto choice in practice. </li></ul>



<p class="justify-text"><strong>Illustration</strong>. We illustrate the result above by considering \(R\) as the square loss and \(y\) being generated from \(x\) through a neural network with \(m_0=5\) neurons. When running the gradient flow above, as soon as \(m \geqslant 5\), the model is sufficiently flexible to attain zero loss, which is thus the global optimum of the cost function. However, the gradient flow may not reach it, as it gets trapped in a local optimum. Our theoretical result suggests that when \(m\) is large, we should converge to the original neurons, which we see below. The surprising (and still unexplained) phenomenon is that \(m\) does not need to be much larger than \(m_0\) to see practical global convergence.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="389" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m5-1024x683.png" class="wp-image-3942" height="259" />Position of \(m = 5\) neurons, plotted as \(|a_i| b_i \in \mathbb{R}^2\) for a two-dimensional problem. The five dotted lines are the directions of the generating neurons. Although \(m\) is large enough to lead to the global optimum, the flow gets stuck in a local optimum.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="365" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m10-1024x683.png" class="wp-image-3943" height="243" />Position of \(m = 10\) neurons; same setting as above. The flow converges to the global optimum, although \(m\) is not large.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="379" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100-1024x683.png" class="wp-image-3944" height="252" />Position of \(m = 100\) neurons; same setting as above. The flow converges to the global optimum, with \(m\) large. See video below.</figure></div>



<figure class="wp-block-video aligncenter justify-text"><video src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4"></video>Position of \(m = 100\) neurons; exact same setting as above.</figure>



<h2>Discussion and open problems</h2>



<p class="justify-text">In this blog post, I described theoretical results showing the benefits of overparameterization: when the number of hidden neurons \(m\) tends to infinity, then the corresponding gradient flow converges to the global optimum of the cost function. The proof relies notably on homogeneity properties of the relu activation. </p>



<p class="justify-text">The main weakness of  this result is that is only <em>qualitative</em>: we cannot quantify how big \(m\) need to be to be close to the infinite width limit, or how fast the gradient flow converges to the global optimum. These are still open problems. Additional interesting areas of research are to extend these results to convolutional and/or deep networks.</p>



<p class="justify-text">Now that we know that we can obtain global convergence, I will describe next month the generalization properties when interpolating the training data with an overparameterized relu network [<a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat for producing the nice figures and video of neural networks, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980">Convex Optimization: Algorithms and Complexity</a>. <em>Foundations and Trends in Machine Learning</em>, <em>8</em>(3-4), 231-357, 2015.<br />[2] Léon Bottou, Frank E. Curtis, Jorge Nocedal. <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">Optimization methods for large-scale machine learning</a>. SIAM Review, 60(2):223-311, 2018.<br />[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski. <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. <em>Foundations and Trends in Machine Learning, </em>4(1):1-106, 2012.<br />[4] Shai Shalev-Shwartz, Shai Ben-David. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding machine learning: From theory to algorithms</a>. Cambridge University Press, 2014.<br />[5] Larry Wasserman. <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">All of statistics: a concise course in statistical inference</a>. Springer Science &amp; Business Media, 2013.<br />[6] Bernhard Schölkopf, Alexander J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.<br />[7] Ali Rahimi and Benjamin Recht. <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines</a>. <em>Advances in neural information processing systems</em>, 2008.<br />[8] Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco. <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">Falkon: An optimal large scale kernel method</a>. <em>Advances in Neural Information Processing Systems</em>, 2017.<br />[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a href="https://www.deeplearningbook.org/">Deep learning</a>. MIT Press, 2016.<br />[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. <a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks</a>. <em>International Conference on Artificial Intelligence and Statistics</em>, 2011.<br />[11] Francis Bach and Eric Moulines. <a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)</a>. <em>Advances in Neural Information Processing Systems</em>, 2013.<br />[12] MIkhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off</a>. <em>Proceedings of the National Academy of Sciences</em>, 116(32), 15849-15854, 2019.<br />[13] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018.<br />[14] Gabriel Peyré, Marco Cututi. <em><a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a></em>. Foundations and Trends in Machine Learning, 51(1):1–44, 2019.<br />[15] Lénaïc Chizat, Edouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On Lazy Training in Differentiable Programming</a>. <em>Advances in Neural Information Processing Systems</em>, 2019.<br />[16] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. Technical report, arXiv:2002.04486, 2020.<br />[17] Andrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function</a>. <em>IEEE Transactions on Information theory</em>, <em>39</em>(3), 930-945, 1993.<br />[18] Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte. <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">Convex neural networks</a>. <em>Advances in neural information processing systems</em>, 2006.<br />[19] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a>.<strong> </strong><em>Journal of Machine Learning Research</em>, 18(19):1-53, 2017.<br />[20] Venkatesan Guruswami, Prasad Raghavendra. <a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">Hardness of learning halfspaces with noise</a>. <em>SIAM Journal on Computing</em>, 39(2):742-765, 2009.<br />[21] Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler. <a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">Reliably Learning the ReLU in Polynomial Time</a>. <em>Conference on Learning Theory</em>, 2017.<br />[22] Atsushi Nitanda, Taiji Suzuki. <a href="https://arxiv.org/pdf/1712.05438">Stochastic particle gradient descent for infinite ensembles</a>. Technical report, arXiv:1712.05438, 2017.<br />[23] Song Mei, Andrea Montanari, Phan-Minh Nguyen. <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">A mean field view of the landscape of two-layer neural networks</a>. <em>Proceedings of the National Academy of Sciences</em> 115(33):E7665-E7671, 2018.<br />[24] Grant M. Rotskoff, Eric Vanden-Eijnden. <a href="https://arxiv.org/pdf/1805.00915">Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error</a>. Technical report, arXiv:1805.00915, 2018.<br />[25] Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré. <a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">Gradient flows: in metric spaces and in the space of probability measures</a>. Springer Science &amp; Business Media, 2008<br />[26] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br />[27] Itay Safran, Ohad Shamir. <a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a>. <em>International Conference on Machine Learning</em>, 2018.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/"><span class="datestr">at June 01, 2020 07:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/diana-davis-beautiful-pentagons/">Diana Davis’s beautiful pentagons</a> (<a href="https://mathstodon.xyz/@11011110/104182521046531449"></a>). I briefly mentioned her regular-pentagon billiards-trajectory art in <a href="https://11011110.github.io/blog/2019/02/15/linkage.html">an earlier post</a> but now Evelyn Lamb has a much more detailed column on her and her work.</p>
  </li>
  <li>
    <p><a href="https://inference-review.com/article/points-and-lines">Points and lines</a> (<a href="https://mathstodon.xyz/@11011110/104190473820256744"></a>). A new review of my book <em><a href="https://www.ics.uci.edu/~eppstein/forbidden/">Forbidden Configurations in Discrete Geometry</a></em>, by Daniel Kleitman, in <em>Inference</em>.</p>
  </li>
  <li>
    <p><a href="https://www.mathi.uni-heidelberg.de/~roquette/noetherphoto-engl.pdf">About photos of Emmy Noether</a> (<a href="https://mathstodon.xyz/@11011110/104193371530927420"></a>), in which Peter Roquette apologizes for having indirectly caused <a href="https://books.google.com/books/about/Emmy_Noether.html?id=IePqBgAAQBAJ">Margaret Tent’s young-adult historical-fiction about Noether</a> to have a photo of someone else on its cover. Via MarkH<sub>21</sub> on Wikipedia, in the context of <a href="https://commons.wikimedia.org/wiki/Commons:Deletion_requests/File:Noether.jpg">a discussion of the provenance of a different photo of Noether</a>.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.4007/annals.2020.191.2.5">The Conway knot is not slice</a> (<a href="https://mathstodon.xyz/@11011110/104196369094406901"></a>). Newly published result by Lisa Piccirillo in <em>Ann. Math.</em>, with <a href="https://www.quantamagazine.org/graduate-student-solves-decades-old-conway-knot-problem-20200519/">an overview of the significance of the result (if not much of its detail) in <em>Quanta</em></a>.</p>
  </li>
  <li>
    <p><a href="https://gowers.wordpress.com/2020/05/20/mathematical-research-reports-a-new-mathematics-journal-is-launched/">Tim Gowers relates the convoluted history of a mathematical announcements journal</a> (<a href="https://mathstodon.xyz/@11011110/104205046534922072"></a>). <em>Electronic Research Announcements of the AMS</em> (founded 1995) moved to the American Inst. of Mathematical Sciences in 2007, but recent heavyhanded moves by the publisher led the editorial board to quit, and its name changed to <em>Electronic Research Archive</em>. In the meantime the old editorial board have a new journal: <em><a href="https://mrr.centre-mersenne.org/">Mathematical Research Reports</a></em>. See the Gowers link for details.</p>
  </li>
  <li>
    <p>In preparation for time travel week in graduate data structures, here’s <a href="http://www.tasteofcinema.com/2014/the-20-best-time-travel-movies-of-all-time/">a listicle of 20 time travel movies</a> (<a href="https://mathstodon.xyz/@11011110/104208167329418315"></a>). There are any number of these lists but for me they should include at least <em>12 Monkeys</em>, <em>Primer</em>, <em>Safety Not Guaranteed</em>, <em>Donnie Darko</em>, and <em>Time Bandits</em>. This one adds <em>The Girl Who Leapt Through Time</em>, also good. I’d have thrown in <em>The Infinite Man</em> but it’s obscure enough that I’m not offended by its absence. The relevant one for my class, <em>Retroactive</em>, can stay off.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2020/05/xavier-puente-vilardell-wood-sculpture/">Spirals and loops twist through wooden sculptures by Xavier Puente Vilardell</a> (<a href="https://mathstodon.xyz/@11011110/104216153044887740"></a>).</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/a/361166/440">Dmitri Panov constructs infinitely many convex 4-polytopes with no triangle or quadrilateral 2-faces</a> (<a href="https://mathstodon.xyz/@11011110/104222101065596394"></a>). The construction is pretty: take some 120-cells (all 2-faces regular pentagons and all 3-faces regular dodecahedra), embedded into hyperbolic space so all dihedrals are right angles, and glue them together on shared facets. If at most two touch at each ridge, the result is convex. Some pairs of pentagons merge into hexagons, but you still have no triangles or quads.</p>
  </li>
  <li>
    <p><a href="https://cp4space.wordpress.com/2013/09/06/ten-things-you-possibly-didnt-know-about-the-petersen-graph/">Ten things you (possibly) didn’t know about the Petersen graph</a> (<a href="https://mathstodon.xyz/@11011110/104226455130356772"></a>). Found while making <a href="https://en.wikipedia.org/wiki/The_Petersen_Graph">a Wikipedia article on the book <em>The Petersen Graph</em></a> (for the graph itself, see <a href="https://en.wikipedia.org/wiki/Petersen_graph">its Wikipedia article</a>).</p>
  </li>
  <li>
    <p><a href="https://www.taylorfrancis.com/books/9781351247771">Godfried Toussaint’s book <em>The Geometry of Musical Rhythm:
What Makes a “Good” Rhythm Good?</em>, on the mathematical analysis of drumming patterns, has a new expanded posthumous 2nd edition</a> (<a href="https://mathstodon.xyz/@11011110/104233265094045761"></a>). I was able to download free from there but that may be via a campus subscription; your access may vary. For a description of the 1st edition of the book (probably undetailed enough that it can describe the 2nd as well) see <a href="https://en.wikipedia.org/wiki/The_Geometry_of_Musical_Rhythm">its Wikipedia article</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=CfRSVPhzN5M">French video about self-replicating patterns in cellular automata, with English subtitles</a> (<a href="https://mathstodon.xyz/@11011110/104239007149697974"></a>, <a href="https://cp4space.wordpress.com/2019/06/11/self-replicator-caught-on-video/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@erou/104245194532017940">@erou visualizes the complexity of Karatsuba’s algorithm for integer multiplication as a Sierpiński triangle</a>, inside a square, with a number of dark pixels proportional to the steps of the algorithm. The square itself counts in the same way the complexity of the naive algorithm.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/quicktakes/2020/05/28/proposed-legislation-would-bar-chinese-stem-graduate-students">Republicans propose legislation to bar Chinese from science</a> (<a href="https://mathstodon.xyz/@11011110/104248070190238732"></a>). I’m having difficulty distinguishing this sort of move from “<a href="https://en.wikipedia.org/wiki/Nuremberg_Laws">Nazis propose legislation to bar Jews from science</a>”.</p>
  </li>
  <li>
    <p><a href="https://wrog.dreamwidth.org/63735.html">Arthur C. Clarke and the projective plane</a> (<a href="https://mathstodon.xyz/@ColinTheMathmo/104253017415627794"></a>). Wrog beanplates Clarke’s “The Wall of Darkness” (1949).</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1112/blms/16.3.278">Volumes of projections of unit cubes</a> (<a href="https://mathstodon.xyz/@11011110/104261517104618440"></a>), Peter McMullen, <em>Bull LMS</em> 1984. A cute theorem that deserves to be better known: if you hold a unit cube in the noonday sun, at any angle, its shadow’s area equals its height (elevation difference between lowest and highest point). It follows immediately that the biggest possible shadow is a hexagon with area = long diagonal length = , and the smallest shadow is a unit square. Similar things happen in higher dimensions.</p>
  </li>
  <li>
    <p><a href="http://roberthodgin.com/project/meander">Meander, a procedural system for generating historical maps of rivers that never existed</a> (<a href="https://mathstodon.xyz/@11011110/104265448606504741"></a>, <a href="https://www.metafilter.com/187292/Meander-generating-historical-maps-of-rivers-that-never-existed">via</a>). The way this system models the motion of river beds over time looks a lot like the time-reversal of the curve-shortening flow, but with added tangential motion that causes bends to flow downstream (and maybe helps maintain smoothness) and with shortcutting of oxbows.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/05/31/linkage.html"><span class="datestr">at May 31, 2020 04:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/083">TR20-083 |  Proximity Gaps for Reed-Solomon Codes | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Yuval Ishai, 

	Swastik Kopparty, 

	Shubhangi Saraf</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A collection of sets displays a proximity gap with respect to some property if for every set in the collection, either (i) all members are $\delta$-close to the property in relative Hamming distance or (ii) only a tiny fraction of members are $\delta$-close to the property. In particular, no set in the collection has roughly half of its members $\delta$-close to the property and the others $\delta$-far from it.

We show that the collection of affine spaces displays a proximity gap with respect to Reed-Solomon (RS) codes, even over small fields, of size polynomial in the dimension of the code, and the gap applies to any $\delta$ smaller than the Johnson/Guruswami-Sudan list-decoding bound of the RS code. We also show near-optimal gap results, over fields of (at least) linear size in the RS code dimension, for $\delta$ smaller than the unique decoding radius. Finally, we discuss several applications of our proximity gap results to distributed storage, multi-party cryptographic protocols, and concretely efficient proof systems.

We prove the proximity gap results by analyzing the execution of classical algebraic decoding algorithms for Reed-Solomon codes (due to Berlekamp-Welch and Guruswami-Sudan) on a formal element of an affine space. This involves working with Reed-Solomon codes whose base field is an (infinite) rational function field. Our proofs are obtained by developing an extension (to function fields) of a strategy of Arora and Sudan for analyzing low-degree tests.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/083"><span class="datestr">at May 30, 2020 07:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17089">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/">Just Arvind</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Theory and practice</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/unknown-142/" rel="attachment wp-att-17098"><img width="180" alt="" class="alignright  wp-image-17098" src="https://rjlipton.files.wordpress.com/2020/05/unknown-2.jpeg?w=180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ MIT ]</font></td>
</tr>
</tbody>
</table>
<p>
Arvind Mithal—almost always referred to as Arvind—is now the head of the faculty of computer science at a Boston trade school. The school, also known as MIT, is of course one of the top places for all things computer science. From education to service to startups to research, MIT is perhaps the best in the world.</p>
<p>
Today I thought we might discuss one of Arvind’s main research themes.</p>
<p>
Although this work started in the 1980’s I believe that it underscores an important point. This insight might apply to current research topics like <a href="https://en.wikipedia.org/wiki/Timeline_of_quantum_computing">quantum computers</a>.</p>
<p>
But first I cannot resist saying something personal about him. Arvind is a long time friend of mine, and someone who gets the <i>stuck in elevator measure</i> of many hours. This measure is one I made up, but I hope you get the idea.</p>
<p>
Arvind is the only name I have ever known for Arvind. I was a bit surprised to see that the Wikipedia reference for <a href="https://en.wikipedia.org/wiki/Arvind_(computer_scientist)">him</a> states his “full” name. He told me that he found having one name—not two—was a challenge. For example, he sometimes had to explain when arriving at a check-in desk at a conference that he only had one name. His name tag often became: </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/tag/" rel="attachment wp-att-17092"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/tag.png?w=300&amp;h=222" class="aligncenter size-medium wp-image-17092" height="222" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>because the conference software could not handle people with one name. </p>
<p>
</p><p></p><h2> Dataflow—The prehistory </h2><p></p>
<p></p><p>
About forty years ago one of the major open problem in CS was how to make computers go faster. It is still an issue, but in the 1980’s this problem was one of all-hands-on-deck. It was worked on by software engineers, by electrical engineers, by researchers of all kinds including complexity theorists. Conferences like FOCS and STOC—hardcore theory conferences—often contained papers on how to speed up computations. </p>
<p>
Two examples come to mind:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The idea of <a href="https://en.wikipedia.org/wiki/Systolic_array">systolic</a> arrays led by Hsiang-Tsung Kung then at CMU. Also his students, especially Charles Leiserson, made important contributions. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The idea of the <a href="https://en.wikipedia.org/wiki/Ultracomputer">Ultracomputer</a> led by Jacob Schwartz at NYU. An ultracomputer has N processors, N memories and an N log N message-passing switch connecting them. Note, the use of “N” so you can <a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/">tell</a> that it came from a theorist. </p>
<p>
Arvind tried to make computers faster by inventing a new type of computer architecture. Computers then were based on the classic von Neumann architecture or control flow architecture. He and his colleagues worked for many years trying to replace this architecture by <a href="https://en.wikipedia.org/wiki/Dataflow#Hardware_architecture">dataflow</a>. </p>
<p>
A bottleneck in von Neumann style machines is caused by the program counter fetch cycle. Each cycle the program counter decides which instruction to get, and thus which data to get. These use the same hardware channel, which causes the famous <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">von Neumann bottleneck</a>. We have modified Wikipedia’s graphic to make the bottleneck aspect clearer:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/neck2/" rel="attachment wp-att-17111"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/neck2.png?w=300&amp;h=204" class="aligncenter size-medium wp-image-17111" height="204" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p></p><h2> Dataflow—The promise </h2><p></p>
<p></p><p>
Arvind is usually identified with the invention of <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a191029.pdf">dataflow</a> computer architecture. The key idea of this architecture is to avoid the above bottleneck by eliminating the program counter. If there are no instructions to fetch, then it would seem that we can beat the bottleneck. Great idea.</p>
<p>
Dataflow architectures do not have a program counter, and so data is king. Roughly data objects move around in such a machine, and they eventually appear at computational units. Roughly these machines operate, at a high level, like a directed graph from complexity theory. Data moves along edges to nodes that compute values. Moreover, nodes compute as soon as all the input data is present. After the node computes the value, the new data is sent to the next node; and so on. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/flow-2/" rel="attachment wp-att-17094"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/flow.png?w=300&amp;h=229" class="aligncenter size-medium wp-image-17094" height="229" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The hope was that this would yield a way to increase the performance of computers. No program counter, no instruction fetching, could make dataflow machines faster. </p>
<p>
</p><p></p><h2> Dataflow—The actual </h2><p></p>
<p></p><p>
The dataflow idea is clever. The difficulty is while dataflow can work, classic von Neumann machines have been augmented in various ways. The competition rarely stays put. These updates did not eliminate the von Neumann bottleneck, but they reduce the cost of it, and let von Neumann machines continue to get faster. Caches are one example of how they avoid the bottleneck, thus making the dataflow machines less attractive. Thus, a cache for program instructions makes it likely that the program fetch step does not actually happen. This is an attack on the main advantage of dataflow machines. </p>
<p>
The <a href="http://www.cs.cornell.edu/tve/papers-ucb/limits.pdf">paper</a> by David Culler, Klaus Schauser, and Thorsten von Eicken titled <i>Two Fundamental Limits on Dataflow Multiprocessing?</i> discusses these issues in detail. They start by saying: </p>
<blockquote><p><b> </b> <em> The advantages of dataflow architectures were argued persuasively in a seminal 1983 paper by Arvind and Iannucci and in a 1987 revision entitled “Two Fundamental Issues in Multiprocessing”. However, reality has proved less favorable to this approach than their arguments would suggest. This motivates us to examine the line of reasoning that has driven dataflow architectures and fine-grain multithreading to understand where the argument went awry. </em>
</p></blockquote>
<p>
</p><p></p><h2> Dataflow—Lessons </h2><p></p>
<p></p><p>
What are the lessons? Is there one? </p>
<p>
I claim there are lessons. The work of Arvind on dataflow was and is important. It did not lead to the demise of von Neumann machines. I am using one right now to write this.</p>
<p>
Dataflow did lead to insights on programming that have many applications. The dataflow idea may yet impact special computational situations: there is interest in using them for data intensive applications.</p>
<p>Ken adds that dataflow has been realized in other ways. Caches and pipes and subsequent architecture innovations profit from designs that enhance <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality</a>. The <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a> programming model gives a general framework that fits this well. The paradigm of <a href="http://\href{https://en.wikipedia.org/wiki/Stream_processing">streaming</a> is even a better fit. Note that Wikipedia says both that it is “equivalent to dataflow programming” and “was explored within dataflow programming”—so perhaps the parent lost out to the wider adaptability of her children.</p>
<p>
I think there are lessons: Practical goals like making hardware go faster, are complex and many faceted. A pure theory approach such as systolic arrays or ultra computers or dataflow machines is unlikely to suffice. Also the existing technology like von Neumann machines will continue to evolve. </p>
<p>
A question is: Could quantum computers be subject to the same lesson? Will non-quantum machines continue to evolve in a way to make the quantum advantage less than we think? Or is this type of new architecture different? Could non-quantum machines incorporate tricks from quantum and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Speaking about theory and practice: Noga Alon, Phillip Gibbons, Yossi Matias, and Mario Szegedy are the winners of the 2019 ACM Paris Kanellakis Theory and Practice <a href="https://awards.acm.org/kanellakis">Award</a>. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/paris/" rel="attachment wp-att-17096"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/paris.png?w=300&amp;h=169" class="aligncenter size-medium wp-image-17096" height="169" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>We applaud them on receiving this award. Sadly the Paris award reminds us that Paris Kanellakis died in the terrible plane crash of American Airlines Flight 965 on December 20, 1995. Also on that flight were his wife, Maria Otoya, and their two children, Alexandra and Stephanos.</p>
<p>
The citation for the award says: </p>
<blockquote><p><b> </b> <em> They pioneered a framework for algorithmic treatment of streaming massive datasets, and today their sketching and streaming algorithms remain the core approach for streaming big data and constitute an entire subarea of the field of algorithms. </em>
</p></blockquote>
<p></p><p>
In short they invented streaming. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/"><span class="datestr">at May 29, 2020 09:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/05/29/liberation-from-grades/">Liberation from grades</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This semester, like many other universities, Harvard switched to a pass/fail grade model. (In typical Harvard style, we give them different names – “Emergency Satisfactory” and “Emergency Unsatisfactory” –  but that doesn’t matter much).</p>



<p>One unexpected but happy consequence of this policy is that even though I already submitted the grades for my <a href="https://cs127.boazbarak.org/schedule/">crypto course</a>, I can now take the time and send students detailed feedback on their final projects. Typically,  both students and faculty tend to be focused on the “bottom line” of exams or papers – what is the final grade. The comments are viewed as of marginal importance and only serve to justify why points have been deducted.</p>



<p>Now that there is no grade, I am actually giving many more comments on the write ups, trying to focus on giving students feedback on writing and presentation that will be useful for them later on. I benefited immensely from the extensive comments on my writing that I received from my advisor Oded Goldreich. While I will never match Oded’s thoroughness and dedication, I try to at least provide some of this to my students (though unlike Oded, I use blue and not red ink, and also do not intersperse the comments with Hebrew curses for emphasis <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> )</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/05/29/liberation-from-grades/"><span class="datestr">at May 29, 2020 04:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=439">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/05/28/tcs-talk-wednesday-june-3-michael-p-kim-stanford-university/">TCS+ talk: Wednesday, June 3 — Michael P. Kim, Stanford University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk (and penultimate of the season!) will take place this coming Wednesday, June 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Michael Kim</strong> from Stanford University will speak about “<em>Learning from Outcomes:  Evidence-Based Rankings</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br />
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In this work, we address the task of ranking members of a population according to their qualifications based on a training set of binary outcome data. A natural approach for ranking is to reduce to prediction: first learn to predict individuals’ “probability” of success; then rank individuals in the order specified by the predictions. A concern with this approach is that such rankings may be vulnerable to manipulation. The rank of an individual depends not only on their own qualification but also on every other individuals’ qualifications, so small inaccuracies in prediction may result in a highly inaccurate and unfair induced ranking. We show how to obtain rankings that satisfy a number of desirable accuracy and fairness criteria, despite the coarseness of binary outcome data.<br />
We develop two parallel definitions of evidence-based rankings. First, we study a semantic notion of <em>domination-compatibility</em>: if the training data suggest that members of a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> are on-average more qualified than the members of <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" />, then a ranking that favors <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" /> over <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> (i.e., where <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" /> dominates <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />) is blatantly inconsistent with the data, and likely to be discriminatory. Our definition asks for domination-compatibility, not just for a pair of sets (e.g., majority and minority populations), but rather for every pair of sets from a rich collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> of subpopulations. The second notion—evidence-consistency—aims at precluding even more general forms of discrimination: the ranking must be justified on the basis of consistency with the expectations for every set in the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" />. Somewhat surprisingly, while evidence-consistency is a strictly stronger notion than domination-compatibility when the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> is predefined, the two notions are equivalent when the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> may depend on the ranking itself. Finally, we show a tight connection between evidence-based rankings and multi-calibrated predictors [HKRR’18]. This connection establishes a way to reduce the task of ranking to prediction that ensures strong guarantees of fairness in the resulting ranking.<br />
Joint work with Cynthia Dwork, Omer Reingold, Guy N. Rothblum, and Gal Yona. Appeared at FOCS 2019.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/05/28/tcs-talk-wednesday-june-3-michael-p-kim-stanford-university/"><span class="datestr">at May 28, 2020 11:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/05/28/polymath-reu/">Polymath REU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I am excited to announce a new program for undergraduates: The Polymath REU will run during the summer of 2020. Due to the pandemic, many students are stuck at home without a summer program. The aim of the Polymath REU program is to provide research opportunities for such students. The program consists of research projects […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/05/28/polymath-reu/"><span class="datestr">at May 28, 2020 04:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
