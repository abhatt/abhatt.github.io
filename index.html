<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 22, 2022 02:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09562">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09562">Fast Circular Pattern Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Will Solow, Matthew Barich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mumey:Brendan.html">Brendan Mumey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09562">PDF</a><br /><b>Abstract: </b>The Exact Circular Pattern Matching (ECPM) problem consists of reporting
every occurrence of a rotation of a pattern $P$ in a text $T$. In many
real-world applications, specifically in computational biology, circular
rotations are of interest because of their prominence in virus DNA. Thus, given
no restrictions on pre-processing time, how quickly all such circular rotation
occurrences is of interest to many areas of study. We highlight, to the best of
our knowledge, a novel approach to the ECPM problem and present four data
structures that accompany this approach, each with their own time-space
trade-offs, in addition to experimental results to determine the most
computationally feasible data structure.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09562"><span class="datestr">at April 21, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09535">Theoretical analysis of edit distance algorithms: an applied perspective</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medvedev:Paul.html">Paul Medvedev</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09535">PDF</a><br /><b>Abstract: </b>Given its status as a classic problem and its importance to both
theoreticians and practitioners, edit distance provides an excellent lens
through which to understand how the theoretical analysis of algorithms impacts
practical implementations. From an applied perspective, the goals of
theoretical analysis are to predict the empirical performance of an algorithm
and to serve as a yardstick to design novel algorithms that perform well in
practice. In this paper, we systematically survey the types of theoretical
analysis techniques that have been applied to edit distance and evaluate the
extent to which each one has achieved these two goals. These techniques include
traditional worst-case analysis, worst-case analysis parametrized by edit
distance or entropy or compressibility, average-case analysis, semi-random
models, and advice-based models. We find that the track record is mixed. On one
hand, two algorithms widely used in practice have been born out of theoretical
analysis and their empirical performance is captured well by theoretical
predictions. On the other hand, all the algorithms developed using theoretical
analysis as a yardstick since then have not had any practical relevance. We
conclude by discussing the remaining open problems and how they can be tackled.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09535"><span class="datestr">at April 21, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09228">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09228">Tight Last-Iterate Convergence of the Extragradient Method for Constrained Monotone Variational Inequalities</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Yang.html">Yang Cai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oikonomou:Argyris.html">Argyris Oikonomou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Weiqiang.html">Weiqiang Zheng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09228">PDF</a><br /><b>Abstract: </b>The monotone variational inequality is a central problem in mathematical
programming that unifies and generalizes many important settings such as smooth
convex optimization, two-player zero-sum games, convex-concave saddle point
problems, etc. The extragradient method by Korpelevich [1976] is one of the
most popular methods for solving monotone variational inequalities. Despite its
long history and intensive attention from the optimization and machine learning
community, the following major problem remains open. What is the last-iterate
convergence rate of the extragradient method for monotone and Lipschitz
variational inequalities with constraints? We resolve this open problem by
showing a tight $O\left(\frac{1}{\sqrt{T}}\right)$ last-iterate convergence
rate for arbitrary convex feasible sets, which matches the lower bound by
Golowich et al. [2020]. Our rate is measured in terms of the standard gap
function. The technical core of our result is the monotonicity of a new
performance measure -- the tangent residual, which can be viewed as an
adaptation of the norm of the operator that takes the local constraints into
account. To establish the monotonicity, we develop a new approach that combines
the power of the sum-of-squares programming with the low dimensionality of the
update rule of the extragradient method. We believe our approach has many
additional applications in the analysis of iterative methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09228"><span class="datestr">at April 21, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09178">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09178">Counting and enumerating optimum cut sets for hypergraph $k$-partitioning problems for fixed $k$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beideman:Calvin.html">Calvin Beideman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chandrasekaran:Karthekeyan.html">Karthekeyan Chandrasekaran</a>, Weihang Wang <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09178">PDF</a><br /><b>Abstract: </b>We consider the problem of enumerating optimal solutions for two hypergraph
$k$-partitioning problems -- namely, Hypergraph-$k$-Cut and
Minmax-Hypergraph-$k$-Partition. The input in hypergraph $k$-partitioning
problems is a hypergraph $G=(V, E)$ with positive hyperedge costs along with a
fixed positive integer $k$. The goal is to find a partition of $V$ into $k$
non-empty parts $(V_1, V_2, \ldots, V_k)$ -- known as a $k$-partition -- so as
to minimize an objective of interest.
</p>
<p>1. If the objective of interest is the maximum cut value of the parts, then
the problem is known as Minmax-Hypergraph-$k$-Partition. A subset of hyperedges
is a minmax-$k$-cut-set if it is the subset of hyperedges crossing an optimum
$k$-partition for Minmax-Hypergraph-$k$-Partition.
</p>
<p>2. If the objective of interest is the total cost of hyperedges crossing the
$k$-partition, then the problem is known as Hypergraph-$k$-Cut. A subset of
hyperedges is a min-$k$-cut-set if it is the subset of hyperedges crossing an
optimum $k$-partition for Hypergraph-$k$-Cut.
</p>
<p>We give the first polynomial bound on the number of minmax-$k$-cut-sets and a
polynomial-time algorithm to enumerate all of them in hypergraphs for every
fixed $k$. Our technique is strong enough to also enable an $n^{O(k)}p$-time
deterministic algorithm to enumerate all min-$k$-cut-sets in hypergraphs, thus
improving on the previously known $n^{O(k^2)}p$-time deterministic algorithm,
where $n$ is the number of vertices and $p$ is the size of the hypergraph. The
correctness analysis of our enumeration approach relies on a structural result
that is a strong and unifying generalization of known structural results for
Hypergraph-$k$-Cut and Minmax-Hypergraph-$k$-Partition. We believe that our
structural result is likely to be of independent interest in the theory of
hypergraphs (and graphs).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09178"><span class="datestr">at April 21, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09136">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09136">The White-Box Adversarial Data Stream Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Miklos Ajtai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Vladimir.html">Vladimir Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jayram:T=_S=.html">T. S. Jayram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Alec.html">Alec Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09136">PDF</a><br /><b>Abstract: </b>We study streaming algorithms in the white-box adversarial model, where the
stream is chosen adaptively by an adversary who observes the entire internal
state of the algorithm at each time step. We show that nontrivial algorithms
are still possible. We first give a randomized algorithm for the $L_1$-heavy
hitters problem that outperforms the optimal deterministic Misra-Gries
algorithm on long streams. If the white-box adversary is computationally
bounded, we use cryptographic techniques to reduce the memory of our
$L_1$-heavy hitters algorithm even further and to design a number of additional
algorithms for graph, string, and linear algebra problems. The existence of
such algorithms is surprising, as the streaming algorithm does not even have a
secret key in this model, i.e., its state is entirely known to the adversary.
One algorithm we design is for estimating the number of distinct elements in a
stream with insertions and deletions achieving a multiplicative approximation
and sublinear space; such an algorithm is impossible for deterministic
algorithms.
</p>
<p>We also give a general technique that translates any two-player deterministic
communication lower bound to a lower bound for {\it randomized} algorithms
robust to a white-box adversary. In particular, our results show that for all
$p\ge 0$, there exists a constant $C_p&gt;1$ such that any $C_p$-approximation
algorithm for $F_p$ moment estimation in insertion-only streams with a
white-box adversary requires $\Omega(n)$ space for a universe of size $n$.
Similarly, there is a constant $C&gt;1$ such that any $C$-approximation algorithm
in an insertion-only stream for matrix rank requires $\Omega(n)$ space with a
white-box adversary. Our algorithmic results based on cryptography thus show a
separation between computationally bounded and unbounded adversaries.
</p>
<p>(Abstract shortened to meet arXiv limits.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09136"><span class="datestr">at April 21, 2022 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2204.09113">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2204.09113">Representation of short distances in structurally sparse graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Zdeněk Dvořák <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2204.09113">PDF</a><br /><b>Abstract: </b>A partial orientation $\vec{H}$ of a graph $G$ is a weak $r$-guidance system
if for any two vertices at distance at most $r$ in $G$, there exists a shortest
path $P$ between them such that $\vec{H}$ directs all but one edge in $P$
towards this edge. In case $\vec{H}$ has bounded maximum outdegree, this gives
an efficient representation of shortest paths of length at most $r$ in $G$. We
show that graphs from many natural graph classes admit such weak guidance
systems, and study the algorithmic aspects of this notion.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2204.09113"><span class="datestr">at April 21, 2022 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/missingness/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/missingness/">Missingness Bias in Model Debugging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2204.08945" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/missingness" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br /></p>

<p><i>
“Missingness”, or the absence of features from an input, is a concept that is fundamental to many model debugging tools. In our <a href="https://arxiv.org/abs/2204.08945">latest paper</a>, we examine the challenges of implementing missingness in computer vision. In particular, we demonstrate how current approximations of missingness introduce biases into the debugging process of computer vision models. We then show how a natural implementation of missingness based on VITs can mitigate these biases and lead to more reliable model debugging.
</i></p>

<p>Deep learning models can learn powerful features. However, these learned features can be unintuitive or spurious. Indeed, recent studies have pointed out that ML models often leverage unexpected—and, in fact, undesirable—associations. These include image pathology detection models that rely on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologist</a> and image classifiers that focus too much on <a href="https://arxiv.org/abs/2006.09994">backgrounds</a> or on <a href="https://arxiv.org/abs/1811.12231">texture</a>. Such prediction mechanisms can cause models to fail in downstream tasks or new environments.</p>

<p>How can we detect model failures caused by relying on such brittle or undesirable associations? Answering this question is a major goal of model debugging. Work in this context brought forth techniques that allow, for example, surfacing <a href="https://arxiv.org/abs/1704.05796">human aligned</a> <a href="https://gradientscience.org/glm_saga/">concepts</a>, <a href="https://gradientscience.org/3db/">probing specific types of bias a model uses</a>, or highlighting features that were <a href="https://arxiv.org/abs/1904.07451">important for a</a> <a href="https://arxiv.org/abs/1602.04938">specific prediction</a>.</p>

<p>A common theme in many of these debugging methods is to study model behavior on so-called counterfactual inputs, i.e., inputs with and without specific features. For example, consider the image of a dog being held by its owner below. By removing the owner from the image, we can study how much our model’s prediction depends on the presence of a human. In a similar vein, we can remove parts of the dog (head, body, paws) to identify which ones among them are most critical for correctly classifying the image. This concept of “the absence of a feature” from the input is sometimes referred to as <a href="https://distill.pub/2020/attribution-baselines/">missingness</a>.</p>

<p><img src="https://gradientscience.org/assets/missingness/woman_dog_example.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
        By removing parts of the image, we can study how much our model depends on a given input feature (such as the human, or the dog's snout).  
</div>

<p>Indeed, this primitive of missingness is used quite a lot in model debugging techniques. For example, widely-used methods such as <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1703.01365">integrated gradients</a> leverage it. It also has been applied to <a href="https://arxiv.org/abs/1802.07623">radiology images</a> to understand the regions of the scan that are important for diagnoses, or to <a href="https://arxiv.org/abs/2110.04301">flag spurious correlations</a>. Finally, in natural language processing, model designers often remove words <a href="https://arxiv.org/abs/2010.12487">to understand their impact on the output</a>.</p>

<p><img src="https://gradientscience.org/assets/missingness/main_lime.png" style="width: 100%;" alt="Example of LIME" /></p>
<div class="footnote">
LIME, a popular model debugging method, relies on the  missingness primitive to generate model explanations. The method first generates a set of ablations by randomly removing parts of the image. These ablations are then used to generate a model explanation, typically obtained by fitting a sparse linear model to them. 
</div>

<h2 id="challenges-of-implementing-missingness-in-computer-vision">Challenges of implementing missingness in computer vision</h2>

<p>Missingness is a rather intuitive notion: we simply would like the model to predict as if the corresponding part of the input didn’t exist. Also, in a domain such as NLP implementing this primitive is fairly straightforward: we simply drop the corresponding words from the sentence. However, in the context of computer vision, its proper implementation turns out to be much more challenging. This is because images are spatially contiguous objects: it is unclear how to leave a “hole” in the image.</p>

<p>Researchers thus have come up with all kinds of ways to “fill” such hole, i.e., approximating missingness by replacing the region with other pixels. This involves <a href="https://arxiv.org/pdf/1703.01365.pdf">blacking out the pixels</a>, replacing them with <a href="https://arxiv.org/pdf/1706.03825.pdf">random noise</a>, or <a href="https://arxiv.org/pdf/1704.03296.pdf">blurring the corresponding image region</a>. However, these approaches, even though very natural, turn out to have unintended effects. For example, researchers found that <a href="https://distill.pub/2020/attribution-baselines/">saliency maps generated with integrated gradients are quite sensitive to the chosen baseline color of the filling</a>, and thus can change significantly based on the (arbitrary) choice of that missingness approximation.</p>

<p><img src="https://gradientscience.org/assets/missingness/various_approx_original.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
    Practitioners often approximate missingness in computer vision by replacing the removed region with other types of pixels. Examples include blacking out, using the mean pixel color, filling with random noise, or blurring the image.
</div>



<h2 id="missingness-approximations-can-create-bias">Missingness approximations can create bias</h2>

<p>So, what impact do such missingness approximations actually have on the resulting model predictions? In our <a href="https://arxiv.org/abs/2204.08945">recent work</a>, we systematically investigate the bias that these approximations can imbue. Specifically, <i>we find that models end up using masked out regions to make predictions rather than simply ignoring them.</i></p>

<p>For example, take a look below at the image of a spider from ImageNet, where we have removed various regions from the image by blacking out the corresponding pixels.</p>

<div style="overflow: auto; text-align: center;" id="bias_examples_widget"></div>
<div class="footnote">
    Various regions of an image are removed by blacking out pixels. Regardless of what regions from the image are masked, a ResNet-50 outputs a wrong class. 
</div>

<p>It turns out that irrespective of what subregions of the image are removed, a standard CNN (e.g., ResNet-50) outputs incorrect classes, even when most of the foreground object is not obscured. In fact, taking a closer look at the randomly masked images, we find that the model seems to be relying on the masking pattern itself to make the prediction (e.g., predicting class “crossword”).</p>

<p>To analyze this bias more quantitatively, we measure how missingness approximations impact the output class distribution of a Resnet-50 classifier (over the whole dataset).  Specifically, we iteratively black out parts of ImageNet images, and keep track of how the probability of predicting any one class changes. Before blacking out any pixels, the model predicts a roughly equal number of each class on the ImageNet test set (which is what one would expect given that this test set is class balanced). However, when we apply missingness approximations, this distribution skews heavily toward classes such as maze, crossword puzzle, and carton.</p>

<div id="anno"> 
    <canvas width="30%" id="resnet_bar_chart" height="15%"></canvas>
</div>
<div class="footnote">
    We measure the shift in output class distribution after applying a (blacking out) missingness approximations. As patches are increasingly blacked out, the ResNet-50 classifier's predictions skew from a uniform distribution toward a few specific classes such as maze, crossword puzzle, and carton.
</div>

<p>Our <a href="https://arxiv.org/abs/2204.08945">paper</a> investigates this missingness bias in more depth by exploring different approximations, datasets, mask sizes, etc.</p>

<h2 id="missingness-bias-in-model-debugging-a-case-study-on-lime">Missingness bias in model debugging: A case study on LIME</h2>
<p>As we mentioned earlier, missingness primitive is often used by model debugging techniques—how does the above-mentioned bias impact them? To answer this question, we focus on a popular feature attribution method: local interpretable model-agnostic explanations, or <a href="https://arxiv.org/abs/1602.04938">LIME</a>.</p>

<div style="overflow: auto; text-align: center;" id="lime_examples_widget"></div>
<div class="footnote">
    Examples of generated LIME explanations represented as a heat map. High-intensity regions contribute more to the prediction of the model than low-intensity ones. On the right, the top 20 image regions identified by the LIME explanation are masked in black.
</div>

<p>The image above depicts an example LIME explanation generated for a ResNet-50 classifier. Specifically, for a given image, LIME generates a heatmap highlighting the regions of the image based on how much they impact prediction (according to LIME). While sometimes LIME highlights the foreground object (which is what one would expect), in most cases it also highlights rather irrelevant regions scattered all over the image. Why is this the case?</p>

<p>Although we can’t say for sure, we suspect that this behavior is caused in large part by the fact that the underlying ResNet-50 classifier relies on masked regions in the image to make predictions, and this tricks it into believing that these regions are important. In <a href="https://arxiv.org/abs/2204.08945">our paper</a>, we perform a more detailed quantitative analysis of the missingness bias in LIME explanations, and find that it indeed makes them more inconsistent and, during evaluation, indistinguishable from random explanations.</p>

<h2 id="a-more-natural-implementation-of-missingness">A more natural implementation of missingness</h2>
<p>What is the right way to represent missing pixels then? Ideally, since replacing pixels with other pixels can lead to missingness bias, we would like to be able to remove these regions altogether. But what about the need of having the images be represented by contiguous inputs?</p>

<p>Certainly, convolutional neural networks (CNNs) require such spatial contiguity because convolutions slide filters across the image. But do we even need to use CNNs? Not really!</p>

<p>How about we turn to a different architecture:  <a href="https://arxiv.org/abs/2010.11929">vision transformers</a> (ViTs)? Unlike CNNs, ViTs do not use convolutions and operate instead on sets of patches that correspond to positionally encoded regions of the image. Specifically, a ViT has two stages when processing an input image:</p>

<ul>
  <li><strong>Tokenization</strong>: split the image into square patches, and positionally encode these patches into tokens.</li>
  <li><strong>Self-attention</strong>: pass the set of tokens through several self-attention layers.</li>
</ul>

<p>After tokenization, the self-attention layers operate on the set of tokens rather than the entire image. Thus, ViTs enables a far more natural implementation of missingness: <i>we can simply drop the tokens that encode the region we want to remove!</i></p>

<p><img src="https://gradientscience.org/assets/missingness/vit_dropping.png" style="width: 100%;" alt="Woman-Dog Example" /></p>
<div class="footnote">
    ViTs split the image into a set of tokens, where each token represents a patch in the image. In order to implement missingness, we can simply drop the tokens corresponding to the regions we would like to remove.
</div>

<h2 id="mitigating-missingness-bias-through-vits">Mitigating missingness bias through ViTs</h2>

<p>As we now demonstrate,  using this more natural implementation of missingness turns out to substantially mitigate the missingness bias that we saw earlier. Indeed, recall that, with a ResNet-50, blacking out pixels biased the model toward specific classes (such as crossword). But, a similarly sized ViT-small (ViT-S) architecture is able to side-step this bias (when we implement token dropping), maintaining a correct (or at least related) prediction.</p>

<div style="overflow: auto; text-align: center;" id="bias_examples_widget2"></div>
<div class="footnote">
   When we remove image regions by dropping tokens with a ViT-S classifier, the model maintains its original prediction or, at least, predicts a related class.
</div>

<p>This difference is even more striking when we examine the output class distribution corresponding to removal of image regions: while for our ResNet-50 classifier we have seen this distribution being skewed toward certain classes, for the ViT classifier, this distribution remains close to uniform.</p>

<div id="anno"> 
    <canvas width="30%" id="vit_bar_chart" height="15%"></canvas>
</div>
<div class="footnote">
The shift in output class distribution after dropping tokens using a ViT architecture. In contrast to the ResNet-50 classifier, ViT largely maintains its original uniform class distribution.
</div>

<h2 id="improving-model-debugging-with-vits">Improving model debugging with ViTs</h2>

<p>So, we have seen that ViTs allow us to mitigate missingness bias. Can we thus use ViTs to improve model debugging too? We return to our case study of LIME. As we demonstrated earlier, LIME relies on a notion of missingness, and is thus vulnerable to missingness bias when using approximations like blacking out pixels. What happens when we use ViTs and token dropping instead?</p>

<div style="overflow: auto; text-align: center;" id="lime_examples_widget2"></div>
<div class="footnote">
Examples of generated LIME explanations for a ResNet (blacking out pixels) and a ViT (dropping tokens). 
</div>

<p>The figure above displays examples of the LIME explanations generated for a ResNet classifier and for a ViT classifier (dropping tokens). Qualitatively, one can see that the explanations for the ViT seem more aligned with human intuition, highlighting the main object instead of regions in the background. We confirm these observations with a more quantitative analysis in <a href="https://arxiv.org/abs/2204.08945">our paper</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we studied how missingness approximations can lead to biases and, in turn, impact the model debugging techniques that leverage them. We also demonstrated how transformer-based architectures can enable a seamless implementation of missingness that allows us to side-step missingness bias and lead to more reliable model debugging.</p></div>







<p class="date">
<a href="https://gradientscience.org/missingness/"><span class="datestr">at April 20, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/">IDEAL Workshop on Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
April 22-23, 2022 Northwestern University &amp; Online https://www.ideal.northwestern.edu/events/clustering/ We are inviting you to attend the IDEAL Workshop on Clustering. The workshop will take place at Northwestern University on Friday, April 22, and Saturday, April 23. It will be in a hybrid format. If you are interested in participating in the workshop (in-person or remotely), please … <a href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on Clustering</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/19/ideal-workshop-on-clustering/"><span class="datestr">at April 19, 2022 03:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/">Postdoc at University of Texas at San Antonio (apply by April 30, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applicants with background in one of the following fields are invited to apply: discrete and convex geometry, algebraic algorithms, convex optimization, randomized numerical analysis, average case complexity theory. One year research only position. Please send your CV, a letter on your technical background and research interests, and two references for recommendation letters to Alperen Ergur.</p>
<p>Website: <a href="http://alpergur.xyz">http://alpergur.xyz</a><br />
Email: alperen.ergur@utsa.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/04/18/postdoc-at-university-of-texas-at-san-antonio-apply-by-april-30-2022/"><span class="datestr">at April 18, 2022 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/052">TR22-052 |  Verifying The Unseen: Interactive Proofs for Label-Invariant Distribution Properties | 

	Tal Herman, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given i.i.d. samples from an unknown distribution over a large domain $[N]$, approximating several basic quantities, including the distribution's support size, its entropy, and its distance from the uniform distribution, requires $\Theta(N / \log N)$ samples [Valiant and Valiant, STOC 2011].

Suppose, however, that we can interact with a powerful but untrusted prover, who knows the entire distribution (or a good approximation of it). Can we use such a prover to approximate (or rather, to approximately {\em verify}) such statistical quantities more efficiently? We show that this is indeed the case: the support size, the entropy, and the distance from the uniform distribution, can all be approximately verified via a 2-message interactive proof, where the communication complexity, the verifier's running time, and the sample complexity are $\widetilde{O}({\sqrt{N}})$. For all these quantities, the sample complexity is tight up to $\polylog N$ factors (for any interactive proof, regardless of its communication complexity or verification time).

More generally, we give a tolerant interactive proof system with the above sample and communication complexities for verifying a distribution's proximity to any label-invariant property (any property that is invariant to re-labeling of the elements in the distribution's support). The verifier's running time in this more general protocol is also $\widetilde{O}({\sqrt{N}})$, under a mild assumption about the complexity of deciding, given a compact representation of a distribution, whether it is in the property or far from it.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/052"><span class="datestr">at April 18, 2022 06:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/051">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/051">TR22-051 |  Low Degree Testing over the Reals | 

	Noah Fleming, 

	Vipul Arora, 

	Arnab Bhattacharyya, 

	Esty Kelman, 

	Yuichi Yoshida</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the problem of testing whether a function $f: \mathbb{R}^n \to \mathbb{R}$ is a polynomial of degree at most $d$ in the distribution-free testing model. Here, the distance between functions is measured with respect to an unknown distribution $\mathcal{D}$ over $\mathbb{R}^n$ from which we can draw samples. In contrast to previous work, we do not assume that $\mathcal{D}$ has finite support. 
    
We design a tester that given query access to $f$, and sample access to $\mathcal{D}$, makes $(d/\varepsilon)^{O(1)}$ many queries to $f$, accepts with probability $1$ if $f$ is a polynomial of degree $d$, and rejects with probability at least $2/3$ if every degree-$d$ polynomial $P$ disagrees with $f$ on a set of mass at least $\varepsilon$ with respect to $\mathcal{D}$. Our result also holds under mild assumptions when we receive only a polynomial number of bits of precision for each query to $f$, or when $f$ can only be queried on rational points representable using a logarithmic number of bits. Along the way, we prove a new stability theorem for multivariate polynomials that may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/051"><span class="datestr">at April 18, 2022 06:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8738724093968037845">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html">1-week long Summer School for Ugrads Interested in Theory, and my comments on it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Recently a grad student in CS at UMCP emailed me the following email he got,  thinking (correctly) that I should forward it to interested ugrads. </p><p>-------------------------------------------------------</p><p>Are you interested in theoretical computer science including topics like algorithms, cryptography, machine learning, and others? If so, please consider applying to the New Horizons in Theoretical Computer Science week-long online summer school! The school will contain several mini-courses from top researchers in the field. The course is free of charge,and we welcome applications from undergraduates majoring in computer science or related fields. We particularly encourage applications from students that are members of groups that are currently under-represented in theoretical computer science.</p><p>Students from previous years have shared with us that the mini-lectures, online group activities, and interactions with other students and the friendly TAs were extraordinarily engaging and fun.</p><p>For full consideration, please complete the application (it’s short and easy!) by April 25, 2022. The summer school will take place online from June 6 to June 10.</p><p>Please see our website for details: see <a href="https://tcs-summerschool.ttic.edu/">here</a> </p><p>Any questions can be directed to summer-school-admin-2022@ttic.edu.</p><p>--------------------------------------------------------------------------</p><p>A few points about this</p><p>1) I emailed them asking `why do people need to apply if its online and free?'</p><p>I had one answer in mind, but they gave me another one</p><p><i>Their Answer: </i>They want to have SMALL online activities in groups. If they had X students and want groups of size g then if X is large, X/g may be too large. </p><p><i>My Answer</i>: If people REGISTER for something they are more likely to actually show up. (I know of a conference that got MORE people going once they had registation, and even MORE when they began charging for it.) </p><p>2) I emailed them asking if the talks will, at some later point, be on line. They will be. I then realized that there are already LOTS of theory talks online that I have not gotten around to watching, and perhaps never will. Even so, the talks on line may well benefit people who goto the summer school if they want to look back and something. </p><p>3) Online conferences PROS and CONS:</p><p>PROS: Free (or very low cost), no hassle getting airfare and hotel, and if talks are recorded then you can see them later (that applies to in-person as well). </p><p>CONS: Less committed to going to it. Can go in a half-ass way. For example, you can go and then in the middle of a talk go do your laundry. Being FORCED to be in a ROOM with the SPEAKER may be good. Also, of course, no informal conversations in the hallways.  Also, less serendipity. </p><p>I want to say <i>It would to be good to see talks outside of my area </i>however, this may only be true for easy talks, perhaps talks in a new field, OR talks that are just barely outside my area so I have some context. </p><p>4) I was surprised I didn't get the email directly since I have more contact with ugrads (and I have this blog) then the grad student who alerted me to it. However, I have learned that information gets to people in random ways so perhaps not to surprising. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/1-week-long-summer-school-for-ugrads.html"><span class="datestr">at April 18, 2022 01:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/050">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/050">TR22-050 |  Circuits Resilient to Short-Circuit Errors | 

	Raghuvansh Saxena, 

	Gillat Kol, 

	Bernhard Haeupler, 

	Pritish Kamath, 

	Nicolas Resch, 

	Klim Efremenko, 

	Yael Kalai</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given a Boolean circuit $C$, we wish to convert it to a circuit $C'$ that computes the same function as $C$ even if some of its gates suffer from adversarial short circuit errors, i.e., their output is replaced by the value of one of their inputs [KLM97]. Can we design such a resilient circuit $C'$ whose size is roughly comparable to that of $C$? Prior work [KLR12, BEGY19] gave a positive answer for the special case where $C$ is a formula.

We study the general case and show that any Boolean circuit $C$ of size $s$ can be converted to a new circuit $C'$ of quasi-polynomial size $s^{O(\log s)}$ that computes the same function as $C$ even if a $1/51$ fraction of the gates on any root-to-leaf path in $C'$ are short circuited. Moreover, if the original circuit $C$ is a formula, the resilient circuit $C'$ is of near-linear size $s^{1+\epsilon}$. The construction of our resilient circuits utilizes the connection between circuits and DAG-like communication protocols} [Raz95, Pud10, Sok17], originally introduced in the context of proof complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/050"><span class="datestr">at April 17, 2022 01:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/04/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/04/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://twitter.com/RobFathauerArt/status/1494044593486131200">As Robert Fathauer points out</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108064413451021876">\(\mathbb{M}\)</a>),</span> the <a href="https://www.nmc.edu/about/nmc-stories/success-stories/viscomm-grad-designs-stamp.html">US “Star Ribbon” postage stamp</a> depicts a star-shaped Möbius strip. If you slice the depicted ribbon lengthwise into two linked rings, its central white portion and outer red and blue portions, the inner part is again a Möbius strip while the outer part becomes doubly twisted and twice as long, with red on one side and blue on the other.</p>
  </li>
  <li>
    <p>When its Japanese-American students were released from the concentration camps at the end of World War II, the University of Southern California refused to readmit them or even allow them to transfer their credits elsewhere <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108069391491491798">\(\mathbb{M}\)</a>).</span> Now, 80 years after the students were taken to the camps, <a href="https://www.latimes.com/california/story/2022-04-02/usc-offers-posthumous-degrees-japanese-students">USC is giving them posthumous honorary degrees</a>.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/father-son-team-solves-geometry-problem-with-infinite-folds-20220404/">Erik and Marty Demaine profiled for their work on squishing polyhedral surfaces flat</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108074681771364763">\(\mathbb{M}\)</a>),</span> in <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://www.npr.org/2022/04/01/1090279187/russia-wikipedia-fine">Russian government threatens to fine Wikipedia for not sticking to Russian propaganda in its reporting of the invasion of Ukraine</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108082233497808371">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30899708">via</a>, <a href="https://boingboing.net/2022/04/05/putins-government-threatens-to-fine-wikipedia-for-publishing-unreliable-socially-significant-information.html">see also</a>). Laughable, but the likely outcome appears to be that Wikipedia becomes blocked within Russia (as there is no possibility of complying with these demands) and Russia deepens its self-imposed isolation from civilized society.</p>
  </li>
  <li>
    <p>Longstanding but still-active theoretical CS blog “Gödel’s Lost Letter and P=NP” makes a <a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/">roundup of other still-active math and theoretical CS blogs</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108089353162729425">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.acm.org/articles/bulletins/2022/april/50-years-backfile">ACM makes its early publications, from 1951 to 2000, open access</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108091645356237331">\(\mathbb{M}\)</a>).</span> It’s not everything I’d want, but it’s a start…</p>
  </li>
  <li>
    <p>You’re probably aware that Twitter has gradually joined Facebook and Instagram in deliberately walling itself off from the open web and making itself close-to-unusable by those of us without accounts. Manuel Grabowski writes that <a href="https://annoying.technology/posts/e6901c0ea272f57d/">it’s also becoming significantly less usable even by those with accounts</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108100999866052164">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30949048">via</a>).</p>
  </li>
  <li>
    <p>I’m sad to hear that <a href="https://algo.rwth-aachen.de/">algorithmist Gerhard Woeginger died recently, only aged 57</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108106575959194943">\(\mathbb{M}\)</a>).</span> Much of his early work was in computational geometry (how we became coauthors), and I’ve posted on other papers of his, but some of my favorites include:</p>

    <ul>
      <li>
        <p>Surveys on exact algorithms for hard problems, in <a href="https://doi.org/10.1007/3-540-36478-1_17"><em>Combinatorial Optimization — Eureka, You Shrink!</em></a> and <a href="https://doi.org/10.1016/j.dam.2007.03.023"><em>Disc. Appl. Math.</em> 2007</a>.</p>
      </li>
      <li>
        <p>Arranging lines with given numbers of crossings, in <a href="https://doi.org/10.1016/j.tcs.2004.04.006"><em>Theor. Comput. Sci.</em> 2004</a>.</p>
      </li>
      <li>
        <p>On river-crossing puzzles, in <a href="https://doi.org/10.1137/080736661"><em>SIAM J. Disc. Math.</em> 2010</a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/108106319690622089">Christian Lawson-Perfect finds his perfect love match</a>: someone who agrees on the best method of stacking eggs. Which raises “Kepler’s egg-packing problem”: what is that  method, in an egg box of bounded width?</p>

    <p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/kepleregg/Kepler-m.jpg" style="border-style: solid; border-color: black;" alt="Eggs stacked in an egg box" /></p>
  </li>
  <li>
    <p>It’s the long-overdue Python2 apocalypse! <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108114873951435174">\(\mathbb{M}\)</a>).</span>  I upgraded my laptop to MacOS 12.3.1 and somehow the upgrade nuked any old python2 executables I might have had in my path, so from now on everything has to be python3. Fortunately any of my old python2 scripts that I’ve tried to use so far have been very easy to fix (most complicated issue: installing Pillow to replace PIL). But I suspect that if I ever want to update and rebuild my old PyObjC-based apps, the changes will be more substantial.</p>
  </li>
  <li>
    <p><a href="https://thehighergeometer.wordpress.com/2022/04/12/its-a-messy-job-but-someone-had-to-do-it-fixing-all-the-links/">David Roberts on fixing all the old broken links from StackExchange to the “Front for the Mathematics arXiv”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108125670540879313">\(\mathbb{M}\)</a>),</span> a user interface veneer for arXiv that used to be available from UC Davis and has since gone away. You would think it would be a small matter of parsing the arXiv id in the link and replacing it with a current arXiv url for the same id, but…</p>
  </li>
  <li>
    <p><a href="https://www.science.org/content/article/russian-website-peddles-authorships-linked-reputable-journals">You may have seen <em>Science</em> on cracking a Russian pay-for-coauthorship ring</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108129289678280113">\(\mathbb{M}\)</a>).</span> As the <a href="https://arxiv.org/abs/2112.13322">associated preprint arXiv:2112.13322</a> details, many reputable publishers had their journals caught up in publications of this ring. But it also points to suspicious patterns in MDPI journals, where several people from one country were both coauthors and editors of ring papers, and coauthorship slots were advertised as reserved for those editors.</p>
  </li>
  <li>
    <p>Zugzwang describes a situation where moving first is a disadvantage, especially in games where it might usually be an advantage <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108136997760810042">\(\mathbb{M}\)</a>).</span> <em>Quanta</em> outlines <a href="https://www.quantamagazine.org/zugzwang-in-chess-math-and-pizzas-20220222/">four zugzwang puzzles in games ranging from chess to pizza-division</a>. <a href="https://www.quantamagazine.org/the-secrets-of-zugzwang-in-chess-math-and-pizzas-20220408/">Solutions</a>.</p>
  </li>
  <li>
    <p><a href="https://www.acm.org/media-center/2022/april/allen-award-2021">The ACM has given Carla Brodley their inaugural ACM Frances E. Allen Award for Outstanding Mentoring</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108139193039016625">\(\mathbb{M}\)</a>),</span> citing her work to improve the representation of women among students and faculty in her work as dean of computer science at Northeastern University, her founding of the Center for Inclusive Computing, which helps other institutions make similar improvements, and her leadership on the Computing Research Association Committee on Widening Participation.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/04/15/linkage.html"><span class="datestr">at April 15, 2022 06:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/">AMSI–AustMS Workshop on Bridging Maths and Computer Science</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 31 – June 3, 2022 Sydney, Australia https://sites.google.com/view/2022-workshop-bridgingmathstcs/ This 4-day workshop will bring together Australian and Australasian researchers in mathematics and theoretical computer science, in view of fostering exchanges and collaborations. Specifically, the workshop will focus on two themes, “Computational Complexity and Cryptography” and “Graph Theory and Combinatorics,” from the point of view of … <a href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/" class="more-link">Continue reading <span class="screen-reader-text">AMSI–AustMS Workshop on Bridging Maths and Computer Science</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/15/amsi-austms-workshop-on-bridging-maths-and-computer-science/"><span class="datestr">at April 15, 2022 07:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/">IDEAL Workshop on “Clustering” -Friday &amp; Saturday, April 22-23, 2022, 8:40 am-4:00 pm Central Time in Mudd Library 3514</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
April 22-23, 2022 Mudd Library 3514 https://www.ideal.northwestern.edu/events/clustering/ IDEAL Workshop on Clustering. The workshop will take place at Northwestern University on Friday, April 22, and Saturday, April 23, 8:40 am- 4:00 pm CST (Chicago Time) in MUDD 3514. It will be in a hybrid format. If you are interested in participating in the workshop (in-person or … <a href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL Workshop on “Clustering” -Friday &amp; Saturday, April 22-23, 2022, 8:40 am-4:00 pm Central Time in Mudd Library 3514</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2022/04/15/ideal-workshop-on-clustering-friday-saturday-april-22-23-2022-840-am-400-pm-central-time-in-mudd-library-3514/"><span class="datestr">at April 15, 2022 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=126">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2022/04/14/wednesday-april-20th-2022-gabriel-peyre-from-cnrs-and-ens/">Wednesday April 20th 2022 — Gabriel Peyré from CNRS and ENS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 20th</strong> at <strong>12:00 PM Pacific Time</strong> (15:00 Eastern Time, 21:00 Central European Time, 19:00 UTC). <a href="http://www.gpeyre.com/">Gabriel Peyré</a> from<strong> CNRS and Ecole Normale Supérieure</strong> will speak about “Scaling Optimal Transport for High dimensional Learning.<em>”</em></p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p><strong>Abstract</strong>:  Optimal transport (OT) has recently gained lot of interest in machine learning. It is a natural tool to compare in a geometrically faithful way probability distributions. It finds applications in both supervised learning (using geometric loss functions) and unsupervised learning (to perform generative model fitting). OT is however plagued by the curse of dimensionality, since it might require a number of samples which grows exponentially with the dimension. In this talk, I will explain how to leverage entropic regularization methods to define computationally efficient loss functions, approximating OT with a better sample complexity. More information and references can be found on <a href="https://optimaltransport.github.io/">the website of our book</a> “Computational Optimal Transport.”</p>



<p> The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2022/04/14/wednesday-april-20th-2022-gabriel-peyre-from-cnrs-and-ens/"><span class="datestr">at April 14, 2022 01:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=616">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/04/14/tcs-talk-wednesday-april-20-rasmus-kyng-eth-zurich/">TCS+ talk: Wednesday, April 20 — Rasmus Kyng, ETH Zürich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://rasmuskyng.com/"><strong>Rasmus Kyng</strong></a> from ETH Zürich will speak about “<em>Almost-Linear Time Algorithms for Maximum Flow and More</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We give the first almost-linear time algorithm for computing exact maximum flows and minimum-cost flows on directed graphs. By well-known reductions, this implies almost-linear time algorithms for several problems including bipartite matching, optimal transport, and undirected vertex connectivity.</p>
<p>Our algorithm uses a new Interior Point Method (IPM) that builds the optimal flow as a sequence of an almost-linear number of approximate undirected minimum-ratio cycles, each of which is computed and processed very efficiently using a new dynamic data structure.</p>
<p>Our framework extends to give an almost-linear time algorithm for computing flows that minimize general edge-separable convex functions to high accuracy. This gives the first almost-linear time algorithm for several problems including entropy-regularized optimal transport, matrix scaling, p-norm flows, and Isotonic regression.</p>
<p>Joint work with Li Chen, Yang Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/04/14/tcs-talk-wednesday-april-20-rasmus-kyng-eth-zurich/"><span class="datestr">at April 14, 2022 12:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6339126729709498263">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html">The Roeder Seq Problems was Solved Before I Posed it (Math)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>  (Joint Post by Bill Gasarch, David Harris, and Tomas
Harris) </p><p class="MsoNormal"></p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">The divisor graph D(n) is an undirected graph with</p>

<p class="MsoNormal">vertex set V={1,...,n}$ and</p>

<p class="MsoNormal">edge set E={(a,b) :  a  divides  b 
or  b  divides  a }</p>

<p class="MsoNormal">We denote the length of the longest simple path in D(n) by
L(n).</p>

<p class="MsoNormal">EXAMPLE: if n=10 then one long-ish sequence is</p>

<p class="MsoNormal">1,8,4,2,6,3,9</p>

<p class="MsoNormal">so L(10) GE 7. I leave it to the reader to do better OR to
show its optimal. </p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">In 2017 Oliver Roeder asked for L(100) (see <a href="https://fivethirtyeight.com/features/pick-a-number-any-number/">here</a>)
In a later post Roeder reported that Anders Kaseorg claimed L(100)=77
(see  <a href="https://fivethirtyeight.com/features/is-this-bathroom-occupied/">here</a>).
Anders gave a sequence and claimed that, by a computer search, this was
optimal. The column also claims that other people also claimed 77 and nobody
got a sequence of length 78, so the answer probably is 77 (it is now known that
it IS 77).  Roeder also mentions the case of n=1000 for which Kaseorg
showed L(1000) GE 418. No nontrivial lower bounds are known. </p>

<p class="MsoNormal">In 2019 I (Gasarch) asked about asymptotic results for
L(n)  (see my blog post <a href="https://blog.computationalcomplexity.org/2017/09/a-problem-i-thought-was-interesting-now.html">here</a> and
my open problems column <a href="https://www.cs.umd.edu/~gasarch/open/nate.pdf">here</a>.)
I began working on it with David and Tomas Harris. David proved that </p>

<p class="MsoNormal">Omega( n/( (log n)^{1.68} )  LE  L(n) 
LE  O( n/( (log n)^{0.79} ).</p>

<p class="MsoNormal">We also studied human-readable proofs that L(100) LE X for
some reasonable X, though getting a human-readable proof for X=77 seemed
impossible. We did get L(100) LE 83, in a human-readable proof. (Some
commenters on my post to sketched a proof  that L(100) LE 83 and another
that L(100) LE 80 as well.) </p>

<p class="MsoNormal"> But it turned out that this problem had already been
studied, predating Roeder's column. (This blog post is all about the math, bout
the math, no treble.  My next post will be about how we didn't know the
literature until our paper was close to being finished.) </p>

<p class="MsoNormal">In 1982 Pomerance showed L(n)  LE o(n) (see <a href="https://math.dartmouth.edu/~carlp/divisorgraph.pdf">here</a>).
Pollington had earlier shown </p>

<p class="MsoNormal">               
                     
    L(n) GE ne^{polylog(n)};</p>

<p class="MsoNormal">however, the paper is not online and hence is lost to
history forever. (If you can find an online copy please email me the pointer
and I will edit this post.) </p>

<p class="MsoNormal">In 1995 Gerald Tenenbaum showed, in a paper written in
French,  that there exists a,b such that </p>

<p class="MsoNormal">               
               n/(log n)^a LE L(n) LE
n/(log n)^b (see <a href="http://www.numdam.org/item/ASENS_1995_4_28_2_115_0/">here</a>). </p>

<p class="MsoNormal">More recently, in 2021, Saias showed, in a paper written in
French, that </p>

<p class="MsoNormal">               
                     
L(n) GE (0.3 - o(1)) n/log n (see <a href="https://arxiv.org/abs/2107.03855">here</a>). </p><p class="MsoNormal">(ADDED LATER:  I got a very angry email telling me that the paper was in English and that I am a moron. It turns out that the abstract is in English but the paper is in French, hence the person who send the letter only read the abstract which explains their mistake.) </p>

<p class="MsoNormal">He conjectures that L(n)  SIM cn/log n where c is
likely in the interval [3,7]. (Apparently, no other information is known about
the relevant constant factors in the estimates.)</p>

<p class="MsoNormal">Interestingly, the work of Tenenbaum and Saias also
demonstrates why the study of L(n)  is not an idle problem in recreational
mathematics. The upper bounds come from results on certain density conditions
for prime factorization of random integers. That is, given an integer x chosen
uniformly at random from the range {1,..., n} with prime factorization p1 GE p2
GE ... one wants to show that, with high probability, the primes pi are close
to each other in a certain sense. Most recent results on L(n) have been tied
closely with improved asymptotic estimates for deep number theory problems.</p>

<p class="MsoNormal">Determining the value of L(100) (i.e., Roeder's problem) was
mentioned in Saias's paper. He claims that L(100) = 77 was discovered by Arnaud
Chadozeau, who himself has written a number of papers on other properties of
D(n). Since this paper was in 2021 it was after Roeder's column; however, we
believe that the different discoveries of L(100) are independent. The recent
work around Roeder's column appears to be done independently from the extensive
French-language literature on the topic.</p>

<p class="MsoNormal">The following problems are  likely still open:</p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">a) Find L(n) exactly for as many n as you can.  This
would clearly need a computer program.</p>

<p class="MsoNormal">A listing of L(n) for n = 1 ... 200, computed by Rob Pratt
and Nathan McNew,</p>

<p class="MsoNormal">appears as OEIS #A337125. This also includes additional
references.</p>

<p class="MsoNormal"> </p>

<p class="MsoNormal">b) Find human-readable proofs for upper bounds on L(n)
(likely not exact) for as many</p>

<p class="MsoNormal">n as you can.</p>

<p class="MsoNormal">  </p><p class="MsoNormal"></p>

<p class="MsoNormal">ADDED LATER: Gaétan Berthe emailed me </p>

<p class="MsoNormal">-----------------------------------------------------------------</p><p class="MsoNormal"></p>

<p class="MsoNormal">I'm the author of the last comment on your article about
Roeder Sequence , as your curious about the subject I can share what we've done
with my friend Paul Revenant those last few years for fun.</p><p class="MsoNormal">
<br />
It all started with a competition between our classmates (see <a href="https://perso.ens-lyon.fr/gaetan.berthe/challenge.php">here</a> though note that its in French) for the 100 and 1000 cases, after a few months Paul using a MIP solver gurobi
was able to found a solution of size 666, and last year by studying the
structure of the 666 solution we were able, with the help of gurobi again, to
prove that there was no 667 solution either.<br />
<br />
Paul then achieve to find very probable value of the sequence for 1 to 1000 (we
didn't automatize the proof of 666 but it should be doable). On my side I tried
to look for good solutions for the 10000 case, again using gurobi and the structure
that appeared in the solution of size 666. The structure enable to cut the
problem in two subpart, so the search goes faster I was able to find a solution
of size 5505.<br />
<br />
So I would say that the two mains reasons we're able to prove optimality for high
numbers as 1000 are:<br />
<br />
- MIP solver such as gurobi are very powerful tools.<br />
<br />
- The longest path in the divisor graph are highly structured.<br />
<br />
I joined our informal proof of the 666 case (the solution at the end), what is
interesting is to understand how the solution is composed of different blocks
depending of the prime decomposition of the elements. I joined lower bound from
1 to 1000 computed by Paul, that are very likely to be optimal.</p>

<p class="MsoNormal">---------------------------------------------------------------------------------------------------------</p>

<p class="MsoNormal">He also emailed me</p><p class="MsoNormal"></p>

<p class="MsoNormal">1)  a list of the numbers I call L(n) for n=1 to 1000.
These have not been refereed though I think they are correct. The list is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/1-1000.txt">here</a></p><p class="MsoNormal"></p>

<p class="MsoNormal">AND</p>

<p class="MsoNormal">2)  a PROOF that L(1000)\le 666 (and they HAVE a
sequence of length 666, so L(1000)=666).</p>

<p class="MsoNormal">Again, not refereed, but you can read the proof
yourself <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/proof1000.pdf">here</a> WARNING- the proof is in ENGLISH, so you cannot use it to improve your
mathematical French. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/the-roeder-seq-problems-was-solved.html"><span class="datestr">at April 11, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2843">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2022/04/11/tcs-women-spotlight-workshop-at-stoc-2022-travel-grants-and-call-for-speaker-nominations/">TCS Women Spotlight Workshop at STOC 2022: Travel grants and call for speaker nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From the orgenizers:</p>



<p>===============</p>



<p>You are cordially invited to the TCS Women Spotlight Workshop! The workshop will be held on Monday, June 20, 2022, in Rome, Italy, as part of the <a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener">54th Symposium on Theory of Computing (STOC) and TheoryFest</a>.  The workshop is open to all.<br />More information about the workshop is available here: </p>



<p><a href="https://sigact.org/tcswomen/" target="_blank" rel="noreferrer noopener">https://sigact.org/tcswomen/</a> In particular, we would like to highlight the TCS Women Travel Scholarships (deadline April 25) and a call for nominations for Rising Stars talks at the workshop (deadline May 9).  More information below.</p>



<p>Hope to see you in Rome!</p>



<p><strong><em>TCS Women Travel Scholarship</em></strong></p>



<p>TCS Women Travel Scholarships are intended for researchers at the beginning of their career. Preference will be given to students at the beginning of their studies. If we have sufficient funding, we will give awards to more senior students and possibly postdocs.</p>



<p>To apply, you will need to fill out the following form by <strong>April 25th</strong>, 2022 (11:59 pm PDT) in which you provide basic information about yourself, an estimate of your expenses, and a brief statement: <a href="https://forms.gle/eMj1es2GnJRkAGdB6" target="_blank" rel="noreferrer noopener">Apply for a travel grant here.</a></p>



<p>In addition, you will need to have your advisor (or department head or other faculty mentor if you do not yet have an advisor) send a letter of support to <a href="mailto:tcswomen@gmail.com" target="_blank" rel="noreferrer noopener">tcswomen@gmail.com</a> by April 25th, 2022. Your advisor’s letter should also describe the availability of other travel funds.  This letter should be sent with the subject line “support letter for [your name]”. This is very important. Your application is not complete without this letter.  Note for advisors: Specifics about alternative funding are very helpful — statements like “funding is tight” are not very helpful. Late applications (after April 25th) will not be accepted. You will be notified about your status by April 29th, which is prior to the STOC early registration deadline and hotel cut-off date.</p>



<p><br />Notes: Receipts will be required for all travel awards, and reimbursements will be made after the conference. Food or visa expenses will not be reimbursed.<br /><br /><strong><em>Nominations for Rising Star talks:</em></strong></p>



<p>We invite nominations for speakers in our TCS Women Rising Star talks at the TCS Women Spotlight Workshop at STOC 2022. To be eligible, your nominee has to be a senior PhD student with expected graduation no later than August 2022, or a postdoc; in theoretical computer science (all topics represented at STOC are welcome); female or an underrepresented minority; and not a speaker at a previous TCS Women Spotlight Workshop.  Preference will be given to speakers who are currently on the job market for postdoctoral/faculty positions, or who expect to be on the job market in Fall 2022.</p>



<p>You can make your nomination by filling this form by <strong>May 9th</strong>: <a href="https://forms.gle/3hoaieSFFYSjmAwt7" target="_blank" rel="noreferrer noopener">https://forms.gle/3hoaieSFFYSjmAwt7</a></p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2022/04/11/tcs-women-spotlight-workshop-at-stoc-2022-travel-grants-and-call-for-speaker-nominations/"><span class="datestr">at April 11, 2022 05:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/04/10/tcs-women-spotlight-workshop-at-stoc-2022-travel-grants-and-call-for-speaker-nominations-guest-post-by-mary-wooters/">TCS Women Spotlight Workshop at STOC 2022: Travel grants and call for speaker nominations. (Guest post by Mary Wooters)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Mary Wooters shares the following information – I highly recommend attending the TCS Women Spotlight workshop at any time, but especially when it’s in Rome! –Boaz]</em><br /><br />You are cordially invited to the TCS Women Spotlight Workshop! The workshop will be held on Monday, June 20, 2022, in Rome, Italy, as part of the <a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener">54th Symposium on Theory of Computing (STOC) and TheoryFest</a>.  The workshop is open to all.<br />More information about the workshop is available here: </p>



<p><a href="https://sigact.org/tcswomen/" target="_blank" rel="noreferrer noopener">https://sigact.org/tcswomen/</a> In particular, we would like to highlight the TCS Women Travel Scholarships (deadline April 25) and a call for nominations for Rising Stars talks at the workshop (deadline May 9).  More information on those are below.</p>



<p>Hope to see you in Rome!</p>



<p><strong><em>TCS Women Travel Scholarship:</em></strong></p>



<p>TCS Women Travel Scholarships are intended for researchers at the beginning of their career. Preference will be given to students at the beginning of their studies. If we have sufficient funding, we will give awards to more senior students and possibly postdocs.</p>



<p>To apply, you will need to fill out the following form by <strong>April 25th</strong>, 2022 (11:59 pm PDT) in which you provide basic information about yourself, an estimate of your expenses, and a brief statement: <a href="https://forms.gle/eMj1es2GnJRkAGdB6" target="_blank" rel="noreferrer noopener">Apply for a travel grant here.</a></p>



<p>In addition, you will need to have your advisor (or department head or other faculty mentor if you do not yet have an advisor) send a letter of support to <a href="mailto:tcswomen@gmail.com" target="_blank" rel="noreferrer noopener">tcswomen@gmail.com</a> by April 25th, 2022. Your advisor’s letter should also describe the availability of other travel funds.  This letter should be sent with the subject line “support letter for [your name]”. This is very important. Your application is not complete without this letter.  Note for advisors: Specifics about alternative funding are very helpful — statements like “funding is tight” are not very helpful. Late applications (after April 25th) will not be accepted. You will be notified about your status by April 29th, which is prior to the STOC early registration deadline and hotel cut-off date.</p>



<p><br />Notes: Receipts will be required for all travel awards, and reimbursements will be made after the conference. Food or visa expenses will not be reimbursed.<br /><br /><strong><em>Nominations for Rising Star talks:</em></strong></p>



<p>We invite nominations for speakers in our TCS Women Rising Star talks at the TCS Women Spotlight Workshop at STOC 2022. To be eligible, your nominee has to be a senior PhD student with expected graduation no later than August 2022, or a postdoc; in theoretical computer science (all topics represented at STOC are welcome); female or an underrepresented minority; and not a speaker at a previous TCS Women Spotlight Workshop.  Preference will be given to speakers who are currently on the job market for postdoctoral/faculty positions, or who expect to be on the job market in Fall 2022.</p>



<p>You can make your nomination by filling this form by <strong>May 9th</strong>: <a href="https://forms.gle/3hoaieSFFYSjmAwt7" target="_blank" rel="noreferrer noopener">https://forms.gle/3hoaieSFFYSjmAwt7</a></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/04/10/tcs-women-spotlight-workshop-at-stoc-2022-travel-grants-and-call-for-speaker-nominations-guest-post-by-mary-wooters/"><span class="datestr">at April 10, 2022 10:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19893">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">Discussion About Proving—Again</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>Deja Vu All Over Again—Yogi Berra</i></p>
<p>
Rich DeMillo is one of my greatest friends, colleagues, and terrific co-authors. He is currently fighting an illness, and we wish him the best.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/radold/" rel="attachment wp-att-19896"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/radold.jpg?resize=300%2C300&amp;ssl=1" class="aligncenter size-medium wp-image-19896" height="300" /></a></p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/rad/" rel="attachment wp-att-19899"><img width="225" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/rad.jpg?resize=225%2C300&amp;ssl=1" class="aligncenter size-medium wp-image-19899" height="300" /></a></p>
<p>
Rich was the key guest for Harry Lewis’s class <a href="https://mobile.twitter.com/richde/status/1398627350984728580">last June</a>. This time it is my turn to be part of the discussion of our paper <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">Social Process and Proofs of Theorems and Programs</a> that starts:  </p>
<p><em>Many people have argued that computer programming should strive to become more like mathematics. Maybe so, but not in the way they seem to think.</em></p>
<p>Sadly the paper’s third author, Alan Perlis, is unable to ever take part.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/ap/" rel="attachment wp-att-19900"><img width="261" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/ap.png?resize=261%2C193&amp;ssl=1" class="aligncenter size-full wp-image-19900" height="193" /></a></p>
<p>
</p><p><b> The Class </b></p>
<p></p><p>
Lewis’s class is based on his <a href="https://www.amazon.com/Ideas-That-Created-Future-Computer/dp/0262045303/ref=sr_1_1?dchild=1&amp;keywords=lewis+ideas+that+created+the+future&amp;qid=1595538528&amp;s=books&amp;sr=1-1">book</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/book-8/" rel="attachment wp-att-19902"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/book.png?resize=300%2C182&amp;ssl=1" class="aligncenter size-medium wp-image-19902" height="182" /></a></p>
<p>
<em>The book highlights results that played a key part in the history of CS.  Ideas That Created the Future collects forty-six classic papers in computer science that map the evolution of the field. It covers all aspects of computer science: theory and practice, architectures and algorithms, and logic and software systems, with an emphasis on the period of 1936-1980 but also including important early work. Offering papers by thinkers ranging from Aristotle and Leibniz to Alan Turing and Nobert Wiener, the book documents the discoveries and inventions that created today’s digital world. Each paper is accompanied by a brief essay by Harry Lewis, the volume’s editor, offering historical and intellectual context. </em></p>
<p>
Some have asked: How could Rich and Alan and myself be included with such notables? Many hate our paper, but perhaps that is just life. This Tuesday it will be discussed again. </p>
<p>
</p><p><b> Our Paper </b></p>
<p></p><p>
Rich and I were in the late 1970’s quite interested in program correctness. We had three approaches:</p>
<li>Random Testing: We were the first to show that certain programs could be mathematically tested via random tests. The insight was that if the correctness was based on a polynomial restriction, then randomness could be proved to work with very small probability of failure. This was <a href="https://www.researchgate.net/publication/220112604_A_Probabilistic_Remark_on_Algebraic_Program_Testing">A probabilistic remark on algebraic program testing.</a>
</li><li>Program Testing: The method here is based on what is called the <a href="https://en.wikipedia.org/wiki/Mutation_testing">mutation method</a>. It is powerful, but cannot be raised to mathematical certainty.
</li><li>Proving Programs is Hard: This was our negative result that is the key to the debate.
<p>
</p><p><b> The Debate—Starts </b></p>
<p></p><p>
The key to the argument is two part: </p>
<ol>
<li> The correctness of real systems is not clean. It is very complex, very case based. Terribly complex.
</li><li> The proof method of math is not geared to huge complex statements of correctness.
</li></ol>
<p>
</p><p><b> The Debate—Continues </b></p>
<p></p><p>
Today papers on proof methods split into two classes: </p>
<ol>
<li> Those that refer to us. Usually negatively, but do at least they refer to us.
</li><li> Those that ignore us completely.
</li></ol>
<p>
Here are three examples from countless many papers: They should reference us but do not: </p>
<ol>
<li> <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/automatic-theorem-proving/philosophy.htm">Philosophical Interests in Program Verification</a>
</li><li> <a href="https://link.springer.com/chapter/10.1007/978-3-319-91908-9_18">Deductive Software Verification: From Pen-and-Paper Proofs to Industrial Tools</a>
</li><li> <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.522.3577&amp;rep=rep1&amp;type=pdf">Correctness: A Very Important Quality Factor In Programming</a>
</li></ol>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Should there be some middle ground? Do we deserve to be in Harry’s book?</p>
<p></p></li></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/"><span class="datestr">at April 10, 2022 06:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1368">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/04/09/theoryfest-2022-registration-now-open-and-travel-grant-applications-due-soon/">TheoryFest 2022: Registration now open and travel grant applications due soon!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Call for Participation </strong></p>



<p><a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener"><strong>54th ACM Symposium on Theory of Computing (STOC 2022) – Theory Fest </strong></a></p>



<p><strong>June 20-24, 2022 </strong></p>



<p><strong>Rome, Italy </strong></p>



<p>The 54th ACM Symposium on Theory of Computing (STOC 2022) is sponsored by the ACM Special Interest Group on Algorithms and Computation Theory and will be held in Rome, Italy, Monday June 20 – Friday, June 24, 2022.</p>



<p>STOC 2022 – Theory Fest will feature technical talk sessions, <a href="http://acm-stoc.org/stoc2022/workshops.html" target="_blank" rel="noreferrer noopener">6 workshops</a> with introductory tutorials, poster sessions, social events, and a special joint session with “<a href="https://www.lincei.it/en" target="_blank" rel="noreferrer noopener">Accademia Nazionale dei Lincei</a>”, the oldest and most prestigious Italian academic institution, followed by a reception and a concert at the <a href="https://www.lincei.it/en/corsini-palace" target="_blank" rel="noreferrer noopener">Academy historic site</a>. </p>



<p><strong>Registration</strong></p>



<p>STOC 2022 registration is available <a href="http://acm-stoc.org/stoc2022/registration.html" target="_blank" rel="noreferrer noopener">here</a>.</p>



<p><strong>Early registration deadline: April 30th. </strong></p>



<p><strong>Student Travel Grants </strong></p>



<p>Information for student travel grant applications is available <a href="http://acm-stoc.org/stoc2022/travel-support.html" target="_blank" rel="noreferrer noopener">here</a>. </p>



<p><strong>Application deadline: April 20th.</strong></p>



<p>STOC 2022 is sponsored by Algorand, Amazon, Apple, Google, IOHK, Microsoft, Sapienza University of Rome. </p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/04/09/theoryfest-2022-registration-now-open-and-travel-grant-applications-due-soon/"><span class="datestr">at April 09, 2022 02:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4616">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2022/04/08/renato-capocelli-1940-1992/">Renato Capocelli (1940-1992)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Thirty years ago, I was in the middle of the second semester of my third year of undergrad, and one of the courses that I was enrolled in was on information theory. I was majoring in computer science, a major that had just been established at Sapienza University when I signed up for it in 1989, <s>organized by a computer science department that had also just been established in 1989</s>. A computer science department was established in 1991.</p>



<p>The new Sapienza computer science department was founded mostly by faculty from the Sapienza mathematics department, plus a number of people that came from other places to help start it. Among the latter, Renato Capocelli had moved to Rome from the University of Salerno, where he had been department chair of computer science.</p>



<p>Capocelli worked on combinatorics and information theory. In the early 90s, he had also become interested in the then-new area of zero-knowledge proofs.</p>



<p>Capocelli taught the information-theory course that I was attending, and it was a very different experience from the classes I had attended up to that point. To get the new major started, several professors were teaching classes outside their area, sticking close to their notes. Those teaching mathematical classes, were experts but were not deviating from the definition-theorem-proof script. Capocelli had an infectious passion for his subject, took his time to make us gain an intuitive understanding of the concepts of information theory, was full of examples and anecdotes, and  always emphasized the high-level idea of the proofs.</p>



<p>I subsequently met several other charismatic and inspiring computer scientists and mathematicians, though Capocelli had a very different personality from most of them. He was like an earlier generation of Southern Italian intellectuals, who could be passionate about their subject in a peculiarly non-nerdy way, loving it the way one may love food, people, nature, or a full life in general.</p>



<p>On April 8, 1992, Renato Capocelli <a href="https://www.di.uniroma1.it/sites/default/files/dipartimento/capocelli.pdf">died suddenly and unexpectedly</a>, though his memory lives on in the many people he inspired. The Computer Science department of the University of Salerno was named after him for a period of time.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2022/04/08/renato-capocelli-1940-1992/"><span class="datestr">at April 08, 2022 03:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">Summer visiting researcher position at Boston College, Computer Science, Bento’s Lab (apply by May 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for multiple visiting scholar positions in any of the following areas:</p>
<p>* parallel and distributed algorithms/optimization, convex and non-convex optimization</p>
<p>* machine learning, graphical models, information theory</p>
<p>* network theory and graph matching</p>
<p>* deep learning, robustness in machine learning</p>
<p>Start and end dates flexible. Duration up to 4 months.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/21519">https://academicjobsonline.org/ajo/jobs/21519</a><br />
Email: visiting@jbento.info</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/"><span class="datestr">at April 06, 2022 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/049">TR22-049 |  Non-Adaptive Universal One-Way Hash Functions from Arbitrary One-Way Functions | 

	Noam Mazor, 

	Jiapeng Zhang, 

	Xinyu Mao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Two of the most useful cryptographic primitives that can be constructed from one-way functions are pseudorandom generators (PRGs) and universal one-way hash functions (UOWHFs). The three major efficiency measures of these primitives are: seed length, number of calls to the one-way function, and adaptivity of these calls. Although a long and successful line of research studied these primitives, their optimal efficiency is not yet fully understood: there are gaps between the known upper bound and the known lower bound for black-box constructions.

Interstingly, the first construction of PRGs by H ?astad, Impagliazzo, Levin, and Luby [SICOMP ’99], and the UOWHFs construction by Rompel [STOC ’90] shared a similar structure. Since then, there was an improvement in the efficiency of both constructions: The state of the art construction of PRGs by Haitner, Reingold, and Vadhan [STOC ’10] uses $O(n^4)$ bits of random seed and $O(n^3)$ non-adaptive calls to the one-way function, or alternatively, seed of size $O(n^3)$ with $O(n^3)$ adaptive calls (Vadhan and Zheng [STOC ’12]). Constructing a UOWHF with similar parameters is still an open question. Currently, the best UOWHF construction by Haitner, Holenstein, Reingold, Vadhan, and Wee [Eurocrypt ’10] uses $O(n^{13})$ adaptive calls with a key of size $O(n^5)$.

In this work we give the first non-adaptive construction of UOWHFs from arbitrary one-way functions. Our construction uses $O(n^9)$ calls to the one-way function, and key of length $O(n^{10})$. By the result of Applebaum, Ishai, and Kushilevitz [FOCS ’04], the above implies the existence of UOWHFs in NC0, given the existence of one-way functions in NC1. We also show that the PRG construction of Haitner et al., with small modifications, yields a relaxed notion of UOWHFs. In order to analyze this construction, we introduce the notion of next-bit unreachable entropy, which replaces the next-bit pseudoentropy notion, used in the PRG construction above.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/049"><span class="datestr">at April 06, 2022 06:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1641">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2022/04/news-for-march-2022/">News for March 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This was a relatively sleepy month with only two property testing papers. Do let us know if we missed any. Let us dig in. (<strong><em>EDIT</em></strong>: Two updates.) </p>



<ol><li>I missed two papers. One on the estimation of quantum entropies and the other on algorithms and lower bounds for estimating MST and TSP costs. </li><li>Finally, I forgot to welcome our new editor. <em>Welcome onboard, <a href="https://www.cmi.ac.in/~nithinvarma/">Nithin Varma</a></em>!!</li></ol>



<p></p>



<p><strong>Private High-Dimensional Hypothesis Testing</strong> by  Shyam Narayanan (<a href="https://arxiv.org/abs/2203.01537">arXiv</a>) This paper continues the novel study of distribution testing under the constraints brought forth by differential privacy extending the work of Canonne-Kamath-McMillan-Ullman-Zakynthinou (henceforth CKMUZ, covered in our <a href="https://ptreview.sublinear.info/2019/06/news-for-may-2019/">May 2019 post</a>). In particular, the paper presents algorithms with optimal sample complexity for private identity testing of \(d\)-dimensional Gaussians. In more detail, the paper shows that can be done with a mere \(\widetilde{O}\left(  \frac{d^{1/2}}{\alpha^2} + \frac{  d^{1/3} }{ \alpha^{4/3} \cdot \varepsilon^{2/3}} + \frac{1}{\alpha \cdot \varepsilon} \right)\). Here \(\alpha\) is the proximity parameter and \(\varepsilon\) is the privacy parameter. Combined with a previous result of Acharya-Sun-Zhang, the paper proves that private identity testing of \(d\)-dimensional Gaussians is doable with a sample complexity smaller than that of private identity testing of discrete distributions over a domain of size \(d\) thereby refuting a conjecture of CKMUZ.</p>



<p></p>



<p><strong>Differentially Private All-Pairs Shortest Path Distances: Improved Algorithms and Lower Bounds</strong> by Badih Ghazi, Ravi Kumar, Pasin Manurangsi and Jelani Nelson (<a href="https://arxiv.org/abs/2203.16476">arXiv</a>) Adam Sealfon considered the classic All Pairs Shortest Path Problem (the APSP problem) with privacy considerations in 2016. In the \((\varepsilon, \delta)\)-DP framework, Sealfon presented an algorithm which on input an edge-weighted graph \(G=(V,E,w)\) adds Laplace noise to all edge weights and computes the shortest paths on this noisy graph. The output of the algorithm satisfies that the estimated distance between every pair is within an additive \(\pm O(n \log n/\varepsilon)\) of the actual distance (the absolute value of this parameter is called the <em>accuracy of the algorithm</em>). Moreover, this error is tight up to a logarithmic factor if the algorithm is required to <em>release</em> the shortest paths. The current paper shows you can privately release all the pairwise distances while suffering only a sublinear accuracy if you additionally release the edge weights (in place of releasing the shortest paths). In particular, this paper presents an \(\varepsilon\)-DP algorithm with sublinear \(\widetilde{O}(n^{2/3})\) accuracy.</p>



<p></p>



<p><strong>Quantum algorithms for estimating quantum entropies</strong> by Youle Wang, Benchi Zhao, Xin Wang (<a href="https://arxiv.org/abs/2203.02386">arXiv</a>) So, remember <a href="https://ptreview.sublinear.info/2021/12/news-for-november-2021/">our post from December</a> on sublinear quantum algorithms for estimation of quantum (von Neumann) entropy?  The current paper begins by noting that the research so far (along the lines of the work above) assumes access to a quantum query model for the input state which we do not yet know how to construct efficiently. This paper addresses this issue and gives quantum algorithms to estimate the von Neumann entropy of a  \(n\)-qubit quantum state \(\rho\) by using independent copies of the input state.</p>



<p></p>



<p><strong>Sublinear Algorithms and Lower Bounds for Estimating MST and TSP Cost in General Metrics</strong> by Yu Chen, Sanjeev Khanna, Zihan Tan (<a href="https://arxiv.org/abs/2203.14798">arXiv</a>) As mentioned in the title, this paper studies sublinear algorithms for the metric MST and the metric TSP problem. The paper obtains a wide assortment of results and shows that both these problems admit an \(\alpha\)-approximation algorithm which uses \(O(n/\alpha)\) space. This algorithm assumes that the input is given as a stream of \(n \choose 2\) metric entries. Under this model, the paper also presents an \(\Omega(n/\alpha^2)\) space lower bound. Let me highlight one more result from the paper. In a <a href="https://ptreview.sublinear.info/2020/07/news-for-june-2020/">previous news</a> (from June 2020), we covered a result detailing a better than \(2\)-approximation for the <em>graphic</em> TSP and \((1,2)\) TSP which runs in sublinear time. This paper extends this result and obtains better than \(2\)-approximation for TSP on a relaively richer class of metrics.</p></div>







<p class="date">
by Akash <a href="https://ptreview.sublinear.info/2022/04/news-for-march-2022/"><span class="datestr">at April 05, 2022 08:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19873">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/">Blogs that are Current</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<i>Just because we can’t find a solution, it doesn’t mean there isn’t one—Andrew Wiles.</i></p>
<p>
Harry Lewis is the Gordon McKay Professor of Computer Science in Harvard’s School of Engineering and Applied Sciences, where he has taught since 1974. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/lewis-harry_200x300_0/" rel="attachment wp-att-19883"><img width="200" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/04/Lewis.Harry_200x300_0.jpg?resize=200%2C300&amp;ssl=1" class="aligncenter size-full wp-image-19883" height="300" /></a></p>
<p>
</p><p><b> A Current Blog </b></p>
<p></p><p>
A blog is <b>current</b> provided its last post refers to stuff that happen in late 2021 or 2022. Lewis has a current blog and this one is also current. I like a current one—some blogs are very incurrent indeed. </p>
<p>
</p><p><b> List of Current Blogs </b></p>
<p></p><p>
The first group below are ones we have listed before and the second group are those that were found elsewhere. All below are current. Note the symbol <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /> marks the next alphabetic grouping.</p>
<p>
<a href="https://agtb.wordpress.com">Turing Hand</a><br />
 <a href="https://johncarlosbaez.wordpress.com">Baez</a><br />
 <a href="http://harry-lewis.blogspot.com">Harry Lewis</a><br />
 <a href="https://gilkalai.wordpress.com">Gil Kalai</a><br />
 <a href="https://gentzen.wordpress.com">Gentzen</a><br />
 <a href="http://jdh.hamkins.org">Hawkins</a><br />
 <a href="https://inquiryintoinquiry.com">inquiry</a><br />
 <a href="https://lucatrevisan.wordpress.com">Trevisan</a><br />
 <a href="https://cameroncounts.wordpress.com">Peter Cameron’s Blog</a><br />
 <a href="https://priorprobability.com">Prior probability</a><br />
 <a href="https://thehighergeometer.wordpress.com">Geometer</a><br />
 <a href="https://windowsontheory.org">Windows On Theory</a></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://11011110.github.io/blog/">11011110</a><br />
 <a href="https://blogs.ams.org/inclusionexclusion/">AMS blogs</a><br />
 <a href="https://blogs.ams.org/mathgradblog/">AMS Graduate Student Blog</a><br />
 <a href="https://www.siam.org/conferences/cm/conference/pd22">Analysis &amp; PDE Conferences</a><br />
 <a href="http://avzel.blogspot.com">Avzel’s journal</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://gilkalai.wordpress.com">Combinatorics and more</a><br />
 <a href="http://dsp.rice.edu">Compressed sensing resources</a><br />
 <a href="https://blog.computationalcomplexity.org">Computational Complexity</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://sites.math.rutgers.edu/~zeilberg/OPINIONS.html">Dr. Zeilberger</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://equatorialmaths.wordpress.com">Equatorial Mathematics</a><br />
 <a href="https://ramismovassagh.wordpress.com/2022/03/24/ergodic-bridging-math-and-physics/">Ergodic</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://burttotaro.wordpress.com">Geometry Bulletin Board</a><br />
 <a href="https://gshakan.wordpress.com/about-the-blogger/">George Shakan</a><br />
 <a href="https://gerardbesson.wordpress.com">Gerard Besson’s Blog</a><br />
 <a href="http://www.girlsangle.org">Girl’s Angle</a><br />
 <a href="https://gottwurfelt.com">God Plays Dice</a><br />
 <a href="https://rjlipton.wpcomstaging.com">Godel’s Lost Letter and P=NP</a><br />
 <a href="https://dms.umontreal.ca/~andrew/">Andrew Granville</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://robwilson1.wordpress.com">Hidden Assumptions</a><br />
 <a href="https://alanrendall.wordpress.com">Hydrobates</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://igorpak.wordpress.com">Igor Pak’s blog</a><br />
 <a href="https://images.math.cnrs.fr/?lang=fr">Images des mathematiques</a><br />
 <a href="https://lucatrevisan.wordpress.com">In theory</a><br />
 <a href="http://combinatoricsinstitute.blogspot.com">Institute of Combinatorics</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://jbuzzi.wordpress.com">Jerome Buzzi’s Mathematical Ramblings</a><br />
 <a href="http://jdh.hamkins.org">Joel David Hamkins</a><br />
 <a href="https://www.ams.org/publications/journals/journalsframework/jams">Journal of the American Mathematical Society</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://kourovka-notebook.org">Kouroka</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://yetaspblog.wordpress.com">Le Petit Chercheur Illustre</a><br />
 <a href="https://djalil.chafai.net/blog/">Libres pensees d’un mathematicien ordinaire</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.math3ma.com">Math3ma</a><br />
 <a href="https://mathbabe.org">Mathbabe</a><br />
 <a href="https://mathblogging.org">Mathblogging</a><br />
 <a href="http://notable.math.ucdavis.edu/wiki/Mathematics_Jobs_Wiki">Mathematics Jobs Wiki</a><br />
 <a href="https://mattbaker.blog">Matt Baker’s Math Blog</a><br />
 <a href="http://muchadoaboutnothinggg.blogspot.com">Much ado about nothing</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/HomePage">nLab</a><br />
 <a href="http://www.math.columbia.edu/~woit/wordpress/">Not Even Wrong</a><br />
 <a href="http://www.numbertheory.org/ntw/number_theory.html">Number theory web</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://sites.google.com/view/o-a-r-s">Online Analysis Research Seminar</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="http://www.quantumfrontiers.com">Pengfei Zhang’s blog</a><br />
 <a href="https://www.galoisrepresentations.com">Persiflage</a><br />
 <a href="https://cameroncounts.wordpress.com">Peter Cameron’s Blog</a><br />
 <a href="https://philippelefloch.org">Phillipe LeFloch’s blog</a><br />
 <a href="http://processalgebra.blogspot.com">Process Algebra</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://reasonabledeviations.com">Reasonable Deviations</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.gregegan.net/SCIENCE/Science.html">Science Notes by Greg Egan</a><br />
 <a href="https://gowers.wordpress.com/2013/06/16/the-selected-papers-network/">Selected Papers Network</a><br />
 <a href="https://scottaaronson.blog">Shtetl-Optimized</a><br />
 <a href="https://georgerrmartin.com/notablog/">Since it is not…</a><br />
 <a href="https://sketchesoftopology.wordpress.com">Sketches of topology</a><br />
 <a href="https://nttoan81.wordpress.com">Snapshots in Mathematics!</a><br />
 <a href="https://www.math.columbia.edu/~dejong/wordpress/">Stacks Project Blog</a><br />
 <a href="https://symomega.wordpress.com">SymOmega</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://blog.tanyakhovanova.com">Tanya Khovanova’s Math Blog</a><br />
 <a href="https://terrytao.wordpress.com">Terry Tao</a><br />
 <a href="https://www.flickr.com/photos/gforsythe/6800089347">The Cost of Knowledge</a><br />
 <a href="http://matroidunion.org">The Matroid Union</a><br />
 <a href="https://golem.ph.utexas.edu/category/">The n-Category Cafe</a><br />
 <a href="https://golem.ph.utexas.edu/category/">The n-geometry cafe</a></p>
<p> <a href="https://thuses.com">Thuses</a><br />
 <a href="https://gowers.wordpress.com/about/">Tim Gowers’ blog</a><br />
 <a href="https://gowers.wordpress.com">Tim Gowers’ mathematical discussions</a><br />
 <a href="https://www.theoremoftheday.org">Theorem of Day</a><br />
 <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /><a href="https://www.math.uh.edu/~climenha/math.html">Vaughn Climenhaga</a></p>
<p>
</p><p><b> Open Problems </b></p>
<p></p><p>
Can we add more posts that are current? Do we also include some that are not current?</p>
<p></p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2022/04/05/blogs-that-are-current/"><span class="datestr">at April 05, 2022 08:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8659181144938614989">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/masks.html">Masks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Illinois Tech removed the last of their mandatory masking restrictions yesterday. Chicago had zero Covid deaths. Yet I still get messages like this in my twitter feed.</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I don't care who you are. If I see you without a mask indoors when infections are rampant and there are many still at risk, I will see you as ignorant of science, narcissistic, and uncaring of the vulnerable around you. I will truly see you unmasked for what you are.</p>— Bill Comeau 🇨🇦🇺🇦 (@Billius27) <a href="https://twitter.com/Billius27/status/1510714029072912390?ref_src=twsrc%5Etfw">April 3, 2022</a></blockquote> 
<p></p><p>The science is unequivocal for vaccines, which do a good job preventing infection and a strong job saving lives. I just got my second booster on Sunday.</p><p>Masks give you some protection but nothing like the vaccines. It's impossible to completely remove the risk of Covid so people need to make their own choices and tradeoffs. If you are vaccinated your chance of serious illness is tiny, whether or not your wear a mask. And mask wearing is not cost-free.</p><p>I just don't like wearing masks. Wearing a mask bends my ears and is mildly painful. People can't always understand me when I talk through a mask, and they can't read my facial expressions. People and computers don't recognize me in a mask. Masks fog up my glasses. I can't exercise with a mask, it gets wet with sweat and hard to breath. You can't eat or drink wearing a mask.</p><p>Now everyone has their own tolerance and I respect that. I'll wear a mask if someone asks nicely or if it is required, like on public transit and many theaters. If I have a meeting with someone wearing a mask, I'll ask if they would like me to put mine on. In most cases they remove theirs.</p><p>On the other hand, the Chicago Symphony concert I planned to attend tonight was cancelled because the conductor, Riccardo Muti, <a href="https://chicago.suntimes.com/2022/4/4/23010828/riccardo-muti-tests-positive-for-covid-chicago-symphony-orchestra-concert-canceled">tested positive for Covid</a> (with minor symptoms). For my own selfish reasons, I wish he had worn a mask.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg"><img src="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg" border="0" width="320" height="213" /></a></div><br /><p><br /></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/masks.html"><span class="datestr">at April 05, 2022 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/">Asynchronous Agreement Part 5: Binary Byzantine Agreement from a strong common coin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post we show how to use Binding Crusader Agreement from the previous post, along with a strong common coin to get a simple and efficient Binary Byzantine Agreement with only an expected $O(n^2)$ message complexity. In the four previous posts we: (1) defined the problem and discussed the...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-five-ABBA/"><span class="datestr">at April 05, 2022 12:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/">Asynchronous Agreement Part 4: Crusader Agreement and Binding Crusader Agreement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post we introduce a key building block in the Byzantine Model called Binding Crusader Agreement. We show how to use it in the next post. In the three previous posts we (1) defined the problem and discussed the FLP theorem; (2) presented Ben-Or’s protocol for crash failures; and...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-04-05-aa-part-four-CA-and-BCA/"><span class="datestr">at April 05, 2022 10:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=806">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2022/04/05/lichess/">Lichess</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>After my <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">previous post, </a>a reader (who amazingly has a nearly identical playing routine) pointed me to it.  It’s great!  Much better than chess.com.</p>



<p>“<em>No ads, no subscriptions; but open-source and passion.</em>“</p>



<p>“<em>Are some features reserved to Patrons? No, because Lichess is entirely free, forever, and for everyone. That’s a promise.</em>“</p>



<p>It doesn’t get better than that, in this world; and that’s why I love computers and the community surrounding them.</p>



<p>Another good news is that I got tired of 5 | 5!  The games are just too slow, and my chess isn’t even worth this much of my time.  I am now playing 3 | 2.  It’s fun the adrenaline kick I get with every match.  Also I got much stricter with my routine, and I don’t lose more than once a day.</p>



<p>Here’s my <a href="https://lichess.org/@/EAmaiLeNuvole">profile</a>.  It’s an anagram of my unpronounceable name which can be loosely translated as <em>…and I loved the clouds.</em></p>



<p><strong>IMPORTANT</strong>: Excessive activity and blunders merely reflect my generous sharing of credentials.</p>



<p>And then I thought, why don’t we automatically analyze existing games to find interesting situations, and present them to the user as a puzzle?  Naturally, the beauty — and dismay — of living in the AI (after-internet) era is that… <a href="https://chesstempo.com/chess-tactics/">it’s already done</a>.</p>



<p>OK, time for my daily fix.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2022/04/05/lichess/"><span class="datestr">at April 05, 2022 10:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/048">TR22-048 |  On the Range Avoidance Problem for Circuits | 

	Hanlin Ren, 

	Rahul Santhanam, 

	Zhikun Wang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the range avoidance problem (called Avoid): given the description of a circuit $C:\{0, 1\}^n \to \{0, 1\}^\ell$ (where $\ell &gt; n$), find a string $y\in\{0, 1\}^\ell$ that is not in the range of $C$. This problem is complete for the class APEPP that corresponds to explicit constructions of objects whose existence follows from the probabilistic method (Korten, FOCS 2021). 
    
Motivated by applications in explicit constructions and complexity theory, we initiate the study of the range avoidance problem for weak circuit classes, and obtain the following results:
    
    
1. Generalising Williams's connections between circuit-analysis algorithms and circuit lower bounds (J. ACM 2014), we present a framework for solving C-Avoid in $FP^{NP}$ using circuit-analysis data structures for C, for "typical" multi-output circuit classes C. As an application, we present a non-trivial $FP^{NP}$ range avoidance algorithm for De Morgan formulas.
An important technical ingredient is a construction of rectangular PCPs of proximity, building on the rectangular PCPs by Bhangale, Harsha, Paradise, and Tal (FOCS 2020).
        
2. Using the above framework, we show that circuit lower bounds for $E^{NP}$ are equivalent to circuit-analysis algorithms with $E^{NP}$ preprocessing. This is the first equivalence result regarding circuit lower bounds for $E^{NP}$. Our equivalences have the additional advantages that they work in both infinitely-often and almost-everywhere settings, and that they also hold for larger (e.g., subexponential) size bounds.

3. Complementing the above results, we show that in some settings, solving C-Avoid would imply breakthrough lower bounds, even for very weak circuit classes C. In particular, an algorithm for $AC^0$-Avoid with polynomial stretch (i.e., $\ell = poly(n)$) implies lower bounds against $NC^1$, and an algorithm for $NC^0_4$-Avoid with very small stretch (i.e., $\ell = n + n^{o(1)}$) implies lower bounds against $NC^1$ and branching programs.

4. We show that Avoid is in FNP if and only if there is a propositional proof system that breaks every non-uniform proof complexity generator. This result connects the study of range avoidance with fundamental questions in proof complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/048"><span class="datestr">at April 04, 2022 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/047">TR22-047 |  Linear Hashing with $\ell_\infty$ guarantees and two-sided Kakeya bounds | 

	Manik Dhar, 

	Zeev Dvir</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that a randomly chosen linear map over a finite field gives a good hash function in the $\ell_\infty$ sense.  More concretely, consider a set $S \subset \mathbb{F}_q^n$ and a randomly chosen linear map $L :  \mathbb{F}_q^n \to  \mathbb{F}_q^t$ with $q^t$ taken to be sufficiently smaller than $|S|$. Let $U_S$ denote a random variable distributed uniformly on $S$. Our main theorem shows that, with high probability over the choice of $L$, the random variable  $L(U_S)$ is close to uniform in the $\ell_\infty$ norm.  In other words, every element in the range $\mathbb{F}_q^t$ has about the same number of elements in $S$ mapped to it. This complements the widely-used Leftover Hash Lemma (LHL) which proves the analog statement under the statistical, or $\ell_1$, distance  (for a richer class of functions) as well as prior work on the expected largest 'bucket size' in linear hash functions  [ADMPT99].  Our proof leverages a connection between linear hashing and the finite field Kakeya problem and extends some of the tools developed in this area, in particular the polynomial method.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/047"><span class="datestr">at April 04, 2022 07:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/046">TR22-046 |  Automating OBDD proofs is NP-hard | 

	Artur Riazanov, 

	Dmitry Itsykson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that the proof system OBDD(and, weakening) is not automatable unless P = NP. The proof is based upon the celebrated result of Atserias and Muller [FOCS 2019] about the hardness of automatability for resolution. The heart of the proof is lifting with a multi-output indexing gadget from resolution block-width to dag-like multiparty number-in-hand communication protocol size with $o(n)$ parties, where $n$ is the number of variables in the non-lifted formula. A similar lifting theorem for protocols with $n + 1$ participants was proved by Göös et. el. [STOC 2020] to establish the hardness of automatability result for Cutting Planes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/046"><span class="datestr">at April 04, 2022 03:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/045">TR22-045 |  Relaxed Locally Decodable and Correctable Codes: Beyond Tensoring | 

	Gil Cohen, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In their highly influential paper, Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004) introduced the notion of a relaxed locally decodable code (RLDC). Similarly to a locally decodable code (Katz-Trevisan; STOC 2000), the former admits access to any desired message symbol with only a few queries to a possibly corrupted codeword. An RLDC, however, is allowed to abort when identifying corruption. The natural analog to locally correctable codes, dubbed relaxed locally correctable codes (RLCC), was introduced by Gur, Ramnarayan and Rothblum (ITCS 2018) who constructed asymptotically-good length-$n$ RLCC and RLDC with $(\log{n})^{O(\log\log{n})}$ queries.

In this work we construct asymptotically-good RLDC and RLCC with an improved query complexity of $(\log{n})^{O(\log\log\log{n})}$. To achieve this, we devise a mechanism--an alternative to the tensor product--that squares the length of a given code. Compared to the tensor product that was used by Gur et al. and by many other constructions, our mechanism is significantly more efficient in terms of rate deterioration, allowing us to obtain our improved construction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/045"><span class="datestr">at April 04, 2022 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6837">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/information-theory-with-kernel-methods/">Information theory with kernel methods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In <a href="https://francisbach.com/von-neumann-entropy/">last month blog post</a>, I presented the von Neumann entropy. It is defined as a spectral function on positive semi-definite (PSD) matrices, and leads to a Bregman divergence called the <a href="https://en.wikipedia.org/wiki/Quantum_relative_entropy">von Neumann relative entropy</a> (or matrix Kullback Leibler divergence), with interesting convexity properties and applications in optimization (mirror descent, or smoothing) and probability (concentration inequalities for matrices). </p>



<p class="justify-text">This month, I will show how the relative entropy relates to usual notions of information theory when applied to specific covariance matrices or covariance operators, with potential applications to all areas where entropy plays a role, and with a simple use of quantum information theory. This post is based on a recent preprint: see all details in [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>], as well as the conclusion section below for several avenues for future work.</p>



<h2>Von Neumann relative entropy between covariance matrices</h2>



<p class="justify-text">We consider some set \(\mathcal{X}\), with a probability distribution \(p\). We assume given a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\), from \(\mathcal{X}\) to a \(d\)-dimensional vector space. This naturally defines a (non-centered) <em>covariance matri</em>x $$ \Sigma_{p} = \int_{\mathcal{X}} \varphi(x) \varphi(x)^\top dp(x), $$ which is a \(d \times d\) symmetric PSD matrix. We will assume throughout this post that the set \(\mathcal{X}\) is compact and that the Euclidean norm of \(\varphi\) is bounded on \(\mathcal{X}\).</p>



<p class="justify-text">Given two distributions \(p\) and \(q\), and their associated covariance matrices \(\Sigma_p\) and \(\Sigma_q\), we can compute their von Neumann relative entropy as: $$ D( \Sigma_p \| \Sigma_q) = {\rm tr} \big[ \Sigma_p ( \log \Sigma_p \, – \log \Sigma_q) \big] – {\rm tr}\big[  \Sigma_p \big] +  {\rm tr} \big[ \Sigma_q \big] .$$</p>



<p class="justify-text">We immediately get that \(D(\Sigma_p \| \Sigma_q)\) is always non-negative, and zero if and only if \(\Sigma_p = \Sigma_q\). This property, that makes it a potentially interesting measure of dissimilarity, is shared among all Bregman divergences on covariance matrices, and has been extensively used in data science (see, e.g., [<a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">2</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">3</a>]). What makes the von Neumann entropy special?</p>



<p class="justify-text">One important property is that it is jointly convex in \(p\) and \(q\), but this is also shared by other divergences [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Appendix A.4]. What really makes the von Neumann entropy special is its explicit link with with information theory.</p>



<h2>Finite sets with orthonormal embeddings</h2>



<p class="justify-text">Let’s start with the simplest situation. If \(\mathcal{X}\) is finite with \(d\) elements, and we have an orthonormal embedding, that is, for all \(x,y \in \mathcal{X}\), \(\varphi(x)^\top \varphi(y) = 1_{x = y}\), then, after a transformation by a rotation matrix, we can assume that each \(\varphi(x)\) is one element of the canonical basis, and all covariance matrices are diagonal (without the rotation, we would have that all covariance matrices are jointly diagonalizable), and for probability distributions \(p\) and \(q\), using the same notation for their probability mass functions, we have: $$D( \Sigma_p \| \Sigma_q)  = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = D(p \|q ), $$ which is exactly the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leilbler</a> (KL) divergence between the two distributions \(p\) and \(q\) [4].</p>



<p class="justify-text">Can we push this further beyond finite sets and finite-dimensional feature maps, and obtain a link with classical notions from information theory?</p>



<p class="justify-text">A first remark is the invariance by rotation within the feature space (that is, if we replace \(\varphi(x)\) by \(R \varphi(x)\), where \(R\) is an \(d \times d\) orthogonal matrix, the von Neumann entropy does not change), so that it depends only on the dot-product \(k(x,y) = \varphi(x)^\top \varphi(y)\), which is the traditional kernel function. As will see, this opens us the entire area of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel machines</a>.</p>



<p class="justify-text">A second remark is that we can go infinite-dimensional, and replace \(\mathbb{R}^d\) by any Hilbert space. This complicates a bit the exposition, and I refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for more formal definitions. In this post, I will shamelessly go infinite-dimensional by using matrices with infinite dimensions when needed.</p>



<h2>Lower-bound on Shannon relative entropy</h2>



<p class="justify-text">We first upper-bound \(D(\Sigma_p \| \Sigma_q)\) by joint convexity of the von Neumann relative entropy (see <a href="https://francisbach.com/von-neumann-entropy/">last post</a>) and Jensen’s inequality: $$ D(\Sigma_p \| \Sigma_q) =  D \Big( \int_{\mathcal{X}}\varphi(x)  \varphi(x)^\top dp(x) \Big\| \int_{\mathcal{X}} \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top dp(x) \Big)$$ $$\ \ \,  \leqslant  \int_{\mathcal{X}} D \Big( \varphi(x)  \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \Big) dp(x).$$ The two matrices  \( \varphi(x)  \varphi(x)^\top\) and \(\frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \) are proportional to each other, and have \(\varphi(x)\) as the only eigenvector with a potentially non zero eigenvalue, so that we have $$ D \Big( \varphi(x)  \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x)   \varphi(x)^\top \Big) =  \| \varphi(x) \|^2  \bigg[ \log \Big( \frac{dp}{dq}(x) \Big)  \ – 1 +  \frac{dq}{dp}(x) \bigg].$$ Integrating with respect to \(p\) leads to cancellations: $$  D(\Sigma_p \| \Sigma_q) \leqslant \int_{\mathcal{X}}\| \varphi(x) \|^2  \log \Big( \frac{dq}{dp}(x) \Big) dp(x) \tag{1}.$$ </p>



<p class="justify-text">If we assume a unit norm bound on all features, that is, \(\| \varphi(x)\| \leqslant 1\) for all \(x \in \mathcal{X}\), we get  $$  D(\Sigma_p \| \Sigma_q) \leqslant  \int_{\mathcal{X}} \log \Big( \frac{ dp}{dq}(x) \Big) dp(x) = D(p\|q), $$ and thus we have found a lower bound on the regular Shannon relative entropy. </p>



<p class="justify-text"><strong>From relative entropy to entropy. </strong>We can also define a new notion of entropy by considering the relative entropy with respect to the uniform distribution on \(\mathcal{X}\), which we denote \(\tau\). That is, defining \(\Sigma\) the covariance matrix for the uniform distribution, we consider $$ H(\Sigma_p) = \ – D(\Sigma_p \| \Sigma),$$ which is thus greater than (with a unit norm bound on all features) $$- \int_{\mathcal{X}}   \log \big( \frac{d p}{d \tau}(x)\big) dp(x),$$ which is essentially the <a href="https://en.wikipedia.org/wiki/Differential_entropy">differential entropy</a>.</p>



<p class="justify-text">We now look at a few simple examples, and present further properties later (such as a well-defined bound in the other direction). We consider only one-dimensional examples, but the theory extends to all dimensions and all kernels. In particular, finite large sets such as \(\{-1,1\}^d\) can be considered (see [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>]).</p>



<h2>Polynomials and moments</h2>



<p class="justify-text">We consider \(\mathcal{X} = [-1,1]\) and \(\varphi(x) \in \mathbb{R}^{r+1}\) spanning all degree \(r\) polynomials, with \(k(x,y) = \varphi(x)^\top \varphi(y)\). For these examples, \(\Sigma_p\) is composed of all classical moments of order up to \(2r\). </p>



<p class="justify-text"><strong>Regular polynomial kernel. </strong>Following traditional kernel methods, we can first consider \(\displaystyle k(x,y) = \frac{1}{2^r} ( 1 + xy)^r\), which corresponds to $$\varphi(x)_j =  \frac{1}{2^{r/2}} {{ r \choose j}\!\!}^{1/2} x^j,$$ for \(j \in \{0,\dots,r\}\). We also have \(\sup_{ x\in \mathcal{X} } k(x,x) = 1\), which allows for the Shannon relative entropy upper-bound. Let us consider two distributions \(p\) and \(q\) on \([-1,1]\), and understand if letting \(r\) go to infinity leads to a good estimation of the Shannon relative entropy. </p>



<p class="justify-text">For example, we consider the distribution \(p\) with density \(\frac{2}{\pi} \sqrt{1-x^2}\) (this is a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>  affinely rescaled to \([-1,1]\) instead of \([0,1]\)), plotted below, and \(q\) the uniform distribution, with density \(1/2\). </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="290" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/density.png" class="wp-image-6964" height="181" /></figure></div>



<p class="justify-text">All moments are available in closed form (see end of post), and the Shannon relative entropy is equal to \(D(p\|q) = \int_{-1}^1 \! \frac{2}{\pi} \sqrt{1-x^2} \log \frac{4}{\pi} \sqrt{1-x^2} dx \approx 0.0484\). We can then compute for various values of \(r\), \(D(\Sigma_p \| \Sigma_q)\), which is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="289" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/reg_pol_kernel-2.png" class="wp-image-6937" height="269" /></figure></div>



<p class="justify-text">We should hope that the approximation improves as we use more monomials, but this is not the case. As common in kernel methods, the standard polynomial kernel does not have a good behavior. This is partly due to the fact that \(k(x,x) = \big( \frac{1+x^2}{2} \big)^r\) is far from uniform on \([-1,1]\).</p>



<p>Let’s now consider a better polynomial kernel.</p>



<p class="justify-text"><strong>Christofell-Darboux kernels. </strong>Other polynomial bases could be used as well beyond \(1, X, \dots, X^r\), in particular any basis of polynomials with increasing degrees \(P_0, \dots, P_r\) can be used, in particular polynomials that are <a href="https://en.wikipedia.org/wiki/Orthogonal_polynomials">orthogonal</a> with respect to a probability distribution on \([-1,1]\), with $$ \varphi(x)_j = \frac{1}{\sqrt{\alpha_r}} P_j(x) \mbox{ and } k(x,y) = \frac{1}{\alpha_r} \sum_{j=0}^r P_j(x) P_j(y),$$ where \(\alpha_r\) is here to ensure that \(k(x,x) \leqslant 1\) on \([-1,1]\). The kernel function \(k(x,y)\) has then itself a simple expression through the <a href="https://en.wikipedia.org/wiki/Christoffel%E2%80%93Darboux_formula">Christoffel–Darboux formula</a>, as $$ k(x,y) \propto   \frac{P_r(x)P_{r+1}(y)-P_r(y)P_{r+1}(x)}{x-y},$$ with an explicit proportionality constant. For more details on these kernels and their application to data analysis, see [<a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">5</a>].</p>



<p class="justify-text">For example, for <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev polynomials</a>, which are orthonormal with respect to the probability measure with density \(\big(\pi \sqrt{1-x^2}\big)^{-1}\) on \([-1,1]\), we have \(P_0 = 1\), \(P_1 =  \sqrt{2} X\), and \(P_2 = \sqrt{2}( 2X^2 – 1) \), and more generally, for \(k&gt;0\), \(P_k(\cos \theta) = \sqrt{2}\cos k\theta\). One can then check that for \(x = \cos \theta\) and \(y = \cos \eta\), we have $$k(x,y) = \frac{1}{2} \Big( \frac{ \sin ( 2r+1)( \theta-\eta)/2}{\sin ( \theta-\eta)/2} + \frac{ \sin ( 2r+1)( \theta+\eta)/2}{\sin ( \theta+\eta)/2}  \Big).$$ Moreover, we can check that $$ \sup_{x \in \, [-1,1]} k(x,x) = 2r+1, $$ leading to \(\alpha_r = 2r+1\).</p>



<p class="justify-text"> </p>



<p class="justify-text">As shown below, after computing (still in closed form, see end of the post) \(\Sigma_p\) and \(\Sigma_q\), we can plot the relative entropy, and compare it to the true one. This leads to an improved estimation.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="293" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/cheb_pol_kernel-2.png" class="wp-image-6936" height="273" /></figure></div>



<p class="justify-text">This is better, but still not quite there, and we get in fact convergence to exactly one half of the true relative entropy. The motivated reader will check that the limit of \(k(x,x)\) when \(r \) tends to infinity is \(1/2\) for all \(x \in (-1,1)\), and \(1\), for \(x \in \{-1,1\}\) (that is, the upper-bound of \(1\) is valid, but it is almost surely equal to \(1/2\)). This is where we lose the factor of \(1/2\) in Eq. (1) above.</p>



<p class="justify-text">One key overall insight is that the span of the feature space is not the only important aspect in the approximation, and that some rescalings are better than others (more on this below when optimize with respect to the chosen basis). Given that we end up estimating entropies from moments, it would be clearly interesting to compare with other ways of estimating entropies from moments, such as using asymptotic expansions based on Edgeworth expansions [8].</p>



<h2>Fourier series and translation-invariant kernels</h2>



<p class="justify-text">We still consider \(\mathcal{X} = [-1,1]\). For a \(2\)-periodic function \(q: \mathbb{R} \to \mathbb{R}\), such that \(q(0) = 1\), with Fourier series \(\hat{q}: \mathbb{Z} \to \mathbb{C}\), we can consider $$k(x,y) = q(x-y) = \sum_{\omega \in \mathcal{Z}} \hat{q}(\omega) e^{i\pi \omega(x-y)}.$$ We can for example take $$q(x) = \frac{\sin ( 2r+1) \pi x/2}{(2r+1)\sin \pi x/2} $$ which corresponds to \(\hat{q}(\omega) = \frac{1}{2r+1} 1_{|\omega| \leqslant r}\), and having $$ \varphi(x)_\omega = \frac{1}{\sqrt{2r+1}} e^{ i\pi \omega x } $$ for \(\omega \in \{-r-1,\dots,r+1\}\) (we consider complex-valued features for simplicity). We refer to this kernel as the <a href="https://en.wikipedia.org/wiki/Dirichlet_kernel">Dirichlet kernel</a>.</p>



<p class="justify-text"><strong>Link with eigenvalues of Toeplitz matrices.</strong> For the specific choice of the Dirichlet kernel, the element of the covariance matrix indexed by \(\omega\) and \(\omega’\) has value \(\int_{-1}^1 e^{i\pi (\omega-\omega’)} dp(x)\), which is the <a href="https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)">characteristic function</a> of \(p\) evaluated at \(\pi(\omega-\omega’)\). One consequence is that \(\Sigma_p\) is a <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrix</a> with constant values across all diagonals. It is then well-known that as \(m\) goes to infinity, the eigenvalues of \(\Sigma_p\) are tending in distribution to \(p\) (see [<a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">10</a>]), which provides another proof of good estimation of the entropy in this very particular case.</p>



<p class="justify-text">Note that we could also consider an infinite-dimensional kernel \(q(x) =\Big( 1+ \frac{\sin^2 \pi x/2}{\sinh^2 \sigma}\Big)^{-1}\) which corresponds to \( \hat{q}(\omega) \propto e^{-2\sigma |\omega|}\), where the link with Toeplitz matrices does not apply. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for details.</p>



<p class="justify-text"><strong>Increasing number of frequencies. </strong>We can then plot the estimation of the same relative entropy as for polynomial kernels, as the number of frequency grows (we can still get the Fourier moments in closed form, see end of post).</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="305" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/dirichlet_kernel-1.png" class="wp-image-6944" height="283" /></figure></div>



<p class="justify-text">We now see that as the dimension of the feature space grows, we get a perfect estimation of the relative entropy. </p>



<p class="justify-text">This estimation is provably from below, but how far is it? How big \(r\) needs to be? </p>



<h2>Upper-bound trough quantum information theory</h2>



<p class="justify-text">In order to find a lower-bound \(D(\Sigma_p \| \Sigma_q)\), we cannot rely on Jensen’s inequality, as it is in the wrong direction (this happens a lot!). But here, <a href="https://en.wikipedia.org/wiki/Quantum_information">quantum information theory</a> can come to the rescue. </p>



<p class="justify-text">Using the covariance matrix \(\Sigma\) associated to the uniform distribution \(\tau\) on \(\mathcal{X}\), we consider the PSD matrices \(D(y)\), indexed by \(y \in \mathcal{X}\), equal to $$D(y) = \Sigma^{-1/2} \varphi(y) \varphi(y)^\top \Sigma^{-1/2},$$ for which we have $$\int_{\mathcal{X}} D(y) d\tau(y) =\Sigma^{-1/2} \Big( \int_{\mathcal{X}}  \varphi(y) \varphi(y)^\top d\tau(y)\Big)\Sigma^{-1/2} =   I. $$ We can now consider, in quantum information theory terms, that each \(D(y)\) corresponds to a measurement that can be applied to a density operator, and that we look at measures obtained from \(p\), as \(\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p)\), and from \(q\) as \(\tilde{q}(y) = {\rm tr } ( D(y) \Sigma_q)\). This is a proper <a href="https://en.wikipedia.org/wiki/Quantum_operation">quantum operation</a> because the matrices \(D(y)\) are PSD and sum to identity.</p>



<p class="justify-text">The monotonicity of the von Neumann entropy with respect to quantum operations [9] applies, and we immediately get $$D(\Sigma_p \| \Sigma_q) \geqslant \int_{\mathcal{X}} \tilde{p}(y) \log \frac{ \tilde{p}(y)}{\tilde{q}(y)} d\tau(y).$$ The lower-bound can be seen as the Shannon relative entropy \(D( \tilde{p} \| \tilde{q} )\) between the distributions with density \(\tilde{p}\) and \(\tilde{q}\) with respect to \(\tau\).</p>



<p class="justify-text">These densities have direct relationships with \(p\) and \(q\). Indeed,  we have $$\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p) = \int_{\mathcal{X}} \big( \varphi(x)^\top \Sigma^{-1/2} \varphi(y) \big)^2 dp(x).$$ Denoting $$h(x,y) = \big( \varphi(x)^\top \Sigma_\tau^{-1/2} \varphi(y) \big)^2,$$ we have \(\int_{\mathcal{X}} h(x,y) d\tau(x) = \| \varphi(y)\|^2\) for all \(y \in \mathcal{X}\), and thus, when the features have unit norms, we can consider \(h\) as a smoothing kernel, and thus \(\tilde{p}\) and \(\tilde{q}\) as smoothed versions of \(p\) and \(q\). Thus, overall we have: $$D(\tilde{p} \| \tilde{q}) \leqslant D(\Sigma_p \| \Sigma_q)  \leqslant D(p\|q),$$ and we have sandwitched the kernel relative entropy by Shannon relative entropies.</p>



<p class="justify-text">We can then characterize the error made in estimating the relative entropy by measuring how the smoothing kernel \(h(x,y)\) differs from a Dirac. In  [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Section 4.2], I show how, for metric spaces, the difference between \(D(\tilde{p} \| \tilde{q})\) and \(D({p} \|{q}) \), and thus between \(D(\Sigma_p \| \Sigma_q)\) and \(D(\tilde{p} \| \tilde{q})\), is upperbounded by an explicit function of \(p\) and \(q\), times $$ \sup_{x \in \mathcal{X}} \int_{\mathcal{X}} h(x,y) d(x,y)^2 d\tau(y),$$ which is indeed small when \(h(x,y)\) is closed to a Dirac.</p>



<p class="justify-text">For the Dirichlet kernel presented above, we can show that $$h(x,y) = \frac{1}{4r+2} \bigg( \frac{\sin ( 2r+1) \pi (x-y)/2}{\sin \pi (x-y)/2}\bigg)^2,$$ and that the quantity above characterizing convergence scales as \(1 / (2r+1)\), thus providing an explicit convergence rate. As an illustration, we plot the smoothing kernel at \(y=0\), that is, \(h(x,0)\) in regular scale (left) and log-scale (right), for the Dirichlet kernel above (for which we have unit norm features) for various values of \(r\). We see how it converges to a Dirac as \(r\) grows.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="503" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/smoothh_both-2.gif" class="wp-image-6999" height="223" /></figure></div>



<h2>Estimation</h2>



<p class="justify-text">Above, we have relied on finite-dimensional feature maps and explicit expectations under \(p\) and \(q\). This is clearly not scalable to generic situations and potentially infinite-dimensional feature spaces. But we can rely here on 30 years of progress in kernel methods.</p>



<p class="justify-text">For simplicity here, we will only estimate the von Neumann entropy \({\rm tr}\big[ \Sigma_p \log \Sigma_p \big]\) from \(n\) independent and identically distributed samples. The natural estimator is \({\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big]\), where \(\hat{\Sigma}_p\) and \(\hat{\Sigma}_q\) are empirical covariance matrices (the averages of \(\varphi(x_i) \varphi(x_i)^\top\) over all data points \(x_1,\dots,x_n\)).</p>



<p class="justify-text">In order to compute it efficiently, we can use the usual kernel trick, exactly as used within <a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel principal component analysis</a>, and if \(K\) is the \(n \times n\) kernel matrix associated with the \(n\) data points, then the eigenvalues of \(\frac{1}{n} K\) and \(\hat{\Sigma}_p\) are the same, and thus $$  {\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big] =  {\rm tr}\big[ \big(\frac{1}{n}K  \big)\log \big( \frac{1}{n} K \big)  \big].$$ </p>



<p class="justify-text">In terms of statistical analysis, we refer to [1] where it is shown that the difference between the estimate and the true value is of order \(C / \sqrt{n}\), with an explicit constant \(C\), for a wide range of kernels. Surprisingly, there is no need to add any form of regularization.</p>



<p class="justify-text"><strong>Entropy estimation. </strong>Together with the approximation result presented above, by letting \(r\) go to infinity for the Dirichlet kernel, we get an estimator of the regular Shannon entropy which is not that bad (see [<a href="https://arxiv.org/pdf/1711.02141">7</a>] for an optimal one).</p>



<h2>Learning representations</h2>



<p class="justify-text">As mentioned above for polynomials, a key question is how to choose a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\). Given that we have a lower-bound on the relative entropy, it is tempting to try to <em>maximize</em> this lower bound, subject to \(\forall x \in \mathcal{X}\), \(\| \varphi(x)^\top \varphi(y)\|^2 \leqslant 1\). The magic is that this is a tractable optimization problem (concave maximization with convex constraints) in the kernel function \((x,y) \mapsto \varphi(x)^\top \varphi(y)\). This is not an obvious result (in fact, I first tried to prove that the von Neumann relative entropy was convex in the kernel, while it is in fact concave). See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for the concavity proofs and simple optimization algorithms.</p>



<h2>Duality and log-partition functions</h2>



<p class="justify-text">Given that our kernel notions of relative entropies are lower bounds on the regular notions, by the traditional convex duality results between maximum entropy and log-partition functions (see, e.g., [<a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">11</a>]), we should obtain upper-bounds on log-partition functions. This is exactly what we can get, we simply provide the gist here and refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for further details. </p>



<p class="justify-text">Given a probability distribution \(q\) on \(\mathcal{X}\) and a function \(f: \mathcal{X} \to \mathbb{R}\), the log-partition function is defined as $$a(f) = \log \bigg( \int_{\mathcal{X}} e^{f(x)} dq(x)\bigg) \tag{2}.$$ It plays a crucial role in probabilistic and statistical inference, and obtaining upper-bounds is instrumental in deriving approximate inference algorithms [11]. A classical convex duality result shows a link with the Shannon relative entropy, as we can express \(a(f)\) as $$a(f) = \sup_{ p\  {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x) \ – D( p \| q).$$</p>



<p class="justify-text">Assuming that all features have unit norm, we have \(D(p \|q) \geqslant D(\Sigma_p \| \Sigma_q)\), and thus $$ a(f) \leqslant \sup_{ p \ {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  – D(\Sigma_p \| \Sigma_q),$$ which is itself smaller than the supremum over all signed measures with unit mass (thus removing the non-negativity constraint). Therefore, $$ a(f) \leqslant \sup_{ p \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  – D(\Sigma_p \| \Sigma_q) \tag{3},$$ which can be made computationally tractable in a number of cases.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I tried to exhibit a new link between kernels and information theory, which mostly relies on the convexity properties of von Neumann entropy and a bit of quantum analysis. Clearly, the practical relevance of this link remains to be established, but it opens tractable information theory tools to all types of data on which positive kernels can be defined (essentially all of them), with many potential avenues for future work.</p>



<p class="justify-text"><strong>Multivariate modelling.</strong> Regular entropies and relative entropies are used extensively in data science, such as for variational inference, but also to characterize various forms of statistical dependences between random variable. It turns out that the new kernel notions allow for this as well, connecting with a classical line of work within kernel methods [<a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">12</a>, <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">13</a>].</p>



<p class="justify-text"><strong>Other divergences.</strong> I have focused primarily on the link with Shannon relative entropies, but most of the results extend to all <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a>. For example, we can get kernel-based lower bounds on the square <a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a>, or the \(\chi^2\)-divergences.</p>



<p class="justify-text"><strong>Kernel sum-of-squares with a probabilistic twist.</strong> As a final note, there is a link between the new notions of kernel entropies with global optimization with <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">kernel sum-of-squares</a> [<a href="http://arxiv.org/abs/2012.11978">14</a>]. Indeed, in Eq. (2), if we add a temperature parameter \(\varepsilon\) and define $$a_\varepsilon(f) =\varepsilon  \log \bigg( \int_{\mathcal{X}} e^{f(x) / \varepsilon} dq(x) \bigg), $$ then when \(\varepsilon\) tends to zero, \(a_\varepsilon(f) \) tends to \(\sup_{x \in \mathcal{X}} f(x)\). The lower bound from Eq. (3) then becomes $$\sup_{ p \ {\rm measure}}  \int_{\mathcal{X}} f(x) dp(x) \ – \varepsilon D(\Sigma_p \| \Sigma_q),$$ which exactly tends to the sum-of-squares relaxation for the optimization problem. Given that for global optimization, smoothness can be leveraged to circumvent the curse of dimensionality, we can imagine something similar for log-partition functions and entropies.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Francis Bach. <a href="http://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical report, arXiv:2202.08545, 2022. <br />[2] Brian Kulis, Mátyás A. Sustik, Inderjit S. Dhillon. <a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">Low-Rank Kernel Learning with Bregman Matrix Divergences</a>. <em>Journal of Machine Learning Research</em> 10(2):341-376, 2009.<br />[3] Inderjit S. Dhillon, Joel A. Tropp. <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">Matrix nearness problems with Bregman divergences</a>. <em>SIAM Journal on Matrix Analysis and Applications</em> 29(4):1120-1146, 2008.<br />[4] Thomas M. Cover and Joy A. Thomas. <em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br />[5] Edouard Pauwels, Mihai Putinar, Jean-Bernard Lasserre. <a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">Data analysis from empirical moments and the Christoffel function</a>. <em>Foundations of Computational Mathematics</em> 21(1):243-273, 2021.<br />[6] Ronald De Wolf. <a href="https://theoryofcomputing.org/articles/gs001/gs001.pdf">A brief introduction to Fourier analysis on the Boolean cube</a>. <em>Theory of Computin</em>g, 1:1-20, 2008.<br />[7] Yanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. <a href="https://arxiv.org/pdf/1711.02141">Optimal rates of entropy estimation<br />over Lipschitz balls</a>. <em>The Annals of Statistics</em>, 48(6):3228–3250, 2020.<br />[8] Marc M. Van Hulle. <a href="https://direct.mit.edu/neco/article-abstract/17/9/1903/7008/Edgeworth-Approximation-of-Multivariate">Edgeworth approximation of multivariate differential entropy</a>. <em>Neural computation</em>, 17(9):1903-1910, 2005.<br />[9] Mark M. Wilde. <em>Quantum Information Theory</em>. Cambridge University Press, 2013.<br />[10] Robert M. Gray. <a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">Toeplitz and Circulant Matrices: A Review</a>. <em>Foundations and Trends in Communications and Information Theory</em>, 2(3):155-239, 2006.<br />[11] Martin J. Wainwright and Michael I. Jordan. <a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">Graphical Models, Exponential Families, and Variational<br />Inference</a>. <em>Foundations and Trends in Machine Learning,</em> 1(1–2), 1-305, 2008.<br />[12] Francis Bach and Michael I. Jordan. <a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">Kernel independent component analysis</a>. Journal of Machine<br />Learning Research, 3(Jul):1–48, 2002<br />[13] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R. G.<br />Lanckriet. <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">Hilbert space embeddings and metrics on probability measures</a>. <em>Journal of Machine Learning Research</em>, 11:1517–1561, 2010.<br />[14] Alessandro Rudi, Ulysse Marteau-Ferey, and Francis Bach. <a href="https://arxiv.org/abs/2012.11978">Finding global minima via kernel approximations</a>. Technical Report 2012.11978, arXiv, 2020</p>



<h2>Integrals in closed form</h2>



<p class="justify-text">In order to compute in closed form \(\Sigma_p\) for the distribution \(p\) with density on \([-1,1]\) equal to \( \frac{2}{\pi} \sqrt{1-x^2}\), we need to compute some integrals</p>



<ul class="justify-text"><li><strong>Polynomial kernel</strong>: we have $$\int_{-1}^1   \frac{2}{\pi} \sqrt{1-x^2}x^{2k} dx = 2^{1-2k} \frac{ ( 2k-1)!}{(k-1)! (k+1)!}.$$</li><li><strong>Chebyshev kernel</strong>: we need to compute the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos^2 k\theta \, d\theta,$$ which is equal to \(1\) for \(k=0\), to \(1/4\) for \(k=1\), and to \(1/2\) for all larger integers. We also need the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos k\theta \cos(k+2)\theta \, d\theta,$$ which is equal to \(-1/2\) for \(k=0\), and to \(-1/4\) for all larger integers. </li><li><strong>Dirichlet kernel</strong>: we have $$ \int_{-1}^1 \frac{2}{\pi} \sqrt{1-x^2} \cos k \pi x dx = \frac{2}{k \pi} J_1(k\pi),$$ where \(J_1\) is the<a href="https://en.wikipedia.org/wiki/Bessel_function#Bessel_functions_of_the_first_kind"> Bessel function of the first kind</a>.</li></ul>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/information-theory-with-kernel-methods/"><span class="datestr">at April 04, 2022 04:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/044">TR22-044 |  An Optimal Algorithm for Certifying Monotone Functions | 

	Meghal Gupta, 

	Naren Manoj</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given query access to a monotone function $f\colon\{0,1\}^n\to\{0,1\}$ with certificate complexity $C(f)$ and an input $x^{\star}$, we design an algorithm that outputs a size-$C(f)$ subset of $x^{\star}$ certifying the value of $f(x^{\star})$. Our algorithm makes $O(C(f) \cdot \log n)$ queries to $f$, which matches the information-theoretic lower bound for this problem and resolves the concrete open question posed in the STOC '22 paper of Blanc, Koch, Lange, and Tan [BKLT22].

We extend this result to an algorithm that finds a size-$2C(f)$ certificate for a real-valued monotone function with $O(C(f) \cdot \log n)$ queries. We also complement our algorithms with a hardness result, in which we show that finding the shortest possible certificate in $x^{\star}$ may require $\Omega\left(\binom{n}{C(f)}\right)$ queries in the worst case.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/044"><span class="datestr">at April 04, 2022 04:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html">Dissection into rectangles and tensor rank</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>It’s easy to cut a Greek cross into pieces that can be reassembled into a rectangle:</p>

<p style="text-align: center;"><img width="60%" alt="Dissection of a Grek cross to a 4x5 rectangle" src="https://11011110.github.io/blog/assets/2022/cross2rect.svg" /></p>

<p>Here’s another example. The three yellow rectangles below have dimensions <span style="white-space: nowrap;">\(2^{2/3}\times 1\),</span> <span style="white-space: nowrap;">\(2^{1/3}\times 2^{1/3}\),</span> <span style="white-space: nowrap;">and \(1\times 2^{2/3}\).</span> (The order of multiplication matters here!) Put them together, and you get the blue stealth aircraft shape. Can you cut this shape into pieces and reassemble them into a single rectangle?</p>

<p style="text-align: center;"><img width="80%" alt="Union of three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect.svg" /></p>

<p>The answer is yes; any polygon can be cut and reassembled into a rectangle, or into any other polygon of the same area. This is the <a href="https://en.wikipedia.org/wiki/Wallace%E2%80%93Bolyai%E2%80%93Gerwien_theorem">Wallace–Bolyai–Gerwien theorem</a>. But the Greek cross dissection above uses only axis-parallel cuts and translation of the pieces. To get a single rectangle from the stealth shape, you’re going to need a more general class of operations that cut it at odd angles and rotate the pieces. It’s impossible to make a rectangle from this shape using only axis-parallel cuts and translation. And even for the Greek cross, although you can make a rectangle as shown, it’s impossible to make a square using only axis-parallel cuts and translation.</p>

<p>To see why, we need to understand the <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a>. As usually defined, this is a value that can be determined for any three-dimensional polyhedron. It’s called an invariant because it stays unchanged when you cut up the polyhedron into polyhedral parts and reassemble the pieces into something else, allowing odd-angled cuts and rotations. In order for one polyhedron to have a dissection into another, they must both have the same volume and the same Dehn invariant. This can be used to show, for instance, that you cannot dissect a regular tetrahedron into a cube. But as several authors have described, a version of the same method also applies to axis-parallel dissections of orthogonal <span style="white-space: nowrap;">polygons.<sup id="fnref:s"><a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fn:s" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:b"><a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fn:b" class="footnote" rel="footnote">2</a></sup></span></p>

<p>To evaluate the Dehn invariant of an orthogonal polygon, the first step is to represent the polygon in Cartesian coordinates, and to find a <em>rational basis</em> for its coordinates, a system of numbers such that every coordinate has a unique representation as a linear combination of basis elements with rational-number coefficients. This is just a basis, in the usual meaning of the word from linear algebra, when we interpret the real numbers as being a vector space over the rational numbers. In the example of the stealth shape, we can use the same basis for both the <span style="white-space: nowrap;">\(x\)- and</span> <span style="white-space: nowrap;">\(y\)-coordinates,</span> the set of numbers <span style="white-space: nowrap;">\(\{1,2^{1/3},2^{2/3}\}\).</span> The choice of basis is arbitrary (as long as it can represent all coordinates, without any redundancy) and will affect the rest of the calculation but not the conclusions we draw from it. If we’re comparing two shapes, we can use a common basis that can represent the coordinates from both; it won’t hurt if some of the basis elements are only used for one of the two shapes. So for instance, if the Greek cross is made out of unit squares, the square we’d like to dissect it into has side length \(\sqrt5\) and we can use the basis \(\{1,\sqrt 5\}\) to compare the cross to the square.</p>

<p>Next, we decompose the given shape into rectangles through its vertices (like the yellow rectangles that we’ve already used to decompose the stealth shape). Express the width and height of each rectangle as a rational combination of basis elements; in our example, each rectangle is just a product of two basis elements, but sometimes more complex combinations might be required. If we multiply the expression for the width of a rectangle by the expression for the height of a rectangle, we get an expression for the area of the whole rectangle as a sum of terms \(a\cdot b_x\cdot b_y\) where \(a\) is a rational number and \(b_x\) and \(b_y\) are <span style="white-space: nowrap;">\(x\)-basis</span> and <span style="white-space: nowrap;">\(y\)-basis</span> elements, respectively. Adding these expressions for every rectangle in the decomposition gives a similar expression for the area of the whole shape. Since we’re separating out the terms for each product of basis elements, it’s convenient to write it out as a matrix. For the Greek cross and same-area square, with the basis \(\{1,\sqrt 5\}\) for both <span style="white-space: nowrap;">\(x\) and \(y\),</span> we get the matrices</p>

\[\begin{pmatrix} 5 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \qquad\text{and}\qquad
\begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix},\]

<p>respectively. For the stealth shape, with the basis \(\{1,2^{1/3},2^{2/3}\}\) for both <span style="white-space: nowrap;">\(x\) and \(y\),</span> we get the matrix</p>

\[\begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \end{pmatrix}.\]

<p>That’s the Dehn invariant! Because the Greek cross and square have different invariants, they cannot be dissected into each other (for this restricted axis-parallel translation-only form of dissection). But what about the dissection of the stealth shape into a single rectangle? How can we apply this analysis when there are infinitely many possible rectangles to try?</p>

<p>Here it’s convenient to move from the notion of bases and matrices to a more <a href="https://en.wikipedia.org/wiki/Coordinate-free">coordinate-free</a> formulation of the same thing using <a href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)">abstract tensors</a>. When we’re expressing the area of an \(x\times y\) rectangle as a product of rational linear combinations of basis elements, what we’re really doing is constructing the element \(x\otimes y\) of the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a> <span style="white-space: nowrap;">\(\mathbb{R}\otimes_{\mathbb{Q}}\mathbb{R}\).</span> Then when we add the expressions arising from the decomposition into rectangles, to form a matrix describing the whole shape, this is just addition of these elements within the tensor product. The subscript \(\mathbb{Q}\) in the tensor notation corresponds to the fact that we’re interpreting real numbers as forming a vector space over the field \(\mathbb{Q}\) of rational numbers. (Some sources use a subscript \(\mathbb{Z}\) but this is not an important difference here.)</p>

<p>One thing we know about matrices is that they have a <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank</a>, the dimension of the vector space generated by their rows or columns. But tensors also have a notion of rank. The rank of a tensor is the minimum number of terms needed to express it as a sum of products</p>

\[x_1\otimes y_1+x_2\otimes y_2+\cdots,\]

<p>where the \(x_i\) and \(y_i\) can be any elements of the spaces that are being tensored together (they are not required to be basis elements). For tensor products of three or more spaces, the rank is still defined in the same way, but it can get very tricky to calculate. This is a big part about why the time bounds for fast matrix multiplication are still so mysterious: they involve tensors of order three. But for the order-two tensor space \(\mathbb{R}\otimes_{\mathbb{Q}}\mathbb{R}\), we’re saved by linear algebra: the rank of a tensor of order two is just the rank of a matrix representing it, for any basis over the appropriate field <span style="white-space: nowrap;">(here \(\mathbb{Q}\)).</span></p>

<p>Suppose that we could dissect a <span style="white-space: nowrap;">polygon \(P\),</span> using axis-parallel cuts and translations, into a collection of \(k\) rectangles. Then the sum of the tensor expressions for those rectangles would be an expression of the Dehn invariant <span style="white-space: nowrap;">of \(P\),</span> as a sum of \(k\) products. Therefore, the rank of the Dehn invariant of \(P\) would have to be at <span style="white-space: nowrap;">most \(k\).</span> But for our stealth shape, we know the rank: we have expressed its Dehn invariant as a matrix of rank three. Therefore, to dissect it into rectangles, we need at least three rectangles.</p>

<p>Another use of the polyhedral Dehn invariant, besides dissection of one shape into another, involves tiling. Any polyhedron that tiles space must have Dehn invariant zero, and any polyhedron with Dehn invariant zero can be dissected into a different polyhedron that tiles space. For the axis-parallel polygonal Dehn invariant we’re looking at here, things don’t work out quite so neatly. The Greek cross can tile, but has nonzero Dehn invariant. More, any axis-parallel polygon can be cut into multiple rectangles, and these can tile space aperiodically by grouping them into rows of the same type of rectangle.</p>

<p style="text-align: center;"><img width="65%" alt="Aperiodic tiling by three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect-row-tiling.svg" /></p>

<p>So the Dehn invariant cannot be used to prove that such a thing is impossible, because it is always possible. If we could rotate pieces, we could also form these three rectangles into a single-piece axis-parallel hexagon that can tile the plane:</p>

<p style="text-align: center;"><img width="80%" alt="Periodic tiling by a hexagon formed from three irrational rectangles" src="https://11011110.github.io/blog/assets/2022/trirect-flip-tiling.svg" /></p>

<p>But this is not possible without rotation. Every periodic tiling of the plane has a <a href="https://en.wikipedia.org/wiki/Fundamental_domain">fundamental region</a> in the shape of an axis-parallel hexagon, like the hexagons in this periodic tiling.  If one or more copies of a polygon could be cut up by axis-parallel cuts and reassembled by translations to form a single polygon that can tile the plane periodically, it would also be possible to dissect copies of the polygon (possibly a larger number of copies of it) into an axis-parallel hexagon. But these hexagons have a Dehn invariant with rank at most two (they can be cut into two rectangles), while any number of copies of our stealth shape combine to have rank three (making more copies just multiplies the matrix by a scalar). Because the Dehn invariants have different ranks, no dissection is possible.</p>

<p>This same rank analysis can also be applied to the more familiar polyhedral form of the Dehn invariant. For that form, a polyhedron whose Dehn invariant has high rank cannot be dissected into another polyhedron with fewer edges than the rank.</p>

<div class="footnotes">
  <ol>
    <li id="fn:s">
      <p>John Stillwell (1998), <em><a href="https://doi.org/10.1007/978-1-4612-0687-3">Numbers and Geometry</a></em>, Springer, p. 164. <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fnref:s" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:b">
      <p>David Benko (2007), “<a href="https://www.jstor.org/stable/27642302">A new approach to Hilbert’s third problem</a>”, <em>Amer. Math. Monthly</em> 114 (8): 665–676. <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html#fnref:b" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/108071296947021436">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html"><span class="datestr">at April 03, 2022 06:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=22548">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2022/04/02/amazing-jinyoung-park-and-huy-tuan-pham-settled-the-expectation-threshold-conjecture/">Amazing: Jinyoung Park and Huy Tuan Pham settled the expectation threshold conjecture!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p></p>


<p><img width="1024" alt="kahn-kalai" src="https://gilkalai.files.wordpress.com/2022/04/kahn-kalai.webp" class="alignnone size-full wp-image-22549" height="762" /></p>
<p>A brief summary: <a href="https://arxiv.org/abs/2203.17207">In the paper, A proof of the Kahn-Kalai conjecture,</a> <a href="https://sites.google.com/view/jinyoungpark">Jinyoung Park</a> and <a href="https://web.stanford.edu/~huypham/">Huy Tuan Pham</a> proved the 2006 expectation threshold conjecture posed by Jeff Kahn and me. The proof is wonderful. Congratulations Jinyoung and Huy Tuan!</p>
<p>The 2006 <a href="https://arxiv.org/abs/math/0603218">expectation threshold conjecture</a> gives a justification for a naive way to estimate the threshold probability of a random graph property. Suppose that you are asked about the critical probability for a random graph in G(n,p) for having a perfect matching (or a Hamiltonian cycle). You compute the expected number of perfect matchings and realize that when p is C/n this expected number equals 1/2. (For Hamiltonian cycles it will be C’/n.) Of course, if the expectation is one half, the probability for a perfect matching can still be very low; indeed, in this case, an isolated vertex is quite likely but when there is no isolated vertices the expected number of perfect matchings is rather large. Our 2006 conjecture boldly asserts that the gap between the value given by such a naive computation and the true threshold value is at most logarithmic in the number of vertices. Jeff and I tried hard to find a counterexample but instead we managed to find more general and stronger forms of the conjecture that we could not disprove.</p>
<p>Two years ago Keith Frankston, Jeff Kahn, Bhargav Narayanan, and Jinyoung Park proved a weak form of the conjecture which was proposed in a 2010 paper by Michel Talagrand. (See <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">this post</a>.) Indeed, the expectation threshold conjecture had some connections with a 1995 paper of Michel Talagrand entitled <a href="https://link.springer.com/chapter/10.1007%2F978-3-0348-9090-8_25">Are all sets of positive measure essentially convex?</a> In a 2010 STOC paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Are Many Small Sets Explicitly Small?</a> Michel formulated a whole array of interesting conjectures and commented that he feels that these conjectures are related to the expectation threshold conjecture to which he offered a weaker fractional version. This weak version suffices for various applications of the original conjecture. Keith, Jeff, Bhargav, and Jinyoung’s work built on the breakthrough work of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture. </p>
<p>Proving the full expectation threshold conjecture looked like a  difficult task. The only path that people saw was to try to relate Talagrand’s fractional expectation threshold with our expectation threshold. (Indeed Talagrand also conjectured that they only differ by a multiplicative constant factor. This looked if true very difficult to prove.)  However this is not the path taken by Jinyoung Park and Huy Tuan Pham and they found a direct simple argument! Jinyoung and Huy Tuan also used their method to settle one of the central conjectures (Conj 5.7) from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Talagrand’s paper</a> and this will be presented in a forthcoming paper.</p>
<p>The <em>logn</em> gap in our conjecture looked rather narrow but now that it was proved we can ask for conditions that will guarantee a smaller gap. For example, when is the gap only a constant?</p>
<p>Here is a nice IAS video on Jinyoung Park’s path to math.</p>
<p></p>
<p>Here is a lecture by Michel Talagrand.</p>
<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2022/04/02/amazing-jinyoung-park-and-huy-tuan-pham-settled-the-expectation-threshold-conjecture/"><span class="datestr">at April 02, 2022 04:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
