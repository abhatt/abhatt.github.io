<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.minimizingregret.com/" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A> &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at January 15, 2019 04:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://bit-player.org/?p=2108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hayes.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://bit-player.org/2019/glaubers-dynamics">Glauber’s dynamics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://bit-player.org" title="bit-player">bit-player</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Roy J. Glauber, Harvard physics professor for 65 years, longtime Keeper of the Broom at the annual <a href="https://en.wikipedia.org/wiki/Ig_Nobel_Prize">Ig Nobel</a> ceremony, and winner of a non-Ig Nobel, has died at age 93. Glauber is known for his work in quantum optics; roughly speaking, he developed a mathematical theory of the laser at about the same time that device was invented, circa 1960. His two main papers on the subject, published in <em>Physical Review</em> in 1963, did not meet with instant acclaim; the Nobel committee’s recognition of their worth came more than 40 years later, in 2005. A third paper from 1963, titled “<a href="https://aip.scitation.org/doi/10.1063/1.1703954">Time-dependent statistics of the Ising model</a>,” also had a delayed impact. It is the basis of a modeling algorithm now called Glauber dynamics, which is well known in the cloistered community of statistical mechanics but deserves wider recognition.</p>
<p>Before digging into the dynamics, however, let us pause for a few words about the man himself, drawn largely from the obituaries in the <em><a href="https://www.nytimes.com/2019/01/08/obituaries/roy-j-glauber-dead.html">New York Times</a></em> and the <a href="https://www.thecrimson.com/article/2019/1/3/roy-glauber-harvard-professor-obituary/"><em>Harvard Crimson</em></a>.</p>
<p>Glauber was a member of the first class to graduate from the Bronx High School of Science, in 1941. From there he went to Harvard, but left in his sophomore year, at age 18, to work in the theory division at Los Alamos, where he helped calculate the critical mass of fissile material needed for a bomb. After the war he finished his degree at Harvard and went on to complete a PhD under Julian Schwinger. After a few brief adventures in Princeton and Pasadena, he was back at Harvard in 1952 and never left. A poignant aspect of his life is mentioned briefly in a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3149897/">2009 interview</a>, where Glauber discusses the challenge of sustaining an academic career while raising two children as a single parent.</p>
<hr />
<p>Here’s a glimpse of Glauber dynamics in action. Click the <em>Go</em> button, then try fiddling with the slider.</p>
<div style="width: 600px; margin: 15px auto;" id="glauber-viz">
	<canvas width="600px" id="glauber-canvas" height="600px"></canvas><p></p>
<div style="display: flex; margin: auto;" id="glauber-controls">
		<output style="font-family: sans-serif; font-style: bold; margin: auto 10px;" class="slider-output" for="glauber-slider" id="glauber-temp">3.00</output><br />
		<input style="width: 300px; margin: auto 10px;" min="0.01" max="5.0" value="3.00" step="0.01" type="range" id="glauber-slider" /><br />
		<button style="width: 50px; margin: 10px;" id="glauber-go-button">Go</button>
	</div>
</div>
<p class="indent">In the computer program that drives this animation, the slider controls a variable representing temperature. At high temperature (slide the control all the way to the right), you’ll see a roiling, seething mass of colored squares, switching rapidly and randomly between light and dark shades. There are no large-scale or long-lived structures. Occasionally the end point is not a monochromatic field. Instead the panel is divided into broad stripes—horizontal, vertical, or diagonal. This is an artifact of the finite size of the lattice and the use of wraparound boundary conditions. On an infinite lattice, the stripes would not occur.At low temperature (slide to the left), the tableau congeals into a few writhing blobs of contrasting color. Then the minority blobs are likely to evaporate, and you’ll be left with an unchanging, monochromatic panel. Between these extremes there’s some interesting behavior. Adjust the slider to a temperature near 2.27 and you can expect to see persistent fluctuations at all possible scales, from isolated individual blocks to patterns that span the entire array.</p>
<p>What we’re looking at here is a simulation of a model of a ferromagnet—the kind of magnet that sticks to the refrigerator. The model was introduced almost 100 years ago by Wilhelm Lenz and his student Ernst Ising. They were trying to understand the thermal behavior of ferromagnetic materials such as iron. If you heat a block of magnetized iron above a certain temperature, called the Curie point, it loses all traces of magnetization. Slow cooling below the Curie point allows it to spontaneously magnetize again, perhaps with the poles in a different orientation. The onset of ferromagnetism at the Curie point is an abrupt phase transition.</p>
<p>Lenz and Ising created a <a href="http://bit-player.org/bph-publications/AmSci-2000-09-Hayes-Ising.pdf">stripped-down model of a ferromagnet</a>. In the two-dimensional version shown here, each of the small squares represents the spin vector of an unpaired electron in an iron atom. The vector can point in either of two directions, conventionally called <em>up</em> and <em>down</em>, which for graphic convenience are represented by two contrasting colors. There are \(100 \times 100 = 10{,}000\) spins in the array. This would be a minute sample of a real ferromagnet. On the other hand, the system has \(2^{10{,}000}\) possible states—quite an enormous number.</p>
<p>The essence of ferromagnetism is that adjacent spins “prefer” to point in the same direction. To put that  more formally: The energy of neighboring spins is lower when they are parallel, rather than antiparallel. <img src="http://bit-player.org/wp-content/uploads/2019/01/Ising-spin-pairs.png" style="margin-bottom: 0px;" height="111" width="262" alt="four possible orientations of two spins (up up, up down, down up, down down), with their corresponding energies" border="0" class="alignright" />For the array as a whole, the energy is minimized if all the spins point the same way, either up or down. Each spin contributes a tiny magnetic moment. When the spins are parallel, all the moments add up and the system is fully magnetized.</p>
<p>If energy were the only consideration, the Ising model would always settle into a magnetized configuration, but there is a countervailing influence: Heat tends to randomize the spin directions. At infinite temperature, thermal fluctuations completely overwhelm the spins’ tendency to align, and all states are equally likely. Because the vast majority of those \(2^{10{,}000}\) configurations have nearly equal numbers of <em>up</em> and <em>down</em> spins, the magnetization is negligible. At zero temperature, nothing prevents the system from condensing into the fully magnetized state. The interval between these limits is a battleground where energy and entropy contend for supremacy. Clearly, there must be a transition of some kind. For Lenz and Ising in the 1920s, the crucial question was whether the transition comes at a sharply defined critical temperature, as it does in real ferromagnets. A more gradual progression from one regime to the other would signal the model’s failure to capture important aspects of ferromagnet physics.</p>
<p>In his doctoral dissertation Ising investigated the one-dimensional version of the model—a chain or ring of spins, each one holding hands with its two nearest neighbors. The result was a disappointment: He found no abrupt phase transition. And he speculated that the negative result would also hold in higher dimensions. The Ising model seemed to be dead on arrival.</p>
<p>It was revived a decade later by Rudolf Peierls, who gave suggestive evidence for a sharp transition in the two-dimensional lattice. Then in 1944 Lars Onsager “solved” the two-dimensional model, showing that the phase transition does exist. The phase diagram looks like this:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2019/01/magnetization-v-temperature.png" height="566" width="640" alt="Graph of the gagnetization v temperature phase diagram for the Ising model." border="0" class="centered" /></p>
<p>As the system cools, the salt-and-pepper chaos of infinite temperature evolves into a structure with larger blobs of color, but the <em>up</em> and <em>down</em> spins remain balanced on average (implying zero magnetization) down to the critical temperature \(T_C\). At that point there is a sudden bifurcation, and the system will follow one branch or the other to full magnetization at zero temperature.</p>
<hr />
<p>If a model is classified as <em>solved</em>, is there anything more to say about it? In this case, I believe the answer is yes. The solution to the two-dimensional Ising model gives us a prescription for calculating the probability of seeing any given configuration at any given temperature. That’s a major accomplishment, and yet it leaves much of the model’s behavior unspecified. The solution defines the probability distribution at equilibrium—after the system has had time to settle into a statistically stable configuration. It doesn’t tell us anything about how the lattice of spins reaches that equilibrium when it starts from an arbitrary initial state, or how the system evolves when the temperature changes rapidly.</p>
<p>It’s not just the solution to the model that has a few vague spots. When you look at the finer details of how spins interact, the model itself leaves much to the imagination. When a spin reacts to the influence of its nearest neighbors, and those neighbors are also reacting to one another, does everything happen all at once? Suppose two antiparallel spins both decide to flip at the same time; they will be left in a configuration that is still antiparallel. It’s hard to see how they’ll escape repeating the same dance over and over, like people who meet head-on in a corridor and keep making mirror-image evasive maneuvers. This kind of standoff can be avoided if the spins act sequentially rather than simultaneously. But if they take turns, how do they decide who goes first?</p>
<p>Within the intellectual traditions of physics and mathematics, these questions can be dismissed as foolish or misguided. After all, when we look at the procession of the planets orbiting the sun, or at the colliding molecules in a gas, we don’t ask who takes the first step; the bodies are all in continuous and simultaneous motion. Newton gave us a tool, calculus, for understanding such situations. If you make the steps small enough, you don’t have to worry so much about the sequence of marching orders.</p>
<p>However, if you want to write a computer program simulating a ferromagnet (or simulating planetary motions, for that matter), questions of sequence and synchrony cannot be swept aside. With conventional computer hardware, “let everything happen at once” is not an option. The program must consider each spin, one at a time, survey the surrounding neighborhood, apply an update rule that’s based on both the state of the neighbors and the temperature, and then decide whether or not to flip. Thus the program must choose a sequence in which to visit the lattice sites, as well as a sequence in which to visit the neighbors of each site, and those choices can make a difference in the outcome of the simulation. So can other details of implementation. Do we look at all the sites, calculate their new spin states, and then update all those that need to be flipped? Or do we update each spin as we go along, so that spins later in the sequence will see an array already modified by earlier actions? The original definition of the Ising model is silent on such matters, but the programmer must make a commitment one way or another.</p>
<p>This is where Glauber dynamics enters the story. Glauber presented a version of the Ising model that’s somewhat more explicit about how spins interact with one another and with the “heat bath” that represents the influence of temperature. It’s a theory of Ising <em>dynamics</em> because he describes the spin system not just at equilibrium but also during transitional stages. I don’t know if Glauber was the first to offer an account of Ising dynamics, but the notion was certainly not commonplace in 1963.</p>
<hr />
<p>There’s no evidence Glauber was  thinking of his method as an algorithm suitable for computer implementation. The subject of simulation doesn’t come up in his 1963 paper, where his primary aim is to find analytic expressions for the distribution of <em>up</em> and <em>down</em> spins as a function of time. (He did this only for the one-dimensional model.) Nevertheless, Glauber dynamics offers an elegant approach to programming an interactive version of the Ising model. Assume we have a lattice of \(N\) spins. Each spin \(\sigma\) is indexed by its coordinates \(x, y\) and takes on one of the two values \(+1\) and \(-1\). Thus flipping a spin is a matter of multiplying \(\sigma\) by \(-1\). The algorithm for a updating the lattice looks like this:</p>
<blockquote><p class="undent">Repeat \(N\) times:</p>
<ol style="margin-bottom: 0px;">
<li>Choose a spin \(\sigma_{x, y}\) at random.</li>
<li>Sum the values of the four neighboring spins, \(S = \sigma_{x+1, y} + \sigma_{x-1, y} + \sigma_{x, y+1} + \sigma_{x, y-1}\). The possible values of \(S\) are \(\{-4, -2, 0, +2, +4\}\).</li>
<li>Calculate \(\Delta E = 2 \, \sigma_{x, y} \, S\), the change in interaction energy if \(\sigma_{x, y}\) were to flip.</li>
<li>If \(\Delta E \lt 0\), set \(\sigma_{x, y} = -\sigma_{x, y}\).</li>
<li>Otherwise, set \(\sigma_{x, y} = -\sigma_{x, y}\) with probability \(\exp(-\Delta E/T)\), where \(T\) is the temperature.</li>
</ol>
<p class="undent">Display the updated lattice.</p>
</blockquote>
<p>Step 4 says: If flipping a spin will reduce the overall energy of the system, flip it. Step 5 says: Even if flipping a spin raises the energy, go ahead and flip it in a randomly selected fraction of the cases. The probability of such spin flips is the Boltzmann factor \(\exp(-\Delta E/T)\). This quantity goes to \(0\) as the temperature \(T\) falls to \(0\), so that energetically unfavorable flips are unlikely in a cold lattice. The probability approaches \(1\) as \(T\) goes to infinity, which is why the model is such a seething mass of fluctuations at high temperature.</p>
<p>(If you’d like to take a look at real code rather than pseudocode—namely the JavaScript program running the simulation above—it’s <a href="https://github.com/bit-player/glauber">on GitHub</a>.)</p>
<p>Glauber dynamics belongs to a family of methods called Markov chain Monte Carlo algorithms (MCMC). The <a href="http://bit-player.org/bph-publications/AmSci-2013-03-Hayes-Markov.pdf">idea of Markov chains</a> was an innovation in probability theory in the early years of the 20th century, extending classical probability to situations where the the next event depends on the current state of the system. <a href="http://bit-player.org/bph-publications/AmSci-1993-03-Hayes-random-numbers.pdf">Monte Carlo algorithms</a> emerged at post-war Los Alamos, not long after Glauber left there to resume his undergraduate curriculum. He clearly kept up with the work of Stanislaw Ulam and other former colleagues in the Manhattan Project.</p>
<p>Within the MCMC family, the distinctive feature of Glauber dynamics is choosing spins at random. The obvious alternative is to march methodically through the lattice by columns and rows, examining every spin in turn. <img src="http://bit-player.org/wp-content/uploads/2019/01/blinking-checkerboard.png" style="margin-bottom: 0px;" height="" width="210" alt="Blinking checkerboard" border="0" class="alignleft" />That procedure can certainly be made to work, but it requires care in implementation. At low temperature the Ising process is very nearly deterministic, since unfavorable flips are extremely rare. When you combine a deterministic flip rule with a deterministic path through the lattice, it’s easy to get trapped in recurrent patterns. For example, a subtle bug yields the same configuration of spins on every step, shifted left by a single lattice site, so that the pattern seems to slide across the screen. Another spectacular failure gives rise to a blinking checkerboard, where every spin is surrounded by four opposite spins and flips on every time step. Avoiding these errors requires much fussy attention to algorithmic details. (My personal experience is that the first attempt is never right.)</p>
<p>Choosing spins by throwing random darts at the lattice turns out to be less susceptible to clumsy mistakes. Yet, at first glance, the random procedure seems to have hazards of its own. In particular, choosing 10,000 spins at random from a lattice of 10,000 sites does <em>not</em> guarantee that every site will be visited once. On the contrary, a few sites will be sampled six or seven times, and you can expect that 3,679 sites (that’s \(1/e \times 10{,}000)\) will not be visited at all. Doesn’t that bias distort the outcome of the simulation? No, it doesn’t. After many iterations, all the sites will get equal attention.</p>
<p>The nasty bit in all Ising simulation algorithms is updating pairs of adjacent sites, where each spin is the neighbor of the other. Which one goes first, or do you try to handle them simultaneously? The column-and-row ordering maximizes exposure to this problem: Every spin is a member of such a pair. Other sequential algorithms—for example, visiting all the black squares of a checkerboard followed by all the white squares—avoid these confrontations altogether, never considering two adjacent spins in succession. Glauber dynamics is the Goldilocks solution. Pairs of adjacent spins do turn up as successive elements in the random sequence, but they are rare events. Decisions about how to handle them have no discernible influence on the outcome.</p>
<hr />
<p>Years ago, I had several opportunities to meet Roy Glauber. Regrettably, I failed to take advantage of them. Glauber’s office at Harvard was in the Lyman Laboratory of Physics, a small isthmus building connecting two larger halls. In the 1970s I was a frequent visitor there, pestering people to write articles for <em>Scientific American</em>. It was fertile territory; for a few years, the magazine found more authors per square meter in Lyman Lab than anywhere else in the world. But I never knocked on Glauber’s door. Perhaps it’s just as well. I was not yet equipped to appreciate what he had to say.</p>
<p>Now I can let him have the last word. This is from the introduction to the paper that introduced Glauber dynamics:</p>
<blockquote><p>If the mathematical problems of equilibrium statistical mechanics are great, they are at least relatively well-defined. The situation is quite otherwise in dealing with systems which undergo large-scale changes with time. The principles of nonequilibrium statistical mechanics remain in largest measure unformulated. While this lack persists, it may be useful to have in hand whatever precise statements can be made about the time-dependent hehavior of statistical systems, however simple they may be.</p></blockquote>
<p></p></div>







<p class="date">
by Brian Hayes <a href="http://bit-player.org/2019/glaubers-dynamics"><span class="datestr">at January 15, 2019 12:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/15/faculty-positions-at-all-levels-at-tu-eindhoven-apply-by-march-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/15/faculty-positions-at-all-levels-at-tu-eindhoven-apply-by-march-1-2019/">faculty positions (at all levels) at TU Eindhoven (apply by March 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Mathematics and Computer Science at TU Eindhoven is looking for new faculty members to expand its academic staff. We welcome applications in all areas of computer science and at all levels, ranging from (tenure-track) assistant professor to full professor.</p>
<p>Website: <a href="https://jobs.tue.nl/en/vacancy/faculty-members-computer-science-assistant-associate-and-full-professor-level-418993.html">https://jobs.tue.nl/en/vacancy/faculty-members-computer-science-assistant-associate-and-full-professor-level-418993.html</a><br />
Email: M.T.d.Berg@tue.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/15/faculty-positions-at-all-levels-at-tu-eindhoven-apply-by-march-1-2019/"><span class="datestr">at January 15, 2019 08:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04377">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04377">Lower bounds for multilinear bounded order ABPs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramya:C=.html">C. Ramya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:B=_V=_Raghavendra.html">B. V. Raghavendra Rao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04377">PDF</a><br /><b>Abstract: </b>Proving super-polynomial size lower bounds for syntactic multilinear
Algebraic Branching Programs(smABPs) computing an explicit polynomial is a
challenging problem in Algebraic Complexity Theory. The order in which
variables in $\{x_1,\ldots,x_n\}$ appear along source to sink paths in any
smABP can be viewed as a permutation in $S_n$. In this article, we consider the
following special classes of smABPs where the order of occurrence of variables
along a source to sink path is restricted:
</p>
<p>Strict circular-interval ABPs: For every subprogram the index set of
variables occurring in it is contained in some circular interval of
$\{1,\ldots,n\}$.
</p>
<p>L-ordered ABPs: There is a set of L permutations of variables such that every
source to sink path in the ABP reads variables in one of the L orders.
</p>
<p>We prove exponential lower bound for the size of a strict circular-interval
ABP computing an explicit n-variate multilinear polynomial in VP. For the same
polynomial, we show that any sum of L-ordered ABPs of small size will require
exponential ($2^{n^{\Omega(1)}}$) many summands, when $L \leq
2^{n^{1/2-\epsilon}}, \epsilon&gt;0$. At the heart of above lower bound arguments
is a new decomposition theorem for smABPs: We show that any polynomial
computable by an smABP of size S can be written as a sum of O(S) many
multilinear polynomials where each summand is a product of two polynomials in
at most 2n/3 variables computable by smABPs. As a corollary, we obtain a low
bottom fan-in version of the depth reduction by Tavenas [MFCS 2013] in the case
of smABPs. In particular, we show that a polynomial having size S smABPs can be
expressed as a sum of products of multilinear polynomials on $O(\sqrt{n})$
variables, where the total number of summands is bounded by $2^{O(\sqrt{n}\log
n \log S)}$. Additionally, we show that L-ordered ABPs can be transformed into
L-pass smABPs with a polynomial blowup in size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04377"><span class="datestr">at January 15, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04372">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04372">Online Inventory Management with Application to Energy Procurement in Data Centers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Lin.html">Lin Yang</a>, Mohammad H. Hajiesmaili, Ramesh Sitaraman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mallada:Enrique.html">Enrique Mallada</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Wing_S=.html">Wing S. Wong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wierman:Adam.html">Adam Wierman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04372">PDF</a><br /><b>Abstract: </b>Motivated by the application of energy storage management in electricity
markets, this paper considers the problem of online linear programming with
inventory management constraints. Specifically, a decision maker should satisfy
some units of an asset as her demand, either form a market with time-varying
price or from her own inventory. The decision maker is presented a price in
slot-by-slot manner, and must immediately decide the purchased amount with the
current price to cover the demand or to store in inventory for covering the
future demand. The inventory has a limited capacity and its critical role is to
buy and store assets at low price and use the stored assets to cover the demand
at high price. The ultimate goal of the decision maker is to cover the demands
while minimizing the cost of buying assets from the market. We propose BatMan,
an online algorithm for simple inventory models, and BatManRate, an extended
version for the case with rate constraints. Both BatMan and BatManRate achieve
optimal competitive ratios, meaning that no other online algorithm can achieve
a better theoretical guarantee. To illustrate the results, we use the proposed
algorithms to design and evaluate energy procurement and storage management
strategies for data centers with a portfolio of energy sources including the
electric grid, local renewable generation, and energy storage systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04372"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04358">Quotient Hash Tables - Efficiently Detecting Duplicates in Streaming Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=eacute=raud:R=eacute=mi.html">Rémi Géraud</a>, Marius Lombard-Platet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naccache:David.html">David Naccache</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04358">PDF</a><br /><b>Abstract: </b>This article presents the Quotient Hash Table (QHT) a new data structure for
duplicate detection in unbounded streams. QHTs stem from a corrected analysis
of streaming quotient filters (SQFs), resulting in a 33\% reduction in memory
usage for equal performance. We provide a new and thorough analysis of both
algorithms, with results of interest to other existing constructions.
</p>
<p>We also introduce an optimised version of our new data structure dubbed
Queued QHT with Duplicates (QQHTD).
</p>
<p>Finally we discuss the effect of adversarial inputs for hash-based duplicate
filters similar to QHT.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04358"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04237">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04237">Topology is relevant (in the infinite-domain dichotomy conjecture for constraint satisfaction problems)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodirsky:Manuel.html">Manuel Bodirsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mottet:Antoine.html">Antoine Mottet</a>, Miroslav Olšák, Jakub Opršal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pinsker:Michael.html">Michael Pinsker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Willard:Ross.html">Ross Willard</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04237">PDF</a><br /><b>Abstract: </b>The algebraic dichotomy conjecture for Constraint Satisfaction Problems
(CSPs) of reducts of (infinite) finitely bounded homogeneous structures states
that such CSPs are polynomial-time tractable when the model-complete core of
the template has a pseudo-Siggers polymorphism, and NP-complete otherwise.
</p>
<p>One of the important questions related to this conjecture is whether,
similarly to the case of finite structures, the condition of having a
pseudo-Siggers polymorphism can be replaced by the condition of having
polymorphisms satisfying a fixed set of identities of height 1, i.e.,
identities which do not contain any nesting of functional symbols. We provide a
negative answer to this question by constructing for each non-trivial set of
height 1 identities a structure whose polymorphisms do not satisfy these
identities, but whose CSP is tractable nevertheless.
</p>
<p>An equivalent formulation of the dichotomy conjecture characterizes
tractability of the CSP via the local satisfaction of non-trivial height 1
identities by polymorphisms of the structure. We show that local satisfaction
and global satisfaction of non-trivial height 1 identities differ for
$\omega$-categorical structures with less than double exponential orbit growth,
thereby resolving one of the main open problems in the algebraic theory of such
structures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04237"><span class="datestr">at January 15, 2019 02:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04068">Longest Common Subsequence on Weighted Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Evangelos Kipouridis, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsichlas:Kostas.html">Kostas Tsichlas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04068">PDF</a><br /><b>Abstract: </b>We consider the general problem of the Longest Common Subsequence (LCS) on
weighted sequences. Weighted sequences are an extension of classical strings,
where in each position every letter of the alphabet may occur with some
probability. In this paper we provide faster algorithms and prove a series of
hardness results for more general variants of the problem. In particular, we
provide an NP-Completeness result on the general variant of the problem instead
of the log-probability version used in earlier papers, already for alphabets of
size 2. Furthermore, we design an EPTAS for bounded alphabets, which is also an
improved, compared to previous results, PTAS for unbounded alphabets. These are
in a sense optimal, since it is known that there is no FPTAS for bounded
alphabets, while we prove that there is no EPTAS for unbounded alphabets.
Finally, we provide a matching conditional (under the Exponential Time
Hypothesis) lower bound for any PTAS. As a side note, we prove that it is
sufficient to work with only one threshold in the general variant of the
problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04068"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.04008">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.04008">Fast and Simple Deterministic Algorithms for Highly-Dynamic Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, Neta Dafni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolobov:Victor_I=.html">Victor I. Kolobov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paz:Ami.html">Ami Paz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schwartzman:Gregory.html">Gregory Schwartzman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.04008">PDF</a><br /><b>Abstract: </b>This paper provides a surprisingly simple method for obtaining fast (constant
amortized time) deterministic distributed algorithms for a highly-dynamic
setting, in which arbitrarily many edge changes may occur in each round. Among
the implications of our results are deterministic algorithms that maintain
solutions to many problems, including $(degree+1)$-coloring, maximal matching,
maximal independent set and the seemingly unrelated problem of a
2-approximation for minimum weight vertex cover (2-MWVC).
</p>
<p>These significantly improve upon prior work in various aspects, such as
having $O(1)$ amortized round complexity, using message of logarithmic size
only, handling arbitrarily many concurrent topology changes, being
deterministic, having correctness guarantees for intermediate rounds, and more.
</p>
<p>The core of our work is in defining a subclass of locally-checkable labelings
which we call locally-fixable labelings (LFLs). Very roughly speaking, these
are labelings that allow a node to fix its neighborhood based solely on their
old labels. We present a simple algorithm for LFLs with small labels, which
handles multiple edge insertions/deletions while keeping the amortized round
complexity at a small constant. We then extend it, for specific tasks, to
handle the insertion and deletion of nodes. Moreover, we show that the same
approach can also fix labeling with large labels, given that they can be made
to behave as small labels.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.04008"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03880">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03880">Model checking: the interval way</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molinari:Alberto.html">Alberto Molinari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03880">PDF</a><br /><b>Abstract: </b>[...] The most famous MC techniques were developed from the late 80s, bearing
in mind the well-known "point-based" temporal logics LTL and CTL. However,
while the expressiveness of such logics is beyond doubt, there are some
properties we may want to check that are inherently "interval-based" and thus
cannot be expressed by point-based temporal logics, e.g., "the proposition p
has to hold in at least an average number of system states in a given
computation sector". Here interval temporal logics (ITLs) come into play,
providing an alternative setting for reasoning about time. Such logics deal
with intervals, instead of points, as their primitive entities: this feature
gives them the ability of expressing temporal properties, such as actions with
duration, accomplishments, and temporal aggregations, which cannot be dealt
with in standard point-based logics. The Halpern and Shoham's modal logic of
time intervals (HS, for short) is one of the most famous ITLs: it features one
modality for each of the 13 possible ordering relations between pairs of
intervals, apart from equality. In this thesis we focus our attention on MC
based on HS, in the role of property specification language, for which a little
work has been done if compared to MC for point-based temporal logics. [...]
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03880"><span class="datestr">at January 15, 2019 02:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03783">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03783">On Huang and Wong's Algorithm for Generalized Binary Split Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chrobak:Marek.html">Marek Chrobak</a>, Mordecai Golin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munro:J=_Ian.html">J. Ian Munro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Young:Neal_E=.html">Neal E. Young</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03783">PDF</a><br /><b>Abstract: </b>Huang and Wong [5] proposed a polynomial-time dynamic-programming algorithm
for computing optimal generalized binary split trees. We show that their
algorithm is incorrect. Thus, it remains open whether such trees can be
computed in polynomial time. Spuler [11, 12] proposed modifying Huang and
Wong's algorithm to obtain an algorithm for a different problem: computing
optimal two-way-comparison search trees. We show that the dynamic program
underlying Spuler's algorithm is not valid, in that it does not satisfy the
necessary optimal-substructure property and its proposed recurrence relation is
incorrect. It remains unknown whether the algorithm is guaranteed to compute a
correct overall solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03783"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03744">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03744">Exponentially Faster Massively Parallel Maximal Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behnezhad:Soheil.html">Soheil Behnezhad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harris:David_G=.html">David G. Harris</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03744">PDF</a><br /><b>Abstract: </b>The study of graph problems in the Massively Parallel Computations (MPC)
model has recently seen a burst of breakthroughs. Czumaj et al. [STOC'18],
Assadi et al. [SODA'19], and Ghaffari et al. [PODC'18], gave algorithms for
finding a $1+\varepsilon$ approximate maximum matching in $O(\log \log n)$
rounds using $\widetilde{O}(n)$ memory per machine. Despite this progress, we
still have a far more limited understanding of the central symmetry-breaking
problem of maximal matching. The round complexity of all these algorithms blows
up to $\Omega(\log n)$ in this case, which is considered inefficient. In fact,
the only known subpolylogarithmic round algorithm remains to be that of
Lattanzi et al. [SPAA'11] which undesirably requires a strictly super-linear
space of $n^{1+\Omega(1)}$ per machine.
</p>
<p>We resolve this shortcoming by providing exponentially faster algorithms for
maximal matching. Perhaps more importantly, we obtain this by analyzing an
extremely simple and natural algorithm. The algorithm edge-samples the graph,
partitions the vertices at random, and finds a greedy maximal matching within
each partition. We show that this algorithm drastically reduces the vertex
degrees. This, among some other results, leads to an $O(\log \log \Delta)$
round algorithm for maximal matching with $O(n)$ space. The space can be
further improved to mildly sublinear in $n$ by standard techniques.
</p>
<p>As an immediate corollary, we get a $2$ approximation for minimum vertex
cover in essentially the same rounds and space. This is the best possible
approximation factor under standard assumptions, culminating a long line of
research. Other corollaries include more efficient algorithms for $1 +
\varepsilon$ approximate matching and $2 + \varepsilon$ approximate weighted
matching. All these results can also be implemented in the congested clique
model within $O(\log \log \Delta)$ rounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03744"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1804.02075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1804.02075">A Framework for Searching in Graphs in the Presence of Errors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dereniowski:Dariusz.html">Dariusz Dereniowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tiegel:Stefan.html">Stefan Tiegel</a>, Przemysław Uznański, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolleb=Graf:Daniel.html">Daniel Wolleb-Graf</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1804.02075">PDF</a><br /><b>Abstract: </b>We consider the problem of searching for an unknown target vertex $t$ in a
(possibly edge-weighted) graph. Each \emph{vertex-query} points to a vertex $v$
and the response either admits $v$ is the target or provides any neighbor
$s\not=v$ that lies on a shortest path from $v$ to $t$. This model has been
introduced for trees by Onak and Parys [FOCS 2006] and for general graphs by
Emamjomeh-Zadeh et al. [STOC 2016]. In the latter, the authors provide
algorithms for the error-less case and for the independent noise model (where
each query independently receives an erroneous answer with known probability
$p&lt;1/2$ and a correct one with probability $1-p$).
</p>
<p>We study this problem in both adversarial errors and independent noise
models. First, we show an algorithm that needs $\frac{\log_2 n}{1 - H(r)}$
queries against \emph{adversarial} errors, where adversary is bounded with its
rate of errors by a known constant $r&lt;1/2$. Our algorithm is in fact a
simplification of previous work, and our refinement lies in invoking
amortization argument. We then show that our algorithm coupled with Chernoff
bound argument leads to an algorithm for independent noise that is simpler and
with a query complexity that is both simpler and asymptotically better to one
of Emamjomeh-Zadeh et al. [STOC 2016].
</p>
<p>Our approach has a wide range of applications. First, it improves and
simplifies Robust Interactive Learning framework proposed by Emamjomeh-Zadeh et
al. [NIPS 2017]. Secondly, performing analogous analysis for
\emph{edge-queries} (where query to edge $e$ returns its endpoint that is
closer to target) we actually recover (as a special case) noisy binary search
algorithm that is asymptotically optimal, matching the complexity of Feige et
al. [SIAM J. Comput. 1994]. Thirdly, we improve and simplify upon existing
algorithm for searching of \emph{unbounded} domains due to Aslam and Dhagat
[STOC 1991].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1804.02075"><span class="datestr">at January 15, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/01/14/summer-school-on-geometric-and-algebraic-combinatorics/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/01/14/summer-school-on-geometric-and-algebraic-combinatorics/">Summer School on Geometric and Algebraic Combinatorics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 17-28, 2019 Paris, France http://gac-school.imj-prg.fr/ Submission deadline: March 27, 2019 Registration deadline: March 27, 2019 This two-week summer school will present an overview of a selection of topics in the crossroads between geometry, algebra, and combinatorics. It will consist of four one-week mini-courses given by leading experts, complemented with supervised exercise sessions, lectures by … <a href="https://cstheory-events.org/2019/01/14/summer-school-on-geometric-and-algebraic-combinatorics/" class="more-link">Continue reading <span class="screen-reader-text">Summer School on Geometric and Algebraic Combinatorics</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/01/14/summer-school-on-geometric-and-algebraic-combinatorics/"><span class="datestr">at January 14, 2019 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6137751989330501107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/01/one-postdoc-position-available-at.html">One postdoc position available at Reykjavik University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="background-color: white; color: #222222; font-family: Arial,Helvetica,sans-serif; font-size: small; font-style: normal; font-weight: 400; letter-spacing: normal; text-align: center; text-indent: 0px; white-space: normal;"><b><br class="m_1631593900696185630gmail-Apple-interchange-newline" />Open Problems in the Equational Logic of Processes</b></div><span style="background-color: white; color: #222222; font-family: Arial,Helvetica,sans-serif; font-size: x-small; font-style: normal; font-weight: 400; letter-spacing: normal; text-align: start; text-indent: 0px; white-space: normal;" class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"><div style="text-align: center;"><br /><b>School of Computer Science, <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">Reykjavik</span> <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">University</span></b><br /><br /><b>One Postdoc</b><b> </b><b>Position</b></div><br />Applications are invited for one post-doctoral position at the School of Computer Science, <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">Reykjavik</span> <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">University</span>.   The position is part of a three-year research project funded by the  Icelandic Research Fund, under the direction of Luca Aceto (Gran Sasso  Science Institute and <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">Reykjavik</span> <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">University</span>) <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il"></span>and Anna Ingolfsdottir (<span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">Reykjavik</span> <span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-il">University</span>) in cooperation with Bas Luttik (TU Eindhoven) and Alexandra Silva (University C<span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im">ollege London)</span>.  The overarching goal of this project is to solve some of the  challenging open problems in the equational axiomatization of  behavioural equivalences over process calculi. Interested applicants can  contact Luca Aceto (email: <a style="color: #1155cc;" href="mailto:luca@ru.is" target="_blank">luca@ru.is</a>) for further details on the research proposal.</span><span style="background-color: white; color: #222222; font-family: Arial,Helvetica,sans-serif; font-size: x-small; font-style: normal; font-weight: 400; letter-spacing: normal; text-align: start; text-indent: 0px; white-space: normal;" class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"></span><div><span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"><div style="font-family: Arial,Helvetica,sans-serif; font-size: small;"><br />The  successful candidate will benefit from, and contribute to, the research  environment at the Icelandic Centre of Excellence in Theoretical  Computer Science (ICE-TCS). For information about ICE-TCS and its  activities, see<br /><br /><div style="text-align: center;"><a style="color: #1155cc;" href="http://www.icetcs.ru.is/" target="_blank">http://www.icetcs.ru.is/</a>.</div></div><div style="font-family: Arial,Helvetica,sans-serif; font-size: small;">Moreover,  she/he will cooperate with Bas Luttik and Alexandra Silva  during the  project work and will benefit from the interaction with their research  groups at TU Eindhoven and University College London. The postdoc will  also have a chance to interact with Clemens Grabmayer and the CS group  at the Gran Sasso Science Institute (<a style="color: #1155cc;" href="http://cs.gssi.it/" target="_blank">http://cs.gssi.it/</a>), L'Aquila, Italy. </div><br /><b style="font-family: Arial,Helvetica,sans-serif; font-size: small;">Qualification requirements</b><span style="font-family: Arial,Helvetica,sans-serif; font-size: x-small;"><span lang="IS" style="font-family: Arial,sans-serif; font-size: 10pt;"></span><b><span lang="IS" style="font-family: Arial,sans-serif; font-size: 10pt;"></span></b><b><span lang="IS" style="font-family: Arial,sans-serif; font-size: 10pt;"></span></b><span lang="IS" style="font-family: Arial,sans-serif; font-size: 10pt;"><br /></span></span>Applicants  for the postdoctoral position should have, or be about to hold, a PhD  degree in Computer Science or closely related fields. Previous knowledge  of at least one of concurrency theory, process calculi, (structural)  operational semantics and logic in computer science is highly desirable.</span></div><div style="font-family: Arial,Helvetica,sans-serif; font-size: small;"><span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"><br /><b>Remuneration</b><span><span lang="IS" style="font-family: Arial,sans-serif; font-size: 10pt;"><br /></span></span>The wage for the postdoctoral position is </span>530,000 ISK (roughly 3,830  € at the present exchange rate) per month before taxes. (See <a href="http://payroll.is/en/" target="_blank">http://payroll.is/en/</a> for information on what the wage will be after taxes.)<span> </span>The position is  for two years, starting as soon as possible, and is renewable for  another year, based on good performance and mutual satisfaction.</div><div><span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"><br /><b style="font-family: Arial,Helvetica,sans-serif; font-size: small;">Application details</b><br /><br />Interested  applicants should send their CV, including a list of publications, in  PDF to all the addresses below, together with a statement outlining  their suitability for the project and the names of at least two  referees.<br /><br />Luca Aceto<br />email: <a style="color: #1155cc; font-family: Arial,Helvetica,sans-serif; font-size: small;" href="mailto:luca@ru.is" target="_blank">luca@ru.is</a><br /><br />Anna Ingolfsdottir<br />email: <a style="color: #1155cc; font-family: Arial,Helvetica,sans-serif; font-size: small;" href="mailto:annai@ru.is" target="_blank">annai@ru.is</a></span></div><div style="font-family: Arial,Helvetica,sans-serif; font-size: small;"><br /></div><div style="font-family: Arial,Helvetica,sans-serif; font-size: small;"><span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im"></span></div><span class="m_1631593900696185630gmail-m_5141331556671273973gmail-m_-7043862561460987118gmail-m_2410624989205033738im">We  will start reviewing applications as soon as they arrive and will  continue to accept applications until the position is filled. We  strongly encourage interested applicants to send their applications as  soon as possible<span> and no later than 8 February 2019. </span></span></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/01/one-postdoc-position-available-at.html"><span class="datestr">at January 14, 2019 10:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03689">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03689">Depth First Search in the Semi-streaming Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khan:Shahbaz.html">Shahbaz Khan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehta:Shashank_K=.html">Shashank K. Mehta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03689">PDF</a><br /><b>Abstract: </b>Depth first search (DFS) tree is a fundamental data structure for solving
various graph problems. The classical DFS algorithm requires $O(m+n)$ time for
a graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm
is allowed several passes (preferably single) over the input graph having a
restriction on the size of local space used.
</p>
<p>Trivially, a DFS tree can be computed using a single pass using $O(m)$ space.
In the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$
passes, where each pass adds one vertex to the DFS tree. However, it remains an
open problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even
in any relaxed streaming environment.
</p>
<p>We present the first semi-streaming algorithms that compute a DFS tree of an
undirected graph in $o(n)$ passes using $o(m)$ space. We first describe an
extremely simple algorithm that requires at most $\lceil n/k\rceil$ passes
using $O(nk)$ space, where $k$ is any positive integer. We then improve this
algorithm by using more involved techniques to reduce the number of passes to
$\lceil h/k\rceil$ under similar space constraints, where $h$ is the height of
the computed DFS tree. In particular, this algorithm improves the bounds for
the case where the computed DFS tree is shallow (having $o(n)$ height).
Moreover, this algorithm is presented as a framework that allows the
flexibility of using any algorithm to maintain a DFS tree of a stored sparser
subgraph as a black box, which may be of independent interest. Both these
algorithms essentially demonstrate the existence of a trade-off between the
space and number of passes required for computing a DFS tree. Furthermore, we
evaluate these algorithms experimentally which reveals their exceptional
performance in practice. For both random and real graphs, they require merely a
few passes even when allowed just $O(n)$ space.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03689"><span class="datestr">at January 14, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03627">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03627">Destroying Bicolored $P_3$s by Deleting Few Edges</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=uuml=ttemeier:Niels.html">Niels Grüttemeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a>, Jannik Schestag, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sommer:Frank.html">Frank Sommer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03627">PDF</a><br /><b>Abstract: </b>We introduce and study the Bicolored $P_3$ Deletion problem defined as
follows. The input is a graph $G=(V,E)$ where the edge set $E$ is partitioned
into a set $E_b$ of blue edges and a set $E_r$ of red edges. The question is
whether we can delete at most $k$ edges such that $G$ does not contain a
bicolored $P_3$ as an induced subgraph. Here, a bicolored $P_3$ is a path on
three vertices with one blue and one red edge. We show that Bicolored $P_3$
Deletion is NP-hard and cannot be solved in $2^{o(|V|+|E|)}$ time on
bounded-degree graphs if the ETH is true. Then, we show that Bicolored $P_3$
Deletion is polynomial-time solvable when $G$ does not contain a bicolored
$K_3$, that is, a triangle with edges of both colors. Moreover, we provide a
polynomial-time algorithm for the case if $G$ contains no blue $P_3$, red
$P_3$, blue $K_3$, and red $K_3$. Finally, we show that Bicolored $P_3$
Deletion can be solved in $\mathcal{O}(1.85^k\cdot |V|^5)$ time and that it
admits a kernel with $\mathcal{O}(\Delta k^2)$ vertices, where $\Delta$ is the
maximum degree of $G$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03627"><span class="datestr">at January 14, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03615">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03615">Decremental Strongly-Connected Components and Single-Source Reachability in Near-Linear Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernstein:Aaron.html">Aaron Bernstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Probst:Maximilian.html">Maximilian Probst</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wulff=Nilsen:Christian.html">Christian Wulff-Nilsen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03615">PDF</a><br /><b>Abstract: </b>Computing the Strongly-Connected Components (SCCs) in a graph $G=(V,E)$ is
known to take only $O(m + n)$ time using an algorithm by Tarjan from 1972[SIAM
72] where $m = |E|$, $n=|V|$. For fully-dynamic graphs, conditional lower
bounds provide evidence that the update time cannot be improved by polynomial
factors over recomputing the SCCs from scratch after every update.
Nevertheless, substantial progress has been made to find algorithms with fast
update time for \emph{decremental} graphs, i.e. graphs that undergo edge
deletions.
</p>
<p>In this paper, we present the first algorithm for general decremental graphs
that maintains the SCCs in total update time $\tilde{O}(m)$, thus only a
polylogarithmic factor from the optimal running time. Previously such a result
was only known for the special case of planar graphs [Italiano et al, STOC
2017]. Our result should be compared to the formerly best algorithm for general
graphs achieving $\tilde{O}(m\sqrt{n})$ total update time by Chechik et.al.
[FOCS 16] which improved upon a breakthrough result leading to $O(mn^{0.9 +
o(1)})$ total update time by Henzinger, Krinninger and Nanongkai [STOC 14,
ICALP 15]; these results in turn improved upon the longstanding bound of
$O(mn)$ by Roditty and Zwick [STOC 04].
</p>
<p>All of the above results also apply to the decremental Single-Source
Reachability (SSR) problem, which can be reduced to decrementally maintaining
SCCs. A bound of $O(mn)$ total update time for decremental SSR was established
already in 1981 by Even and Shiloach [JACM 1981].
</p>
<p>Using a well known reduction, we can maintain the reachability of pairs $S
\times V$, $S \subseteq V$ in fully-dynamic graphs with update time
$\tilde{O}(\frac{|S|m}{t})$ and query time $O(t)$ for all $t \in [1,|S|]$; this
generalizes an earlier All-Pairs Reachability where $S = V$ [{\L}\k{a}cki, TALG
2013].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03615"><span class="datestr">at January 14, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03582">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03582">On Kernelization for Edge Dominating Set under Structural Parameters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hols:Eva=Maria_C=.html">Eva-Maria C. Hols</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03582">PDF</a><br /><b>Abstract: </b>In the NP-hard Edge Dominating Set problem (EDS) we are given a graph
$G=(V,E)$ and an integer $k$, and need to determine whether there is a set
$F\subseteq E$ of at most $k$ edges that are incident with all (other) edges of
$G$. It is known that this problem is fixed-parameter tractable and admits a
polynomial kernel when parameterized by $k$. A caveat for this parameter is
that it needs to be large, i.e., at least equal to half the size of a maximum
matching of $G$, for instances not to be trivially negative. Motivated by this,
we study the existence of polynomial kernels for EDS when parameterized by
structural parameters that may be much smaller than $k$.
</p>
<p>Unfortunately, at first glance this looks rather hopeless: Even when
parameterized by the deletion distance to a disjoint union of paths $P_3$ of
length two there is no polynomial kernelization (under standard assumptions),
ruling out polynomial kernels for many smaller parameters like the feedback
vertex set size. In contrast, somewhat surprisingly, there is a polynomial
kernelization for deletion distance to a disjoint union of paths $P_5$ of
length four. As our main result, we fully classify for all finite sets
$\mathcal{H}$ of graphs, whether a kernel size polynomial in $|X|$ is possible
when given $X$ such that each connected component of $G-X$ is isomorphic to a
graph in $\mathcal{H}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03582"><span class="datestr">at January 14, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1901.03364">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1901.03364">On the Descriptive Complexity of Color Coding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannach:Max.html">Max Bannach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tantau:Till.html">Till Tantau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1901.03364">PDF</a><br /><b>Abstract: </b>Color coding is an algorithmic technique used in parameterized complexity
theory to detect "small" structures inside graphs. The idea is to derandomize
algorithms that first randomly color a graph and then search for an
easily-detectable, small color pattern. We transfer color coding to the world
of descriptive complexity theory by characterizing -- purely in terms of the
syntactic structure of describing formulas -- when the powerful second-order
quantifiers representing a random coloring can be replaced by equivalent,
simple first-order formulas. Building on this result, we identify syntactic
properties of first-order quantifiers that can be eliminated from formulas
describing parameterized problems. The result applies to many packing and
embedding problems, but also to the long path problem. Together with a new
result on the parameterized complexity of formula families involving only a
fixed number of variables, we get that many problems lie in FPT just because of
the way they are commonly described using logical formulas.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1901.03364"><span class="datestr">at January 14, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/004">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/004">TR19-004 |  UG-hardness to NP-hardness by Losing Half | 

	Amey Bhangale, 

	Subhash Khot</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The $2$-to-$2$ Games Theorem of [KMS-1, DKKMS-1, DKKMS-2, KMS-2] implies that it is NP-hard to distinguish between Unique Games instances with assignment satisfying at least  $(\frac{1}{2}-\varepsilon)$ fraction of the constraints $vs.$ no assignment satisfying more than $\varepsilon$ fraction of the constraints, for every constant $\varepsilon&gt;0$. We show that the reduction can be transformed in a non-trivial way to give a stronger guarantee in the completeness case: For at least $(\frac{1}{2}-\varepsilon)$ fraction of the vertices on one side, all the constraints associated with them in the Unique Games instance can be satisfied. 


We use this guarantee to convert the known UG-hardness results to NP-hardness. We show:

1. Tight inapproximability of approximating independent sets in a degree $d$ graphs within a factor of $\Omega\left(\frac{d}{\log^2 d}\right)$, where $d$ is a constant. 
2. NP-hardness of approximate Maximum Acyclic Subgraph problem within a factor of $\frac{2}{3}+\varepsilon$, improving the previous ratio of $\frac{14}{15}+\varepsilon$ by Austrin et al.[AMW15].
3. For any predicate $P^{-1}(1) \subseteq [q]^k$ supporting balanced pairwise independent distribution, given a $P$-CSP instance with value at least $\frac{1}{2}-\varepsilon$, it is NP-hard to satisfy more than $\frac{|P^{-1}(1)|}{q^k}+\varepsilon$ fraction of constraints.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/004"><span class="datestr">at January 13, 2019 07:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/01/13/jean-bourgain-1954-2018-and-michael-atiyah-1929-2019/">Jean Bourgain 1954–2018 and Michael Atiyah 1929–2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>A tribute to two premier analysts</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/bourgainatiyah.png"><img width="240" alt="" src="https://rjlipton.files.wordpress.com/2019/01/bourgainatiyah.png?w=240&amp;h=165" class="alignright wp-image-15571" height="165" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">From Flanders Today <a href="http://www.flanderstoday.eu/innovation/face-flanders-jean-bourgain">src1</a> and Ryle Trust Lecture <a href="http://www.sgr.org.uk/events/islam-and-west-personal-perspective">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Baron Jean Bourgain and Sir Michael Atiyah passed away within the past three weeks. They became mathematical nobility by winning the Fields Medal, Atiyah in 1966 and Bourgain in 1994. Bourgain was created Baron by King Philippe of Belgium in 2015. Atiyah’s knighthood did not confer nobility, but he held the dynastic <a href="https://en.wikipedia.org/wiki/Order_of_Merit">Order of Merit</a>, which is limited to 24 living members and has had fewer than 200 total since its inception in 1902. Atiyah had been #2 by length of tenure after Prince Philip and ahead of Prince Charles. </p>
<p>
Today we discuss how they ennobled mathematics by their wide contributions.</p>
<p>
Bourgain was affiliated to IAS by the IBM John von Neumann Professorship. He had been battling cancer for a long time. Here is the middle section of the coat of arms he created for his 2015 investiture:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/bourgainarmsmiddle.png"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2019/01/bourgainarmsmiddle.png?w=300&amp;h=165" class="aligncenter wp-image-15581" height="165" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Detail from IAS <a href="https://www.ias.edu/ideas/jean-bourgain%E2%80%99s-coat-arms">source</a></font>
</td>
</tr>
</tbody></table>
<p>
The shield shows the beginning of an Apollonian circle <a href="https://en.wikipedia.org/wiki/Apollonian_gasket">packing</a>, in which every radius is the reciprocal of an integer. This property continues as circles are recursively inscribed in the curvilinear regions—see this 2000 <a href="http://www.math.ucsd.edu/~ronspubs/03_02_appolonian.pdf">survey</a> for a proof. To quote Bourgain’s <a href="https://www.ias.edu/ideas/jean-bourgain's-coat-arms">words</a> accompanying his design:</p>
<blockquote><p><b> </b> <em> The theory of these [packings] is today a rich mathematical research area, at the interface of hyperbolic geometry, dynamics, and number theory. </em>
</p></blockquote>
<p></p><p>
Bourgain’s affinity to topics we hold dear in computing theory is shown by this 2009 <a href="https://www.youtube.com/watch?v=xMWlv_1DqyM">talk</a> titled, “The Search for Randomness.” It covers not only PRNGs and crypto but also expander graphs and <em>succinctness</em> in quantum computing. He has been hailed for diversity in other mathematical areas and editorships of many journals. We will talk about a problem in analysis which he helped solve not by analytical means but by connecting the problem to additive combinatorics.</p>
<p>
</p><p></p><h2> From Analysis to Combinatorics </h2><p></p>
<p>
</p><p>
Sōichi Kakeya posed the <a href="https://en.wikipedia.org/wiki/Kakeya_set">problem</a> of the minimum size of a subset of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{R}^{2}}" class="latex" title="{\mathbb{R}^{2}}" /> in which a unit-length needle can be rotated through 360 degrees. Abram Besicovitch showed in 1928 that such sets can have Lebesgue measure <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" />. He had already shown that one can achieve measure zero with a weaker property, which he had used to show a strong failure of Fubini’s theorem for Riemann integrals:</p>
<blockquote><p><b> </b> <em> For all <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> there is a measure-zero subset of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5Ed%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{R}^d}" class="latex" title="{\mathbb{R}^d}" /> that contains a unit line segment in every direction. </em>
</p></blockquote>
<p></p><p>
The surprise to many of us is that such strange sets would have important further consequences in analysis. A 2008 <a href="http://www.ams.org/journals/bull/2008-45-01/S0273-0979-07-01189-5/S0273-0979-07-01189-5.pdf">survey</a> in the <em>AMS Bulletin</em> by Izabella Łaba, titled “From Harmonic Analysis to Arithmetic Combinatorics,” brings out breakthrough contributions by Bourgain to conjectures and problems that involve further properties of these sets, which seem to retain Kakeya’s name: </p>
<blockquote><p><b> </b> <em> <b>Conjecture:</b> A Kakeya set in <img src="https://s0.wp.com/latex.php?latex=%7BR%5E%7Bd%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{R^{d}}" class="latex" title="{R^{d}}" /> must have Hausdorff dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />. </em>
</p></blockquote>
<p></p><p>
This and the formally weaker conjecture that the set must have Minkowski dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> are proved in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{R}^2}" class="latex" title="{\mathbb{R}^2}" /> but open for all <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d \geq 3}" class="latex" title="{d \geq 3}" />. Bourgain first proved that the <em>restriction conjecture</em> of Elias Stein, which is about extensions of the Fourier transform from certain subspaces of functions from <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{R}^d}" class="latex" title="{\mathbb{R}^d}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{C}}" class="latex" title="{\mathbb{C}}" /> to operators from <img src="https://s0.wp.com/latex.php?latex=%7BL%5Eq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^q}" class="latex" title="{L^q}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BL%5Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^p}" class="latex" title="{L^p}" /> functions on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{R}^d}" class="latex" title="{\mathbb{R}^d}" />, implies the Kakeya conjecture. It is likewise open for <img src="https://s0.wp.com/latex.php?latex=%7Bd%2Cp+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d,p \geq 3}" class="latex" title="{d,p \geq 3}" />. As Łaba writes, associated estimates “with <img src="https://s0.wp.com/latex.php?latex=%7Bp+%3E+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p &gt; 2}" class="latex" title="{p &gt; 2}" /> require deeper geometrical information, and this is where we find Kakeya sets lurking under the surface.” </p>
<p>
What Bourgain showed is that the restriction estimates place constraints on sets of lower Hausdorff dimension that force them to align “tubes” along discrete directions that can be approximated via integer lattices. This led to the following “key lemma”:</p>
<blockquote><p><b>Lemma 1</b> <em> Consider subsets <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Ctimes+B%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A \times B}" class="latex" title="{A \times B}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> are finite subsets of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%5Ed%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}^d}" class="latex" title="{\mathbb{Z}^d}" />, and define </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%5E%7B%2B%7D+%3D+%5C%7Ba%2Bb%3A+%28a%2Cb%29+%5Cin+S%5C%7D%2C+%5Cqquad+S%5E%7B-%7D+%3D+%5C%7Ba+-+b%3A+%28a%2Cb%29+%5Cin+S%5C%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  S^{+} = \{a+b: (a,b) \in S\}, \qquad S^{-} = \{a - b: (a,b) \in S\}. " class="latex" title="\displaystyle  S^{+} = \{a+b: (a,b) \in S\}, \qquad S^{-} = \{a - b: (a,b) \in S\}. " /></p>
</em><p><em>For every <img src="https://s0.wp.com/latex.php?latex=%7BC+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C &gt; 0}" class="latex" title="{C &gt; 0}" /> there is <img src="https://s0.wp.com/latex.php?latex=%7BC%27+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C' &gt; 0}" class="latex" title="{C' &gt; 0}" /> such that whenever <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%5E%7B%2B%7D%7C+%5Cleq+Cn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{|S^{+}| \leq Cn}" class="latex" title="{|S^{+}| \leq Cn}" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+%5Cmax%5C%7B%7CA%7C%2C%7CB%7C%5C%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n = \max\{|A|,|B|\}}" class="latex" title="{n = \max\{|A|,|B|\}}" />, we have <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%5E%7B-%7D%7C+%5Cleq+C%27n%5E%7B2+-+%5Cfrac%7B1%7D%7B13%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{|S^{-}| \leq C'n^{2 - \frac{1}{13}}}" class="latex" title="{|S^{-}| \leq C'n^{2 - \frac{1}{13}}}" />. </em>
</p></blockquote>
<p></p><p>
To quote Łaba: “Bourgain’s approach, however, provided a way out. Effectively, it said that our hypothetical set would have <em>structure</em>, to the extent that many of its lines would have to be parallel instead of pointing in different directions. <em>Not a Kakeya set, after all.</em>” She further says:</p>
<p>
</p><blockquote><p><b> </b> <em> Bourgain’s argument was, to this author’s knowledge, the first application of additive number theory to Euclidean harmonic analysis. It was significant, not only because it improved Kakeya bounds, but perhaps even more so because it introduced many harmonic analysts to additive number theory, including [Terence] Tao who contributed so much to the subject later on, and jump-started interaction and communication between the two communities. The Green-Tao theorem [on primes] and many other developments might have never happened, were it not for Bourgain’s brilliant leap of thought in 1998. </em>
</p></blockquote>
<p></p><p>
Among <a href="https://www.ias.edu/press-releases/2019/jean-bourgain-obituary">many</a> <a href="https://gilkalai.wordpress.com/2019/01/02/jean/">sources</a>, note this <a href="http://www.math.ucsd.edu/~fan/reading/ross/">seminar</a> sponsored by Fan Chung and links from Tao’s own memorial <a href="https://terrytao.wordpress.com/2018/12/29/jean-bourgain/">post</a>.</p>
<p>
</p><p></p><h2> Michael Atiyah </h2><p></p>
<p></p><p>
Michael Atiyah was also much more than an analyst—indeed, he was first a topologist and algebraic geometer. He was also a theoretical physicist. Besides all these scientific hats, he engaged with society at large. After heading Britain’s Royal Society from 1990 to 1995, he became president of the <a href="https://pugwash.org/">Pugwash</a> <a href="https://en.wikipedia.org/wiki/Pugwash_Conferences_on_Science_and_World_Affairs">Conferences</a> on Science and World Affairs. This organization was founded by Joseph Rotblat and Bertrand Russell in the 1950s to avert nuclear war and misuse of science, and won the 1995 Nobel Peace Prize.</p>
<p>
The “misuse of science” aspect comes out separately in Atiyah’s 1999 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1127049/">article</a> in the <em>British Medical Journal</em> titled, “Science for evil: the scientist’s dilemma.” It lays out a wider scope of ethical and procedural concerns than the original anti-war purpose. This is furthered in his 1999 <a href="https://www.researchgate.net/publication/304691079_The_Social_Responsibility_of_Scientists">book</a> <a href="https://link.springer.com/chapter/10.1057/9780230508606_15">chapter</a>, “The Social Responsibility of Scientists,” which laid out six points including:</p>
<ul>
<li>
First there is the argument of moral responsibility. If you create something you should be concerned with its consequences. This should apply as much to making scientific discoveries as to having children. <p></p>
</li><li>
Scientists will understand the technical problems better than the average politician or citizen and knowledge brings responsibility. <p></p>
</li><li>
[T]here is need to prevent a public backlash against science. Self-interest requires that scientists must be fully involved in public debate and must not be seen as “enemies of the people.”
</li></ul>
<p>
As he says in its abstract:</p>
<blockquote><p><b> </b> <em> In my own case, after many years of quiet mathematical research, working out of the limelight, a major change occurred when unexpectedly I found myself president of the Royal Society, in a very public position, and expected to act as a general spokesman for the whole of science. </em>
</p></blockquote>
<p></p><p>
Within physics and mathematics, he also ventured into a debate that comes closer to the theory-as-social-process topic we <a href="https://rjlipton.wordpress.com/2011/12/21/proofs-proofs-and-proofs/">have</a> <a href="https://rjlipton.wordpress.com/2010/01/11/why-do-we-need-cyber-security/">discussed</a> on this blog. In 1994 he led a collection of community <a href="https://arxiv.org/abs/math/9404229">responses</a> to a 1993 <a href="https://arxiv.org/abs/math/9307227">article</a> by Arthur Jaffe and Frank Quinn that began with the question, “Is speculative mathematics dangerous?” Atiyah replied by saying he agreed with many of their points, especially the need to distinguish between results based on rigorous proofs and heuristic arguments,</p>
<blockquote><p><b> </b> <em> …But if mathematics is to rejuvenate itself and break exciting new ground it will have to allow for the exploration of new ideas and techniques which, in their creative phase, are likely to be as dubious as in some of the great eras of the past. …[I]n the early stages of new developments, we must be prepared to act in more buccaneering style. </em>
</p></blockquote>
<p></p><p>
Now we cannot help recalling his <a href="https://drive.google.com/file/d/17NBICP6OcUSucrXKNWvzLmrQpfUrEKuY/view">claim</a> last September of heuristic arguments that will build a proof of the Riemann Hypothesis, which we <a href="https://rjlipton.wordpress.com/2018/09/26/reading-into-atiyahs-proof/">covered</a> in <a href="https://rjlipton.wordpress.com/2018/09/23/preview-of-the-atiyah-talk/">several</a> <a href="https://rjlipton.wordpress.com/2018/09/21/the-specter-of-simpler-proofs/">posts</a>. As we stated in our New Year’s <a href="https://rjlipton.wordpress.com/2019/01/06/predictions-for-2019/">post</a>, nothing more of substance has come to our attention. We do not know how much more work was done on the promised longer paper. We will move toward discussing briefly how his most famous work is starting to matter in algorithms and complexity.</p>
<p>
</p><p></p><h2> Indexes and Invariants </h2><p></p>
<p></p><p>
We will not try to go into even as much detail as we did for Kakeya sets about Atiyah’s signature contributions to <a href="https://en.wikipedia.org/wiki/Topological_K-theory">topological</a> <a href="https://en.wikipedia.org/wiki/K-theory">K-theory</a>, physical <a href="https://en.wikipedia.org/wiki/Michael_Atiyah#Gauge_theory_(1977-1985)">gauge</a> <a href="https://en.wikipedia.org/wiki/Gauge_theory">theory</a>, his celebrated <a href="https://en.wikipedia.org/wiki/Atiyah-Singer_index_theorem">index theorem</a> with Isadore Singer, and much else. But we can evoke reasons for us to be interested in the last. We start with the simple statement from the <a href="http://www.abelprize.no/c53865/binfil/download.php?tid=53804">essay</a> by John Rognes of Oslo that accompanied the 2004 Abel Prize award to Atiyah and Singer:</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BP%28f%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{P(f) = 0}" class="latex" title="{P(f) = 0}" /> be a system of differential equations. Then </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7Banalytical+index%7D%28P%29+%3D+%5Ctext%7Btopological+index%7D%28P%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \text{analytical index}(P) = \text{topological index}(P). " class="latex" title="\displaystyle  \text{analytical index}(P) = \text{topological index}(P). " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
Here the <em>analytical index</em> equals the dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_k}" class="latex" title="{d_k}" /> of the kernel of <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> minus the dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd_c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_c}" class="latex" title="{d_c}" /> of the co-kernel of <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" />, which (again quoting Rognes) “is equal to the number of parameters needed to describe all the solutions of the equation, minus the number of relations there are between the expressions <img src="https://s0.wp.com/latex.php?latex=%7BP%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P(f)}" class="latex" title="{P(f)}" />.” The <em>topological index</em> has a longer laundry list of items in its definition, but the point is, <em>those items are usually all easily calculable</em>. It is further remarkable that in many cases we can get <img src="https://s0.wp.com/latex.php?latex=%7Bd_k+-+d_c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_k - d_c}" class="latex" title="{d_k - d_c}" /> without knowing how to compute <img src="https://s0.wp.com/latex.php?latex=%7Bd_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_k}" class="latex" title="{d_k}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd_c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_c}" class="latex" title="{d_c}" /> individually. The New York Times <a href="https://www.nytimes.com/2019/01/11/obituaries/michael-atiyah-dead.html">obituary</a> quotes Atiyah from 2015:</p>
<blockquote><p><b> </b> <em> It’s a bit of black magic to figure things out about differential equations even though you can’t solve them. </em>
</p></blockquote>
<p></p><p>
One thing it helps figure out is satisfiability. Besides cases where knowing the number of solutions does help in finding them, there are many theorems that needed only information about the number and the parameterization. </p>
<p>
We have an analogous situation in complexity theory with the lower bound theorem of Walter Baur and Volker Strassen, which we covered in this <a href="https://rjlipton.wordpress.com/2010/08/19/projections-can-be-tricky/">post</a>: The number of multiplication gates needed to compute an arithmetical function <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is bounded below by a known constant times the log-base-2 of the maximum number of solutions to a system formed from the partial derivatives of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> and a certain number of linear equations, over cases where that number is finite. Furthermore, both theorems front on algebraic geometry and geometric invariant theory, whose rapid ascent in our field was witnessed by a workshop at IAS that we <a href="https://rjlipton.wordpress.com/2018/06/06/princeton-is-invariant/">covered</a> last June. That workshop mentioned not only Atiyah but also the further work in algebraic geometry by his student Frances Kirwan, who was contemporaneous with Ken while at Oxford. Thus we may see more of the kind of connections in which Atiyah delighted, as noted in <a href="https://www.maths.ox.ac.uk/node/31190">current</a> <a href="https://www.heidelberg-laureate-forum.org/blog/laureate/sir-michael-francis-atiyah/">tributes</a> and the “<a href="https://www.wired.com/2016/04/mathematical-matchmaker-michael-atiyah-dreams-quantum-union/">matchmaker</a>” label which was <a href="https://www.facebook.com/ICM2018/posts/1023543974470240">promoted</a> at last August’s ICM.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Our condolences go out to their families and colleagues.</p>
<p>
[more tribute links]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/01/13/jean-bourgain-1954-2018-and-michael-atiyah-1929-2019/"><span class="datestr">at January 13, 2019 06:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/11/postdoc-position-at-duke-university-apply-by-february-28-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/11/postdoc-position-at-duke-university-apply-by-february-28-2019/">Postdoc position at Duke University (apply by February 28, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The algorithms and computational economics research groups at Duke University invite applications for one or potentially more postdoctoral positions, starting on or after July 1, 2019. The AcademicJobsOnline URL has full details as well as online application form.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/13114">https://academicjobsonline.org/ajo/jobs/13114</a><br />
Email: kamesh@cs.duke.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/11/postdoc-position-at-duke-university-apply-by-february-28-2019/"><span class="datestr">at January 11, 2019 08:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e2022ad3cdb3fe200b">
</div>
<div class="entry">
<div class="content" lang="en-US">
<div><p><br />
</p><p class="asset asset-video"><br />
	<br />
</p><br />
I'm a bit disappointed that Michael describes al-Nayrizi's and Perigal's (families of) simple perfect squarings of the (flat square) torus as different. They are, in fact, identical—not just “congruent” or “equivalent” or “isomorphic”, but actually indistinguishable. They only look different because al-Nayrizi and Perigal cut the torus into a square in two different ways.<p></p></div></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2019/01/im-a-bit-disappointed-that-michael-describes-al-nayrizis-and-perigals-families-of-simple-perfect-squarings-of-the-fla.html"><span class="datestr">at January 11, 2019 01:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3373">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/01/11/sigecom-dissertation-award-call/">SIGecom Dissertation Award call</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div>Please consider nominating graduating Ph.D. students for the SIGecom Dissertation Award.  If you are a graduating student, consider asking your adviser or other senior mentor to nominate you.</div>
<div></div>
<div>Nominations are due on February 28, 2019.  This award is given to a student who defended a thesis in 2018.  It is a prestigious award and is accompanied by a $1500 prize.  In the past, the grand prize has been awarded to:</div>
<div></div>
<div>2017: Aviad Rubinstein, “Hardness of Approximation Between P and NP”</div>
<div>2016: Peng Shi, “Prediction and Optimization in School Choice”</div>
<div>2015: Inbal Talgam-Cohen, “Robust Market Design: Information and Computation “</div>
<div>2014: S. Matthew Weinberg, “Algorithms for Strategic Agents”</div>
<div>2013: Balasubramanian Sivan, “Prior Robust Optimization”</div>
<div></div>
<div>And the award has had seven runner-ups: Rachel Cummings, Christos Tzamos, Bo Waggoner, James Wright, Xi (Alice) Gao, Yang Cai, and Sigal Oren.  You can find detailed information about the nomination process at: <a href="http://www.sigecom.org/awardd.html" target="_blank" rel="noopener">http://www.sigecom.org/awardd.html</a>. We look forward to reading your nominations!</div>
<div></div>
<div>Your Award Committee,</div>
<div></div>
<div>Renato Paes Leme</div>
<div>Aaron Roth (Chair)</div>
<div>Inbal Talgam-Cohen</div></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2019/01/11/sigecom-dissertation-award-call/"><span class="datestr">at January 11, 2019 03:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-2366077530752985091">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/01/dear-all-please-consider-nominating.html">2019 SIGecom Dissertation Award: Call for Nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="margin: 0px;" class="MsoNormal">Dear all,<br /><br />Please consider nominating graduating Ph.D. students for the SIGecom Dissertation Award.  If you are a graduating student, consider asking your adviser or other senior mentor to nominate you.<br /><br />Nominations are due on February 28, 2019.  This award is given to a student who defended a thesis in 2018.  It is a prestigious award and is accompanied by a $1500 prize.  In the past, the grand prize has been awarded to:<br /><br />2017: Aviad Rubinstein, "Hardness of Approximation Between P and NP"<br />2016: Peng Shi, "Prediction and Optimization in School Choice"<br />2015: Inbal Talgam-Cohen, "Robust Market Design: Information and Computation "<br />2014: S. Matthew Weinberg, "Algorithms for Strategic Agents"<br />2013: Balasubramanian Sivan, "Prior Robust Optimization"<br /><br /><br />And the award has had seven runner-ups: Rachel Cummings, Christos Tzamos, Bo Waggoner, James Wright, Xi (Alice) Gao, Yang Cai, and Sigal Oren.  You can find detailed information about the nomination process at: <a href="http://www.sigecom.org/awardd.html">http://www.sigecom.org/awardd.html</a>. We look forward to reading your nominations!<br /><br /><br />Your Award Committee,<br /><b><br /></b><b>Renato Paes Leme</b><br /><b>Aaron Roth</b> (Chair)<br /><b>Inbal Talgam-Cohen</b></div><div style="background-color: white; color: #222222; font-size: small;" class="m_-6389343170732676329gmail-adL"><span style="color: #500050;" class="m_-6389343170732676329gmail-im"></span><br /><div style="font-family: arial, helvetica, sans-serif;"></div></div></div>







<p class="date">
by Aaron Roth (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/01/dear-all-please-consider-nominating.html"><span class="datestr">at January 10, 2019 08:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4590649641895956020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2019/01/analco-sosa-soda-post.html">ANALCO, SOSA, SODA post</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I spent the last few days at SODA-ANALCO-ALENEX-SOSA in San Diego.  (Nice location choice, I'd say!)  Here's some news.<br /><br />This will be the last ANALCO (Analytic Algorithms and Combinatorics).  Apparently submissions have been decreasing, so they've decided it will halt and the work on these topics will go into SODA and other conferences.  I'm not sure how to think of it -- I think we as a community have far too many conferences/workshops generally, but I think the SODA model of having ANALCO and ALENEX (and now SOSA, I imagine) folded in cleanly into the main conference is an excellent model.  I also like the ANALCO topics.  But I can understand the time may have come to do something else.  Thanks to everyone who worked to organize ANALCO and keep it going these many years.<br /><br />It looks like SOSA (Symposium on Simplicity in Algorithms) will be taking its place in the SODA lineup.  I co-chaired the symposium with Jeremy Fineman this year, the second for the symposium.  I was surprised by the high quality of the submissions, and was then further surprised by the strong turnout at SODA.  The room was quite full for the Tuesday afternoon sessions, and there were easily 75+ people at several of the talks.  I do think there's a need for SOSA -- no other workshop/conference hits the theme of simplicity in our area, and it's a really nice fit with the rest of SODA.  I'm hoping it will last, and in particular that they'll continue to have a good number of high quality submissions, but that depends on all of you.  Ideally, there will be a positive feedback loop here -- now that there's a good home for this type of work (besides notes on the arxiv), people will be more inclined to write up and submit things to SOSA.  For Tuesday's talks, I'll call out Josh Alman's great presentation on "An Illuminating Algorithm for the Light Bulb Problem" as my favorite for the day.<br /><br />With ANALCO exiting, though, I think there's more room for additional satellite events at SODA, so hopefully some people will get creative.<br /><br />If I had thought about it I should have live-blogged the business meeting.  I'd say as highlights, first, Sandy Irani presented the report of the ad hoc committee to combat harassment and discrimination in the theory of computing community.   (See <a href="https://www.ics.uci.edu/~irani/safetoc.html">here</a> for the report.)  There was an overwhelming vote to adopt their recommendations going forward.  It's good to see progress in addressing these community concerns.  Second, Shuchi Chawla will be the next PC chair, and she brought forward a plan to have SODA PC members be allowed to submit papers (with a higher bar) that was voted on favorably as well.<br /><br />I suppose the last note is that Jon Kleinberg's invited talk was the conference highlight you expect a Jon Kleinberg talk to be, with interesting results and models related to fairness and implicit bias.<br /><br />Thanks to SIAM and all the organizers for their hard work.</div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2019/01/analco-sosa-soda-post.html"><span class="datestr">at January 09, 2019 10:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/">Mixed-Integer Nonlinear Optimization meets Data Science</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 25, 2018 – June 28, 2019 Ischia, Italy http://www.iasi.cnr.it/minoa/big-data-school/ CNR-IASI, as part of the MINOA project, announces the school for PhD students and post-docs on the theme Mixed Integer Non linear Optimization meets Data Science. The school will cover the following topics: Deep learning for AI Clustering for Big Data Machine Learning for Combinatorial … <a href="https://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/" class="more-link">Continue reading <span class="screen-reader-text">Mixed-Integer Nonlinear Optimization meets Data Science</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/"><span class="datestr">at January 09, 2019 08:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5374">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2019/01/09/the-baruch-distinguished-mathematics-lecture-series/">The Baruch Distinguished Mathematics Lecture Series</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I am happy to announce the beginning of the Baruch Distinguished Mathematics Lecture Series. In this series we will bring established mathematicians to give talks to a general mathematical audience. Our first Distinguished Lecture, by Bjorn Poonen, will be “Undecidability in Number Theory”. Click here for the full details. The talk is open to everyone, […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2019/01/09/the-baruch-distinguished-mathematics-lecture-series/"><span class="datestr">at January 09, 2019 08:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blogs.princeton.edu/imabandit/?p=1350">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/bubeck.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blogs.princeton.edu/imabandit/2019/01/09/nemirovskis-acceleration/">Nemirovski’s acceleration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I will describe here the very first (to my knowledge) acceleration algorithm for smooth convex optimization, which is due to <a href="https://en.wikipedia.org/wiki/Arkadi_Nemirovski" class="liinternal" rel="nofollow">Arkadi Nemirovski</a> (dating back to the end of the 70’s). The algorithm relies on a <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc2da46d9824359f6ac8d33c5fb882dd_l3.png?resize=8%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="8" alt="2" class="ql-img-inline-formula " />-dimensional plane-search subroutine (which, in theory, can be implemented in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-38bfa3c1131fbae41cb358b8b685dc56_l3.png?resize=61%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="19" width="61" alt="\log(1/\epsilon)" class="ql-img-inline-formula " /> calls to a first-order oracle). He later improved it to only require a <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-21b5b4cbe9a10b6d847eeb4265b99898_l3.png?resize=7%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="7" alt="1" class="ql-img-inline-formula " />-dimensional line-search in 1981, but of course the breakthrough that everyone knows about came a year after with the famous 1982 paper by <a href="https://en.wikipedia.org/wiki/Yurii_Nesterov" class="liinternal" rel="nofollow">Nesterov</a> that gets rid of this extraneous logarithmic term altogether (and in addition is based on the <a href="https://blogs.princeton.edu/imabandit/2018/11/21/a-short-proof-for-nesterovs-momentum/" class="liinternal">deep insight</a> of modifying Polyak’s momentum).</p>
<p>Let <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="16" width="10" alt="f" class="ql-img-inline-formula " /> be a <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-21b5b4cbe9a10b6d847eeb4265b99898_l3.png?resize=7%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="7" alt="1" class="ql-img-inline-formula " />-smooth function. Denote <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d69559ebcb4e4ecdf7454cab91bf526b_l3.png?resize=125%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="19" width="125" alt="x^{+} = x - \nabla f(x)" class="ql-img-inline-formula " />. Fix a sequence <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ba9ca1ebe088d1befda0acb3c4644727_l3.png?resize=43%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="43" alt="(\lambda_t)_{t \in \N}" class="ql-img-inline-formula " />, to be optimized later. We consider the “conjugate” point <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d22daf1993fbb2397de0b21ab0ea87ee_l3.png?resize=119%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="23" width="119" alt="\sum_{s =1}^t \lambda_s \nabla f(x_s)" class="ql-img-inline-formula " />. The algorithm simply returns the optimal combination of the conjugate point and the gradient descent point, that is:</p>
<p style="line-height: 54px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-81d8ed71d897fa020844a53ecc9a5cc8_l3.png?resize=479%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="54" width="479" alt="\[ x_{t+1} = \mathrm{argmin}_{x \in P_t} f(x) \, \text{where} \, P_t = \mathrm{span}\left(x_t^+, \sum_{s =1}^t \lambda_s \nabla f(x_s)\right) \,. \]" class="ql-img-displayed-equation " /></p>
<p>Let us denote <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e614d7a76c02a9a308f9898af91df8ff_l3.png?resize=95%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="95" alt="g_s = \nabla f(x_s)" class="ql-img-inline-formula " /> and <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-629dcf91ea83c14ea854c74de1069acd_l3.png?resize=143%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="143" alt="\delta_s = f(x_s) - f(x^*)" class="ql-img-inline-formula " /> for shorthand. The key point is that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9d21b904e2ccc5df54959aa117a37b98_l3.png?resize=77%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="77" alt="g_{t+1} \in P_t^{\perp}" class="ql-img-inline-formula " />, and in particular <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e57279bc2d0342e42087a51af61fb76_l3.png?resize=231%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="231" alt="\|\sum_{s \leq t} \lambda_s g_s\|^2 = \sum_{s \leq t} \lambda_s^2 \|g_s\|^2" class="ql-img-inline-formula " />. Now recognize that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b728147d95760e42ef7cdbb706a8cfd1_l3.png?resize=38%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="38" alt="\|g_s\|^2" class="ql-img-inline-formula " /> is a lower bound on the improvement <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0808cda8024eb11ab7ce37176832a9fe_l3.png?resize=68%2C17&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="17" width="68" alt="\delta_s - \delta_{s+1}" class="ql-img-inline-formula " /> (here we use that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b0878f455a78407f8618e726e941aea6_l3.png?resize=33%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="33" alt="x_{s+1}" class="ql-img-inline-formula " /> is better than <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5acc4c663fcf2f647eb177ebb24bc154_l3.png?resize=20%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="19" width="20" alt="x_s^+" class="ql-img-inline-formula " />). Thus we get:</p>
<p style="line-height: 40px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a12bda17f9c1f0f0e13ef03c9a1c9d2c_l3.png?resize=404%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="40" width="404" alt="\[ \|\sum_{s \leq t} \lambda_s g_s\|^2 \leq \sum_{s \leq t} \lambda_s^2 (\delta_s - \delta_{s+1}) \leq \sum_{s \leq t} \delta_s (\lambda_s^2 - \lambda_{s-1}^2) \,. \]" class="ql-img-displayed-equation " /></p>
<p>In other words if the sequence <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab48baf331239642a00255b86324280a_l3.png?resize=10%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="10" alt="\lambda" class="ql-img-inline-formula " /> is chosen such that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2f241a8315da6dd3f7735135d1d2b7ae_l3.png?resize=114%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="21" width="114" alt="\lambda_s = \lambda_s^2 - \lambda_{s-1}^2" class="ql-img-inline-formula " /> then we get</p>
<p style="line-height: 40px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8e959561f27eec9096b81bd7148a4a75_l3.png?resize=180%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="40" width="180" alt="\[ \|\sum_{s \leq t} \lambda_s g_s\|^2 \leq \sum_{s \leq t} \lambda_s \delta_s \,. \]" class="ql-img-displayed-equation " /></p>
<p>This is good because roughly the reverse inequality also holds true by convexity (and the fact that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-212cae06b1b9b8b6af498b589bb15865_l3.png?resize=56%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="56" alt="x_s \in P_s" class="ql-img-inline-formula " /> so <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cf91835fdc91be361d1c7e89f867c5db_l3.png?resize=78%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="16" width="78" alt="g_s \cdot x_s = 0" class="ql-img-inline-formula " />):</p>
<p style="line-height: 40px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9d1ee450c97dfc04bca3a123fb68daa8_l3.png?resize=391%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="40" width="391" alt="\[ \sum_{s \leq t} \lambda_s \delta_s \leq \sum_{s \leq t} \lambda_s g_s \cdot (x_s - x^*) \leq \|x^*\| \cdot \| \sum_{s \leq t} \lambda_s g_s\| \,. \]" class="ql-img-displayed-equation " /></p>
<p>So finally we get <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d58dea2404181d7f1751fcf8e68ad024_l3.png?resize=143%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="143" alt="\sum_{s \leq t} \lambda_s \delta_s \leq \|x^*\|^2" class="ql-img-inline-formula " />, and it just remains to realize that <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f100f89f751713a1814b3938a510009b_l3.png?resize=16%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="16" alt="\lambda_s" class="ql-img-inline-formula " /> is of order <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bcfb3f0b6b04be3b598743cd774dd78_l3.png?resize=8%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="8" alt="s" class="ql-img-inline-formula " /> so that <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ff4a3e78d1ced5a05f33eb077194504_l3.png?resize=103%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="103" alt="\delta_t \leq \|x^*\|^2 / t^2" class="ql-img-inline-formula " />.</p></div>







<p class="date">
by Sebastien Bubeck <a href="https://blogs.princeton.edu/imabandit/2019/01/09/nemirovskis-acceleration/"><span class="datestr">at January 09, 2019 06:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=606">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/01/09/my-last-3-5-years/">My last 3.5 years</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;">I haven’t breathed (freely) since 3.5 years ago.  Precisely since the day before I left my Cambridge flat, when the Pods guy told me he couldn’t park. I had to vacate within 24 hours, had no place to put all the stuff I had never used since moving there in 2008, and also happened to have a 3-hour CPR course planned long ago, starting in minutes.  I took that life-saving course on the edge of the seat, each 5-minute break dashing out to call movers who might have had an unlikely last minute cancellation in the busiest day of the year (August 31).</p>
<p style="text-align: justify;">Oh the times I wished that the fireproof storage where the things eventually went burned down to the ground.  Instead I was going to have to move my never used belongings a million times up and down stairs.</p>
<p style="text-align: justify;">Anyway, after Cambridge I went to the <a href="https://emanueleviola.wordpress.com/2015/11/16/from-the-simons-institute/">Simons institute</a>. Even with all the help from the staff, finding housing was atrocious, and I had to change it during the semester. I didn’t have a place to come back, and from Berkeley I eventually found a short-term rental in Needham, MA.  The idea was to buy a house in that short term.  This proved <a href="https://emanueleviola.wordpress.com/2015/12/15/how-to-buy-a-house/">impossible</a>.  So we had to find another rental.  In the process, I was discriminated against three times.  One time the landlord rejected in writing my application claiming that they did not want to rent to families. The other two times the landlord simply rejected my application, and then lowered the price. I thought these moves made them dumb, but maybe they are actually much smarter than me, because after toying with the idea I did not, in fact, sue.</p>
<p style="text-align: justify;">Eventually we found another longer-term rental.  From there, with more excruciating difficulties I <a href="https://emanueleviola.wordpress.com/2017/12/19/how-to-buy-a-house-ii/">wrote about earlier</a>, I bought a house, which however required 1 year of renovations (not exactly cosmetic — more about this later).  These were completed just in time to store my useless stuff there: I left for another semester at the Simons institute.</p>
<p style="text-align: justify;">My second visit to the institute was also great.  In fact I enjoyed it even more than the semester on fine-grained: I was there for the program on lower bounds, which are exactly the problems I went into computer science to study. I had the best time, and lots of research exchanges.</p>
<p style="text-align: justify;">But again, the housing situation in Berkeley was desperate.  Twice I lost a house for 1 hour. Meaning, the landlord called to make the deal, I couldn’t pick up the phone, and when I called back 1 hour later the place was gone.  I still think it would be better if the institute bought a block of houses, and also provided computers.  Even better if they make it easier to print, rather than having to stand in a corner or go through a complicated set up.</p>
<p style="text-align: justify;">Another interesting pattern is that during my first visit there was a heat wave and the AC broke, and it was hot.  This time there was a rather serious wildfire, causing very unhealthy conditions in the bay area, and at times they couldn’t run the heating systems to avoid sucking in the smoke, and it was cold.</p>
<p style="text-align: justify;">Berkeley isn’t Princeton, but it’s hard for me not to compare the logistics of my visits to Simons and the IAS in Princeton.  In the latter I was put in a house steps from the Institute, with minimal effort and at a fraction of the price.  In my office there was already a working computer, connected to a printer.</p>
<p>Here’s the meaning of cloud computing, remote desktop, telnet, etc in 2019, here’s the progress, the sustainability, the sharing economy: everybody brings their own laptop.</p>
<p style="text-align: justify;">Back from Simons, I can’t help but be surprised that I still have an office.  In fact this happens every time I go up the stairs, turn the corner and see my name on the tag, and it says “Professor”. Really? Under <em>my name</em>? I have a startle each time.  I know this feeling is irrational, but is there.  Coming back from California, the feeling is intense.</p>
<p style="text-align: justify;">Back to business, I am now teaching algorithms.  I am running an online section, for which I am making videos on my <a href="https://www.youtube.com/channel/UChbOQ1Q8Fv44LbrQMvTPoEQ">youtube channel</a>. It’s the future.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/01/09/my-last-3-5-years/"><span class="datestr">at January 09, 2019 04:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2727898493587029341">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/01/search-versus-decision.html">Search versus Decision</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Shockingly I've never done a post on search versus decision, one of the more interesting dualities in complexity. In short: Decision: Is there a needle in the haystack? Search: Find the needle.<br />
<br />
In Satisfiability, or any other NP-complete problem, the two problems are essentially equivalent. If you can decided SAT you can find a solution (good homework problem) or even the best solution. Often people mix up the two, where people say finding the shortest Traveling Salesman Tour is NP-complete, <a href="https://blog.computationalcomplexity.org/2014/01/is-traveling-salesman-np-complete.html">usually</a> without getting into too much trouble.<br />
<br />
Decision is always at least as easy as search: If you have a solution you know there is one. What about the other direction? We can't actually prove search is hard without separating P and NP, but we have our conjectures.<br />
<br />
Sometimes both are easy. We can easily find the maximum weighted matching.<br />
<br />
Sometimes decision is easy and search is supposedly hard: Composite Numbers. The search version is factoring.<br />
<br />
Sometimes decision is trivial (i.e. they always exist) and search is still hard. Nash Equilibria. <a href="https://blog.computationalcomplexity.org/2006/05/dispersing-ramsey-graphs.html">Ramsey Graphs</a>.<br />
<br />
Often we ask whether search reduces to decision? If you have some oracle (magic black box) that answered decision questions, can you solve the search problem efficiently? SAT has this property, as does Matching (for trivial reasons). Nash Equilibrium and Composite Numbers likely don't.<br />
<br />
Graph Isomorphism does, i.e., given an oracle for graph isomorphism you can find the isomorphism (another good homework problem).<br />
<br />
There's also an interesting non-adaptive version. Given a SAT formula can you find an assignment with questions to a SAT oracle that all have to be asked at the same time?<br />
<br />
Here we get a probable yes. If the formula has one solution you can find it by asking for each bit of the solution. <a href="https://blog.computationalcomplexity.org/2006/09/favorite-theorems-unique-witnesses.html">Randomly you can reduce SAT to several formulas</a>, one of which is likely to have a single assignment that is also an assignment of the original formula. With standard hardness assumptions <a href="https://blog.computationalcomplexity.org/2006/07/full-derandomization.html">you can eliminate the randomness</a>.<br />
<br />
Is the same true for graph isomorphism? I think that's still open.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/01/search-versus-decision.html"><span class="datestr">at January 09, 2019 01:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/01/08/postdoc-at-saint-louis-university-apply-by-january-21-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/01/08/postdoc-at-saint-louis-university-apply-by-january-21-2019/">Postdoc at Saint Louis University (apply by January 21, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I have a openings for 2 postdocs, both starting in 2019: One is flexible in focus, fitting under the broad categories of computational topology/geometry and algorithms. The other is for a shape simplification project, jointly supervised by Dr. David Letscher, focusing on designing and implementing algorithms that use persistent homology as well as other tools from computational topology.</p>
<p>Website: <a href="http://cs.slu.edu/~chambers/">http://cs.slu.edu/~chambers/</a><br />
Email: erin.chambers@slu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/01/08/postdoc-at-saint-louis-university-apply-by-january-21-2019/"><span class="datestr">at January 08, 2019 06:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1259">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2019/01/07/catcs-mailing-list-and-sign-up-link/">CATCS mailing list and sign-up link</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>CATCS is starting up a new mailing list to send out annual newsletters. Messages will be sent out 1-2 times every year describing recent projects undertaken by the committee, funding opportunities, links to useful resources, etc. Anyone interested in hearing about our activities is welcome to sign up at <a href="https://groups.google.com/forum/#!forum/catcs-news">this link</a>. You do not have to be a member of SIGACT to sign up.<span style="color: #000000; font-family: Arial, sans-serif;"><br />
</span></p>
<div></div></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2019/01/07/catcs-mailing-list-and-sign-up-link/"><span class="datestr">at January 07, 2019 08:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4355005625360509962">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/01/when-is-kilogram-not-kilogram.html">When is a kilogram not a kilogram?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A long long time ago the standards for meter's, kilograms, etc was an actual physical object.<br />
<br />
Those days are long gone of course. For example, the meter is defined is the length of the path traveled by light in 1/299,792,458 th of a second. Why such an odd number (can fractions be odd?)? Because they retrofitted it to what that the meter is.  Rather than go to France and compare my stick to the one under a glass case I can just measure the speed of light. Oh. That sounds hard!<br />
<br />
It matters a bit since the weight of what was the standard kilogram did increase over time, though of course not by much. When did the measurements for stuff STOP being based on physical objects and was all done based on constants of the universe?<br />
<br />
The answer surprised me:<br />
<br />
On Nov 16, 2018 (yes, you read that light) they decided that by May 20, 2019, the Kilogram will be defined in terms of Plank's constant. I have not been able to find out how they will use Plank, maybe they don't know yet (they do and its known -- see the first comment) .With that, there are no more standards based on physical objects. Read about it <a href="https://www.wired.com/story/new-kilogram-definition-based-on-planck-constant/">here</a>.<br />
<br />
Why did it take so long? I honestly don't know and I am tossing that question out to my readers. You can leave serious or funny answers, and best if I can't tell which is which!<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/01/when-is-kilogram-not-kilogram.html"><span class="datestr">at January 06, 2019 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15562">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/01/06/predictions-for-2019/">Predictions For 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>The problem of predicting ‘when’ not just ‘what’</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/asimovtorontostar.jpg"><img width="180" alt="" src="https://rjlipton.files.wordpress.com/2019/01/asimovtorontostar.jpg?w=180&amp;h=167" class="alignright wp-image-15564" height="167" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Toronto Star <a href="https://www.thestar.com/news/world/2018/12/27/35-years-ago-isaac-asimov-was-asked-by-the-star-to-predict-the-world-of-2019-here-is-what-he-wrote.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Isaac Asimov was a prolific writer of science fiction and nonfiction. Thirty-five years ago, on the eve of the year 1984, he noted that 35 years had passed since the publication of George Orwell’s <em>1984</em>. He wrote an exclusive <a href="https://www.thestar.com/news/world/2018/12/27/35-years-ago-isaac-asimov-was-asked-by-the-star-to-predict-the-world-of-2019-here-is-what-he-wrote.html">feature</a> for the Toronto Star newspaper predicting what the world would be like 35 years hence, that is, in 2019.</p>
<p>
Today we give our take on his predictions and make our own for the rest of 2019.<br />
<span id="more-15562"></span></p>
<p>
Asimov’s essay began by presupposing the absence of nuclear holocaust without predicting it. It then focused on two subjects: computerization and use of outer space. On the spectrum of evaluations subtended by this laudatory BBC <a href="https://www.bbc.com/news/technology-46736024">piece</a> and this critical <a href="https://www.thestar.com/news/world/2018/12/27/isaac-asimov-you-were-no-nostradamus.html">column</a> in the Toronto Star itself, we’re closer to the latter. On space he predicted we’d be mining the Moon by now; instead nothing more landed on the Moon until the Chinese <a href="https://en.wikipedia.org/wiki/Chang'e_3">Chang’e 3</a> mission in 2013 and <a href="https://en.wikipedia.org/wiki/Chang'e_4">Chang’e 4</a> happening now. His 35-year span should be lengthened to over a century.</p>
<p>
On computerization and robotics he was mostly right except again for the timespan: he said the transition would be “about over” by 2019 whereas it may be entering its period of greatest flux only now. However, for the end of 1983 we think the “whats” of his predictions were easy. Personal computers had already been around for almost a decade. Computer systems for business were plentiful. The Internet was already a proclaimed goal and the text-based <a href="https://en.wikipedia.org/wiki/Usenet">Usenet</a> was already operating. Asimov’s essay seems to miss how the combination of these three would soon move points of control outward to end-users. </p>
<p>
We still think what he wrote about space and robots will happen. This shows the problem of predictions is not just ‘what’ but ‘when.’ For another instance of being wrong on ‘when’ too soon, Ken told a Harvard Law graduate who visited him in Oxford in 1984 that what we now call <a href="https://en.wikipedia.org/wiki/Deepfake">deepfake</a> videos were imminent. We’ll make the rest of this post more about ‘when’ than ‘what.’</p>
<p>
</p><p></p><h2> Predictions in Past Years </h2><p></p>
<p></p><p>
Here are some predictions that we have made before. Seems we did not make any new predictions last year—oh well—but see <a href="https://rjlipton.wordpress.com/2018/01/02/predictions-we-didnt-make/">this</a>.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>No circuit lower bound of <img src="https://s0.wp.com/latex.php?latex=%7B1000n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1000n}" class="latex" title="{1000n}" /> or better will be proved for SAT.</em> Well that’s a freebie.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>A computer scientist will win a Nobel Prize.</em> No—indeed, less close than other years.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>At least five claims that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%3D%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P}=\mathsf{NP}}" class="latex" title="{\mathsf{P}=\mathsf{NP}}" /> and five that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D+%5Cneq+%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P} \neq \mathsf{NP}}" class="latex" title="{\mathsf{P} \neq \mathsf{NP}}" /> will be made.</em> </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> A “provably” secure crypto-system will be broken. For this one we don’t have to check any claims. We just pocket the ‘yes’ answer. Really, could you ever prove the opposite? How about the <a href="https://cacm.acm.org/magazines/2019/1/233523-imperfect-forward-secrecy/abstract">attack</a> on Diffie-Hellman in the current CACM?</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <em>An Earth-sized planet will be detected orbiting within the habitable zone of its single star.</em> The “when” for this one came in 2017 already. We are retiring it.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <em>A Clay problem will be solved, or at least notable progress made.</em> Again we sense that the answer on progress is “no.” This includes saying that nothing substantial seems to have emerged from Sir Michael Atiyah’s <a href="https://aperiodical.com/2018/09/atiyah-riemann-hypothesis-proof-final-thoughts/">claim</a> of proving the Riemann Hypothesis. However, we note <a href="https://gilkalai.wordpress.com/2018/12/25/amazing-karim-adiprasito-proved-the-g-conjecture-for-spheres/">via</a> Gil Kalai’s blog that a longstanding problem called the <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />-conjecture for spheres has been <a href="https://arxiv.org/abs/1812.10454">solved</a> by Karim Adiprasito.</p>
<p>
</p><p></p><h2> Predictions This Year </h2><p></p>
<p></p><p>
We will add some new predictions—it seems unfair to keep repeating sure winners. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>Deep learning methods will be found able to solve integer factoring.</em> This will place current cryptography is trouble.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>Deep learning methods will be found to help prove that factoring is hard.</em></p>
<p>
These may not be as contradictory as they seem. There is a long-known <a href="http://www.cs.sfu.ca/~kabanets/papers/natural-learning-short.pdf">connection</a> between certain learning algorithms and the <a href="https://en.wikipedia.org/wiki/Natural_proof">natural</a> <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">proofs</a> of Alexander Razborov and Stephen Rudich. The hardness predicate at the core of a natural proof is a classifier to distinguish (succinct) hard Boolean functions from easy ones. There is a duality between upper and lower bounds that in particular leads to the unconditional result that the discrete log problem, which is related to factoring and equally amenable to Peter Shor’s famous polynomial-time quantum algorithm, does not have natural proofs of hardness—because their existence would make discrete log relatively easy. </p>
<p>
Talking about quantum, we predict:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>Quantum supremacy will be proved—finally.</em> But be careful: there is a problem with this whole direction. See the next section.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>An algorithm originating in a theoretical model will be enshrined in law.</em> </p>
<p>
There are several near-term opportunities for this. The Supreme Court yesterday agreed to <a href="https://www.cnn.com/2019/01/04/politics/supreme-court-gerrymandering-cases/index.html">hear</a> two cases on partisan gerrymandering, at least one of which promises to codify an algorithmic criterion for excessive vote dilution. Maine adopted a automatic-runoff voting system whose dependence on computer implementation gave grounds for an unsuccessful <a href="https://www.americanthinker.com/blog/2018/11/maine_gop_rep_sues_to_stop_counting_ranked_choice_ballots.html">lawsuit</a>. Algorithmic fairness is a burgeoning area which we <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">discussed</a> a year-plus ago. <a href="https://www.sciencemag.org/news/2019/01/can-set-equations-keep-us-census-data-private">Use</a> of differential privacy by the U.S. Census could involve legislation. We distinguish legal provisions from the myriad problematic uses of algorithmic models in public and private <em>policy</em> ranging from credit evaluations to parole decisions to college admissions and much else.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> <em>The lines between heuristically solvable and really hard problems will become clearer.</em> We have <a href="https://rjlipton.wordpress.com/2016/07/10/the-world-turned-upside-down/">previously</a> <a href="https://rjlipton.wordpress.com/2014/02/28/practically-pnp/">opined</a> that the great success of SAT solvers in particular renders the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P=NP}}" class="latex" title="{\mathsf{P=NP}}" /> question moot for many purposes. Well, now we say the opposite: SAT solvers will hit a wall.</p>
<p>
</p><p></p><h2> Quantum Supremacy and Advantage </h2><p></p>
<p></p><p>
Ken recently attended a workshop in central New York that aimed to bring together researchers in many fields working on quantum devices. Materials for the workshop led off with the question of building quantum computers and highlighted Gil Kalai’s skeptical position in particular. An <a href="https://rjlipton.wordpress.com/2012/01/30/perpetual-motion-of-the-21st-century/">eight</a>–<a href="https://rjlipton.wordpress.com/2012/02/15/nature-does-not-conspire/">part</a> <a href="https://rjlipton.wordpress.com/2012/06/20/can-you-hear-the-shape-of-a-quantum-computer/">debate</a> between him and Aram Harrow which we hosted in 2012 <a href="https://rjlipton.wordpress.com/2012/03/05/the-quantum-super-pac/">involved</a> also John Preskill and <a href="https://rjlipton.wordpress.com/2012/10/03/quantum-supremacy-or-classical-control/">ended</a> with a discussion of quantum <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">supremacy</a>, a term advanced that year by Preskill. The workshop preferred the term quantum <em>advantage</em>. We interpret these terms as having the following distinction:</p>
<ul>
<li>
(a) Quantum <em>supremacy</em> means that a quantum device can perform general-purpose computations that no classical program or device can emulate in comparably feasible time. <p></p>
</li><li>
(b) Quantum <em>advantage</em> means that some particular practical task can be achieved by available quantum devices at lower costs than near-term available classical devices.
</li></ul>
<p>
As theoreticians we tend to think about (a) but many businesses and public-sector organizations would be ecstatic to have (b) in important applications. </p>
<p>
A new angle on (a) was shown by the new construction by Ran Raz and Avishay Tal of an oracle <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{BQP}^A}" class="latex" title="{\mathsf{BQP}^A}" /> is not in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPH%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{PH}^A}" class="latex" title="{\mathsf{PH}^A}" />. This was <a href="https://blog.computationalcomplexity.org/2018/12/complexity-year-in-review-2018.html">hailed</a> as the “result of the year” by Lance Fortnow (his second and our first is this <a href="https://eccc.weizmann.ac.il/report/2018/006/">progress</a> on the Unique Games Conjecture), and Scott Aaronson furnished a great <a href="https://www.scottaaronson.com/blog/?p=3827">discussion</a> of its genesis and further ramifications in complexity theory. <a href="https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/">Several</a> <a href="https://cacm.acm.org/magazines/2019/1/233514-quantum-leap/fulltext">popular</a> <a href="https://www.thehindu.com/sci-tech/science/quantum-computers-have-an-edge-over-classical-ones-says-the-oracle/article24420375.ece">articles</a> tried to pump this as non-oracle evidence for (a). But there is the over-arching problem:</p>
<blockquote><p><b> </b> <em> We know <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Csubseteq+BPP+%5Csubseteq+BQP+%5Csubseteq+PP+%5Csubseteq+P%5E%7B%5C%23P%7D+%5Csubseteq+PSPACE%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{P \subseteq BPP \subseteq BQP \subseteq PP \subseteq P^{\#P} \subseteq PSPACE}}" class="latex" title="{\mathsf{P \subseteq BPP \subseteq BQP \subseteq PP \subseteq P^{\#P} \subseteq PSPACE}}" /> but we don’t know <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+PSPACE%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{P \neq PSPACE}}" class="latex" title="{\mathsf{P \neq PSPACE}}" />. </em>
</p></blockquote>
<p></p><p>
So how are we ever going to be able to <em>prove</em> any form of supremacy? Even if we replace ‘polynomial time’ as our definition of ‘feasible’ by something more concrete, how can we prove that successful classical heuristics <em>do not exist</em>? On a certain practical problem of general import, Ewin Tang, a teenager in Texas advised by Scott, <a href="https://arxiv.org/abs/1807.04271">designed</a> an improved classical algorithm for low-rank matrix completion that <a href="https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/">eliminated</a> a previous quantum exponential advantage in the time dependence on the rank parameter. It is not just a case of <em>whether</em> we can prove supremacy, but judging <em>when</em> general quantum computers will be built to realize it.</p>
<p>
Whereas, the <em>when</em> involved in (b) is <em>now</em>. If a quantum device can do something useful now that classical methods are not delivering now, then it does not matter if the latter could be improved at greater hardware and development cost to work a year from now. This has been the gung-ho tenor of many responses to the recently-<a href="https://www.fedscoop.com/trump-signs-national-quantum-initiative-law/">signed</a> National Quantum Initiative Act. We do, however, still need to find and build said devices…</p>
<p>
As for the status of (a), we don’t know any better thought for January than the Janus-like title of this <a href="https://arxiv.org/abs/1807.10749">paper</a> by Igor Markov, Aneeqa Fatima, Sergei Isakov, and Sergio Boixo: </p>
<blockquote><p><b> </b> <em> “Quantum Supremacy Is Both Closer and Farther than It Appears.” </em>
</p></blockquote>
<p>
</p><p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your predictions for 2019? What are the most important matters we’ve left unsaid?</p>
<p>
[added some words to end of intro]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/01/06/predictions-for-2019/"><span class="datestr">at January 06, 2019 07:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/003">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/003">TR19-003 |  Near-Optimal Lower Bounds on the Threshold Degree and Sign-Rank of AC^0 | 

	Alexander A. Sherstov, 

	Pei Wu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The threshold degree of a Boolean function $f\colon\{0,1\}^n\to\{0,1\}$ is the minimum degree of a real polynomial $p$ that represents $f$ in sign: $\mathrm{sgn}\; p(x)=(-1)^{f(x)}.$ A related notion is sign-rank, defined for a Boolean matrix $F=[F_{ij}]$ as the minimum rank of a real matrix $M$ with $\mathrm{sgn}\; M_{ij}=(-1)^{F_{ij}}$.  Determining the maximum threshold degree and sign-rank achievable by constant-depth circuits ($\text{AC}^{0}$) is a well-known and extensively studied open problem, with complexity-theoretic and algorithmic applications.

We give an essentially optimal solution to this problem. For any $\epsilon&gt;0,$ we construct an $\text{AC}^{0}$ circuit in $n$ variables that has threshold degree $\Omega(n^{1-\epsilon})$ and sign-rank $\exp(\Omega(n^{1-\epsilon})),$ improving on the previous best lower bounds of $\Omega(\sqrt{n})$ and $\exp(\tilde{\Omega}(\sqrt{n}))$, respectively. Our results subsume all previous lower bounds on the threshold degree and sign-rank of $\text{AC}^{0}$ circuits of any given depth, with a strict improvement starting at depth $4$. As a corollary, we also obtain near-optimal bounds on the discrepancy, threshold weight, and threshold density of $\text{AC}^{0}$, strictly subsuming previous work on these quantities.  Our work gives some of the strongest lower bounds to date on the communication complexity of $\text{AC}^{0}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/003"><span class="datestr">at January 06, 2019 08:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/002">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/002">TR19-002 |  Complexity of Linear Operators | 

	Alexander Kulikov, 

	Ivan Mikhailin, 

	Vladimir Podolskii, 

	Andrey Mokhov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Let $A \in \{0,1\}^{n \times n}$ be a matrix with $z$ zeroes and $u$ ones and $x$ be an $n$-dimensional vector of formal variables over a semigroup $(S, \circ)$. How many semigroup operations are required to compute the linear operator $Ax$?

As we observe in this paper, this problem contains as a special case the well-known range queries problem and has a rich variety of applications in such areas as graph algorithms, functional programming, circuit complexity, and others. It is easy to compute $Ax$ using $O(u)$ semigroup operations. The main question studied in this paper is: can $Ax$ be computed using $O(z)$ semigroup operations? We prove that in general this is not possible: there exists a matrix $A \in \{0,1\}^{n \times n}$ with exactly two zeroes in every row (hence $z=2n$) whose complexity is $\Theta(n\alpha(n))$ where $\alpha(n)$ is the inverse Ackermann function. However, for the case when the semigroup is commutative, we give a constructive proof of an $O(z)$ upper bound. This implies that in commutative settings, complements of sparse matrices can be processed as efficiently as sparse matrices (though the corresponding algorithms are more involved). Note that this covers the cases of Boolean and tropical semirings that have numerous applications, e.g., in graph theory. 

As a simple application of the presented linear-size construction, we show how to multiply two $n\times n$ matrices over an arbitrary semiring in $O(n^2)$ time if one of these matrices is a 0/1-matrix with $O(n)$ zeroes (i.e., a complement of a sparse matrix).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/002"><span class="datestr">at January 06, 2019 05:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1075">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1075">News for December 2018</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Happy near year, and best wishes to those close and \(\varepsilon\)-far! December concluded the year with 4 new preprints, spanning quite a lot of the property testing landscape:</p>



<p><strong>Testing Stability Properties in Graphical Hedonic Games</strong>, by Hendrik Fichtenberger and Anja Rey (<a href="https://arxiv.org/abs/1812.09249">arXiv</a>). The authors of this paper consider the problem of deciding whether a given <em>hedonic game</em>  possesses some “coalition stability” in a property testing framework. Namely, recall that a hedonic game is a game where players (nodes) form coalitions (subsets of nodes) based on their individual preferences and local information about the considered coalition, thus resulting in a partition of the original graph. <br /> Several notions exist to evaluate how good such a partition is, based on how “stable” the given coalitions are. This work focuses on hedonic games corresponding to bounded-degree graphs, introducing and studying the property testing question of deciding <em>(for several such notions of stability)</em> whether a given game admits a stable coalition structure, or is far from admitting such a partition.</p>



<p><strong>Spectral methods for testing cluster structure of graphs</strong>, by Sandeep Silwal and Jonathan Tidor (<a href="https://arxiv.org/abs/1812.11564">arXiv</a>). Staying among bounded-degree graphs, we turn to testing clusterability of graphs, the focus of this paper. Given an \(n\)-node graph \(G\) of degree at most \(d\) and parameters \(k, \phi\), say that \(G\) is \((k, \phi)\)-clusterable if it can be partitioned in \(k\) parts of inner conductance at least \(\phi\).<br />Analyzing properties of a random walk on \(G\), this work gives a bicriterion guarantee (\((k, \phi)\)-clusterable vs. \(\varepsilon\)-far from \((k, \phi^\ast)\)-clusterable, where \(\phi^\ast \approx \varepsilon^2\phi^2\)) for the case \(k=2\), improving on previous work by Czumaj, Peng, and Sohler’15.</p>



<p>We then switch from graphs to probability distributions with our third paper:</p>



<p><strong>Inference under Information Constraints I: Lower Bounds from Chi-Square Contraction</strong>, by Jayadev Acharya, Clément Canonne, and Himanshu Tyagi (<a href="https://arxiv.org/abs/1812.11476">arXiv</a>). <em>(Disclaimer: I’m one of the authors.)</em> In this paper, the first of an announced series of three, the authors generalize the settings of two previous works we covered <a href="https://ptreview.sublinear.info/?m=201805">here</a> and <a href="https://ptreview.sublinear.info/?m=201809">there</a> to consider the general question of distribution testing and learning when the \(n\) i.i.d. samples are distributed among \(n\) players, which each can only communicate their sample to the central algorithm by respecting some pre-specified local information constraint <em>(e.g., privacy, or noise, or communication budget)</em>. This paper develops a general lower bound framework to study such questions, with a systematic focus on the power of public vs. private randomness between the \(n\) parties, and instantiate it to obtain tight bounds in the aforementioned locally private and communication-limited settings. (Spoiler: public randomness strictly helps, but not always.)</p>



<p>Finally, after games, graphs, and distributions, our fourth paper of the month concerns testing of functions:</p>



<p><strong>Partial Function Extension with Applications to Learning and Property Testing</strong>, by Umang Bhaskar and Gunjan Kumar (<a href="https://arxiv.org/abs/1812.05821">arXiv</a>). This work focuses on a problem quite related to property testing, that of partial function extension: given as input \(n\) pairs point/value from a purported function on a domain \(X\) of size \(|X| &gt; n\), one is tasked with deciding whether there does exist (resp., with finding) a function  \(f\) on \(X\) consistent with these \(n\) values which further satisfies a specific property, such as linearity or convexity. This is indeed very reminiscent of property testing, where one gets to query these \(n\) points and must decide (approximate) consistency with such a well-behaved function. Here, the authors study the computational hardness of this partial function extension problem, specifically for properties such as subadditivity and XOS (a sub-property of subadditivity); and as corollaries obtain new property testers for the classes of subadditive and XOS functions.</p>



<p>As usual, if you know of some work we missed from last December, let us know in the comments!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1075"><span class="datestr">at January 05, 2019 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/001">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/001">TR19-001 |  On OBDD-based algorithms and proof systems that dynamically change order of   variables | 

	Alexander Knop, 

	Dmitry Itsykson, 

	Dmitry Sokolov, 

	Andrei Romashchenko</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In 2004 Atserias, Kolaitis and Vardi proposed OBDD-based propositional proof systems that prove unsatisfiability of a CNF formula by deduction of identically false OBDD from OBDDs representing clauses of the initial formula. All OBDDs in such proofs have the same order of variables. We initiate the study of OBDD based proof systems that additionally contain a rule that allows changing the order in OBDDs. At first, we consider a proof system OBDD($\land$, reordering) that uses the conjunction (join) rule and the rule that allows changing the order. We exponentially separate this proof system from OBDD($\land$) proof system that uses only the conjunction rule. We prove two exponential lower bounds on the size of OBDD($\land$, reordering) refutations of Tseitin formulas and the pigeonhole principle. The first lower bound was previously unknown even for OBDD($\land$) proofs and the second one extends the result of Tveretina et al. from OBDD($\land$) to OBDD($\land$, reordering).

In 2004 Pan and Vardi proposed an approach to the propositional satisfiability problem based on OBDDs and symbolic quantifier elimination (we denote algorithms based on this approach as OBDD($\land$, $\exists$) algorithms). An instance of the propositional satisfiability problem is considered as an existential quantified propositional formula. The algorithm chooses an order on variables and creates an ordered binary decision diagram (OBDD) $D$ that initially represents the constant $1$ function. Then the algorithm downloads to $D$ clauses of the CNF one by one, and applies to $D$ the elimination of the existential quantifier for variable $x$ if all clauses that contain $x$ are already downloaded. We augment these algorithms with the operation of reordering of variables and call the new scheme OBDD($\land$, $\exists$, reordering) algorithms. We notice that there exists an OBDD($\land$, $\exists$) algorithm that solves satisfiable and unsatisfiable Tseitin formulas in polynomial time. In contrast, we show that there exist formulas representing systems of linear equations over $\mathbb{F}_2$ that are hard for OBDD($\land$, $\exists$, reordering)  algorithms. Our hard instances are satisfiable formulas representing systems of linear equations over $\mathbb{F}_2$ that
correspond to some checksum matrices of error correcting codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/001"><span class="datestr">at January 05, 2019 07:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1474">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2019/01/04/on-pac-analysis-and-deep-neural-networks/">On PAC Analysis and Deep Neural Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Guest post by <a href="http://amitdaniely.com/">Amit Daniely</a> and <a href="https://cs.stanford.edu/~rfrostig/">Roy Frostig</a>.</em></p>
<p>For years now—especially since the landmark work of <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Krishevsky et. al.</a>—learning deep neural networks has been a method of choice in prediction and regression tasks, especially in perceptual domains found in computer vision and natural language processing. How effective might it be for solving <em>theoretical</em> tasks?</p>
<p>Specifically, focusing on supervised learning:</p>
<blockquote><p>Can a deep neural network, paired with a stochastic gradient method, be shown to <a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">PAC learn</a> any interesting concept class in polynomial time?</p></blockquote>
<p>Depending on assumptions, and on one’s definition of “interesting,” present-day learning theory gives answers ranging from “no, that would solve hard problems,” to, more recently:</p>
<blockquote><p><strong>Theorem:</strong> Networks with depth between 2 and <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\log(n)" class="latex" title="\log(n)" />,<a href="https://theorydish.blog/feed/#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> having standard activation functions,<a href="https://theorydish.blog/feed/#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> with weights initialized at random and trained with stochastic gradient descent, learn, in polynomial time, constant degree large margin polynomial thresholds.</p></blockquote>
<p>Learning constant-degree polynomials can also be done simply <em>with a linear predictor</em> over a polynomial embedding, or, in other words, by learning a halfspace. That said, what a linear predictor can do is also <em>essentially the state of the art</em> in PAC learning, so this result pushes neural net learning at least as far as one might hope at first. We will return to this point later, and discuss some limitations of PAC analysis once they are more apparent. In this sense, this post will turn out to be as much an overview of some PAC learning theory as it is about neural networks.</p>
<p>Naturally, there is a wide variety of theoretical perspectives on neural network analysis, especially in the past couple of years. Our goal in this post is not to survey or cover any extensive body of work, but simply to summarize our own recent line (from two papers: <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity">DFS’16</a> and <a href="https://papers.nips.cc/paper/6836-sgd-learns-the-conjugate-kernel-class-of-the-network">D’17</a>), and to highlight the interaction with PAC learning.</p>
<h2 id="neural-network-learning">Neural network learning</h2>
<p>First, let’s define a learning task. To keep things simple, we’ll focus on binary classification over the boolean cube, without noise. Formally:</p>
<blockquote><p><strong>(Binary classification.)</strong> Given examples of the form <img src="https://s0.wp.com/latex.php?latex=%28x%2Ch%5E%2A%28x%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(x,h^*(x))" class="latex" title="(x,h^*(x))" />, where <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x" class="latex" title="x" /> is sampled from some unknown distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" /> on <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{\pm 1\}^n" class="latex" title="\{\pm 1\}^n" />, and <img src="https://s0.wp.com/latex.php?latex=h%5E%2A%3A%5C%7B%5Cpm+1%5C%7D%5En%5Cto%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h^*:\{\pm 1\}^n\to\{\pm 1\}" class="latex" title="h^*:\{\pm 1\}^n\to\{\pm 1\}" /> is some unknown function (the one that we wish to learn), find a function <img src="https://s0.wp.com/latex.php?latex=h%3A%5C%7B%5Cpm+1%5C%7D%5En%5Cto%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h:\{\pm 1\}^n\to\{\pm 1\}" class="latex" title="h:\{\pm 1\}^n\to\{\pm 1\}" /> whose error, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h%29+%3D+%5Cmathrm%7BPr%7D_%7Bx%5Csim%5Cmathcal%7BD%7D%7D+%5Cleft%28h%28x%29+%5Cne+h%5E%2A%28x%29%5Cright%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{Err}(h) = \mathrm{Pr}_{x\sim\mathcal{D}} \left(h(x) \ne h^*(x)\right)" class="latex" title="\mathrm{Err}(h) = \mathrm{Pr}_{x\sim\mathcal{D}} \left(h(x) \ne h^*(x)\right)" />, is small.</p></blockquote>
<p>Second, define a neural network <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal N" class="latex" title="\mathcal N" /> formally as a directed acyclic graph <img src="https://s0.wp.com/latex.php?latex=%28V%2C+E%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(V, E)" class="latex" title="(V, E)" /> whose vertices <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="V" class="latex" title="V" /> are called neurons. Of them, <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" /> are input neurons, one is an output neuron, and the rest are called hidden neurons.<a href="https://theorydish.blog/feed/#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> A network together with a weight vector <img src="https://s0.wp.com/latex.php?latex=w+%3D+%5C%7Bw_%7Buv%7D+%3A+uv+%5Cin+E%5C%7D+%5Ccup+%5C%7Bb_v+%3A+v+%5Cin+V+%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w = \{w_{uv} : uv \in E\} \cup \{b_v : v \in V \}" class="latex" title="w = \{w_{uv} : uv \in E\} \cup \{b_v : v \in V \}" /> defines a predictor <img src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmathcal+N%2C+w%7D+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{\mathcal N, w} : \{\pm 1\}^n \to \{\pm 1\}" class="latex" title="h_{\mathcal N, w} : \{\pm 1\}^n \to \{\pm 1\}" /> whose prediction is computed by propagating <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x" class="latex" title="x" /> forward through the network. Concretely:</p>
<ul>
<li>For an input neuron <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="v" class="latex" title="v" />, <img src="https://s0.wp.com/latex.php?latex=h_%7Bv%2Cw%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{v,w}(x)" class="latex" title="h_{v,w}(x)" /> is the corresponding coordinate in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x" class="latex" title="x" />.</li>
<li>For a hidden neuron <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="v" class="latex" title="v" />, define<img src="https://s0.wp.com/latex.php?latex=h_%7Bv%2Cw%7D%28x%29+%3D+%5Csigma%5Cleft%28+%5Csum_%7Bu+%5Cin+%5Cmathrm%7BIN%7D%28v%29%7D+w_%7Buv%7D+h_%7Bu%2Cw%7D%28x%29+%2B+b_v+%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{v,w}(x) = \sigma\left( \sum_{u \in \mathrm{IN}(v)} w_{uv} h_{u,w}(x) + b_v \right)." class="latex" title="h_{v,w}(x) = \sigma\left( \sum_{u \in \mathrm{IN}(v)} w_{uv} h_{u,w}(x) + b_v \right)." />The scalar weight <img src="https://s0.wp.com/latex.php?latex=b_v&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="b_v" class="latex" title="b_v" /> is called a “bias.” In this post, the function <img src="https://s0.wp.com/latex.php?latex=%5Csigma+%3A+%5Cmathbb%7BR%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\sigma : \mathbb{R} \to \mathbb{R}" class="latex" title="\sigma : \mathbb{R} \to \mathbb{R}" /> is the ReLU activation <img src="https://s0.wp.com/latex.php?latex=%5Csigma%28t%29+%3D+%5Cmax%5C%7Bt%2C+0%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\sigma(t) = \max\{t, 0\}" class="latex" title="\sigma(t) = \max\{t, 0\}" />, though others are possible as well.</li>
<li>For the output neuron <img src="https://s0.wp.com/latex.php?latex=o&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="o" class="latex" title="o" />, we drop the activation: <img src="https://s0.wp.com/latex.php?latex=h_%7Bo%2Cw%7D%28x%29+%3D+%5Csum_%7Bu+%5Cin+%5Cmathrm%7BIN%7D%28o%29%7D+w_%7Buo%7D+h_%7Bu%2Cw%7D%28x%29+%2B+b_o&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{o,w}(x) = \sum_{u \in \mathrm{IN}(o)} w_{uo} h_{u,w}(x) + b_o" class="latex" title="h_{o,w}(x) = \sum_{u \in \mathrm{IN}(o)} w_{uo} h_{u,w}(x) + b_o" />.</li>
</ul>
<p>Finally, let <img src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmathcal+N%2C+w%7D%28x%29+%3D+h_%7Bo%2C+w%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{\mathcal N, w}(x) = h_{o, w}(x)" class="latex" title="h_{\mathcal N, w}(x) = h_{o, w}(x)" />. This computes a real-valued function, so where we’d like to use it for classification, we do so by thresholding, and abuse the notation <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h_w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{Err}(h_w)" class="latex" title="\mathrm{Err}(h_w)" /> to mean <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28%5Cmathrm%7Bsign%7D+%5Ccirc+h_w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{Err}(\mathrm{sign} \circ h_w)" class="latex" title="\mathrm{Err}(\mathrm{sign} \circ h_w)" />.</p>
<p>Some intuition for this definition would come from verifying that:</p>
<ul>
<li>Any function <img src="https://s0.wp.com/latex.php?latex=h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h : \{\pm 1\}^n \to \{\pm 1\}" class="latex" title="h : \{\pm 1\}^n \to \{\pm 1\}" /> can be computed by a network of depth two and <img src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="2^n" class="latex" title="2^n" /> hidden neurons.</li>
<li>The parity function <img src="https://s0.wp.com/latex.php?latex=h%28x%29+%3D+%5Cprod_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h(x) = \prod_{i=1}^n x_i" class="latex" title="h(x) = \prod_{i=1}^n x_i" /> can be computed by a network of depth two and <img src="https://s0.wp.com/latex.php?latex=4n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="4n" class="latex" title="4n" /> hidden neurons. (NB: this one is a bit more challenging.)</li>
</ul>
<p>In practice, the network architecture (this DAG) is designed based on some domain knowledge, and its design can impact the predictor that’s later selected by SGD. One default architecture, useful in the absence of domain knowledge, is the multi-layer perceptron, comprised of layers of complete bipartite graphs:</p>
<figure><img width="431" alt="full_con_net" src="https://theorydish.files.wordpress.com/2019/01/full_con_net.png?w=431&amp;h=426" class="  wp-image-1479 aligncenter" height="426" />A toy “fully-connected neural network”, a.k.a. a multi-layer perceptronAnother paradigmatic architecture is a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional network</a>:<p></p>
</figure>
<figure><img width="490" alt="conv_net" src="https://theorydish.files.wordpress.com/2019/01/conv_net.png?w=490&amp;h=463" class="  wp-image-1478 aligncenter" height="463" />A toy convolutional neural network</figure>
<p>Convolutional nets capture the notion of spatial input locality in signals such as images and audio.<a href="https://theorydish.blog/feed/#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> In the toy example drawn, each clustered triple of neurons is a so-called convolution filter applied to two components below it. In image domains, convolutions filters are two-dimensional and capture responses to spatial 2-D patches of the image or of an intermediate layer.</p>
<p>Training a neural net comprises (i) initialization, and (ii) iterative optimization run until <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsign%7D%28h_w%28x%29%29+%3D+h%5E%2A%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{sign}(h_w(x)) = h^*(x)" class="latex" title="\mathrm{sign}(h_w(x)) = h^*(x)" /> for sufficiently many examples <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x" class="latex" title="x" />. The initialization step sets the starting values of the weights <img src="https://s0.wp.com/latex.php?latex=w%5E0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w^0" class="latex" title="w^0" /> at random:</p>
<blockquote><p><strong>(Glorot initialization.)</strong> Draw weights <img src="https://s0.wp.com/latex.php?latex=%5C%7Bw%5E0_%7Buv%7D%5C%7D_%7Buv%5Cin+E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{w^0_{uv}\}_{uv\in E}" class="latex" title="\{w^0_{uv}\}_{uv\in E}" /> from centered Gaussians with variance <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathrm%7BIN%7D%28v%29%7C%5E%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="|\mathrm{IN}(v)|^{-1}" class="latex" title="|\mathrm{IN}(v)|^{-1}" /> and biases <img src="https://s0.wp.com/latex.php?latex=%5C%7Bb%5E0_%7Bv%7D%5C%7D_%7Bv%5Cin+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{b^0_{v}\}_{v\in V}" class="latex" title="\{b^0_{v}\}_{v\in V}" /> from independent standard Gaussians.<a href="https://theorydish.blog/feed/#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p></blockquote>
<p>While other initialization schemes exists, this one is canonical, simple, and, as the reader can verify, satisfies <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%5E0%7D%5Cleft%5B%28h_%7Bv%2Cw%5E0%7D%28x%29%29%5E2%5Cright%5D+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{E}_{w^0}\left[(h_{v,w^0}(x))^2\right] = 1" class="latex" title="\mathbb{E}_{w^0}\left[(h_{v,w^0}(x))^2\right] = 1" /> for every neuron <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="v" class="latex" title="v" /> and input <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x \in \{\pm 1\}^n" class="latex" title="x \in \{\pm 1\}^n" />.</p>
<p>The optimization step is essentially a local search method from the initial point, using <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (SGD) or a variant thereof.<a href="https://theorydish.blog/feed/#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> To apply SGD, we need a function suitable for descent, and we’ll use the commonplace logistic loss <img src="https://s0.wp.com/latex.php?latex=%5Cell%28z%29+%3D+%5Clog_2%281%2Be%5E%7B-z%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\ell(z) = \log_2(1+e^{-z})" class="latex" title="\ell(z) = \log_2(1+e^{-z})" />, which bounds the zero-one loss <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B0-1%7D%28z%29+%3D+%5Cmathbf%7B1%7D%5Bz+%5Cle+0%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\ell^{0-1}(z) = \mathbf{1}[z \le 0]" class="latex" title="\ell^{0-1}(z) = \mathbf{1}[z \le 0]" /> from above:</p>
<figure><img width="329" alt="losses" src="https://theorydish.files.wordpress.com/2019/01/losses.png?w=329&amp;h=246" class="  wp-image-1480 aligncenter" height="246" />The logistic and zero-one losses</figure>
<p> </p>
<p>Define <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D%28w%29+%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathcal+D%7D%5Cleft%5B+%5Cell%28h_w%28x%29h%5E%2A%28x%29%29+%5Cright%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L_{\mathcal D}(w) = \mathbb{E}_{x\sim\mathcal D}\left[ \ell(h_w(x)h^*(x)) \right]" class="latex" title="L_{\mathcal D}(w) = \mathbb{E}_{x\sim\mathcal D}\left[ \ell(h_w(x)h^*(x)) \right]" />. Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BErr%7D%28h_w%29+%5Cle+L_%7B%5Cmathcal+D%7D%28w%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{Err}(h_w) \le L_{\mathcal D}(w)" class="latex" title="\mathrm{Err}(h_w) \le L_{\mathcal D}(w)" />, so finding weights <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w" class="latex" title="w" /> for which the upper bound <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L_{\mathcal D}" class="latex" title="L_{\mathcal D}" /> is small enough implies low error in turn. Meanwhile, <img src="https://s0.wp.com/latex.php?latex=L_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L_{\mathcal D}" class="latex" title="L_{\mathcal D}" /> is amenable to iterative gradient-based minimization.</p>
<p>Given samples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />, stochastic gradient descent creates an unbiased estimate of the gradient at each step <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="t" class="latex" title="t" /> by drawing a batch of i.i.d. samples <img src="https://s0.wp.com/latex.php?latex=S_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_t" class="latex" title="S_t" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />. The gradient <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+L_%7BS_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\nabla L_{S_t}" class="latex" title="\nabla L_{S_t}" /> at a point <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w" class="latex" title="w" /> can be computed efficiently by the <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> algorithm.</p>
<p>In more complete detail, our prototypical neural network training algorithm is as follows. On input a network <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal N" class="latex" title="\mathcal N" />, an iteration count <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="T" class="latex" title="T" />, a batch size <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="b" class="latex" title="b" />, and a step size <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\eta &gt; 0" class="latex" title="\eta &gt; 0" />:</p>
<p><strong>Algorithm: <em>SGDNN</em></strong></p>
<ol type="1">
<li>Let <img src="https://s0.wp.com/latex.php?latex=w%5E0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w^0" class="latex" title="w^0" /> be random weights sampled per Glorot initialization</li>
<li>For <img src="https://s0.wp.com/latex.php?latex=t+%3D+1%2C+%5Cldots%2C+T&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="t = 1, \ldots, T" class="latex" title="t = 1, \ldots, T" />:
<ol type="1">
<li>Sample a batch <img src="https://s0.wp.com/latex.php?latex=S_%7Bt%7D+%3D+%5C%7B%28x%5Et_1%2C+h%5E%2A%28x%5Et_1%29%29%2C+%5Cldots%2C+%28x%5Et_b%2C+h%5E%2A%28x%5Et_b%29%29%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_{t} = \{(x^t_1, h^*(x^t_1)), \ldots, (x^t_b, h^*(x^t_b))\}" class="latex" title="S_{t} = \{(x^t_1, h^*(x^t_1)), \ldots, (x^t_b, h^*(x^t_b))\}" />, where <img src="https://s0.wp.com/latex.php?latex=x%5Et_1%2C+%5Cldots%2C+x%5Et_b&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x^t_1, \ldots, x^t_b" class="latex" title="x^t_1, \ldots, x^t_b" /> are i.i.d. samples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />.</li>
<li>Update <img src="https://s0.wp.com/latex.php?latex=w%5Et+%5Cgets+w%5E%7Bt-1%7D+-+%5Ceta+%5Cnabla+L_%7BS_t%7D%28w%5E%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w^t \gets w^{t-1} - \eta \nabla L_{S_t}(w^{t-1})" class="latex" title="w^t \gets w^{t-1} - \eta \nabla L_{S_t}(w^{t-1})" />, where<img src="https://s0.wp.com/latex.php?latex=L_%7BS_t%7D%28w%5E%7Bt-1%7D%29+%3D+b%5E%7B-1%7D+%5Csum_%7Bi%3D1%7D%5Eb+%5Cell%28h_%7Bw%7D%28x%5Et_i%29+h%5E%2A%28x%5Et_i%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L_{S_t}(w^{t-1}) = b^{-1} \sum_{i=1}^b \ell(h_{w}(x^t_i) h^*(x^t_i))" class="latex" title="L_{S_t}(w^{t-1}) = b^{-1} \sum_{i=1}^b \ell(h_{w}(x^t_i) h^*(x^t_i))" />.</li>
</ol>
</li>
<li>Output <img src="https://s0.wp.com/latex.php?latex=w%5ET&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w^T" class="latex" title="w^T" /></li>
</ol>
<h2 id="pac-learning">PAC learning</h2>
<p>Learning a predictor from example data is a general task, and a hard one in the worst case. We cannot efficiently (i.e. in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{poly}(n)" class="latex" title="\mathrm{poly}(n)" /> time) compute, let alone learn, general functions from <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{\pm 1\}^n" class="latex" title="\{\pm 1\}^n" /> to <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{\pm 1\}" class="latex" title="\{\pm 1\}" />. In fact, any learning algorithm that is guaranteed to succeed in general (i.e. with any target predictor <img src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h^*" class="latex" title="h^*" /> over any data distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />) runs, in the worst case, in time exponential in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" />. This is true even for rather weak definitions of “success,” such as finding a predictor with error less than <img src="https://s0.wp.com/latex.php?latex=1%2F2+-+2%5E%7B-n%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="1/2 - 2^{-n/2}" class="latex" title="1/2 - 2^{-n/2}" />, i.e. one that slightly outperforms a random guess.</p>
<p>While it is impossible to efficiently learn general functions under general distributions, it might still be possible to learn efficiently under some assumptions on the target <img src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h^*" class="latex" title="h^*" /> or the distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />. Charting out such assumptions is the realm of learning theorists: by now, they’ve built up a broad catalog of function classes, and have studied the complexity of learning when the target function is in each such class. Although their primary aim has been to develop theory, the potential guidance for practice is easy to imagine: if one’s application domain happens to be modeled well by one of these easily-learnable function classes, there’s a corresponding learning algorithm to consider as well.</p>
<p>The vanilla PAC model makes no assumptions on the data distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal D" class="latex" title="\mathcal D" />, but it does assume the target <img src="https://s0.wp.com/latex.php?latex=h%5E%2A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h^*" class="latex" title="h^*" /> belongs to some simple, predefined class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal H" class="latex" title="\mathcal H" />. Formally, a <em>PAC learning problem</em> is defined by a function class<a href="https://theorydish.blog/feed/#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H+%5Csubset+%5C%7B%5Cpm+1%5C%7D%5E%7B%5C%7B%5Cpm+1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal H \subset \{\pm 1\}^{\{\pm 1\}^n}" class="latex" title="\mathcal H \subset \{\pm 1\}^{\{\pm 1\}^n}" />. A learning algorithm <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal A" class="latex" title="\mathcal A" /> <em>learns</em> the class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal H" class="latex" title="\mathcal H" /> if, whenever <img src="https://s0.wp.com/latex.php?latex=h%5E%2A+%5Cin+%5Cmathcal+H&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h^* \in \mathcal H" class="latex" title="h^* \in \mathcal H" />, and provided <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\epsilon &gt; 0" class="latex" title="\epsilon &gt; 0" />, it runs in time <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%281%2F%5Cepsilon%2C+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathrm{poly}(1/\epsilon, n)" class="latex" title="\mathrm{poly}(1/\epsilon, n)" />, and returns a function of error at most <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />, with probability at least 0.9. Note that:</p>
<ol type="1">
<li>The learning algorithm need not return a function from the learnt class.</li>
<li>The polynomial-time requirement means in particular that the learning algorithm cannot output a complete truth table, as its size would be exponential. Instead, it must output a short description of a hypothesis that can be evaluated in polynomial time.</li>
</ol>
<p>For a taste of the computational learning theory literature, here are some of the function classes studied by theorists over the years:</p>
<ol type="1">
<li><em>Linear thresholds (halfspaces):</em> functions that map a halfspace to 1 and its complement to -1. Formally, functions of the form <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Ctheta%28%5Clangle+w%2C+x+%5Crangle%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x \mapsto \theta(\langle w, x \rangle)" class="latex" title="x \mapsto \theta(\langle w, x \rangle)" /> for some <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="w \in \mathbb{R}^n" class="latex" title="w \in \mathbb{R}^n" />, where <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28z%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\theta(z) = 1" class="latex" title="\theta(z) = 1" /> when <img src="https://s0.wp.com/latex.php?latex=z+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z &gt; 0" class="latex" title="z &gt; 0" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%28z%29+%3D+-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\theta(z) = -1" class="latex" title="\theta(z) = -1" /> when <img src="https://s0.wp.com/latex.php?latex=z+%5Cle+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z \le 0" class="latex" title="z \le 0" />.</li>
<li><em>Large-margin linear thresholds:</em> for<img src="https://s0.wp.com/latex.php?latex=%5Crho%28z%29+%3D+%5Cbegin%7Bcases%7D+1+%26+z+%5Cge+1+%5C%5C+%2A+%26+-1+%5Cle+z+%5Cle+1+%5C%5C+-1+%26+z+%5Cle+-1+%5Cend%7Bcases%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\rho(z) = \begin{cases} 1 &amp; z \ge 1 \\ * &amp; -1 \le z \le 1 \\ -1 &amp; z \le -1 \end{cases}," class="latex" title="\rho(z) = \begin{cases} 1 &amp; z \ge 1 \\ * &amp; -1 \le z \le 1 \\ -1 &amp; z \le -1 \end{cases}," />the class<img src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7B+h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D+%5Cmid+h%28x%29+%3D+%5Crho%28%5Clangle+w%2Cx+%5Crangle%29+%5Ctext%7B+with+%7D+%5C%7Cw%5C%7C_2%5E2+%5Cle+%5Cmathrm%7Bpoly%7D%28n%29+%5Cright%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho(\langle w,x \rangle) \text{ with } \|w\|_2^2 \le \mathrm{poly}(n) \right\}." class="latex" title="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho(\langle w,x \rangle) \text{ with } \|w\|_2^2 \le \mathrm{poly}(n) \right\}." /></li>
<li><em>Intersections of halfspaces:</em> functions that map an intersection of polynomially many halfspaces to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="1" class="latex" title="1" /> and its complement to <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="-1" class="latex" title="-1" />.</li>
<li><em>Polynomial threshold functions:</em> thresholds of constant-degree polynomials.</li>
<li><em>Large-margin polynomial threshold functions:</em> the class</li>
</ol>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7B+h+%3A+%5C%7B%5Cpm+1%5C%7D%5En+%5Cto+%5C%7B%5Cpm+1%5C%7D+%5Cmid+h%28x%29+%3D+%5Crho%5Cleft%28+%5Csum_%7BA+%5Csubset+%5Bn%5D%2C+%7CA%7C+%5Cle+O%281%29%7D+%5Calpha_A+%5Cprod_%7Bi+%5Cin+A%7D+x_i+%5Cright%29+%5C%3B%5Ctext%7B+with+%7D%5C%3B+%5Csum_%7BA%7D+%5Calpha%5E2_A+%5Cle+%5Cmathrm%7Bpoly%7D%28n%29+%5Cright%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho\left( \sum_{A \subset [n], |A| \le O(1)} \alpha_A \prod_{i \in A} x_i \right) \;\text{ with }\; \sum_{A} \alpha^2_A \le \mathrm{poly}(n) \right\}." class="latex" title="\left\{ h : \{\pm 1\}^n \to \{\pm 1\} \mid h(x) = \rho\left( \sum_{A \subset [n], |A| \le O(1)} \alpha_A \prod_{i \in A} x_i \right) \;\text{ with }\; \sum_{A} \alpha^2_A \le \mathrm{poly}(n) \right\}." /></p>
<ol type="1">
<li><em>Decision trees</em>, <em>deterministic automata</em>, and <em><a href="https://en.wikipedia.org/wiki/Disjunctive_normal_form">DNF</a> formulas</em> of polynomial size.</li>
<li><em>Monotone conjunctions:</em> functions that, for some <img src="https://s0.wp.com/latex.php?latex=A+%5Csubset+%5Bn%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A \subset [n]" class="latex" title="A \subset [n]" /> map <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x" class="latex" title="x" /> to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="1" class="latex" title="1" /> if <img src="https://s0.wp.com/latex.php?latex=x_i+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_i = 1" class="latex" title="x_i = 1" /> for all <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i \in A" class="latex" title="i \in A" />, and to <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="-1" class="latex" title="-1" /> otherwise.</li>
<li><em>Parities:</em> functions of the form <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cprod_%7Bi+%5Cin+A%7D+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x \mapsto \prod_{i \in A} x_i" class="latex" title="x \mapsto \prod_{i \in A} x_i" /> for some <img src="https://s0.wp.com/latex.php?latex=A+%5Csubset+%5Bn%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A \subset [n]" class="latex" title="A \subset [n]" />.</li>
<li><em>Juntas:</em> functions that depend on at most <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\log(n)" class="latex" title="\log(n)" /> variables.</li>
</ol>
<p>Learning theorists look at these function classes and work to distinguish those that are efficiently learnable from those that are <em>hard</em> to learn. They establish hardness results by reduction from other computational problems that are conjectured to be hard, such as random XOR-SAT (though none today are conditioned outright on NP hardness); see for example <a href="https://arxiv.org/abs/1404.3378">these</a> <a href="https://arxiv.org/abs/1505.05800">two</a> results. Meanwhile, halfspaces are learnable by linear programming. Parities, or more generally, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{F}" class="latex" title="\mathbb{F}" />-linear functions for a field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{F}" class="latex" title="\mathbb{F}" />, are learnable by Gaussian elimination. In turn, via reductions, many other classes are efficiently learnable. This includes polynomial thresholds, decision lists, and more. To give an idea of what’s known in the literature, here is an artist’s depiction of some of what’s currently known:</p>
<figure><img src="https://theorydish.files.wordpress.com/2019/01/classes.png?w=620" alt="classes" class=" size-full wp-image-1477 aligncenter" />Learnable and conjectured hard-to-learn function classes</figure>
<p> </p>
<p>At a high-level, the upshot from all of this—and if you take away just one thing from this quick tour of PAC—is that:</p>
<blockquote><p>Barring a small handful of exceptions, all known efficiently learnable classes can be reduced to halfspaces or <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{F}" class="latex" title="\mathbb{F}" />-linear functions.</p></blockquote>
<p>Or, to put it more bluntly, <strong>the state of the art in PAC-learnability is essentially linear prediction</strong>.</p>
<h2 id="pac-analyzing-neural-nets">PAC analyzing neural nets</h2>
<p>Research in algorithms and complexity often follows these steps:</p>
<ol type="1">
<li>define a computational problem,</li>
<li>design an algorithm that solves it, and then</li>
<li>establish bounds on the resource requirements of that algorithm.</li>
</ol>
<p>A bound on the algorithm’s performance forms, in turn, a bound on the <em>computational problem’s</em> inherent complexity.</p>
<p>By contrast, we have already decided on our SGDNN algorithm, and we’d like to attain some grasp on its capabilities. So we’d like to do things in a different order:</p>
<ol type="1">
<li>define an <em>algorithm</em> (done),</li>
<li>design a computational problem to which the algorithm can be applied, and then</li>
<li>establish bounds on the resource requirements of the algorithm in solving the problem.</li>
</ol>
<p>Our computational problem will be a PAC learning problem, corresponding to a function class. For SGDNN, an ambitious function class we might consider is the class of all functions realizable by the network. But if we were to follow this approach, we would run up against the same hardness results mentioned before.</p>
<p>So instead, we’ve established the theorem stated at the top of this post. That is, that SGDNN, over a range of network configurations, learns a class that we <em>already know</em> to be learnable: large margin polynomial thresholds. Restated:</p>
<blockquote><p><strong>Theorem, again:</strong> There is a choice of SGDNN step size <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\eta" class="latex" title="\eta" /> and number of steps <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="T" class="latex" title="T" />, as well as a with parameter <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="r" class="latex" title="r" />, where <img src="https://s0.wp.com/latex.php?latex=T%2C+r+%5Cle+%5Cmathrm%7Bpoly%7D%28n%2F%5Cepsilon%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="T, r \le \mathrm{poly}(n/\epsilon)" class="latex" title="T, r \le \mathrm{poly}(n/\epsilon)" />, such that SGDNN on a multi-layer perceptron of depth between 2 and <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\log(n)" class="latex" title="\log(n)" />, and of width<a href="https://theorydish.blog/feed/#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="r" class="latex" title="r" />, learns large magin polynomials.</p></blockquote>
<p>How rich are large margin polynomials? They contain disjunctions, conjunctions, DNF and <a href="https://en.wikipedia.org/wiki/Conjunctive_normal_form">CNF</a> formulas with a constant many terms, DNF and CNF formulas with a constant many literals in each term. By corollary, SGDNN can PAC learn these classes as well. And at this point, we’ve covered a considerable fraction of the function classes known to be poly-time PAC learnable by <em>any</em> method.</p>
<p>Exceptions include constant-degree polynomial thresholds with no restriction on the coefficients, decision lists, and parities. It is well known that SGDNN cannot learn parities, and in ongoing work with Vitaly Feldman, we show that SGDNN cannot learn decision lists nor constant-degree polynomial thresholds with unrestricted coefficients. So the picture becomes more clear:</p>
<figure><img src="https://theorydish.files.wordpress.com/2019/01/classes_nn.png?w=620" alt="classes_nn" class=" size-full wp-image-1476 aligncenter" />Conjectured hard-to-learn classes, known learnable classes, and those known to be learnable by SGDNN.</figure>
<p> </p>
<p>The theorem above runs SGDNN with a multi-layer perceptron. What happens if we change the network architecture? It can be shown then that SGDNN learns a qualitatively different function class. For instance, with convolutional networks, the learnable functions include certain polynomials of <em>super-constant</em> degree.</p>
<h3 id="a-word-on-the-proof">A word on the proof</h3>
<p>The path to the theorem traverses two papers. There’s a corresponding outline for the proof.</p>
<p>The first step is to show that, with high probability, the Glorot random initialization renders the network in a state where the final hidden layer (just before the output node) is rich enough to approximate all large-margin polynomial threshold functions (LMPTs). Namely, every LMPT can be approximated by the network up to some setting of the weights that enter the output neuron (all remaining weights random). The tools for this part of the proof include (i) the connection between kernels and random features, (ii) a characterization of symmetric kernels of the sphere, and (iii) a variety of properties of Hermite polynomials. It’s described in our <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity">2016 paper</a>.</p>
<p>An upshot of this correspondence is that if we run SGD <em>only on the top layer</em> of a network, leaving the remaining weights as they were randomly initialized, we learn LMPTs. (Remember when we said that we won’t beat what a linear predictor can do? There it is again.) The second step of the proof, then, is to show that the correspondence continues to hold even if we train all the weights. In the assumed setting (e.g. provided at most logarithmic depth, sufficient width, and so forth), what’s represented in the final hidden layer changes sufficiently slowly that, over the course of SGDNN’s iterations, it <em>remains</em> rich enough to approximate all LMPTs. The final layer does the remaining work of picking out the right LMPT. The argument is in Amit’s <a href="https://papers.nips.cc/paper/6836-sgd-learns-the-conjugate-kernel-class-of-the-network">2017 paper</a>.</p>
<h2 id="pacing-up">PACing up</h2>
<p>To what extent should we be satisfied, knowing that our algorithm of interest (SGDNN) can solve a (computationally) easy problem?</p>
<p>On the positive side, we’ve managed to say something at all about neural network training in the PAC framework. Roughly speaking, some class of non-trivially layered neural networks, trained as they typically are, learns any known learnable function class that isn’t “too sensitive.” It’s also appealing that the function classes vary across different architectures.</p>
<p>On the pessimistic side, we’re confronted to a major limitation on the “function class” perspective, prevalent in PAC analysis and elsewhere in learning theory. All of the classes that SGDNN learns, <em>under the assumptions</em> touched on in this post, are so-called large-margin classes. Large-margin classes are essentially linear predictors over a <em>fixed and data-independent</em> embedding of input examples, as alluded to before. These are inherently “shallow models.”</p>
<p>That seems rather problematic in pursuing any kind of theory for learning layered networks, where the entire working premise is that a deep network uses its hidden layers to learn a representation adapted to the example domain. Our analysis—both its goal and its proof—clash with this intuition: it works out that a “shallow model” can be learned when assumptions imply that “not too much” change takes place in hidden layers. It seems that the representation learning phenomenon is what’s interesting, yet the typical PAC approach, as well as the analysis touched on in this post, all avoid capturing it.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1">Here <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" /> is the dimension of the instance space.<a href="https://theorydish.blog/feed/#fnref1"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn2">For instance, ReLU activations, of the form <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cmax%5C%7Bx%2C0%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x \mapsto \max\{x,0\}" class="latex" title="x \mapsto \max\{x,0\}" />.<a href="https://theorydish.blog/feed/#fnref2"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn3">Recurrent networks allow for cycles, but in this post we stick to DAGs.<a href="https://theorydish.blog/feed/#fnref3"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn4">Convolutional networks often also constrain subsets of their weights to be equal; that turns out not to bear much on this post.<a href="https://theorydish.blog/feed/#fnref4"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn5">Although not essential to the results described, it also simplifies this post to zero the weights on edges incident to the output node as part of the initialization.<a href="https://theorydish.blog/feed/#fnref5"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn6"><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants">Variants of SGD</a> are used in practice, including algorithms used elsewhere in optimization (e.g. <a href="https://distill.pub/2017/momentum/">SGD with momentum</a>, <a href="http://www.jmlr.org/papers/v12/duchi11a.html">AdaGrad</a>) or techniques developed more specifically for neural nets (e.g. RMSprop, <a href="https://arxiv.org/abs/1412.6980">Adam</a>, <a href="https://arxiv.org/abs/1502.03167">batch norm</a>). We’ll stick to plain SGD.<a href="https://theorydish.blog/feed/#fnref6"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn7">More accurately, a sequence of function classes <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+H_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal H_n" class="latex" title="\mathcal H_n" /> for <img src="https://s0.wp.com/latex.php?latex=n+%3D+1%2C+2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n = 1, 2, \ldots" class="latex" title="n = 1, 2, \ldots" />.<a href="https://theorydish.blog/feed/#fnref7"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
<li id="fn8">The width of a multi-layer perceptron is the number of neurons in each hidden layer.<a href="https://theorydish.blog/feed/#fnref8"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" alt="↩" style="height: 1em;" class="wp-smiley" /></a></li>
</ol>
</section></div>







<p class="date">
by amitdanielymailhujiacil <a href="https://theorydish.blog/2019/01/04/on-pac-analysis-and-deep-neural-networks/"><span class="datestr">at January 04, 2019 03:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
