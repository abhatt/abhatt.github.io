<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 17, 2019 06:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06917">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06917">Separating Structure from Noise in Large Graphs Using the Regularity Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Marco Fiorucci. Francesco Pelosin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pelillo:Marcello.html">Marcello Pelillo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06917">PDF</a><br /><b>Abstract: </b>How can we separate structural information from noise in large graphs? To
address this fundamental question, we propose a graph summarization approach
based on Szemer\'edi's Regularity Lemma, a well-known result in graph theory,
which roughly states that every graph can be approximated by the union of a
small number of random-like bipartite graphs called `regular pairs'. Hence, the
Regularity Lemma provides us with a principled way to describe the essential
structure of large graphs using a small amount of data. Our paper has several
contributions: (i) We present our summarization algorithm which is able to
reveal the main structural patterns in large graphs. (ii) We discuss how to use
our summarization framework to efficiently retrieve from a database the top-k
graphs that are most similar to a query graph. (iii) Finally, we evaluate the
noise robustness of our approach in terms of the reconstruction error and the
usefulness of the summaries in addressing the graph search task.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06917"><span class="datestr">at May 17, 2019 01:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06896">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06896">Target Set in Threshold Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zehmakan:Ahad_N=.html">Ahad N. Zehmakan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06896">PDF</a><br /><b>Abstract: </b>Consider a graph $G$ and an initial coloring, where each node is blue or red.
In each round, all nodes simultaneously update their color based on a
predefined rule. In a threshold model, a node becomes blue if a certain number
or fraction of its neighbors are blue and red otherwise. What is the minimum
number of nodes which must be blue initially so that the whole graph becomes
blue eventually? We study this question for graphs which have expansion
properties, parameterized by spectral gap, in particular the
Erd\H{o}s-R\'{e}nyi random graph and random regular graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06896"><span class="datestr">at May 17, 2019 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06895">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06895">Switches in Eulerian graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zehmakan:Ahad_N=.html">Ahad N. Zehmakan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nummenpalo:Jerri.html">Jerri Nummenpalo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilz:Alexander.html">Alexander Pilz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolleb=Graf:Daniel.html">Daniel Wolleb-Graf</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06895">PDF</a><br /><b>Abstract: </b>We show that the graph transformation problem of turning a simple graph into
an Eulerian one by a minimum number of single edge switches is NP-hard.
Further, we show that any simple Eulerian graph can be transformed into any
other such graph by a sequence of 2-switches (i.e., exchange of two edge
pairs), such that every intermediate graph is also Eulerian. However, finding
the shortest such sequence also turns out to be an NP-hard problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06895"><span class="datestr">at May 17, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06894">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06894">Fake news and rumors: a trigger for proliferation or fading away</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zehmakan:Ahad_N=.html">Ahad N. Zehmakan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galam:Serge.html">Serge Galam</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06894">PDF</a><br /><b>Abstract: </b>The dynamics of fake news and rumor spreading is investigated using a model
with three kinds of agents who are respectively the Seeds, the Agnostics and
the Others. While Seeds are the ones who start spreading the rumor being
adamantly convinced of its truth, Agnostics reject any kind of rumor and do not
believe in conspiracy theories. In between, the Others constitute the main part
of the community. While Seeds are always Believers and Agnostics are always
Indifferents, Others can switch between being Believer and Indifferent
depending on who they are discussing with. The underlying driving dynamics is
implemented via local updates of randomly formed groups of agents. In each
group, an Other turns into a Believer as soon as $m$ or more Believers are
present in the group. However, since some Believers may lose interest in the
rumor as time passes by, we add a flipping fixed rate $0&lt;d&lt;1$ from Believers
into Indifferents. Rigorous analysis of the associated dynamics reveals that
switching from $m=1$ to $m\ge2$ triggers a drastic qualitative change in the
spreading process. When $m=1$ even a small group of Believers may manage to
convince a large part of the community very quickly. In contrast, for $m\ge 2$,
even a substantial fraction of Believers does not prevent the rumor dying out
after a few update rounds. Our results provide an explanation on why a given
rumor spreads within a social group and not in another, and also why some
rumors will not spread in neither groups.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06894"><span class="datestr">at May 17, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06783">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06783">Time-Energy Tradeoffs for Evacuation by Two Robots in the Wireless Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Czyzowicz:Jurek.html">Jurek Czyzowicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiou:Konstantinos.html">Konstantinos Georgiou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Killick:Ryan.html">Ryan Killick</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kranakis:Evangelos.html">Evangelos Kranakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krizanc:Danny.html">Danny Krizanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lafond:Manuel.html">Manuel Lafond</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Lata.html">Lata Narayanan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Opatrny:Jaroslav.html">Jaroslav Opatrny</a>, Sunil Shende <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06783">PDF</a><br /><b>Abstract: </b>Two robots stand at the origin of the infinite line and are tasked with
searching collaboratively for an exit at an unknown location on the line. They
can travel at maximum speed $b$ and can change speed or direction at any time.
The two robots can communicate with each other at any distance and at any time.
The task is completed when the last robot arrives at the exit and evacuates. We
study time-energy tradeoffs for the above evacuation problem. The evacuation
time is the time it takes the last robot to reach the exit. The energy it takes
for a robot to travel a distance $x$ at speed $s$ is measured as $xs^2$. The
total and makespan evacuation energies are respectively the sum and maximum of
the energy consumption of the two robots while executing the evacuation
algorithm.
</p>
<p>Assuming that the maximum speed is $b$, and the evacuation time is at most
$cd$, where $d$ is the distance of the exit from the origin, we study the
problem of minimizing the total energy consumption of the robots. We prove that
the problem is solvable only for $bc \geq 3$. For the case $bc=3$, we give an
optimal algorithm, and give upper bounds on the energy for the case $bc&gt;3$.
</p>
<p>We also consider the problem of minimizing the evacuation time when the
available energy is bounded by $\Delta$. Surprisingly, when $\Delta$ is a
constant, independent of the distance $d$ of the exit from the origin, we prove
that evacuation is possible in time $O(d^{3/2}\log d)$, and this is optimal up
to a logarithmic factor. When $\Delta$ is linear in $d$, we give upper bounds
on the evacuation time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06783"><span class="datestr">at May 17, 2019 01:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06706">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06706">Efficiently Generating Geometric Inhomogeneous and Hyperbolic Random Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katzmann:Maximilian.html">Maximilian Katzmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyer:Ulrich.html">Ulrich Meyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penschuck:Manuel.html">Manuel Penschuck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weyand:Christopher.html">Christopher Weyand</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06706">PDF</a><br /><b>Abstract: </b>Hyperbolic random graphs (HRG) and geometric inhomogeneous random graphs
(GIRG) are two similar generative network models that were designed to resemble
complex real world networks. In particular, they have a power-law degree
distribution with controllable exponent $\beta$, and high clustering that can
be controlled via the temperature $T$.
</p>
<p>We present the first implementation of an efficient GIRG generator running in
expected linear time. Besides varying temperatures, it also supports underlying
geometries of higher dimensions. It is capable of generating graphs with ten
million edges in under a second on commodity hardware. The algorithm can be
adapted to HRGs. Our resulting implementation is the fastest sequential HRG
generator, despite the fact that we support non-zero temperatures. Though
non-zero temperatures are crucial for many applications, most existing
generators are restricted to $T = 0$. Our generators support parallelization,
although this is not the focus of this paper. We note that our generators draw
from the correct probability distribution, i.e., they involve no approximation.
</p>
<p>Besides the generators themselves, we also provide an efficient algorithm to
determine the non-trivial dependency between the average degree of the
resulting graph and the input parameters of the GIRG model. This makes it
possible to specify the expected average degree as input.
</p>
<p>Moreover, we investigate the differences between HRGs and GIRGs, shedding new
light on the nature of the relation between the two models. Although HRGs
represent, in a certain sense, a special case of the GIRG model, we find that a
straight-forward inclusion does not hold in practice. However, the difference
is negligible for most use cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06706"><span class="datestr">at May 17, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06626">Two-sided profile-based optimality in the stable marriage problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cooper:Frances.html">Frances Cooper</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manlove:David.html">David Manlove</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06626">PDF</a><br /><b>Abstract: </b>We study the problem of finding "fair" stable matchings in the Stable
Marriage problem with Incomplete lists (SMI). In particular, we seek stable
matchings that are optimal with respect to profile, which is a vector that
indicates the number of agents who have their first-, second-, third-choice
partner, etc. In a rank maximal stable matching, the maximum number of agents
have their first-choice partner, and subject to this, the maximum number of
agents have their second-choice partner, etc., whilst in a generous stable
matching $M$, the minimum number of agents have their $d$th-choice partner, and
subject to this, the minimum number of agents have their $(d-1)$th-choice
partner, etc., where $d$ is the maximum rank of an agent's partner in $M$.
Irving et al. presented an $O(n^5\log n)$ algorithm for finding a rank-maximal
stable matching, which can be adapted easily to the generous stable matching
case, where $n$ is the number of men / women. An $O(n^{4.5})$ algorithm for the
rank-maximal stable problem was later given by Feder. However these approaches
involve the use of weights that are in general exponential in $n$, potentially
leading to inaccuracies or memory issues upon implementation. In this paper we
present an $O(n^5\log n)$ algorithm for finding a rank-maximal stable matching
using an approach that involves weights that are polynomially-bounded in $n$.
We show how to adapt our algorithm for the generous case to run in O$(n^2d^3
\log n)$ time. Additionally we conduct an empirical evaluation to compare
various measures over many different types of "fair" stable matchings,
including rank-maximal, generous, egalitarian, sex-equal and median stable
matchings. In particular, we observe that a generous stable matching is
typically considerably closer than a rank-maximal stable matching in terms of
the egalitarian and sex-equality optimality criteria.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06626"><span class="datestr">at May 17, 2019 01:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06503">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06503">Parameterized Inapproximability of Exact Cover and Nearest Codeword</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Patrick.html">Patrick Lin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06503">PDF</a><br /><b>Abstract: </b>The $k$-ExactCover problem is a parameterized version of the ExactCover
problem, in which we are given a universe $U$, a collection $S$ of subsets of
$U$, and an integer $k$, and the task is to determine whether $U$ can be
partitioned into $k$ sets in $S$. This is a natural extension of the
well-studied SetCover problem; though in the parameterized regime we know it to
be $W[1]$-complete in the exact case, its parameterized complexity with respect
to approximability is not well understood.
</p>
<p>We prove that, assuming ETH, for some $\gamma &gt; 0$ there is no time $f(k)
\cdot N^{\gamma k}$ algorithm that can, given a $k$-ExactCover instance $I$,
distinguish between the case where $I$ has an exact cover of size $k$ and the
case where every set cover of $I$ has size at least $\frac14
\sqrt[k]{\frac{\log N}{\log \log N}}$. This rules out even more than FPT
algorithms, and additionally rules out any algorithm whose approximation ratio
depends only on the parameter $k$. By assuming SETH, we instead improve the
lower bound to requiring time $f(k) \cdot N^{k - \varepsilon}$, for any
$\varepsilon &gt; 0$.
</p>
<p>In this work we also extend the inapproximability result to the
$k$-Nearest-Codeword ($k$-NCP) problem. Specifically, given a generator matrix
$A \in \mathbb{F}_2^{m \times n}$, a vector $y \in \mathbb{F}_2^m$, and the
parameter $k$, we show that it is hard to distinguish between the case where
there exists a codeword with distance at most $k$ from $y$ and the case where
every codeword has distance at least $\frac18 \sqrt[k]{\frac{\log N}{\log \log
N}}$ from $y$. This improves the best known parameterized inapproximability
result, which rules out approximations with a factor of $\text{poly} (\log k)$,
but requires us to assume ETH instead of $W[1] \neq FPT$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06503"><span class="datestr">at May 17, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06394">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06394">Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel $k$-means Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernandez:Manuel.html">Manuel Fernandez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yasuda:Taisuke.html">Taisuke Yasuda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06394">PDF</a><br /><b>Abstract: </b>We present tight lower bounds on the number of kernel evaluations required to
approximately solve kernel ridge regression (KRR) and kernel $k$-means
clustering (KKMC) on $n$ input points. For KRR, our bound for relative error
approximation to the minimizer of the objective function is
$\Omega(nd_{\mathrm{eff}}^\lambda/\varepsilon)$ where
$d_{\mathrm{eff}}^\lambda$ is the effective statistical dimension, which is
tight up to a $\log(d_{\mathrm{eff}}^\lambda/\varepsilon)$ factor. For KKMC,
our bound for finding a $k$-clustering achieving a relative error approximation
of the objective function is $\Omega(nk/\varepsilon)$, which is tight up to a
$\log(k/\varepsilon)$ factor. Our KRR result resolves a variant of an open
question of El Alaoui and Mahoney, asking whether the effective statistical
dimension is a lower bound on the sampling complexity or not. Furthermore, for
the important practical case when the input is a mixture of Gaussians, we
provide a KKMC algorithm which bypasses the above lower bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06394"><span class="datestr">at May 17, 2019 01:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.02110">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.02110">On the Entanglement Cost of One-Shot Compression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hadiashar:Shima_Bab.html">Shima Bab Hadiashar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayak:Ashwin.html">Ashwin Nayak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02110">PDF</a><br /><b>Abstract: </b>We revisit the task of compressing an ensemble of quantum states in the
one-shot setting. The protocols achieving the best compression use shared
entanglement that may be much larger than the original message, while others
(with potentially larger communication cost) have entanglement cost bounded by
the message length. This motivates the question as to whether entanglement is
truly necessary for compression, and if so, how much of it is needed.
</p>
<p>Motivated by questions in communication complexity, we lift certain
restrictions imposed on compression protocols in tasks such as state-splitting
and channel simulation. We show that an ensemble constructed by Jain,
Radhakrishnan, and Sen (ICALP'03) saturates the known bounds on the sum of
communication and entanglement costs, even with the relaxed compression
protocols we study.
</p>
<p>The ensemble and the associated one-way communication protocol have several
remarkable properties. The ensemble is incompressible by more than a constant
number of qubits without entanglement, even when constant error is allowed.
Moreover, in the presence of entanglement, the communication cost of
compression can be arbitrarily smaller than the entanglement cost. The quantum
information cost of the protocol can thus be arbitrarily smaller than the cost
of compression without entanglement. The ensemble can also be used to show the
impossibility of reducing, via compression, the entanglement used in two-party
protocols for computing Boolean functions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.02110"><span class="datestr">at May 17, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1801.05965">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1801.05965">The Complexity of Combinations of Qualitative Constraint Satisfaction Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodirsky:Manuel.html">Manuel Bodirsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Greiner:Johannes.html">Johannes Greiner</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1801.05965">PDF</a><br /><b>Abstract: </b>The CSP of a first-order theory $T$ is the problem of deciding for a given
finite set $S$ of atomic formulas whether $T \cup S$ is satisfiable. Let $T_1$
and $T_2$ be two theories with countably infinite models and disjoint
signatures. Nelson and Oppen presented conditions that imply decidability (or
polynomial-time decidability) of $\mathrm{CSP}(T_1 \cup T_2)$ under the
assumption that $\mathrm{CSP}(T_1)$ and $\mathrm{CSP}(T_2)$ are decidable (or
polynomial-time decidable). We show that for a large class of
$\omega$-categorical theories $T_1, T_2$ the Nelson-Oppen conditions are not
only sufficient, but also necessary for polynomial-time tractability of
$\mathrm{CSP}(T_1 \cup T_2)$ (unless P=NP).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1801.05965"><span class="datestr">at May 17, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4244">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">Online Optimization Post 4: Regularity Lemmas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 We now discuss how to view proofs of certain <em>regularity lemmas</em> as applications of the FTRL methodology.</p>
<p>
The Szemeredi Regularity Lemma states (in modern language) that every dense graph is well approximate by a graph with a very simple structure, made of the (edge-disjoint) union of a constant number of weighted complete bipartite subgraphs. The notion of approximation is a bit complicated to describe, but it enables the proof of <em>counting lemmas</em>, which show that, for example, the number of triangles in the original graph is well approximated by the (appropriately weighted) number of triangles in the approximating graph. </p>
<p>
Analogous regularity lemmas, in which an arbitrary object is approximated by a low-complexity object, have been proved for hypergraphs, for subsets of abelian groups (for applications to additive combinatorics), in an analytic setting (for applications to graph limits) and so on. </p>
<p>
The <em>weak regularity lemma</em> of Frieze and Kannan provides, as the name suggests, a weaker kind of approximation than the one promised by Szemeredi’s lemma, but one that is achievable with a graph that has a much smaller number of pieces. If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> is the “approximation error” that one is willing to tolerate, Szemeredi’s lemma constructs a graph that is the union of a <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B2%5E%7B%5Cvdots%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{2^{\vdots}}}" class="latex" title="{2^{2^{\vdots}}}" /> weighted complete bipartite subgraphs where the height of the tower of exponentials is polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />. In the Frieze-Kannan construction, that number is cut down to a single exponential <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{O(1/\epsilon^2)}}" class="latex" title="{2^{O(1/\epsilon^2)}}" />. This result too can be generalized to graph limits, subsets of groups, and so on.</p>
<p>
With Tulsiani and Vadhan, we proved an abstract version of the Frieze-Kannan lemma (which can be applied to graphs, functions, distributions, etc.) in which the “complexity” of the approximation is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\epsilon^2)}" class="latex" title="{O(1/\epsilon^2)}" />. In the graph case, the approximating graph is still the union of <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{O(1/\epsilon^2)}}" class="latex" title="{2^{O(1/\epsilon^2)}}" /> complete bipartite subgraphs, but it has a more compact representation. One consequence of this result is that for every high-min-entropy distribution <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" />, there is an efficiently samplable distribution with the same min-entropy as <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" />, that is indistinguishable from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal D}" class="latex" title="{\cal D}" />. Such a result could be taken to be a proof that what GANs attempt to achieve is possible in principle, except that our result requires an unrealistically high entropy (and we achieve “efficient samplability” and “indistinguishability” only in a weak sense).</p>
<p>
All these results are proved with a similar strategy: one starts from a trivial approximator, for example the empty graph, and then repeats the following iteration: if the current approximator achieves the required approximation, then we are done; otherwise take a counterexample, and modify the approximator using the counterexample. Then one shows that: </p>
<ul>
<li> The number of iterations is bounded, by keeping track of an appropriate potential function;
</li><li> The “complexity” of the approximator does not increase too much from iteration to iteration.
</li></ul>
<p>
Typically, the number of iterations is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\epsilon^2)}" class="latex" title="{O(1/\epsilon^2)}" />, and the difference between the various results is given by whether at each iteration the “complexity” increases exponentially, or by a multiplicative factor, or by an additive term.</p>
<p>
Like in the post on pseudorandom constructions, one can view such constructions as an online game between a “builder” and an “inspector,” except that now the online optimization algorithm will play the role of the builder, and the inspector is the one acting as an adversary. The <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\epsilon^2)}" class="latex" title="{O(1/\epsilon^2)}" /> bound on the number of rounds comes from the fact that the online optimization algorithms that we have seen so far achieve amortized error per round <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\sqrt T)}" class="latex" title="{O(1/\sqrt T)}" /> after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> rounds, so it takes <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\epsilon^2)}" class="latex" title="{O(1/\epsilon^2)}" /> rounds for the error bound to go below <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />.</p>
<p>
We will see that the abstract weak regularity lemma of my paper with Tulsiani and Vadhan (and hence the graph weak regularity lemma of Frieze and Kannan) can be immediately deduced from the theory developed in the previous post. </p>
<p>
When I was preparing these notes, I was asked by several people if the same can be done for Szemeredi’s lemma. I don’t see a natural way of doing that. For such results, one should maybe use the online optimization techniques as a guide rather than as a black box. In general, iterative arguments (in which one constructs an object through a series of improvements) require the choice of a potential function, and an argument about how much the potential function changes at every step. The power of the FTRL method is that it creates the potential function and a big part of the analysis automatically and, even where it does not work directly, it can serve as an inspiration. </p>
<p>
One could imagine a counterfactual history in which people first proved the weak regularity lemma using online optimization out of the box, as we do in this post, and then decided to try and use an L2 potential function and an iterative method to get the Szemeredi lemma, subsequently trying to see what happens if the potential function is entropy, thus discovering Jacob Fox’s major improvement on the “triangle removal lemma,” which involves the construction of an approximator that just approximates the number of triangles.</p>
<p>
<span id="more-4244"></span></p>
<p>
</p><p><b>1. A “vanilla” weak regularity lemma </b></p>
<p></p><p>
Frieze and Kannan proved the following basic result about graph approximations, which has a number of algorithmic applications. If <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> is a set of vertices which is understood from the context, and <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B \subseteq V}" class="latex" title="{A,B \subseteq V}" /> are disjoint subsets of vertices, then let <img src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D+%3D+%7B%5Cbf+1%7D_A+%5Ccdot+%7B%5Cbf+1%7D_B%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K_{A,B} = {\bf 1}_A \cdot {\bf 1}_B^T}" class="latex" title="{K_{A,B} = {\bf 1}_A \cdot {\bf 1}_B^T}" />, that is, the boolean matrix such that <img src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D+%28i%2Cj%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K_{A,B} (i,j) = 1}" class="latex" title="{K_{A,B} (i,j) = 1}" /> iff <img src="https://s0.wp.com/latex.php?latex=%7Bi%5Cin+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i\in A}" class="latex" title="{i\in A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bj%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j\in B}" class="latex" title="{j\in B}" />.</p>
<p>
The <em>cut norm</em> of a matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M+%7C%7C_%7B%5Csquare%7D+%3A%3D+%5Cmax_%7BA%2CB%7D+%7C+%5Clangle+M%2C+K_%7BA%2CB%7D+%5Crangle+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || M ||_{\square} := \max_{A,B} | \langle M, K_{A,B} \rangle | " class="latex" title="\displaystyle  || M ||_{\square} := \max_{A,B} | \langle M, K_{A,B} \rangle | " /></p>
<p>
In the following we will identify a graph with its adjacency matrix.</p>
<blockquote><p><b>Theorem 1</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=(V,E)}" class="latex" title="{G=(V,E)}" /> be an graph on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> vertices and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" /> be an approximation parameter. </em></p><em>
<p>
Then there are sets <img src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" title="{A_1,B_1,\ldots,A_T,B_T}" /> and scalars <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha_1,\ldots,\alpha_T}" class="latex" title="{\alpha_1,\ldots,\alpha_T}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BT+%5Cleq+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T \leq O(1/\epsilon^2)}" class="latex" title="{T \leq O(1/\epsilon^2)}" />, such that if we define</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H%3A%3D+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+K_%7BA_i%2CB_i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  H:= \sum_{i=1}^T \alpha_i K_{A_i,B_i} " class="latex" title="\displaystyle  H:= \sum_{i=1}^T \alpha_i K_{A_i,B_i} " /></p>
<p> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+G+-+H+%7C%7C_%7B%5Csquare%7D+%5Cleq+%5Cepsilon+n%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " class="latex" title="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
We will prove the following more general version.</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> be a set, <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: X \rightarrow [0,1]}" class="latex" title="{g: X \rightarrow [0,1]}" /> be a bounded function, <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> be a family of functions mapping <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[0,1]}" class="latex" title="{[0,1]}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> be an approximation parameter. Then there are functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> and scalars <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha_1,\ldots,\alpha_T}" class="latex" title="{\alpha_1,\ldots,\alpha_T}" />, with <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = O(1/\epsilon^2)}" class="latex" title="{T = O(1/\epsilon^2)}" />, such that if we define </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h+%3A%3D+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+f_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  h := \sum_{i=1}^T \alpha_i f_i " class="latex" title="\displaystyle  h := \sum_{i=1}^T \alpha_i f_i " /></p>
<p> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " class="latex" title="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
We could also, with the same proof, argue about a possibly infinite set <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> with a measure <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu}" class="latex" title="{\mu}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu(X)}" class="latex" title="{\mu(X)}" /> is finite, and, after defining the inner product</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+f%2C+g+%5Crangle+%3A%3D+%5Cint_X+f%5Ccdot+g%5C+d+%5Cmu+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle f, g \rangle := \int_X f\cdot g\ d \mu \ , " class="latex" title="\displaystyle  \langle f, g \rangle := \int_X f\cdot g\ d \mu \ , " /></p>
<p>
we could prove the same conclusion of the theorem, with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%5Ccdot+%5Cmu%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon \cdot \mu(X)}" class="latex" title="{\epsilon \cdot \mu(X)}" /> instead of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon |X|}" class="latex" title="{\epsilon |X|}" /> as an error bound.</p>
<p>
Here is the proof: run the FTRL algorithm with L2-squared regularizer in the setup in which the space of solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set of all functions <img src="https://s0.wp.com/latex.php?latex=%7B+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ X \rightarrow {\mathbb R}}" class="latex" title="{ X \rightarrow {\mathbb R}}" /> and the loss functions are linear. Every time the algorithm proposes a solution <img src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_t}" class="latex" title="{h_t}" />, if there is a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t \in {\cal F}}" class="latex" title="{f_t \in {\cal F}}" /> such that either <img src="https://s0.wp.com/latex.php?latex=%7B+%5Clangle+h_t+-+g+%2C+f_t+%5Crangle+%3E+%5Cepsilon%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ \langle h_t - g , f_t \rangle &gt; \epsilon|X|}" class="latex" title="{ \langle h_t - g , f_t \rangle &gt; \epsilon|X|}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B+%5Clangle+h_t+-+g+%2C+f_t+%5Crangle+%3C+-+%5Cepsilon%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ \langle h_t - g , f_t \rangle &lt; - \epsilon|X|}" class="latex" title="{ \langle h_t - g , f_t \rangle &lt; - \epsilon|X|}" />, the adversary will pick, respectively, <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B-f_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-f_t}" class="latex" title="{-f_t}" /> as a loss function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" />. When the adversary has no such choice, we stop and the function <img src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_t}" class="latex" title="{h_t}" /> is our desired approximation.</p>
<p>
First of all, let us analyze the number of rounds. Here the maximum norm of the functions in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7B%7CX%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt {|X|}}" class="latex" title="{\sqrt {|X|}}" />, so after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> rounds we have the regret bound</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+h+%3A+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t%2C+h_t+-+h+%5Crangle+%5Cleq+%5Csqrt%7B%7CX%7C%7D+%5Ccdot+%7C%7C+h+%7C%7C+%5Ccdot+%5Csqrt%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall h : \ \ \sum_{t=1}^T \langle \ell_t, h_t - h \rangle \leq \sqrt{|X|} \cdot || h || \cdot \sqrt{2T} " class="latex" title="\displaystyle  \forall h : \ \ \sum_{t=1}^T \langle \ell_t, h_t - h \rangle \leq \sqrt{|X|} \cdot || h || \cdot \sqrt{2T} " /></p>
<p> Now let us consider <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to be our offline solution: we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cepsilon+T+%7CX%7C+%3C+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t%2C+h_t+-+g+%5Crangle+%5Cleq+%7CX%7C+%5Ccdot+%5Csqrt%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \epsilon T |X| &lt; \sum_{t=1}^T \langle \ell_t, h_t - g \rangle \leq |X| \cdot \sqrt{2T} " class="latex" title="\displaystyle  \epsilon T |X| &lt; \sum_{t=1}^T \langle \ell_t, h_t - g \rangle \leq |X| \cdot \sqrt{2T} " /></p>
<p> which implies </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++T+%3C+%5Cfrac+2%7B%5Cepsilon%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  T &lt; \frac 2{\epsilon^2} " class="latex" title="\displaystyle  T &lt; \frac 2{\epsilon^2} " /></p>
<p> Finally, recall that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h_T+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT-1%7D+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  h_T = \sum_{t=1}^{T-1} - \frac 1 {2c} \ell_t " class="latex" title="\displaystyle  h_T = \sum_{t=1}^{T-1} - \frac 1 {2c} \ell_t " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> is the scaling constant in the definition of the regularizer (<img src="https://s0.wp.com/latex.php?latex=%7B1%2F2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/2c}" class="latex" title="{1/2c}" /> is of order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> when <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is order of <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon^2}" class="latex" title="{1/\epsilon^2}" />), and so our final approximator <img src="https://s0.wp.com/latex.php?latex=%7Bh_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_T}" class="latex" title="{h_T}" /> computed at the last round is a weighted sum of functions from <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />.</p>
<p>
</p><p><b>2. The weak regularity lemma </b></p>
<p></p><p>
Frieze and Kannan’s weak regularity lemma has the following form.</p>
<blockquote><p><b>Theorem 3</b> <em><a name="th.fk"></a> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=(V,E)}" class="latex" title="{G=(V,E)}" /> be an graph on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> vertices and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" /> be an approximation parameter. </em></p><em>
<p>
Then there is a partition of <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> into <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 2^{O(1/\epsilon^2)}}" class="latex" title="{k = 2^{O(1/\epsilon^2)}}" /> sets <img src="https://s0.wp.com/latex.php?latex=%7BS_1%2C%5Cldots%2CS_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1,\ldots,S_k}" class="latex" title="{S_1,\ldots,S_k}" />, and there are bounded weights <img src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+p_%7Bi%2Cj%7D+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0\leq p_{i,j} \leq 1}" class="latex" title="{0\leq p_{i,j} \leq 1}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj+%5Cin+%5C%7B1%2C%5Cldots%2C+k%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j \in \{1,\ldots, k\}}" class="latex" title="{i,j \in \{1,\ldots, k\}}" /> such that if we defined the weighted graph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> where the weight of the edge <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(u,v)}" class="latex" title="{(u,v)}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_{i,j}}" class="latex" title="{p_{i,j}}" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bu%5Cin+S_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u\in S_i}" class="latex" title="{u\in S_i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%5Cin+S_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v\in S_j}" class="latex" title="{v\in S_j}" />, then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+G+-+H+%7C%7C_%7B%5Csquare%7D+%5Cleq+%5Cepsilon+n%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " class="latex" title="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
Notice that if we did not require the weights to be between 0 and 1 then the result of the previous section can also be cast in the above language, because we can take the partition <img src="https://s0.wp.com/latex.php?latex=%7BS_1%2C%5Cldots%2CS_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1,\ldots,S_k}" class="latex" title="{S_1,\ldots,S_k}" /> to be the “Sigma-algebra generated by” the sets <img src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" title="{A_1,B_1,\ldots,A_T,B_T}" />.</p>
<p>
For a scalar <img src="https://s0.wp.com/latex.php?latex=%7Bz%5Cin+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z\in {\mathbb R}}" class="latex" title="{z\in {\mathbb R}}" />, let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tau(z)}" class="latex" title="{\tau(z)}" /> be defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctau%28z%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Brl%7D+0+%26+%5Cmbox%7B+if+%7D+z+%3C0%5C%5C+z+%26+%5Cmbox%7B+if+%7D+0%5Cleq+z+%5Cleq+1%5C%5C+1+%26+%5Cmbox%7B+if+%7D+z+%3E+1+%5Cend%7Barray%7D+%5Cright.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \tau(z) = \left\{ \begin{array}{rl} 0 &amp; \mbox{ if } z &lt;0\\ z &amp; \mbox{ if } 0\leq z \leq 1\\ 1 &amp; \mbox{ if } z &gt; 1 \end{array} \right. " class="latex" title="\displaystyle  \tau(z) = \left\{ \begin{array}{rl} 0 &amp; \mbox{ if } z &lt;0\\ z &amp; \mbox{ if } 0\leq z \leq 1\\ 1 &amp; \mbox{ if } z &gt; 1 \end{array} \right. " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tau}" class="latex" title="{\tau}" /> stands for <em>t</em>runcation. Note that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tau(z)}" class="latex" title="{\tau(z)}" /> is the L2 projection of <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> on <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[0,1]}" class="latex" title="{[0,1]}" />.</p>
<p>
Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.fk">3</a> is a special case of the following result, proved in our paper with Tulsiani and Vadhan. </p>
<blockquote><p><b>Theorem 4</b> <em><a name="th.ttv"></a> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> be a set, <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: X \rightarrow [0,1]}" class="latex" title="{g: X \rightarrow [0,1]}" /> be a bounded function, <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> be a family of functions mapping <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[0,1]}" class="latex" title="{[0,1]}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> be an approximation parameter. Then there are functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> and scalars <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha_1,\ldots,\alpha_T}" class="latex" title="{\alpha_1,\ldots,\alpha_T}" />, with <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = O(1/\epsilon^2)}" class="latex" title="{T = O(1/\epsilon^2)}" />, such that if we define </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h+%3A%3D+%5Ctau%5Cleft+%28+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+f_i+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  h := \tau\left ( \sum_{i=1}^T \alpha_i f_i \right) " class="latex" title="\displaystyle  h := \tau\left ( \sum_{i=1}^T \alpha_i f_i \right) " /></p>
<p> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " class="latex" title="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
To prove Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a> we play the same online game as in the previous section: the online algorithm proposes a solution <img src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_t}" class="latex" title="{h_t}" />; if <img src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| }" class="latex" title="{\forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| }" /> then we stop and output <img src="https://s0.wp.com/latex.php?latex=%7Bh%3Dh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h=h_t}" class="latex" title="{h=h_t}" />, otherwise we let the loss function be a function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" /> such that either <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B-%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-\ell_t}" class="latex" title="{-\ell_t}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+g-+h_t+%5Crangle+%5Cgeq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , g- h_t \rangle \geq \epsilon |X| " class="latex" title="\displaystyle  \langle \ell_t , g- h_t \rangle \geq \epsilon |X| " /></p>
<p>
The only difference is that we use the FTRL algorithm with L2 regularizer that has the set feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> defined to be the set of all functions <img src="https://s0.wp.com/latex.php?latex=%7Bh+%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h : X \rightarrow [0,1]}" class="latex" title="{h : X \rightarrow [0,1]}" /> rather than the set of all functions <img src="https://s0.wp.com/latex.php?latex=%7Bh%3A+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h: X \rightarrow {\mathbb R}}" class="latex" title="{h: X \rightarrow {\mathbb R}}" />. Then each function <img src="https://s0.wp.com/latex.php?latex=%7Bh_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_T}" class="latex" title="{h_T}" /> is the projection to <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bt%3D1%7D%5E%7BT-1%7D+-+%5Cfrac+1+%7B2c%7D%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{t=1}^{T-1} - \frac 1 {2c}\ell_t}" class="latex" title="{\sum_{t=1}^{T-1} - \frac 1 {2c}\ell_t}" />, and the projection to <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is just composition with <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tau}" class="latex" title="{\tau}" />. The bound on the number of steps is the same as the one in the previous section.</p>
<p>
Looking at the case in which <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is the set of edges of a clique on <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> is the set of graphs of the form <img src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K_{A,B}}" class="latex" title="{K_{A,B}}" />, and considering the Sigma-algebra generated by <img src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" title="{A_1,B_1,\ldots,A_T,B_T}" /> gives Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.fk">3</a> from Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a>.</p>
<p>
</p><p><b>3. Sampling High-Entropy Distributions </b></p>
<p></p><p>
Finally we discuss the application to sampling high-entropy distributions. </p>
<p>
Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is a distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+n-d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\geq n-d}" class="latex" title="{\geq n-d}" />, meaning that for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in \{ 0,1 \}^n}" class="latex" title="{x\in \{ 0,1 \}^n}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%29+%5Cleq+2%5E%7B-%28n-d%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x) \leq 2^{-(n-d)} " class="latex" title="\displaystyle  D(x) \leq 2^{-(n-d)} " /></p>
<p> where we think of the <em>entropy deficiency</em> <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> as being small, such as a constant or <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\log n)}" class="latex" title="{O(\log n)}" /></p>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> be a class of functions <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n \rightarrow \{ 0,1\}}" class="latex" title="{\{ 0,1 \}^n \rightarrow \{ 0,1\}}" /> that we think of as being “efficient.” For example, <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" /> could be the set of all functions computable by circuits of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq S(n)}" class="latex" title="{\leq S(n)}" /> for some size bound <img src="https://s0.wp.com/latex.php?latex=%7BS%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S(\cdot)}" class="latex" title="{S(\cdot)}" />, such as, for example <img src="https://s0.wp.com/latex.php?latex=%7BS%28n%29+%3D+10+n%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S(n) = 10 n^2}" class="latex" title="{S(n) = 10 n^2}" />. We will assume that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%5Cequiv+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x) \equiv 1}" class="latex" title="{f(x) \equiv 1}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />. Define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+2%5E%7Bn-d%7D+%5Ccdot+D%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x) = 2^{n-d} \cdot D(x) " class="latex" title="\displaystyle  g(x) = 2^{n-d} \cdot D(x) " /></p>
<p> to be a bounded function <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: \{ 0,1 \}^n \rightarrow [0,1]}" class="latex" title="{g: \{ 0,1 \}^n \rightarrow [0,1]}" />. Fix an approximation parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" />.</p>
<p>
Then from Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a> we have that there are <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = O(1/\epsilon^2)}" class="latex" title="{T = O(1/\epsilon^2)}" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" />, and scalars <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha_1,\ldots,\alpha_T}" class="latex" title="{\alpha_1,\ldots,\alpha_T}" />, all equal to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%2F2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 1/2c}" class="latex" title="{\pm 1/2c}" /> for a certain parameter <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" />, such that if we define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h%28x%29+%3D+%5Ctau+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Calpha_t+f_t%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  h(x) = \tau \left( \sum_{t=1}^T \alpha_t f_t(x) \right) " class="latex" title="\displaystyle  h(x) = \tau \left( \sum_{t=1}^T \alpha_t f_t(x) \right) " /></p>
<p> we have <a name="eq.ttv.main"></a></p><a name="eq.ttv.main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+g%28x%29+f%28x%29+-+h%28x%29+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) f(x) - h(x) f(x) \right ) \right | \leq \epsilon 2^n \ \ \ \ \ (1)" class="latex" title="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) f(x) - h(x) f(x) \right ) \right | \leq \epsilon 2^n \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.ttv.main"></a> and so, multiplying by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B-%28n-d%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{-(n-d)}}" class="latex" title="{2^{-(n-d)}}" /> <a name="eq.ttv.b"></a></p><a name="eq.ttv.b">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+h%28x%292%5E%7B-%28n-d%29%7D+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5Ed+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - h(x)2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d \ \ \ \ \ (2)" class="latex" title="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - h(x)2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d \ \ \ \ \ (2)" /></p>
</a><p><a name="eq.ttv.b"></a> Now define the probability distribution </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H%28x%29+%3D+%5Cfrac+%7Bh%28x%29%7D%7B%5Csum_%7By%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28y%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  H(x) = \frac {h(x)}{\sum_{y\in \{ 0,1 \}^n} h(y) } " class="latex" title="\displaystyle  H(x) = \frac {h(x)}{\sum_{y\in \{ 0,1 \}^n} h(y) } " /></p>
<p> Applying <a href="https://lucatrevisan.wordpress.com/feed/#eq.ttv.main">(1)</a> to the case <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%5Cequiv+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x) \equiv 1}" class="latex" title="{f(x) \equiv 1}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+g%28x%29+-+h%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) - h(x) \right ) \right | \leq \epsilon 2^n " class="latex" title="\displaystyle  \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) - h(x) \right ) \right | \leq \epsilon 2^n " /></p>
<p> and we know that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+g%28x%29+%3D+2%5E%7Bn-d%7D+%5Csum_x+D%28x%29+%3D+2%5E%7Bn-d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_x g(x) = 2^{n-d} \sum_x D(x) = 2^{n-d}}" class="latex" title="{\sum_x g(x) = 2^{n-d} \sum_x D(x) = 2^{n-d}}" />, so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%7C+2%5E%7Bn-d%7D+-+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28x%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left | 2^{n-d} - \sum_{x\in \{ 0,1 \}^n} h(x) \right | \leq \epsilon 2^n " class="latex" title="\displaystyle  \left | 2^{n-d} - \sum_{x\in \{ 0,1 \}^n} h(x) \right | \leq \epsilon 2^n " /></p>
<p> and we can rewrite <a href="https://lucatrevisan.wordpress.com/feed/#eq.ttv.b">(2)</a> as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+H%28x%29+%5Ccdot+%28%5Csum_y+h%28y%29%29+2%5E%7B-%28n-d%29%7D+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) \cdot (\sum_y h(y)) 2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d " class="latex" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) \cdot (\sum_y h(y)) 2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d " /></p>
<p> and, finally </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+H%28x%29+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+2+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) f(x) \right ) \right | \leq 2 \epsilon 2^d " class="latex" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) f(x) \right ) \right | \leq 2 \epsilon 2^d " /></p>
<p> that is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5CPr_%7Bx%5Csim+D%7D+%5Bf%28x%29+%3D+1%5D+-+%5CPr_%7Bx%5Csim+H%7D+%5Bf%28x%29+%3D1+%5D+%5Cright+%7C+%5Cleq+2+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \Pr_{x\sim D} [f(x) = 1] - \Pr_{x\sim H} [f(x) =1 ] \right | \leq 2 \epsilon 2^d " class="latex" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \Pr_{x\sim D} [f(x) = 1] - \Pr_{x\sim H} [f(x) =1 ] \right | \leq 2 \epsilon 2^d " /></p>
<p> which says that <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon 2^{d+1}}" class="latex" title="{\epsilon 2^{d+1}}" />-indistinguishable by functions in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />. If we chose <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />, for example, to be the class of functions computable by circuits of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq S(n)}" class="latex" title="{\leq S(n)}" />, then <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon 2^{d+1}}" class="latex" title="{\epsilon 2^{d+1}}" />-indistinguishable by circuits of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq S(n)}" class="latex" title="{\leq S(n)}" />.</p>
<p>
But <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is also samplable in a relatively efficient way using rejection sampling: pick a random <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, then output <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h(x)}" class="latex" title="{h(x)}" /> and fail with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-h%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-h(x)}" class="latex" title="{1-h(x)}" />. Repeat the above until the procedure does not fail. At each step, the probability of success is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28x%29+%5Cgeq+2%7B-d%7D+-+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}_{x\in \{ 0,1 \}^n} h(x) \geq 2{-d} - \epsilon}" class="latex" title="{\mathop{\mathbb E}_{x\in \{ 0,1 \}^n} h(x) \geq 2{-d} - \epsilon}" />, so, assuming (because otherwise all of the above makes no sense) that, say, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3C+2%5E%7B-d-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &lt; 2^{-d-1}}" class="latex" title="{\epsilon &lt; 2^{-d-1}}" />, the procedure succeeds on average in at most <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{d+1}}" class="latex" title="{2^{d+1}}" /> attempts. And if each <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> is computable by a circuit of size <img src="https://s0.wp.com/latex.php?latex=%7BS%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S(n)}" class="latex" title="{S(n)}" />, then <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h}" class="latex" title="{h}" /> is computable by a circuit of size <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29+%2B+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/\epsilon^2) + S(n)}" class="latex" title="{O(1/\epsilon^2) + S(n)}" />.</p>
<p>
The undesirable features of this result are that the complexity of sampling and the quality of indistinguishability depend exponentially on the randomness deficiency, and the sampling circuit is a non-uniform circuit that it’s not clear how to construct without advice. Impagliazzo’s recent results address both these issues.</p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/"><span class="datestr">at May 16, 2019 07:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9175787683601603334">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/getting-through-clutter.html">Getting Through the Clutter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
My daughter showed up at her doctor's office the other day and found it closed. She complained that she had received all these alerts, texts, emails, voicemails, reminding her of the appointment and then they weren't there when she showed up. She had stopped reading the alerts, the last of which said the appointment need to be rescheduled.<br />
<br />
We all get too many alerts. I just assume many of the emails I send to faculty never get read or remembered. I get many requests to mention conferences, workshop and other theory happenings on this blog because nobody knows how to get the word out through the clutter. If we followed through on all these requests, this blog would become clutter itself.<br />
<br />
Back in 2009, Nikhil Devanur and I wrote a <a href="https://doi.org/10.1145/1562814.1562830">paper</a> trying to capture this phenomenon using complexity. Building on Levin's notion of universal search, the unawareness of a string x in an environment E is the amount of time needed to generate x from a context c given an oracle for E. Levin showed that one can have a universal algorithm, only a constant time slower to generate x than any other algorithm. One should think of E as the sum total of all our knowledge including search engines on the Internet. Technically we should have used the term "attention" instead of "awareness".<br />
<br />
One example is using a calendar as part of your environment, E, that reminds you of an event, x, on that date, c. We use calendars, contact lists, emails searches and many other ways to keep strings we need to remember. Advertisers try to alter your E to get the unawareness of x down.<br />
<br />
One of these papers that didn't go far because we didn't have great applications for the definition. But it follows a good philosophy, when life seems complex, model it.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/getting-through-clutter.html"><span class="datestr">at May 16, 2019 12:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06104">About a certain NP complete problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Stepan Margaryan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06104">PDF</a><br /><b>Abstract: </b>In this article, we introduce the concept of special decomposition of a set
under the given pairs of subsets of that set, and the concept of special
covering of a set under such a decomposition. We study the conditions for
existence of special coverings of sets, under special decomposition of the set.
Such conditions of formulated problem have important applications in the field
of satisfiability of functions. Our goal is to study the relationship between
sat CNF problem and the problem of existance of special covering of te set. We
also study the relationship between classes of computational complexity by
searching for special coverings of the sets. We prove, that the decidability of
sat CNF problem, in polynomial time reduces to the problem of existence of a
special covering of a set. We also prove, that the problem of existence of a
special covering of a set, in polynomial time reduces to the decidability of
the sat CNF problem. Therefore, the mentioned problems are polynomially
equivalent. And then, the problem of existence of a special covering of a set
is NP-complete problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06104"><span class="datestr">at May 16, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.06084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.06084">Restricted Max-Min Allocation: Approximation and Integrality Gap</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Siu=Wing.html">Siu-Wing Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Yuchen.html">Yuchen Mao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06084">PDF</a><br /><b>Abstract: </b>Asadpour, Feige, and Saberi proved that the integrality gap of the
configuration LP for the restricted max-min allocation problem is at most $4$.
However, their proof does not give a polynomial-time approximation algorithm. A
lot of efforts have been devoted to designing an efficient algorithm whose
approximation ratio can match this upper bound for the integrality gap. In
ICALP 2018, we present a $(6 + \delta)$-approximation algorithm where $\delta$
can be any positive constant, and there is still a gap of roughly $2$. In this
paper, we narrow the gap significantly by proposing a
$(4+\delta)$-approximation algorithm where $\delta$ can be any positive
constant. The approximation ratio is with respect to the optimal value of the
configuration LP, and the running time is $\mathit{poly}(m,n)\cdot
n^{\mathit{poly}(\frac{1}{\delta})}$ where $n$ is the number of players and $m$
is the number of resources. We also improve the upper bound for the integrality
gap of the configuration LP to $3 + \frac{21}{26} \approx 3.808$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.06084"><span class="datestr">at May 16, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.05765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.05765">Quantum Complexity of Time Evolution with Chaotic Hamiltonians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balasubramanian:Vijay.html">Vijay Balasubramanian</a>, Matthew DeCross, Arjun Kar, Onkar Parrikar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05765">PDF</a><br /><b>Abstract: </b>We study the quantum complexity of time evolution in large-$N$ chaotic
systems, with the SYK model as our main example. This complexity is expected to
increase linearly for exponential time prior to saturating at its maximum
value, and is related to the length of minimal geodesics on the manifold of
unitary operators that act on Hilbert space. Using the Euler-Arnold formalism,
we demonstrate that there is always a geodesic between the identity and the
time evolution operator $e^{-iHt}$ whose length grows linearly with time. This
geodesic is minimal until there is an obstruction to its minimality, after
which it can fail to be a minimum either locally or globally. We identify a
criterion - the Eigenstate Complexity Hypothesis (ECH) - which bounds the
overlap between off-diagonal energy eigenstate projectors and the $k$-local
operators of the theory, and use it to show that the linear geodesic will at
least be a local minimum for exponential time. We show numerically that the
large-$N$ SYK model (which is chaotic) satisfies ECH and thus has no local
obstructions to linear growth of complexity for exponential time, as expected
from holographic duality. In contrast, we also study the case with $N=2$
fermions (which is integrable) and find short-time linear complexity growth
followed by oscillations. Our analysis relates complexity to familiar
properties of physical theories like their spectra and the structure of energy
eigenstates and has implications for the hypothesized computational complexity
class separations PSPACE $\nsubseteq$ BQP/poly and PSPACE $\nsubseteq$
BQSUBEXP/subexp, and the "fast-forwarding" of quantum Hamiltonians.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.05765"><span class="datestr">at May 16, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://culturacientifica.com/2019/05/01/el-poema-de-los-numeros-primos/">El poema de los números primos</a> (<a href="https://mastodon.social/@victorhck/102020247143426975"></a>). Exhibit of the mathematically-inspired artworks of <a href="https://en.wikipedia.org/wiki/Esther_Ferrer">Esther Ferrer</a>, at <a href="https://en.wikipedia.org/wiki/Tabakalera">Tabakalera</a> in San Sebastián, Spain.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2019/04/26/how-combinatorics-became-legitimate-according-to-laszlo-lovasz-and-endre-szemeredi/">How combinatorics became legitimate</a> (<a href="https://mathstodon.xyz/@11011110/102036466457669447"></a>). Igor Pak recommends two interesting video interviews with László Lovász and Endre Szemerédi. The whole interviews are quite long but they’re broken into 10-minute clips and Igor has picked out the ones relevant to the title.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/how-twisted-graphene-became-the-big-thing-in-physics-20190430/">Magic angles and superconductivity in twisted graphene</a> (<a href="https://mathstodon.xyz/@11011110/102044547042991323"></a>). If you twist two sheets of hexagonally tiled carbon relative to each other you can get a superconductor, but only for certain very specific twist angles.</p>
  </li>
  <li>
    <p><a href="https://practicaltypography.com/ligatures-in-programming-fonts-hell-no.html">Matthew Butterick says no to ligatures in programming fonts</a> (<a href="https://mathstodon.xyz/@11011110/102047783718443248"></a>, <a href="https://news.ycombinator.com/item?id=19805053">via</a>). I tend to agree. They make some things cuter but more things inconsistent. The lack of a short double back arrow in the Fira example is telling. And anyone who expects to see individual characters has to know what font they’re displayed in and how it mangles them to understand what they’re reading. But if you like these for your own text editing, whatever. Just show me the ASCII when I have to view it in my browser.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.01325">Breaking the Bellman–Ford shortest-path bound</a> (<a href="https://mathstodon.xyz/@11011110/102051970319136719"></a>). Amr Elmasry claims a time bound of  for single-source shortest paths in graphs that may have cycles and negative edge weights, but no negative cycles. If correct, this would be a big improvement over the  time for Bellman–Ford. However, I got stuck somewhere around Lemma 3 when trying to understand it. Anyone else have better progress?</p>
  </li>
  <li>
    <p><a href="http://www.generalist.org.uk/blog/2019/gender-and-deletion-on-wikipedia/">Some actual data on how the subject’s gender influences biography creation and deletion on Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/102058999289661319"></a>). Still-existing older articles on women are more likely to have gone through a deletion discussion than men, but we don’t know whether more were nominated or equally many nominated but women survived better, and whether the inequality of nominations has lessened recently or the greater nomination rate for women takes longer to kick in and is still prevalent.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.12761">The graphs behind Reuleaux polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/102064338449854355"></a>), by
Luis Montejano, Eric Pauli, Miguel Raggi, and Edgardo Roldán-Pensado.
These shapes are the intersections of equal-radius balls centered at their vertices; smoothing some edges gives them constant width. Their vertices are the finite point sets with the most diameters. Their vertex-edge graphs are self-dual, unlike other polyhedral graphs. And their vertex-diameter graphs are 4-colorable. Examples include pyramids over odd polygons.</p>
  </li>
  <li>
    <p>It’s not like it’s difficult to make your own out of, you know, paper, but if you want a colorful kit to teach yourself about the Miura-ori and three other folds, <a href="https://www.thisiscolossal.com/2019/05/paper-folding-kit-by-kelli-anderson/">this one looks pretty if a little overpriced at $20 for eight sheets of paper</a> (<a href="https://mathstodon.xyz/@11011110/102070078247023735"></a>).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.02167">Tensor products of graphs can require fewer colors than their factors</a> (<a href="https://mathstodon.xyz/@11011110/102072759747197000"></a>). This short new preprint by Yaroslav Shitov gives counterexamples to <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Hedetniemi’s conjecture</a> from 1966. In a new blog post <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">Gil Kalai explains the construction</a>.</p>
  </li>
  <li>
    <p><a href="https://discrete-notes.github.io/natural-history">Algorithms and natural history</a> (<a href="https://mathstodon.xyz/@11011110/102080291126103628"></a>). In a new blog, Laurent Feuilloley writes about some algorithmic problems on polyhedra coming from the measurement of skulls, diamond cutting, and the use of symmetry to undo deformations of fossils.</p>
  </li>
  <li>
    <p>Did you know that Swiss mathematician <a href="https://en.wikipedia.org/wiki/Alice_Roth">Alice Roth</a> invented <a href="https://en.wikipedia.org/wiki/Swiss_cheese_(mathematics)">Swiss cheese</a>? (<a href="https://mathstodon.xyz/@11011110/102087095765063877"></a>). A Swiss cheese is a disk with smaller disks removed, leaving no interior. <a href="https://blogs.scientificamerican.com/roots-of-unity/the-serendipity-of-swiss-cheese/"><em>Scientific American</em> alerted me to this amusing terminology</a> but I got a clearer idea what they’re good for from <a href="http://www.math.tamu.edu/~boas/courses/618-2015a/roth.pdf">an exercise using them to show complex conjugation to be well-behaved on a compact domain but hard to approximate by rational functions</a>.</p>
  </li>
  <li>
    <p><a href="https://ooni.torproject.org/post/2019-china-wikipedia-blocking/">China is now blocking all language editions of Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/102089517303119369"></a>, <a href="https://boingboing.net/2019/05/13/report-china-now-blocks-wikip.html">via</a>), expanding its previous block which applied only to the Mandarin edition.</p>

    <p>Of course their internet blockage is hardly the biggest problem with China these days. I was surprised to find that some of my usually-well-informed friends hadn’t even heard of “<a href="https://www.france24.com/en/20190510-reporters-plus-surviving-china-uighur-camps-repression">the largest mass incarceration of the 21st century</a>” and “<a href="https://www.theguardian.com/world/2018/dec/07/uighur-leaders-warn-chinas-actions-could-be-precursors-to-genocide">precursors to genocide</a>”, <a href="https://www.washingtonpost.com/opinions/global-opinions/china-cant-prettify-the-human-rights-catastrophe-in-xinjiang/2019/03/24/4c844f62-45ca-11e9-90f0-0ccfeec87a61_story.html">China’s concentration camps</a> for <a href="https://www.amnesty.org/en/latest/news/2018/09/china-up-to-one-million-detained/">up to a million Uighur people</a>. So read and learn.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Hazel_Perfect">Hazel Perfect</a> (<a href="https://mathstodon.xyz/@11011110/102097362417443508"></a>). A new Wikipedia article on the inventor of <a href="https://en.wikipedia.org/wiki/Gammoid">gammoids</a> (how I came across her name this time) and <a href="https://aperiodical.com/2013/03/much-ado-about-noether/">Christian Lawson-Perfect’s mathematical hero</a> (despite or because of the unexplained similarity of names).</p>
  </li>
  <li>
    <p><a href="https://www.dailykos.com/stories/2019/5/13/1857360/-Poll-says-that-56-of-Americans-don-t-want-kids-taught-Arabic-numerals-We-have-some-bad-news">In a recent poll, “56% of Americans said Arabic numerals should not be taught in American schools”</a> (<a href="https://mathstodon.xyz/@11011110/102100871760570133"></a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/15/linkage.html"><span class="datestr">at May 15, 2019 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/15/postdoc-at-boston-college-apply-by-june-1-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/15/postdoc-at-boston-college-apply-by-june-1-2019/">Postdoc at Boston College (apply by June 1, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and massive parallel computation algorithms. The starting date is flexible between Fall 2019 and Spring 2020. The position is for a period of up to two years.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br />
Email: suhx@bc.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/15/postdoc-at-boston-college-apply-by-june-1-2019/"><span class="datestr">at May 15, 2019 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4191">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4191">The SSL Certificate of Damocles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Ever since I “upgraded” this website to use SSL, it’s become completely inaccessible once every three months, because the SSL certificate expires.  Several years in, I’ve been unable to find any way to prevent this from happening, and Bluehost technical support was unable to suggest any solution.  The fundamental problem is that, as long as the site remains up, the Bluehost control panel tells me that there’s nothing to do, since there <em>is</em> a current certificate.  Meanwhile, though, I start getting menacing emails saying that my SSL certificate is about to expire and “you must take action to secure the site”—<em>never</em>, of course, specifying what action to take.  The only thing to do seems to be to wait for the whole site to go down, then frantically take random certificate-related actions until somehow the site goes back up.  Those actions vary each time and are not repeatable.</p>



<p>Does anyone know a simple solution to this ridiculous problem?</p>



<p>(The deeper problem, of course, is that a PhD in theoretical computer science left me utterly unqualified for the job of webmaster.  And webmasters, as it turns out, need to do a lot just to prevent anything from changing.  And since childhood, I’ve been accustomed to countless tasks that are trivial for most people being difficult for me—-if that ever stopped being the case, I’d no longer feel like myself.)</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4191"><span class="datestr">at May 15, 2019 02:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/071">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/071">TR19-071 |  Time-Space Lower Bounds for Two-Pass Learning | 

	Sumegha Garg, 

	Ran Raz, 

	Avishay Tal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A line of recent works showed that for a large class of learning problems, any learning algorithm  requires either super-linear memory size or a super-polynomial number of samples [Raz16,KRT17,Raz17,MM18,BOGY18,GRT18]. For example, any algorithm for learning parities of size $n$ requires either a memory of size $\Omega(n^{2})$ or an exponential number of samples [Raz16].

All these works modeled the learner as a one-pass branching program, allowing only one pass over the stream of samples. In this work, we prove the first memory-samples lower bounds (with a super-linear lower bound on the memory size and super-polynomial lower bound on the number of samples) when the learner is allowed two passes over the stream of samples. For example, we prove that any two-pass algorithm for learning parities of size $n$ requires either a memory of size $\Omega(n^{1.5})$ or at least $2^{\Omega(\sqrt{n})}$ samples.

More generally, a matrix $M: A \times X \rightarrow \{-1,1\}$ corresponds to the following learning problem: An unknown element $x \in X$ is chosen uniformly at random. A learner tries to learn $x$ from a stream of samples, $(a_1, b_1), (a_2, b_2) \ldots$, where for every $i$, $a_i \in A$ is chosen uniformly at random and $b_i = M(a_i,x)$.

Assume that $k,l, r$ are such that any submatrix of $M$ of at least $2^{-k} \cdot |A|$ rows and at least $2^{-l} \cdot |X|$ columns, has a bias of at most $2^{-r}$. We show that any two-pass learning algorithm for the learning problem corresponding to $M$ requires either a memory of size at least $\Omega\left(k \cdot  \min\{k,\sqrt{l}\} \right)$, or at least $2^{\Omega(\min\{k,\sqrt{l},r\})}$ samples.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/071"><span class="datestr">at May 14, 2019 04:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15858">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/">Internet, Dogs, and the ABC Conjecture</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>An inappropriate comment on the ABC conjecture</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/220px-david_masser/" rel="attachment wp-att-15867"><img width="150" alt="" class="alignright wp-image-15867" src="https://rjlipton.files.wordpress.com/2019/05/220px-david_masser.jpg?w=150" /></a><br /><a href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/220px-oesterle_joseph/" rel="attachment wp-att-15868"><img width="150" alt="" class="alignright  wp-image-15868" src="https://rjlipton.files.wordpress.com/2019/05/220px-oesterle_joseph.jpg?w=150" /></a><br /><table class="image alignright">
<tbody>
<tr>
<td>
</td>
</tr>
<tr>


</tr>
</tbody>
</table>
<p>
Joseph Oesterlé and David Masser are famous for their independent discovery of the ABC conjecture. </p>
<p>
Today I want to point out an unfair comment about their discovery.</p>
<p>
Anonymity on the Internet was captured by a famous 1993 <a href="https://en.wikipedia.org/wiki/On_the_Internet,_nobody_knows_you%27re_a_dog">cartoon</a> in the <i>New Yorker</i> magazine titled, “On the Internet, nobody knows you’re a dog.”  Amazing to think that was more than a quarter-century ago and remains true.  But people <i>can</i> tell if what you’ve written is something inappropriate.</p>
<p></p><h2> The Comment </h2><p></p>
<p></p><p>
The comment is: </p>
<blockquote><p><b> </b> <em> <i>SAYS WHO??? I have some trouble with this item.</i> </em>
</p></blockquote>
<p></p><p>
Masser is a Fellow of the Royal Society, who was elected in 2005. He is </p>
<blockquote><p><b> </b> <em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> also responsible, following an earlier insight of Joseph Oesterlé, for formulating the abc conjecture; this is a simple statement about integers which seems to hold the key to much of the future direction of number theory. </em>
</p></blockquote>
<p></p><p>
See this <a href="https://royalsociety.org/people/david-masser-11903/">link</a> for his full citation and the comment. Click on the <i>show more bibliography</i> button there. The comment is apparently anonymous, although the author is probably known to some. I thank Joël Ouaknine for pointing out this strange comment.</p>
<p>
<b>Update:</b> Ken speculates that it’s a misplaced comment by an editor of the Royal Society website itself.  Perhaps they compose HTML from MS Word or Acrobat or other software that provides comment bubbles—but this one escaped the bubble and wasn’t noticed.  Editors of Wikipedia have automatic tools for flagging assertions that are unsupported or at least need citation.  </p>
<p>
What the comment undoubtedly shows is vigorous debate behind the walls of Britain’s august institution.  So let’s say a little more on what the comment is about.</p>
<p></p><h2> The ABC Conjecture </h2><p></p>
<p></p><p>
The biggest mysteries about numbers often concern the interaction between addition and multiplication. For example: </p>
<ul>
<li>
The <a href="https://en.wikipedia.org/wiki/Twin_prime">twin</a> prime conjecture: There are an infinite number of primes <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bp%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p+2}" class="latex" title="{p+2}" /> is also prime. This is due to Alphonse de Polignac. <p></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Goldbach%27s_conjecture">Goldbach</a> conjecture: Every even number greater than four is the sum of two odd primes. This is due to Christian Goldbach. <p></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Brocard%27s_problem">Brocard</a> conjecture: There are only a finite number of solutions to <img src="https://s0.wp.com/latex.php?latex=%7Bn%21+%3D+m%5E%7B2%7D+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n! = m^{2} + 1}" class="latex" title="{n! = m^{2} + 1}" /> in natural numbers. This is due to Henri Brocard.
</li></ul>
<p>
Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BA+%2B+B+%3D+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A + B = C}" class="latex" title="{A + B = C}" /> where <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" /> are positive and co-prime natural numbers. Let <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> be the product of all the distinct prime divisors of <img src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ABC}" class="latex" title="{ABC}" />. Then the ABC conjecture says that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+O%28D%5E%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le O(D^{2}). " class="latex" title="\displaystyle  C \le O(D^{2}). " /></p>
<p>Note, this inequality does indeed connect adding with multiplying. The usual conjecture is stronger, see <a href="https://en.wikipedia.org/wiki/Abc_conjecture">this</a> for details.	</p>
<p>
The ABC conjecture appears to be open, even though Shinichi Mochizuki has claimed a <a href="https://rjlipton.wordpress.com/2012/09/12/the-abc-conjecture-and-cryptography/">proof</a> for years. See <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920">this</a> for a discussion about the status of the conjecture. </p>
<blockquote><p><b> </b> <em> Despite multiple conferences dedicated to explicating Mochizuki’s proof, number theorists have struggled to come to grips with its underlying ideas. His series of papers, which total more than 500 pages, are written in an impenetrable style, and refer back to a further 500 pages or so of previous work by Mochizuki, creating what one mathematician, Brian Conrad of Stanford University, has called “a sense of infinite regress.” </em>
</p></blockquote>
<p>
</p><p></p><h2> The Comment II </h2><p></p>
<p></p><p>
The comment on Masser’s work is wrong, strange, inappropriate. Oesterlé and Masser deserve more credit, not less, for their brilliant discovery of the ABC conjecture. There are now many—perhaps hundreds—of applications of the ABC conjecture. For example consider generalizations of Fermat’s Last Theorem. Suppose that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bp%7D+%2B+y%5E%7Bq%7D+%3D+z%5E%7Br%7D+%5Ctext%7B++%28%2A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x^{p} + y^{q} = z^{r} \text{  (*)}" class="latex" title="\displaystyle  x^{p} + y^{q} = z^{r} \text{  (*)}" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bp%2Cq%2Cr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p,q,r}" class="latex" title="{p,q,r}" /> are odd primes. And <img src="https://s0.wp.com/latex.php?latex=r+%3E+6&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="r &gt; 6" class="latex" title="r &gt; 6" />. Provided <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z}" class="latex" title="{x,y,z}" /> are positive and co-prime, it follows by the ABC conjecture that <img src="https://s0.wp.com/latex.php?latex=%7Bz%5E%7Br%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z^{r}}" class="latex" title="{z^{r}}" /> is bounded by <img src="https://s0.wp.com/latex.php?latex=%7BO%28z%5E%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(z^{2})}" class="latex" title="{O(z^{2})}" />. This is impossible for <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> large enough since <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r \ge 3}" class="latex" title="{r \ge 3}" />. Therefore, (*) can only have a finite number of solutions. Pretty neat.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do you know of any other inappropriate comments of this kind?</p>
<p></p><p><br />
[added remark by Ken, linked rather than embed dog cartoon]<br />
[Added prime r must be 7 or larger. Thanks to comment by MadHatter.]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/"><span class="datestr">at May 14, 2019 02:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/070">TR19-070 |  On Local Testability in the Non-Signaling Setting | 

	Alessandro Chiesa, 

	Peter Manohar, 

	Igor Shinkar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Non-signaling strategies are a generalization of quantum strategies that have been studied in physics for decades, and have recently found applications in theoretical computer science. These applications motivate the study of local-to-global phenomena for non-signaling functions.

We present general results about the local testability of linear codes in the non-signaling setting. Our contributions include formulating natural definitions that capture the condition that a non-signaling function "belongs" to a given code, and characterizing the sets of local constraints that imply membership in the code. We prove these results by relating the Fourier spectrum of non-signaling functions to Cayley hypergraphs induced by local constraints.

We apply the above results to show a separation between locally testable codes in the classical and non-signaling setting by proving that bivariate low-degree testing fails spectacularly in the non-signaling setting. Specifically, we show that there exist non-signaling functions that pass bivariate low-degree tests with probability 1, and yet are maximally far from low-degree.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/070"><span class="datestr">at May 14, 2019 05:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=633">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/05/13/and-this-is-me-with-my-buddies/">And this is me with my buddies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="RY3tic">
<div class="eGiHwc"><img src="https://emanueleviola.files.wordpress.com/2019/05/warriors2.jpg?w=640" alt="warriors2" class="alignnone size-full wp-image-634" /></div>
<div class="KYCEmd"></div>
</div>
<div class="RY3tic">
<div class="eGiHwc"></div>
<div class="KYCEmd"></div>
</div>
<p> </p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/05/13/and-this-is-me-with-my-buddies/"><span class="datestr">at May 13, 2019 03:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3507110503322010708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html">Ronald Graham's other large number. Well---- it was large in 1964 anyway.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Graham's number (see <a href="https://en.wikipedia.org/wiki/Graham%27s_number">here</a>) was at one time the largest number to appear in a math proof.<br />
<br />
a) GN was an upper bound on a problem in Ramsey theory. There are now better upper bounds, see <a href="https://www.sciencedirect.com/science/article/pii/S0195669814000936?via%3Dihub">here</a>. These better upper bounds are still large- Hales-Jewitt-Large, but that's already much smaller than the original GN.<br />
<br />
b) Other proofs now have numbers even larger than GN. For example Harvey Friedman's work on the finite versions of Kruskal's Tree Theorem. (There may be other cases- if you know some then let me know in the comments.)<br />
<br />
Since my dept recently moved buildings I found old papers that I had not looked at in years. One of them was<br />
<br />
<i>Old and New Problems and Results in Combinatorial Number Theory</i><br />
<br />
by Erdos and Graham<br />
<br />
(see <a href="http://www.math.ucsd.edu/~ronspubs/79_09_combinatorial_number_theory.pdf">here</a>)<br />
<br />
So I began reading it and came across a result of Graham from 1964 that used large numbers. No where near as large as GN, but I found it interesting that Graham was involved with large numbers way back then.<br />
<br />
Here is the problem:<br />
<br />
A <i>Lucas Sequence</i> is a sequence that obeys<br />
<br />
a(n) = a(n-1) + a(n-2).<br />
<br />
Clearly such a sequence is determined by a(0) and a(1).<br />
<br />
QUESTION: Does there exists a(0) and a(1)  that are rel prime such that the sequence has only composite numbers?<br />
<br />
By ingenuity and some computing power Graham found YES. For how the got the numbers see <a href="http://www.math.ucsd.edu/~ronspubs/64_06_fibonacci.pdf">here</a>. The numbers are of course in the paper, and how they got them is interesting, but I present them anyway. Hope I don't make a typo:<br />
<br />
a(0) = 1786772701928802632268715130455793<br />
<br />
a(1) = 1059683225053915111058164141686995<br />
<br />
The paper Old and New... says its open if there is a smaller pair of numbers, I do not know if it is still open. If you know, let us all  know in the comments!<br />
<br />
These numbers seem small today since we have modern computers that can store and manipulate them easily. Were the considered large numbers in 1964? They were never called Graham Numbers which is probably just as well since that honor lay ahead.<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html"><span class="datestr">at May 13, 2019 04:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-7244168862511547770">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/05/ice-tcs-theory-day-2019.html">ICE-TCS Theory Day 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<i>This post also appears on the <a href="https://ice-tcs.blogspot.com/2019/05/ice-tcs-theory-day-2019.html">ICE-TCS blog</a>. </i><br /><br />On Friday, 3 May, ICE-TCS hosted its <a href="http://icetcs.ru.is/theory-day2019.html">15th annual Theory Day</a>. The event consisted of two 45-minute presentations by <a href="https://dblp.uni-trier.de/pers/hd/b/Boppana:Ravi_B=">Ravi  Boppana</a> (Department of Mathematics, MIT) and <a href="https://dcc.fceia.unr.edu.ar/~erivas/">Exequiel Rivas</a>(Inria Paris - Rocquencourt, France),  and three ten-minute presentations by ICE-TCS researchers highlighting some of the recent research directions pursued by members of the centre.<br /><br /><a href="https://dblp.uni-trier.de/pers/hd/b/Boppana:Ravi_B=">Ravi  Boppana</a> kicked off the Theory Day with a wonderfully paced talk on his <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v24i3p40">work</a> with <a href="https://holzman.technion.ac.il/">Ron Holzman</a> on Tomaszewski’s problem on randomly signed sums. The problem is as follows. Let <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-1-Frame"><span class="mjx-math" id="MJXc-Node-1"><span class="mjx-mrow" id="MJXc-Node-2"><span class="mjx-msubsup" id="MJXc-Node-3"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-4"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">v</span></span></span><span class="mjx-sub"><span class="mjx-mn" id="MJXc-Node-5"><span style="padding-top: 0.39em;" class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span></span></span>, <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-2-Frame"><span class="mjx-math" id="MJXc-Node-6"><span class="mjx-mrow" id="MJXc-Node-7"><span class="mjx-msubsup" id="MJXc-Node-8"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-9"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">v</span></span></span><span class="mjx-sub"><span class="mjx-mn" id="MJXc-Node-10"><span style="padding-top: 0.39em;" class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span></span></span>, ..., <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-3-Frame"><span class="mjx-math" id="MJXc-Node-11"><span class="mjx-mrow" id="MJXc-Node-12"><span class="mjx-msubsup" id="MJXc-Node-13"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-14"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">v</span></span></span><span class="mjx-sub"><span class="mjx-mi" id="MJXc-Node-15"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span> be real numbers whose squares add up to 1.  Consider the <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-4-Frame"><span class="mjx-math" id="MJXc-Node-16"><span class="mjx-mrow" id="MJXc-Node-17"><span class="mjx-msubsup" id="MJXc-Node-18"><span class="mjx-base"><span class="mjx-mn" id="MJXc-Node-19"><span style="padding-top: 0.39em;" class="mjx-char MJXc-TeX-main-R">2</span></span></span><span style="font-size: 70.7%; padding-left: 0px; vertical-align: 0.591em;" class="mjx-sup"><span class="mjx-mi" id="MJXc-Node-20"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span> signed sums of the form <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-5-Frame"><span class="mjx-math" id="MJXc-Node-21"><span class="mjx-mrow" id="MJXc-Node-22"><span class="mjx-mi" id="MJXc-Node-23"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-24"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-25"><span class="mjx-char MJXc-TeX-size1-R">∑</span></span><span class="mjx-mo MJXc-space1" id="MJXc-Node-26"><span style="padding-top: 0.39em;" class="mjx-char MJXc-TeX-main-R">±</span></span><span class="mjx-msubsup" id="MJXc-Node-27"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-28"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">v</span></span></span><span class="mjx-sub"><span class="mjx-mi" id="MJXc-Node-29"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">i</span></span></span></span></span></span></span>.  Can there be more signed sums whose value is greater than 1 then those whose value  is at most 1? <a href="https://holzman.technion.ac.il/files/2012/09/combsigns.pdf">Holzman and Kleitman (1992) </a>proved that at least 3/8 of these sums satisfy <span style="font-size: 129%;" tabindex="0" class="mjx-chtml MathJax_CHTML" id="MathJax-Element-6-Frame"><span class="mjx-math" id="MJXc-Node-30"><span class="mjx-mrow" id="MJXc-Node-31"><span class="mjx-texatom" id="MJXc-Node-32"><span class="mjx-mrow" id="MJXc-Node-33"><span class="mjx-mo" id="MJXc-Node-34"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi" id="MJXc-Node-35"><span style="padding-bottom: 0.28em;" class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-texatom" id="MJXc-Node-36"><span class="mjx-mrow" id="MJXc-Node-37"><span class="mjx-mo" id="MJXc-Node-38"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-39"><span class="mjx-char MJXc-TeX-main-R">≤</span></span><span class="mjx-mn MJXc-space3" id="MJXc-Node-40"><span style="padding-top: 0.39em;" class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span>.  In his talk, Ravi showed us the main ideas Holzman and he used to improve the bound to  13/32.<br /><br />Computational effects model the interaction of computer programs with their environment. In his talk, <a href="https://dcc.fceia.unr.edu.ar/~erivas/"> Exequiel Rivas</a>taught us how <a href="https://en.wikipedia.org/wiki/Monad_(category_theory)">monads</a>can be used to capture computational effects (a research programme that started with <a href="https://core.ac.uk/download/pdf/21173011.pdf">Moggi's award-winning work</a>), and then, discussed some attempts to incorporate merging operations in the monadic picture.<br /><br />Two of the short talks were given by Henning A. Úlfarsson and Elli  Anastasiadi. Henning described the work of his group on  a tool, called  the CombSpecSearcher, that automates the methods used by  combinatorialists to prove some of their theorems, The tool is able to  prove results featured in dozens of research papers. Watch this space for updates on its  development and for its successes!<br /><br />Elli Anastasiadi, a PhD student who is already playing an important role  for the centre, gave a clear seven-minute introduction to <a href="https://people.csail.mit.edu/virgi/ipec-survey.pdf">fine-grained complexity</a> and to the notion of <a href="https://en.wikipedia.org/wiki/Fine-grained_reduction">fine-grained reduction</a>.<br /><br />The 2019 Theory Day was well attended, at least by the standards of a  TCS event in Iceland. If all goes well, we'll be back next year.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/05/ice-tcs-theory-day-2019.html"><span class="datestr">at May 12, 2019 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/069">TR19-069 |  Bounded-depth Frege complexity of Tseitin formulas for all graphs | 

	Artur Riazanov, 

	Dmitry Itsykson, 

	Nicola  Galesi, 

	Anastasia Sofronova</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that there is a constant $K$ such that \emph{Tseitin} formulas for an undirected graph $G$ requires proofs of 
size $2^{\mathrm{tw}(G)^{\Omega(1/d)}}$ in depth-$d$ Frege systems for $d&lt;\frac{K \log n}{\log \log n}$, where $\tw(G)$ is the treewidth of $G$. This extends  H{\aa}stad recent lower bound for the grid graph to any graph. Furthermore, we prove tightness of our bound up to a multiplicative constant in the top exponent. 
Namely, we show that if a Tseitin formula for a graph $G$ has size $s$, then for all large enough $d$, it has a depth-$d$ Frege proof of size $2^{\mathrm{tw}(G)^{\O(1/d)}} \mathrm{poly}(s)$. 
Through this result we settle the question posed by M. Alekhnovich and A. Razborov  of showing that the class of Tseitin formulas is quasi-automatizable for resolution.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/069"><span class="datestr">at May 12, 2019 09:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15846">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/">Making Elections Safe</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>A new proof that MAJORITY is not in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{AC}^{0}}" class="latex" title="{\mathsf{AC}^{0}}" />.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/unknown-121/" rel="attachment wp-att-15848"><img src="https://rjlipton.files.wordpress.com/2019/05/unknown.jpeg?w=600" alt="" class="alignright size-full wp-image-15848" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Rich DeMillo ]</font></td>
</tr>
</tbody>
</table>
<p>
Rich DeMillo is a strong leader, a famous researcher, and a long-time best friend. <i>Proof</i>: He was the first CTO at HP and was the Dean of Computing at Georgia Tech; He helped created <a href="https://en.wikipedia.org/wiki/Mutation_testing">mutation</a> a powerful software testing method and did seminal work in complexity theory. The last is clear.</p>
<p>
Today I want to talk about his recent work on voting systems. </p>
<p>
The 2020 election is over a year away, yet it is on our collective minds. People voice concerns everyday on social media, on TV, on cable, in print, everywhere. Their concerns are that our next national election will be compromised. Rich has turned his concern into activism: he is working hard to make elections trusted in general and the 2020 election in particular. </p>
<p>
Rich is scheduled to give a talk this coming Monday, May 13, at Georgia Tech. I wish I could be there, but cannot. I do plan to watch the video of his presentation—see <a href="https://www.cc.gatech.edu/calendar/day/2019/05/13/15151">here</a>. The talk is based on his recent <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3375755">paper</a> joint with Andrew Appel and Philip Stark titled, “Ballot-Marking devices (BMDs) cannot assure the will of the voters.” As an aside, their paper (ADS) has already generated measurable interest—it’s been downloaded over <img src="https://s0.wp.com/latex.php?latex=%7B300%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{300}" class="latex" title="{300}" /> times and viewed over <img src="https://s0.wp.com/latex.php?latex=%7B5%2C000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{5,000}" class="latex" title="{5,000}" /> times.</p>
<p>
</p><p></p><h2> What Makes A Good Election? </h2><p></p>
<p></p><p>
There are several criterion that a “good” election should have. </p>
<ul>
<li>
<i>During the voting</i>: Only eligible voters can vote. <p></p>
</li><li>
<i>During the tabulation</i>: Each voter gets exactly one vote. <p></p>
</li><li>
<i>After the election</i>: Each vote remains secret.
</li></ul>
<p>
Rich says in his talk abstract: </p>
<blockquote><p><b> </b> <em> Many people believe that, in an Internet-enabled world, secure, safe voting should be easy to achieve. For example, using known cryptographically secure protocols (maybe even blockchains), a secure website might be developed to relieve voters of the burden of driving to a polling place on election day. </em>
</p></blockquote>
<p></p><p>
This belief is wrong. Elections are hard—impossible?—to safeguard. A U.S. national election is the union of about ten thousand local elections. Each has different rules and protocols, which makes safeguarding the national election difficult. </p>
<ul>
<li>
<i>During the voting</i>: Who votes is subject to human decisions? People must prove they are eligible voters. Even computer identification is subject to bias. <p></p>
</li><li>
<i>During the tabulation</i>: Counting is usually a combination of human and computer systems. The former make errors and as do the latter. Computers can have bugs or can be hacked by adversaries. <p></p>
</li><li>
<i>After the election</i>: Since records are usually kept of the voting, they must be safeguarded to avoid disclosing how someone voted.
</li></ul>
<p>
The last point is central. There must be a record of the votes to allow audits after the election is over. We must be able to audit and check that the tabulation was correct. This is the central question that Rich and his co-authors discuss. We will turn to this issue in a moment. </p>
<p>
Before that I note that keeping a vote secret is impossible in an absolute sense. Suppose that you vote “yes” in some district. After the election suppose that the count in that district is made public, as it usually is. Say <img src="https://s0.wp.com/latex.php?latex=%7B61%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{61\%}" class="latex" title="{61\%}" /> of the votes were yes. Then clearly information is leaked about how you voted.</p>
<p>
</p><p></p><h2> How Are Votes Recorded? </h2><p></p>
<p></p><p>
There are two main ways to record votes. One method is to have voters hand-mark their ballots, in the old-fashioned way. It is simple, cheap, and not 21st century. Hand-marked ballots can be read by automatic scanners, at least in principle. A difficulty is voters are human and may mark their ballots incorrectly. They may miss a box, or mark two boxes, or make some other mistake. What if the voter is instructed to:</p>
<blockquote><p><b> </b> <em> Select two of the following six choices. </em>
</p></blockquote>
<p></p><p>
There can be other difficulties: Some voters may have special needs and may require instructions to be in a large type font, for example.</p>
<p>
Another recoding method is to have voters use a device to print their paper ballots. These are cleverly called <a href="https://en.wikipedia.org/wiki/Ballot_marking_device">Ballot Marking Devices</a> (BMDs). The name sounds slightly strange to me; there is an alternative name, electronic ballot markers (EBMs).</p>
<p>
The authors, ADS, argue that BMDs are dangerous. Such devices can fail, they argue, and not protect the election. The BMDs rely on software, complex special purpose software, and thus are subject to bugs, errors, mistakes, and to active attacks by adversaries. </p>
<p>
A BMD device takes input from a voter and then prints out a paper ballot. Often the ballot will contain a machine-readable bar code. This is so scanners can more easily read the paper ballots, later. The problem, the danger, is that most voters cannot tell if a bar code is correct or not. An attacker need only have the BMD confirm that you voted say “yes” and print a ballot that says “yes”. Then the attacker has the BMD cheat you by printing a bar code that says “no”. This is a nasty attack, which is hard to stop. The ADS team discusses this and related problems with BDMs. </p>
<p>
<a href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/bar/" rel="attachment wp-att-15852"><img src="https://rjlipton.files.wordpress.com/2019/05/bar.png?w=300&amp;h=147" alt="" width="300" class="aligncenter size-medium wp-image-15852" height="147" /></a></p>
<p>
</p><p></p><h2> Mathematics and Voting </h2><p></p>
<p></p><p>
Can complexity theory help us design better elections? Unclear. Can election theory help us understand complexity theory? Perhaps.</p>
<p>
</p><p></p><h3> Elections Inform Complexity Theory </h3><p></p>
<p>
</p><blockquote><p><b>Theorem 1</b> <em> The <i>Election Hardness Axiom</i> implies that the MAJORITY function cannot be computed by a polynomial size constant depth Boolean circuit of NOT, AND, and OR gates. </em>
</p></blockquote>
<p></p><p>
That is, the MAJORITY function is not in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{AC}^{0}}" class="latex" title="{\mathsf{AC}^{0}}" />. What is the Election Hardness Axiom? It is the empirical fact that there is no practical way to compute who won an election. The MAJORITY function is the tabulation of votes: The outcome of an election is the same as computing the MAJORITY function of the votes—“yes” is a <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and “no” is a <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />.</p>
<p>
Okay we are kidding. But not completely. Suppose that MAJORITY function were in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{AC}^{0}}" class="latex" title="{\mathsf{AC}^{0}}" />. Then a series of decisions of the form:</p>
<blockquote><p><b> </b> <em> The tabulators have looked at the following ballots <img src="https://s0.wp.com/latex.php?latex=%7BB_%7B1%7D%2C%5Cdots%2CB_%7Bm%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{B_{1},\dots,B_{m}}" class="latex" title="{B_{1},\dots,B_{m}}" /> and we agree that there is a “yes” vote in ballot <img src="https://s0.wp.com/latex.php?latex=%7BB_%7Bi%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{B_{i}}" class="latex" title="{B_{i}}" />. </em>
</p></blockquote>
<p>
</p><p></p><h3> Complexity Informs Elections </h3><p></p>
<p></p><p>
Rich, in his talk abstract, states that it is unlikely that crypto theory could be used to create trusted elections. His reason is voters will not trust elections that rely on crypto results. I agree. But I wonder if ideas from theory could be useful. Here are two high-level thoughts.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> There is a vast literature on computing in the presence of faults. Usually “faults” are thought to occur at the nano level: the faults are due to hiccups in electronics. What if the faults came from errors in the counting of votes? What if the faults were at the macro level? That is at the level of human decisions? Perhaps we will revisit this in the future.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> There is a vast literature on computing as a “game”. An election is usually viewed as being run by some trusted party. This could be replaced by assuming that the election is a game. Imagine two parties D and R. As the tabulation is performed D and R can challenge each other. They interact as in game. Could this help make the election trusted? Perhaps we will revisit this too in the future.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can we elections be trusted? Can we formalize the connection between election and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{AC}^{0}}" class="latex" title="{\mathsf{AC}^{0}}" />? Could this connect be useful? Can theory help with future elections?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/"><span class="datestr">at May 10, 2019 12:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17447">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">A sensation in the morning news –  Yaroslav Shitov: Counterexamples to Hedetniemi’s conjecture.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two days ago Nati Linial sent me an email entitled “A sensation in the morning news”. The link was to a new arXived paper by Yaroslav Shitov:<a href="https://arxiv.org/abs/1905.02167"> Counterexamples to Hedetniemi’s conjecture</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Hedetniemi’s 1966 conjecture</a> asserts that if <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> are two graphs, then the chromatic number of their tensor product <img src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \times H" class="latex" title="G \times H" /> equals the minimum of their individual chromatic numbers.  Here, the vertex set of <img src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \times H" class="latex" title="G \times H" /> is the Cartesian product of <img src="https://s0.wp.com/latex.php?latex=V%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V(G)" class="latex" title="V(G)" /> and <img src="https://s0.wp.com/latex.php?latex=V%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V(H)" class="latex" title="V(H)" /> and two vertices <img src="https://s0.wp.com/latex.php?latex=%28g_1%2Ch_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(g_1,h_1)" class="latex" title="(g_1,h_1)" /> and <img src="https://s0.wp.com/latex.php?latex=%28g_2%2Ch_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(g_2,h_2)" class="latex" title="(g_2,h_2)" /> are adjacent if <img src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_1" class="latex" title="g_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_2" class="latex" title="g_2" /> and  <img src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_1" class="latex" title="h_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_2" class="latex" title="h_2" />. (mistake corrected.) Every coloring of <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> induces a coloring  of <img src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \times H" class="latex" title="G \times H" />, and so is every coloring of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" />.  Therefore, <img src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G+%5Ctimes+H%29+%5Cle+%5Cmin+%28%5Cchi+%28G%29%2C+%5Cchi+%28H%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\chi (G \times H) \le \min (\chi (G), \chi (H))" class="latex" title="\chi (G \times H) \le \min (\chi (G), \chi (H))" />. Hedetniemi conjectured that equality always hold and this is now refuted by  by Yaroslav Shitov.</p>
<p>The example and the entire proof are quite short (the entire paper is less than 3 pages; It is a bit densely-written).</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/js.png"><img src="https://gilkalai.files.wordpress.com/2019/05/js.png?w=640" alt="" class="alignnone size-full wp-image-17450" /></a></p>
<p><span style="color: #ff0000;"><strong> Yaroslav Shitov </strong></span></p>
<p>To tell you what the construction is, I need two important definitions.  The first  is the notion of the exponential graph <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\cal E}_c(H)" class="latex" title="{\cal E}_c(H)" />.</p>
<p>The exponential graph  <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\cal E}_c(H)" class="latex" title="{\cal E}_c(H)" /> arose in the study of Hedetniemi’s conjecture in a 1985 paper by El-Zahar and Sauer. The vertices of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\cal E}_c(H)" class="latex" title="{\cal E}_c(H)" /> are all maps from <img src="https://s0.wp.com/latex.php?latex=V%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V(H)" class="latex" title="V(H)" /> to <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cc%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{1,2,\dots,c\}" class="latex" title="\{1,2,\dots,c\}" />. Two maps <img src="https://s0.wp.com/latex.php?latex=%5Cphi%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi, \psi" class="latex" title="\phi, \psi" /> are adjacent if whenever <img src="https://s0.wp.com/latex.php?latex=v%2Cu&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v,u" class="latex" title="v,u" /> are adjacent vertices of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> then <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28u%29+%5Cne+%5Cpsi+%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\phi(u) \ne \psi (v)" class="latex" title="\phi(u) \ne \psi (v)" />.  <a href="https://link.springer.com/article/10.1007/BF02579374">El-Zahar and Sauer showed</a> that importance of the case  that <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is a graph and <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is an exponential graph of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> for Hedetniemi’s conjecture. (The entire conjecture reduces to this case.) It is thus crucial to study  coloring of exponential graphs which is the subject of the three claims of Section 1 of Shitov’s paper.</p>
<p>The second definition is another important notion of product of graphs: <a href="https://en.wikipedia.org/wiki/Strong_product_of_graphs">The strong product <i>G</i> ⊠ <i>H </i>of two graphs <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> and <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" />.</a> The set of vertices is again the Cartesian product of the two sets of vertices. This time,  <img src="https://s0.wp.com/latex.php?latex=%28g_1%2Ch_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(g_1,h_1)" class="latex" title="(g_1,h_1)" /> and <img src="https://s0.wp.com/latex.php?latex=%28g_2%2Ch_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(g_2,h_2)" class="latex" title="(g_2,h_2)" /> are adjacent in <i>G</i> ⊠ <i>H</i> if either</p>
<p>(a) <img src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_1" class="latex" title="g_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_2" class="latex" title="g_2" /> and  <img src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_1" class="latex" title="h_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_2" class="latex" title="h_2" /></p>
<p>OR</p>
<p>(b) <img src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_1" class="latex" title="g_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_2" class="latex" title="g_2" /> and  <img src="https://s0.wp.com/latex.php?latex=h_1+%3D+h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_1 = h_2" class="latex" title="h_1 = h_2" /> or <img src="https://s0.wp.com/latex.php?latex=g_1+%3Dg_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="g_1 =g_2" class="latex" title="g_1 =g_2" /> and  <img src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_1" class="latex" title="h_1" /> is adjacent to <img src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_2" class="latex" title="h_2" /></p>
<p>(The edges of condition (a) are the edges of the <em>tensor product</em> of the two graphs and the edges of condition (b) are the edges of the <em>Cartesian product</em> of the two graphs.)</p>
<p>For Shitov’s counterexample given in Section 2 of his paper, <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H" class="latex" title="H" /> is the strong product of a graph <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L" class="latex" title="L" /> with girth at least 10 and fractional chromatic number at least 4.1 with a large clique of size <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="q" class="latex" title="q" />. The second graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G" class="latex" title="G" /> is the exponential graph <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="{\cal E}_c(H)" class="latex" title="{\cal E}_c(H)" />. Put <img src="https://s0.wp.com/latex.php?latex=c+%3D%5Clceil+4.1q+%5Crceil&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c =\lceil 4.1q \rceil" class="latex" title="c =\lceil 4.1q \rceil" />.  Shitov shows that when <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c" class="latex" title="c" /> is sufficiently large then  the chromatic number of both <img src="https://s0.wp.com/latex.php?latex=H%2CG&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="H,G" class="latex" title="H,G" /> is <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c" class="latex" title="c" />,  but the chromatic number of their tensor product is smaller than <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c" class="latex" title="c" />.</p>
<p><span style="color: #ff0000;"> (Have a look also at Yaroslav’s other <a href="https://arxiv.org/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Shitov&amp;terms-0-field=author&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">arXived papers</a>! )</span></p>
<h3>Finite and infinite combinatorics</h3>
<p>Let me make one more remark. (See the <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Wikipedea article</a>.) The infinite version of Hedetniemi’s conjecture was known to be false.  Hajnal (1985) gave an example of two infinite graphs, each requiring an uncountable number of colors, such that their product can be colored with only countably many colors. Rinot (2013) proved that in the <a href="https://en.wikipedia.org/wiki/Constructible_universe" title="Constructible universe">constructible universe</a>, for every infinite cardinal <span class="mwe-math-element"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54ddec2e922c5caea4e47d04feef86e782dc8e6d" alt="\kappa " class="mwe-math-fallback-image-inline" /></span>, there exist a pair of graphs of chromatic number greater than <span class="mwe-math-element"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54ddec2e922c5caea4e47d04feef86e782dc8e6d" alt="\kappa " class="mwe-math-fallback-image-inline" /></span>, such that their product can still be colored with only countably many colors. (<a href="https://arxiv.org/pdf/1307.6841.pdf">Here is the paper</a>.) Is there a relation between the finite case and the infinite case? (Both theories are quite exciting but direct connections are rare. A rare statement where the same proof applies for the finite and infinite case is the inequality <img src="https://s0.wp.com/latex.php?latex=2%5En%3En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2^n&gt;n" class="latex" title="2^n&gt;n" />.</p>
<h3>More information</h3>
<p>Here is a link to a survey article by Claude Tardif, (2008), <a href="http://www.mast.queensu.ca/~ctardif/articles/gtn5406rp.pdf" class="external text" rel="nofollow">“Hedetniemi’s conjecture, 40 years later”</a> .</p>
<p>A few more thing worth knowing:</p>
<p>1) The weak version of the conjecture that asserts that If <img src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G%29%3D+%5Cchi+%28H%29%29%3Dn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\chi (G)= \chi (H))=n" class="latex" title="\chi (G)= \chi (H))=n" />, then <img src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G+%5Ctimes+H%29+%5Cge+f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\chi (G \times H) \ge f(n)" class="latex" title="\chi (G \times H) \ge f(n)" /> where <img src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f(n)" class="latex" title="f(n)" /> tends to infinity with <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> is still open.</p>
<p>2)  Xuding Zhu proved in 2011 that <a href="https://www.sciencedirect.com/science/article/pii/S0195669811000552">the fractional version of the conjecture is correct</a>,</p>
<p>3) The directed version of the conjecture was known to be false (Poljak and Rodl, 1981).</p>
<p>4) The conjecture is part of a rich and beautiful theory of graph homomorphisms (and the category of graphs) that I hope to come back to in another post.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/"><span class="datestr">at May 10, 2019 12:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/10/postdoc-in-drone-safety-at-department-of-electronic-systems-aalborg-university-apply-by-june-6-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/10/postdoc-in-drone-safety-at-department-of-electronic-systems-aalborg-university-apply-by-june-6-2019/">Postdoc in drone safety at Department of Electronic Systems, Aalborg University (apply by June 6, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>At the Faculty of IT and Design, Department of Electronic Systems, a position as postdoc in the research project SafeEYE is open for appointment from July 1st, 2019, or soon hereafter, and until May 31, 2021.</p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1042671">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1042671</a><br />
Email: alc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/10/postdoc-in-drone-safety-at-department-of-electronic-systems-aalborg-university-apply-by-june-6-2019/"><span class="datestr">at May 10, 2019 07:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/">Theory and Practice of Differential Privacy (part of CCS 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
November 11, 2019 London, UK https://tpdp.cse.buffalo.edu/2019/ Submission deadline: June 21, 2019 The fifth annual workshop on the Theory and Practice of Differential Privacy will be held as a part of CCS 2019 on November 11. We seek contributions from different research areas of computer science and statistics. Authors are invited to submit a short abstract … <a href="https://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/" class="more-link">Continue reading <span class="screen-reader-text">Theory and Practice of Differential Privacy (part of CCS 2019)</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/"><span class="datestr">at May 10, 2019 03:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/068">TR19-068 |  LARGE CLIQUE IS HARD ON AVERAGE FOR RESOLUTION | 

	Shuo Pang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove resolution lower bounds for $k$-Clique on the Erdos-Renyi random graph $G(n,n^{-{2\xi}\over{k-1}})$ (where $\xi&gt;1$ is constant). First we show for $k=n^{c_0}$, $c_0\in(0,1/3)$, an $\exp({\Omega(n^{(1-\epsilon)c_0})})$ average lower bound on resolution where $\epsilon$ is arbitrary constant. 

We then propose the model of $a$-irregular resolution. Extended from regular resolution, this model is interesting in that the power of general-over-regular resolution from all {\it known} exponential separations is below it. We prove an $n^{\Omega(k)}$ average lower bound of $k$-Clique for this model, for {\it any} $k&lt;n^{1/3-\Omega(1)}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/068"><span class="datestr">at May 09, 2019 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-722419176401069744">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html">Multiple Provers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Just over thirty years ago on May 5, 1989, I defended my PhD Thesis <a href="https://lance.fortnow.com/papers/files/thesis.pdf">Complexity-Theoretic Aspects of Interactive Proof Systems</a>. It's starts off with a parable for interactive proof systems.<br />
<blockquote class="tr_bq">
Victor, a venture capitalist, had everything a man could desire: money, women and power.
But he felt something missing. He decided he lacked knowledge. So Victor packed up his
bags and headed to the Himalayas in search of ultimate truths.
The natives pointed Victor to a tall mountain and mentioned rumors of a great man full of
wisdom. Victor, who smartly brought some climbing equipment, tackled the mountain until
he reached a small cave near the summit. Victor found the great Pulu, grand guru of all that
is known. Victor inquired to some ultimate truths and Pulu responded,
<i>I will teach you but you must not trust my words</i>.
Victor agreed and found he learned much even though he had to verify all the sayings
of the great Pulu. Victor though lacked complete happiness and he asked if he could learn
knowledge beyond what he could learn in this manner. The grand guru replied,
<i>You may ask and I will answer</i>.
Victor pondered this idea for a minute and said, "Since you know all that is known, why can you not predict my questions?" A silence reigned over the mountain for a short while until the guru  finally spoke,
<i>You must use other implements, symbols of your past life</i>.
Victor thought for a while and reached into his backpack and brought out some spare
change he had unwittingly carried with him. Even the great Pulu can not predict the flip of
a coin. He started flipping the coins to ask the guru and wondered what can I learn now?</blockquote>
Without the coins, one gets the complexity class NP. My thesis didn't answer the last question, but by the end of the year, <a href="https://doi.org/10.1109/FSCS.1990.89519">Shamir</a> building on work of <a href="https://doi.org/10.1145/146585.146605">Lund, Fortnow, Karloff and Nisan</a> showed this class IP was equal to PSPACE, the problems we could solve in a polynomial amount of memory.<br />
<br />
Part of my thesis explored the class MIP where we had multiple Pulus (provers) on different mountain tops unable to communicate. The news was disappointing, we failed to get a PSPACE upper bound for MIP, only NEXP (nondeterministic exponential time) and our proof that two provers sufficed relied on a bad assumption on how errors get reduced when you run multiple protocols in parallel. Later Babai, Lund and myself showed <a href="http://doi.org/10.1007/BF01200056">MIP = NEXP</a> and Ran Raz <a href="https://doi.org/10.1137/S0097539795280895">showed</a> parallel repetition does reduce the error sufficiently.<br />
<br />
Back in the 80's we didn't even imagine the possibility that the Pulus had shared entangled quantum bits. Does the entanglement allow the provers to cheat or can the entanglement allow them to prove more things? Turns out to be much more, as a <a href="https://arxiv.org/abs/1904.05870">new result</a> by Anand Natarajan and John Wright shows that MIP*, MIP with classical communication, classical verifier and two provers with previously entangled quantum bits, can compute everything in NEEXP, nondeterministic double exponential time. This is only a lower bound for MIP*, possibly one can do even more.<br />
<br />
Neat to see my three-decade old thesis explored ideas that people are still thinking about today.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html"><span class="datestr">at May 09, 2019 12:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=353">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/05/08/tcs-talk-wednesday-may-15th-ewin-tang-university-of-washington/">TCS+ talk: Wednesday, May 15th — Ewin Tang, University of Washington</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 15th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="https://ewintang.com/" target="_blank" rel="noopener">Ewin Tang</a></strong> from University of Washington will tell us about “<em>Quantum-inspired classical linear algebra algorithms: why and how?</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Over the past ten years, the field of quantum machine learning (QML) has produced many polylogarithmic-time procedures for linear algebra routines, assuming certain “state preparation” assumptions. Though such algorithms are formally incomparable with classical computing, a recent line of work uses an analogous classical model of computation as an effective point of comparison to reveal speedups (or lack thereof) gained by QML. The resulting “dequantized” algorithms assume sampling access to input to speed up runtimes to polylogarithmic in input size.</p>
<p>In this talk, we will discuss the motivation behind this model and its relation to existing randomized linear algebra literature. Then, we will delve into an example quantum-inspired algorithm: Gilyen, Lloyd, and Tang’s algorithm for low-rank matrix inversion. This dequantizes a variant of Harrow, Hassidim, and Lloyd’s matrix inversion algorithm, a seminal work in QML. Finally, we will consider the implications of this work on exponential speedups in QML. No background of quantum computing is assumed for this talk.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/05/08/tcs-talk-wednesday-may-15th-ewin-tang-university-of-washington/"><span class="datestr">at May 08, 2019 10:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/067">TR19-067 |  Sign rank vs Discrepancy | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Sign-rank and discrepancy are two central notions in communication complexity. The seminal work of  Babai, Frankl, and Simon from 1986  initiated an active line of research that investigates  the gap between these two notions.
In this article, we establish the strongest possible separation  by constructing a Boolean matrix whose sign-rank is only $3$, and yet its discrepancy is  $2^{-\Omega(n)}$. We note that every matrix of sign-rank $2$ has discrepancy $n^{-O(1)}$.
Our result in particular implies that there are Boolean functions with $O(1)$ unbounded error randomized communication complexity while having $\Omega(n)$ weakly unbounded error randomized communication complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/067"><span class="datestr">at May 07, 2019 09:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15840">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/">The Network Coding Conjecture Is Powerful</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>More hard Boolean functions</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg"><img src="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg?w=197&amp;h=162" alt="" width="197" class="alignright wp-image-15841" height="162" /></a></p>
<p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent <a href="https://arxiv.org/abs/1902.10935">paper</a> which we just <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">discussed</a>. </p>
<p>
Today Ken and I will update our discussion. </p>
<p>
Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL is:</p>
<blockquote><p><b>Theorem 1</b> <em> If the NCC is true, then every Boolean circuit that computes the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function has size <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" />. </em>
</p></blockquote>
<p></p><p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function is: Given an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit number <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and a number <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \le k \le n}" class="latex" title="{1 \le k \le n}" />, compute the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k}}" class="latex" title="{2^{k}}" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Ctimes+2%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x \times 2^{k}. " class="latex" title="\displaystyle  x \times 2^{k}. " /></p>
<p>This is a special case of the integer multiplication problem. In symbols it maps <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B0%5Ek+x+0%5E%7Bn-k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^k x 0^{n-k}}" class="latex" title="{0^k x 0^{n-k}}" />, as in our photo above. </p>
<p>
Our point, however, is not about integer multiplication. Nor even about NCC—no knowledge of it will be needed today, so read on even if you are not aware of NCC. No. Our point is that a whole lot of other Boolean functions would inherit the same <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> circuit lower bound as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. And several aspects of that seem troubling.<span id="more-15840"></span></p>
<p>
</p><p></p><h2> Some Worry </h2><p></p>
<p></p><p>
We are impressed by the AFKL paper but also worried. Proving a super-linear lower bound in the unrestricted Boolean complexity model has long been considered a difficult problem. Maybe a hopeless problem. Yes they are proving it not for a single-output function; they are proving it for a multiple-output function. Still I thought that it seems too good to be correct. Even worse, assuming NCC they also resolve other open problems in complexity theory. I am worried.</p>
<p>
What we suggest is to catalog and study the consequences of their results. If we find that their results lead to a contradiction, then there was something to be worried about. Or perhaps it would mean that NCC is false. If we find no contradiction, then everything we discover is also a consequence of NCC. Either way we learn more.</p>
<p>
</p><p></p><h2> AFKL Functions </h2><p></p>
<p></p><p>
Let’s call a Boolean function an <i>AFKL function</i> provided it has Boolean circuit complexity <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" /> if the NCC is true. Thanks to AFKL, we now know that integer multiplication is an AFKL function. I started to think about: What functions are in this class? Here are some examples: </p>
<ul>
<li>
Integer multiplication <p></p>
</li><li>
Integer multiplication by a power of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> <p></p>
</li><li>
Discrete convolution <p></p>
</li><li>
Sparse convolution
</li></ul>
<p>
We describe the last three next. We show they have linear size-preserving reductions from the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> function.</p>
<p>
</p><p></p><h2> The Flip Function </h2><p></p>
<p></p><p>
Define <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%3A+%5C%7B0%2C1%5C%7D%5E%7Bn%7D+%5Crightarrow+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" class="latex" title="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BFLIP%7D_%7Bn%7D%280%5E%7Bk%7D1%5Calpha%29%3D+1%5Calpha+0%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " class="latex" title="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \ge 1}" class="latex" title="{k \ge 1}" />. For any input not of this form, let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> be <img src="https://s0.wp.com/latex.php?latex=%7B0%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^{n}}" class="latex" title="{0^{n}}" />.</p>
<blockquote><p><b>Theorem 2</b> <em> The Boolean function <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is an AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,k}" class="latex" title="{x,k}" /> be the input to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x| = n}" class="latex" title="{|x| = n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \leq n}" class="latex" title="{k \leq n}" /> in binary. In linear size we can test <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 0}" class="latex" title="{k = 0}" />, when there is nothing to do, so presume <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k \geq 1}" class="latex" title="{k \geq 1}" />. The first step is to create </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1}. " class="latex" title="\displaystyle  0^{n-k}10^{k-1}. " /></p>
<p>This is just binary-to-unary conversion and has linear-size circuits—as in multiplex decoding and as remarked by AFKL. This becomes the first <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits of an application of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> to the <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" />-bit string </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D+x.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0^{n-k}10^{k-1} x. " class="latex" title="\displaystyle  0^{n-k}10^{k-1} x. " /></p>
<p>It yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++10%5E%7Bk-1%7D+x+0%5E%7Bn-k%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  10^{k-1} x 0^{n-k}. " class="latex" title="\displaystyle  10^{k-1} x 0^{n-k}. " /></p>
<p>Changing the first bit to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> then leaves the desired output of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{shift}}" class="latex" title="{\mathsf{shift}}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The point is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}_{n}}" class="latex" title="{\mathsf{FLIP}_{n}}" /> is a super-simple function. It just moves the initial block of <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />‘s in a string to the end. It is amazing that this function should have only non-linear, indeed <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />-sized, circuits. </p>
<p>
This also means that Ken’s <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">function</a>, which takes <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%2C2%5C%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in \{0,1,2\}^*}" class="latex" title="{x \in \{0,1,2\}^*}" /> and moves all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />s to the end of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, is hard even in the special cases where all the <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />‘s are at the front. What’s strange is that Ken proves his function equivalent to another special case where <img src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|x|}" class="latex" title="{|x|}" /> is even and exactly half the characters are <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. This latter case is one in which <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> is easy, but the two cases are separate. All this is touch-and-go enough to compound our “worry.”</p>
<p>
</p><p></p><h2> The Sparse Convolution Function </h2><p></p>
<p></p><p>
The following is also an AFKL function. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi%3D1%7D%5E%7Bn-l%7D+w_%7Bi%7D+%5Cwedge+x_%7Bi%2Bl%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " class="latex" title="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bl%3D1%2C%5Cdots%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{l=1,\dots,n}" class="latex" title="{l=1,\dots,n}" /> where an empty OR is defined to be <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. This can even further be restricted to the case where exactly one of the <img src="https://s0.wp.com/latex.php?latex=%7Bw_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_{i}}" class="latex" title="{w_{i}}" /> are <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> and the rest are <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. Call this the <i>sparse convolution function</i>.</p>
<blockquote><p><b>Theorem 3</b> <em> The sparse convolution is a monotone AFKL function. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  We will give a sketch of why this is true. Define 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi+%3D+1%7D%5E%7Bn-l%7D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D+%5Cwedge+x_%7Bi%2Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " class="latex" title="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " /></p>
<p>It is not hard to show that this yields the FLIP function. We can reduce computing it to a convolution of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{i}}" class="latex" title="{x_{i}}" />‘s and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28i%29+%3D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " class="latex" title="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " /></p>
<p>The key is to note that exactly one <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Gamma(i)}" class="latex" title="{\Gamma(i)}" /> will be non-zero, and so the convolution is sparse. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The sparse convolution function raises an interesting question: Are the methods for sparse FFT useful here? The lower bound for AFKL functions suggests that they are not applicable. </p>
<p>
</p><p></p><h2> Is the NCC-Boolean Connection New? </h2><p></p>
<p></p><p>
The subtitle of our <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">post</a> marveled that a core-theory advance on circuits for multiplication had come via the practical side of throughput in computer networks. AFKL deserve plaudits for linking two communities. We should mention that one theoretician we both know well, Mark Braverman, with his students Sumegha Garg and Ariel Schvartzman at Princeton, <a href="https://arxiv.org/pdf/1608.06545.pdf">proved</a> a fact about NCC that is relevant to this discussion:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="BGS"></a> Either NCC is false, or bit-operations save a whole <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7B%5COmega%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{(\log n)^{\Omega(1)}}" class="latex" title="{(\log n)^{\Omega(1)}}" /> factor in the network size. </em>
</p></blockquote>
<p></p><p>
Even this paper, however, does not address lower bounds on Boolean circuits. The only prior link between NCC and Boolean complexity is a 2007 <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v14i1r44">paper</a> by Søren Riis, which is cited by AFKL, and has a 2011 <a href="http://emis.impa.br/EMIS/journals/EJC/Volume_18/PDF/v18i1p192.pdf">followup</a> by Demetres Christofides and Klas Markström. The paper by Riis has a new “guessing game” on graphs and a demonstration that a lower-bound conjecture of Leslie Valiant needs to be rescued by dividing by a <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> factor. Theorem <a href="https://rjlipton.wordpress.com/feed/#BGS">4</a>, however, seems to say that no such <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n}" class="latex" title="{\log\log n}" /> shading can apply to NCC.</p>
<p>
When we ask Google “network coding conjecture Boolean circuit lower bounds” (without quotes), the first page shows AFKL, our posts, and this 2014 <a href="https://people.csail.mit.edu/rrw/ccc14-survey.pdf">survey</a> by Ryan Williams—which mentions neural networks but not NCC. On the next page of hits we see Riis and the followup paper but nothing else that seems directly relevant. Nor does appending `-multiplication’ help screen out AFKL and our posts.</p>
<p>
There is said to be empirical evidence for NCC. We wonder, however, whether that has reached the intensity of thought about circuit lower bounds. We say this because the implications from NCC make three giant steps:</p>
<ol>
<li>
Not only does it assert a super-linear circuit lower bound (okay, for a function with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> output bits), but… <p></p>
</li><li>
…it asserts <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" />… <p></p>
</li><li>
…for functions that are easily in Turing machine linear time.
</li></ol>
<p>
So one side of our worry is whether NCC can actually shed light on so many fundamental issues from complexity theory, more than absorbing light. At the very least, AFKL have re-stimulated interest in all of these issues. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{FLIP}}" class="latex" title="{\mathsf{FLIP}}" /> hard? Is NCC true? What other Boolean functions are AFKL functions? What about other consequences of the NCC to complexity theory?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/"><span class="datestr">at May 07, 2019 03:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kintali.wordpress.com/?p=1235">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kintali.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/">Preventing future college admissions scandals using Blockchain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="graf graf--p graf-after--h3" id="5bf6">The recent <a href="https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener" target="_blank">college admissions scandal</a> resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases like “broken admissions system”, “rich can buy their way into college”, etc etc.</p>
<p class="graf graf--p graf-after--p" id="4b6a">There must be several other cases of fraud that go unnoticed on a daily basis. It’s in human nature to shortcut the rules, collude and cheat to achieve one’s own short-term goals, hoping to get away with one’s fraudulent actions. Let’s discuss how to use Blockchain technology and create a rigorous system to prevent some aspects of such scandals in future.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b66e"><strong class="markup--strong markup--p-strong">Problems</strong>: Fake athletic certificates and phony athletic profile. Taking photos of students on a stationary rowing machine. Photoshopping students’ face on another athlete’s photo.</p>
<p class="graf graf--p graf-after--p" id="ad90"><strong class="markup--strong markup--p-strong">Solution</strong>: A genuine high-school athlete achieves his/her athletic credentials during a four year period. Getting a genuine athletic certificate involves achieving several intermediate goals. For example, if you achieved a black belt in karate in your 11th grade, you must have progressed through beginner level (with white, yellow and orange belts), intermediate level (with green, blue, purple, brown and red belts) and then reached the advanced level (with a black belt). <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">TrueCerts</a> technology allows sports coaching centers to create certificates for each of these intermediate athletic achievements, sign them using their private keys and post only the SHA-256 hash of the certificate on a public blockchain, thus immutably time-stamping each achievement at the specific day/time the athlete achieved it. The athletic photos can be time-stamped similarly.</p>
<p class="graf graf--p graf-after--p" id="2c0c">This process achieves the seemingly impossible combination of <strong class="markup--strong markup--p-strong">security, privacy and transparency </strong>!! Security is achieved by the asymmetric key encryption. Privacy is achieved by the one-way nature of the cryptographic hash function <a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">SHA 256</a>. Transparency is achieved by the fact that anybody with the access to the sports certificate (upon student’s consent) can compute the SHA 256 hash of the certificate and verify that it is stored on a public blockchain validly signed by the private key of the issuer (sports coaching center). This eliminates the fraudulent behavior of creating a bunch of fake credentials and photos, all at once, in a brief period of time. Using a fairly decentralized public blockchain is very important here.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="c253"><strong class="markup--strong markup--p-strong">Problem</strong>: Bribing coaches to accept certain students in their sports teams or issue fake credentials.</p>
<p class="graf graf--p graf-after--p" id="6290"><strong class="markup--strong markup--p-strong">Solution</strong>: This problem can be solved by having several people (perhaps five coaches, some administrative assistants, one principal, one vice-principal, etc) in the organization collectively responsible (using multi-signature wallets and m-of-n signatures) to issue credentials. Each of the involved person is accountable for every issued certificate.</p>
<p class="graf graf--p graf-after--p" id="eb99">Bribing one coach is easy. Bribing ten people is hard. People often hesitate bribing multiple people. Collusion becomes increasingly hard when you increase the size of the group involved. When there are more people involved, there is a higher chance that at least one of them is honest (and brave) to overcome the pressure of the others and blow the whistle.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="0d42"><strong class="markup--strong markup--p-strong">Problem</strong>: Fake college entrance exam (SAT, ACT) test scores</p>
<p class="graf graf--p graf-after--p" id="fe6e"><strong class="markup--strong markup--p-strong">Solution</strong>: Current paper-based test score issuance and verification system is too time-consuming and error-prone. These credentials can be easily faked or tampered with. An ideal solution involves creating a digital certificate, validly signed (or multi-signed) by the issuer and time-stamped with a <a href="https://en.wikipedia.org/wiki/SHA-2" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SHA 256</a> hash on a public blockchain. See <a href="https://medium.com/@truecerts/dr-kintalis-motivation-behind-developing-truecerts-platform-aba93f4d290a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">my previous post</a> about preventing fraud in academic transcripts and making the entire system efficient.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b957"><strong class="markup--strong markup--p-strong">Problem</strong>: Other students taking entrance tests on your behalf.</p>
<p class="graf graf--p graf-after--p" id="0e07"><strong class="markup--strong markup--p-strong">Solution</strong>: This is a problem of verifying the identity of the student taking the test. The current system of using paper-based credentials is broken. It’s easy to create fake driver’s license, passports etc. At <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>, we have created an automated multi-step identity verification that combines your standard KYC identity procedures, utility bills and more importantly biometrics. We define identity as a several data points achieved over a period of time, not just one piece of paper. It’s very hard to cheat all of these steps. If you have a look-alike twin then consider yourself lucky <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /></p>
<p> </p>
<p class="graf graf--p graf-after--p" id="a16a"><strong class="markup--strong markup--p-strong">Partially solvable problems</strong>: Bribing officials to change student’s answers in paper-based exam can be solved to certain extent by using a completely computer-based exam. Bribing proctors to tell the answers to a student (during the test) can be solved with computer-vision-based cheating detection software. Some photoshopped images can be detected using image analysis techniques.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="4d0f"><strong class="markup--strong markup--p-strong">Hard to solve problems</strong>: (1) Getting a fake medical certificate that your kid requires an isolated room to take the test and then bribing the proctor to tell him/her all the answers during the test. (2) Laundering bribes using a non-profit entity. Phew…. These things actually happened during this college admissions scandal. As the famous saying goes “a person is capable of as much atrocity as he/she has imagination”.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="5ad0">In summary, combining the existing technologies we can solve several of the above mentioned problems and simultaneously achieve <strong class="markup--strong markup--p-strong">security, privacy and transparency</strong>. The main goal here is to make the bad people’s job as difficult as possible and simultaneously making the good people’s job very efficient.</p>
<p class="graf graf--p graf-after--p" id="a4f5">If you want to know more about how we (at <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TrueCerts</a>) are using Blockchain and other technologies to prevent fraud and corruption in broad range of areas, <a href="https://truecerts.co/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">contact us</a>.</p>
<p class="graf graf--p graf-after--p" id="e889">Stay tuned for my next post about a huge list of real-world fraud and corruption stories that can be prevented rigorously by using cutting-edge technologies.</p>
<p><img src="https://kintali.files.wordpress.com/2019/05/truecertstrustsimplified.png?w=660" alt="TrueCertsTrustSimplified" class="alignnone size-full wp-image-1236" /></p></div>







<p class="date">
by kintali <a href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/"><span class="datestr">at May 07, 2019 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2431469456247088523">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html">Thoughts on the recent Jeopardy streak (SPOILERS)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
James Holzhauer  has won 22 consecutive games of Jeopardy and has made around 1.6 million dollars. Nice work if you can get it. Here are some thoughts no this<br />
<br />
1) Before James H the records for number of consecutive games was, and still is, Ken Jennings winning 74 in a row, and second place was 20. I was surprised that Ken was that much better than the competition.<br />
<br />
2) Before James H the record for amount of money in normal play (not extra from, say, tournament of champions or losing to a computer) was around 400,000. I was surprised that Ken was that much better than the competition.<br />
<br />
3) James is obliterating the records for most wins in a single game. He holds the top 12 records for this.  This is due to his betting A LOT on the daily doubles and the final jeop, as well as of course answering so many questions right.<br />
<br />
4) One reason players in Jeopardy don't have long streaks is fatigue. The actually play<br />
5 games a day, two days of the week.  James H is getting a break since he has two weeks off now since they will soon have the Teachers Tournament. This could work either way--- he gets a break or he loses being in-the-zone.<br />
<br />
5) James strategy is:<br />
<br />
a) Begin with the harder (and more lucrative) questions.<br />
<br />
b) Bet A LOT on the daily doubles (which are more likely to be in the more lucrative questions) and almost always go into final jeop with more than twice your opponent (He failed to do this only once.)<br />
<br />
c) Bet A LOT on Final Jeop- though not enough so that if you lose you lose the game. I think he's gotten every Final Jeop question right.<br />
<br />
For more on his strategy see this article by Oliver Roeder in Nate Silvers Blog: <a href="https://fivethirtyeight.com/features/the-man-who-solved-jeopardy/">here</a><br />
<br />
6) I tend to think of this as being a high-risk, high-reward strategy and thus it is unlikely he will beat Ken Jennings, but every time he wins that thought seems sillier and sillier. While we are here, how likely is it that someone will beat Ken Jennings? In an article before all of this Ben Morrison in Nate Silvers Blog wrote that it was quite likely SOMEONE would break Ken J's record,  see <a href="https://fivethirtyeight.com/features/ken-jennings-has-nothing-on-joe-dimaggio/">here</a>.<br />
<br />
7) OKAY, how does James H compare to Ken J? According to Oliver Roeder in Nate Silvers Blog,<br />
<a href="https://fivethirtyeight.com/features/the-battle-for-jeopardy-supremacy/">here</a>, they are similar in terms of percent of questions answered right, but James H bets so much more (bets better?) which is why he is getting so much money. I'll be curious to see a head-to-head contest at some point. But to the issue at hand, they don't give James H that good a chance to break Ken J's record.<br />
<br />
8) Jeop used to have a  5-game limit. Maybe that was a good idea- its not that interesting seeing the same person with the same strategy win 22 in a row. Also, the short-talk-with-Alex T-- James is running out of interesting things to say. I wonder what Alex did with Ken J after 50 games.<br />
``So Ken, I hear you're good at Jeopardy''<br />
<br />
9) Misc: Ken J was the inspiration for IBM to do Watson.<br />
<br />
10) Will future players use James Strategy? Note that you have to be REALLY GOOD in the first place for it to help you. Maybe a modified version where you go for the lucrative questions and bet a lot on Daily Doubles (more than people have done in the past) when its an area you know really well (I'll take Ramsey Theory for $2000.)<br />
<br />
11) I used to DVR and watch Jeop but didn't mind if I was a few behind. Now I have to stay on top of it so articles like those pointed to above don't give me a spoiler.<br />
<br />
12) My prediction: He will beat Ken Jenning for money but not for number-of-games. I have no real confidence in these predictions.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html"><span class="datestr">at May 07, 2019 12:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/">Answer to TYI 37: Arithmetic Progressions in 3D Brownian Motion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Consider a Brownian motion in three dimensional space. <a href="https://gilkalai.wordpress.com/2019/03/07/test-your-intuition-or-simply-guess-37-arithmetic-progressions-for-brownian-motion-in-space/">We asked (TYI 37)</a> What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, <img src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_1,x_2, x_t" class="latex" title="x_1,x_2, x_t" />, so that all <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2B1%7D-x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x_{i+1}-x_i" class="latex" title="x_{i+1}-x_i" /> are equal.)</p>
<p>Here is what you voted for</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png?w=640" alt="" class="alignnone size-full wp-image-17423" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Final-results</strong></span></p>
<p>Analysis of the poll results:  Almost surely 2 is the winner with 30.14% of the 209 votes, and almost surely infinity (28.71%) comes close at second place. In the  third place is  almost surely 3 (14.83%),  and then comes positive probability for each integer (13.4%), almost surely 5 (5.26%),  almost surely 6 (2.87%), and  almost surely 4 (2.39%).</p>
<h2>Test your political intuition: which coalition is going to be formed?</h2>
<p>Almost surely 2 (briefly AS2) and almost surely infinity (ASI) can form a government  with no need for a larger coalition. But they represent two political extremes. Is AS3 politically closer to AS2 or to ASI? “k with probability p_k for every k&gt;2” (briefly, COM) represent a complicated political massage. Is it closer to AS2 or to ASI? (See the old posts on <a href="https://gilkalai.wordpress.com/2009/02/16/which-coalition/">which coalition</a> <a href="https://gilkalai.wordpress.com/2009/02/17/which-coalition-to-form-2/">will be formed</a>.)</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-br.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-br.png?w=131&amp;h=300" alt="" width="131" class="alignnone size-medium wp-image-17424" height="300" /></a> <a href="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png?w=133&amp;h=300" alt="" width="133" class="alignnone size-medium wp-image-17425" height="300" /> </a><a href="https://gilkalai.files.wordpress.com/2019/05/poll189.png"><img src="https://gilkalai.files.wordpress.com/2019/05/poll189.png?w=127&amp;h=300" alt="" width="127" class="alignnone size-medium wp-image-17427" height="300" /></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Partial results. It was exciting to see how the standing of the answers changed in the process of counting the votes.</strong></span></p>
<p>And the correct answer is: <span id="more-17422"></span></p>
<h2><strong>5 (FIVE)</strong></h2>
<p>See the paper:</p>
<p class="title mathjax">Itai Benjamini and Gady Kozma: <a href="https://arxiv.org/abs/1810.10077">Arithmetic progressions in the trace of Brownian motion in space</a></p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/"><span class="datestr">at May 06, 2019 09:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4240">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Online Optimization Post 3: Follow the Regularized Leader</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its analysis have statistical physics interpretations, but even this observation brings up more questions than it answers. The Gibbs distribution, for example, does put more weight on lower-energy states, and so it makes sense in an optimization setting, but to get good approximations one wants to use lower temperatures, while the distributions used by the multiplicative weights algorithms have temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\epsilon}" class="latex" title="{2\epsilon}" /> is the final “amortized” regret bound, so that one uses, quite counterintuitively, higher temperatures for better approximations. </p>
<p>
Furthermore, it is not clear how we would generalize the ideas of multiplicative weights to the case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is anything other than the set of distributions.</p>
<p>
Today we discuss the <em>“Follow the Regularized Leader”</em> method, which provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will then see how we can “discover” the definition and analysis of multiplicative weights, and how to “discover” another online algorithm which can be seen as a generalization of projected gradient descent (that is, one can derive the projected gradient descent algorithm and its analysis from this other online algorithm).</p>
<p>
<span id="more-4240"></span></p>
<p>
</p><p><b>1. Follow The Regularized Leader </b></p>
<p></p><p>
We will first state some results in full generality, making no assumptions on the set <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of feasible solutions or on the set of loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" title="{f_t : K \rightarrow {\mathbb R}}" /> encountered by the algorithm at each step.</p>
<p>
Let us try to define an online optimization algorithm from scratch. The solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> proposed by the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> can only depend on the previous cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_{t-1}}" class="latex" title="{f_1,\ldots,f_{t-1}}" />; how should it depend on it? If the offline optimal solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is consistently better than all others at each time step, then we would like <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be that solution, so we want <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to be a solution that would have worked well in the previous steps. The most extreme way of implementing this idea is the <em>Follow the Leader</em> algorithm (abbreviated FTL), in which we set the solution at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /></p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> to be the best solution for the previous steps. (Note that the algorithm does not prescribe what solution to use at step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1}" class="latex" title="{t=1}" />.)</p>
<p>
It is possible for FTL to perform very badly. Consider for example the “experts” setting in which we analyzed multiplicative weights: the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta}" class="latex" title="{\Delta}" /> of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\ldots,n\}}" class="latex" title="{\{1,\ldots,n\}}" />, and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Csum_i+%5Cell_t%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \sum_i \ell_t(i) x(i)}" class="latex" title="{f_t(x) = \sum_i \ell_t(i) x(i)}" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+%5Cell_t%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0\leq \ell_t(i) \leq 1}" class="latex" title="{0\leq \ell_t(i) \leq 1}" />. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> and that <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%280.5%2C.0.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (0.5,.0.5)}" class="latex" title="{x_1 = (0.5,.0.5)}" />. Then a possible run of the algorithm could be: </p>
<ol>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%28.5%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = (.5,.5)}" class="latex" title="{x_1 = (.5,.5)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1+%3D+%280%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_1 = (0,.5)}" class="latex" title="{\ell_1 = (0,.5)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2 = (1,0)}" class="latex" title="{x_2 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2 = (1,0)}" class="latex" title="{\ell_2 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_3 = (0,1)}" class="latex" title="{x_3 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_3 = (0,1)}" class="latex" title="{\ell_3 = (0,1)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_4 = (1,0)}" class="latex" title="{x_4 = (1,0)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_4 = (1,0)}" class="latex" title="{\ell_4 = (1,0)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_5 = (0,1)}" class="latex" title="{x_5 = (0,1)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_5 = (0,1)}" class="latex" title="{\ell_5 = (0,1)}" />
</li></ol>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cvdots&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \vdots" class="latex" title="\displaystyle \vdots" /></p>
<p>
In which, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm suffers a loss of <img src="https://s0.wp.com/latex.php?latex=%7BT-+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T- O(1)}" class="latex" title="{T- O(1)}" /> while the offline optimum is <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2 + O(1)}" class="latex" title="{T/2 + O(1)}" />. Thus, the regret is about <img src="https://s0.wp.com/latex.php?latex=%7BT%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T/2}" class="latex" title="{T/2}" />, which compares very unfavorably to the <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt T)}" class="latex" title="{O(\sqrt T)}" /> regret of the multiplicative weight algorithm. For general <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, a similar example shows that the regret of FTL can be as high as about <img src="https://s0.wp.com/latex.php?latex=%7BT%5Ccdot+%5Cleft%28+1-+%5Cfrac+1n+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T\cdot \left( 1- \frac 1n \right)}" class="latex" title="{T\cdot \left( 1- \frac 1n \right)}" />.</p>
<p>
In the above bad example, the algorithm keeps “overfitting” to the past history: if an expert is a bit better than the others, the algorithm puts all its probability mass on that expert, and the algorithm keeps changing its mind at every step. Interestingly, this is the only failure mode of the algorithm.</p>
<blockquote><p><b>Theorem 1 (Analysis of FTL)</b> <em> For any sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_t}" class="latex" title="{f_1,\ldots,f_t}" /> and any number of time steps <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, the FTL algorithm satisfies the regret bound </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t+%29+-+f_t%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " /></p>
</em><p><em> </em></p></blockquote>
<p> So that if the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> are Lipschitz with respect to a distance function on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, then the only way for the regret to be large is for <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> to typically be far, in that distance, from <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t+1}}" class="latex" title="{x_{t+1}}" />.</p>
<p>
<em>Proof:</em>  Recalling the definition of regret, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " class="latex" title="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " /></p>
<p> the theorem is equivalent to <a name="ftl.analysis"></a></p><a name="ftl.analysis">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" class="latex" title="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" /></p>
</a><p><a name="ftl.analysis"></a> We will prove <a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a> by induction. The base case <img src="https://s0.wp.com/latex.php?latex=%7BT%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T=1}" class="latex" title="{T=1}" /> is just the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_2}" class="latex" title="{x_2}" />. Assuming that $latex {<a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a>}&amp;fg=000000$ is true up to <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+f_%7BT%2B1%7D+%28x_%7BT%2B2%7D%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7BT%2B2%7D%29+%3D+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " class="latex" title="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " /></p>
<p> where the middle step follows from the use of the inductive assumption, which gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7BT%2B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " class="latex" title="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
The above example and analysis suggest that we should modify FTL in such a way that the choices of the algorithm don’t change too much from step to step, and that the solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> should be a compromise between optimizing with respect to previous cost functions and not changing too much from step to step.</p>
<p>
In order to do this, we introduce a new function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" />, called a <em>regularizer</em> (more on it later), and, at each step, we compute the solution</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " class="latex" title="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " /></p>
<p> This algorithm is called <em>Follow the Regularized Leader</em> or FTRL. Typically, the function <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is chosen to be strictly convex and to take values that are rather big in magnitude. Then <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> will be the unique minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and, at each subsequent step, <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> will be selected in a way to balance the pull toward the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and the pull toward the FTL solution <img src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_k+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\arg\min_{x\in K} \sum_k f_k(x)}" class="latex" title="{\arg\min_{x\in K} \sum_k f_k(x)}" />. In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is large in magnitude compared to each <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" />, the solution will not change too much from step to step.</p>
<p>
We have the following analysis that makes no assumptions on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, on the cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(\cdot)}" class="latex" title="{f_t(\cdot)}" /> and on the regularizer (not even that the regularizer is convex).</p>
<blockquote><p><b>Theorem 2 (Analysis of FTRL)</b> <em> For every sequence of cost functions and every regularizer function, the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps of the FTRL algorithm is bounded as follows: for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_%7Bt%7D%29+-+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+R%28x%29+-+R%28x_1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" /></p>
<p> where </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%28x%29+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " class="latex" title="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let us run for <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the FTRL algorithm with regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> and cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,\ldots,f_T}" class="latex" title="{f_1,\ldots,f_T}" />, and call <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,\ldots,x_T}" class="latex" title="{x_1,\ldots,x_T}" /> the solutions computed by the FTL algorithm. </p>
<p>
Now consider the following mental experiment: we run the FTL algorithm for <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" /> steps, with the sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7BR%2Cf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R,f_1,\ldots,f_t}" class="latex" title="{R,f_1,\ldots,f_t}" />, and we use <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> as a first solution. Then we see that the solutions computed by the FTL algorithm will be precisely <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_1%2Cx_2%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,x_1,x_2,\ldots,x_T}" class="latex" title="{x_1,x_1,x_2,\ldots,x_T}" />. The regret bound for FTL implies that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />,</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x_1%29+-+R%28x%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_t%28x%29+%5Cleq+R%28x_1%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " class="latex" title="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Having established these results, the general recipe to solve an online optimization problem will be to find a regularizer function <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> such that the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> “pulls away from” solutions that would make the FTL algorithm overfit, and such that there is a good balance between how big <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> gets over <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> (because we pay <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%5E%2A%29+-+R%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x^*) - R(x_1)}" class="latex" title="{R(x^*) - R(x_1)}" /> in the regret, where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum) and how stable is the minimum of <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" class="latex" title="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> varies.</p>
<p>
</p><p><b>2. Negative-Entropy Regularization </b></p>
<p></p><p>
Let us consider again the “experts” setting, that is, the online optimization setup in which <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set of probability distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2C+n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots, n\}}" class="latex" title="{\{ 1,\ldots, n\}}" /> and the cost functions are linear <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+%5Cell_t+%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i \ell_t (i) x(i)}" class="latex" title="{f_t (x) = \sum_i \ell_t (i) x(i)}" /> with bounded coefficients.</p>
<p>
The example we showed above showed that FTL will tend to put all the probability mass on one expert. We would like to choose a regularizer that fights this tendency by penalizing “concentrated” distributions and favoring “spread-out” distributions. This observation might trigger the thought that the <em>entropy</em> of a distribution is a good measure of how concentrated or spread out it is, although the entropy is actually higher for spread-out distribution and smaller for concentrated ones. So we will use as a regularizer <em>minus the entropy</em>, multiplied by an appropriate scaling factor: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> (Entropy is usually defined using logarithms in base 2, but using natural logarithms will make it cleaner to take derivatives, and it only affects the constant factor <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" />.) With this choice of regularizer, we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%5CDelta%7D+%5C+%5C+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " /></p>
<p> To compute the minimum of the above function we will use the method of Lagrange multipliers. Specialized to our setting, the method of Lagrange multiplier states that if we want to solve the constrained minimization problem </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx+%3A+%5C+a%5ETx+%3D+b+%7D+%5C+%5C+f%28x+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " class="latex" title="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " /></p>
<p> we introduce a new parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> and define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%5Clambda+%28x%29+%3A%3D+f%28x%29+%2B+%5Clambda+%5Ccdot+%28a%5ET+x+-+b+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " class="latex" title="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " /></p>
<p> Then it is possible to prove that if <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a feasible minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />, then there is at least a value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%5E%2A%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x^*) = 0}" class="latex" title="{\nabla f_\lambda (x^*) = 0}" />, that is, such that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is a stable point of <img src="https://s0.wp.com/latex.php?latex=%7Bf_%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_\lambda}" class="latex" title="{f_\lambda}" />. So one can proceed by finding all <img src="https://s0.wp.com/latex.php?latex=%7Bx%2C%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,\lambda}" class="latex" title="{x,\lambda}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla f_\lambda (x) = 0}" class="latex" title="{\nabla f_\lambda (x) = 0}" /> and then filtering out the values of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5ET+x+%5Cneq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a^T x \neq b}" class="latex" title="{a^T x \neq b}" />, and finally looking at which of the remaining <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> minimizes <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\cdot)}" class="latex" title="{f(\cdot)}" />.</p>
<p>
Ignoring for a moment the non-negativity constraints, the constraint <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in \Delta}" class="latex" title="{x\in \Delta}" /> reduces to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_i x_i = 1}" class="latex" title="{\sum_i x_i = 1}" />, so we have to consider the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+%5Ccdot+%5Cleft%28+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle+-+1%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " /></p>
<p> The partial derivative of the above expression with respect to <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i}" class="latex" title="{x_i}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+1+%2B+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " class="latex" title="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " /></p>
<p> If we want the gradient to be zero then we want all the above expressions to be zero, which translates to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%7B%5Crm+exp%7D+%5Cleft%28+-1+-+%5Cfrac+%5Clambda+c+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " class="latex" title="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " /></p>
<p> There is only one value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda}" class="latex" title="{\lambda}" /> that makes the above solution a probability distribution, and the corresponding solution is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%5Cfrac+%7B%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+%7D+%7B%5Csum_j+%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28j%29+%5Cright%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " class="latex" title="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " /></p>
<p> Notice that this is exactly the solution computed by the multiplicative weights algorithm, if we choose <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = 1/\epsilon}" class="latex" title="{c = 1/\epsilon}" />. So we have “rediscovered” the multiplicative weights algorithm and we have also “explained” what it does: at every step it balances the goals of finding a solution that is good for the past and that has large entropy.</p>
<p>
Now it remains to bound, at each time step, </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x_t%29+-+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " class="latex" title="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " /></p>
<p> For this, it is convenient to return to the notation that we used in describing the multiplicative weights algorithm, that is, it is convenient to work with the weights defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_1%28i%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_1(i) = 1" class="latex" title="\displaystyle  w_1(i) = 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_%7Bt%2B1%7D+%28i%29+%3D+w_t+%28i%29+%5Ccdot+e%5E%7B+%5Cell_t+%28i%29+%2F+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" class="latex" title="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" /></p>
<p> so that, at each time step </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7Bw_t%28i%29%7D%7B%5Csum_j+w_t+%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " class="latex" title="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " /></p>
<p> We are assuming <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cell_t+%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 \leq \ell_t (i) \leq 1}" class="latex" title="{0 \leq \ell_t (i) \leq 1}" />, so the weights are non-increasing with time. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%28i%29+%3D+%5Cfrac+%7Bw_%7Bt%2B1%7D+%28i%29+%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%3D+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%5Cgeq+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%7D+%28j%29+%7D+%5Cgeq+x_t%28i%29+%5Ccdot+e%5E%7B-1%2Fc%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " class="latex" title="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " /></p>
<p> For every <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c \geq 1}" class="latex" title="{c \geq 1}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-1%2Fc%7D+%5Cgeq+1+-+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-1/c} \geq 1 - 1/c}" class="latex" title="{e^{-1/c} \geq 1 - 1/c}" />, so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%7D%28i%29+-+x_%7Bt%2B1%7D%28i%29+%5Cleq+%5Cfrac+1c+x_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " class="latex" title="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%5Csum_i+%5Cell_t%28i%29+%5Ccdot+%5Cfrac+1c+x_t%28i%29+%5Cleq+%5Cfrac+1c+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " class="latex" title="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " /></p>
<p> Putting it all together, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cfrac+Tc+%2B+c+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " class="latex" title="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " /></p>
<p> Choosing <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B%5Cln+n%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {\ln n}}}" class="latex" title="{c = \sqrt{\frac T {\ln n}}}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
<p> Thus, we have reconstructed the analysis of the multiplicative weights algorithm.</p>
<p>
Interestingly, the analysis that we derived today is not exactly identical to the one from the post on multiplicative weights. There, we derived the bound</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t%5E2+%28i%29+x_t+%28i%29+%5C+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " /></p>
<p> while here, setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = 1/c}" class="latex" title="{\epsilon = 1/c}" />, we derived </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t+%28i%29+x_t%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+-+%5Cfrac+1+%5Cepsilon+H%28x%5E%2A%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^*}" class="latex" title="{x^*}" /> is the offline optimum and <img src="https://s0.wp.com/latex.php?latex=%7BH%28x%29+%3D+%5Csum_i+x_i+%5Cln+%5Cfrac+1+%7Bx_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" class="latex" title="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" /> is the entropy function (computed using natural logarithms). </p>
<p>
</p><p><b>3. L2 Regularization </b></p>
<p></p><p>
Now that we have a general method, let us apply it to a new context: suppose that, as before, our cost functions are linear, but let <img src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K = {\mathbb R}^n}" class="latex" title="{K = {\mathbb R}^n}" />. With linear cost functions and no bound on the size of solutions, it will not be possible to talk about regret with respect to the offline optimum, because the offline optimum will always be <img src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-\infty}" class="latex" title="{-\infty}" />, but it will be possible to talk about regret with respect to a particular offline solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, which will already lead to interesting consequences.</p>
<p>
What regularizer should we use? In reasoning about regularizers, it can be helpful to think about what would go wrong if we use FTL, and then considering what regularizer would successfully “pull away” from the bad solutions found by FTL. In this context of linear loss functions and unbounded solutions, FTL will pick an infinitely big solution at each step, or, to be more precise, the “max” in the definition of FTL is undefined. To fight this tendency of FTL to go off to infinity, it makes sense for the regularizer to be a measure of how big a solution is. Since we are going to have to compute derivatives, it is good to use a measure of “bigness” with a nice gradient, and <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{||x ||^2}" class="latex" title="{||x ||^2}" /> is a natural choice. So, for a scale parameter <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> to be optimized later, our regularizer will be </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%7C%7C+x%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) := c || x||^2 " class="latex" title="\displaystyle  R(x) := c || x||^2 " /></p>
<p> This tells us that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_+1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_ 1 = {\bf 0} " class="latex" title="\displaystyle  x_ 1 = {\bf 0} " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cx%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Clangle+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " /></p>
<p> The function that we are minimizing in the above expression is convex, so we just have to compute the gradient and set it to zero </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2c+x+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " class="latex" title="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+-+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " class="latex" title="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " /></p>
<p> Which can be also expressed as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%7B%5Cbf+0%7D%3B+%5C+%5C+%5C+x_%7Bt%2B1%7D+%3D+x_t+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " class="latex" title="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " /></p>
<p> This makes perfect sense because, in the “experts” interpretation, we want to penalize the experts that performed badly in the past. Here we have no constraints on our allocations, so we simply decrease (additively this time, not multiplicatively) the allocation to the experts that caused a higher loss.</p>
<p>
To compute the regret bound, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%3D+%5Cleft%5Clangle+%5Cell_t+%2C+%5Cfrac+1+%7B2c%7D+%5Cell_t+%5Cright%5Crangle+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t%7C%7C%5E2+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " class="latex" title="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " /></p>
<p> and so the regret with respect to a solution <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+R%28x%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%7C%7C+x%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " class="latex" title="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " /></p>
<p> If we know a bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+t%3A+%5C+%5C+%7C%7C+%5Cell_t+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall t: \ \ || \ell_t || \leq L " class="latex" title="\displaystyle  \forall t: \ \ || \ell_t || \leq L " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x|| \leq D " class="latex" title="\displaystyle  || x|| \leq D " /></p>
<p> then we can optimize <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B2D%5E2+L%5E2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \sqrt{\frac T {2D^2 L^2}}}" class="latex" title="{c = \sqrt{\frac T {2D^2 L^2}}}" /> and we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D+L+%5Csqrt%7B+2+T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " /></p>
<p>
</p><p><b>  3.1. Dealing with Constraints </b></p>
<p></p><p>
Consider now the case in which the loss functions are linear and <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is an arbitrary convex set. Using the same regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) = c || x||^2}" class="latex" title="{R(x) = c || x||^2}" /> we have the algorithm </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " class="latex" title="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+c+%7C%7Cx+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " /></p>
<p> How can we solve the above constrained optimization problem? A very helpful observation is that we can first solve the unconstrained optimization and then project on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, that is we can proceed as follows: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cy+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " class="latex" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5CPi_K+%28y_%7Bt%2B1%7D+%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p> and we claim that we always have <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D+%3D+x_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1} = x_{t+1}}" class="latex" title="{x'_{t+1} = x_{t+1}}" />. The fact that we can reduce a regularized constrained optimization problem to an unconstrained problem and a projection is part of a broader theory that we will describe in a later post. For now, we will limit to prove the equivalence in this specific setting. First of all, we already have an expression for <img src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_{t+1}}" class="latex" title="{y_{t+1}}" />, namely </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " class="latex" title="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " /></p>
<p> Now the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'_{t+1}}" class="latex" title="{x'_{t+1}}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" title="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+%2B+%7C%7C+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft%5Clangle+x+%2C+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " class="latex" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx%7C%7C%5E2+-+%5Csum_%7Bk%3D1%7D%5Et+%5Cleft%5Clangle+x+%2C+%5Cell_t+%5Cright%5Crangle+%3D+x_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " class="latex" title="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " /></p>
<p>
In order to bound the regret, we have to compute </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_t%28x_%7Bt%2B1%7D+%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%7C%7C+%5Cell_t+%7C%7C+%5Ccdot+%7C%7Cx_t+-+x_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " class="latex" title="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " /></p>
<p> and since L2 projections cannot increase L2 distances, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_t+-+x_%7Bt%2B1%7D+%7C%7C+%5Cleq+%7C%7C+y_t+-+y_%7Bt%2B1%7D+%7C%7C+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " class="latex" title="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " /></p>
<p>
So the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+%7C%7Cx%5E%2A%7C%7C%5E2+-+c%7C%7C+x_1%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is an upper bound to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{x\in K} || x||}" class="latex" title="{\max_{x\in K} || x||}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> is an upper bound to the norm <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+%5Cell_t+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| \ell_t ||}" class="latex" title="{|| \ell_t ||}" /> of all the loss vectors, then</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+D%5E2+%2B+%5Cfrac+1+%7B2c%7D+T+L%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " class="latex" title="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " /></p>
<p> which can be optimized to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+DL+%5Csqrt+%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " class="latex" title="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " /></p>
<p>
</p><p><b>  3.2. Deriving the Analysis of Gradient Descent </b></p>
<p></p><p>
Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: K \rightarrow {\mathbb R}}" class="latex" title="{g: K \rightarrow {\mathbb R}}" /> is a convex function whose gradient <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla g}" class="latex" title="{\nabla g}" /> is well defined at all points in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, and that we are interested in minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />. Then a way to reduce this problem to online optimization would be to use the function <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as loss function at each step. Then the offline optimum would be the minimizer of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and achieving small regret means that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1T+%5Csum_t+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1T \sum_t g(x_t)}" class="latex" title="{\frac 1T \sum_t g(x_t)}" /> is close to the minimum of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, and so the best <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer.</p>
<p>
Unfortunately, this is not a very helpful idea, because if we ran an FTRL algorithm against an adversary that keeps proposing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost function at each step then we would have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29+%2B+t+%5Ccdot+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " /></p>
<p> which, for large <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, is essentially the same problem as minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, so we have basically reduced the problem of minimizing <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to itself.</p>
<p>
Indeed, the power of the FTRL algorithm is that the algorithm does well even though it does not know the cost function, and if we keep using the same cost function at each step we are not making a good use of its power. Now, suppose that we use cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> such that </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+x%5Cin+K+%5C+%5C+f_t%28x%29+%5Cleq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\forall x\in K \ \ f_t(x) \leq g(x)}" class="latex" title="{\forall x\in K \ \ f_t(x) \leq g(x)}" />
</li></ul>
<p> Then, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%3D+%7B%5Crm+Regret%7D_T+%2B+%5Cmin%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cleq+%7B%5Crm+Regret%7D_T+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> meaning </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so one of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is an approximate minimizer. Indeed, using convexity, we also have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright%29+%5Cleq+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" title="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " /></p>
<p> and so the average of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is also an approximate minimizer. From the point of view of exploiting FTRL do to minimize <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> as above work just as well as presenting <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> as a cost functions at each step.</p>
<p>
How do we find cost functions that satisfy the above two properties and for which the FTRL algorithm is easy to implement? The idea is to let <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t}" class="latex" title="{f_t}" /> be the linear approximation of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+g%28x_t%29+%2B+%5Clangle+%5Cnabla+g+%28x_t%29%2C+x+-+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " class="latex" title="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " /></p>
<p> The <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) = g(x_t)}" class="latex" title="{f_t(x_t) = g(x_t)}" /> condition is immediate, and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%5Cgeq+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x) \geq f_t (x) " class="latex" title="\displaystyle  g(x) \geq f_t (x) " /></p>
<p> is a consequence of the convexity of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />.</p>
<p>
The cost functions that we have defined are affine functions, that is, each of them equals a constant plus a linear function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x%29+%3D+%5Cleft%28+g%28x_t%29+-+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x_t%5Crangle+%5Cright%29+%2B+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " class="latex" title="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " /></p>
<p>
Adding a constant term to a cost function does not change the iteration of FTRL, and does not change the regret (because the same term is added both to the solution found by the algorithm and to the offline optimum), so the algorithm is just initialized with</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_1 = {\bf 0} " class="latex" title="\displaystyle  y_1 = {\bf 0} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5CPi_K%28%7B%5Cbf+0%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " class="latex" title="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " /></p>
<p> and then continues with the update rules </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3Dy_t+-%5Cfrac+1+%7B2c%7D+%5Cnabla+g+%28x_t%29+%5Cmbox%7B+for+%7D+t+%5Cgeq+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" class="latex" title="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5CPi_K%28y_%7Bt%2B1%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+%5Cmbox%7B+for+%7D+t+%5Cgeq+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " class="latex" title="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " /></p>
<p> which is just projected gradient descent.</p>
<p>
If we have known upper bounds </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+%5Cnabla+g%28x%29+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " class="latex" title="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+x+%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x \in K \ \ || x || \leq D " class="latex" title="\displaystyle  \forall x \in K \ \ || x || \leq D " /></p>
<p> then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1+T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright+%29+%5Cleq+DL+%5Ccdot+%5Csqrt%7B%5Cfrac+2+T%7D+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" title="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " /></p>
<p> which means that to achieve additive error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> it is enough to proceed for <img src="https://s0.wp.com/latex.php?latex=%7B2D%5E2L%5E2+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2D^2L^2 / \epsilon^2}" class="latex" title="{2D^2L^2 / \epsilon^2}" /> steps. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"><span class="datestr">at May 06, 2019 02:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/">E-ink on the move</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;">Today I was overjoyed to notice that the <a href="https://www.mbta.com/projects/solar-powered-e-ink-signs">MBTA is installing e-ink signs</a>. I didn’t know about this when <a href="https://emanueleviola.wordpress.com/2019/03/07/a-dream-come-true-sort-of-e-ink-monitors/">I wrote in the previous post</a> that the market for e-ink monitors will be huge.</p>
<p style="text-align: justify;">I was actually about to report more on my experience, and by another standard coincidence today a reader asks:</p>
<blockquote><p>Some time have passed, is your evaluation the same? Did you come across any unexpected difficulties?</p></blockquote>
<p style="text-align: justify;">Well, I wrote a <a href="http://www.ccs.neu.edu/home/viola/papers/tm.pdf">paper</a> entirely in e-ink. But I regret to admit that towards the end of the semester I got really busy with the usual end-of-Spring matters at the university, and I switched back to my back-lit 30-inch Dell monitor.  I had to interact with a number of computer systems where I could not easily change font size (the story of my life), and where color tended to matter, and I felt that the new monitor was slowing me down.  I haven’t switched back to the e-ink monitor yet, partly because I am still recovering from the burst.</p>
<p style="text-align: justify;">However I look forward to using the e-ink monitor more during this summer, especially outdoors.  Here the fact that it’s usb powered will be essential.  In the MBTA project they use solar power which I think is really cool and makes me think of bringing my monitor to the secluded off-the-grid cabin in Maine I don’t have.</p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/"><span class="datestr">at May 06, 2019 12:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
