<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 14, 2021 07:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/037">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/037">TR21-037 |  Separating ABPs and Some Structured Formulas in the Non-Commutative Setting | 

	Prerona Chatterjee</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The motivating question for this work is a long standing open problem, posed by Nisan (1991), regarding the relative powers of algebraic branching programs (ABPs) and formulas in the non-commutative setting. Even though the general question continues to remain open, we make some progress towards its resolution. To that effect, we generalise the notion of ordered polynomials in the non-commutative setting (defined by Hrubes, Wigderson and Yehudayoff (2011)) to define abecedarian polynomials and models that naturally compute them.
		
Our main contribution is a possible new approach towards separating formulas and ABPs in the non-commutative setting, via lower bounds against abecedarian formulas. In particular, we show the following.
	
There is an explicit $n$-variate degree $d$ abecedarian polynomial $f_{n,d}(x)$ such that 
1. $f_{n, d}(x)$ can be computed by an abecedarian ABP of size O(nd);
2. any abecedarian formula computing $f_{n, \log n}(x)$ must have size that is super-polynomial in $n$.

We also show that a  super-polynomial lower bound against abecedarian formulas for $f_{\log n, n}(x)$ would separate the powers of formulas and ABPs in the non-commutative setting.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/037"><span class="datestr">at March 14, 2021 05:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/036">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/036">TR21-036 |  Ideal-theoretic Explanation of Capacity-achieving Decoding | 

	Mrinal Kumar, 

	Madhu Sudan, 

	Siddharth Bhandari, 

	Prahladh Harsha</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, we present an abstract framework for some algebraic error-correcting codes with the aim of capturing codes that are list-decodable to capacity, along with their decoding algorithm. In the polynomial ideal framework, a code is specified by some ideals in a polynomial ring, messages are polynomials and their encoding is the residue modulo the ideals. We present an alternate way of viewing this class of codes in terms of linear operators, and show that this alternate view makes their algorithmic list-decodability amenable to analysis.

Our framework leads to a new class of codes that we call {\em affine Folded Reed-Solomon codes} (which are themselves a special case of the broader class we explore). These codes are common generalizations of the well-studied Folded Reed-Solomon codes and Multiplicity codes, while also capturing the less-studied Additive Folded Reed-Solomon codes as well as a large family of codes that were not previously known/studied. 

More significantly our framework also captures the algorithmic list-decodability of the constituent codes. Specifically, we present a unified view of the decoding algorithm for ideal-theoretic codes and show that the decodability reduces to the analysis of the distance of some related codes. We show that good bounds on this distance lead to capacity-achieving performance of the underlying code, providing a unifying explanation of known capacity-achieving results. In the specific case of affine Folded Reed-Solomon codes, our framework shows that they are list-decodable up to capacity (for appropriate setting of the parameters), thereby unifying the previous results for Folded Reed-Solomon, Multiplicity and Additive Folded Reed-Solomon codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/036"><span class="datestr">at March 14, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06707">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06707">Hardness of Token Swapping on Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aichholzer:Oswin.html">Oswin Aichholzer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Matias.html">Matias Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lubiw:Anna.html">Anna Lubiw</a>, Zuzana Mas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rudoy:Mikhail.html">Mikhail Rudoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06707">PDF</a><br /><b>Abstract: </b>Given a graph where every vertex has exactly one labeled token, how can we
most quickly execute a given permutation on the tokens? In (sequential) token
swapping, the goal is to use the shortest possible sequence of swaps, each of
which exchanges the tokens at the two endpoints of an edge of the graph. In
parallel token swapping, the goal is to use the fewest rounds, each of which
consists of one or more swaps on the edges of a matching. We prove that both of
these problems remain NP-hard when the graph is restricted to be a tree.
</p>
<p>These token swapping problems have been studied by disparate groups of
researchers in discrete mathematics, theoretical computer science, robot motion
planning, game theory, and engineering. Previous work establishes
NP-completeness on general graphs (for both problems); polynomial-time
algorithms for simple graph classes such as cliques, stars, paths, and cycles;
and constant-factor approximation algorithms in some cases. The two natural
cases of sequential and parallel token swapping in trees were first studied
over thirty years ago (as "sorting with a transposition tree") and over
twenty-five years ago (as "routing permutations via matchings"), yet their
complexities were previously unknown.
</p>
<p>We also show limitations on approximation of sequential token swapping on
trees: we identify a broad class of algorithms that encompass all three known
polynomial-time algorithms that achieve the best known approximation factor
(which is $2$) and show that no such algorithm can achieve an approximation
factor less than $2$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06707"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06665">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06665">Arc-Completion of 2-Colored Best Match Graphs to Binary-Explainable Best Match Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schaller:David.html">David Schaller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gei=szlig=:Manuela.html">Manuela Geiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06665">PDF</a><br /><b>Abstract: </b>Best match graphs (BMGs) are vertex-colored digraphs that naturally arise in
mathematical phylogenetics to formalize the notion of evolutionary closest
genes w.r.t. an a priori unknown phylogenetic tree. BMGs are explained by
unique least resolved trees. We prove that the property of a rooted,
leaf-colored tree to be least resolved for some BMG is preserved by the
contraction of inner edges. For the special case of two-colored BMGs, this
leads to a characterization of the least resolved trees (LRTs) of
binary-explainable trees and a simple, polynomial-time algorithm for the
minimum cardinality completion of the arc set of a BMG to reach a BMG that can
be explained by a binary tree.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06665"><span class="datestr">at March 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06614">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06614">Hitting minors on bounded treewidth graphs. III. Lower bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baste:Julien.html">Julien Baste</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thilikos:Dimitrios_M=.html">Dimitrios M. Thilikos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06614">PDF</a><br /><b>Abstract: </b>For a finite collection of graphs ${\cal F}$, the ${\cal F}$-M-DELETION
problem consists in, given a graph $G$ and an integer $k$, decide whether there
exists $S \subseteq V(G)$ with $|S| \leq k$ such that $G \setminus S$ does not
contain any of the graphs in ${\cal F}$ as a minor. We are interested in the
parameterized complexity of ${\cal F}$-M-DELETION when the parameter is the
treewidth of $G$, denoted by $tw$. Our objective is to determine, for a fixed
${\cal F}$, the smallest function $f_{{\cal F}}$ such that ${\cal
F}$-M-DELETION can be solved in time $f_{{\cal F}}(tw) \cdot n^{O(1)}$ on
$n$-vertex graphs. We provide lower bounds under the ETH on $f_{{\cal F}}$ for
several collections ${\cal F}$. We first prove that for any ${\cal F}$
containing connected graphs of size at least two, $f_{{\cal F}}(tw)=
2^{\Omega(tw)}$, even if the input graph $G$ is planar. Our main contribution
consists of superexponential lower bounds for a number of collections ${\cal
F}$, inspired by a reduction of Bonnet et al.~[IPEC, 2017]. In particular, we
prove that when ${\cal F}$ contains a single connected graph $H$ that is either
$P_5$ or is not a minor of the banner (that is, the graph consisting of a $C_4$
plus a pendent edge), then $f_{{\cal F}}(tw)= 2^{\Omega(tw \cdot \log tw)}$.
This is the third of a series of articles on this topic, and the results given
here together with other ones allow us, in particular, to provide a tight
dichotomy on the complexity of $\{H\}$-M-DELETION, in terms of $H$, when $H$ is
connected.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06614"><span class="datestr">at March 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06536">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06536">Hitting minors on bounded treewidth graphs. II. Single-exponential algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baste:Julien.html">Julien Baste</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thilikos:Dimitrios_M=.html">Dimitrios M. Thilikos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06536">PDF</a><br /><b>Abstract: </b>For a finite collection of graphs ${\cal F}$, the ${\cal F}$-M-DELETION
(resp. ${\cal F}$-TM-DELETION) problem consists in, given a graph $G$ and an
integer $k$, decide whether there exists $S \subseteq V(G)$ with $|S| \leq k$
such that $G \setminus S$ does not contain any of the graphs in ${\cal F}$ as a
minor (resp. topological minor). We are interested in the parameterized
complexity of both problems when the parameter is the treewidth of $G$, denoted
by $tw$, and specifically in the cases where ${\cal F}$ contains a single
connected planar graph $H$. We present algorithms running in time $2^{O(tw)}
\cdot n^{O(1)}$, called single-exponential, when $H$ is either $P_3$, $P_4$,
$C_4$, the paw, the chair, and the banner for both $\{H\}$-M-DELETION and
$\{H\}$-TM-DELETION, and when $H=K_{1,i}$, with $i \geq 1$, for
$\{H\}$-TM-DELETION. Some of these algorithms use the rank-based approach
introduced by Bodlaender et al. [Inform Comput, 2015]. This is the second of a
series of articles on this topic, and the results given here together with
other ones allow us, in particular, to provide a tight dichotomy on the
complexity of $\{H\}$-M-DELETION in terms of $H$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06536"><span class="datestr">at March 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5683">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/03/13/the-2021-polymath-jr-program/">The 2021 Polymath Jr Program</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Summer Opportunity: Polymath Jr Research Experience for Undergraduates. The goal of this remote program is to provide opportunities to undergraduates who wish to explore research mathematics. The program consists of research projects on a wide variety of mathematical topics. Each project is guided by an active researcher with experience in undergraduate mentoring. All undergraduates who […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/03/13/the-2021-polymath-jr-program/"><span class="datestr">at March 13, 2021 06:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/13/hilberts-tenth-again/">Hilbert’s Tenth Again</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>I think it’s never too late to start anything, except maybe being a ballerina. — Wendy Liebman</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/03/13/hilberts-tenth-again/ms/" rel="attachment wp-att-18308"><img width="180" alt="" src="https://rjlipton.files.wordpress.com/2021/03/ms.png?w=180&amp;h=160" class="alignright wp-image-18308" height="160" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Mujeres Con Ciencia <a href="https://twitter.com/mujerconciencia/status/1257679058084728833">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Marta Sved was a Hungarian mathematician who in 1930 moved to Down Under and became a teacher of math at the University of Adelaide. Over fifty years later, in 1985, Sved got a PhD. Clearly Liebman’s quote applies to Sved’s pursuit of a PhD. </p>
<p>
Today I cannot resist talking about a particular <a href="https://web.archive.org/web/20160304191325/http://www.maa.org/sites/default/files/Sved50816668.pdf">paper</a> of hers in the Mathematics Magazine—Volume 63 Number 1, February 1990: </p>
<p><a href="https://rjlipton.wordpress.com/2021/03/13/hilberts-tenth-again/mm/" rel="attachment wp-att-18312"><img width="200" alt="" class="aligncenter  wp-image-18312" src="https://rjlipton.files.wordpress.com/2021/03/mm.png?w=200" /></a></p>
<p>
Mathematics Magazine is a reviewed journal that focuses on exposition, history, and connections to other parts of math. I am a longtime reader of this journal. I have been getting this and other similar math mag’s for years. I am sure I saw this paper and the theorem. I did not see any reason to be overly excited by the result. However, the theorem plays a critical role in solving a long time open problem.</p>
<p>
Note, this paper did not make the cover, but I believe it should have. To see why we must look at the famous Hilbert Tenth Problem. </p>
<p></p><h2> Hilbert’s Tenth </h2><p></p>
<p>
Recall this is the problem: Given a polynomial equation 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%28x_1%2Cx_2%2Cx_3%2C%5Cdots%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P(x_1,x_2,x_3,\dots) = 0 " class="latex" /></p>
<p>over the integers, decide if there is a solution over the integers. This was famously proved to be <a href="https://en.wikipedia.org/wiki/Hilbert%27s_tenth_problem">undecidable</a> by the combined work of Martin Davis, Hilary Putnam, and Julia Robinson, and was completed by Yuri Matiyasevich in 1970. </p>
<p>
Davis, Putnam, and Robinson proved this first for exponential equations—they allowed <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y}" class="latex" />. Later, Matiyasevich showed how to express exponentiation in by polynomials—this completed the proof. </p>
<p>
Solving a famous open problem only increases our interest in related questions. One class of questions involved replacing the integers by other rings. One of the major questions is what happens if we ask for solutions to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%28x_1%2Cx_2%2Cx_3%2C%5Cdots%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P(x_1,x_2,x_3,\dots) = 0 " class="latex" /></p>
<p>over the rationals? This is currently open—see our <a href="https://rjlipton.wordpress.com/2010/08/07/hilberts-tenth-over-the-rationals/">posted</a> on this and two <a href="https://rjlipton.wordpress.com/2011/07/19/hilberts-10-5th-problem/">related</a> <a href="https://rjlipton.wordpress.com/2019/06/19/diophantine-equations/">ones</a>, plus a topical <a href="https://math.mit.edu/~poonen/papers/subrings.pdf">paper</a> by Bjorn Poonen, for some comments. </p>
<p></p><h2> Hilbert Tenth Over Rationals </h2><p></p>
<p>
Following the history of the classic version of Hilbert’s Tenth, Mihai Prunescu proved: </p>
<blockquote><p><b>Theorem 1</b> <em> It is undecidable to determine whether an exponential equation in many variables is solvable over the rationals. </em>
</p></blockquote>
<p>
This just appeared as <a href="https://www.cambridge.org/core/journals/journal-of-symbolic-logic/issue/E4FD92AFC9B9DE3E9855EEAFAF14A924"> The Exponential Diophantine Problem For Rationals</a> in the current issue of the JSL: </p>
<p>
<a href="https://rjlipton.wordpress.com/2021/03/13/hilberts-tenth-again/jsl/" rel="attachment wp-att-18309"><img width="200" alt="" class="aligncenter  wp-image-18309" src="https://rjlipton.files.wordpress.com/2021/03/jsl.jpg?w=200" /></a></p>
<p>
It is a short paper—two pages. The key is it shows that it is possible to define the integers by an exponential equation over the rationals. Is there a Matiyasevich out there who will remove the need for exponentials? </p>
<p></p><h2> Using Exponentials over the Rationals </h2><p></p>
<p>
Prunescu uses the following equation: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5Ey+%3D+y%5Ex.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x^y = y^x. " class="latex" /></p>
<p>The solutions of this equation form a single parameterized family, and allow one to achieve the needed task:</p>
<blockquote><p><b> </b> <em> <i>Define the natural numbers from the rational solutions to this equation.</i> </em>
</p></blockquote>
<blockquote><p><b>Lemma 2</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+x+%3C+y%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 &lt; x &lt; y}" class="latex" /> are rationals so that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey+%3D+y%5Ex%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y = y^x}" class="latex" />. Then for some integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" /> 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+%281%2B1%2Fn%29%5En+%5Ctext%7B+and+%7D+y+%3D+%281%2B1%2Fn%29%5E%7Bn%2B1%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x = (1+1/n)^n \text{ and } y = (1+1/n)^{n+1}. " class="latex" /></p>
</em><p><em>Moreover, every <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" /> arises this way. </em>
</p></blockquote>
<p>
His proof outline is to suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey+%3D+y%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y = y^x}" class="latex" /> for rationals <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+x+%3C+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 &lt; x &lt; y}" class="latex" />. Then by the lemma we can recover the <i>integer</i> <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> using only addition and multiplication. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y%2Fx+%3D+1+%2B+1%2Fn%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  y/x = 1 + 1/n, " class="latex" /></p>
<p>for some <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. Thus, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n+%3D+%5Cfrac%7Bx%7D%7By-x%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n = \frac{x}{y-x}." class="latex" /></p>
<p>This is an amazing trick—in my opinion. </p>
<p>
Prunescu deserves a tip-of-the-hat for this insight. At the high level he is using this: Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x,y}" class="latex" /> rationals satisfy an equation 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Cy%29+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x,y) = 0. " class="latex" /></p>
<p>Then there is a rational operation on <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" /> that recovers a unique integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" />. Moreover, for any <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" /> there are rational solutions <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x,y}" class="latex" /> that yield <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. This allows him to prove that exponential equations over the rationals is undecidable. </p>
<p></p><h2> The Lemma </h2><p></p>
<p>
Prunescu needs the above lemma. He does not prove it, but says: </p>
<blockquote><p><b> </b> <em> The equation <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey+%3D+y%5Ex%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y = y^x}" class="latex" /> for rationals was studied by Christian Goldbach and Leonhard Euler—as stated in Leonhard Dickson’s <a href="https://www.amazon.com/History-Theory-Numbers-Vol-Diophantine/dp/0821819356">book</a>. </em>
</p></blockquote>
<p>
The results had been available for a long time—the book was published over a hundred years ago in 1920.</p>
<p>
It is interesting to ask why it took so long to realize the connection? I looked at Dickson’s book and found the reference not so clean. I then did a Google search for results on the equation: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5Ey+%3D+y%5Ex.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x^y = y^x. " class="latex" /></p>
<p>The search found that Sved proved in the <a href="https://web.archive.org/web/20160304191325/http://www.maa.org/sites/default/files/Sved50816668.pdf">paper</a> in the Mathematics Magazine—Volume 63 Number 1, February 1990 the following: </p>
<blockquote><p><b>Lemma 3</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+x+%3C+y%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 &lt; x &lt; y}" class="latex" /> are rationals so that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey+%3D+y%5Ex%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y = y^x}" class="latex" />. Then for some integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" /> 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+%281%2B1%2Fn%29%5En+%5Ctext%7B+and+%7D+y+%3D+%281%2B1%2Fn%29%5E%7Bn%2B1%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x = (1+1/n)^n \text{ and } y = (1+1/n)^{n+1}. " class="latex" /></p>
</em><p><em>Moreover, every <img src="https://s0.wp.com/latex.php?latex=%7Bn%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n&gt;0}" class="latex" /> arises this way. </em>
</p></blockquote>
<p>
That is, Sved proved the key lemma in full detail in 1990. Thus over thirty years ago, in an available journal, the exact lemma needed was available. </p>
<blockquote><p><b> </b> <em> Why did so all of us miss that Sved’s analysis of the equation <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%3Dy%5Ex%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^y=y^x}" class="latex" /> could be used to solve an open problem. One that was open for decades? </em>
</p></blockquote>
<p>
Perhaps it says that Prunescu’s insight was indeed surprising. Perhaps. It does raise questions about improving his theorem. For example: Can a similar theorem be proved with only exponentials of the form 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Ex%2C+3%5Ex%2C+%5Cdots%3F+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^x, 3^x, \dots? " class="latex" /></p>
<p>This would improve the undecidability theorem a bit.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
Perhaps we also should look at results from not top “professional” journals. Hmmm I think I will start looking at them with a new eye. How about you?</p>
<p>
Perhaps we also should look at results from not top “professional” journals. Hmmm I think I will start looking at them with a new eye. How about you?</p>
<p>
[some layout improvements]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/03/13/hilberts-tenth-again/"><span class="datestr">at March 13, 2021 01:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/035">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/035">TR21-035 |  Amortized Circuit Complexity, Formal Complexity Measures, and Catalytic Algorithms | 

	Robert Robere, 

	Jeroen Zuiddam</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the amortized circuit complexity of boolean functions.
	
Given a circuit model $\mathcal{F}$ and a boolean function $f : \{0,1\}^n \rightarrow \{0,1\}$, the $\mathcal{F}$-amortized circuit complexity is defined to be the size of the smallest circuit that outputs $m$ copies of $f$ (evaluated on the same input), divided by $m$, as $m \rightarrow \infty$. We prove a general duality theorem that characterizes the amortized circuit complexity in terms of "formal complexity measures". More precisely, we prove that the amortized circuit complexity in any circuit model composed out of gates from a finite set is equal to the pointwise maximum of the family of "formal complexity measures" associated with $\mathcal{F}$. Our duality theorem captures many of the formal complexity measures that have been previously studied in the literature for proving lower bounds (such as formula complexity measures, submodular complexity measures, and branching program complexity measures), and thus gives a characterization of formal complexity measures in terms of circuit complexity. We also introduce and investigate a related notion of catalytic circuit complexity, which we show is "intermediate" between amortized circuit complexity and standard circuit complexity, and which we also characterize (now, as the best integer solution to a linear program). 

Finally, using our new duality theorem as a guide, we strengthen the known upper bounds for non-uniform catalytic space, introduced by Buhrman et. al (this is related to, but not the same as, as our notion of catalytic circuit size). Potechin proved that for any boolean function $f:\{0,1\}^n \rightarrow \{0,1\}$, there is a catalytic branching program computing $m = 2^{2^n}-1$ copies of $f$ with total size $O(mn)$ --- that is, linear size per copy --- refuting a conjecture of Girard, Koucky and McKenzie. Potechin then asked if the number of copies $m$ can be reduced while retaining the amortized upper bound. We show that the answer is yes: if $f$ has degree $d$ when represented as polynomial over $\mathbb{F}_2$, then there is a catalytic branching program computing $m = 2^{n \choose \leq d}$ copies of $f$ with total size $O(mn)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/035"><span class="datestr">at March 13, 2021 02:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/03/13/quarterly-theory-workshop-algorithms-and-their-social-impact/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/03/13/quarterly-theory-workshop-algorithms-and-their-social-impact/">Quarterly Theory Workshop: Algorithms and their Social Impact</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
March 19, 2021 Virtual (on Gather.town and Zoom) https://theory.cs.northwestern.edu/events/quarterly-theory-workshop-algorithms-and-their-social-impact/ The focus of this workshop will be on the societal impacts of algorithms. From designing self-driving cars to selecting the order of news posts on Facebook to automating credit checks, the use of algorithms for decision making is now commonplace. Hence it is more important than … <a href="https://cstheory-events.org/2021/03/13/quarterly-theory-workshop-algorithms-and-their-social-impact/" class="more-link">Continue reading <span class="screen-reader-text">Quarterly Theory Workshop: Algorithms and their Social Impact</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/03/13/quarterly-theory-workshop-algorithms-and-their-social-impact/"><span class="datestr">at March 13, 2021 01:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06801">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06801">Upward Planar Drawings with Three Slopes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klawitter:Jonathan.html">Jonathan Klawitter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zink:Johannes.html">Johannes Zink</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06801">PDF</a><br /><b>Abstract: </b>We study upward planar straight-line drawings that use only three different
slopes. We show that deciding whether a digraph admits such a drawing is
NP-hard already for embedded outerplanar digraphs, though linear-time solvable
for trees with and without given embedding.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06801"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06696">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06696">Terrain prickliness: theoretical grounds for low complexity viewsheds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharyya:Ankush.html">Ankush Acharyya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jallu:Ramesh_K=.html">Ramesh K. Jallu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=ouml=ffler:Maarten.html">Maarten Löffler</a>, Gert G. T. Meijer, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saumell:Maria.html">Maria Saumell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silveira:Rodrigo_I=.html">Rodrigo I. Silveira</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Staals:Frank.html">Frank Staals</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tiwary:Hans_Raj.html">Hans Raj Tiwary</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06696">PDF</a><br /><b>Abstract: </b>An important task when working with terrain models is computing viewsheds:
the parts of the terrain visible from a given viewpoint. When the terrain is
modeled as a polyhedral terrain, the viewshed is composed of the union of all
the triangle parts that are visible from the viewpoint. The complexity of a
viewshed can vary significantly, from constant to quadratic in the number of
terrain vertices, depending on the terrain topography and the viewpoint
position.
</p>
<p>In this work we study a new topographic attribute, the \emph{prickliness},
that measures the number of local maxima in a terrain from all possible
perspectives. We show that the prickliness effectively captures the potential
of 2.5D terrains to have high complexity viewsheds, and we present near-optimal
algorithms to compute the prickliness of 1.5D and 2.5D terrains. We also report
on some experiments relating the prickliness of real word 2.5D terrains to the
size of the terrains and to their viewshed complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06696"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06649">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06649">Hiding solutions in model RB: Forced instances are almost as hard as unforced ones</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Guangyan.html">Guangyan Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06649">PDF</a><br /><b>Abstract: </b>In this paper we study the forced instance spaces of model RB, where one or
two arbitrary satisfying assignments have been imposed. We prove rigorously
that the expected number of solutions of forced RB instances is asymptotically
the same with those of unforced ones. Moreover, the distribution of forced RB
instances in the corresponding forced instance space is asymptotically the same
with that of unforced RB instances in the unforced instance space. These
results imply that the hidden assignments will not lead to easily solvable
formulas, and the hardness of solving forced RB instances will be the same with
unforced RB instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06649"><span class="datestr">at March 13, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06620">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06620">Topological Data Analysis of Korean Music in Jeongganbo: A Cycle Structure</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tran:Mai_Lan.html">Mai Lan Tran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Park:Changbom.html">Changbom Park</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jung:Jae=Hun.html">Jae-Hun Jung</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06620">PDF</a><br /><b>Abstract: </b>Jeongganbo is a unique music representation invented by Sejong the Great.
Contrary to the western music notation, the pitch of each note is encrypted and
the length is visualized directly in a matrix form in Jeongganbo. We use
topological data analysis (TDA) to analyze the Korean music written in
Jeongganbo for Suyeonjang, Songuyeo, and Taryong, those well-known pieces
played at the palace and among noble community. We are particularly interested
in the cycle structure. We first define and determine the node elements of each
music, characterized uniquely with its pitch and length. Then we transform the
music into a graph and define the distance between the nodes as their adjacent
occurrence rate. The graph is used as a point cloud whose homological structure
is investigated by measuring the hole structure in each dimension. We identify
cycles of each music, match those in Jeongganbo, and show how those cycles are
interconnected. The main discovery of this work is that the cycles of
Suyeonjang and Songuyeo, categorized as a special type of cyclic music known as
Dodeuri, frequently overlap each other when appearing in the music while the
cycles found in Taryong, which does not belong to Dodeuri class, appear
individually.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06620"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06597">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06597">Reducing Moser's Square Packing Problem to a Bounded Number of Squares</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neuwohner:Meike.html">Meike Neuwohner</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06597">PDF</a><br /><b>Abstract: </b>The problem widely known as Moser's Square Packing Problem asks for the
smallest area $A$ such that for any set $S$ of squares of total area $1$, there
exists a rectangle $R$ of area $A$ into which the squares in $S$ permit an
internally-disjoint, axis-parallel packing. It was formulated by Moser in 1966
and remains unsolved so far. The best known lower bound of
$\frac{2+\sqrt{3}}{3}\leq A$ is due to Novotn\'y and has been shown to be
sufficient for up to $11$ squares by Platz, while Hougardy and Ilhan have
established that $A &lt; 1.37$. In this paper, we reduce Moser's Square Packing
Problem to a problem on a finite set of squares in the following sense: We show
how to compute a natural number $N$ such that it is enough to determine the
value of $A$ for sets containing at most $N$ squares with total area $1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06597"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.06408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.06408">Geometric Approaches on Persistent Homology</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adams:Henry.html">Henry Adams</a>, Baris Coskunuzer <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06408">PDF</a><br /><b>Abstract: </b>We introduce several geometric notions, including thick-thin decompositions
and the width of a homology class, to the theory of persistent homology. These
ideas provide geometric interpretations of persistence diagrams. Indeed, we
give quantitative and geometric descriptions of the "size" or "persistence" of
a homology class. As a case study, we analyze the power filtration on
unweighted graphs, and provide explicit bounds for the life spans of homology
classes in persistence diagrams in all dimensions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.06408"><span class="datestr">at March 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9044150569954177517">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/cake-cutting-in-overtime.html">Cake Cutting in Overtime</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>There's a new <a href="https://www.foxsports.com/stories/nfl/baltimore-ravens-coach-john-harbaugh-overtime-rule">proposal out of Baltimore</a> for a new way to handle overtime in National Football League games. This post is about American Football, soccer has its own overtime issues we won't talk about here.</p><p>Instead of randomly choosing who gets the ball, which gives an advantage to the team on offense, the Raven's coach John Harbaugh suggests a "spot and choose" rule based on cake cutting. One team picks a starting position and the other team decides whether to be on offense or defense.</p><p>Sounds intriguing but there's a problem. In cake cutting, if you cut off a crumb, everyone would definitely choose the rest of the cake. But in football suppose the spotting team chose the offensive's team one-yard line (99 yards needed for a touchdown). For spot and choose to work the one-yard line would have to be an obvious choice for defense. But many teams might still choose starting at the one-year line on offense. There's a chance you can march down the field and if not you can always punt it away. So the team that gets to choose whether to be on offense could get an advantage no matter what the spotting team did.</p><p>I still like the idea of "spot and choose". Maybe you let the first team choose not only the yard line but what down to start. Because no one would start at their one yard line at 4th down.</p><p>Are there variations for spot and choose in other sports? I like using game theory to figure out how to play actual games. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/cake-cutting-in-overtime.html"><span class="datestr">at March 12, 2021 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/03/12/beyondlogconcave3/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/03/12/beyondlogconcave3/">Beyond log-concave sampling (Part 3)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">first post</a> of this series, we introduced the challenges of sampling distributions beyond log-concavity. In <a href="http://www.offconvex.org/2021/03/01/beyondlogconcave2/">Part 2</a> we tackled sampling from <em>multimodal</em> distributions: a typical obstacle occuring in problems involving statistical inference and posterior sampling in generative models. In this (final) post of the series, we consider sampling in the presence of <em>manifold structure in the level sets of the distribution</em> – which also frequently manifests in the same settings. It will cover the paper <a href="https://arxiv.org/abs/2002.05576">Fast convergence for Langevin diffusion with matrix manifold structure</a> by Ankur Moitra and Andrej Risteski .</p>

<h1 id="sampling-with-matrix-manifold-structure">Sampling with matrix manifold structure</h1>

<p>The structure on the distribution we consider in this post is <em>manifolds</em> of equiprobable points: this is natural, for instance, in the presence of invariances in data (e.g. rotations of images). It can also appear in neural-network based probabilistic models due to natural invariances they encode (e.g., scaling invariances in ReLU-based networks).</p>

<center>
<img width="400" src="http://www.andrew.cmu.edu/user/aristesk/manifold.jpg" />
</center>



<p>At the level of techniques, the starting point for our results is a close connection between the geometry, more precisely <em>Ricci curvature</em> of a manifold, and the mixing time of Brownian motion on a manifold. The following theorem holds:</p>

<blockquote>
  <p><strong>Theorem (Bakry and Émery ‘85, informal)</strong>: If the manifold $M$ has  positive Ricci curvature, Brownian motion on the manifold mixes rapidly in $\chi^2$ divergence.</p>
</blockquote>

<p>We will explain the notions from differential geometry shortly, but first we sketch our results, and how they use this machinery. We present two results: the first is a “meta”-theorem that provides a generic decomposition framework, and the second is an instantiation of this framework for a natural family of problems that exhibit manifold structure: posteriors for matrix factorization, sensing, and completion. 
</p>

<h2 id="a-general-manifold-decomposition-framework">A general manifold decomposition framework</h2>

<p>Our first result is a general decomposition framework for analyzing mixing time of Langevin in the presence of manifolds of equiprobable points.</p>

<p>To motivate the result, note that if we consider the distribution $p_{\beta}(x) \propto e^{-\beta f(x)}$, for large (but finite) $\beta$, the Langevin chain corresponding to that distribution, started close to a manifold of local minima, will tend to stay close to (but not on!) it for a long time. See the figure below for an illustration. This, we will state a “robust” version of the above manifold result, for a chain that’s allowed to go off the manifold.</p>

<center>
<img width="300" src="http://www.andrew.cmu.edu/user/aristesk/single_manifold.gif" />
</center>

<p>We show the following statement. (Recall that a bounded Poincaré constant corresponds to rapid mixing for Langevin. See the <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">first post</a> for a refresher.)</p>

<blockquote>
  <p><strong>Theorem 1 (Moitra and Risteski ‘20, informal)</strong>: 
Suppose the Langevin chain corresponding to $p(x) \propto e^{-f(x)}$ is initialized close to a manifold $M$ satisfying the following two properties: 
<br /><br />
(1) It stays in some neighborhood $D$ of the manifold $M$ with large probability for a long time. 
<br /><br />
(2) $D$ can be partitioned into manifolds $M^{\Delta}$ satisfying: 
<br /><br />
(2.1) The conditional distribution of $p$ restricted to $M^{\Delta}$ has a upper bounded Poincare constant. 
<br /><br />
(2.2) The marginal distribution over $\Delta$ has a upper bounded Poincare constant. 
<br /><br />
(2.3) The conditional probability distribution over $M^{\Delta}$ does not “change too quickly” as $\Delta$ changes.
<br /><br />
Then Langevin mixes quickly to a distribution close to the conditional distribution of $p$ restricted to $D$.</p>
</blockquote>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/partition_illustration.gif" />
</center>

<p>While the above theorem is a bit of a mouthful (even very informally stated) and requires a choice of partitioning of $D$ to be “instantiated”, it’s quite natural to think of it as an analogue of local convergence results for gradient descent in optimization. Namely, it gives geometric conditions under which Langevin started near a manifold mixes to the “local” stationary distribution (i.e. the conditional distribution $p$ restricted to $D$).</p>

<p>The proof of the theorem uses similar decomposition ideas as result on sampling multimodal distributions from the <a href="http://www.offconvex.org/2021/03/01/beyondlogconcave2/">previous post</a>, albeit is complicated by measure theoretic arguments. Namely, the manifolds $M^{\Delta}$ have technically zero measure under the distribution $p$, so care must be taken with how the “projected” and “restricted” chain are defined—the key tool for this is the so-called <a href="https://en.wikipedia.org/wiki/Smooth_coarea_formula#:~:text=In%20Riemannian%20geometry%2C%20the%20smooth,with%20integrals%20over%20their%20codomains.&amp;text=%2C%20i.e.%20the%20determinant%20of%20the,orthogonal%20complement%20of%20its%20kernel.">co-area formula</a>.</p>

<p>The challenge in using the above framework is instantiating the decomposition: namely, the choice of the partition of $D$ into manifolds $M^{\Delta}$. In the next section, we show how this can be done for posteriors in problems like matrix factorization/sensing/completion.</p>

<h2 id="matrix-factorization-and-relatives">Matrix factorization (and relatives)</h2>

<p>To instantiate the above framework in a natural setting, we consider distributions exhibiting invariance under orthogonal transformations. Namely, we consider distributions of the type</p>

\[p: \mathbb{R}^{d \times k} \to \mathbb{R}, \hspace{0.5cm} p(X) \propto e^{-\beta \| \mathcal{A}(XX^T) - b \|^2_2}\]

<p>where $b \in \mathbb{R}^{m}$ is a fixed vector and $\mathcal{A}$ is an operator that returns a $m$-dimensional vector given a $d \times d$ matrix. For this distribution, we have $p(X) = p(XO)$ for any orthogonal matrix $O$, since $XX^T = XO (XO)^T$ . Depending on the choice of $\mathcal{A}$, we can easily recover some familiar functions inside the exponential: e.g. the $l_2$ losses for (low-rank) matrix factorization, matrix sensing and matrix completion. These losses received a lot of attention as simple examples of objectives that are non-convex but can still be optimized using gradient descent. (See e.g. <a href="https://arxiv.org/abs/1704.00708">Ge et al. ‘17</a>.)</p>

<p>These distributions also have a very natural statistical motivation. Namely, consider the distribution over $m$-dimensional vectors, such that</p>

\[b = \mathcal{A}(XX^T) + n, \hspace{0.5cm} n \sim N\left(0,\frac{1}{\sqrt{\beta}}I\right).\]

<p>Then, the distribution $p(X) \propto e^{-\beta  | \mathcal{A}(XX^T) - b |^2_2 }$ can be viewed as the posterior distribution over $X$ with a uniform prior. Thus, sampling from these distributions can be seen as the distributional analogue of problems like matrix factorization/sensing/completion, the difference being that we are not merely trying to find the <em>most likely</em> matrix $X$, but also trying to sample from the posterior.</p>

<p>We will consider the case when $\beta$ is sufficiently large (in particular, $\beta = \Omega(\mbox{poly}(d))$: in this case, the distribution $p$ will concentrate over two (separated) manifolds: $E_1 = \{X_0 R: R \mbox{ is orthogonal with det 1}\}$ and $E_2 = \{X_0 R: R \mbox{ is orthogonal with det }-1\}$, where $X_0$ is any fixed minimizer of $| \mathcal{A}(XX^T) - b |^2_2$. Hence, when started near one of these manifolds, we expect Langevin to stay close to it for a long time (see figure below).</p>

<center>
<img width="500" src="http://www.andrew.cmu.edu/user/aristesk/langevin_matrix.gif" />
</center>

<p>We show:</p>

<blockquote>
  <p><strong>Theorem 2 (Moitra and Risteski ‘20, informal)</strong>: Let $\mathcal{A}$ correspond to matrix factorization, sensing or completion under standard parameter assumptions for these problems. Let $\beta = \Omega(\mbox{poly}(d))$. 
If initialized close to one of $E_i, i \in \{1, 2\}$, after a polynomial number of steps the discretized Langevin dynamics will converge to a distribution that is close in total variation distance
to p(X) when restricted to a neighborhood of $E_i$.</p>
</blockquote>



<p>We remark that the closeness condition for the first step is easy to ensure using existing results on gradient-descent based optimization for these objectives. It’s also easy to use the above result to sample approximately from the distribution $p$ itself, rather than only the “local” distributions $p^i$ – this is due to the fact that the distribution $p$ looks like the “disjoint union” of the distributions $p^1$ and $p^2$.</p>

<p>Before we describe the main elements of the proof, we review some concepts from differential geometry.</p>

<h2 id="extremely-brief-intro-to-differential-geometry">(Extremely) brief intro to differential geometry</h2>

<p>We won’t do a full primer on differential geometry in this blog post, but we will briefly informally describe some of the relevant concepts. See Section 5 of <a href="https://arxiv.org/abs/2002.05576">our paper</a> for an intro to differential geometry (written with a computer science reader in mind, so more easy-going than a differential geometry textbook).</p>

<p>Recall, the <em>tangent</em> space at $x$, denoted by $T_x M$, is the set of all derivatives $v$ of a curve passing through $x$. The <em>Ricci curvature</em> at a point $x$, in direction $v \in T_x M$, denoted $\mbox{Ric}_x(v)$, captures the second-order term in the rate of change of volumes of sets in a small neighborhood around $x$, as the points in the set are moved along the geodesic (i.e. shortest path curve) in direction $v$ (or more precisely, each point $y$ in the set is moved along the geodesic in the direction of the parallel transport of $v$ at $y$; see the right part of the figure below from <a href="https://projecteuclid.org/euclid.aspm/1543086328">(Ollivier 2010)</a>). A Ricci curvature of $0$ preserves volumes (think: a plane), a Ricci curvature $&gt;0$ shrinks volume (think: a sphere), and a Ricci curvature $&lt;0$ expands volume (think: a hyperbola).</p>

<center>
<img width="800" src="http://www.andrew.cmu.edu/user/aristesk/tangent.jpg" />
</center>



<p>The connections between curvature and mixing time of diffiusions is rather deep and we won’t attempt to convey it fully in a blog post - the definitive reference is <a href="https://link.springer.com/book/10.1007/978-3-319-00227-9">Analysis and Geometry of Markov Diffusion Operators</a> by Bakry, Gentil and Ledoux. The main idea is that mixing time can be bounded by how long it takes for random walks starting at different locations to “join together,” and positive curvature brings them together faster.
</p>

<p>To make this formal, we define a <em>coupling</em> of two random variables $X, Y$ to be any random variable $W = (X’,Y’)$ such that the marginal distribution of the coordinates $X’$ and $Y’$ are the same as the distributions of $X$ and $Y$. It’s well known that the convergence time of a random walk in total variation distance can be upper bounded by the expected time until two coupled copies of the walk join. On the plane, a canonical coupling (the <em>reflection coupling</em>) between two Brownian motions can be constructed by reflecting the move of the second process through the perpendicular bisector between the locations of the two processes (see figure below). On a positively curved manifold (like a sphere), an analogous reflection can be defined, and the curvature only brings the two processes closer faster.</p>

<center>
<img width="500" src="http://www.andrew.cmu.edu/user/aristesk/reflection.jpg" />
</center>

<p>As a final tool, our proof uses a very important theorem due to <a href="https://www.sciencedirect.com/science/article/pii/S0001870876800023">Milnor</a> about manifolds with algebraic structure:</p>

<blockquote>
  <p><strong>Theorem (Milnor ‘76, informal)</strong>: The Ricci curvature of a Lie group equipped with a left-invariant metric is non-negative.</p>
</blockquote>

<p>In a pinch, a Lie group is a group that also is a smooth manifold, and furthermore, the group operations result in a smooth transformation on the manifold - so that the “geometry” and “algebra” combine together. A metric is left-invariant for the group if acting on the left by any group element leaves the metric “unchanged”.</p>

<h2 id="implementing-the-decomposition-framework">Implementing the decomposition framework</h2>

<p>To apply the framework we sketched out as part of Theorem 1, we need to verify the conditions of the Theorem.</p>

<p>To prove <strong>Condition 1</strong>, we need to show that for large $\beta$, the random walk stays near to the manifold it’s been initialized close to. The main tools for this are <a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma">Ito’s lemma</a>, local convexity of the function $| \mathcal{A}(XX^T) - b |_2^2$ and basic results in the theory of <a href="https://en.wikipedia.org/wiki/Cox%E2%80%93Ingersoll%E2%80%93Ross_model">Cox-Ingersoll-Ross</a> processes. Namely, Ito’s lemma (which can be viewed as a “change-of-variables” formula for random variables) allows us to write down a stochastic differential equation for the evolution of the distance of $X$ from the manifold, which turns out to have a “bias” towards small values, due to the local convexity of $| \mathcal{A}(XX^T) - b |_2^2$. This can in turn be analyzed approximately as Cox-Ingersoll-Ross process - a well-studied type of non-negative stochastic process.</p>

<p>To prove <strong>Condition 2</strong>, we need to specify the partition of the space around the manifolds $E_i$. Describing the full partition is somewhat technical, but importantly, the manifolds $M^{\Delta}$ have the form $M^{\Delta} = \{\Delta U: U \mbox{ is an orthogonal matrix with det 1}\}$ for some matrix $\Delta \in \mathbb{R}^{n \times k}$.</p>

<p>The proof that $M^{\Delta}$ has a good Poincare constant (i.e. Condition 2.1) relies on two ideas: first, $M^{\Delta}$ is a Lie group with group operation $\circ$ defined such that $(\Delta U)   \circ (\Delta V) := \Delta (UV)$, along with a corresponding left-invariant metric - thus, by Milnor’s theorem, it has a non-negative Ricci curvature; second, we can relate the Ricci curvatures with the Euclidean metric to the curvature with the left-invariant metric. The proof that the marginal distribution over $\Delta$ has a good Poincaré constant involves showing that this distribution is approximately log-concave. Finally, the “change-of-conditional-probability” condition (Condition 2.3) can be proved by explicit calculation.</p>

<h1 id="closing-remarks">Closing remarks</h1>

<p>In this series of posts, we surveyed two recent approaches to analyzing Langevin-like sampling algorithms <em>beyond log-concavity</em> - the most natural analogue to non-convexity in the world of sampling/inference. The structures we considered, <em>multi-modality</em> and <em>invariant manifolds</em>, are common in practice in modern machine learning.</p>

<p>Unlike non-convex optimization, provable guarantees for sampling beyond log-concavity is still under-studied and we hope our work will inspire and excite further efforts. For instance, how do we handle modes of different “shape”? Can we handle an exponential number of modes, if they have further structure (e.g., posteriors in concrete latent-variable models like Bayesian networks)? Can we handle more complex manifold structure (e.g. the matrix distributions we considered for <em>any</em> $\beta$)?</p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/03/12/beyondlogconcave3/"><span class="datestr">at March 12, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/flavoursofdelta/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/flavoursofdelta/">What is δ, and what δifference does it make?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>There are many variants or flavours of differential privacy (DP) some weaker than others: often, a given variant comes with own guarantees and “conversion theorems” to the others. As an example, “pure” DP has a single parameter \(\varepsilon\), and corresponds to a very stringent notion of DP:</p>

<blockquote>
  <p>An algorithm \(M\) is \(\varepsilon\)-DP if, for all neighbouring inputs \(D,D'\) and all measurable \(S\), \( \Pr[ M(D) \in S ] \leq e^\varepsilon\Pr[ M(D’) \in S ] \).</p>
</blockquote>

<p>By relaxing this a little, one obtains the standard definition of approximate DP, a.k.a. \((\varepsilon,\delta)\)-DP:</p>

<blockquote>
  <p>An algorithm \(M\) is \((\varepsilon,\delta)\)-DP if, for all neighbouring inputs \(D,D'\) and all measurable \(S\), \( \Pr[ M(D) \in S ] \leq e^\varepsilon\Pr[ M(D’) \in S ]+\delta \).</p>
</blockquote>

<p>This definition is very useful, as in many settings achieving the stronger \(\varepsilon\)-DP guarantee (i.e., \(\delta=0\)) is impossible, or comes at a very high utility cost. But how to interpret it? The above definition, on its face, doesn’t preclude what one may call “<em>catastrophic failures of privacy</em> 💥:” most of the time, things are great, but with some small probability \(\delta\) all hell breaks loose. For instance, the following algorithm is \((\varepsilon,\delta)\)-DP:</p>

<ul>
  <li>Get a sensitive database \(D\) of \(n\) records</li>
  <li>Select uniformly at random a fraction \(\delta\) of the database (\(\delta n\) records)</li>
  <li>Output that subset of records in the clear 💥</li>
</ul>

<p>(actually, this is even \((0,\delta)\)-DP!). This sounds preposterous, and obviously something that one would want to avoid in practice (lest one wants to face very angry customers or constituents). This is one of the rules of thumb for picking \(\delta\) small enough (or even “cryptographically small”), typically \(\delta \ll 1/n\), so that the records are safe (hard to disclose \(\delta n \ll 1\) records).</p>

<p>So: good privacy most of the time, but with probably \(\delta\) then all bets are off.</p>

<p>However, those catastrophic failure of privacy, while technically allowed by the definition of \((\varepsilon,\delta)\)-DP, <strong>are not something that can really happen with the DP algorithms and techniques used both in practice and in theoretical work.</strong> Before explaining why, let’s see what is the kind of desirable behaviour one would expect: a <em>“smooth, manageable tradeoff of privacy parameters.”</em> For that discussion, let’s introduce the <em>privacy loss random variable</em>: given an algorithm M and two neighbouring inputs D,D’, let \(f(y)\) be defined as
\[
	f(y) = \log\frac{\Pr[M(D)=y]}{\Pr[M(D’)=y]}
\]
for every possible output \(y\in\Omega\). Now, define the random variable \(Z := f(M(D))\) (implicitly, \(Z\) depends on \(D,D',M\)). This random variable quantifies how much observing the output of the algorithm \(M\) helps distinguishing between \(D\) and \(D'\).</p>

<p>Now, going a little bit fast, you can check that saying that \(M\) is \(\varepsilon\)-DP corresponds to the guarantee “<em>\(\Pr[Z &gt; \varepsilon] = 0\) for all neighbouring inputs \(D,D'\).</em>”
Similarly, \(M\) being \((\varepsilon,\delta)\)-DP is the guarantee \(\Pr[Z &gt; \varepsilon] \leq \delta\).\({}^{(\dagger)}\) For instance, the “catastrophic failure of privacy” corresponds to the scenario below, which depicts a possible distribution for \(Z\): \(Z\leq \varepsilon\) with probability \(1-\delta\), but then with probability \(\delta\) we have \(Z\gg 1\).</p>

<p><img width="600" style="margin: auto; display: block;" alt="The type of (bad) distribution of Z corresponding to 'our catastrophic failure of privacy'" src="https://differentialprivacy.org/images/flavours-delta-fig1.png" /></p>

<p>What we would like is a smoother thing, where even when \(Z&gt;\varepsilon\) is still remains reasonable and doesn’t immediately become large. A nice behaviour of the tails, ideally something like this:</p>

<p><img width="600" style="margin: auto; display: block;" alt="A distribution for Z with nice tails, leading to smooth tradeoffs between ε and δ" src="https://differentialprivacy.org/images/flavours-delta-fig2.png" /></p>

<p>For instance, if we had a bound on \(\mathbb{E}[|Z|]\), we could use Markov’s inequality to get, well, <em>something</em>. For instance, imagine we had \(\mathbb{E}[|Z|]\leq \varepsilon\delta\): then 
\[
	\Pr[ |Z| &gt; \varepsilon ] \leq \frac{\mathbb{E}[|Z|]}{\varepsilon }\leq \delta
\]
<em>(great! We have \((\varepsilon,\delta)\)-DP)</em>; but also  \(\Pr[ |Z| &gt; 10\varepsilon ] \leq \frac{\delta}{10}\). Privacy violations do not blow up out of proporxtion immediately, we can trade \(\varepsilon\) for \(\delta\). That seems like the type of behaviour we would like our algorithms to exhibit.</p>

<p><img width="600" style="margin: auto; display: block;" alt="The type of privacy guarantees a Markov-type tail bound would give" src="https://differentialprivacy.org/images/flavours-delta-fig3.png" /></p>

<p>But why stop at Markov’s inequality then, which gives some nice but still weak tail bounds? Why not ask for <em>stronger</em>: Chebyshev’s inequality? Subexponential tail bounds? Hell, <em>subgaussian</em> tail bounds? This is, basically, what some stronger notions of differential privacy than approximate DP give.</p>

<ul>
  <li>
    <p><strong>Rényi DP</strong> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. Renyi Differential Privacy. CSF 2017"><strong>[Mironov17]</strong></a>, for instance, is a guarantee on the moment-generating function (MGF) of the privacy random variable \(Z\): it has two parameters, \(\alpha&gt;1\) and \(\tau\), and requires that \(\mathbb{E}[e^{(\alpha-1)Z}] \leq e^{(\alpha-1)\tau}\) for all neighbouring \(D,D'\). In turn, by applying for instance Markov’s inequality to the MGF of \(Z\), we can control the tail bounds, and get a nice, smooth tradeoff in terms of \((\varepsilon,\delta)\)-DP.</p>
  </li>
  <li>
    <p><strong>Concentrated DP</strong> (CDP)  <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun and Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016"><strong>[BS16]</strong></a> is an even stronger requirement, which roughly speaking requires the algorithm to be Rényi DP <em>simultaneously</em> for all \(1&lt; \alpha \leq \infty\). More simply, this is “morally” a requirement on the MGF of \(Z\) which asks it to be subgaussian.</p>
  </li>
</ul>

<p>The above two examples are not just fun but weird variants of DP: they actually capture the behaviour of many well-known differentially private algorithms, and in particular that of the Gaussian mechanism. While the guarantees they provide are less easy to state and interpret than \(\varepsilon\)-DP or \((\varepsilon,\delta)\)-DP, they are incredibly useful to analyze those algorithms, and enjoy very nice composition properties… and, of course, lead to that smooth tradeoff between \(\varepsilon\) and \(\delta\) for \((\varepsilon,\delta)\)-DP.</p>

<p><strong>To summarize:</strong></p>
<ul>
  <li>\(\varepsilon\)-DP gives great guarantees, but is a very stringent requirement. Corresponds to the privacy loss random variable supported on \([-\varepsilon,\varepsilon]\) (no tails!)</li>
  <li>\((\varepsilon,\delta)\)-DP gives guarantees easy to parse, but on its face allows for very bad behaviours. Corresponds to the privacy loss random variable in \([-\varepsilon,\varepsilon]\) with probability \(1-\delta\) (but outside, all bets are off!)</li>
  <li>Rényi DP and Concentrated DP correspond to something in between, controlling the tails of the privacy loss random variable by a guarantee on its MGF. A bit harder to interpret, but capture the behaviour of many DP building blocks can be converted to \((\varepsilon,\delta)\)-DP (with nice trade-offs between \(\varepsilon\) and \(\delta\).</li>
</ul>

<hr />
<p>\({}^{(\dagger)}\) The astute reader may notice that this is not <em>quite</em> true. Namely, the guarantee \(\Pr[Z &gt; \varepsilon] \leq \delta\) on the privacy loss random variable (PLRV) does imply \((\varepsilon,\delta)\)-differential privacy, but the converse does not hold. See, for instance, Lemma 9 of <a href="https://arxiv.org/abs/2004.00010" title="Clément L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020"><strong>[CKS20]</strong></a> for an exact characterization of \((\varepsilon,\delta)\)-DP in terms of the PLRV.</p></div>







<p class="date">
by Clément Canonne <a href="https://differentialprivacy.org/flavoursofdelta/"><span class="datestr">at March 12, 2021 01:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/03/11/tcs-talk-wednesday-march-17-avishay-tal-uc-berkeley/">TCS+ talk: Wednesday, March 17 — Avishay Tal, UC Berkeley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <a href="http://www.avishaytal.org/"><strong>Avishay Tal</strong></a> from UC Berkeley will speak about “<em>Junta Distance Approximation with Sub-Exponential Queries</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Joint Work with Vishnu Iyer and Michael Whitmeyer.</p>
<p>A Boolean function <img src="https://s0.wp.com/latex.php?latex=f%5Ccolon+%5C%7B0%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f\colon \{0,1\}^n \to \{0,1\}" class="latex" /> is called a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-junta if it depends only on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" /> out of the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" /> input bits. Junta testing is the task of distinguishing between <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-juntas and functions that are far from <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-juntas. A long line of work settled the query complexity of testing <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-juntas, which is <img src="https://s0.wp.com/latex.php?latex=O%28k+log%28k%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="O(k log(k))" class="latex" /> [Blais, STOC 2009; Saglam, FOCS 2018]. Suppose, however, that <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is not a perfect <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-junta but rather correlated with a <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-junta. How well can we estimate this correlation? This question was asked by De, Mossel, and Neeman [FOCS 2019], who gave an algorithm for the task making <img src="https://s0.wp.com/latex.php?latex=%5Csim%5Cexp%28k%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\sim\exp(k)" class="latex" /> queries. We present an improved algorithm that makes <img src="https://s0.wp.com/latex.php?latex=%5Csim%5Cexp%28%5Csqrt%7Bk%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\sim\exp(\sqrt{k})" class="latex" /> many queries. Along the way, we also give an algorithm, making <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%28k%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathrm{poly}(k)" class="latex" /> queries, that provides “implicit oracle access” to the underlying <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" />-junta. Our techniques are Fourier analytical and introduce the notion of “normalized influences” that might be of independent interest.</p>
<p>Paper: <a href="https://eccc.weizmann.ac.il/report/2021/004/" rel="nofollow">https://eccc.weizmann.ac.il/report/2021/004/</a></p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/03/11/tcs-talk-wednesday-march-17-avishay-tal-uc-berkeley/"><span class="datestr">at March 11, 2021 11:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5382">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5382">Long-delayed UT Austin Quantum Complexity Theory Student Project Showcase!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Back at MIT, whenever I taught my graduate course on Quantum Complexity Theory (<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-845-quantum-complexity-theory-fall-2010/lecture-notes/">see here</a> for lecture notes), I had a tradition of showcasing the student projects on this blog: see <a href="https://www.scottaaronson.com/blog/?p=515">here (Fall 2010)</a>, <a href="https://www.scottaaronson.com/blog/?p=1181">here (Fall 2012)</a>, <a href="https://www.scottaaronson.com/blog/?p=2109">here (Fall 2014)</a>.  I was incredibly proud that, each time I taught, at least some of the projects led to publishable original research—sometimes highly significant research, like Paul Christiano’s work on quantum money (which led to my later <a href="https://arxiv.org/abs/1203.4740">paper with him</a>), Shelby Kimmel’s <a href="https://arxiv.org/abs/1101.0797">work</a> on quantum query complexity, Jenny Barry’s <a href="https://arxiv.org/abs/1406.2858">work</a> on quantum partially observable Markov decision processes (“QOMDPs”), or Matt Coudron and Henry Yuen’s work on randomness expansion (which led to their later <a href="https://arxiv.org/abs/1310.6755">breakthrough</a> in the subject).</p>



<p>Alas, after I moved to UT Austin, for some reason I discontinued the tradition of these blog-showcases—and inexcusably, I did this even though the wonderful new research results continued!  Now that I’m teaching Quantum Complexity Theory at UT for the third time (via Zoom, of course), I decided that it was finally time to remedy this.  To keep things manageable, this time I’m going to limit myself to research projects that began their lives in my course <em>and that are already public on the arXiv</em> (or in one case, that will soon be).</p>



<p>So please enjoy the following smorgasbord, from 2016 and 2019 iterations of my course!  And if you have any questions about any of the projects—well, I’ll try to get the students to answer in the comments section!  Thanks so much and congratulations to the students for their work.</p>



<h2>From the Fall 2016 iteration of the course</h2>



<p>William Hoza (project turned into a joint paper with Cole Graham), <strong><a href="https://arxiv.org/abs/1612.05680">Universal Bell Correlations Do Not Exist</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove that there is no finite-alphabet nonlocal box that generates exactly those correlations that can be generated using a maximally entangled pair of qubits. More generally, we prove that if some finite-alphabet nonlocal box is strong enough to simulate arbitrary local projective measurements of a maximally entangled pair of qubits, then that nonlocal box cannot itself be simulated using any finite amount of entanglement. We also give a quantitative version of this theorem for approximate simulations, along with a corresponding upper bound.</p></blockquote>



<p>Patrick Rall, <strong><a href="https://arxiv.org/abs/1702.06990">Signed quantum weight enumerators characterize qubit magic state distillation</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Many proposals for fault-tolerant quantum computation require injection of ‘magic states’ to achieve a universal set of operations. Some qubit states are above a threshold fidelity, allowing them to be converted into magic states via ‘magic state distillation’, a process based on stabilizer codes from quantum error correction.<br />We define quantum weight enumerators that take into account the sign of the stabilizer operators. These enumerators completely describe the magic state distillation behavior when distilling T-type magic states. While it is straightforward to calculate them directly by counting exponentially many operator weights, it is also an NP-hard problem to compute them in general. This suggests that finding a family of distillation schemes with desired threshold properties is at least as hard as finding the weight distributions of a family of classical codes.<br />Additionally, we develop search algorithms fast enough to analyze all useful 5 qubit codes and some 7 qubit codes, finding no codes that surpass the best known threshold.</p></blockquote>



<h2>From the Spring 2019 iteration of the course</h2>



<p>Ying-Hao Chen, <strong><a href="https://arxiv.org/abs/1909.03787">2-Local Hamiltonian with Low Complexity is QCMA-complete</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove that 2-Local Hamiltonian (2-LH) with Low Complexity problem is QCMA-complete by combining the results from the QMA-completeness of 2-LH and QCMA-completeness of 3-LH with Low Complexity. The idea is straightforward. It has been known that 2-LH is QMA-complete. By putting a low complexity constraint on the input state, we make the problem QCMA. Finally, we use similar arguments as in [Kempe, Kitaev, Regev] to show that all QCMA problems can be reduced to our proposed problem.</p></blockquote>



<p>Jeremy Cook, <strong><a href="https://arxiv.org/abs/1907.11368">On the relationships between Z-, C-, and H-local unitaries</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Quantum walk algorithms can speed up search of physical regions of space in both the discrete-time [<a href="https://arxiv.org/abs/quant-ph/0402107">arXiv:quant-ph/0402107</a>] and continuous-time setting [<a href="https://arxiv.org/abs/quant-ph/0306054">arXiv:quant-ph/0306054</a>], where the physical region of space being searched is modeled as a connected graph. In such a model, Aaronson and Ambainis [<a href="https://arxiv.org/abs/quant-ph/0303041">arXiv:quant-ph/0303041</a>] provide three different criteria for a unitary matrix to act locally with respect to a graph, called <em>Z</em>-local, <em>C</em>-local, and <em>H</em>-local unitaries, and left the open question of relating these three locality criteria. Using a correspondence between continuous- and discrete-time quantum walks by Childs [<a href="https://arxiv.org/abs/0810.0312">arXiv:0810.0312</a>], we provide a way to approximate <em>N</em>×<em>N H</em>-local unitaries with error <em>δ</em> using <em>O</em>(1/<em>√δ,√N</em>) <em>C</em>-local unitaries, where the comma denotes the maximum of the two terms.</p></blockquote>



<p>Joshua A. Cook, <strong><a href="https://arxiv.org/abs/1906.10495">Approximating Unitary Preparations of Orthogonal Black Box States</a></strong>.</p>



<blockquote class="wp-block-quote"><p>In this paper, I take a step toward answering the following question: for m different small circuits that compute m orthogonal n qubit states, is there a small circuit that will map m computational basis states to these m states without any input leaving any auxiliary bits changed. While this may seem simple, the constraint that auxiliary bits always be returned to 0 on any input (even ones besides the m we care about) led me to use sophisticated techniques. I give an approximation of such a unitary in the m = 2 case that has size polynomial in the approximation error, and the number of qubits n.</p></blockquote>



<p>Sabee Grewal (project turned into a joint paper with me), <strong><a href="https://arxiv.org/abs/2102.10458">Efficient Learning of Non-Interacting Fermion Distributions</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We give an efficient classical algorithm that recovers the distribution of a non-interacting fermion state over the computational basis. For a system of <em>n</em> non-interacting fermions and <em>m</em> modes, we show that <em>O</em>(<em>m</em><sup>2</sup><em>n</em><sup>4</sup>log(<em>m</em>/<em>δ</em>)/<em>ε</em><sup>4</sup>) samples and <em>O</em>(<em>m</em><sup>4</sup><em>n</em><sup>4</sup>log(<em>m</em>/<em>δ</em>)/<em>ε</em><sup>4</sup>) time are sufficient to learn the original distribution to total variation distance <em>ε</em> with probability 1−<em>δ</em>. Our algorithm empirically estimates the one- and two-mode correlations and uses them to reconstruct a succinct description of the entire distribution efficiently.</p></blockquote>



<p>Sam Gunn and Niels Kornerup, <strong><a href="https://arxiv.org/abs/1906.07673">Review of a Quantum Algorithm for Betti Numbers</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We looked into the algorithm for calculating Betti numbers presented by Lloyd, Garnerone, and Zanardi (LGZ). We present a new algorithm in the same spirit as LGZ with the intent of clarifying quantum algorithms for computing Betti numbers. Our algorithm is simpler and slightly more efficient than that presented by LGZ. We present a thorough analysis of our algorithm, pointing out reasons that both our algorithm and that presented by LGZ do not run in polynomial time for most inputs. However, the algorithms do run in polynomial time for calculating an approximation of the Betti number to polynomial multiplicative error, when applied to some class of graphs for which the Betti number is exponentially large.</p></blockquote>



<p>William Kretschmer, <strong><a href="https://arxiv.org/abs/1907.06731">Lower Bounding the AND-OR Tree via Symmetrization</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove a simple, nearly tight lower bound on the approximate degree of the two-level AND-OR tree using symmetrization arguments. Specifically, we show that ~deg(AND<em>m</em>∘OR<em>n</em>)=Ω(~<em>√</em>(<em>mn</em>)). To our knowledge, this is the first proof of this fact that relies on symmetrization exclusively; most other proofs involve the more complicated formulation of approximate degree as a linear program [BT13, She13, BDBGK18]. Our proof also demonstrates the power of a symmetrization technique involving Laurent polynomials (polynomials with negative exponents) that was previously introduced by Aaronson, Kothari, Kretschmer, and Thaler [AKKT19].</p></blockquote>



<p>Jiahui Liu and Ruizhe Zhang (project turned into a joint paper with me, Mark Zhandry, and Qipeng Liu), <br /><strong><a href="https://arxiv.org/abs/2004.09674">New Approaches for Quantum Copy-Protection</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Quantum copy protection uses the unclonability of quantum states to construct quantum software that provably cannot be pirated. Copy protection would be immensely useful, but unfortunately little is known about how to achieve it in general. In this work, we make progress on this goal, by giving the following results:<br />– We show how to copy protect any program that cannot be learned from its input/output behavior, relative to a classical oracle. This improves on Aaronson [CCC’09], which achieves the same relative to a quantum oracle. By instantiating the oracle with post-quantum candidate obfuscation schemes, we obtain a heuristic construction of copy protection.<br />– We show, roughly, that any program which can be watermarked can be copy detected, a weaker version of copy protection that does not prevent copying, but guarantees that any copying can be detected. Our scheme relies on the security of the assumed watermarking, plus the assumed existence of public key quantum money. Our construction is general, applicable to many recent watermarking schemes.</p></blockquote>



<p>John Kallaugher, <strong>Triangle Counting in the Quantum Streaming Model</strong>.  Not yet available but coming soon to an arXiv near you!</p>



<blockquote class="wp-block-quote"><p>We give a quantum algorithm for counting triangles in graph streams that uses less space than the best possible classical algorithm.</p></blockquote></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5382"><span class="datestr">at March 11, 2021 08:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5376">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5376">Sayonara Majorana?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Many of you have surely <a href="https://www.wired.com/story/microsoft-retracts-disputed-quantum-computing-paper/">already</a> <a href="https://www.nature.com/articles/d41586-021-00612-z">seen</a> the news that the Kouwenhoven group in Delft—which in 2018 published a paper in <em>Nature</em> claiming to have detected <a href="https://en.wikipedia.org/wiki/Majorana_fermion">Majorana particles</a>, a type of nonabelian <a href="https://en.wikipedia.org/wiki/Anyon">anyon</a>—have <a href="https://www.nature.com/articles/s41586-021-03373-x?utm_medium=affiliate&amp;utm_source=commission_junction&amp;utm_campaign=3_nsn6445_deeplink_PID100095187&amp;utm_content=deeplink">retracted the paper</a> and apologized for “insufficient scientific rigour.”  This work was considered one of the linchpins of Microsoft’s experimental effort toward building topological quantum computers.</p>



<p>Like most quantum computing theorists, I guess, I’m thrilled if Majorana particles can be created using existing technology, I’m sad if they can’t be, but I don’t have any special investment in or knowledge of the topic, beyond what I read in the news or hear from colleagues.  Certainly Majorana particles seem neither necessary nor sufficient for building a scalable quantum computer, although they’d be a step forward for the topological approach to QC.</p>



<p>The purpose of this post is to invite <em>informed scientific discussion</em> of the relevant issues—first and foremost so that I can learn something, and second so that my readers can!  I’d be especially interested to understand:</p>



<ol><li>Weren’t there, like, several <em>other</em> claims to have produced Majoranas?  What of those then?</li><li>If, today, no one has convincingly demonstrated the existence of Majoranas, then do people think it more likely that they were produced but not detected, or that they weren’t even produced?</li><li>How credible are the explanations as to what went wrong?</li><li>Are there any broader implications for the prospects of topological QC, or Microsoft’s path to topological QC, or was this just an isolated mistake?</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5376"><span class="datestr">at March 10, 2021 10:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">Amazing: Feng Pan and Pan Zhang Announced a Way to “Spoof” (Classically Simulate) the Google’s Quantum Supremacy Circuit!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Feng Pan and Pan Zhang uploaded a new paper on the arXive  <a href="https://arxiv.org/abs/2103.03074">“Simulating the Sycamore supremacy circuits.”</a> with an amazing announcement.</p>
<blockquote><p><span style="color: #0000ff;"><em><strong> Abstract:</strong> We propose a general tensor network method for simulating quantum circuits. The method is massively more efficient in computing a large number of correlated bitstring amplitudes and probabilities than existing methods. As an application, we study the sampling problem of Google’s Sycamore circuits, which are believed to be beyond the reach of classical supercomputers and have been used to demonstrate quantum supremacy. Using our method, employing a small computational cluster containing 60 graphical processing units (GPUs), we have generated one million correlated bitstrings with some entries fixed, from the Sycamore circuit with 53 qubits and 20 cycles, with linear cross-entropy benchmark (XEB) fidelity equals 0.739, which is much higher than those in Google’s quantum supremacy experiments.</em></span></p></blockquote>
<p><strong><span style="color: #ff0000;">Congratulations to Feng Pan and Pan Zhang for this remarkable breakthrough!</span></strong></p>
<p>Of course, we can expect that in the weeks and months to come, the community will learn, carefully check, and digest this surprising result and will ponder about its meaning and interpretation. Stay tuned!</p>
<p><span style="color: #993366;">Here is a technical matter I am puzzled about: the paper claims the ability to compute precisely the amplitudes for a large number of bitstrings. (Apparently computing the amplitudes is even more difficult computational task than sampling.) But then, it is not clear to me where the upper bound of 0.739 comes from? If you have the precise amplitudes it seems that you can sample with close to perfect fidelity. (And, if you wish, you can get a F_XEB score larger than 1.)<br />
</span></p>
<p><span style="color: #993366;">Update: This is explained just before the discussion part of the paper. The crucial thing is that the probabilities for the 2^21 strings are distributed close to Porter-Thomas (exponentials). If you take samples for them indeed you can get samples with F_XEB between -1 and 15. Picking the highest 10^6  strings from 2^21 get you 0.739 (so this value has no special meaning.) Probably by using Metropolis sampling you can get (smaller, unless you enlarge 2^21 to 2^25, say) samples with F_XEB close to 1 and size-biased distribution (the distribution of probabilities of sampled strings) that fits the theoretical size biased distribution.  And you can also use metropolis sampling to get a sample of size 10^6 with the correct distribution of probabilities for somewhat smaller fidelity. </span></p>
<p>The paper mentions several earlier papers in this direction, including an earlier result by Johnnie Gray and Stefanos Kourtis in the paper <a href="https://arxiv.org/abs/2002.01935">Hyper-optimized tensor network contraction</a> and another earlier result in the paper <a href="https://arxiv.org/abs/2005.06787">Classical Simulation of Quantum Supremacy Circuits</a> by a group of researchers Cupjin Huang, Fang Zhang, Michael Newman, Junjie Cai, Xun Gao, Zhengxiong Tian, Junyin Wu, Haihong Xu, Huanjun Yu, Bo Yuan, Mario Szegedy, Yaoyun Shi, and Jianxin Chen, from Alibaba co.  Congratulations to them as well.</p>
<p>I am thankful to colleagues who told me about this paper.</p>
<h3>Some links:</h3>
<p><a href="https://thequantumdaily.com/2021/03/05/scientists-say-they-used-classical-computers-to-outperform-googles-sycamore-qc/"><span dir="ltr">Scientists Say They Used Classical Approach to outperform Google’s Sycamore QC</span></a> (“The Quantum” Written by Matt Swayne with interesting quotes from the paper. )</p>
<p><a href="https://www.scottaaronson.com/blog/?p=5371" rel="bookmark" title="Permanent Link: Another axe swung at the Sycamore">Another axe swung at the Sycamore</a> (Shtetl-Optimized; with interesting preliminary thoughts by Scott;  )</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/03/pan-zhang.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/03/pan-zhang.png?w=640&amp;h=396" class="alignnone size-full wp-image-21373" height="396" /></a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/"><span class="datestr">at March 10, 2021 01:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18282">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/10/making-algorithms-fair/">Making Algorithms Fair</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>We don’t need to be good. But let’s try to be fair. —Holly Black</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jamie-morgenstern.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2021/03/jamie-morgenstern.jpg?w=150&amp;h=150" class="alignright wp-image-18297" height="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">FairVis <a href="https://fredhohman.com/papers/fairvis">source</a>/personal <a href="http://jamiemorgenstern.com">website</a> </font></td>
</tr>
</tbody>
</table>
<p>
Jamie Morgenstern is an assistant professor in Computer Science and Engineering at the University of Washington. Previously she was at the School of Computer Science at Georgia Tech—a place where I, Dick, spent some time too. We were sad when Jamie left to go west.</p>
<p>
Today we thought we would talk about making algorithms fair.<br />
<span id="more-18282"></span></p>
<p>
We will focus on an aspect that first seems like something you would never think of.  Then, when you think of it, getting fairness seems impossible.  So when I thought of it, Ken and I thought maybe no one else had, and Ken came up only with one maybe-relevant paper that used <i>blockchain</i>.  Then I found that some people have thought of it—including Jamie.</p>
<p>
You can kind-of just make it out in the “elevator pitch” description of her work at the very top of her personal <a href="https://jamiemorgenstern.com">website</a>:</p>
<blockquote><p><b> </b> <em> How should machine learning be made robust to behavior of the people generating training or test data for it? How should ensure that the models we design do not exacerbate inequalities already present in society? </em>
</p></blockquote>
<p>
It’s in the word “generating.”  Often we generate or select test data <i>randomly</i>.  Or at least we say we do.  How can we assure that our use of randomness is <i>fair</i>?  Is that even a question?  After all, randomness doesn’t intend bias.  The epitome of randomness is something we call “flipping a <i>fair</i> coin” to begin with.</p>
<p>
</p><h2> Never Thought of It </h2><p></p>
<p>
I believe that one of the issues that delayed the rise of fairness as a property of algorithms must be that it is not an obvious property. </p>
<p>
First, what makes a Turing machine <i>efficient?</i> Complexity theory is based on time and space which was first defined for Turing machines. This started with the famous 1965  <a href="https://en.wikipedia.org/wiki/Juris_Hartmanis">paper</a>, “On the Computational Complexity of Algorithms” by Juris Hartmanis and Richard Stearns. The notion of <i>time</i> is clear—just count each step of the Turing machine. <i>Space</i> is more complicated—just counting squares of the tapes is too simple, one must allow the input tape to be different from the work tape.  A workable definition is counting the tape cells that are ever <i>changed</i>.</p>
<p>
Next, what makes a Turing machine <i>fair?</i> It seems impossible to imagine a universal definition like the above paper—fairness requires domain-specific information. There is no definition that looks just at the structure of a Turing machine.</p>
<p>
So we went past the turn of the millennium without considering fairness, and missed a concept that has been around for millennia more.  As stated in last June’s <a href="https://www.simonsfoundation.org/2020/06/18/foundation-announces-simons-collaboration-on-the-theory-of-algorithmic-fairness/">announcement</a> of a Simons collaboration directed by Omer Reingold:</p>
<blockquote><p><b> </b> <em> The study of fairness is ancient and multidisciplinary: philosophers, legal experts, economists, statisticians, social scientists and others have been concerned with fairness for as long as these fields have existed. Nevertheless, the scale of decision-making in the age of big data, the computational complexities of algorithmic decision-making and simple professional responsibility mandate that computer scientists contribute to this research endeavor. </em></p></blockquote>
<p>
Indeed we could quote many others on why fairness is important. </p>
<p>
One of the embarrassments is that computer scientists did not study fairness earlier. I must admit I never did too. I was concerned about being “good”: about making algorithms faster and making them use less space. But not being fair.</p>
<p>
</p><h2> In Search of Fairness </h2><p></p>
<p>
Okay, fairness requires domain-specific information. But are there some cases that could be defined in a way that avoids specialized knowledge? A more general definition would allow results to have more applications. And one domain that should naturally be more generic is the use of randomness.</p>
<p>
The <a href="https://arxiv.org/pdf/1906.03284.pdf">paper</a> that caught my eye is titled, “Equalized odds post processing under imperfect group information,” by Morgenstern with Pranjal Awasthi and Matthaus Kleindessner. They say: </p>
<blockquote><p><b> </b> <em> Most approaches aiming to ensure a model’s fairness with respect to a protected attribute (such as gender or race) assume to know the true value of the attribute for every data point. In this paper, we ask to what extent fairness interventions can be effective even when only imperfect information about the protected attribute is available. </em>
</p></blockquote>
<p>
At the high level the result is about how the ability to achieveness depends on the amount of information one has about values of the attribute <img src="https://s0.wp.com/latex.php?latex=%7BA%7D%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A};" class="latex" /> that one is trying to protect against bias.  The result states that even having imperfect information yields nontrivial improvements in fairness. But at the low level are techniques for equalizing conditional probabilities on values of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and a long technical appendix employing random variables.</p>
<p>
Another area that yields a reasonably generic definition of fairness is algorithms for games of chance. The insight is: These all invoke randomness and the algorithms must not cheat. That is the generation and use of randomness must be fair—no cheating. 	</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/03/blockchainbullets.jpg"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2021/03/blockchainbullets.jpg?w=550&amp;h=290" class="aligncenter wp-image-18298" height="290" /></a>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from CasinosBlockchain.io <a href="https://casinosblockchain.io/provably-fair-gambling/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
There are two different types of approaches to making the randomness fair. One uses blockchain technology, as presented in the <a href="https://casinosblockchain.io/provably-fair-gambling/">article</a> accompanying the above picture.</p>
<blockquote><p><b> </b> <em> In a contest system on hybrid blockchain Dragonchain, a future hash of Bitcoin and Ethereum that has not been created yet, combined with an algorithm anyone can execute themselves, a provably fair random selection and revealing occurs, which is near impossible to profitably manipulate, as it is backed by hundreds of millions of dollars worth of proof from decentralized blockchains that Interchain with Dragonchain. </em>
</p></blockquote>
<p>
Another uses a more standard way to make the randomness secure. Here is one such <a href="https://en.wikipedia.org/wiki/Provably_fair_algorithm#Benefits_And_Drawbacks_of_Provable_Fairness">paper</a>.</p>
<blockquote><p><b> </b> <em> In a provably fair gambling system, a player places bets on games offered by the service operator. The service operator will publish a method for verifying each transaction in the game. This is usually done by using open source algorithms for random seed generation, hashing, and for the random number generator.</em></p><em>
<p>
Once a game has been played, the player can use these algorithms to test the game’s response to their in-game decisions, and evaluate the outcome by only using the published algorithms, the seeds, hashes, and the events which transpired during the game.</p>
</em><p><em>
In a simplified way, players can always check that the outcome of every game round is fair and wasn’t tampered with, and so can the game’s operators. As such, cheating is arguably impossible. </em>
</p></blockquote>
<p>
</p><h2> In Search of Correctness </h2><p></p>
<p>
Let’s look at fairness from a complexity point of view. We are interested in seeing if NP gives us some insight to how to define fairness, at lest for games.</p>
<p>
Consider an algorithm <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> that given a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> must output a coloring of the vertices with <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> colors. The algorithm outputs a map from <img src="https://s0.wp.com/latex.php?latex=%7BV%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V(G)}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{1,2,3\}}" class="latex" />: from vertices of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> to the colors <img src="https://s0.wp.com/latex.php?latex=%7B1%2C2%2C3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1,2,3}" class="latex" />. The algorithm may output a legal coloring or not. We are left with two choices: </p>
<ol>
<li>Check for each edge <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2C+b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(a, b)}" class="latex" /> that <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> have different colors.
</li><li>Prove that the algorithm <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> is always correct.
</li></ol>
<p>
We argue that there is a similar situation with an algorithm <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> that claims that it is fair. The algorithm leaves us with one choice: Prove that the algorithm <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> always satisfies our definition of fair. Recall the famous <a href="https://en.wikipedia.org/wiki/Trust,_but_verify">saying</a>: </p>
<blockquote><p><b> </b> <em> Trust, but verify. </em>
</p></blockquote>
<p>
In the above we can either trust the algorithm <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> or it can check that its output is a three coloring. What we want to know is can we also avoid trusting that the algorithm <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> is fair? We believe that it is possible in some situations to have the fairness of an algorithm be checkable. That is we will not have to prove the algorithm is fair, but can look at the output of the algorithm and conclude that it is fair.</p>
<p>
Let’s look at this next.</p>
<p>
</p><h2> A Fairness Problem </h2><p></p>
<p>
The question we consider is this: Imagine that <img src="https://s0.wp.com/latex.php?latex=%7BTotal%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Total}" class="latex" /> people apply to our department of Computer Science for the PhD program. We have some reasonable criterion that will select the eligible applicants and yields <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D+%5Cle+Total%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb N} \le Total}" class="latex" />. Among the <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb N}}" class="latex" /> we must choose randomly say <img src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Accept}" class="latex" />—no other criterion can be used. How can we be sure that this actually is what happens?</p>
<p>
One possible solution is to have an algorithm that operates like this: </p>
<ol>
<li>Let us input the <img src="https://s0.wp.com/latex.php?latex=%7BTotal%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Total}" class="latex" /> people with their properties.
</li><li>Then use the acceptance criterion to reduce this to a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> people that meet all the criteria.
</li><li>Finally randomly select <img src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Accept}" class="latex" /> from the set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />.
</li></ol>
<p>
Note: Any choice of <img src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Accept}" class="latex" /> from the <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb N}}" class="latex" /> could be fair. But what if it was based on some decision that was biased? What if we select further from the <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\mathbb N}}" class="latex" /> based on race or gender or some other criterion? This is the problem. The algorithm could claim that it is fair, but it could cheat. If the criteria are complex the code that checks them could be messy. We could hide the fact that we then “randomly” selected as required. </p>
<p>
</p><h2> A Solution </h2><p></p>
<p>
Here is a solution that uses a protocol between the selection algorithm and <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> other players. </p>
<ol>
<li>It is a protocol that uses several players <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" />.
</li><li>Only by having all <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> cheat can the protocol be defeated.
</li><li>It is public and can be checked after the fact.
</li><li>Its correctness assumes standard crypto methods are safe.
</li></ol>
<p>
The protocol assume that <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> object must be selected from <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> total objects. Note the general case follows the same method. For each <img src="https://s0.wp.com/latex.php?latex=%7Bj%3D1%2C%5Cdots%2CP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{j=1,\dots,P}" class="latex" /> objects a bit commitment <img src="https://s0.wp.com/latex.php?latex=%7BB_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B_j}" class="latex" /> is sent to the <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{j}" class="latex" /> player. This encodes either a <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> or a <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> as usual. The <img src="https://s0.wp.com/latex.php?latex=%7Bj%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{j^{th}}" class="latex" /> player then randomly flips a coin and sends back <img src="https://s0.wp.com/latex.php?latex=%7Br_j+%5Cin+%5C%7B0%2C1%5C%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r_j \in \{0,1\}.}" class="latex" /> Let <img src="https://s0.wp.com/latex.php?latex=%7Bs_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s_j}" class="latex" /> be equal to <img src="https://s0.wp.com/latex.php?latex=%7Br_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r_j}" class="latex" /> if the bit committed <img src="https://s0.wp.com/latex.php?latex=%7BB_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B_j}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1-r_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1-r_j}" class="latex" /> otherwise. Then compute 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++s_1+%2B+%5Cdots+%2B+s_P+%5Cbmod+2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  s_1 + \dots + s_P \bmod 2. " class="latex" /></p>
<p>If it is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> then pick object <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> else pick object <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" />.</p>
<p>
The claim is that unless the bit method is unsafe or all the players are cheating this is correct.</p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>
Does making coin flips fair solve any real problem? Does it help make some algorithms easier to show that they are fair?</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/03/10/making-algorithms-fair/pd/" rel="attachment wp-att-18287"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/03/pd.jpg?w=600&amp;h=405" class="alignright size-full wp-image-18287" height="405" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/03/10/making-algorithms-fair/"><span class="datestr">at March 10, 2021 12:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=86">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/">Thursday March 18 — Tim Roughgarden  from Columbia University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, March 18</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Tim Roughgarden</strong> from <strong>Columbia Univeristy</strong> will speak about “<strong>Data-Driven Algorithm Design</strong>.”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: The best algorithm for a computational problem generally depends on the “relevant inputs”, a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem.</p>



<p class="has-text-align-justify">We adapt concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models are straightforward to understand, but also expressive enough to capture several existing approaches in the theoretical computer science and AI communities, ranging from self-improving algorithms to empirical performance models. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binary- and real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.</p>



<p>Joint work with Rishi Gupta.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/"><span class="datestr">at March 09, 2021 08:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1823">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/03/09/automated-design-of-error-correcting-codes-part-2/">Automated Design of Error-Correcting Codes, Part 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In our <a href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/">previous post</a>, we discussed the automation of error correcting codes and how formal methods are quite helpful toward this goal. In this post, we will discuss machine learning techniques as well as possible directions for future research.</p>



<p><strong>Machine Learning. </strong>The use of machine-learning techniques to study ECCs has increased exponentially in recent years. The main strength of machine learning methods is that they can adapt well to a variety of conditions (c.f., <a href="https://arxiv.org/pdf/1911.03038.pdf">this paper</a>), unlike the codes produced by formal methods which are often designed for a rigid application. On the other hand, it is much more difficult to give any formal guarantees for error-correcting codes designed with machine learning, although such an obstacle may be overcome in the future (see the conclusion). </p>



<p><strong>Error-correcting output codes. </strong>One interface between machine learning and error correcting codes which has been studied for decades is the construction of error correcting codes for multiclass learning. Multiclass learning is classifying a group of objects into a number of categories. Instead of trying to distinguish the categories all up front, one can instead try to do a binary classification of some subset of the categories from another subset of the categories.” If an error correcting code (called an error-correcting output code–ECOC) is used for the distinguishing, it can provide a more robust classification.</p>



<p>This technique was studied theoretically for a preset error-correcting code (e.g., <a href="https://dl.acm.org/doi/pdf/10.1145/307400.307429">[Guruswami, Sahai, 1999]</a>). There are also works which synthesize an ECOC for multiclass learning. For example, the work of <a href="https://ieeexplore.ieee.org/abstract/document/1624364">Pujol, Radeva, and Vitria [2006]</a> used a heuristic which tries to find tests which reveal the highest amount of information (subject to a prior that each class is a <a href="https://en.wikipedia.org/wiki/Mixture_of_gaussians#Gaussian_mixture_model">mixture of gaussians</a>).</p>



<p>A related recent work <a href="https://arxiv.org/pdf/1808.01942.pdf">[Xiang, Wang, Kitani, 2018]</a> uses the technique of <em>deep hashing</em> to hash images so that images of a similar class have hashes which are close in Hamming distance, while images of different classes have hashes which are far in edit distance.</p>



<p><strong>Deep Learning. </strong>The bulk of recent machine learning research on the automation of ECCs has been using <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> techniques; that is, training a constructed deep learning network to either encode–given a message as input, output a robust encoding–or decode–given a noisy transmission, recover the original message. As deep neural networks pass messages from earlier neurons to deeper neurons, the commonly referred to benchmark is that of <em>belief propagation</em>.</p>



<p>In brief (see also <a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>), the <a href="https://en.wikipedia.org/wiki/Belief_propagation">belief propagation</a> (BP) algorithm starts by taking the parity-check matrix defining a given linear code and converts it into a bipartite graph–one side of the vertices are the symbols of the code and the other vertices are the parity checks. Each vertex starts with a prior (i.e., probability that the vertex should be a 0 or 1) based on the received transmission, and then messages are passed back and forth. In odd-numbered rounds, the symbols give their confidence levels to the parity checks, which then update probabilities based on being satisfied or not. In even-numbered rounds the parity checks inform the symbols if they should change their prior to better satisfy the parity checks. After some number of rounds, a decoding is deduced.</p>



<p>Some of the earlier papers using deep learning to automate ECCs [<a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>, <a href="https://arxiv.org/pdf/1706.07043.pdf">2018</a>; <a href="https://arxiv.org/pdf/1702.06901.pdf">Crammerer, et.al., 2017</a>; <a href="https://arxiv.org/pdf/1701.07738.pdf">Gruber, et.al., 2017</a>] tried to generalize a belief propagation algorithm for decoding ECCs. At a high level, the works of Nachmani, et.al. “unroll” the belief propagation into a deep neural network with many layers. Unlike BP which has a fixed weight for the value of each message in each stage, they have trainable weights which allow one to obtain a lower BER than plain BP. Both Nachmani, et.al., and the works of Crammerer, et.al., and Gruber, et.al., consider more complex neural architectures which use BP as a subroutine. The latter two works show success in these methods for <a href="https://en.wikipedia.org/wiki/Polar_code_(coding_theory)">polar codes</a> and random linear codes. Note that all of these works are only training a decoder, as the encodings are fixed.</p>



<p>A later group of papers [<a href="https://arxiv.org/pdf/1807.00801.pdf">Kim, et.al. 2020</a>; <a href="https://arxiv.org/pdf/1903.02295.pdf">Jiang, et.al., 2019a</a>, <a href="https://arxiv.org/pdf/1911.03038.pdf">2019b</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9053254">2020</a>] (see also <a href="https://deepcomm.github.io/">their blog</a>) introduce what is known as DeepCode. Unlike previous work, the initial DeepCode paper [Kim, et.al., 2020] builds both a custom encoder and decoder. A particular tool which the authors utilized is that of <em>feedback</em>–after each bit is transmitted the noisy received bit is sent back (possibly with more noise added). Intuitively, feedback allows for the encoder to have an approximate understanding of what the decoder doesn’t know, allowing for on-the-fly adjustments to the encoding. The encoder sends the original message in plain-text and then incorporates the feedback with <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a> units to add two redundant bits per message bit (and thus the rate is one-third). The decoder incorporates the noisy plain text and these redundant bits using bidirectional-<a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a>s. Their methods can get a BER of as low as <img src="https://s0.wp.com/latex.php?latex=10%5E%7B-7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="10^{-7}" class="latex" /> for a block length of 50 bits.</p>



<p>Follow-up work [Jiang, et.al., 2019a, 2019b, 2020] consider variants of the model. In [Jiang, et.al., 2019a], the encoder is fixed to be a <a href="https://en.wikipedia.org/wiki/Turbo_code">turbo code</a> (a type of <a href="https://en.wikipedia.org/wiki/Convolutional_code">convolutional code</a>), a “standard” code in many applications, but the goal is to train a deep net to beat the <a href="https://en.wikipedia.org/wiki/BCJR_algorithm">standard decoder</a> in a “non-Gaussian noise” model. The work [Jiang, et.al., 2019b] tries to beat the turbo code at its own game by constructing a turbo code-like deep net where a commonly used finite automata is replaced by a deep net (and a corresponding decoder is constructed as well). Finally, the work [Jiang, et.al, 2020] generalized [Jiang, et.al., 2019b] by also using feedback, like in DeepCode.</p>



<p>Outside of deep learning, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> techniques have also been used to construct a decoder (e.g., <a href="https://arxiv.org/pdf/2009.09277.pdf">[Liao, et.al., 2020]</a>).</p>



<p><strong>Conclusion and Future Directions</strong>. Although the technology is still in its infancy, the automation of error correction codes is an exciting domain which could have a variety of applications in the future. As we have seen in this post, both formal methods and machine learning allow for powerful methods for obtaining automating various aspects of ECCs, whether it be constructing error correcting codes with provable guarantees or designing non-standard decodes which are robust to a variety of conditions. Looking ahead, there are a number of exciting directions which could be ripe for theoretical and practical contributions.</p>



<ol><li>A very active field of research currently is DNA storage–that is, synthesizing DNA molecules which encode data (rather than biological life). The primary theoretical challenge of DNA storage is that data loss is no longer a “bit flip,” rather nucleotides can be inserted, deleted, replicated, etc. Very recently, convolutional codes have proved successful in this setting <a href="https://www.biorxiv.org/content/10.1101/2019.12.20.871939v2.full">[Chandak, et.al., 2020]</a>, but an analogue to DeepCode does not currently exist in this setting. One barrier is that it is difficult for a fixed-architecture neural network to easily accommodate “index shifting” taking place during insertions and deletions. Automated techniques have been used to give lower bounds on how much redundancy is needed for such codes <a href="https://publish.illinois.edu/kiyavash/files/2015/06/Kulkarni_it_trans_2013.pdf">[Kulkarni, Kiyavash, 2013]</a>.</li><li>One potential theoretical contribution in this space is understanding how these automation techniques relate to <a href="https://en.wikipedia.org/wiki/Augmented_learning"><em>augmented learning</em></a>. An example of augmented learning is the problem of picking the “best” algorithm from a class of algorithms for a given problem. A theoretical framework for understanding such questions has recently been developed by <a href="https://theory.stanford.edu/~tim/papers/features.pdf">Gupta and Roughgarden</a>. Such a framework seems natural in the context of error-correcting codes, as there are many different kinds of codes, and even within one family of codes, such as Reed-Solomon codes, there are many parameter choices whose optimal values can vary significantly from application to application. </li><li>As a final thought, the use of “Formal Methods” in contrast to “Machine Learning” for automating ECCs has been largely separate. Recently, outside the context of ECCs, there have been a number of works (e.g., <a href="https://arxiv.org/pdf/1705.01320.pdf">[Ehlers, 2017]</a>, <a href="https://arxiv.org/abs/1811.01057">[Raghunathan, Steinhardt, Liang, 2018]</a>) which use formal methods to <em>provably verify</em> that a machine learning model such as a deepnet runs correctly under given conditions. It would be exciting if such methods could be extended to ECCs by showing that machine learning encoders/decoders like DeepCode can be utilized correctly. </li></ol>



<p>Are there other directions for which you would like to see the automation of ECCs extended? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. I would also like to thank Sivakanth Gopi and Sergey Yekhanin for insightful discussion on the relationship between automation of ECCs and DNA storage as well as Ofir Geri for discussion on augmented learning.</p></div>







<p class="date">
by Joshua Brakensiek <a href="https://theorydish.blog/2021/03/09/automated-design-of-error-correcting-codes-part-2/"><span class="datestr">at March 09, 2021 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/">Good-case Latency of Byzantine Broadcast: the Synchronous Case</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In our first post, we presented a summary of our good-case latency results for Byzantine broadcast (BB) and state machine replication (SMR), where the good case measures the latency to commit given that the broadcaster or leader is honest. In our second post, we discussed our results for partial synchrony,...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/"><span class="datestr">at March 09, 2021 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/034">TR21-034 |  Robust Self-Ordering versus Local Self-Ordering | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study two notions that refers to asymmetric graphs, which we view as graphs having a unique ordering that can be reconstructed by looking at an unlabeled version of the graph.

A {\em local self-ordering} procedure for a graph $G$ is given oracle access to an arbitrary isomorphic copy of $G$, denoted $G'$, and a vertex $v$ in $G'$, and is required to identify the name (or location) of $v$ in $G$, while making few (i.e., polylogarithmically many) queries to $G'$.
A graph $G=(V,E)$ is {\em robustly self-ordered} if the size of the symmetric difference between $E$ and the edge-set of the graph obtained by permuting $V$ using any permutation $\pi:V\to V$ is proportional to the number of non-fixed-points of $\pi$ and to the maximal degree of $G$; that is, any permutation of the vertices that displaces $t$ vertices must ``displace'' $\Omega(t\cdot d)$ edges, where $d$ is the maximal degree of the graph. 

We consider the relation between these two notions in two regimes: The bounded-degree graph regime, where oracle access to a graph means oracle access to its incidence function, and the dense graph regime, where oracle access to the graph means access to its adjacency predicate. 

We show that, {\em in the bounded-degree regime}, robustly self-ordering and local self-ordering are almost orthogonal; that is, even extremely strong versions of one notion do not imply very weak versions of the other notion. 
Specifically, we present very efficient local self-ordering procedures for graphs that possess derangements that are almost automorphisms (i.e., a single incidence is violated).  
One the other hand, we show robustly self-ordered graphs having no local self-ordering procedures even when allowing a number of queries that is a square root of the graph's size. 

{\em In the dense graph regime}, local self-ordering procedures are shown to yield a quantitatively weaker version of the robust self-ordering condition, in which the said proportion is off by a factor that is related to the query complexity of the local self-ordering procedure. Furthermore, we show that this quantitatively loss is inherent.
On the other hand, we show how to transform any robustly self-ordered graph 
into one having a local self-ordering procedure, while preserving the robustness condition. Combined with prior work, this yields explicit constructions of graphs that are both robustly and locally self-ordered, and an application to property testing.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/034"><span class="datestr">at March 09, 2021 09:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/03/08/more-mathematics-books">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html">More mathematics books by women</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A year ago, for International Women’s Day, I made <a href="https://11011110.github.io/blog/2020/03/08/mathematics-books-women.html">a list of mathematics books by women covered by then-new Wikipedia articles</a>. I thought it would be worthwhile to revisit the same topic and list several more mathematics books with at least one female author, at many different levels of audience, and again covered by new Wikipedia articles. They are (alphabetical by title):</p>

<ul>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Combinatorics_on_Partial_Words">Algorithmic Combinatorics on Partial Words</a></em> (2008), Francine Blanchet-Sadri. Partial words are strings with “don’t care” symbols; Blanchet-Sadri looks at the combinatorics of repeated patterns within these strings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Geometry">Algorithmic Geometry</a></em> (1995), Jean-Daniel Boissonnat and Mariette Yvinec. One of several standard computational geometry textbooks; this is the French one, but it has also been published in translation into English.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Puzzles">Algorithmic Puzzles</a></em> (2011), Anany and Maria Levitin. A nice collection of classic logic puzzles involving algorithmic thinking.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Braids,_Links,_and_Mapping_Class_Groups">Braids, Links, and Mapping Class Groups</a></em> (1975), Joan Birman. A classic research monograph on the topology of braid groups.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Code_of_the_Quipu">Code of the Quipu</a></em> (1981), Marcia and Robert Ascher. A general-audience book on how the Inca used knotted strings to record numbers and other information.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics:_The_Rota_Way">Combinatorics: The Rota Way</a></em> (2009), Joseph P. S. Kung, Catherine Yan, and (posthumously) Gian-Carlo Rota. A graduate textbook on algebraic combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics_of_Experimental_Design">Combinatorics of Experimental Design</a></em> (1987), Anne Penfold Street and her daughter Deborah Street. A textbook on the design of experiments, an area that crosses between statistics and combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Computability_in_Analysis_and_Physics">Computability in Analysis and Physics</a></em> (1989), Marian Pour-El and J. Ian Richards. A research monograph on problems involving differential equations including the wave equation whose initial conditions are continuous and computable, but that evolve to states whose values cannot be computed.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Diophantus_and_Diophantine_Equations">Diophantus and Diophantine Equations</a></em> (1972), Isabella Bashmakova. A somewhat idiosyncratic history based on the idea that Diophantus knew some very general techniques for finding rational-number solutions to equations, that can be inferred from the much more specific solutions to individual equations that have survived to us.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Elementary_Number_Theory,_Group_Theory_and_Ramanujan_Graphs">Elementary Number Theory, Group Theory, and Ramanujan Graphs</a></em> (2003), Giuliana Davidoff, Peter Sarnak, and Alain Valette. An attempt to make the construction of expander graphs accessible to undergraduate mathematics students.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Equivalents_of_the_Axiom_of_Choice">Equivalents of the Axiom of Choice</a></em> (1963, updated 1985), Herman and Jean Rubin. A large catalog of problems in mathematics whose solution is equivalent to the axiom of choice, from a time when the independence of choice from ZF set theory had not been proven.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Erd%C5%91s_on_Graphs">Erdős on Graphs: His Legacy of Unsolved Problems</a></em> (1998), 
Fan Chung and Ronald Graham. The open problems in graph theory from this book have been further collected and updated on a web site, <a href="http://www.math.ucsd.edu/~erdosproblems/">Erdős’s Problems on Graphs</a>, maintained by Chung.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Extensions_of_First_Order_Logic">Extensions of First Order Logic</a></em> (1996), María Manzano. Attempts to unify second-order logic, modal logic, and dynamic logic, by translating them all into many-sorted logic.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Fat_Chance:_Probability_from_0_to_1">Fat Chance: Probability from 0 to 1</a></em> (2019), Benedict Gross, Joe Harris, and Emily Riehl. A general-audience undergraduate textbook on probability theory based on a metaphor of games of chance.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Fractal_Dimension_of_Architecture">The Fractal Dimension of Architecture</a></em> (2016), Michael J. Ostwald and Josephine Vaughan. Studies the fractal dimension of floor plans as a way to model the changing demands on the complexity of housing structures and to classify buildings by architect and style.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Geometry_of_Numbers">The Geometry of Numbers</a></em> (2000), Carl D. Olds, Anneli Cahn Lax, and Giuliana Davidoff. A textbook on connections between number theory and integer grids, rescued twice from the posthumous works of its first two coauthors.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_History_of_Mathematical_Tables">The History of Mathematical Tables: from Sumer to Spreadsheets</a></em> (2003), Martin Campbell-Kelly, Mary Croarken, Raymond Flood, and Eleanor Robson. An edited volume with chapters on tables from many different periods in mathematical history.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Incidence_and_Symmetry_in_Design_and_Architecture">Incidence and Symmetry in Design and Architecture</a></em> (1983), Jenny Baglivo and Jack E. Graver. A textbook on graph theory and symmetry aimed at architecture students, also including interesting material on structural rigidity.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_the_Theory_of_Error-Correcting_Codes">Introduction to the Theory of Error-Correcting Codes</a></em> (1982, updated 1989 and 1998), Vera Pless. An advanced undergraduate textbook centered on algebraic constructions of linear block codes.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_3-Manifolds">Introduction to 3-Manifolds</a></em> (2014), Jennifer Schultens. An introductory graduate textbook on low-dimensional topology, leading up to the use of normal surfaces and Heegard splittings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Journey_into_Geometries">Journey into Geometries</a></em> (1991), Márta Svéd. A conversational Alice-in-wonderland-inspired tour of non-Euclidean geometry.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Knots_Unravelled">Knots Unravelled: From String to Mathematics</a></em> (2011), Meike Akveld and Andrew Jobbings. Knot theory for schoolchildren, centered on knot invariants.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Lectures_in_Geometric_Combinatorics">Lectures in Geometric Combinatorics</a></em> (2006), Rekha R. Thomas. An advanced undergraduate or introductory graduate textbook on the combinatorics of convex polytopes and their connections to abstract algebra through secondary polytopes and toric varieties.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Making_Mathematics_with_Needlework">Making Mathematics with Needlework: Ten Papers and Ten Projects</a></em> (2008), sarah-marie belcastro and Carolyn Yackel. The projects come from eight different contributors and include photos, instructions, mathematical analyses, and teaching activities.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematical_Excursions">Mathematical Excursions: Side Trips along Paths Not Generally Traveled in Elementary Courses in Mathematics</a></em> (1933), Helen Abbot Merrill. An early book on recreational mathematics, aimed at getting high school students interested in mathematics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematics_in_India">Mathematics in India: 500 BCE–1800 CE</a></em> (2009), Kim Plofker. Organized chronologically, this has become the standard overview of this large topic. It also includes material on the history of astronomy in India, which was often tied to the mathematics of its era.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Mathematics_of_Chip-Firing">The Mathematics of Chip-Firing</a></em> (2018), Caroline Klivans. A textbook on chip-firing games and abelian sandpile models.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Markov_Chains_and_Mixing_Times">Markov Chains and Mixing Times</a></em> (2009, 2017), David A. Levin and Yuval Peres, with contributions by Elizabeth Wilmer. A graduate-level text and research reference on how quickly random walks converge to their stable distributions.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mirrors_and_Reflections">Mirrors and Reflections: The Geometry of Finite Reflection Groups</a></em> (2009), Alexandre V. and Anna Borovik. An undergraduate textbook on the classification of finite reflection groups and their associated root systems.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Pioneering_Women_in_American_Mathematics">Pioneering Women in American Mathematics: The Pre-1940 PhD’s</a></em> (2009), Judy Green and Jeanne LaDuke. Biographical profiles of over 200 women who earned doctorates in mathematics in the US before 1940, with some background material on what it was like for women to work in mathematics in those times.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Playing_with_Infinity">Playing with Infinity: Mathematical Explorations and Excursions</a></em> (1955, translated into English 1961), Rózsa Péter. An attempt to explain the nature of mathematics and of the infinite in mathematics to non-mathematicians, based on a series of letters from Péter to a literary friend.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Point_Processes">Point Processes</a></em> (1980), David Cox and Valerie Isham. A research reference on processes that randomly place points on the real line or other geometric spaces.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Power_in_Numbers:_The_Rebel_Women_of_Mathematics">Power in Numbers: The Rebel Women of Mathematics</a></em> (2018), Talithia Williams. A selection of profiles of famous women mathematicians, aimed at motivating young women to become mathematicians.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Primality_Testing_for_Beginners">Primality Testing for Beginners</a></em> (2009, translated into English 2014), Lasse Rempe-Gillen and Rebecca Waldecker. An undergraduate text on primality testing algorithms, based on a course from a summer research program for undergraduates.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Quantum_Computing:_A_Gentle_Introduction">Quantum Computing: A Gentle Introduction</a></em> (2011), Eleanor Rieffel and Wolfgang Polak. One of many texts on this fast-moving subject.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Robust_Regression_and_Outlier_Detection">Robust Regression and Outlier Detection</a></em> (1987), Peter Rousseeuw and Annick M. Leroy. A monograph on statistical methods that can tolerate the total corruption of a large fraction of the data points that they analyze, and still produce meaningful results.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Two-Sided_Matching">Two-Sided Matching: A Study in Game-Theoretic Modeling and Analysis</a></em> (1990), Alvin E. Roth and Marilda Sotomayor. A survey of methods related to stable matching, aimed at economics practitioners and focused on applications.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/When_Topology_Meets_Chemistry">When Topology Meets Chemistry: A Topological Look At Molecular Chirality</a></em> (2000), Erica Flapan. Many biomolecules are different than their mirror images; classical examples include sugars, whose mirrored molecules may taste different and have different effects. This undergraduate-level text studies how to model this effect using a combination of graph theory and knot theory.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Women_in_Mathematics">Women in Mathematics</a></em> (1974), Lynn Osen. This is the one that based its coverage of Hypatia on an early-20th-century children’s book that gave her a made-up backstory and attributed made-up modern rationalist quotes to her. Not recommended, and included mainly as a warning not to use this as a reference.</p>
  </li>
</ul>

<p>To keep from ending on a sour note, I’ll add one more, that I found recently on Wikipedia (although the article there is very old) and I think is worthy of expansion: <em><a href="https://en.wikipedia.org/wiki/Logic_Made_Easy">Logic Made Easy: How to Know When Language Deceives You</a></em> (2004), Deborah J. Bennett, a popular-audience book on how to translate English phrases into logical formalisms and use that translation to understand more clearly what they mean.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105857580884627445">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html"><span class="datestr">at March 08, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/033">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/033">TR21-033 |  Automating Tree-Like Resolution in Time $n^{o(\log n)}$ Is ETH-Hard | 

	Susanna de Rezende</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that tree-like resolution is not automatable in time $n^{o(\log n)}$ unless ETH is false. This implies that, under ETH, the algorithm given by Beame and Pitassi (FOCS 1996) that automates tree-like resolution in time $n^{O(\log n)}$ is optimal. We also provide a simpler proof of the result of Alekhnovich and Razborov (FOCS 2001) that unless the fixed parameter hierarchy collapses, tree-like resolution is not automatable in polynomial time. The proof of our results builds on a joint work with Göös, Nordström, Pitassi, Robere and Sokolov (STOC 2021), which presents a simplification of the recent breakthrough of Atserias and Müller (FOCS 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/033"><span class="datestr">at March 08, 2021 05:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-61276742347305278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html">When do I need to warn about Spoilers?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In a recent post <a href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html">here</a> I mentioned in passing a plot point from the last season of The Big Bang Theory. Note that the last season was in 2019.  WARNING- do not read that post if you are watching The Big Bang Theory and do not want a plot point revealed. </p><p>Someone who perhaps thinks Lance and I are the same person (are we? See <a href="https://blog.computationalcomplexity.org/2014/04/i-am-bill-gasarch.html">here</a>) left Lance a tweet complaining about the spoiler. At least I think they are complaining. The tweet is in Spanish and its <a href="https://twitter.com/deoxyt2/status/1366120364070338560">here</a>.</p><p>Either</p><p>1) Some country is two years behind America on showing The Big Bang Theory. </p><p>2) The person who tweeted has them on DVR (or something like that) and is watching them a few years after they air (I watched Firefly on a DVD I borrowed from a friend 10 years after it went off he air. Ask your grandparents what a DVD used to be.) </p><p>3) They are kidding us and making fun of the notion of spoilers.</p><p>This raises the question: When is it okay to post spoilers without warning? A few random thoughts:</p><p>1) ``Don't tell me who won the superb owl! I have it on tape and want to watch it without knowing who won!''  This always seemed odd to me.  Routing for events to happen that have already happened seems weird to me. When I was 10 years old I was in New York listening to a Knicks-Celtics Basketball game on the radio and during halftime I accidentally found a Boston radio station that had the game 30 minutes later (I did not realize that the channel I was on originally was 30 minutes behind). So I heard how the game ended, then switched back <i>listening to a game knowing how it would end. </i>I didn't route for my team (the Knicks, who lost) but it just felt very weird listening to it. If I had thought of it I might have noticed how the different broadcasts differ and got a paper out of the data, but as a 10 year old I was not thinking about how to pad my resume quite yet. </p><p>2) I like seeing a mystery twice- first time I don't know who did it, second time I do but can look for clues I missed the first time.</p><p>3) I would have thought 2 years after a show is off the air its fine to spoil. But... maybe not.</p><p>4) It also matters how important the plot point is. I didn't think the plot point I revealed was that important. </p><p>5) Many TV shows are predictable so I am not sure what `spoiler' even means. If I said to Darling:</p><p><i> The bad guy is an unimportant character we meet in the first 10 minutes.</i></p><p>that does not show I've seen it before. It shows that I am a master of TV-logic.</p><p>6) With Arc TV shows this is more of a problem. While it was possible to spoil an episode (Captain Kir will survive but Ensign Red Shirt will bite the dust) it was impossible to spoil a long-term arc. TV has gotten to complicated. And I say that without having watched Game of Thrones. </p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html"><span class="datestr">at March 08, 2021 02:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5371">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5371">Another axe swung at the Sycamore</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>So there’s an interesting new paper on the arXiv by Feng Pan and Pan Zhang, entitled <a href="https://arxiv.org/abs/2103.03074">“Simulating the Sycamore supremacy circuits.”</a>  It’s about a new tensor contraction strategy for classically simulating Google’s 53-qubit quantum supremacy experiment from Fall 2019.  Using their approach, and using just 60 GPUs running for a few days, the authors say they managed to generate a million <em>correlated</em> 53-bit strings—meaning, strings that all agree on a specific subset of 20 or so bits—that achieve a high linear cross-entropy score.</p>



<p>Alas, I haven’t had time this weekend to write a “proper” blog post about this, but several people have by now emailed to ask my opinion, so I thought I’d share the brief response I sent to a journalist.</p>



<p>This does look like a significant advance on simulating Sycamore-like random quantum circuits!  Since it’s based on tensor networks, you don’t need the literally largest supercomputer on the planet filling up tens of petabytes of hard disk space with amplitudes, as in the brute-force strategy <a href="https://arxiv.org/abs/1910.09534">proposed by IBM</a>.  Pan and Zhang’s strategy seems most similar to the strategy previously <a href="https://arxiv.org/pdf/2005.06787.pdf">proposed by Alibaba</a>, with the key difference being that the new approach generates millions of correlated samples rather than just one.</p>



<p>I guess my main thoughts for now are:</p>



<ol><li>Once you knew about this particular attack, you could evade it and get back to where we were before by switching to a more sophisticated verification test — namely, one where you not only computed a Linear XEB score for the observed samples, you <em>also</em> made sure that the samples didn’t share too many bits in common.  (Strangely, though, the paper never mentions this point.)</li><li>The other response, of course, would just be to redo random circuit sampling with a slightly bigger quantum computer, like the ~70-qubit devices that Google, IBM, and others are now building!</li></ol>



<p>Anyway, very happy for thoughts from anyone who knows more.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5371"><span class="datestr">at March 07, 2021 07:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18263">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/07/advancing-and-counting/">Advancing and Counting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Announcing tomorrow’s Women in Data Science workshop (global start tonight 8pm ET), plus a US State Department event for International Women’s Day (also March 8)</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/dr.png"><img width="145" alt="" src="https://rjlipton.files.wordpress.com/2021/03/dr.png?w=145&amp;h=175" class="alignright wp-image-18265" height="175" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Santa Fe Inst. external faculty <a href="https://www.scs.gatech.edu/news/610412/dana-randall-named-external-faculty-santa-fe-institute">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Dana Randall is an ADVANCE Professor of Computing and also is an Adjunct Professor of Mathematics at the Georgia Institute of Technology. She is a terrific <a href="https://www.youtube.com/watch?v=MhYpfBUjQFQ">speaker</a> and <a href="https://www.ratemyprofessors.com/ShowRatings.jsp?tid=1579140">teacher</a> and leader. Her class ratings are off the charts. See also <a href="http://www.ams.org/publicoutreach/students/mathgame/arl2009">AMS</a> for her past special talks.</p>
<p>
Today I thought we would discuss her research and its connections to complexity theory and to physics and to math in general.<br />
<span id="more-18263"></span></p>
<p>
Dana does research into the boundary between math and physics. At the highest level Dana seeks to understand random processes, especially those connected to physical systems. The difficulty, in my opinion, is that sometimes the random system is not artificial. This means that we have no control over the system, and this makes the analysis of its behavior that much harder. </p>
<p>
Another way to say this is: we often fare better when we can control the exact random process. When someone else gets to decide on what the process is, we are often in trouble. The system might behave badly, or even worse, might be hard to understand. Nature is often that way—not always thinking about making the analysis of a system easy.</p>
<p>
</p><h2> Shuffling </h2><p></p>
<p>
Let’s make this concrete. Suppose that you or Dana were presented with three methods for shuffling a deck of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> cards—when we play cards <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{52}" class="latex" />. Imagine the methods are: </p>
<ol>
<li>This method selects <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> numbers in the range <img src="https://s0.wp.com/latex.php?latex=%7B%5B1%2Cn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{[1,n]}" class="latex" /> and checks for repeats. If there are, then try again. Use the permutation to shuffle.
</li><li>This method takes a deck of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> numbers and repeatedly shuffles them.
</li><li>This method executes the following code:
</li></ol>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/qscode.png"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2021/03/qscode.png?w=550&amp;h=100" class="aligncenter wp-image-18266" height="100" /></a></p>
<p>The task of understanding these methods is before us. The first (1) is slow, even for modest size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. The chance of getting a permutation on a given trial is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bn%21%7D%7Bn%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{n!}{n^n}}" class="latex" />, which for <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n= 52}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B4.7257911+%5Ctimes+10%5E%7B-22%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.7257911 \times 10^{-22}}" class="latex" />, which equals </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0.00000000000000000000047257911.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  0.00000000000000000000047257911. " class="latex" /></p>
<p>But it does generate a fair shuffle—all possible ones are equally likely. And the proof of this is easy. The second (2) is more complicated. The final shuffle depends on the number and manner we use to shuffle the deck. The final analysis is messy.</p>
<p>
The third one (3) is due to Ronald Fisher and Frank Yates, who discovered it in 1938. It has an elegant, but nontrivial, analysis. It is both exact in that all orderings are equally likely, and it takes time linear in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. The <a href="https://en.wikipedia.org/wiki/Fisher-Yates_shuffle">history</a> of it is:</p>
<blockquote><p><b> </b> <em> The modern version of the Fisher-Yates shuffle, designed for computer use, was introduced by Richard Durstenfeld in 1964 and popularized by Donald Knuth in The Art of Computer Programming as “Algorithm P (Shuffling)” […apparently unawares; Fisher and Yates were acknowledged in later editions of Knuth’s text…]</em>
</p></blockquote>
<p>
My guess is that Dana, if given these methods, would not be interested in (1): too slow on one hand and trivial on the other. Nor interested in (2): not elegant and messy. Perhaps (3) would accord with her work: it has a nice analysis and runs in linear time. </p>
<p>
</p><h2> Advancing </h2><p></p>
<p>
As we said earlier, Dana is part of <a href="http://www.advance.gatech.edu/team/gt-advance-professors">ADVANCE</a> at Georgia Tech: </p>
<blockquote><p><b> </b> <em> Georgia Tech’s ADVANCE Program seeks to develop systemic and institutional approaches that increase the representation, full participation, and advancement of women and minorities in academic STEM careers—thus contributing to a more diverse workforce, locally and nationally. </em>
</p></blockquote>
<p>
Dana has been and is a leader in helping advance these goals. It is especially relevant since this Monday, March 8th is special. It is <i>Celebrate International Women’s Day with WiDS</i>. See <a href="https://www.widsconference.org/conference.html">this</a> for details:</p>
<blockquote><p><b> </b> <em> Join us for the 24-hour virtual WiDS Worldwide Conference. We’ll follow the sun, bringing you speakers from around the world on International Women’s Day beginning at 1:00 am GMT March 8 (5:00 pm PST March 7). </em>
</p></blockquote>
<p>
In making a collage of their speaker <a href="https://www.widsconference.org/speakers.html">page</a>, we have compressed and rearranged it somewhat. And we have added the logo for the <a href="https://www.widsconference.org/regional-events-2021.html">regional events</a> happening around the globe (some already past) and one for their sponsors.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png?w=600&amp;h=590" class="aligncenter size-large wp-image-18268" height="590" /></a></p>
<p></p><p><br />
If you could not take time to follow all the talks, you might take a few for a sample. Maybe you would figure that taking the first four speakers in alphabetical order, or the last four, would be as random a sample as any—after all, what’s in a name?  Well, if you took the last four, you would actually get three of the four <a href="https://www.widsconference.org/conference.html">keynote</a> speakers. Sometimes procedures that we hope would give “random” samples in fact give special ones. That takes us back to one more topic in Randall’s work.</p>
<p>
</p><h2> Counting </h2><p></p>
<p>
One benefit of a random process is that under good conditions it can give us an accurate small sampling of a large and complex system. We have <a href="https://rjlipton.wordpress.com/2016/08/14/a-surprise-for-big-data-analytics/">mentioned</a> dimension reduction in this context. A simpler task is just to get an approximate count of entities in the system. Sometimes one can control the system, but sometimes not.</p>
<p>
One success is represented by a <a href="https://www.sciencedirect.com/science/article/pii/S0166218X15002255">paper</a> with Sarah Miracle of the University of St. Thomas. It is about counting colorings in multigraphs <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G = (V,E)}" class="latex" /> that do not violate simple constraints on the edges <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(u,v)}" class="latex" />. Each edge has a forbidden pair <img src="https://s0.wp.com/latex.php?latex=%7B%28c%2Cc%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(c,c')}" class="latex" /> of colors, and a coloring <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi}" class="latex" /> defined on <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V}" class="latex" /> is legal provided it does not have both <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28u%29+%3D+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi(u) = c}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28v%29+%3D+c%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi(v) = c'}" class="latex" />. Multiple edges between <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> can enforce multiple such constraints. Several natural problems can be represented via this one. The paper is one where their Markov Chain methods do well.</p>
<p>
A second recent <a href="https://arxiv.org/pdf/1611.03385.pdf">paper</a> uses Markov chains to count elements of a given rank in finite partially ordered sets. The chains should be biased according to the structure of the Hasse diagram of the poset. The trick in the paper is a way to balance the bias so as to prevent states that need to be counted from have too low frequency. This enables a direct analysis of the mixing time. The notable application was the first provably efficient ways to sample uniformly from certain kinds of partitions of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> quite large. </p>
<p>
The flip side—a phase change being a kind of a flip—is represented by other work with myriad collaborators that is summarized in a wonderful <a href="http://dimacs.rutgers.edu/events/details?eID=1409">talk</a> she gave at a <a href="http://dimacs.rutgers.edu/events/details?eID=1226&amp;loc=1409#1409">workshop</a> marking the 30th anniversary of DIMACS. As with an earlier <a href="https://drops.dagstuhl.de/opus/volltexte/2017/8021/pdf/LIPIcs-DISC-2017-3.pdf">version</a> at a Schloss Dagstuhl workshop, the talk was titled, “Phase Transitions and Emergent Phenomena in Algorithms and Applications”: </p>
<blockquote><p><b> </b> <em> Markov chain Monte Carlo methods have become ubiquitous across science and engineering as a means of exploring large configuration spaces. The idea is to walk among the configurations so that even though you explore a very small part of the space, samples will be drawn from a desirable distribution. Over the last 30 years there have been tremendous advances in the design and analysis of efficient sampling algorithms for this purpose, largely building on insights from statistical physics. One of the striking discoveries has been the realization that many natural Markov chains undergo a phase transition where they change from being efficient to inefficient as some parameter of the system is varied. </em>
</p></blockquote>
<p>
Here are a <a href="https://www.youtube.com/watch?v=IYkikVLkpoU">video</a> and <a href="http://dimacs.rutgers.edu/tools/fileman/Uploads/Documents/DIMACS-30/Randall_DIMACS30.pdf">slides</a>. The first main slide is about <em>programmable active matter</em> and it interests me especially to see DNA computing included. These systems can have <em>emergent behavior</em>, and while that can ruin randomized procedures that would bank on the system staying stable, it opens other opportunities. </p>
<p>
I, Dick, have run into this type of issue before. I have been on the wrong side, with theorems that were weak because we assumed the process could not be changed. Others who followed us changed the process—got stronger results with easier proofs. Is there a name for this?</p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>
Take a look at the <a href="https://www.widsconference.org/conference.html">talks</a> for the WiDS Worldwide Conference this Monday. </p>
<p>
Ken also notes another event happening tomorrow: a 10am <a href="https://www.state.gov/2021-international-women-of-courage-award-recipients-announced/">ceremony</a> for the International Women of Courage Award. His friend the Iranian chess arbiter Shohreh Bayat is among the honorees, after a story told in her own words <a href="https://www.washingtonpost.com/opinions/i-loosened-my-hijab-at-a-chess-championship-now-im-afraid-to-return-to-iran/2020/02/17/1a670f66-5194-11ea-9e47-59804be1dcfb_story.html">here</a> in the Washington Post. Ken was working with her, on statistical assurance against cheating in the championship match, at the time. The ceremony starts at 10am ET hosted by the US Department of State, with opening remarks by Dr. Jill Biden.</p>
<p>
We must add that there is something that is hard about the type of random processes that Dana studies. We tried to explain what makes her work deep, but perhaps we did not properly explain it. </p>
<p></p><p><br />
[added global start time of workshop to subtitle]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/03/07/advancing-and-counting/"><span class="datestr">at March 07, 2021 05:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5711">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/self-concordant-analysis-for-logistic-regression/">Going beyond least-squares – II : Self-concordant analysis for logistic regression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text"><a href="https://francisbach.com/self-concordant-analysis-newton/">Last month</a>, we saw that self-concordance is a key property in optimization, to use local quadratic approximations in the sharpest possible way. In particular it was an affine-invariant quantity leading to a simple and elegant analysis of Newton method. The key assumption was a link between third and second-order derivatives, which took the following form for one-dimensional functions, $$|f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ Alas, some of the most classical smooth functions appearing in machine learning are not self-concordant with this particular link between derivatives. The main example is the logistic loss, which is widely used across machine learning.</p>



<p class="justify-text">Indeed, if we take this logistic loss function \(f(x) = \log ( 1 + \exp(-x))\), it satisfies $$ f^\prime(x) =  \frac{ -\exp(-x)}{1+\exp(-x)} =\  – \frac{1}{1+\exp(x)} = \ – \sigma(-x),$$ where \(\sigma\) is the usual <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> defined as \(\sigma(x) = \frac{1}{1+\exp(-x)}\), and which is increasing from \(0\) to \(1\). We then have \(f^{\prime\prime}(x) = \sigma(x) ( 1- \sigma(x) )\) and \(f^{\prime \prime \prime}(x) = \sigma(x) ( 1- \sigma(x) )( 1 – 2 \sigma(x) )\) leading to $$|f^{\prime\prime\prime}(x)| \leqslant f^{\prime\prime}(x).$$ See below for plots of the logistic loss (left) and its derivatives (right).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="639" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/losses_logistic.png" class="wp-image-5795" height="267" /></figure></div>



<p class="justify-text">There is thus a link between third and second-order derivatives, but <em>without the power \(3/2\)</em>. Does this difference really matter? In this post, I will show how some properties from classical self-concordance can be extended to this slightly different notion. We will then present applications to stochastic gradient descent as well as the statistical analysis of generalized linear models, and in particular logistic regression.</p>



<p class="justify-text">I will describe applications to Newton method for large-scale logistic regression [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>] in later posts (you read well: Newton method for large-scale machine learning can be useful, in particular for severely ill-conditioned problems).</p>



<h2>\(\nu\)-self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said \(\nu\)-self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and there exists \(R &gt; 0\), such that $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant R f^{\prime\prime}(x)^{\nu\: \!  /2}.$$ </p>



<p class="justify-text">Note the difference with classical self-concordance (which corresponds to \(\nu=3\) and \(R=2\)). All positive powers are possible (see [<a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">1</a>]), but we will focus primarily on \(\nu=2\), for which most of the properties below were derived in [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>].</p>



<p class="justify-text">Note that the definition above in one dimension is still “affine-invariant” if the constant \(R\) is allowed to change (that is, if it is true for \(f\), it is true for \(x \mapsto f(ax)\) for any \(a\)). However, unless \(\nu = 3\), this will not be true in higher dimension, and therefore, the analysis of Newton method will be more complicated.</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), we need the same property along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime\prime}]= \sum_{i,j,k=1}^d h_i h_j^\prime h^{\prime\prime}_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the third-order tensor (with three different arguments, as needed below) and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the symmetric second-order one, then there exists \(R\) such that $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime}]| \leqslant R \| h\| \cdot f^{\prime\prime}(x)[h^\prime,h^{\prime}],$$ where \(\| h\|\) is the standard Euclidean norm of \(h\). Note here the difference with classical self-concordance where we could consider the symmetric third-order tensor (that is, no need for \(h^\prime\) and \(h^{\prime\prime}\)), and only the Euclidean norm based on the Hessian \(f^{\prime\prime}(x)\) was used.</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are \(2\)-self-concordant, then so is their average \(\frac{1}{2} ( f+g ) \) with the same constant \(R\) (this is one key advantage over \(3\)-self-concordance). Moreover, if \(f\) is \(2\)-self-concordant with constant \(R\), then \(g(x) = f(Ax)\) is also \(2\)-self concordant, with constant \(R \| A\|_{\rm op}\).</p>



<p class="justify-text">Classical examples are all linear and quadratic functions (with constant \(R = 0\)), the exponential function and the logistic loss \(f(x) = \log(1+\exp(-x))\), both with constant \(R=1\). This extends to the “log-sum-exp” function \(f(x) = \log\big( \sum_{i=1}^d \exp(x_i)\big)\), which is \(2\)-self-concordant with constant \(R = \sqrt{2}\). More generally, as shown at the end of the post, any log-partition function of the form $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big) $$ arising from <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear models</a> with bounded features, will be \(2\)-self-concordant, with constant the diameter of the set of features. Thus, self-concordance applies to all generalized linear models with the canonical link function. This includes <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">softmax regression</a> (for multiple classses), <a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random fields</a>, and of course logistic regression which I will focus on below.</p>



<p class="justify-text"><strong>Logistic regression.</strong> The most classical example is thus logistic regression, with $$f(x) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp( – x^\top a_i b_i ) ),$$ for observations \((a_i,b_i) \in \mathbb{R}^d \times \{-1,1\}\). See an example below in \(d=2\) dimensions.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img width="660" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/video_log_reg.gif" class="wp-image-5814" height="261" />Logistic regression in two dimensions: data space with \(a_i \in \mathbb{R}^2\) represented with a different color/mark depending on the label \(b_i\) (left), parameter space (right) with level sets of the objective function \(f\) and its minimizer (purple asterisk).</figure></div>



<p class="justify-text"><strong>Properties in one dimension.</strong>  Mimicking what was done <a href="https://francisbach.com/self-concordant-analysis-newton/">last month</a>, a nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( \! \log( f^{\prime\prime}(x)) \big) \big| = \big|   f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-1} \big| \leqslant R,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ – Rx \leqslant \log( f^{\prime\prime}(x))  \, – \log(f^{\prime\prime}(0)) \leqslant Rx,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3}  f^{\prime\prime}(0) \exp(\  – R x ) \leqslant f^{\prime\prime}(x) \leqslant f^{\prime\prime}(0) \exp( R x ).$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$  f^{\prime\prime}(0) \frac{1-\exp( \ – R x )}{R} \leqslant f^\prime(x)-f^\prime(0) \leqslant f^{\prime\prime}(0) \frac{\exp( R x )\  – 1}{R},$$ and  $$ \tag{4} \!\!\!\!\!\! f^{\prime\prime}(0) \frac{\exp( \ – R x ) + Rx \ – 1}{R^2}\leqslant f(x) \ – f(0) \ – f^\prime(0) x \leqslant  f^{\prime\prime}(0) \frac{\exp( R x ) \ – Rx \ – 1}{R^2}.$$ We thus get a bound $$f(x) \ – f(0) \ – f^\prime(0) x \in f^{\prime\prime}(0) \frac{x^2}{2} \cdot [ \rho(-Rx), \rho(Rx) ],$$ with \(\displaystyle \rho(u) =\ \frac{\exp( u ) \ – u\  – 1}{u^2 / 2 } \sim 1 \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. These upper and lower Taylor expansions are illustrated below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="292" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/rho_log.png" class="wp-image-5792" height="227" /></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(\Delta \in \mathbb{R}^d\), we have upper and lower bounds for the Hessian, the gradient (not presented below) and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm $$\tag{5}\exp(\ – R\|\Delta\|) f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq \exp( R\|\Delta\|)  f^{\prime \prime}(x),$$ and $$\tag{6} \!\!\!\!\!\!\!\!\! \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(-R \|\Delta\|_2) \leqslant f(x+\Delta)\ -f(x) \ – f^\prime(x)^\top \Delta \leqslant \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(R \|\Delta\|_2).\! $$ These approximations are “second-order tight” at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact. These can be derived by considering \(g(t) = f(x+ t\Delta)\), which is \(2\)-self-concordant with constant \(R \|\Delta\|_2\), and applying the one-dimensional properties above in Eqs. (2) and (3) between \(0\) and \(1\).</p>



<p class="justify-text"><strong>Avoiding exponentially decaying constants.</strong> In this post, I will focus primarily on the use self-concordant functions in optimization (stochastic gradient descent in this post and Newton method in another post) as well as in statistics.</p>



<p class="justify-text">The main benefit of using self-concordance is to avoid exponential constants traditionally associated with the analysis of logistic regression. Indeed, for the logistic loss, the second-derivative at \(x\) is is equal to \(\sigma(x) ( 1 – \sigma(x) )\) and is equivalent to \(\exp(-|x|)\) when  \(| x| \) is large. Thus, if we are willing to only apply the logistic loss to small values of \(|x|\), let’s say less than \(M\), then the logistic loss is strongly-convex with constant greater than \(\exp(-M)\). Therefore, we can apply many results in optimization and statistics that apply to such losses. However, all of these results will be impacted by the constant \(\exp(-M)\), which is strictly positive but can be very small. With self-concordance, the analysis will get rid of these annoying constants and replace them by eigenvalues of Hessian matrices at the optimum, which are typically much larger (note that in the worst case, the exponential constants are unavoidable [<a href="http://proceedings.mlr.press/v35/hazan14a.pdf">3</a>]).</p>



<h2>Adaptivity of stochastic gradient descent</h2>



<p class="justify-text">I have not written about stochastic gradient descent for quite a while. Self-concordance gives me the occasion to talk about <em>adaptivity</em>.</p>



<p class="justify-text">It is well known that for smooth convex functions, gradient descent will converge exponentially fast if the function is also strongly-convex (essentially all eigenvalues of all Hessians being strictly positive). If the problem is ill-conditioned, then the exponential convergence rate turns into a rate of \(O(1/t)\) where \(t\) is the number of iterations. Gradient descent is <em>adaptive</em> as the exact same algorithm (with constant step-size or line-search) can be applied without the need to know the strong-convexity parameter. Moreover, if locally around the global optimum, the Hessians are better conditioned, gradient descent will also benefit from it. Therefore, gradient descent is great! What about stochastic gradient descent (SGD)?</p>



<p class="justify-text">It turns out that similar adaptivity exists for a well-defined version of SGD, and that self-concordance is one way to achieve simple non-asymptotic bounds (asymptotic bounds exist more generally [4]).</p>



<p class="justify-text"><strong>Logistic regression. </strong>We consider the logistic regression problem where we aim to minimize the expectation $$f(x) = \mathbb{E}_{a,b} \log( 1 + \exp(-b a^\top x) ) = \mathbb{E}_{a,b} g(x|a,b) ,$$ where \((a,b) \in \mathbb{R}^d \times \{-1,1\}\) is a pair of input \(a\) and output \(b\) (hopefully, the machine learning police will excuse my use of \(x\) as the parameter and not the input). We are given \(n\) independent and identically distributed observations \((a_1,b_1),\dots, (a_n,b_n)\) and we aim at finding the minimizer \(x_\ast\) of \(f\) (which is the logistic loss on unseen data), which we assume to exist. We assume that the feature norms \(\|a\|\) are almost surely bounded by \(R\).</p>



<p>Note here that we are not trying to minimize the empirical risk and by using a single pass, we obtain bounds on the generalization performance. This is one of the classical benefits of SGD.</p>



<p class="justify-text"><strong>Averaged stochastic gradient descent.</strong> We consider the stochastic gradient recursion: $$x_i = x_{i-1} – \gamma_i g^\prime(x_{i-1}|a_i,b_i),$$ for \(i=1,\dots,n\), with a single pass over the data. We also consider the average iterate \(\bar{x}_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i\). </p>



<p class="justify-text">Standard results from the stochastic gradient descent literature [<a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>, <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">6</a>] show that if \(\gamma_i = \frac{1}{R^2 \sqrt{i}}\), then, up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i)\  – f(x_\ast) \leqslant (1 + R^2 \| x_0 – x_\ast\|^2) \frac{\log n}{\sqrt{n}}.$$ If in addition, the function \(f\) is \(\mu\)-strongly-convex, then with the step-size \(\gamma_i = \frac{1}{\mu i}\), up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i) \ – f(x_\ast) \leqslant   \frac{R^2 \log n}{n\mu}.$$ The strongly convex result seems beneficial as we get a rate in \(O(( \log n) / n)\) instead of \(O(( \log n) / \sqrt{n})\), <em>but</em>, (1) it depends on \(\mu\), which can be very small in problems in high dimension \(d\), (2) it depends on this global strong-convexity constant \(\mu\), that is a lower bound on all Hessians, which is zero for logistic regression unless a projection step is used (and with exponentially small constant as explained above), and (3) the step-size has to be adapted. </p>



<p class="justify-text"><strong>Adaptivity. </strong>Wouldn’t it be great if these three problems could be solved at once? This is what I worked on a few years ago [<a href="http://">7</a>], where I showed that for constant step-size \(\gamma\) proportional to \(\frac{1}{R^2 \sqrt{n}}\) (thus dependent on the total number of gradient steps), we have. up to constants: $$ \mathbb{E} f(\bar{x}_i)\ – f(x_\ast) \leqslant (1 + R^2 \| x_0 – x_\ast\|^2) \frac{1}{\sqrt{n}},$$ <em>and</em> $$ \mathbb{E} f(\bar{x}_i)\ – f(x_\ast) \leqslant (1 + R^4 \| x_0 – x_\ast\|^4) \frac{R^2 }{\mu_\ast n},$$ where \(\mu_\ast\) is the smallest eigenvalue of the Hessian \(f^{\prime\prime}(x_\ast)\) <em>at the optimum</em>. The two bounds are always satisfied and one can be bigger than the other depending on \(n\) and the condition number \(R^2 / \mu_\ast\).</p>



<p class="justify-text">We thus get (almost) the best of all worlds! The proof relies strongly on self-concordance and applies to all generalized linear models. Note that (a) no new algorithm is proposed here, I am simply providing partial theoretical justifications why a classical algorithm works so well, (b) this is <em>only an upper-bound</em> on performance (more on this below).</p>



<h2>Generalization bounds for generalized linear models</h2>



<p class="justify-text">Beyond optimization, the use of self-concordant can make the non-asymptotic <em>statistical</em> analysis of logistic regression, and more generally all generalized linear models, sharper in the regularized unregularized setting [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], with \(\ell_1\)-norm [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], or with non-parametric kernel-based models [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>]. The first benefit is to avoid exponential constants associated with usual strong-convexity arguments of the loss (which can also be achieved with other tools, see [<a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">11</a>]). But there is another important benefit that requires some digression.</p>



<p class="justify-text"><strong>Asymptotic statistics is great…</strong> Supervised learning through empirical risk minimization is the workhorse of machine learning. It can be analyzed from different perspectives and with different tools. As shown in the great book by Aad Van der Vaart [12], asymptotics statistics is a very clean way of understanding the behavior of statistical estimators when the number of observations \(n\) goes to infinity. </p>



<p class="justify-text">Empirical risk minimization is indeed an example of M-estimation problems (estimators based on minimizing the empirical average of some loss functions), and it is known that under general conditions, the estimator has a known asymptotic mean and variance (with the traditional <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrices</a>), which leads to an asymptotic equivalent of the unseen population risk (e.g., the “test error”). We recover the usual \(d/n\) bound for unregularized problems as well as dimension-independent results when using regularization with squared Euclidean norms (then with worse dependence in \(n\)).</p>



<p class="justify-text">Because we deal with <em>limits</em>, one can formally compare two methods by favoring the one with the smallest asymptotic risk. This is not possible when non-asymptotic <em>upper bounds</em> are available: the fact that they are true for all \(n\) is a strong benefit, but since they are only bounds, they don’t say anything about which method is best.</p>



<p class="justify-text"><strong>… But it is only asymptotic.</strong> Of course, these comparisons are only true in the limit of large \(n\), and, in particular for high-dimensional problems (e.g., data and/or parameters in large dimensions), we are unlikely to be in the asymptotic regime. So we cannot really rely only on letting \(n\) go to infinity. Moreover, these asymptotic limits typically depend on some information which is not available at training time. But does this mean that we have to throw away all asymptotic results?</p>



<p class="justify-text"><strong>Self-concordance to the rescue.</strong> Since many of the asymptotic results are obtained by second-order Taylor expansions, we need non-asymptotic ways of dealing with these expansions, which is exactly what self-concordance allows you to do (with some extra effort of course). Therefore the best of both worlds can be achieved with such tools; see, e.g., [<a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], for examples of analysis, and [<a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">13</a>, <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">14</a>] for other tools that can achieve similar results. It is then possible to prove that some asymptotic expansions are valid non-asymptotically.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post I focused on two aspects of self-concordant analysis for logistic regression and its extensions, namely adaptivity of stochastic gradient descent and statistical generalization bounds.</p>



<p class="justify-text">In a later post, I will go back to Newton’s method, where the lack of affine invariance of \(2\)-self-concordance makes the analysis more complicated. However it will come with some interesting benefits for large-scale severely ill-conditioned problems [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>]. You may wonder why we should bother with Newton method for large-scale machine learning when stochastic gradient descent, with or without variance reduction, seems largely enough. Stay tuned!</p>



<h2>References</h2>



<p class="justify-text">[1] Tianxiao Sun, and Quoc Tran-Dinh. <a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">Generalized self-concordant functions: a recipe for Newton-type methods</a>. <em>Mathematical Programming</em> 178(1): 145-213, 2019.<br />[2] Francis Bach. <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010.<br />[3] Elad Hazan, Tomer Koren, and Kfir Y. Levy. <a href="http://proceedings.mlr.press/v35/hazan14a.pdf">Logistic regression: Tight bounds for stochastic and online optimization</a>. Proceedings of the International Conference on Learning Theory (COLT), 2014.<br />[4] Boris T. Polyak, and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.<br />[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. SIAM Journal on optimization, 19(4), 1574-1609, 2009.<br />[6] Francis Bach, and Eric Moulines. <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. Advances in Neural Information Processing Systems (NIPS), 2011.<br />[7] Francis Bach. <a href="http://jmlr.org/papers/volume15/bach14a/bach14a.pdf">Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression</a>. Journal of Machine Learning Research, 15(Feb):595−627, 2014.<br />[8] Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi. <a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">Globally convergent Newton methods for ill-conditioned generalized self-concordant Losses</a>. Advances in Neural Information Processing Systems (NeurIPS), 2019.<br />[9] Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, Alessandro Rudi. <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance</a>. Proceedings of the International Conference on Learning Theory (COLT), 2019<br />[10] Dmitrii Ostrovskii, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">Francis Bach. Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br />[11] Sham Kakade, Ohad Shamir, Karthik Sridharan, and Ambuj Tewari. <a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">Learning exponential families in high-dimensions: Strong convexity and sparsity</a>. In Proceedings of the international conference on artificial intelligence and statistics (AISTATS), 2010.<br />[12] Aad W. Van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.<br />[13] Vladimir Spokoiny. <a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">Parametric estimation. Finite sample theory</a>. The Annals of Statistics, 40(6), 2877-2909, 2012.<br />[14] Tomer Koren, and Kfir Y. Levy. <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">Fast Rates for Exp-concave Empirical Risk Minimization</a>. Advances in Neural Information Processing Systems (NIPS), 2015.<br /></p>



<h2>Self-concordance for generalized linear models</h2>



<p class="justify-text">We consider a probability distribution on some set \(\mathcal{A}\), with density $$\exp\big( \varphi(a)^\top x) \ – f(x) \big)$$ with respect to the positive measure \(d\mu\), with \(f(x)\) the log-partition function, defined so that the total mass is one, that is, $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big). $$ We assume the feature vector \(\varphi(a)\) and the parameter \(x\) are in \(\mathbb{R}^d\).</p>



<p class="justify-text">The theory of <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential families</a> tells us that the function \(f(x)\) is the “cumulant generating” function. That is, the cumulants of \(\varphi(a)\) for the probability distribution defined by \(x\), are exactly the derivatives of \(f\) taken at \(x\). More precisely, for the usual mean and covariance matrix, we get $$ \mathbb{E}_{a|x} \varphi(a) = f^\prime (x),$$ $$ \mathbb{E}_{a|x} \big(\varphi(a) \ –  f^\prime (x)\big) \otimes \big(\varphi(a) \ – f^\prime (x)\big) = f^{\prime\prime}(x).$$ For the third order cumulant, we get: $$ \mathbb{E}_{a|x} \big(\varphi(a)\  – f^\prime (x)\big)\otimes \big(\varphi(a)\  – f^\prime (x)\big) \otimes \big(\varphi(a) \ – f^\prime (x)\big) = f^{\prime\prime\prime}(x).$$ Thus, for any \(h \in \mathbb{R}^d\), \(\big(\varphi(a) \ – f^\prime (x)\big)^\top h \leqslant \| h\| D\), where \(D\) is the diameter of the set \(\{ \varphi(a), \ a \in \mathcal{A} \}\), which leads to the desired \(2\)-self-concordance property.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/self-concordant-analysis-for-logistic-regression/"><span class="datestr">at March 07, 2021 04:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=848">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/03/07/questions-on-the-future-of-lower-bounds/">Questions on the future of lower bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Will any of the yellow books be useful?</p>



<figure class="wp-block-image"><img src="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg" alt="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg" /></figure>



<p>Book 576 (not pictured) was saved just in time from the paper mill.  It was rumored that Lemma 76.7.(ii) could have applications to lower bounds.  Upon closer inspection, that lemma has a one-line proof by linearity of expectation if you change the constant 17 to 19.  This change does not affect the big-Oh.</p>



<p>Will the study of randomness lead to the answer to any of the questions that are open since before randomness became popular? I think it’s a coin-toss.</p>



<p>Will there be any substance to the belief that algebraic lower bounds must be proved <em>first</em>?</p>



<p>Will the people who were mocked for working on DLOGTIME uniformity, top fan-in k circuits, or ZFC independence have the last laugh?</p>



<p>Will someone switch the circuit breaker and lit up CRYPT, DERAND, and PCPOT, or will they remain unplugged amusement parks where you sit in the roller coaster, buckle up, and pretend?</p>



<p>Will diagonalization be forgotten, or will it continue to frustrate combinatorialists with lower bounds they can’t match for functions they don’t care about?</p>



<p>Will decisive progress be made tonight, or will it take centuries?</p>



<p>Only Ketan Mulmuley knows for sure.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/03/07/questions-on-the-future-of-lower-bounds/"><span class="datestr">at March 07, 2021 11:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1485">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1485">News for February 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random <em>graphlets</em>. </p>



<p><strong>Testing Hamiltonicity (and other problems) in Minor-Free Graphs</strong>, by Reut Levi and Nadav Shoshan (<a href="https://arxiv.org/abs/2102.11728">arXiv</a>). Graph Property Testing has been explored pretty well for dense graphs (and reasonably well for bounded degree graphs). However, testing properties in the general case still remains an elusive goal. This paper makes contributions in this direction and as a first result it gives an algorithm for testing Hamiltonicity <em>in minor free graphs</em> (with two sided error) with running time \(poly(1/\varepsilon)\). Let me begin by pointing out that Hamiltonicity is an irksome property to test in the following senses.</p>



<ul><li>It is neither monotone nor additive. So the partition oracle based algorithms do not immediately imply a tester (with running time depending only on \(\varepsilon\) for Hamiltonicity. This annoyance bugs you even in the bounded degree case.</li><li> Czumaj and Sohler characterized what graph properties are testable with one-sided error in general planar graphs. In particular, they show a property of general planar graphs is testable <em>iff</em> this property can be reduced to testing for a finite family of finite forbidden subgraphs. Again, Hamiltonicity does not budge to this result. </li><li>There are (concurrent) results by Goldreich and Adler-Kohler which show that with one-sided error, Hamiltonicity cannot be tested with \(o(n)\) queries. </li></ul>



<p>The paper shows that distance to Hamiltonicity can be exactly captured in terms of a certain combinatorial parameter. Thereafter, the paper tries to estimate this parameter after cleaning up the graph a little. This allows them to estimate the distance to Hamiltonicity and thus also implies a tolerant tester (restricted to mino-free graphs).</p>



<p><strong>Testing properties of signed graphs</strong>, by Florian Adriaens, Simon Apers (<a href="https://arxiv.org/abs/2102.07587">arXiv</a>). Suppose I give you a graph \(G=(V,E)\) where all edges come with a label: which is either “positive” or “negative”. Such signed graphs are used to model various scientific phenomena. Eg, you can use these to model interactions between individuals in social networks into two categories like friendly or antagonistic.</p>



<p>This paper considers property testing problems on signed graphs. The notion of farness from the property extends naturally to these graphs (both in the dense graph model and the bounded degree model). The paper contains explores three problems in both of these models: signed triangle freeness, balance and clusterability. Below I will zoom into the tester for clusterability in the bounded degree setting developed In the paper. A signed graph is considered clusterable if you can partition the vertex set into some number of components such that the edges within any component are all positive and the edges running across components are all negative.</p>



<p>The paper exploits a forbidden subgraph characterization of clusterability which shows that any cycle with exactly one negative edge is a certificate of non-clusterability of \(G\). The tester runs multiple random walks from a handful of start vertices to search for these “bad cycles” by building up on ideas in the seminal work of Goldreich and Ron for testing bipariteness. The authors put all of these ideas together and give a \(\widetilde{O}(\sqrt n)\) time one-sided tester for clusterability in signed graphs.</p>



<p></p>



<p><strong>Local Access to Random Walks</strong>, by Amartya Shankha Biswas, Edward Pyne, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2102.07740">arXiv</a>). Suppose I give you a gigantic graph (with bounded degree) which does not fit in your main memory and I want you to solve some computational problem which requires you to solve longish random walks of length \(t\). And lots of them. It would be convenient to not spend \(\Omega(t)\) units of time performing every single walk. Perhaps it would work just as well for you to have an oracle which provides query access to a \(Position(G,s,t)\) oracle which returns the position of a walk from \(s\) at time \(t\) of your choice. Of course, you would want the sequence of vertices returned to behave consistently with some actual random walk sampled from the distribution of random walks starting at \(s\). Question is: Can I build you this primitive? This paper answers this question in affirmative  and shows that for graphs with spectral gap \(\Delta\), this can be achieved with running time \(\widetilde{O}(\sqrt n/\Delta)\) per query. And you get the guarantee that the joint distribution of the vertices you return at queried times is \(1/poly(n)\) close to the uniform distribution over such walks in \(\ell_1\).  Thus, for a random \(d\)-regular graph, you get running times of the order \(\widetilde{O}(\sqrt n)\) per query. The authors also show tightness of this result by showing to get subconstant error in \(\ell_1\), you necessarily need \(\Omega(\sqrt n/\log n)\) queries in expectation.</p>



<p></p>



<p><strong>Efficient and near-optimal algorithms for sampling connected subgraphs</strong>, by Marco Bressan (<a href="https://arxiv.org/abs/2007.12102">arXiv</a>). As the title suggests, this paper considers efficient algorithms for sampling a uniformly random \(k\)-graphlet from a given graph \(G\) (for \(k \geq 3\)). Recall, a \(k\)-graphlet refers to a collection of \(k\)-vertices which induce a connected graph in \(G\). The algorithm considered in the paper is pretty simple. You just define a Markov Chain \(\mathcal{G}_k\) with all \(k\)-graphlets as its state space. Two states in \(\mathcal{G}_k\) are adjacent <em>iff</em> their intersection is a \((k-1)\)-graphlet. To obtain a uniformly random sample, a classical idea is to just run this Markov Chain and obtain an \(\varepsilon\)-uniform sample. However, the gap between upper and lower bounds on the mixing time of this walk is of the order \(\rho^{k-1}\) where \(\rho = \Delta/\delta\) (that is the ratio of maximum and minimum degrees to the power \(k-1\)). The paper closes this gap up to logarithmic factors and shows that the mixing time of the walk is at most \(t_{mix}(G) \rho^{k-1} \log(n/\varepsilon)\). It also proves an almost matching lower bound. Further, the paper also presents an algorithm with event better running time to return an almost uniform \(k\)-graphlet. This exploits a previous observation: sampling a uniformly random \(k\)-graphlet is equivalent to sampling a uniformly random edge in \(\mathcal{G}_{k-1}\). The paper then proves a lemma which upperbounds the relaxation time of walks in \(\mathcal{G}_k\) to walks in \(\mathcal{G}_{k-1}\). And then you upperbound the mixing time in terms of the relaxation time to get an improved expected running time of the order \(O(t_{mix}(G) \cdot \rho^{k-2} \cdot \log(n/\varepsilon)\).</p>



<p></p>



<p><strong>Toward Instance-Optimal State Certification With Incoherent Measurements</strong>, by Sitan Chen, Jerry Li, Ryan O’Donnell (<a href="https://arxiv.org/abs/2102.13098">arXiv</a>). The problem of quantum state certification has gathered interest over the last few years. Here is the setup: you are given a quantum state \(\sigma \in \mathbb{C}^{d \times d}\) and you are also given \(N\) copies of an unknown state \(\rho\). You want to distinguish between the following two cases: Does \(\rho = \sigma\) or is \(\sigma\) at least \(\varepsilon\)-far from \(\rho\) in trace norm? Badescu et al showed in a recent work that if entangled measurements are allowed, you can do this with a mere \(O(d/\varepsilon^2)\) copies of \(\rho\). But using entangled states comes with its own share of problems. On the other hand if you disallow entanglement, as Bubeck et al show, you need \(\Omega(d^{3/2}/\varepsilon^2)\) measurements. This paper asks: for which states \(\sigma\) can you improve upon this bound. The work takes inspirations from <em>a la</em> “instance optimal” bounds for identity testing. Authors show a fairly general result which (yet again) confirms that the quantum world is indeed weird. In particular, the main result of the paper implies that the copy complexity of (the quantum analog of) identity testing in the quantum world (with non-adaptive queries) grows as \(\Theta(d^{1/2}/\varepsilon^2)\). That is, the number of quantum measurements you need increases with \(d\) (which is the stark opposite of the behavior you get in the classical world).</p></div>







<p class="date">
by Akash <a href="https://ptreview.sublinear.info/?p=1485"><span class="datestr">at March 06, 2021 05:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/032">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/032">TR21-032 |  Fiat-Shamir via List-Recoverable Codes (or: Parallel Repetition of GMW is not Zero-Knowledge) | 

	Ron Rothblum, 

	Justin Holmgren, 

	Alex Lombardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Shortly after the introduction of zero-knowledge proofs, Goldreich, Micali and Wigderson (CRYPTO '86) demonstrated their wide applicability by constructing  zero-knowledge proofs for the NP-complete problem of graph 3-coloring. A long-standing open question has been whether parallel repetition of their protocol preserves zero knowledge. In this work, we answer this question in the negative, assuming a a standard cryptographic assumption (i.e., the hardness of learning with errors (LWE)).

Leveraging a connection observed by Dwork, Naor, Reingold, and Stockmeyer (FOCS '99), our negative result is obtained by making positive progress on a related fundamental problem in cryptography: securely instantiating the Fiat-Shamir heuristic for eliminating interaction in public-coin interactive protocols. A recent line of works has shown how to instantiate the heuristic securely, albeit only for a limited class of protocols.

Our main result shows how to instantiate Fiat-Shamir for parallel repetitions of much more general interactive proofs. In particular, we construct hash functions that, assuming LWE, securely realize the Fiat-Shamir transform for the following rich classes of protocols:

- The parallel repetition of any ``commit-and-open'' protocol (such as the GMW protocol mentioned above), when a specific (natural) commitment scheme is used.  Commit-and-open protocols are a ubiquitous paradigm for constructing general purpose public-coin zero knowledge proofs.

- The parallel repetition of any base protocol that (1) satisfies a stronger notion of soundness called round-by-round soundness, and (2) has an efficient procedure, using a suitable trapdoor, for recognizing ``bad verifier randomness'' that would allow the prover to cheat.

Our results are obtained by establishing a new connection between the Fiat-Shamir transform and  list-recoverable codes.  In contrast to the usual focus in coding theory, we focus on a parameter regime in which the input lists are extremely large, but the rate can be small.  We give a (probabilistic) construction based on Parvaresh-Vardy codes (FOCS '05) that suffices for our applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/032"><span class="datestr">at March 05, 2021 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/031">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/031">TR21-031 |  Upper Bound for Torus Polynomials | 

	Vaibhav Krishan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that all functions that have low degree torus polynomials approximating them with small error also have $MidBit^+$ circuits computing them. This serves as a partial converse to the result that all $ACC$ functions have low degree torus polynomials approximating them with small error, by Bhrushundi, Hosseini, Lovett and Rao (ITCS 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/031"><span class="datestr">at March 05, 2021 02:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/">WSJ Meets Group Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Our whole life is solving puzzles. — Ernő Rubik</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jf-1.png"><img width="163" alt="" src="https://rjlipton.files.wordpress.com/2021/03/jf-1.png?w=163&amp;h=120" class="alignright wp-image-18244" height="120" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="http://www.ws.binghamton.edu/fridrich/pressconnects_com%20%2009-11-03%20%20News%20Story.htm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a>. She has over 34,000 citations—impressive. A lot more than most of us. She also has <a href="http://www.ws.binghamton.edu/fridrich/cube.html">worked</a> on the famous Rubik’s cube.</p>
<p>
Today we look at her work on Rubik’s cube, the WSJ’s interest in Rubik’s cube, and what both say—and don’t say—about fundamental algorithms.<br />
<span id="more-18238"></span></p>
<p>
By the way, WSJ stands for the Wall Street Journal—the American <a href="https://en.wikipedia.org/wiki/The_Wall_Street_Journal">newspaper</a> of business. The WSJ has shown great interest in the Rubik’s cube puzzle and has run many articles over the years on it.</p>
<p>
Recall the cube puzzle was invented…of course you know all about Rubik’s cube. You probably have owned one at one time. Right. Just for a <a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube">refresher</a>: </p>
<blockquote><p><b> </b> <em> The Rubik’s Cube is a 3-D combination puzzle invented in 1974 by Hungarian sculptor and professor of architecture Ernő Rubik. As of January 2009, 350 million cubes had been sold worldwide, making it the world’s top-selling puzzle game. It is widely considered to be the world’s best-selling toy. </em>
</p></blockquote>
<p>
But you may not know all about Fridrich.</p>
<p></p><h2> Speed Solving </h2><p></p>
<p>
Fridrich was one of the progenitors of <em>speed cubing</em>. She took part in the First World Championship in 1982 in Budapest, next-door to her native Czechoslovakia. She finished in the middle of the pack with a time of <b>29.11</b> seconds from a randomly well-mixed starting cube position. Her thoughts on how the cubes could be better prepared for speed are recorded on her <a href="http://www.ws.binghamton.edu/fridrich/cubewrld.html">page</a> about the tournament.</p>
<p>
At the Second World Championship, she improved her average time to <b>20.48</b> seconds and placed <a href="https://www.worldcubeassociation.org/competitions/WC2003">2nd</a>. She had the two fastest solves in the finals but lost on average-of-median-three-of-five.  That championship took place in Toronto—in <b>2003</b>. She is at a loss to explain why there was such a gap. Usually an athlete—in this case a mathlete?—is on the downswing nearing age 40, but even as a self-described “<a href="http://ws2.binghamton.edu/fridrich/history.html">old-timer</a>,” she fended off all but one of a whole next generation. </p>
<p>
Much of the credit goes to her solving method. She originated the “O” and “P” parts of the <a href="https://en.wikipedia.org/wiki/CFOP_method">CFOP</a> method. CFOP stands for: Cross, First 2 Layers, Orient Last Layer, Permute Last Layer. Versions of this are used my most top “cubers” to this day, and her name is often affixed to the method. In a 2008 profile of her, the New York Times <a href="https://www.nytimes.com/2008/12/16/science/16prof.html?_r=1&amp;em">quoted</a> the 2003 winner as saying that Fridrich found the route up the mountain while the rest of the cubers optimize traversing ledges along it. And in 2012, the NYT <a href="https://london2012.blogs.nytimes.com/2012/06/25/master-of-the-shot-put-and-the-cube/?searchResultPosition=5">quoted</a> Olympic shot-putter Reese Hoffa as wanting “to learn the Fridrich Method of solving the puzzle, ‘which is what all of the best cubers use.'” </p>
<p>
At this point, knowing our interest in chess, you might expect a <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(novel)"><i>Queen’s</i></a> <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(miniseries)"><i>Gambit</i></a> reference. But what we have here is not a story of Beth Harmon coming back from a life <a href="https://www.thereviewgeek.com/thequeensgambit-e6review/">adjournment</a> or Roy Hobbs in <a href="https://en.wikipedia.org/wiki/The_Natural"><i>The</i></a> <a href="https://en.wikipedia.org/wiki/The_Natural_(film)"><i>Natural</i></a> rejoining baseball almost 20 years after being shot. It’s about going overseas, earning a PhD, getting two research positions, writing early papers (under the <a href="https://dblp.org/pid/29/4038.html">name</a> Jiri Fridrich), transitioning, then getting a faculty position leading to tenure while developing mathematical formulas and writing tons of code for systems to <a href="https://www.nytimes.com/2004/07/22/technology/what-s-next-for-doctored-photos-a-new-flavor-of-digital-truth-serum.html?searchResultPosition=16">source</a> photos and catch digital pirates and pornographers and other image fraudsters, then coming back to light up an <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{8 \times 8}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3 \times 3}" class="latex" /> universe. Not to mention doing her own stunning <a href="https://www.jessicafridrich.com/">photo art</a> of the American Southwest.</p>
<p></p><h2> Quicker Times and Cubes </h2><p></p>
<p>
Since 2003, the <a href="https://en.wikipedia.org/wiki/Speedcubing#Competitions">championships</a> have been held every other year, thought the 2021 championships set for the Netherlands are uncertain owing to the pandemic. The youngsters soon broke through en-masse, and it strikes me that the cube technology improved so that the cubes are springier and lighter. The winning time fell almost 5 seconds to <b>15.10</b> in 2005 and hit <b>6.74</b> in 2019. That was not the world record, however—an incredible <b>3.47</b> seconds in 2018 by Yusheng Du, beating the previous record of Feliks Zemdegs by a whopping 3/4 of a second.</p>
<p>
Fridrich, however, must claim a distinction no one may ever match. She learned how to solve the cube and traced out the performance of methods of doing so in 1981, months before she saw a cube, let alone owned one. Despite the “Bűvös Kocka” (“Magic Cube,” as Rubik called it) having been on shelves in neighboring Hungary for four years, with worldwide marketing by early 1980, they were hard to come by in her home city, Ostrava. </p>
<p>
She found an article on solving the cube in a Russian magazine. It laid out the concept of group theory and the role of group commutators, which she learned to apply creatively in order to streamline actions. The first time she touched a cube was to help a friend put his back the way it was. A family visiting from France let her keep one, and later in 1981 she was finally able to purchase a few more. This invites analogy to working out chess without a board on a bedroom ceiling as depicted in <em>The Queen’s Gambit</em>.</p>
<p>
We—Dick and Ken—must admit that neither of us has ever done this with the cubes we own, not fast, not slow. Yet we do understand the theory behind it. We believe we do. </p>
<p></p><h2> Group Theory of the Cube </h2><p></p>
<p>
I (Dick) plan on explaining the theory by using a new toy that I have invented: The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Bslider%7D%5E%7BTM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathit{slider}^{TM}}" class="latex" />. 	</p>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png"><img width="96" alt="" src="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png?w=96&amp;h=45" class="aligncenter wp-image-18247" height="45" /></a></p>
<p>
We will write the state as <img src="https://s0.wp.com/latex.php?latex=%7Bxyz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{xyz}" class="latex" /> where each of <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x,y,z}" class="latex" /> is one of <font color="red">1</font>, <font color="green">2</font>, or <font color="blue">3</font>. The operations allowed are the <i>cyclic shift</i> <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" />, which does 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+zxy%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow zxy, " class="latex" /></p>
<p>and the <i>flip</i> <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F}" class="latex" /> of the initial two elements: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+yxz.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow yxz. " class="latex" /></p>
<p>
Note there are 6 possible states. For the real Rubik’s cube, the number of states is just a little bit larger: <b>43,252,003,274,489,856,000</b>. But the basic concept is the same. Suppose we are given the state 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132. " class="latex" /></p>
<p>How fast can you get the initial state <img src="https://s0.wp.com/latex.php?latex=%7B123%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{123}" class="latex" />? Apply <img src="https://s0.wp.com/latex.php?latex=%7BFCC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{FCC}" class="latex" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132+%5Crightarrow+312+%5Crightarrow+231+%5Crightarrow+123+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132 \rightarrow 312 \rightarrow 231 \rightarrow 123 . " class="latex" /></p>
<p>
This is a special case of the general <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/genset.pdf">result</a> that any symmetric group is <a href="https://groupprops.subwiki.org/wiki/Symmetric_group_on_a_finite_set_is_2-generated">generated</a> by two operations: a full cycle and a single flip. The key with the actual Rubik’s cube is since the group is larger and it has more operations that can be applied finding the group operations may be more difficult. But there are algorithms that can find them. See <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/gpaction.pdf">this</a> for another article by Keith Conrad. </p>
<p>
There are many more pages like that on the cube. But Fridrich still shows the <a href="http://www.ws.binghamton.edu/fridrich/system.html">seminal page</a> she posted in “Winter 1996/97.” It links to other pages, ones that also credit other people, such as <a href="http://www.ws.binghamton.edu/fridrich/Mike/middle.html">this</a> explaining the algorithms in great pictorial detail. This was in the infancy of the Internet. Her pages are often credited with spurring the turn-of-the-millennium boom in Rubik’s cube which led to the revival of the championships in 2003. A 2016 New York Post <a href="https://nypost.com/2016/10/31/how-the-internet-brought-the-rubiks-cube-back-to-life/">article</a> whose URL is titled, “how the Internet brought the Rubik’s cube back to life,” says: </p>
<blockquote><p><b> </b> <em> The seeds for Rubik’s Cube’s rediscovery were sown on the internet. In the mid-1990s, a Rubik’s Cube champion-turned-computer-science professor at SUNY Binghamton posted her secrets of the Cube on a primitive Web 1.0 site on the university’s servers. Jessica Fridrich’s method spread and is today the most widely used technique to solve the puzzle. </em>
</p></blockquote>
<p>
See also this <a href="https://uncletyson.wordpress.com/tag/dan-knights/">telling</a> by the 2003 winner, Dan Knights. This shows how one person using spare time on the Internet can power up business.</p>
<p></p><h2> Enter the WSJ </h2><p></p>
<p>
The WSJ has had an interest in Rubik’s cube for years. They had a long feature <a href="https://www.wsj.com/articles/how-to-teach-professors-humility-hand-them-a-rubiks-cube-11614352261">article</a> last week titled, “How to Teach Professors Humility? Hand Them a Rubik’s Cube,” by Melissa Korn. It describes a faculty development challenge among several small colleges in which professors became students again. Last month they also had an <a href="https://www.wsj.com/articles/seeing-things-with-the-power-of-symmetry-11612461325">article</a> on symmetry by the mathematician Eugenia Cheng that mentioned the cube.</p>
<p>
I recall several features the WSJ has run on the cube and its solvers. The 2011 <a href="https://www.wsj.com/articles/SB10001424052970204319004577088513615125328">article</a>, “One Cube, Many Knockoffs, Quintillions of Possibilities,” led off with the Polish teenager Michal Pleskowicz winning the 2011 world championship with a time of <b>8.65</b> seconds, then discussed the performance of pirated cubes: “One reason Mr. Pleskowicz and a new generation of Rubik’s fanatics can solve the notoriously difficult puzzle in record time: They don’t use Rubik’s Cubes at all, instead substituting souped-up Chinese knockoffs engineered for speed…” Their 2014 <a href="https://www.wsj.com/articles/SB10001424052702304518704579523513594900696">article</a>, “Rubik’s Cube Proves It’s Hip to Be Square,” profiled both Rubik and speed-solvers. </p>
<p>
The <a href="https://www.wsj.com/articles/a-thinking-persons-guide-to-the-rubiks-cube-1517586702">feature</a> I recall best was in 2018. It was titled, “A Thinking Person’s Guide to the Rubik’s Cube,” and subtitled, “What’s the best solution method—theory, algorithms or chance?” It was also by Eugenia Cheng. She begins by confessing, “I have always loved playing with a Rubik’s Cube, which combines logic with a satisfying tactile activity. I can solve it—getting each of the six sides to be one color—but not particularly quickly or cleverly.” </p>
<p>
They also like its use for analogies. Scrolling through their advanced search—both Ken and I subscribe to the WSJ—we find:</p>
<ul>
<li><a href="https://www.wsj.com/articles/close-reopen-repeat-restaurants-dont-know-what-covid-19-will-dish-out-next-11613138412">2/12/21</a>: “Running restaurants is now ‘a bit of a Rubik’s Cube,’ said Mr. Mosier, who reopened his casual cafes in late January.”
</li><li><a href="https://www.wsj.com/articles/reopening-schools-is-so-complicated-new-york-struggles-to-schedule-classes-11597939473">8/20/20</a>, headline: “Reopening Schools Is So Complicated, New York Is Struggling to Schedule Classes Nation’s largest district is still hashing out basic details about the school day; ‘a multidimensional Rubik’s Cube’ of challenges.”
</li><li><a href="https://www.wsj.com/articles/new-u-s-rules-on-foreign-students-put-universitiesin-dilemma-11594149280">7/7/20</a>: “The new [pandemic] rules have created a Rubik’s Cube of decisions for schools, which face unique challenges with each of their international student populations.”
</li><li><a href="https://www.wsj.com/articles/an-l-a-home-asking-62-million-includes-a-playful-perk-a-model-racetrack-11590523214">5/26/20</a>, about a home selling for $62 million: “Designed by Seattle-based architecture firm Olson Kundig, the house has interlocking boxes and planes resembling a Rubik’s cube…”
</li><li><a href="https://www.wsj.com/articles/president-trump-announces-19-billion-relief-program-for-farmers-11587165759">4/17/20</a>, quoting Agriculture Secretary Sonny Perdue on the coronavirus relief program for farmers: “It will be a logistical Rubik’s Cube.”
</li><li><a href="https://www.wsj.com/articles/he-wanted-something-more-from-retirement-so-he-got-three-jobs-11573743922">11/14/19</a>, about a retiree who started teaching business classes, keeping books for a non-profit business, and working on a ferry dock: “My society consists of able-bodied seamen, boat captains, truckers hauling bait and lobsters, fishermen, islanders and wide-eyed vacationers,” says Mr. Marshall. It’s “a constant Rubik’s cube. You never know what you’ll find.”
</li></ul>
<p>
In all, using the WSJ advanced search, we find 239 hits for “Rubik” going back to 1980. We should mention in-passing that one of them is their 7/17/20 <a href="https://www.wsj.com/articles/ron-graham-dazzled-admirers-with-math-and-juggling-feats-11594994403">obituary</a> for Ron Graham. We also find 7 hits for “Fridrich” over the same span. But they are all about the housing market, involving the Nashville-based realty Fridrich and Clarke.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
I am happy to see that the WSJ has published multiple articles on a particular algorithmic task. I like that algorithms have been the center of articles. I wish they would talk more about important algorithms. Solving a Rubik’s cube is not an algorithm that is used every day: What about: </p>
<ul>
<li>Sorting
</li><li>Searching
</li><li>Dynamic Programming
</li><li>Fast Arithmetic
</li></ul>
<p>They do have Eugenia Cheng, who wrote a <a href="https://www.wsj.com/articles/algorithms-arent-just-for-computers-11557407055">column</a> comparing sorting algorithms. And they have written on algorithms used in <a href="https://www.wsj.com/graphics/journey-inside-a-real-life-trading-algorithm/">trading</a> and on <a href="https://www.wsj.com/articles/social-media-algorithms-rule-how-we-see-the-world-good-luck-trying-to-stop-them-11610884800">social</a>–<a href="https://www.wsj.com/articles/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753">media</a> <a href="https://www.wsj.com/articles/how-to-win-friends-and-influence-algorithms-11555246800">platforms</a> and for <a href="https://www.wsj.com/articles/algorithms-used-in-policing-face-policy-review-11591003801">policing</a> and <a href="https://www.wsj.com/articles/SB10001424052702304626104579121251595240852">parole</a> and <a href="https://www.wsj.com/articles/algorithm-helps-new-york-decide-who-goes-free-before-trial-11600610400">bail</a> decisions. But that tends away from <em>fundamental algorithms</em> where the math is the matter.</p>
<p>
A 2018 WSJ <a href="https://www.wsj.com/articles/dont-believe-the-algorithm-1536157620">article</a> by Hannah Fry titled “Don’t Believe the Algorithm,” which begins with flaws in using facial recognition to find wanted suspects, brings us back toward Fridrich’s research. Might this all also raise discussion of “algorithms” for what and whom to cover?</p>
<p>
[fixed name at end]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/"><span class="datestr">at March 05, 2021 12:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5359">The Zen Anti-Interpretation of Quantum Mechanics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell?  It feels therapeutic, I have tenure, and anyone who doesn’t like it can close their broswer tab.</p>



<p>So: although I’ve written tens of thousands <a href="https://www.pbs.org/wgbh/nova/article/can-quantum-computing-reveal-the-true-meaning-of-quantum-mechanics/">of</a> <a href="https://arxiv.org/abs/1306.0159">words</a>, <a href="https://www.scottaaronson.com/papers/philos.pdf">on</a> <a href="https://www.scottaaronson.com/blog/?p=1103">this</a> <a href="https://www.scottaaronson.com/blog/?p=3628">blog</a> <a href="https://www.scottaaronson.com/democritus/">and</a> <a href="https://www.scottaaronson.com/qclec.pdf">elsewhere</a>, about interpretations of quantum mechanics, again and again I’ve dodged the question of which interpretation (if any) I <em>really believe myself</em>.  Today, at last, I’ll emerge from the shadows and tell you precisely where I stand.</p>



<p>I hold that all interpretations of QM are just crutches that are better or worse at helping you along to the Zen realization that <strong>QM is what it is and doesn’t need an interpretation</strong>.  As Sidney Coleman <a href="https://arxiv.org/abs/2011.12671">famously argued</a>, what needs reinterpretation is not QM itself, but all our <em>pre</em>-quantum philosophical baggage—the baggage that leads us to demand, for example, that a wavefunction |ψ⟩ either be “real” like a stubbed toe or else “unreal” like a dream.  Crucially, because this philosophical baggage differs somewhat from person to person, the “best” interpretation—meaning, the one that leads most quickly to the desired Zen state—can also differ from person to person.  Meanwhile, though, thousands of physicists (and chemists, mathematicians, quantum computer scientists, etc.) have approached the Zen state merely by spending decades working with QM, never worrying much about interpretations at all.  This is probably the truest path; it’s just that most people lack the inclination, ability, or time.</p>



<p>Greg Kuperberg, one of the smartest people I know, once told me that the problem with the Many-Worlds Interpretation is not that it says anything wrong, but only that it’s “melodramatic” and “overwritten.”  Greg is far along the Zen path, probably further than me.</p>



<p>You shouldn’t confuse the Zen Anti-Interpretation with “Shut Up And Calculate.”  The latter phrase, mistakenly attributed to Feynman but really due to David Mermin, is something one might say at the <em>beginning</em> of the path, when one is as a baby.  I’m talking here only about the <em>endpoint</em> of path, which one can approach but never reach—the endpoint where you intuitively understand exactly what a Many-Worlder, Copenhagenist, or Bohmian would say about any given issue, and also how they’d respond to each other, and how they’d respond to the responses, etc. but after years of study and effort you’ve <em>returned</em> to the situation of the baby, who just sees the thing for what it is.</p>



<p>I don’t mean to say that the interpretations are all interchangeable, or equally good or bad.  If you had to, you could call even me a “Many-Worlder,” but <em>only</em> in the following limited sense: that in fifteen years of teaching quantum information, my experience has consistently been that for <em>most</em> students, <a href="https://en.wikipedia.org/wiki/Many-worlds_interpretation">Everett’s crutch</a> is the best one currently on the market.  At any rate, it’s the one that’s the most like a straightforward <em>picture</em> of the equations, and the least like a wobbly tower of words that might collapse if you utter any wrong ones.  Unlike Bohr, Everett will never make you feel stupid for asking the questions an inquisitive child would ask; he’ll simply give you answers that are as clear, logical, and internally consistent as they are metaphysically extravagant.  That’s a start.</p>



<p>The <a href="https://en.wikipedia.org/wiki/Copenhagen_interpretation">Copenhagen Interpretation</a> retains a place of honor as the <em>first</em> crutch, for decades the <em>only</em> crutch, and the one closest to the spirit of positivism.  Unfortunately, <em>wielding</em> the Copenhagen crutch requires mad philosophical skillz—which parts of the universe should you temporarily regard as “classical”?  which questions should be answered, and which deflected?—to the point where, if you’re capable of all that verbal footwork, then why do you even <em>need</em> a crutch in the first place?  In the hands of amateurs—meaning, alas, nearly everyone—Copenhagen often leads <em>away</em> <em>from</em> rather than toward the Zen state, as one sees with the generations of New-Age bastardizations about “observations creating reality.”</p>



<p>As for <a href="https://en.wikipedia.org/wiki/De_Broglie%E2%80%93Bohm_theory">deBroglie-Bohm</a>—well, that’s a weird, interesting, baroque crutch, one whose actual details (the preferred basis and the guiding equation) are historically contingent and tied to specific physical systems.  It’s probably the right crutch for <em>someone</em>—it gets eternal credit for having led Bell to discover the Bell inequality—but its quirks definitely need to be discarded along the way.</p>



<p>Note that, among those who approach the Zen state, many might still call themselves Many-Worlders or Copenhagenists or Bohmians or whatever—just as those far along in spiritual enlightenment might still call themselves Buddhists or Catholics or Muslims or Jews (or atheists or agnostics)—even though, by that point, they might have more in common with each other than they do with their supposed coreligionists or co-irreligionists.</p>



<p>Alright, but isn’t all this Zen stuff just a way to dodge the <em>actual, substantive</em> questions about QM, by cheaply claiming to have transcended them?  If that’s your charge, then please help yourself to the following FAQ about the details of the Zen Anti-Interpretation.</p>



<ol><li><strong>What is a quantum state?</strong>  It’s a unit vector of complex numbers (or if we’re talking about mixed states, then a trace-1, Hermitian, positive semidefinite matrix), which encodes everything there is to know about a physical system.<br /></li><li><strong>OK, but are the quantum states “ontic” (really out in the world), or “epistemic” (only in our heads)?</strong>  Dude.  Do “basketball games” really exist, or is that just a phrase we use to summarize our knowledge about certain large agglomerations of interacting quarks and leptons?  Do even the “quarks” and “leptons” exist, or are those just words for excitations of the more fundamental fields?  Does “jealousy” exist?  Pretty much<em> all</em> our concepts are complicated grab bags of “ontic” and “epistemic,” so it shouldn’t surprise us if quantum states are too.  Bad dichotomy.<br /></li><li><strong>Why are there probabilities in QM?</strong>  Because QM <em>is</em> a (the?) generalization of probability theory to involve complex numbers, whose squared absolute values are probabilities.  It <em>includes</em> probability as a special case.<br /></li><li><strong>But why do the probabilities obey the Born rule?</strong>  Because, once the unitary part of QM has picked out the 2-norm as being special, for the probabilities <em>also</em> to be governed by the 2-norm is pretty much the only possibility that makes mathematical sense; there are many nice theorems formalizing that intuition under reasonable assumptions.<br /></li><li><strong>What is an “observer”?</strong>  It’s exactly what modern decoherence theory says it is: a particular kind of quantum system that interacts with other quantum systems, becomes entangled with them, and thereby records information about them—reversibly in principle but irreversibly in practice.<br /></li><li><strong>Can observers be manipulated in coherent superposition, as in the <a href="https://en.wikipedia.org/wiki/Wigner%27s_friend">Wigner’s Friend</a> scenario?</strong>  If so, they’d be radically unlike any physical system we’ve ever had direct experience with.  So, are you asking whether such “observers” would be <em>conscious</em>, or if so what they’d be conscious of?  Who the hell knows?<br /></li><li><strong>Do “other” branches of the wavefunction—ones, for example, where my life took a different course—exist in the same sense this one does?</strong>  If you start with a quantum state for the early universe and then time-evolve it forward, then yes, you’ll get not only “our” branch but also a proliferation of other branches, in the overwhelming majority of which Donald Trump was never president and civilization didn’t grind to a halt because of a bat near Wuhan.  But how could we possibly know whether anything “breathes fire” into the other branches and makes them real, when we have no idea what breathes fire into <em>this</em> branch and makes <em>it</em> real?  This is not a dodge—it’s just that a simple “yes” or “no” would fail to do justice to the enormity of such a question, which is above the pay grade of physics as it currently exists. <br /></li><li><strong>Is this it?  Have you brought me to the end of the path of understanding QM?</strong>  No, I’ve just pointed the way toward the <em>beginning</em> of the path.  The most fundamental tenet of the Zen Anti-Interpretation is that there’s no shortcut to actually <a href="https://www.scottaaronson.com/qclec.pdf">working</a> <a href="https://www.amazon.com/Quantum-Mechanics-Theoretical-Leonard-Susskind/dp/0465062903/ref=asc_df_0465062903/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312014159412&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7852945785672685485&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9028280&amp;hvtargid=pla-435140302691&amp;psc=1">through</a> the Bell inequality, quantum teleportation, Shor’s algorithm, the Kochen-Specker and PBR theorems, possibly even a … <em>photon</em> or a <em>hydrogen atom</em>, so you can see quantum probability in action and be enlightened.  I’m further along the path than I was twenty years ago, but not as far along as some of my colleagues.  Even the greatest quantum Zen masters will be able to get further when new quantum phenomena and protocols are discovered in the future.  All the same, though—and this is another major teaching of the Zen Anti-Interpretation—there’s more to life than achieving greater and greater clarity about the foundations of QM.  And on that note…</li></ol>



<p>To those who asked me about Claus Peter Schnorr’s <a href="https://eprint.iacr.org/2021/232">claim</a> to have discovered a fast <em>classical</em> factoring algorithm, thereby “destroying” (in his words) the RSA cryptosystem, see (e.g.) <a href="https://twitter.com/inf_0_/status/1367376526300172288?fbclid=IwAR19Ip7XyoPjHfm9WBzqiUkQpxUVLGfVTgLGQmmncgrkUsvcLIrkzbOPw_U">this Twitter thread by Keegan Ryan</a>, which explains what certainly <em>looks</em> like a fatal error in Schnorr’s paper.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5359"><span class="datestr">at March 04, 2021 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
