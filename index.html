<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at September 07, 2020 09:24 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1108">IGAFIT Algorithmic Colloquium</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" target="_blank" rel="noreferrer noopener">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" target="_blank" rel="noreferrer noopener">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br />Vera Traub, University of Bonn<br />Title: An improved approximation algorithm for ATSP<br />Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br />Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br />Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br />Nathan Klein, University of Bonn<br />Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br />Nikhil Bansal<br />Artur Czumaj<br />Andreas Feldmann<br />Adi Rosén<br />Eva Rotenberg<br />Piotr Sankowski<br />Christian Sohler <br /></p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1108"><span class="datestr">at September 07, 2020 08:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2727">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/">The many faces of integration by parts – II : Randomized smoothing and score functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="556" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" class="wp-image-4633" height="254" />Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="357" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" class="wp-image-4598" height="292" />Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="491" alt="" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" class="wp-image-4545" height="322" />Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="446" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" class="wp-image-4605" height="225" />Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img width="432" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" class="wp-image-4624" height="288" />Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br />[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br />[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br />[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br />[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br />[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br />[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br />[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br />[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br />[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br />[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br />[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br />[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br />[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br />[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br />[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br />[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br />[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br />[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br />[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br />[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br />[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br />[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br />[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/"><span class="datestr">at September 07, 2020 07:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/132">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/132">TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/132"><span class="datestr">at September 07, 2020 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02233">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02233">Access-Adaptive Priority Search Tree</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Massa:Haley.html">Haley Massa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlmann:Jeffrey.html">Jeffrey Uhlmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02233">PDF</a><br /><b>Abstract: </b>In this paper we show that the priority search tree of McCreight, which was
originally developed to satisfy a class of spatial search queries on
2-dimensional points, can be adapted to the problem of dynamically maintaining
a set of keys so that the query complexity adapts to the distribution of
queried keys. Presently, the best-known example of such a data structure is the
splay tree, which dynamically reconfigures itself during each query so that
frequently accessed keys move to the top of the tree and thus can be retrieved
with fewer queries than keys that are lower in the tree. However, while the
splay tree is conjectured to offer optimal adaptive amortized query complexity,
it may require O(n) for individual queries. We show that an access-adaptive
priority search tree (AAPST) can provide competitive adaptive query performance
while ensuring O(log n) worst-case query performance, thus potentially making
it more suitable for certain interactive (e.g.,online and real-time)
applications for which the response time must be bounded.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02233"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.02207">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.02207">Fair and Useful Cohort Selection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Niklas Smedemark-Margulies, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langton:Paul.html">Paul Langton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.02207">PDF</a><br /><b>Abstract: </b>As important decisions about the distribution of society's resources become
increasingly automated, it is essential to consider the measurement and
enforcement of fairness in these decisions. In this work we build on the
results of Dwork and Ilvento ITCS'19, which laid the foundations for the study
of fair algorithms under composition. In particular, we study the cohort
selection problem, where we wish to use a fair classifier to select $k$
candidates from an arbitrarily ordered set of size $n&gt;k$, while preserving
individual fairness and maximizing utility. We define a linear utility function
to measure performance relative to the behavior of the original classifier. We
develop a fair, utility-optimal $O(n)$-time cohort selection algorithm for the
offline setting, and our primary result, a solution to the problem in the
streaming setting that keeps no more than $O(k)$ pending candidates at all
time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.02207"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01986">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01986">Smoothed analysis of the condition number under low-rank perturbations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Rikhav.html">Rikhav Shah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01986">PDF</a><br /><b>Abstract: </b>Let $M$ be an arbitrary $n$ by $n$ matrix of rank $n-k$. We study the
condition number of $M$ plus a \emph{low rank} perturbation $UV^T$ where $U, V$
are $n$ by $k$ random Gaussian matrices. Under some necessary assumptions, it
is shown that $M+UV^T$ is unlikely to have a large condition number. The main
advantages of this kind of perturbation over the well-studied dense Gaussian
perturbation where every entry is independently perturbed is the $O(nk)$ cost
to store $U,V$ and the $O(nk)$ increase in time complexity for performing the
matrix-vector multiplication $(M+UV^T)x$. This improves the $\Omega(n^2)$ space
and time complexity increase required by a dense perturbation, which is
especially burdensome if $M$ is originally sparse. We experimentally validate
our approach and consider generalizations to symmetric and complex settings.
Lastly, we show barriers in applying our low rank model to other problems
studied in the smoothed analysis framework.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01986"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01947">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01947">Nearly Linear-Time, Parallelizable Algorithms for Non-Monotone Submodular Maximization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01947">PDF</a><br /><b>Abstract: </b>We study parallelizable algorithms for maximization of a submodular function,
not necessarily monotone, with respect to a cardinality constraint $k$. We
improve the best approximation factor achieved by an algorithm that has optimal
adaptivity and query complexity, up to logarithmic factors in the size $n$ of
the ground set, from $0.039 - \epsilon$ to $0.193 - \epsilon$. We provide two
algorithms; the first has approximation ratio $1/6 - \epsilon$, adaptivity $O(
\log n )$, and query complexity $O( n \log k )$, while the second has
approximation ratio $0.193 - \epsilon$, adaptivity $O( \log^2 n )$, and query
complexity $O(n \log k)$. Heuristic versions of our algorithms are empirically
validated to use a low number of adaptive rounds and total queries while
obtaining solutions with high objective value in comparison with highly
adaptive approximation algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01947"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01928">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01928">Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Quintino Francesco Lotito, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montresor:Alberto.html">Alberto Montresor</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01928">PDF</a><br /><b>Abstract: </b>Over the last decade, there has been an increasing interest in temporal
graphs, pushed by a growing availability of temporally-annotated network data
coming from social, biological and financial networks. Despite the importance
of analyzing complex temporal networks, there is a huge gap between the set of
definitions, algorithms and tools available to study large static graphs and
the ones available for temporal graphs. An important task in temporal graph
analysis is mining dense structures, i.e., identifying high-density subgraphs
together with the span in which this high density is observed. In this paper,
we introduce the concept of $(k, \Delta)$-truss (span-truss) in temporal
graphs, a temporal generalization of the $k$-truss, in which $k$ captures the
information about the density and $\Delta$ captures the time span in which this
density holds. We then propose novel and efficient algorithms to identify
maximal span-trusses, namely the ones not dominated by any other span-truss
neither in the order $k$ nor in the interval $\Delta$, and evaluate them on a
number of public available datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01928"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01874">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01874">Sum-of-Squares Lower Bounds for Sherrington-Kirkpatrick via Planted Affine Planes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Mrinalkanti.html">Mrinalkanti Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeronimo:Fernando_Granha.html">Fernando Granha Jeronimo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Chris.html">Chris Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potechin:Aaron.html">Aaron Potechin</a>, Goutham Rajendran <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01874">PDF</a><br /><b>Abstract: </b>The Sum-of-Squares (SoS) hierarchy is a semi-definite programming
meta-algorithm that captures state-of-the-art polynomial time guarantees for
many optimization problems such as Max-$k$-CSPs and Tensor PCA. On the flip
side, a SoS lower bound provides evidence of hardness, which is particularly
relevant to average-case problems for which NP-hardness may not be available.
</p>
<p>In this paper, we consider the following average case problem, which we call
the \emph{Planted Affine Planes} (PAP) problem: Given $m$ random vectors
$d_1,\ldots,d_m$ in $\mathbb{R}^n$, can we prove that there is no vector $v \in
\mathbb{R}^n$ such that for all $u \in [m]$, $\langle v, d_u\rangle^2 = 1$? In
other words, can we prove that $m$ random vectors are not all contained in two
parallel hyperplanes at equal distance from the origin? We prove that for $m
\leq n^{3/2-\epsilon}$, with high probability, degree-$n^{\Omega(\epsilon)}$
SoS fails to refute the existence of such a vector $v$.
</p>
<p>When the vectors $d_1,\ldots,d_m$ are chosen from the multivariate normal
distribution, the PAP problem is equivalent to the problem of proving that a
random $n$-dimensional subspace of $\mathbb{R}^m$ does not contain a boolean
vector. As shown by Mohanty--Raghavendra--Xu [STOC 2020], a lower bound for
this problem implies a lower bound for the problem of certifying energy upper
bounds on the Sherrington-Kirkpatrick Hamiltonian, and so our lower bound
implies a degree-$n^{\Omega(\epsilon)}$ SoS lower bound for the certification
version of the Sherrington-Kirkpatrick problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01874"><span class="datestr">at September 07, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-83517349672236531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">Two Math Problems of interest (at least to me)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br /></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br /></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br /></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br /></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html"><span class="datestr">at September 06, 2020 09:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/131">TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/131"><span class="datestr">at September 06, 2020 05:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/130">TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/130"><span class="datestr">at September 06, 2020 04:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01802">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01802">Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a>, Yin-Tat Lee, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nanongkai:Danupon.html">Danupon Nanongkai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Richard.html">Richard Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Di.html">Di Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01802">PDF</a><br /><b>Abstract: </b>We present an $\tilde O(m+n^{1.5})$-time randomized algorithm for maximum
cardinality bipartite matching and related problems (e.g. transshipment,
negative-weight shortest paths, and optimal transport) on $m$-edge, $n$-node
graphs. For maximum cardinality bipartite matching on moderately dense graphs,
i.e. $m = \Omega(n^{1.5})$, our algorithm runs in time nearly linear in the
input size and constitutes the first improvement over the classic
$O(m\sqrt{n})$-time [Dinic 1970; Hopcroft-Karp 1971; Karzanov 1973] and $\tilde
O(n^\omega)$-time algorithms [Ibarra-Moran 1981] (where currently
$\omega\approx 2.373$). On sparser graphs, i.e. when $m = n^{9/8 + \delta}$ for
any constant $\delta&gt;0$, our result improves upon the recent advances of [Madry
2013] and [Liu-Sidford 2020b, 2020a] which achieve an $\tilde O(m^{4/3+o(1)})$
runtime.
</p>
<p>We obtain these results by combining and advancing recent lines of research
in interior point methods (IPMs) and dynamic graph algorithms. First, we
simplify and improve the IPM of [v.d.Brand-Lee-Sidford-Song 2020], providing a
general primal-dual IPM framework and new sampling-based techniques for
handling infeasibility induced by approximate linear system solvers. Second, we
provide a simple sublinear-time algorithm for detecting and sampling
high-energy edges in electric flows on expanders and show that when combined
with recent advances in dynamic expander decompositions, this yields efficient
data structures for maintaining the iterates of both [v.d.Brand et al.] and our
new IPMs. Combining this general machinery yields a simpler $\tilde O(n
\sqrt{m})$ time algorithm for matching based on the logarithmic barrier
function, and our state-of-the-art $\tilde O(m+n^{1.5})$ time algorithm for
matching based on the [Lee-Sidford 2014] barrier (as regularized in [v.d.Brand
et al.]).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01802"><span class="datestr">at September 06, 2020 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01765">Optimal Load Balanced Demand Distribution under Overload Penalties</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramnath:Sarnath.html">Sarnath Ramnath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gunturi:Venkata_M=_V=.html">Venkata M. V. Gunturi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01765">PDF</a><br /><b>Abstract: </b>Input to the Load Balanced Demand Distribution (LBDD) consists of the
following: (a) a set of service centers; (b) a set of demand nodes and; (c) a
cost matrix containing the cost of assignment for each (demand node, service
center) pair. In addition, each service center is also associated with a notion
of capacity and a penalty which is incurred if it gets overloaded. Given the
input, the LBDD problem determines a mapping from the set of n demand vertices
to the set of k service centers, n being much larger than k. The objective is
to determine a mapping that minimizes the sum of the following two terms: (i)
the total cost between demand units and their allotted service centers and,
(ii) total penalties incurred. The problem of LBDD has a variety of
applications. An instance of the LBDD problem can be reduced to an instance of
the min-cost bi-partite matching problem. The best known algorithm for min-cost
matching in an unbalanced bipartite graph yields a complexity of O($n^3k$).
This paper proposes novel allotment subspace re-adjustment based approach which
allows us to characterize the optimality of the mapping without invoking
matching or mincost flow. This approach yields an optimal solution with time
complexity $O(nk^3 +nk^2 log n)$, and also allows us to efficiently maintain an
optimal allotment under insertions and deletions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01765"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01683">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01683">Strong Hanani-Tutte for the Torus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fulek:Radoslav.html">Radoslav Fulek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pelsmajer:Michael_J=.html">Michael J. Pelsmajer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schaefer:Marcus.html">Marcus Schaefer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01683">PDF</a><br /><b>Abstract: </b>If a graph can be drawn on the torus so that every two independent edges
cross an even number of times, then the graph can be embedded on the torus.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01683"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01544">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01544">Fast Byzantine Gathering with Visibility in Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miller:Avery.html">Avery Miller</a>, Ullash Saha <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01544">PDF</a><br /><b>Abstract: </b>We consider the gathering task by a team of $m$ synchronous mobile robots in
a graph of $n$ nodes. Each robot has an identifier (ID) and runs its own
deterministic algorithm, i.e., there is no centralized coordinator. We consider
a particularly challenging scenario: there are $f$ Byzantine robots in the team
that can behave arbitrarily, and even have the ability to change their IDs to
any value at any time. There is no way to distinguish these robots from
non-faulty robots, other than perhaps observing strange or unexpected
behaviour. The goal of the gathering task is to eventually have all non-faulty
robots located at the same node in the same round. It is known that no
algorithm can solve this task unless there at least $f+1$ non-faulty robots in
the team. In this paper, we design an algorithm that runs in polynomial time
with respect to $n$ and $m$ that matches this bound, i.e., it works in a team
that has exactly $f+1$ non-faulty robots. In our model, we have equipped the
robots with sensors that enable each robot to see the subgraph (including
robots) within some distance $H$ of its current node. We prove that the
gathering task is solvable if this visibility range $H$ is at least the radius
of the graph, and not solvable if $H$ is any fixed constant.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01544"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01512">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01512">TopoMap: A 0-dimensional Homology Preserving Projection of High-Dimensional Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Doraiswamy:Harish.html">Harish Doraiswamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tierny:Julien.html">Julien Tierny</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Paulo_J=_S=.html">Paulo J. S. Silva</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nonato:Luis_Gustavo.html">Luis Gustavo Nonato</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Claudio.html">Claudio Silva</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01512">PDF</a><br /><b>Abstract: </b>Multidimensional Projection is a fundamental tool for high-dimensional data
analytics and visualization. With very few exceptions, projection techniques
are designed to map data from a high-dimensional space to a visual space so as
to preserve some dissimilarity (similarity) measure, such as the Euclidean
distance for example. In fact, although adopting distinct mathematical
formulations designed to favor different aspects of the data, most
multidimensional projection methods strive to preserve dissimilarity measures
that encapsulate geometric properties such as distances or the proximity
relation between data objects. However, geometric relations are not the only
interesting property to be preserved in a projection. For instance, the
analysis of particular structures such as clusters and outliers could be more
reliably performed if the mapping process gives some guarantee as to
topological invariants such as connected components and loops. This paper
introduces TopoMap, a novel projection technique which provides topological
guarantees during the mapping process. In particular, the proposed method
performs the mapping from a high-dimensional space to a visual space, while
preserving the 0-dimensional persistence diagram of the Rips filtration of the
high-dimensional data, ensuring that the filtrations generate the same
connected components when applied to the original as well as projected data.
The presented case studies show that the topological guarantee provided by
TopoMap not only brings confidence to the visual analytic process but also can
be used to assist in the assessment of other projection methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01512"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01498">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01498">Physarum Multi-Commodity Flow Dynamics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonifaci:Vincenzo.html">Vincenzo Bonifaci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Facca:Enrico.html">Enrico Facca</a>, Frederic Folz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karrenbauer:Andreas.html">Andreas Karrenbauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolev:Pavel.html">Pavel Kolev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehlhorn:Kurt.html">Kurt Mehlhorn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morigi:Giovanna.html">Giovanna Morigi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shahkarami:Golnoosh.html">Golnoosh Shahkarami</a>, Quentin Vermande <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01498">PDF</a><br /><b>Abstract: </b>In wet-lab experiments \cite{Nakagaki-Yamada-Toth,Tero-Takagi-etal}, the
slime mold Physarum polycephalum has demonstrated its ability to solve shortest
path problems and to design efficient networks, see Figure \ref{Wet-Lab
Experiments} for illustrations. Physarum polycephalum is a slime mold in the
Mycetozoa group. For the shortest path problem, a mathematical model for the
evolution of the slime was proposed in \cite{Tero-Kobayashi-Nakagaki} and its
biological relevance was argued. The model was shown to solve shortest path
problems, first in computer simulations and then by mathematical proof. It was
later shown that the slime mold dynamics can solve more general linear programs
and that many variants of the dynamics have similar convergence behavior. In
this paper, we introduce a dynamics for the network design problem. We
formulate network design as the problem of constructing a network that
efficiently supports a multi-commodity flow problem. We investigate the
dynamics in computer simulations and analytically. The simulations show that
the dynamics is able to construct efficient and elegant networks. In the
theoretical part we show that the dynamics minimizes an objective combining the
cost of the network and the cost of routing the demands through the network. We
also give alternative characterization of the optimum solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01498"><span class="datestr">at September 06, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01446">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01446">New Results and Bounds on Online Facility Assignment Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Saad Al Muttakee, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Abu_Reyan.html">Abu Reyan Ahmed</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rahman:Md=_Saidur.html">Md. Saidur Rahman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01446">PDF</a><br /><b>Abstract: </b>Consider an online facility assignment problem where a set of facilities $F =
\{ f_1, f_2, f_3, \cdots, f_{|F|} \}$ of equal capacity $l$ is situated on a
metric space and customers arrive one by one in an online manner on that space.
We assign a customer $c_i$ to a facility $f_j$ before a new customer $c_{i+1}$
arrives. The cost of this assignment is the distance between $c_i$ and $f_j$.
The objective of this problem is to minimize the sum of all assignment costs.
Recently Ahmed et al. (TCS, 806, pp. 455-467, 2020) studied the problem where
the facilities are situated on a line and computed competitive ratio of
"Algorithm Greedy" which assigns the customer to the nearest available
facility. They computed competitive ratio of algorithm named "Algorithm
Optimal-Fill" which assigns the new customer considering optimal assignment of
all previous customers. They also studied the problem where the facilities are
situated on a connected unweighted graph.
</p>
<p>In this paper we first consider that $F$ is situated on the vertices of a
connected unweighted grid graph $G$ of size $r \times c$ and customers arrive
one by one having positions on the vertices of $G$. We show that Algorithm
Greedy has competitive ratio $r \times c + r + c$ and Algorithm Optimal-Fill
has competitive ratio $O(r \times c)$. We later show that the competitive ratio
of Algorithm Optimal-Fill is $2|F|$ for any arbitrary graph. Our bound is tight
and better than the previous result. We also consider the facilities are
distributed arbitrarily on a plane and provide an algorithm for the scenario.
We also provide an algorithm that has competitive ratio $(2n-1)$. Finally, we
consider a straight line metric space and show that no algorithm for the online
facility assignment problem has competitive ratio less than $9.001$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01446"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01353">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01353">Zuckerli: A New Compressed Representation for Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Versari:Luca.html">Luca Versari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Comsa:Iulia_M=.html">Iulia M. Comsa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conte:Alessio.html">Alessio Conte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grossi:Roberto.html">Roberto Grossi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01353">PDF</a><br /><b>Abstract: </b>Zuckerli is a scalable compression system meant for large real-world graphs.
Graphs are notoriously challenging structures to store efficiently due to their
linked nature, which makes it hard to separate them into smaller, compact
components. Therefore, effective compression is crucial when dealing with large
graphs, which can have billions of nodes and edges. Furthermore, a good
compression system should give the user fast and reasonably flexible access to
parts of the compressed data without requiring full decompression, which may be
unfeasible on their system. Zuckerli improves multiple aspects of WebGraph, the
current state-of-the-art in compressing real-world graphs, by using advanced
compression techniques and novel heuristic graph algorithms. It can produce
both a compressed representation for storage and one which allows fast direct
access to the adjacency lists of the compressed graph without decompressing the
entire graph. We validate the effectiveness of Zuckerli on real-world graphs
with up to a billion nodes and 90 billion edges, conducting an extensive
experimental evaluation of both compression density and decompression
performance. We show that Zuckerli-compressed graphs are 10% to 29% smaller,
and more than 20% in most cases, with a resource usage for decompression
comparable to that of WebGraph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01353"><span class="datestr">at September 06, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.01346">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.01346">Circular Trace Reconstruction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Shyam.html">Shyam Narayanan</a>, Michael Ren <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.01346">PDF</a><br /><b>Abstract: </b>Trace Reconstruction is the problem of learning an unknown string $x$ from
independent traces of $x$, where traces are generated by independently deleting
each bit of $x$ with some deletion probability $q$. In this paper, we initiate
the study of Circular Trace Reconstruction, where the unknown string $x$ is
circular and traces are now rotated by a random cyclic shift. Trace
reconstruction is related to many computational biology problems studying DNA,
which is a primary motivation for this problem as well, as many types of DNA
are known to be circular.
</p>
<p>Our main results are as follows. First, we prove that we can reconstruct
arbitrary circular strings of length $n$ using
$\exp\big(\tilde{O}(n^{1/3})\big)$ traces for any constant deletion probability
$q$, as long as $n$ is prime or the product of two primes. For $n$ of this
form, this nearly matches the best known bound of $\exp\big(O(n^{1/3})\big)$
for standard trace reconstruction. Next, we prove that we can reconstruct
random circular strings with high probability using $n^{O(1)}$ traces for any
constant deletion probability $q$. Finally, we prove a lower bound of
$\tilde{\Omega}(n^3)$ traces for arbitrary circular strings, which is greater
than the best known lower bound of $\tilde{\Omega}(n^{3/2})$ in standard trace
reconstruction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.01346"><span class="datestr">at September 06, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/">Closing An Open Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Crawl, then walk, then run.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img width="150" alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" /></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.</p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p></p>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br />
<a href="https://rjlipton.wordpress.com/2011/02/01/godel’s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p></p><h2> Claiming A Solution </h2><p></p>
<p></p><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p></p>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Of course this implies P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph of edge probability <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" />, the size of the maximum clique is one of the two integers flanking <img src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\log_{1/p}(n)}" class="latex" title="{2\log_{1/p}(n)}" />. <p></p>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" title="{(1+\epsilon)\log_{1/p}(n)}" />, for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" /> and sufficiently large <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.
</li></ul>
<p>
You only need to close a gap of a factor of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p></p><h2> Some Examples </h2><p></p>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3n+1}" class="latex" title="{3n+1}" /> problem. It asks for the long-term behavior of the function: <img src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f(n) " class="latex" title="f(n) " /> which is equal to <img src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n/2 " class="latex" title="n/2 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> even and <img src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="3n+1 " class="latex" title="3n+1 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> odd. </p>
<p>The conjecture is that for every <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, iterating the function eventually hits <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, i.e., <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n) = 1}" class="latex" title="{f^i(n) = 1}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" />. <p></p>
</li><li>
For some <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n)}" class="latex" title="{f^i(n)}" /> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> achieve: <img src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f^i(n) &lt; \log^* n " class="latex" title="f^i(n) &lt; \log^* n " /> for some <img src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i " class="latex" title="i " />.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-\epsilon)}" class="latex" title="{(1-\epsilon)}" /> proportion of values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(n)}" class="latex" title="{g(n)}" /> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> so there is another prime <img src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q&gt;p}" class="latex" title="{q&gt;p}" /> bounded above by <img src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p +C}" class="latex" title="{p +C}" />. His <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was <img src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{70}" class="latex" title="{70}" /> million, but this was still a breakthrough. Previously no bounded <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{n}-1}" class="latex" title="{2^{n}-1}" /> vertex induced subgraph of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-dimensional cube graph has maximum degree at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{n}}" class="latex" title="{\sqrt{n}}" />. The previous best was only order logarithm in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p></p><h2> Advice To Claimers </h2><p></p>
<p></p><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p></p><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP show that you have a exponential algorithm that is better than known. Or for P<img src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{&lt;}" class="latex" title="{&lt;}" />NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{2^{n^{1/3}}}" class="latex" title="{2^{n^{1/3}}}" /> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> </em>
</p></blockquote>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/"><span class="datestr">at September 05, 2020 05:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/129">TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/129"><span class="datestr">at September 05, 2020 03:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5519">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/">Mathematics for Human Flourishing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/"><span class="datestr">at September 04, 2020 09:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20205">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/">Alef Corner: Math Collaboration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Another artistic view by Alef on mathematical collaboration.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg?w=640&amp;h=640" class="alignnone size-full wp-image-20207" height="640" /></a></p>
<p> </p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/"><span class="datestr">at September 04, 2020 07:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=52">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Friday, Sept 11 — Bin Yu from UC Berkeley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/"><span class="datestr">at September 04, 2020 12:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsmath.wordpress.com/?p=2298">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/jrl.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/">ITCS 2021 Call For Papers (deadline extension)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>NOTE</strong>:  The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US.</p>



<p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>. The <strong>submission deadline</strong> is now <strong>September 8, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way! See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong>September 8, 2020 (05:59PM PDT)</li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul></div>







<p class="date">
by James <a href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/"><span class="datestr">at September 03, 2020 09:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4949">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4949">My Utility+ podcast with Matthew Putman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 5):</span></strong> <a href="https://dunctank.podbean.com/e/scott-aaronson-infinite-universes/">Here’s another quantum computing podcast I did</a>, “Dunc Tank” with Duncan Gammie.  Enjoy!</p>



<hr />
<p><br />Thanks so much to <em>Shtetl-Optimized</em> readers, so far we’ve raised $1,371 for the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> and $225 for the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, which I intend to match for $3,192 total.  If you’d like to donate by tonight (Thursday night), there’s still $404 to go!</p>



<p>Meanwhile, a mere three days after declaring my <a href="https://www.scottaaronson.com/blog/?p=4942">“new motto,”</a> I’ve come up with a <em>new</em> new motto for this blog, hopefully a more cheerful one:</p>



<blockquote class="wp-block-quote"><p>When civilization seems on the brink of collapse, sometimes there’s nothing left to talk about but maximal separations between randomized and quantum query complexity.</p></blockquote>



<p>On that note, please enjoy my new <a href="https://nanotronics.co/thinkspace/24-scott-aaronson-before-and-after-the-machine/">one-hour podcast on Spotify</a> (if that link doesn’t work, <a href="https://overcast.fm/+UZFvS_CT8">try this one</a>) with <a href="https://en.wikipedia.org/wiki/Matthew_Putman_(scientist)">Matthew Putman</a> of Utility+.  Alas, my umming and ahhing were more frequent than I now aim for, but that’s partly compensated for by Matthew’s excellent decision to speed up the audio.  This was an unusually wide-ranging interview, covering everything from SlateStarCodex to quantum gravity to interdisciplinary conferences to the challenges of teaching quantum computing to 7-year-olds.  I hope you like it!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4949"><span class="datestr">at September 03, 2020 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/128">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/128">TR20-128 |  An Optimal Separation of Randomized and Quantum Query Complexity | 

	Alexander A. Sherstov, 

	Andrey Storozhenko, 

	Pei Wu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every decision tree, the absolute values of the Fourier coefficients of given order $\ell\geq1$ sum to at most $c^{\ell}\sqrt{{d\choose\ell}(1+\log n)^{\ell-1}},$ where $n$ is the number of variables, $d$ is the tree depth, and $c&gt;0$ is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal (arxiv 2019; FOCS 2020). The bounds prior to our work degraded rapidly with $\ell,$ becoming trivial already at $\ell=\sqrt{d}.$

As an application, we obtain, for any positive integer $k,$ a partial Boolean function on $n$ bits that has bounded-error quantum query complexity at most $\lceil k/2\rceil$ and randomized query complexity $\tilde{\Omega}(n^{1-1/k}).$ This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis (STOC 2015). Prior to our work, the best known separation was polynomially weaker: $O(1)$ versus $n^{2/3-\epsilon}$ for any $\epsilon&gt;0$ (Tal, FOCS 2020).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/128"><span class="datestr">at September 03, 2020 01:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/">Post-doc at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by October 26, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU).</p>
<p>The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5</a><br />
Email: pagh@itu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/"><span class="datestr">at September 03, 2020 08:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/">PhD student at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by September 28, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen.</p>
<p>The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or mathematics.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282</a><br />
Email: pagh@itu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/"><span class="datestr">at September 03, 2020 08:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4174113397967272312">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html">Broad Testing Thank You</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I have a loose association with the Broad Institute, an institute created so that "complementary expertise of the genomic scientists and the chemical biologists across MIT and Harvard be brought together in one place to drive the transformation of medicine with molecular knowledge."  (See <a href="https://www.broadinstitute.org/history">https://www.broadinstitute.org/history</a> )</p><p>They recently passed an amazing milestone, having performed over 1 million Covid tests.  They weren't set up to be a Covid testing lab, but the converted their institute space to respond to the Covid crisis.  (See <a href="https://covid19-testing.broadinstitute.org/">https://covid19-testing.broadinstitute.org/</a> )  </p><p>In short, they stepped up.  They certainly didn't have to, but they did.  Maybe it will help them do good science, now and in the future.  But my understanding is that they saw a clear societal need (lots of <a href="https://www.amazon.com/gp/product/B07DFZ12KV/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=michaelmitzen-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=B07DFZ12KV&amp;linkId=5a5c250c33b2ba71012ff577c69e1a77">outbreak specialists there</a> ) and they realized they had the expertise and equipment to do good that went beyond science.  I just wanted to give a shout out to the Broad for their good works, scientific and societal.  </p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html"><span class="datestr">at September 03, 2020 12:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20194">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/">Alef’s Corner: Math Collaboration 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.files.wordpress.com/2020/09/alef.jpg"><img src="https://gilkalai.files.wordpress.com/2020/09/alef-math-collaboration.jpg?w=640" alt="alef-math-collaboration" class="alignnone size-full wp-image-20202" /></a></p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/"><span class="datestr">at September 02, 2020 01:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html">Isosceles polyhedra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, <a href="http://arxiv.org/abs/2009.00116">arXiv:2009.00116</a>. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new<sup id="fnref:1"><a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:1" class="footnote">1</a></sup> infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/twisted.svg" alt="Twisted augmented bipyramid with isosceles-triangle faces" /></p>

<p>One of the other previously known families, shown below, has two integer parameters (the numbers of sides on the two half-bipyramids it combines), but I don’t know whether the same double parameterization is possible for the new family.</p>

<p style="text-align: center;"><img width="80%" alt="Combination of two half-bipyramids with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/biarc.svg" /></p>

<p>In 2001, Branko Grünbaum published an example of a polyhedron that could not be realized with congruent faces, even non-convexly,<sup id="fnref:2"><a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:2" class="footnote">2</a></sup> but it can be realized as a convex polyhedron with all faces isosceles (or equilateral), as shown below. I don’t know whether there are polyhedra that cannot be realized with all faces isosceles, if one allows the realization to be non-convex (but non-self-crossing and combinatorially equivalent to a convex polyhedron) and the faces to be non-congruent.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/grunbaum.svg" alt="Grünbaum's example of a polyhedron that cannot be realized with congruent faces, realized convexly with isosceles and equilateral triangle faces" /></p>

<p>My new preprint proves that there exist polyhedra (iterated Kleetopes) that cannot be realized as convex polyhedra with isosceles faces. But the construction is a little non-explicit and I don’t know how complicated these polyhedra need to be. For instance, I don’t know whether there is a convex isosceles-face realization of the double Kleetope of the octahedron, shown below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/kkoct.svg" alt="Double Kleetope of an octahedron" /></p>

<p>Grünbaum’s example can be realized convexly with only two edge lengths, and my non-isosceles-faced polyhedra require at least three edge lengths in any convex realization. I don’t know whether the number of required edge lengths can be unbounded, or whether non-convex realizations ever require three lengths (although certain stacked polyhedra require at least two).</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “<a href="https://archive.bridgesmathart.org/2005/bridges2005-489.html">Some new tilings of the sphere with congruent triangles</a>”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations.</p>

      <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/nwb.svg" alt="Non-convex polyhedron with congruent isosceles-triangle faces" /> <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Grünbaum, Branko (2001), “<a href="https://sites.math.washington.edu/~grunbaum/Nonequifacettablesphere.pdf">A convex polyhedron which is not equifacettable</a>”, <em>Geombinatorics</em> 10: 165–171. I don’t know how to access old papers on this journal in general, but fortunately Grünbaum made his one available on his web site. <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:2" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html"><span class="datestr">at September 01, 2020 11:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4407">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/">Time Flies When the World is Falling Apart</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>It has been exactly one year since I moved to Italy.</p>
<p>Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of it.</p>
<p><span id="more-4407"></span></p>
<p>In March and April, being in the hottest hot spot of a global pandemic was not ideal. Something rather unexpected, however, has happened. Italy is not known for very effective governance and Italians are not known as a rule-following people. Yet the government implemented a strict lockdown, and held it long enough that the “reopening” took place with a safely low circulation of the virus. People have been mostly following the rules after the reopening, and for most of the summer Italy has been doing quite well. (There has been a recent surge of new cases  in the last few weeks, so it may be too soon to declare victory, but the numbers of intensive care hospitalizations and of deaths are still low).</p>
<p>Meanwhile, there have been “second waves” in several other European countries and the situation in the United States, as the President memorably said, is what it is.</p>
<p>This picture by Noah Berger for AP has been circulating as the perfect description of what is going on in Northern California this summer</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/img_0077.jpeg?w=584" alt="IMG_0077" class="alignnone size-full wp-image-4409" /></p>
<p>This New York Times headline also does a good job of packing at least five things that are wrong in California right now (the spread of covid-19, the overcrowding of prisons because of excessive sentencing, the budget cuts affecting firefighting efforts, the exploitation of prisoners for underpaid work, and the spread of wildfires) in a single sentence:</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/california-inmates.png?w=584" alt="california-inmates" class="alignnone size-full wp-image-4410" /></p>
<p>At the cost of resorting to banalities and cliches, Italians often do badly when confronted with good opportunities and lucky breaks, that are often wasted, but we do unexpectedly well in times of crisis. This can even be seen in the national football team, that is known for epic come-from-behind victories and for missing penalty kicks. The American spirit is to be ambitious and optimistic, and to pursue high-risk high-reward opportunities when they present themselves. The flip side is that adversity is often met in America with either despair or anger, typically counterproductively. All things considered, I am happy that I am getting to spend 2020 in Italy. Earlier this summer, the president and the rector of Bocconi announced the new long-term strategic plan for the university, which involves creating a new computer science department. If/when such plans come to fruition, there will be several faculty positions in computer science, at internationally competitive salaries, with advantageous taxation for people moving to Italy from other countries, and with English as the language of instructions, so maybe you too will consider such a move.</p>
<p>Bocconi is preparing to restart in-person teaching in a couple of weeks. The plan is to record and post lectures, to allow students to attend lectures remotely if they prefer, and to use classrooms at most at half capacity. If a class has to be scheduled in a classroom whose capacity is less than twice the class enrollment, students will be split in two groups. Each group will (be allowed to) physically attend in alternating weeks, and will (be required to) attend online at other times.</p>
<p>The space between campus buildings has been marked with well-spaced walking paths, and there are even pedestrian roundabouts.</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/img_0122.jpeg?w=584" alt="IMG_0122" class="alignnone size-full wp-image-4412" /></p>
<p>Are we heading for a second-wave disaster? Will students really follow the rules? On Sunday we were in Rome and we took a bus to the city center. Every other seat in the bus was marked with a “do not sit” sign to create distancing between sitting people. All marked seats were empty even as the bus was filling up and several people were standing. Then a group of German tourists got in, and they sat in all the forbidden seats. This is the upside-down world of 2020, all bets are off.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/"><span class="datestr">at September 01, 2020 05:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4647242799583898034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html">A well known theorem that has not been written down- so I wrote it down- CLIQ is #P-complete</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(The two proofs that CLIQ is #P-complete that I discuss in this blog are written up by Lance and myself and are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/sharpclique.pdf">here</a>. I think both are well known but I have not been able to find a writeup,so Lance and I did one.)</p><p><br /></p><p> I have been looking at #P (see my last blog on it it, <a href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html">here</a>, for a refresher on this topic) since I will be teaching this topic in Spring 2021.  Recall</p><p>#SAT(\phi) = the number of satisfying assignments for phi</p><p>#CLIQ((G,k))= the number of cliques of size \ge k of G</p><p>#SAT is #P-complete by a cursory look at the proof of the Cook-Levin theorem.</p><p>A function is #P-complete if everything else in #P is Turing-Poly red to it. To show for some set A </p><p>in NP, #A is #P-complete one usually uses pars reductions. </p><p>I wanted a proof that #CLIQ is #P-complete, so I wanted a parsimonious reduction from SAT to CLIQ (Thats a reduction f: SAT--&gt; CLIQ such that the number of satisfying assignments of phi equals the number of cliques of size \ge k of G.)</p><p>I was sure there is such a reduction and that it was well known and would be on the web someplace. So I tried to find it.</p><p>ONE:</p><p>I tracked some references to a paper by Janos Simon (<a href="https://link.springer.com/content/pdf/10.1007%2F3-540-08342-1_37.pdf">see here</a>)  where he claims that the reduction of SAT to CLIQ  by Karp <a href="https://blog.computationalcomplexity.org/feeds/posts/{http://cgi.di.uoa.gr/~sgk/teaching/grad/handouts/karp.pdf">(see here)</a> was pars.  I had already considered that and decided that Karps reduction was NOT pars.  I looked at both Simon's paper and Karp's paper to make sure I wasn't missing something (e.g., I misunderstood what Simon Says or what Karp ... darn, I can't think of anything as good as `Simon Says'). It seemed to me that Simon was incorrect. If I am wrong let me know.</p><p>TWO</p><p>Someone told me that it was in Valiant's  paper (see <a href="https://epubs.siam.org/doi/10.1137/0208032">here</a>). Nope. Valiant's paper shows that counting the number of maximal cliques is #P-complete. Same Deal Here- if I am wrong let me know. One can modify Valiant's argument to get #CLIQ #P-complete, and I do so in the write up. The proof is a string of reductions and starts with PERM is #P-complete. This does prove that# CLIQ is  #P-complete, but is rather complicated. Also, the reduction is not pars.</p><p>THREE <br />Lance told me an easy pars reduction that is similar to Karp's non-pars reduction, but it really is NOT Karp's reduction. Its in my write up. I think it is well known since I recall hearing it about 10 years ago. From Lance. Hmmm, maybe its just well known to Lance.</p><p><br /></p><p>But here is my question: I am surprised I didn't find it on the web. If someone can point to a place on the web where it is, please let me know. In any case, its on the web NOW (my write up) so hopefully in the future someone else looking for it can find it.</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html"><span class="datestr">at September 01, 2020 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/">QuICS Hartree Postdoctoral Fellowships at Joint Center for Quantum Information and Computer Science (apply by December 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Joint Center for Quantum Information and Computer Science (QuICS, <a href="http://quics.umd.edu">http://quics.umd.edu</a>) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science.<br />
Applications should be submitted through AcademicJobsOnline at <a href="https://academicjobsonline.org/ajo/jobs/16778.">https://academicjobsonline.org/ajo/jobs/16778.</a></p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/16778">https://academicjobsonline.org/ajo/jobs/16778</a><br />
Email: quics-coordinator@umiacs.umd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/"><span class="datestr">at September 01, 2020 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-7958692547156288391">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html">No Regret Algorithms from the Min Max Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The existence of no-regret learning algorithms can be used to prove <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Von-Neumann's min-max theorem</a>. This argument is originally due to <a href="https://www.cs.princeton.edu/~schapire/papers/FreundScYY.pdf">Freund and Schapire</a>, and I <a href="https://www.cis.upenn.edu/~aaroth/courses/slides/agt20/lect08.pdf">teach it to my undergraduates </a>in my algorithmic game theory class. The min-max theorem also can be used to prove the existence of no-regret learning algorithms. Here is a constructive version of the argument (Constructive in that in the resulting algorithm, you only need to solve polynomially sized zero-sum games, so you can do it via linear programming).<div><br /></div><div>Recall the setting. Play proceeds in rounds $t \in \{1,\ldots,T\}$. At each day $t$, the learner chooses one of $k$ actions $i_t \in \{1,\ldots,k\}$, and the adversary chooses a loss vector $\ell^t \in [0,1]^k$. The learner incurs loss $\ell^t_{i_t}$, corresponding to the action he chose. At the end of the interaction, the regret of the learner is defined to be the difference between the cumulative loss he incurred and the cumulative loss of the best fixed action (played consistently)  in hindsight:</div><div>$$\textrm{Regret}_T = \max_j \left(\sum_{t=1}^T \ell^t_{i_t} - \ell^t_j\right)$$ </div><div>A classical and remarkable result is that there exist algorithms that can guarantee that regret grows only sublinearly with time: $\textrm{Regret}_T = O(\sqrt{T})$. Lets prove this. </div><div><br /></div><div><br /></div><div>Define the non-negative portion of our cumulative regret with respect to action $j$ up until day $d$ as:</div><div>$$V_d^j = \left(\sum_{t=1}^d\left(\ell^t_{i_t} - \ell^t_j\right)\right)^+$$</div><div>and our additional regret at day $d+1$ with respect to action $j$ as:</div><div>$$r_{j}^{d+1} = \ell_{i_{d+1}}^{d+1} - \ell^{d+1}_j$$</div><div>Observe that if $V_{d}^j \geq 1$  then $V_{d+1}^j = V_d^j + r_j^{d+1}$. </div><div><br /></div><div><br /></div><div>Define a surrogate loss function as our squared cumulative regrets, summed over all actions: </div><div>$$L_d = \sum_{j=1}^k (V_d^j)^2$$</div><div>Observe that we can write the expected gain in our loss on day $d+1$, conditioned on the history thus far:</div><div>$$\mathbb{E}[L_{d+1} - L_d] \leq \sum_{j : V_d^j \geq 1} \mathbb{E}[(V_d^j+r_j^{d+1})^2 - (V_d^j)^2) ] + 3k$$</div><div>$$= \sum_{j : V_d^j \geq 1} \left(2V_d^j \mathbb{E}[r_{j}^{d+1}] + \mathbb{E}[(r_{j}^{d+1})^2]\right) + 3k $$</div><div>$$\leq \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) + 4k$$</div><div>where the expectations are taken over the randomness of both the learner and the adversary in round $d+1$. </div><div><br /></div><div>Now consider a zero-sum game played between the learner and the adversary in which the learner is the minimization player, the adversary is the maximization player, and the utility function is $$u(i_{d+1}, \ell^{d+1}) = \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right)$$ The min-max theorem says that the learner can guarantee the same payoff for herself in the following two scenarios:</div><div><br /></div><div><ol style="text-align: left;"><li>The learner first has to commit to playing a distribution $p_{d+1}$ over actions $i$, and then the adversary gets to best respond by picking the worst possible loss vectors, or</li><li>The adversary has to first commit to a distribution over loss vectors $\ell$ and then the learner gets the benefit of picking the best action $i_{d+1}$ to respond with. </li></ol>Scenario 1) is the scenario our learner finds herself in, when playing against an adaptive adversary. But 2) is much easier to analyze. If the adversary first commits to a distribution over loss vectors $\ell^{d+1}$, the learner can always choose action $i_{d+1} = \arg\min_j \mathbb{E}[\ell^{d+1}_j]$, which guarantees that $\mathbb{E}[r_{j}^{d+1}] \leq 0$, which in turn guarantees that the value of the game $ \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) \leq 0$.  Hence, the min-max theorem tells us that the learner always has a distribution over actions $p_{d+1}$ that guarantees that $\mathbb{E}[L_{d+1} - L_d] \leq 4k$, <i>even in the worst case over loss functions</i>. If the learner always plays according to this distribution, then by a telescoping sum, we have that:</div><div>$$\mathbb{E}[L_T] \leq 4kT$$.</div><div>We therefore have by Jensen's inequality that:</div><div>$$\mathbb{E}[\max_j (V^j_T)] \leq \sqrt{\mathbb{E}[\max_j (V^j_T)^2]}\leq \sqrt{\mathbb{E}[\sum_{j=1}^k (V_j^T)^2]} \leq 2\sqrt{kT}$$.</div></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html"><span class="datestr">at September 01, 2020 02:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/">postdoc at TU Hamburg (apply by September 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator.</p>
<p>Website: <a href="https://www.tuhh.de/algo/jobs.html">https://www.tuhh.de/algo/jobs.html</a><br />
Email: matthias.mnich@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/"><span class="datestr">at September 01, 2020 08:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/icml2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/icml2020/">Conference Digest - ICML 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://icml.cc/virtual/2020">ICML 2020</a> is one of the premiere venues in machine learning, and generally features a lot of great work in differentially private machine learning.
This year is no exception: the relevant papers are listed below to the best of our ability, including links to the full versions of papers, as well as the conference pages (which contain slides and 15 minute videos for each paper).
As always, please inform us if we overlooked any papers on differential privacy.</p>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1909.12732">Alleviating Privacy Attacks via Causal Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6346">page</a>)<br />
<a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, <a href="http://www.amitsharma.in/">Amit Sharma</a>, <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.10341">An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm</a> (<a href="https://icml.cc/virtual/2020/poster/6240">page</a>)<br />
<a href="https://github.com/dpeng817">Chris DeCarolis</a>, <a href="https://twitter.com/exsidius">Mukul Ram</a>, <a href="https://www.cs.umd.edu/people/sesmaeil">Seyed Esmaeili</a>, <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a>, <a href="http://furong-huang.com/">Furong Huang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.09697">Bayesian Differential Privacy for Machine Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6547">page</a>)<br />
<a href="https://scholar.google.com/citations?user=BCWx7iQAAAAJ">Aleksei Triastcyn</a>, <a href="https://people.epfl.ch/boi.faltings">Boi Faltings</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.03030">Certified Data Removal from Machine Learning Models</a> (<a href="https://icml.cc/virtual/2020/poster/5895">page</a>)<br />
<a href="https://sites.google.com/view/chuanguo">Chuan Guo</a>, <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a>, <a href="https://awnihannun.com/">Awni Hannun</a>, <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.00038">Context Aware Local Differential Privacy</a> (<a href="https://icml.cc/virtual/2020/poster/5775">page</a>)<br />
<a href="https://people.ece.cornell.edu/acharya/">Jayadev Acharya</a>, <a href="https://research.google/people/105175/">Kallista Bonawitz</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://research.google/people/106777/">Daniel Ramage</a>, <a href="http://www.zitengsun.com/">Ziteng Sun</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.12813">Data-Dependent Differentially Private Parameter Learning for Directed Graphical Models</a> (<a href="https://icml.cc/virtual/2020/poster/6262">page</a>)<br />
<a href="https://scholar.google.com/citations?user=lWWAZ4YAAAAJ">Amrita Roy Chowdhury</a>, <a href="http://pages.cs.wisc.edu/~thodrek/">Theodoros Rekatsinas</a>, <a href="http://pages.cs.wisc.edu/~jha/">Somesh Jha</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09745">Differentially Private Set Union</a> (<a href="https://icml.cc/virtual/2020/poster/6541">page</a>)<br />
<a href="https://www.microsoft.com/en-us/research/people/sigopi/">Sivakanth Gopi</a>, <a href="https://www.linkedin.com/in/pankajgulhane/">Pankaj Gulhane</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://heyyjudes.github.io/">Judy Hanwen Shen</a>, <a href="https://www.microsoft.com/en-us/research/people/milads/">Milad Shokouhi</a>, <a href="http://www.yekhanin.org/">Sergey Yekhanin</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.11651">Fair Learning with Private Demographic Data</a> (<a href="https://icml.cc/virtual/2020/poster/6499">page</a>)<br />
<a href="https://husseinmozannar.github.io/">Hussein Mozannar</a>, <a href="https://sites.google.com/site/mesrob/home/">Mesrob Ohannessian</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15744">Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraint</a> (<a href="https://icml.cc/virtual/2020/poster/6365">page</a>)<br />
<a href="https://dblp.org/pid/166/1694.html">Akbar Rafiey</a>, <a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.00706">(Locally) Differentially Private Combinatorial Semi-Bandits</a> (<a href="https://icml.cc/virtual/2020/poster/6315">page</a>)<br />
<a href="https://scholar.google.com/citations?user=sioumZAAAAAJ">Xiaoyu Chen</a>, <a href="https://scholar.google.com/citations?user=Bw-WdyUAAAAJ">Kai Zheng</a>, <a href="https://twitter.com/zixinjackzhou">Zixin Zhou</a>, <a href="https://scholar.google.com/citations?user=m8m9nD0AAAAJ">Yunchang Yang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chan</a>, <a href="http://www.liweiwang-pku.com/">Liwei Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05453">New Oracle-Efficient Algorithms for Private Synthetic Data Release</a> (<a href="https://icml.cc/virtual/2020/poster/5814">page</a>)<br />
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://scholar.google.com/citations?user=dDVIyEQAAAAJ">Grace Tian</a>, <a href="https://cs-people.bu.edu/mbun/">Mark Bun</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1190-Paper.pdf">On Differentially Private Stochastic Convex Optimization with Heavy-tailed Data</a> (<a href="https://icml.cc/virtual/2020/poster/5948">page</a>)<br />
<a href="http://www.acsu.buffalo.edu/~dwang45/">Di Wang</a>, <a href="https://scholar.google.com/citations?user=e3ZhEDEAAAAJ">Hanshen Xiao</a>, <a href="https://people.csail.mit.edu/devadas/">Srinivas Devadas</a>, <a href="https://cse.buffalo.edu/~jinhui/">Jinhui Xu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.13830">Optimal Differential Privacy Composition for Exponential Mechanisms</a> (<a href="https://icml.cc/virtual/2020/poster/6687">page</a>)<br />
<a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://dblp.org/pid/155/9794.html">David Durfee</a>, <a href="https://scholar.google.com/citations?user=jr7gGB4AAAAJ">Ryan Rogers</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.01783">Oracle Efficient Private Non-Convex Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/5815">page</a>)<br />
<a href="https://sethneel.com/">Seth Neel</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, <a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2341-Paper.pdf">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</a> (<a href="https://icml.cc/virtual/2020/poster/6134">page</a>)<br />
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, <a href="https://www.itu.dk/people/pagh/">Rasmus Pagh</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Paper.pdf">Private Outsourced Bayesian Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/6783">page</a>)<br />
<a href="https://scholar.google.com/citations?user=7_2XTQ8AAAAJ">Dmitrii Kharkovskii</a>, <a href="https://daizhongxiang.github.io/">Zhongxiang Dai</a>, <a href="https://www.comp.nus.edu.sg/~lowkh/research.html">Bryan Kian Hsiang Low</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.10941">Private Query Release Assisted by Public Data</a> (<a href="https://icml.cc/virtual/2020/poster/6329">page</a>)<br />
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="https://www.ccs.neu.edu/home/albertcheu/">Albert Cheu</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2453-Paper.pdf">Private Reinforcement Learning with PAC and Regret Guarantees</a> (<a href="https://icml.cc/virtual/2020/poster/6152">page</a>)<br />
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://borjaballe.github.io/">Borja Balle</a>, <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.01327">Privately Detecting Changes in Unknown Distributions</a> (<a href="https://icml.cc/virtual/2020/poster/5854">page</a>)<br />
<a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a>, <a href="https://sites.google.com/view/skrehbiel/home">Sara Krehbiel</a>, <a href="https://scholar.google.com/citations?user=ayasb_wAAAAJ">Yuliia Lut</a>, <a href="https://wanrongz.github.io/">Wanrong Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09463">Privately Learning Markov Random Fields</a> (<a href="https://icml.cc/virtual/2020/poster/5776">page</a>)<br />
<a href="https://huanyuzhang.github.io/">Huanyu Zhang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1903.09822">Scalable Differential Privacy with Certified Robustness in Adversarial Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6401">page</a>)<br />
<a href="https://sites.google.com/site/ihaiphan/">NhatHai Phan</a>, <a href="https://www.cise.ufl.edu/~mythai/">My T. Thai</a>, <a href="https://scholar.google.com/citations?user=OgXtPDIAAAAJ">Han Hu</a>, <a href="http://www.cs.kent.edu/~jin/">Ruoming Jin</a>, <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>, <a href="https://ix.cs.uoregon.edu/~dou/">Dejing Dou</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.04493">Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion</a> (<a href="https://icml.cc/virtual/2020/poster/6734">page</a>)<br />
<a href="https://enosair.github.io/">Qinqing Zheng</a>, <a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://www.med.upenn.edu/apps/faculty/index.php/g275/p8939931">Qi Long</a>, <a href="http://www-stat.wharton.upenn.edu/~suw/">Weijie J. Su</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/icml2020/"><span class="datestr">at August 31, 2020 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
