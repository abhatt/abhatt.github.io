<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 24, 2020 06:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/">Postdoc at University of Vienna, TU Vienna, IST Austria, WU Vienna (apply by June 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad.</p>
<p>Website: <a href="http://vgsco.univie.ac.at/positions">http://vgsco.univie.ac.at/positions</a><br />
Email: vgsco@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/"><span class="datestr">at May 24, 2020 08:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/082">TR20-082 |  MaxSAT Resolution and Subcube Sums | 

	Yuval Filmus, 

	Meena Mahajan, 

	Gaurav Sood, 

	Marc Vinyals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the MaxRes rule in the context of certifying unsatisfiability. We show that it can be exponentially more powerful than tree-like resolution, and when augmented with weakening (the system MaxResW), p-simulates tree-like resolution. In devising a lower bound technique specific to MaxRes (and not merely inheriting lower bounds from Res), we define a new semialgebraic proof system called the SubCubeSums proof system. This system, which p-simulates MaxResW, is a special case of the Sherali-Adams proof system. In expressivity, it is the integral restriction of conical juntas studied in the contexts of communication complexity and extension complexity. We show that it is not simulated by Res. Using a proof technique qualitatively different from the lower bounds that MaxResW inherits from Res, we show that Tseitin contradictions on expander graphs are hard to refute in SubCubeSums. We also establish a lower bound technique via lifting: for formulas requiring large degree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/082"><span class="datestr">at May 23, 2020 07:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10801">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10801">Complexity Analysis Of Next-Generation VVC Encoding and Decoding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pakdaman:Farhad.html">Farhad Pakdaman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adelimanesh:Mohammad_Ali.html">Mohammad Ali Adelimanesh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gabbouj:Moncef.html">Moncef Gabbouj</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hashemi:Mahmoud_Reza.html">Mahmoud Reza Hashemi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10801">PDF</a><br /><b>Abstract: </b>While the next generation video compression standard, Versatile Video Coding
(VVC), provides a superior compression efficiency, its computational complexity
dramatically increases. This paper thoroughly analyzes this complexity for both
encoder and decoder of VVC Test Model 6, by quantifying the complexity
break-down for each coding tool and measuring the complexity and memory
requirements for VVC encoding/decoding. These extensive analyses are performed
for six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD),
Random-Access (RA), and All-Intra (AI) conditions (a total of 320
encoding/decoding). Results indicate that the VVC encoder and decoder are 5x
and 1.5x more complex compared to HEVC in LD, and 31x and 1.8x in AI,
respectively. Detailed analysis of coding tools reveals that in LD on average,
motion estimation tools with 53%, transformation and quantization with 22%, and
entropy coding with 7% dominate the encoding complexity. In decoding, loop
filters with 30%, motion compensation with 20%, and entropy decoding with 16%,
are the most complex modules. Moreover, the required memory bandwidth for VVC
encoding/decoding are measured through memory profiling, which are 30x and 3x
of HEVC. The reported results and insights are a guide for future research and
implementations of energy-efficient VVC encoder/decoder.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10801"><span class="datestr">at May 23, 2020 10:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10800">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10800">New Approximation Algorithms for Maximum Asymmetric Traveling Salesman and Shortest Superstring</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paluch:Katarzyna.html">Katarzyna Paluch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10800">PDF</a><br /><b>Abstract: </b>In the maximum asymmetric traveling salesman problem (Max ATSP) we are given
a complete directed graph with nonnegative weights on the edges and we wish to
compute a traveling salesman tour of maximum weight. In this paper we give a
fast combinatorial $\frac{7}{10}$-approximation algorithm for Max ATSP. It is
based on techniques of {\em eliminating} and {\em diluting} problematic
subgraphs with the aid of {\it half-edges} and a method of edge coloring. (A
{\it half-edge} of edge $(u,v)$ is informally speaking "either a head or a tail
of $(u,v)$".) A novel technique of {\em diluting} a problematic subgraph $S$
consists in a seeming reduction of its weight, which allows its better
handling.
</p>
<p>The current best approximation algorithms for Max ATSP, achieving the
approximation guarantee of $\frac 23$, are due to Kaplan, Lewenstein, Shafrir,
Sviridenko (2003) and Elbassioni, Paluch, van Zuylen (2012). Using a result by
Mucha, which states that an $\alpha$-approximation algorithm for Max ATSP
implies a $(2+\frac{11(1-\alpha)}{9-2\alpha})$-approximation algorithm for the
shortest superstring problem (SSP), we obtain also a $(2 \frac{33}{76} \approx
2,434)$-approximation algorithm for SSP, beating the previously best known
(having an approximation factor equal to $2 \frac{11}{23} \approx 2,4782$.)
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10800"><span class="datestr">at May 23, 2020 10:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10749">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10749">Distributed Verifiers in PCP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaladanki:Nagaganesh.html">Nagaganesh Jaladanki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Wilson.html">Wilson Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10749">PDF</a><br /><b>Abstract: </b>Traditional proof systems involve a resource-bounded verifier communicating
with a powerful (but untrusted) prover. Distributed verifier proof systems are
a new family of proof models that involve a network of verifier nodes
communicating with a single independent prover that has access to the complete
network structure of the verifiers. The prover is tasked with convincing all
verifiers of some global property of the network graph. In addition, each
individual verifier may be given some input string they will be required to
verify during the course of computation. Verifier nodes are allowed to exchange
messaged with nodes a constant distance away, and accept / reject the input
after some computation.
</p>
<p>Because individual nodes are limited to a local view, communication with the
prover is potentially necessary to prove global properties about the network
graph of nodes, which only the prover has access to. In this system of models,
the entire model accepts the input if and only if every individual node has
accepted.
</p>
<p>There are three models in the distributed verifier proof system family:
$\mathsf{LCP}$, $\mathsf{dIP}$, and our proposed $\mathsf{dPCP}$, with the
fundamental difference between these coming from the type of communication
established between the verifiers and the prover. In this paper, we will first
go over the past work in the $\mathsf{LCP}$ and $\mathsf{dIP}$ space before
showing properties and proofs in our $\mathsf{dPCP}$ system.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10749"><span class="datestr">at May 23, 2020 10:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10610">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10610">Combinatorial two-stage minmax regret problems under interval uncertainty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasperski:Adam.html">Adam Kasperski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zielinski:Pawel.html">Pawel Zielinski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10610">PDF</a><br /><b>Abstract: </b>In this paper a class of combinatorial optimization problems is discussed. It
is assumed that a feasible solution can be constructed in two stages. In the
first stage the objective function costs are known while in the second stage
they are uncertain and belong to an interval uncertainty set. In order to
choose a solution, the minmax regret criterion is used. Some general properties
of the problem are established and results for two particular problems, namely
the shortest path and the selection problem, are shown.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10610"><span class="datestr">at May 23, 2020 10:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10566">A Massively Parallel Algorithm for Minimum Weight Vertex Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ce.html">Ce Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nilis:Daan.html">Daan Nilis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10566">PDF</a><br /><b>Abstract: </b>We present a massively parallel algorithm, with near-linear memory per
machine, that computes a $(2+\varepsilon)$-approximation of minimum-weight
vertex cover in $O(\log\log d)$ rounds, where $d$ is the average degree of the
input graph.
</p>
<p>Our result fills the key remaining gap in the state-of-the-art MPC algorithms
for vertex cover and matching problems; two classic optimization problems,
which are duals of each other. Concretely, a recent line of work---by Czumaj et
al. [STOC'18], Ghaffari et al. [PODC'18], Assadi et al. [SODA'19], and Gamlath
et al. [PODC'19]---provides $O(\log\log n)$ time algorithms for
$(1+\varepsilon)$-approximate maximum weight matching as well as for
$(2+\varepsilon)$-approximate minimum cardinality vertex cover. However, the
latter algorithm does not work for the general weighted case of vertex cover,
for which the best known algorithm remained at $O(\log n)$ time complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10566"><span class="datestr">at May 23, 2020 10:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10506">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10506">Hardness of Modern Games</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Costa:Diogo_M=.html">Diogo M. Costa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10506">PDF</a><br /><b>Abstract: </b>We consider the complexity properties of modern puzzle games, Hexiom, Cut the
Rope and Back to Bed. The complexity of games plays an important role in the
type of experience they provide to players. Back to Bed is shown to be
PSPACE-Hard and the first two are shown to be NP-Hard. These results give
further insight into the structure of these games and the resulting
constructions may be useful in further complexity studies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10506"><span class="datestr">at May 23, 2020 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10427">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10427">Sparse Tensor Transpositions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mueller:Suzanne.html">Suzanne Mueller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahrens:Peter.html">Peter Ahrens</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chou:Stephen.html">Stephen Chou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kjolstad:Fredrik.html">Fredrik Kjolstad</a>, Saman Amarasinghe <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10427">PDF</a><br /><b>Abstract: </b>We present a new algorithm for transposing sparse tensors called Quesadilla.
The algorithm converts the sparse tensor data structure to a list of
coordinates and sorts it with a fast multi-pass radix algorithm that exploits
knowledge of the requested transposition and the tensors input partial
coordinate ordering to provably minimize the number of parallel partial sorting
passes. We evaluate both a serial and a parallel implementation of Quesadilla
on a set of 19 tensors from the FROSTT collection, a set of tensors taken from
scientific and data analytic applications. We compare Quesadilla and a
generalization, Top-2-sadilla to several state of the art approaches, including
the tensor transposition routine used in the SPLATT tensor factorization
library. In serial tests, Quesadilla was the best strategy for 60% of all
tensor and transposition combinations and improved over SPLATT by at least 19%
in half of the combinations. In parallel tests, at least one of Quesadilla or
Top-2-sadilla was the best strategy for 52% of all tensor and transposition
combinations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10427"><span class="datestr">at May 23, 2020 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10328">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10328">A Unifying Model for Locally Constrained Spanning Tree Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viana:Luiz_Alberto_do_Carmo.html">Luiz Alberto do Carmo Viana</a>, Manoel Campêlo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Ana.html">Ana Silva</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10328">PDF</a><br /><b>Abstract: </b>Given a graph $G$ and a digraph $D$ whose vertices are the edges of $G$, we
investigate the problem of finding a spanning tree of $G$ that satisfies the
constraints imposed by $D$. The restrictions to add an edge in the tree depend
on its neighborhood in $D$. Here, we generalize previously investigated
problems by also considering as input functions $\ell$ and $u$ on $E(G)$ that
give a lower and an upper bound, respectively, on the number of constraints
that must be satisfied by each edge. The produced feasibility problem is
denoted by \texttt{G-DCST}, while the optimization problem is denoted by
\texttt{G-DCMST}. We show that \texttt{G-DCST} is NP-complete even under strong
assumptions on the structures of $G$ and $D$, as well as on functions $\ell$
and $u$. On the positive side, we prove two polynomial results, one for
\texttt{G-DCST} and another for \texttt{G-DCMST}, and also give a simple
exponential-time algorithm along with a proof that it is asymptotically optimal
under the \ETH. Finally, we prove that other previously studied constrained
spanning tree (\textsc{CST}) problems can be modeled within our framework,
namely, the \textsc{Conflict CST}, the \textsc{Forcing CS, the \textsc{At Least
One/All Dependency CST}, the \textsc{Maximum Degree CST}, the \textsc{Minimum
Degree CST}, and the \textsc{Fixed-Leaves Minimum Degree CST}.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10328"><span class="datestr">at May 23, 2020 10:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.10245">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.10245">Further Questions on Oriented Convex Containers of Convex Planar Regions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>R Nandakumar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.10245">PDF</a><br /><b>Abstract: </b>We define an 'oriented convex region' as a convex region with a direction of
symmetry. An earlier article had touched upon isosceles triangles, rectangles
and ellipses. Here, we examine some more possible oriented containers -
semicircles, segments of circles and sectors - and raise some questions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.10245"><span class="datestr">at May 23, 2020 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2005.09152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2005.09152">Lasso formulation of the shortest path problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Anqi.html">Anqi Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Taghvaei:Amirhossein.html">Amirhossein Taghvaei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiou:Tryphon_T=.html">Tryphon T. Georgiou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2005.09152">PDF</a><br /><b>Abstract: </b>The shortest path problem is formulated as an $l_1$-regularized regression
problem, known as lasso. Based on this formulation, a connection is established
between Dijkstra's shortest path algorithm and the least angle regression
(LARS) for the lasso problem. Specifically, the solution path of the lasso
problem, obtained by varying the regularization parameter from infinity to zero
(the regularization path), corresponds to shortest path trees that appear in
the bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the
LARS formulation provide exact solutions, they become impractical when the size
of the graph is exceedingly large. To overcome this issue, the alternating
direction method of multipliers (ADMM) is proposed to solve the lasso
formulation. The resulting algorithm produces good and fast approximations of
the shortest path by sacrificing exactness that may not be absolutely essential
in many applications. Numerical experiments are provided to illustrate the
performance of the proposed approach.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2005.09152"><span class="datestr">at May 23, 2020 10:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=437">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/05/22/tcs-talk-wednesday-may-27-rahul-ilango-mit/">TCS+ talk: Wednesday, May 27 — Rahul Ilango, MIT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Rahul Ilango</strong> from MIT will speak about “<em>Is it (NP) hard to distinguish order from chaos?</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br />
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: The Minimum Circuit Size Problem (MCSP) roughly asks what the “complexity” of a given string is. Informally, one can think of this as determining the degree of “computational order” a string has.</p>
<p>In the past several years, there has been a resurgence of interest in MCSP. A series of exciting results have begun unraveling what looks to be a fascinating story. This story already reveals deep connections between MCSP and a growing list of fields, including cryptography, learning theory, structural complexity theory, average-case complexity, and circuit complexity. As an example, Santhanam recently proved a conditional equivalence between the complexity of MCSP and the existence of one-way functions.</p>
<p>This talk is split into two parts. The first part is a broad introduction to MCSP, answering the following questions: What is this problem? Why is it interesting? What do we know so far, and where might the story go next? The second part discusses recent joint work with Bruno Loff and Igor Oliveira showing that the “multi-output version” of MCSP is NP-hard.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/05/22/tcs-talk-wednesday-may-27-rahul-ilango-mit/"><span class="datestr">at May 22, 2020 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17063">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/22/math-tells/">Math Tells</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>How to tell what part of math you are from</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/unknown-141/" rel="attachment wp-att-17065"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/unknown-1.jpeg?w=300&amp;h=160" class="alignright size-medium wp-image-17065" height="160" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Gerolamo Cardano is often credited with introducing the notion of complex numbers. In 1545, he wrote a book titled <i>Ars Magna</i>. He introduced us to numbers like <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B-5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{-5}}" class="latex" title="{\sqrt{-5}}" /> in his quest to understand solutions to equations. Cardano was often short of money and gambled and played a certain board game to make money—see the second paragraph <a href="https://en.wikipedia.org/wiki/Gerolamo_Cardano#Mathematics">here</a>. </p>
<p>
Today, for amusement, Ken and I thought we’d talk about tells.</p>
<p>
What are tells? Wikipedia <a href="https://en.wikipedia.org/wiki/Tell_(poker)">says</a>: </p>
<blockquote><p><b> </b> <em> A tell in poker is a change in a player’s behavior or demeanor that is claimed by some to give clues to that player’s assessment of their hand. </em>
</p></blockquote>
<p></p><p>
<a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/tell/" rel="attachment wp-att-17068"><img width="201" alt="" src="https://rjlipton.files.wordpress.com/2020/05/tell.jpg?w=201&amp;h=300" class="aligncenter size-medium wp-image-17068" height="300" /></a></p>
<p>
</p><p></p><h2> Other Tells </h2><p></p>
<p></p><p>
Ken and I have been thinking of tells in a wider sense—when and whether one can declare inferences amid uncertain information. Historians face this all the time. So do biographers, at least when their subjects are no longer living. We would also like to make inferences in our current world, such as about the pandemic. The stakes can be higher than in poker. In poker, if your “tell” inference is wrong and you lose, you can play another hand—unless you went all in. With science and other academic areas the attitude must be that you’re all-in all the time.</p>
<p>
Cardano furnishes several instances. Wikipedia—which we regard as an equilibrium of opinions—says that Cardano </p>
<blockquote><p><b> </b> <em> acknowledged the existence of imaginary numbers … [but] did not understand their properties, [which were] described for the first time by his Italian contemporary Rafael Bombelli. </em>
</p></blockquote>
<p></p><p>
This is a negative inference from how one of Cardano’s books stops short of treating imaginary numbers as objects that follow rules. </p>
<p>
There are also questions about whether Cardano can be considered “the father of probability” ahead of Blaise Pascal and Pierre de Fermat. Part of the problem is that Cardano’s own writings late in life recounted his first erroneous reasonings as well as final understanding in a Hamlet-like fashion. Wikipedia doubts whether he really knew the rule of multiplying probabilities of independent events, whereas the <a href="http://www.columbia.edu/~pg2113/index_files/Gorroochurn-Some Laws.pdf">essay</a> by Prakash Gorroochurn cited there convinces us that he did. Similar doubt extends to how much Cardano knew about the natural sciences, as correct inferences (such as mountains with seashell fossils having once been underwater) are mixed in with what we today regard as howlers.</p>
<p>
Every staging of Shakespeare’s <i>Hamlet</i> shows a book by Cardano—or does it? In Act II, scene 2, Polonius asks, “What do you read, my lord?”; to which Hamlet first replies “Words words words.” Pressed on the book’s topic, Hamlet perhaps references the section “Misery of Old Age” in Cardano’s 1543 book <i>De Consolatione</i> but what he <a href="https://www.sparknotes.com/nofear/shakespeare/hamlet/page_102/">says</a> is so elliptical it is hard to tell. The book also includes particular allusions between sleep and death that go into Hamlet’s soliloquy opening Act III. The book had been published in England in 1573 as <i>Cardan’s Comfort</i> under the aegis of the Earl of Oxford so it was well-known. Yet the writer Italo Calvino <a href="https://books.google.com/books?id=pQabBQAAQBAJ&amp;pg=PA77&amp;lpg=PA77&amp;dq=Hamlet+words+words+words+Cardano&amp;source=bl&amp;ots=mu1gcM6b8E&amp;sig=ACfU3U3W7te91qlDaIIC8EeHLM1GxpR2Dw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwikkNjX1MfpAhWChHIEHQaWDZsQ6AEwCXoECAoQAQ#v=onepage&amp;q=Hamlet words words words Cardano&amp;f=false">held back</a> from the inference:</p>
<blockquote><p><b> </b> <em> To conclude from this that the book read by Hamlet is definitely Cardano, as is held by some scholars of Shakespeare’s sources, is perhaps unjustified. </em>
</p></blockquote>
<p></p><p>
To be sure, there are some who believe Shakespeare’s main source was Oxford, in manuscripts if not flesh and blood. One reason we do not go there is that we do not see the wider community as having been able to establish reliable principles for judging what kinds of inferences are probably valid. We wonder if one could do an experiment of taking resolved cases, removing most of the information to take them down to the level of unresolved cases, and seeing what kinds of inferences from partial information would have worked.  That’s not our expertise, but within our expertise in math and CS, we wonder if a little experiment will be helpful.</p>
<p>
To set the idea, note that imaginary numbers are also called complex numbers. Yet the term complex numbers can mean other things. Besides numbers like <img src="https://s0.wp.com/latex.php?latex=%7B2+%2B+3i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 + 3i}" class="latex" title="{2 + 3i}" /> it also can mean how hard it is to <a href="https://en.wikipedia.org/wiki/Integer_complexity">construct</a> a number. </p>
<blockquote><p><b> </b> <em> In number theory, the integer complexity of an integer is the smallest number of ones that can be used to represent it using ones and any number of additions, multiplications, and parentheses. It is always within a constant factor of the logarithm of the given integer. </em>
</p></blockquote>
<p></p><p>
How easy is it to tell what kind of “complex” is meant if you only have partial information?  We don’t only mean scope-of-terminology issues; often a well-defined math object is used in multiple areas.  Let’s try an experiment.</p>
<p>
</p><p></p><h2> Math Tells </h2><p></p>
<p></p><p>
Suppose you <s> walk in</s> log-in to a talk without any idea of the topic. If the speaker uses one of these terms can you tell what her talk might be about? Several have multiple meanings. What are some of them? A passing score is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /></p>
<ol>
<p></p><li>
She says let <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> be a <i>c.e.</i> set.<p></p>
<p></p></li><li>
She says let <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> be in <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" />.<p></p>
<p></p></li><li>
She says by the König principle.<p></p>
<p></p></li><li>
She says <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> is a <i>prime</i>.<p></p>
<p></p></li><li>
She says <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is a <i>prime</i>.<p></p>
<p></p></li><li>
She says <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is <i>solvable</i>.<p></p>
<p></p></li><li>
She says let its <i>degree</i> be <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />.<p></p>
<p></p></li><li>
She says there is a <i>run</i>.<p></p>
<p></p></li><li>
She says it is <i>reducible</i>.<p></p>
<p></p></li><li>
She says it is <i>satisfiable</i>.<p></p>
</li></ol>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your answers? Do you have some tells of your own?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/"><span class="datestr">at May 22, 2020 04:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/081">TR20-081 |  Algebraic Hardness versus Randomness in Low Characteristic | 

	Robert Andrews</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that lower bounds for explicit constant-variate polynomials over fields of characteristic $p &gt; 0$ are sufficient to derandomize polynomial identity testing over fields of characteristic $p$. In this setting, existing work on hardness-randomness tradeoffs for polynomial identity testing requires either the characteristic to be sufficiently large or the notion of hardness to be stronger than the standard syntactic notion of hardness used in algebraic complexity. Our results make no restriction on the characteristic of the field and use standard notions of hardness.

We do this by combining the Kabanets-Impagliazzo generator with a white-box procedure to take $p$-th roots of circuits computing a $p$-th power over fields of characteristic $p$. When the number of variables appearing in the circuit is bounded by some constant, this procedure turns out to be efficient, which allows us to bypass difficulties related to factoring circuits in characteristic $p$.

We also combine the Kabanets-Impagliazzo generator with recent ``bootstrapping'' results in polynomial identity testing to show that a sufficiently-hard family of explicit constant-variate polynomials yields a near-complete derandomization of polynomial identity testing. This result holds over fields of both zero and positive characteristic and complements a recent work of Guo, Kumar, Saptharishi, and Solomon, who obtained a slightly stronger statement over fields of characteristic zero.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/081"><span class="datestr">at May 21, 2020 11:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/080">TR20-080 |  Continuous LWE | 

	Joan Bruna, 

	Oded Regev, 

	Min Jae Song, 

	Yi Tang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce a continuous analogue of the Learning with Errors (LWE) problem, which we name CLWE. We give a polynomial-time quantum reduction from worst-case lattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees to those of LWE. Alternatively, our result can also be seen as opening new avenues of (quantum) attacks on lattice problems. Our work resolves an open problem regarding the computational complexity of learning mixtures of Gaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As an additional motivation, (a slight variant of) CLWE was considered in the context of robust machine learning (Diakonikolas et al.~FOCS 2017), where hardness in the statistical query (SQ) model was shown; our work addresses the open question regarding its computational hardness (Bubeck et al.~ICML 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/080"><span class="datestr">at May 21, 2020 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/079">TR20-079 |  On Minimizing Regular Expressions Without Kleene Star | 

	Hermann Gruber , 

	Markus Holzer, 

	Simon Wolfsteiner</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Finite languages lie at the heart of literally every regular expression. Therefore, we investigate the approximation complexity of minimizing regular expressions without Kleene star, or, equivalently, regular expressions describing finite languages. On the side of approximation hardness, given such an expression of size~$s$, we prove that it is impossible to approximate the minimum size required by an equivalent regular expression within a factor of  $O\left(\frac{s}{(\log s)^{2+\delta}}\right)$ if the running time is bounded by a quasipolynomial function depending on~$\delta$, for every $\delta&gt;0$, unless the exponential time hypothesis (ETH) fails. For approximation ratio~$O(s^{1-\delta})$, we prove an exponential time lower bound depending on~$\delta$, assuming ETH. The lower bounds apply for alphabets of constant size. On the algorithmic side, we show that the problem can be approximated in polynomial time within~$O(\frac{s\log\log s}{\log s})$, with~$s$ being the size of the given regular expression. For constant alphabet size, the bound improves to~$O(\frac{s}{\log s})$. Finally, we devise a familiy of superpolynomial approximation algorithms that attain the performance ratios of the lower bounds, while their running times are only slightly above those excluded by the ETH.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/079"><span class="datestr">at May 21, 2020 02:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/078">TR20-078 |  The New Complexity Landscape around Circuit Minimization | 

	Eric Allender</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We survey recent developments related to the Minimum Circuit Size Problem</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/078"><span class="datestr">at May 21, 2020 02:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4805">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4805">Quantum Computing Lecture Notes 2.0</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Two years ago, I posted detailed <a href="https://www.scottaaronson.com/blog/?p=3943">lecture notes</a> on this blog for my Intro to Quantum Information Science undergrad course at UT Austin.  Today, with enormous thanks to UT PhD student Corey Ostrove, we’ve gotten the notes into a much better shape (for starters, they’re now in LaTeX).  You can <a href="https://www.scottaaronson.com/qclec.pdf">see the results here</a> (7MB)—it’s basically a 260-page introductory quantum computing textbook in beta form, covering similar material as many other introductory quantum computing textbooks, but in my style for those who like that.  It’s missing exercises, as well as material on quantum supremacy experiments, recent progress in hardware, etc., but that will be added in the next version if there’s enough interest.  Enjoy!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Announcement:</span></strong> Bjorn Poonen at MIT pointed me to <a href="https://researchseminars.org/">researchseminars.org</a>, a great resource for finding out about technical talks that are being held online in the era of covid.  The developers recently added CS as a category, but so far there are very few CS talks listed.  Please help fix that!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4805"><span class="datestr">at May 20, 2020 09:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1701">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/05/19/incentive-compatible-sensitive-surveys/">Incentive Compatible Sensitive Surveys</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="580" alt="" src="https://theorydish.files.wordpress.com/2020/05/survey.jpg?w=764" class="wp-image-1703" height="303" /></figure></div>

<p>Crucial decisions are increasingly being made by automated machine learning algorithms. These algorithms rely on data, and without high quality data, the resulting decisions may be inaccurate and/or unfair. In some cases, data is readily available: for example, location data passively collected by smartphones. In other cases, data may be difficult to obtain by automated means, and it is necessary to directly survey the population.</p>

<p>However, individuals are not always motivated to take surveys if they receive no benefit. Offering a monetary reward may incentivize some individuals to participate, but there is a problem with this approach: what if an individual’s data is correlated with their willingness to take the survey?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="457" alt="" src="https://theorydish.files.wordpress.com/2020/05/correlation.png?w=459" class="wp-image-1708" height="184" /></figure></div>

<p>For concreteness, imagine that you are a health administrator trying to estimate the average weight in a population. This is a sensitive attribute that individuals may be reluctant to disclose, especially if their weight is not considered healthy. A generic survey may yield disproportionately more respondents with “healthy” weights, and thus may result an an inaccurate estimate (see, e.g, Shields et al., 2011).</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="408" alt="" src="https://theorydish.files.wordpress.com/2020/05/scale.jpeg?w=1024" class="wp-image-1707" height="234" /></figure></div>

<p>In this post, we discuss three papers which propose solutions to this problem through the lens of <em>mechanism design</em>. The idea is to carefully design payments so that we received an unbiased sample, leading to a hopefully accurate estimate.</p>

<h2><strong>Model</strong></h2>

<p>We use <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" /> to denote agent <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />‘s data (e.g., her weight). We assume that each agent also has a personal cost <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" />, representing her level of reluctance to reveal her data. Agent <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> is willing to reveal <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" /> if and only if she receives a payment of at least <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" />. Our goal is to allocate higher payments to agents with higher <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" />‘s, in order to get an unbiased sample. However, we also must obey an budget constraint: we cannot spend more than <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="B" class="latex" title="B" /> total. The solution is to transact non-deterministically: with some probability, offer to purchase an agent’s data. Agents with higher costs will receive higher payments, but lower transaction probabilities.</p>

<p>We assume that agents are drawn at random independently from some distribution. Our crucial assumption is that we known the marginal distribution of agent costs, which we denote <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> (we will explore later what happens when this assumption is removed). However, we do not know the distribution of <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />, and that distribution can be arbitrarily correlated with <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal{F}" class="latex" title="\mathcal{F}" />. As mentioned above, one might expect agents with less “desirable” <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />‘s may have higher costs, but one can imagine more complex correlations as well.</p>

<p>Our mechanisms consist of two parts: an <em>allocation rule</em> <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A" class="latex" title="A" />, and a <em>payment rule</em> <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P" class="latex" title="P" />. Given <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P" class="latex" title="P" />, the mechanism works as follows:</p>

<ol><li>Ask each agent <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> to report <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" />. Let <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c" class="latex" title="c" /> denote the actual reported cost.</li><li>With probability <img src="https://s0.wp.com/latex.php?latex=A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A(c)" class="latex" title="A(c)" />, we purchase the agent’s data and pay her <img src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P(c)" class="latex" title="P(c)" />. With probability <img src="https://s0.wp.com/latex.php?latex=1+-+A%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="1 - A(c)" class="latex" title="1 - A(c)" />, we do not buy the data, and no payment is made.</li><li>At the end, use the data we learned to form an estimate of the population average of <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />. Let <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{z}" class="latex" title="\bar{z}" /> denote our estimate.</li></ol>

<p>In this model, agent <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />‘s expected utility for reporting <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c" class="latex" title="c" /> is <img src="https://s0.wp.com/latex.php?latex=u_i%28c%29+%3D+A%28c%29+%28P%28c%29+-+c_i%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="u_i(c) = A(c) (P(c) - c_i)" class="latex" title="u_i(c) = A(c) (P(c) - c_i)" />.</p>

<p>We have four main requirements:</p>

<ol><li><strong>Truthfulness. </strong>It should be in each agent’s best interest to truthfully report <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" />.</li><li><strong>Individual rationality. </strong>Agents should not receive negative utility if they are honest, i.e., we should have <img src="https://s0.wp.com/latex.php?latex=P%28c_i%29+%5Cge+c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P(c_i) \ge c_i" class="latex" title="P(c_i) \ge c_i" /> for all <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />.</li><li><strong>Budget constrained. </strong>Our total expected payment should not exceed <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="B" class="latex" title="B" />, i.e., <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Csum_i+A%28c_i%29+P%28c_i%29%5D+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B" class="latex" title="\mathbb{E}[\sum_i A(c_i) P(c_i)] \le B" />.</li><li><strong>Unbiased. </strong>Our estimate isn’t consistently too high or too low. Specifically, the expected value of our estimate should be equal to the true average, i.e., <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Cbar%7Bz%7D%5D+%3D+%5Cmathbb%7BE%7D%5Bz_i%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]" class="latex" title="\mathbb{E}[\bar{z}] = \mathbb{E}[z_i]" />.</li></ol>

<p>Lack of bias doesn’t mean that our estimate is accurate, however. To this end, our primary goal is to <strong>minimize the variance</strong>, subject to the mechanism obeying the four above criteria. We evaluate variance via a worst-case framework: given a mechanism, we wish minimize the variance with respect to the worst-case distribution of agents for that mechanism. The idea is that the distribution of <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />‘s is not known to the mechanism, so we require it to perform well for all distributions.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="438" alt="" src="https://theorydish.files.wordpress.com/2020/05/goal.jpg?w=970" class="wp-image-1729" height="203" />Goal.</figure></div>

<p>When we refer to the “optimal” mechanism, we mean minimum variance, subject to being truthful, individually rational, budget constrained, and unbiased (henceforth TIBU).</p>

<p><strong>The Horvitz Thomspon Estimator</strong></p>

<p>Once we have learned the <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />‘s, how do we actual form an estimate of the mean? Luckily for us, this question has a simple answer. If we restrict ourselves to linear unbiased estimators, there is a unique way to do this, known as the <em>Horvitz-Thompson estimator</em>:</p>

<p><img src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bz%7D+%3D+%5Cdisplaystyle%5Csum%5Climits_i+%5Ccfrac%7Bz_i%7D%7BA%28c_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}" class="latex" title="\bar{z} = \displaystyle\sum\limits_i \cfrac{z_i}{A(c_i)}" /></p>

<p>Thus our task is simply to choose <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A" class="latex" title="A" /> and <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P" class="latex" title="P" />.</p>

<h2>Approach #1</h2>

<p>This model was first considered by Roth and Schoenebeck (2012). They are able to characterize a mechanism which is TIBU and has variance at most <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="1/n" class="latex" title="1/n" /> more than the optimal variance, where <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" /> is the number of agents. However, they do make the strong assumption that <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" /> is either 0 or 1.</p>

<p>Their approach relies on <em>Take-It-Or-Leave-It</em> mechanisms. Such a mechanism is defined by a distribution $G$ over the positive real numbers, and works as follows:</p>

<ol><li>Each agent <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> reports a cost <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c" class="latex" title="c" />.</li><li>Sample a payment <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" /> from <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="G" class="latex" title="G" />.</li><li>If <img src="https://s0.wp.com/latex.php?latex=p+%5Cge+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p \ge c" class="latex" title="p \ge c" />, buy the agent’s data with payment <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" />. If <img src="https://s0.wp.com/latex.php?latex=p+%3C+c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p &lt; c" class="latex" title="p &lt; c" />, do not buy the agent’s data.</li></ol>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="293" alt="" src="https://pbs.twimg.com/media/EDzLmHAWwAAEPkl.jpg" height="212" /></figure></div>

<p>This amounts to an allocation rule <img src="https://s0.wp.com/latex.php?latex=A%28c%29+%3D+1+-+%5Ctext%7BPr%7D%5Bp%5Cge+c%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A(c) = 1 - \text{Pr}[p\ge c]" class="latex" title="A(c) = 1 - \text{Pr}[p\ge c]" />, and a payment rule <img src="https://s0.wp.com/latex.php?latex=P%28c%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="P(c)" class="latex" title="P(c)" /> equal to the distribution <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="G" class="latex" title="G" /> conditioned on being at least <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c" class="latex" title="c" />. The authors show that these mechanisms are fully general, i.e., any allocation and payment rule can be implemented by a Take-It-Or-Leave-It mechanism.</p>

<p>The proof of their main result is primarily based on using the calculus of variations to optimize over the space of distributions <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="G" class="latex" title="G" />. The paper contains some additional results, for example regarding an alternate model where we wish to minimize the budget, but that is outside the scope of this blog post.</p>

<h2>Approach #2</h2>

<p>Although the above result is a great step, it leaves room for improvement. First of all, the assumption that <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" /> is binary is quite strong, and does not apply to our running example of body weight. Secondly, their mechanism does not quite achieve the optimal variance. Chen et al. (2018) remedy both of these concerns. That is, they allow <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" /> to be any real number, and they characterize the TIBU mechanism with optimal variance. Their result also generalizes to more complex statistical estimates, not just the average <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />, and it holds for both continuous and discrete agent distributions.</p>

<p>The approach of Chen et al. (2018) is based on two primary ideas. First, they show that any monotone allocation rule (i.e., we are always less likely to purchase data from an agent with higher cost) can be implemented in a TIBU fashion by a unique payment rule. Thus we only need to identify the optimal allocation rule. (This is similar to the standard result from auction theory about implementable monotone allocation rules (Myerson 1981).)</p>

<p>The second idea is to view the problem as a zero-sum game between ourselves (the mechanism designer) and an adversary who chooses the distribution of agents. Given a distribution, we choose an allocation rule to minimize the variance, and given an allocation rule, the adversary chooses a distribution to maximize the variance.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="292" alt="" src="https://i2.wp.com/marketbusinessnews.com/wp-content/uploads/2016/08/Zero-Sum-Game.jpg?fit=511%2C521&amp;ssl=1&amp;resize=1200%2C1223.4833659491" height="297" /></figure></div>

<p>The authors are able to solve for the equilibrium of this game and thus identify the TIBU mechanism with minimum possible variance.</p>

<h2>Approach #3</h2>

<p>Approach #2 gave us our desired result: a minimum variance mechanism subject to our four desired properties (TIBU), for any distribution of <img src="https://s0.wp.com/latex.php?latex=z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="z_i" class="latex" title="z_i" />‘s. But we are still making a very strong assumption: that we know the distribution of agent costs.</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="427" alt="" src="https://theorydish.files.wordpress.com/2020/05/prior.png?w=500" class="wp-image-1766" height="230" /></figure></div>

<p>Chen and Zheng (2019) do away with this assumption in a follow-up paper. They consider a model where the mechanism has no prior information on the distribution of costs (or on the distribution of data), and $n$ agents arrive one-by-one in a uniformly random order. Each agent reports a cost, and we decide whether to buy her data, and what to pay her. In order to price well, we need to learn the cost distribution, but we must do this while simultaneously making irrevocable purchasing decisions. The main result is a TIBU mechanism with variance at most a constant factor worse than optimal.</p>

<p>The authors note that after each step <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />, the reported costs up to that point induce an empirical cost distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal{F}_i" class="latex" title="\mathcal{F}_i" />. Using the results of Chen et al. (2018), we can determine the optimal mechanism for <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathcal{F}_i" class="latex" title="\mathcal{F}_i" />. The basic idea is to use that mechanism for the current step, learn a new agent cost <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" /> (note that the agent reports <img src="https://s0.wp.com/latex.php?latex=c_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="c_i" class="latex" title="c_i" /> regardless of whether we purchase her data) and then update our empirical distribution accordingly. (The authors actually end up using an approximately optimal allocation rule, but the idea is the same.) The mechanism also uses more budget in the earlier rounds, to make up for the pricing being less accurate.</p>

<h2>Discussion</h2>

<p>In this post, we considered the problem of surveying a sensitive attribute where an agent’s data may be correlated with their willingness to participate. We discussed three different approaches, all of which rely of giving higher payments to agents with higher costs, in order to incentivize them to participate and to obtain an unbiased estimate. The final approach was able to give a truthful, individually rational, budget feasible, and unbiased mechanism with approximately optimal variance, without making any prior assumptions on the distribution of agents.</p>

<p>However, all three of the approaches assume that agents cannot lie about the data. This is reasonable for some attributes, such as a body weight, where an agent can be asked to step onto a physical scale. However, requiring participants come in person to a particular location will certainly lead to less engagement. Furthermore, for other sensitive attributes, there may not be a verifiable way to obtain the data. Future work could investigate alternative models where this assumption is not necessary. For example, perhaps agents do not maliciously lie, but rather are simply inaccurate at reporting their own attributes. For example, research has demonstrated that people consistently over-report height and under-report weight (e.g., Gorber et al., 2007). Could a mechanism learn the pattern of inaccuracy and compensate for that to still obtain an unbiased estimate?</p>

<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="317" alt="" src="https://theorydish.files.wordpress.com/2020/05/bias.png?w=1024" class="wp-image-1775" height="243" /></figure></div>

<h2><strong>References</strong></h2>

<ol><li>Yiling Chen, Nicole Immorlica, Brendan Lucier, Vasilis Syrgkanis, and Juba Ziani. “Optimal data acquisition for statistical estimation.” In <em>Proceedings of the 2018 ACM Conference on Economics and Computation</em>. 2018.</li><li>Yiling Chen and Shuran Zheng. “Prior-free data acquisition for accurate statistical estimation.” <em>Proceedings of the 2019 ACM Conference on Economics and Computation</em>. 2019.</li><li>Sarah Connor Gorber, Mark S. Tremplay, David Moher, and B. Gorber (2007). A comparison of direct vs. self‐report measures for assessing height, weight and body mass index: a systematic review. <em>Obesity reviews</em>, <em>8</em>(4), 307-326.</li><li>Roger Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):58–73. 1981.</li><li>Aaron Roth and Grant Schoenebeck. “Conducting truthful surveys, cheaply.” <em>Proceedings of the 2012 ACM Conference on Electronic Commerce</em>. 2012.</li><li>Margot Shields, Sarah Connor Gorber, Ian Janssen, and Mark S. Tremblay. (2011). Bias in self-reported estimates of obesity in Canadian health surveys: an update on correction equations for adults. <em>Health Reports</em>, <em>22</em>(3), 35.</li></ol>

<p> </p></div>







<p class="date">
by bplaut <a href="https://theorydish.blog/2020/05/19/incentive-compatible-sensitive-surveys/"><span class="datestr">at May 19, 2020 08:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1781">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/05/19/nominations-for-tcs-women-rising-star-talks-at-stoc/">Nominations for TCS Women Rising Star talks at STOC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div>Directly from the organizers:</div>
<div></div>
<div>———–</div>
<div></div>
<div></div>
<div>Dear colleagues,</div>
<div></div>
<div>We invite you to nominate speakers for TCS Women Rising Star talks at STOC 2019, which are planned as part of our virtual TCS Women Spotlight Workshop. To be eligible, your nominee has to be a female or a minority researcher working in theoretical computer science (all topics represented at STOC are welcome) and has to be a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 28th:</div>
<div></div>
<div><a href="https://forms.gle/R9nmit62ESA6V9vv6" target="_blank" rel="noopener">https://forms.gle/R9nmit62ESA6V9vv6</a></div>
<div></div>
<div>STOC 2020 workshops will happen between June 23 and 25, with exact day/time TBD.</div>
<div></div>
<div>You can see the list of speakers from last year here:</div>
<div><a href="https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/" target="_blank" rel="noopener">https://sigact.org/tcswomen/2nd-tcs-women-meeting/tcs-women-2019/</a></div>
<div></div>
<div>Looking forward to your nominations and to seeing you at the our TCS Women Spotlight Workshop,</div>
<div>Barna Saha, Virginia Vassilevska Williams, and Sofya Raskhodnikova</div></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/05/19/nominations-for-tcs-women-rising-star-talks-at-stoc/"><span class="datestr">at May 19, 2020 08:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8975690027864850581">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/05/obit-for-richard-dudley.html">Obit for Richard Dudley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>
Richard M. (Dick) Dudley died on Jan. 19, 2020 (NOT from Coronavirus).You can find obituaries for him  <a href="http://news.mit.edu/2020/richard-dudley-mit-mathematics-professor-emeritus-dies-0218">here</a>, <a href="https://math.mit.edu/about/history/obituaries/dudley.php">here</a>, and <a href="http://www.ams.org/publicoutreach/in-memory/in-memory">here</a> and an interview with him from 2019  <a href="https://projecteuclid.org/euclid.ss/1555056041">here</a>.</div>
<div>
<br /></div>
<div>
<br /></div>
<div>
Professor Dudley worked in Probability and Statistics. His work is now</div>
<div>
being used in Machine Learning. Here is a guest-post-obit by</div>
<div>
David Marcus who had Prof. Dudley as his PhD Thesis Advisor.</div>
<div>
<br /></div>
<div>
-----------------------------------</div>
<div>
<br /></div>
<div>
Guest Blog Obit by David Marcus:</div>
<div>
<br /></div>
<div>
Dick was my thesis advisor at M.I.T. After I got my Ph.D. in 1983, I went</div>
<div>
to work in industry, so did not work closely with him, as some of his other</div>
<div>
students did. But, I enjoyed working with him very much in graduate school.</div>
<div>
<br /></div>
<div>
Dick was very precise. His lecture notes and articles (and later his books)</div>
<div>
said exactly what needed to be said and didn't waste words. In his classes,</div>
<div>
he always handed out complete lecture notes, thus letting you concentrate</div>
<div>
on the material rather than having to take a lot of notes.</div>
<div>
<br /></div>
<div>
Dick was very organized, but his office had piles of papers and journal</div>
<div>
articles everywhere. There is a picture <a href="http://news.mit.edu/2020/richard-dudley-mit-mathematics-professor-emeritus-dies-0218">here</a>.</div>
<div>
<br /></div>
<div>
Before Dick was my advisor, I took his probability course. My orals were</div>
<div>
going to be towards the end of the term, and I was going to use probability</div>
<div>
as one of my two minor areas. So, I spent a lot of time studying the</div>
<div>
material. Dick gave a final exam in the course. The final exam was unlike</div>
<div>
any other final exam I ever took: The exam listed twelve areas that had</div>
<div>
been covered in the course. The instructions said to pick ten and for each</div>
<div>
area give the main definitions and theorems and, if you had time, prove the</div>
<div>
theorems. Since I had been studying the material for my orals, I didn't</div>
<div>
have much trouble, but if I hadn't been studying it for my orals, it would</div>
<div>
have been quite a shock!(COMMENT FROM BILL: Sounds like a lazy way to make up an exam, though on this</div>
<div>
level of may it works. I know of a prof whose final was</div>
<div>
<br /></div>
<div>
Make up 4 good questions for the final. Now Solve them.</div>
<div>
<br /></div>
<div>
)</div>
<div>
<br /></div>
<div>
Once Dick became my advisor, Dick and I had a regular weekly meeting. I'd</div>
<div>
tell him what I'd figured out or what I'd found in a book or journal</div>
<div>
article over the last week and we'd discuss it and he'd make suggestions.</div>
<div>
At some point, I'd say I needed to think about it, and I'd leave. I never</div>
<div>
did find out how long these meetings were supposed to last because I was</div>
<div>
always the one to end them.(COMMENT FROM BILL: It's good someone ended them! Or else you might never</div>
<div>
had graduated :-) )</div>
<div>
<br /></div>
<div>
When I began working with Dick, he said he already had a full</div>
<div>
load of students, but he would see if he had something I could work on. The</div>
<div>
problem Dick came up with for me to work on was to construct a</div>
<div>
counterexample to a theorem that Dick had published. Dick knew his</div>
<div>
published proof was wrong, and had an idea of what a counterexample might</div>
<div>
look like, so suggested I might be able to prove it was a counterexample.</div>
<div>
In retrospect, this was perhaps a risky thesis problem for me since if the</div>
<div>
student gets stuck, the professor can spend time figuring out how to do it.</div>
<div>
But, in this case, presumably Dick had already put some effort into it</div>
<div>
without success. Regardless, with Dick's guidance, I was able to prove it,</div>
<div>
and soon after got my Ph.D.(COMMENT FROM BILL: Sounds risky since if Dick could not do it, maybe it's too hard.)</div>
<div>
<br /></div>
<div>
In 2003 there was a conference in honor of Dick's 65th birthday. All of his</div>
<div>
ex-students were invited, and many of them attended. There was a day of</div>
<div>
talks, and we all went out to dinner (Chinese food, if I recall correctly)</div>
<div>
in the evening. At dinner, I asked Dick if any of his other students had</div>
<div>
written a thesis that disproved one of his published theorems. He said I</div>
<div>
was the only one.(COMMENT FROM BILL: Really good that not only was he okay with you disproving</div>
<div>
his theorem, he encouraged you to!)</div>
<div>
<br /></div>
<div>
<br /></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/05/obit-for-richard-dudley.html"><span class="datestr">at May 19, 2020 06:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/077">TR20-077 |  Factorization of Polynomials Given by Arithmetic Branching Programs | 

	Amit Sinhababu, 

	Thomas Thierauf</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given a multivariate polynomial computed by an arithmetic branching program (ABP) of size $s$, we show that all its factors can be computed by arithmetic branching programs of size $\text{poly}(s)$. Kaltofen gave a similar result for polynomials computed by arithmetic circuits. The previously known best upper bound for ABP-factors was $\text{poly}(s^{\log s}) $.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/077"><span class="datestr">at May 19, 2020 01:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/data_rep_bias/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/data_rep_bias/">Identifying Statistical Bias in Dataset Replication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2005.09619" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/dataset-replication-analysis" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>
<br /></p>

<p><i> We discuss our <a href="https://arxiv.org/abs/2005.09619">paper</a>
on diagnosing bias in dataset replication 
studies. Zooming in on the <a href="https://arxiv.org/abs/1902.10811">ImageNet-v2</a>
reproduction effort, we explain the majority of the accuracy drop between
ImageNet and ImageNet-v2 (from 11.7% to 3.6%) after accounting
for bias in the data collection process. </i></p>

<h2 id="measuring-progress-in-supervised-learning">Measuring Progress in Supervised Learning</h2>
<p>In the last few years, researchers have made extraordinary progress on
increasing accuracy on vision tasks like those in the <a href="http://image-net.org">ImageNet</a>,
<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, and <a href="http://cocodataset.org/#home">COCO</a> datasets. Progress on these tasks is promising, but
comes with an important caveat: the test sets used to measure performance are
finite, fixed, and have been used and re-used by countless researchers over
several years.</p>

<p>There are (at least) two possible ways in which evaluating solely with test-set
accuracy could hinder our progress on the tasks researchers design benchmarks to
proxy (e.g. general image classification for ImageNet). The first of these
issues is adaptive overfitting: since each dataset has only one test
set to measure performance on, algorithmic progress on that (finite and fixed)
test set could be mistaken for algorithmic progress on the distribution from
which the test set was chosen from.</p>

<p>The second issue that could arise is
oversensitivity to irrelevant properties of the test distribution
arising from the dataset collection process; for example, the image encoding
algorithm used to save images.</p>

<p><em>How can we assess whether models are truly making progress on the tasks that
our benchmarks proxy</em>?</p>

<h3 id="dataset-replication">Dataset replication</h3>
<p>A promising approach to diagnosing the two above issues is <em>dataset
replication</em>, in which one mimics the original test set creation process as
closely as possible to make a new dataset. Then, existing models’ performance on
this newly created test set should identify any models that have adaptively
overfit to the original test set. Moreover, since every intricacy of a dataset
collection process cannot be mimicked exactly, natural variability in the
replication should help us unearth cases of algorithms’ oversensitivity to the
original dataset creation process.</p>

<p>The problem of replicating the original test set creation process is harder than it may initially appear. Particularly challenging is controlling for relevant covariates In experimental design, a <a href="https://www.theanalysisfactor.com/confusing-statistical-terms-5-covariate/">covariate</a>is a variable that is not the <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics">independent variable</a> (in our case the choice of dataset), but affects measurements of the <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics">dependent variable</a> (in our case the accuracy).&lt;p&gt;&lt;/p&gt; For example, suppose we wanted to replicate a study about the effect of a certain drug on an age-linked disease. After gathering subjects, we have to reweight or filter them so that the age distribution matches that of the original study, as otherwise the results of the studies are incomparable. This filtering/reweighting step is analogous to dataset replication, with participant age being the relevant covariate., or variables that we expect to have an impact on the outcome (i.e., model accuracy) we measure.</p>

<p>To match covariate distributions between the new reproduction and the original
dataset, we frame dataset replication as a two-step process. In step one, the
replicator collects candidate data using a data pipeline as similar as possible
to that used by the original dataset creators. Then, after approximating the
original pipeline, the dataset replicator should identify the relevant covariate
statistic(s), and choose candidates (via filtering or reweighting the collected
candidate data) so that the distributions of the statistic under the replicated
and original datasets are equal. We call this process statistic
matchingIn causal inference literature, statistic matching is often
referred to as enforcing covariate balance..</p>

<h3 id="imagenet-v2">ImageNet-v2</h3>
<p>In their recent work, <a href="https://arxiv.org/abs/1902.10811">Recht et al.</a> use
dataset replication to take a closer look at model performance on ImageNet, one
of the most widely used computer vision datasets. They first use an apparatus
similar to that of the original <a href="https://arxiv.org/abs/1409.0575">ImageNet
paper</a> to collect a large set of candidate
images and labels from the photo-sharing site <a href="https://www.flickr.com">Flickr</a>.</p>

<p>Then, in the second step, they identify and control for a covariate called the
“selection frequency.” Selection frequency is a measure of how frequently a
(time-limited) human decides that an image (with its candidate label) is
“correctly labeled.” We can get estimate selection frequencies by asking crowd
workers questions like:</p>

<p><br />
</p>

  <img src="https://gradientscience.org/assets/idbias/selfreq.png" />

<p><br /></p>

<p>Selection frequency is a very reasonable (and in some sense, the
“right”) covariate to control for, since the original ImageNet dataset 
was filtered using a similar measureTo construct ImageNet, the
authors scraped many candidate image-label pairs, and just as above, asked
crowd workers whether the image corresponded to the label---filtering was done
via a convincing-majority vote procedure, making the selection frequency a
relevant metric.</p>

<p><a href="https://arxiv.org/abs/1902.10811">Recht et al.</a> obtain empirical selection
frequency estimates for the ImageNet validation set and their collected
candidate pool by presentingThe figure above is simplified for
clarity—in reality, datapoints are presented to the crowd workers as
groups of 48 images, all corresponding to the same candidate label, and
annotators are asked to select the images in that group that truly correspond to
the
label. each (image, label) pair to 10 crowd workers.</p>

<p>By filtering on the selection frequency in various ways, the authors come up
with three new <code class="language-plaintext highlighter-rouge">ImageNet-v2</code> datasets. One of these, called
<code class="language-plaintext highlighter-rouge">MatchedFrequency</code>, is a “true” dataset replication in the sense that it tries
to control for the selecion frequency by filtering the candidate images and
labels so that the resulting selection frequency distribution matches that of
the original ImageNet validation set. 
Since our focus is on dataset
replication, from now on we’ll ignore the other datasets, and use
“MatchedFrequency” and “ImageNet-v2” interchangeably. For convenience, we’ll
also use “ImageNet” and “ImageNet-v1” interchangeably to refer to the original
ImageNet validation setSince the test set is not released, the
ImageNet validation set usually acts as a de facto test set, and the two terms
(validation and test) are used interchangeably to refer to the validation
set..</p>

<p>The key observation made by the ImageNet-v2 authors—and the one that we’ll
focus on in this post—is a <em>consistent</em> drop in accuracy that models suffer
when evaluated on ImageNet-v2 instead of ImageNet:</p>

<p><br /></p>
<canvas width="400" id="v1v2_orig" height="200"></canvas>
<p><br /></p>

<p>Each dot in the scatter plot above represents a model—the $x$ coordinate is
the model’s ImageNet accuracy, and the $y$ coordinate is the model’s accuracy
when evaluated on ImageNet-v2. (You can mouse over the dots to see
model names!) Ideally, since the generating pipelines for the two datasets are
similar, and the relevant covariates were matched, one would expect all of the
models to fall on the dotted line $y = x$. Yet, across the examined models,
their accuracies dropped by an average of 11.7% between ImageNet and
ImageNet-v2.</p>

<h2 id="our-findings">Our Findings</h2>
<p>The significant accuracy drop above presents an empirical mystery given the
similarity in data pipeline between the two datasets. Why do classifiers perform
so poorly on ImageNet-v2?</p>

<p>In our <a href="https://arxiv.org/abs/2005.09619">work</a>, we identify an aspect of the dataset reproduction process
that might lead to a significant accuracy drop. The general phenomenon we
identify is that in dataset replication, even mean-zero noise in measurements of
the relevant covariate/control variable can result in significant bias in the
resulting matched dataset if not accounted for.</p>

<p>In the case of ImageNet-v2, we show how noisy readings of the selection
frequency statistic can result in bias in the ImageNet-v2 dataset towards lower
selection frequency and consequently, lower accuracyRecht et al. observed previously that changes in selection frequency affect model performance..</p>

<p>After accounting for this bias and thus controlling for selection
frequency, we estimate that the adjusted ImageNet to ImageNet-v2
accuracy gap is less than or equal to 3.6% (instead of the initially observed
11.7%).</p>

<p>In the next few sections, we’ll focus on ImageNet-v2, first trying to get a
clearer picture of the source of statistical bias in this dataset replication
effort, and then discussing ways to correct for it.</p>

<h3 id="identifying-the-bias">Identifying the bias</h3>

<p>Earlier, we decomposed the data replication process into two steps: (1)
replicating the pipeline, and (2) controlling for covariates via statistic
matching. 
To perform the latter of these two steps, one
first needs to find the distribution of the relevant statistic (e.g., selection
frequency, in the case of ImageNet) for both the original test set as well as
the newly collected data. 
Just as with any empirical statistic, however, we can’t 
read the true selection frequency $s(x)$ for any given image $x$. We can only sample from a binomial random variable,</p>

<p>\[
    \widehat{s}(x) = \frac{1}{n}\text{Binomial}(n, s(x)),
\]</p>

<p>to obtain a selection frequency estimate, i.e., where $n$
is the number of crowd workers used.</p>

<p>Now, even though the expected value of the measurement we get is indeed $s(x)$
(i.e., $\widehat{s}(x)$ is an unbiased estimator of $s(x)$), 
the reading itselfThis is not the same as the underlying selection frequency for an image; for example, a
image-label pair $x$ might have a true selection frequency of $s(x) = 0.645$,
meaning that on average a crowd annotator is 64.5% likely to say that the image
corresponds to the label. If we ask 10 crowd annotators to label the pair,
however, we never observe this 64.5% number. will be $k/n$ for some integer $k$ (with $n=10$, in the
case of the ImageNet-v2 replication process).</p>

<p>To see why this seemingly innocuous fact impacts the dataset replication
process, suppose we knew exactly what the distribution of true selection
frequency looked like for both the original ImageNet test set and the candidate
images collected through the ImageNet-v2 replication pipeline:</p>

<p><br /></p>
<canvas id="priorCtx"></canvas>
<p><br /></p>

<p>Suppose now that we estimate a selection frequency of $\widehat{s}(x) = 0.7$ for
a given image $x$ (i.e., seven of the ten workers who were shown the image and
its candidate label, marked it as correctly labeled). The key observation here
is that given this empirical selection frequency, the most likely value for
$s(x)$ is not $\widehat{s}(x)$ (even though $\widehat{s}(x)$ is an unbiased
estimator of $s(x)$). Instead, the maximum likelihood estimate actually depends
on which dataset $x$ is from!</p>

<p>If $x$ was sourced from ImageNet, it’s more likely that $s(x) &gt; 0.7$ and
therefore that $\widehat{s}(x)$ is an underestimate, since most of the mass of
the ImageNet selection frequency distribution—see our (hypothetical) plot
above—is on $s(x) &gt; 0.7$. Conversely, if $x$ was a newly collected candidate
image, then most likely $s(x) &lt; 0.7$.</p>

<div class="footnote">
<strong>Remark:</strong> Mathematically, the phenomenon is that
$E[\widehat{s}(x)|s(x)] = s(x)$, but $E[s(x)|\widehat{s}(x)]$ is not equal to $s(x)$, and
instead depends on the distribution of $s(x)$ via Bayes'
ruleSpecifically, we have that for a given dataset,
$$p(s|\widehat{s}) = \frac{p_{data}(s) \cdot p_{binom}(\widehat{s}|s)} {\int
p(s') p_{binom}(\widehat{s}|s')\ ds'},$$ where $p_{data}$ is the density function
for selection frequencies under this dataset, and $p_{binom}$ is the binomial
probability mass function..
</div>

<p>The interactive graph below visualizes this intuition: by moving the
slider, you can adjust the “observed” selection frequency $\widehat{s}(x)$. The 
shaded curves then show what our belief about the corresponding true
selection frequency $s(x)$ looks like, depending on which pool the image
came from.</p>

<p><br /></p>
<canvas id="posteriorCtx"></canvas>
<div class="slider">
    <label for="updateObs">Observed selection frequency <br /> (Current:
    <span id="updateObsVal"></span>/20)</label>
    <div>
    <span class="inplabel">Hard (0/20)</span>
    <input min="0" max="20" value="14" step="1" type="range" id="updateObs" />
    <span class="inplabel">Easy (20/20)</span>
    </div>
</div>
<p><br /></p>

<p>Thus, if the selection frequency distribution of ImageNet is skewed upwards
compared to that of the collected candidate images (which is likely, since
ImageNet was already filtered for qualityImageNet was
originally constructed by collecting candidate images in a way similar to the
ones described here, and then subsequently filtering for quality using a
"majority vote" based on selection frequency. when it was
constructed), matching the two data sources via observed selection frequency
will result in ImageNet images having systematically higher selection
frequencies than their new ImageNet-v2 counterparts. Also, the noiser our
reading of observed selection frequency are (i.e. the less annotators we use),
the more important the source distribution becomes, and so the greater the
effect of the bias. Indeed, if we were able to use infinite annotators for each
image, the bias would disappear, as we would have perfect readings of the
corresponding selection frequency. The only possible value for $s(x)$ would then
be $s(x) = \widehat{s}(x)$, making the shaded distributions above collapse
into a point mass at the green line.</p>

<p>The bias we are describing is summarized visually in the interactive graph below: by adjusting the
sliders, you can manipulate the distributions of “true” selection frequency for
ImageNet (red) and for the candidate data (black), as well as the number of
annotators used to estimate selection frequencies. The distribution of
ImageNet-v2 selection frequencies resulting from performing statistic matching
is shown in blue. Notice that as long as the candidate selection frequencies do
not come from the same distribution as the ImageNet selection frequencies,
the resulting ImageNet-v2 test set never matches the original test set
statistics. For the reasons we discussed earlier, having less annotators, or having a bigger gap between ImageNet and the candidate
distribution, also exacerbate the effect.</p>


<div class="chartholder">
<div class="chartdata">Mean IN/IN-v2 difference: <span id="gap"></span>% </div>
<canvas width="400" id="myChart" height="200"></canvas>
</div>
<div class="caption">
An interactive graph depicting the source of bias in the ImageNet-v2
generation process. Interact with the sliders below to change the
"easiness" (selection frequency) of the ImageNet-v1 and candidate image datasources,
and the number of annotators used to measure selection frequency (click the
legend to show/hide lines).
</div>
<div class="slider">
    <label for="updateV1">ImageNet selection frequency</label>
    <div>
    <span class="inplabel">Hard</span>
    <input min="2" max="9" value="8" step="0.5" type="range" id="updateV1" />
    <span class="inplabel">Easy</span>
    </div>
</div>
<div class="slider">
    <label for="updateFlickr">Candidates selection frequency</label>
    <div>
        <span class="inplabel">Hard</span>
        <input min="2" max="9" value="3" step="0.5" type="range" id="updateFlickr" /> 
        <span class="inplabel">Easy</span>
    </div>
</div>
<div class="slider">
    <label for="updateNumWorkers">Number of workers</label>
    <div> 
    <span class="inplabel">1</span>
    <input max="100" type="range" id="updateNumWorkers" min="1" /> 
    <span class="inplabel">100</span>
    </div>
</div>
<div class="slider">
    <button> Reset to typical values </button>
    <button> Show / hide <em>s̑</em> distributions</button>
</div>


<h2 id="quantifying-the-effects-of-bias">Quantifying the Effects of Bias</h2>

<p>The model above, paired with the fact that ImageNet (having already been
filtered for quality before) is likely much higher-quality than candidate images sourced from
Flickr, predicts that ImageNet-v2 images’ selection frequencies are
consistently lower than those of ImageNet images. To test this
theory, we set up another crowdsourced task that is extremely similar (but
not quite identicalWe implemented a few changes for quality control but kept
the task instructions and interface constant: the exact differences are outlined in Appendix
B.2 of our paper.) to the one used by the ImageNet-v2
creators, this time using 40 annotators per image (instead of 10) to estimate
its selection frequency. A histogram of the selection frequencies we 
observed in our experiment is shown below:</p>

<p><br /></p>

  <img src="https://gradientscience.org/assets/idbias/selfreq_histogram.png" />

<p><br /></p>

<p>Even though the ImageNet-v2 creators report average selection
frequencies for ImageNet and ImageNet-v2 of 0.71 and 0.73 respectively, our
new experiments yield average selection frequencies of 0.85 and 0.81; note the change
in relative ordering (why are the selection frequencies
higher?We discuss this in depth in Appendix B.2 of our
paper. The task and instructions are the same, so we hypothesize that the
discrepancy boils down to either (a) data quality: we used <a href="https://blog.mturk.com/tutorial-managing-worker-cohorts-with-qualifications-e928cd30b173">worker
qualifications</a> to ensure annotator quality while the original experiment did
not—qualifications have been shown to reduce the share of low-quality or
inattentive crowd workers from 34% to 2% in other studies (e.g., the
study we reference found that 16%/0.4% of workers without/with qualifications reported
having had a <em>fatal</em> heart attack while watching television); or (b) data makeup:
workers are presented with grids of 48 images at a time in both experiments,
with grids containing a mix of ImageNet, ImageNet-v2, and candidate
images—but the exact proportions of this mix differ between the two
experiments.).</p>

<div class="footnote"><strong>Aside</strong>: Why did we need to run a new
crowdsourced study to observe this gap, instead of using the data already
collected for the ImageNet-v2 study? The answer is finite-sample reuse:
the selection frequencies collected in the original study are
precisely the ones used to filter the ImageNet-v2 dataset. So, by
construction, these selection frequency will match the selection
frequencies of the ones of ImageNet test set, regardless of whether
there is bias in the selection processTo draw a crude analogy here,
suppose that instead of matching image datasets we are matching piles of coins: 
Pile A is rigged $P(\text{heads})=1$, but Pile B is fair $P(\text{heads}) =
0.5$. We flip all the coins in both piles 10 times each---inevitably (if
there are enough total coins in Pile B), some of the Pile B coins will land
"heads" all 10 times, and will thus appear identical to the rigged Pile A
coins. Are they in fact identical? (After all, these coins match the Pile A
coins according to the "number of heads" statistic!) The answer is obviously
"no," but the key is that even though all the coins in Pile B are
fair (and flipping them another 10 times would reveal this), it's impossible to
conclude anything other than $P(\text{heads}) = 1$ solely from the
already-collected data on the selected coins.. If we're
careful about avoiding this finite-sample reuse (for example, by
re-performing the filtering process using half of the annotators and then
measuring selection frequencies with the other half) we can actually identify
bias in the original data—the process for doing so
is shown in Appendix C of <a href="https://arxiv.org/abs/2005.09619">our paper</a>.</div>

<h3 id="how-does-this-bias-affect-measured-accuracy">How does this bias affect measured accuracy?</h3>

<p>Our model and experiments suggest that matching empirical statistics from
different sources introduces bias into the dataset replication pipeline, and
that in the case of ImageNet-v2 this means that selection frequencies are
actually lower for the new ImageNet-v2 test set compared to the old one. Since
selection frequency is meant to roughly reflect data quality, and is known (as
found already by the ImageNet-v2 authors) to affect model accuracy, we expect the
downwards selection frequency bias in ImageNet-v2 to directly translate into a downwards
bias in model accuracy.</p>

<p>To test if this is really the case, we use a progressively increasing number of
annotators $n$ out of the 40 that we collected. For each $n$, we matchWe perform this matching via reweighting, rather than
filtering---more details are given in the next section. the
ImageNet-v2 observed selection frequencies (calculated using $n$ annotators) to
the ImageNet onesFor context, the ImageNet-v2 creators
matched their candidate pool to ImageNet with $n = 10$, and measure the
resulting model accuracies.
Our statistical model predicts that more annotators means less noise in the
observed selection frequencies, which in turn means less bias and higher model accuracies
in the original selection frequency used to create ImageNet-v2, and so we
should see the resulting model accuracies
increase. The data confirms this prediction: below we plot model accuracies on
ImageNet versus their adjusted accuracies on ImageNet-v2—using the slider
below the graph, you can vary the number of annotators used to make the
adjustment from zero (i.e. no matching, just raw ImageNet-v2 accuracies) to 40
(accuracies after statistic matching using all 40 annotations).</p>

<p><br /></p>
<div class="chartholder">
    <canvas id="v1v2_varying"></canvas>
</div>
<div class="slider">
    <label for="updateNumWorkersAdjAcc">Number of annotators <br /> 
    (Current: <span id="updateNumWorkersAdjAccVal">0</span>)</label>
    <div> 
    <span class="inplabel">1</span>
    <input min="0" max="40" value="0" step="5" type="range" id="updateNumWorkersAdjAcc" /> 
    <span class="inplabel">40</span>
    </div>
</div>
<p><br /></p>

<p>After using 40 workers to control for selection frequency
between ImageNet and ImageNet-v2, we reduce the 11.7% gap that was originally observed
to a gap of 5.7%. This is already a significant reduction, but the trend of the
graph suggests that 5.7% is still an overestimate—the gap continues to
consistently shrink with each increase in number annotators. In the final part
of this post, we’ll use a technique from classical statistics to get
an even better estimate of the real, bias-adjusted gap between ImageNet and
ImageNet-v2 model accuracies.</p>

<h2 id="adjusting-for-bias-with-the-statistical-jackknife">Adjusting for Bias with the Statistical Jackknife</h2>

<p>The statistic matching that led to the previous graph was based on what we’ll call a
<em>selection frequency-adjusted accuracy</em> estimator, defined for a given
classifier $f$ as:</p>

<p>\[
    \text{Acc}(n) = \sum_{k=1}^n 
    E_{x\sim \text{ImageNet-v2}}\left(1[\text{$f$ is correct on }x] | \widehat{s}(x) = \frac{k}{n}\right)\cdot 
    P_{x\sim \text{ImageNet}}\left(\widehat{s}(x) = \frac{k}{n}\right)
\]</p>

<p>This estimator has a simple interpretation: it is equivalent to
(a) sampling an ImageNet-v1 image and observing its (empirical) selection frequency;
then (b) finding a random ImageNet-v2 image with the same (empirical) selection
frequency, and recording the classifier’s correctness on that input. So what we are
estimating here is what a model’s accuracy on ImageNet-v2 would be, if the
selection frequencies of ImageNet-v2 were distributed as in the ImageNet test
set. (Notice that if the ImageNet and ImageNet-v2 selection frequency
distributions already matched, then this estimator would be independent from $n$
and would evaluate to exactly model accuracy on ImageNet-v2.)</p>

<p>Now, the $\text{Acc}(n)$ estimator is subject to the same bias as dataset
replication itself, as it too ignores the discrepancy between the empirical
selection frequency $\widehat{s}(x)$ and the true selection frequency $s(x)$.
Since we’ve been talking about bias pretty abstractly in this post, it’s worth
noting that the $\text{Acc}(n)$ estimator ties everything back to the formal,
statistical definition of bias. Specifically, our main finding can be restated
(though maybe less intuitively) as “$\text{Acc}(n)$ is a downwards-biased
estimator of the true reweighted accuracy,” that is</p>

<p>\[
    E[\text{Acc}(n)] &lt; \lim_{n\rightarrow\infty} \text{Acc}(n). 
\]</p>

<p>So, from this perspective, our graph in the previous section can be viewed as just a plot of the value of $\text{Acc}(n)$ for every
classifier for various values of $n$. Also, the estimator behaves exactly how as predicted by
our model of the bias—as $n$ increases, $\widehat{s}(x)$ becomes a less
noisy estimator of $s(x)$, so the bias in the matching process decreases and
so $\text{Acc}(n)$ increases.</p>

<p>Now, what we really want to know is what $\text{Acc}(n)$ looks like as $n
\rightarrow \infty$, especially given that even when we use 40 annotators for
statistic matching the adjusted accuracy still improves.</p>

<p>In <a href="https://arxiv.org/abs/2005.09619">our paper</a> we present further techniques for tackling this problem.
These include making use of tools from empirical Bayes estimation techniques,
beta-binomial regression, and kernel density estimation. To keep things short
here, we’ll only discuss the simplest estimation method we use: one based on a
technique known as the statistical jackknife.</p>

<p>The jackknife dates back to the work of <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1015.9344&amp;rep=rep1&amp;type=pdf">Maurice Quenouille</a> and <a href="https://projecteuclid.org/euclid.aoms/1177706647">John
Tukey</a> in the 1950s, and provides a way to estimate the bias of anyTechnically, certain
mild assumptions, such as the estimator being (statistically) consistent and having
bias that is analytic in $1/n$, are needed. statistical estimator. In
short, the jackknife estimate of an $n$-sample estimator
$\Theta_n(X_1,\ldots,X_n)$ is given by</p>

<p>\[
    b_{jack}(\widehat{\theta}_n) 
    = (n-1)\cdot \left(\frac{1}{n}\sum_{i=1} 
    \widehat{\theta}_{n-1}^{(i)} - \widehat{\theta}_n 
    \right),
\]</p>

<p>\[
    \text{where}\qquad \widehat{\theta}_{n-1}^{(i)} =
    \widehat{\theta}(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n) \text{ is the $i$th
    leave-one-out estimate.}
\]</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input type="checkbox" id="ac-1" name="accordion-1" />
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"></span>  How does the jackknife work? (click to expand)</label>
<article class="small">
A brief summary: consider our $n$-sample estimator $\Theta_n$, and define
$\Theta$ to be the true value of the estimator, i.e. $\lim_{n\rightarrow\infty}
\Theta_n$. In our case, $\Theta$ would be the adjusted accuracy of ImageNet-v2
if we had infinite workers. Now, suppose that the bias in $\Theta_n$ is on
the order of $1/n$, i.e.

\[
    E[\Theta_n] = \Theta + \frac{b(\Theta)}{n}
\]

Now, for a given n-sample estimate $\widehat{\Theta}_n$, we define a
<em>leave-one-our</em> estimator $\widehat{\Theta}_n^{(-i)}$ to be $\widehat{\Theta}_n$ 
computed with all but the $i$th datapoint. There are $n$ possible such leave-one-out 
estimators for a given $\widehat{\Theta}_n$. The main idea behind the jackknife
is to make the following two approximations:

\[
    \widehat{\Theta}_n \approx E[\Theta_n] = \Theta + \frac{b(\Theta)}{n}
\]
\[
    \frac{1}{n} \sum_{i=1} \widehat{\Theta}_n^{(-i)} \approx E[\Theta_{n-1}] = \Theta + \frac{b(\Theta)}{n-1}
\]

Using these two assumptions, one can solve for $b(\Theta)$ and end up with the
bias estimate given in the post.
</article>
</div>
</div>
</section>

<p>Averaging across all of the classifiers studied, the jackknife estimates the
bias in our estimator as about 1.0%, meaning that the bias-corrected gap
between ImageNet and ImageNet-v2 shrinks from 11.7% (without any correction) to
5.7% (using the 40 annotators we have to correct), to 4.7% (using this additional
jackknife bias correction). Moreover, as our paper discusses, this is almost
certainly still an overestimate—using more refined methods for bias estimation
reduces the gap to somewhere between 3.4% and 3.8% (with variation being across
different methods).</p>

<h2 id="summary-and-conclusions">Summary and Conclusions</h2>
<p>We find that noise—even if it is mean-zero—can result in bias in dataset reproductions if not
accounted for. Zooming in on the <code class="language-plaintext highlighter-rouge">ImageNet-v2</code> replication effort, we use
statistical modeling to find that majority of the observed accuracy drop can be
explained by differences in selection frequency distribution between the
original dataset and its replication. Our results suggest that statistic modeling
of the data collection process can be an important tool in data replication efforts.
For more details check out <a href="https://arxiv.org/abs/2005.09619">our paper</a>, where we discuss precise modeling
procedures, further experiments, and future directions.</p></div>







<p class="date">
<a href="https://gradientscience.org/data_rep_bias/"><span class="datestr">at May 19, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/">SIAM Symposium on Simplicity in Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 11-12, 2021 Westin Alexandria Old Town, Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa21 Submission deadline: August 12, 2020 Registration deadline: December 7, 2020 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are … <a href="https://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">SIAM Symposium on Simplicity in Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/05/18/siam-symposium-on-simplicity-in-algorithms/"><span class="datestr">at May 18, 2020 09:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/076">TR20-076 |  The Round Complexity of Perfect MPC with Active Security and Optimal Resiliency | 

	Benny Applebaum, 

	Eliran Kachlon, 

	Arpita Patra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In STOC 1988, Ben-Or, Goldwasser, and Wigderson (BGW) established an important milestone in the fields of cryptography and distributed computing by showing that every functionality can be computed with perfect (information-theoretic and error-free) security at the presence of an active (aka Byzantine) rushing adversary that controls up to $n/3$ of the parties.

We study the round complexity of general secure multiparty computation in the BGW model. Our main result shows that every functionality can be realized in only four rounds of interaction, and that some functionalities cannot be computed in three rounds. This completely settles the round-complexity of perfect actively-secure optimally-resilient MPC, resolving a long line of research.

Our lower-bound is based on a novel round-reduction technique that allows us to lift existing three-round  lower-bounds for verifiable secret sharing to four-round lower-bounds for general MPC. To prove the upper-bound, we develop new round-efficient protocols for computing degree-2 functionalities over large fields, and establish the completeness of such functionalities. The latter result extends the recent completeness theorem of Applebaum, Brakerski and Tsabary (TCC 2018, Eurocrypt 2019) that was limited to the binary field.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/076"><span class="datestr">at May 17, 2020 04:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7732">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/05/17/foundations-of-responsible-computing/">Foundations of responsible computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>[Hat tip: Aaron Roth]</p>



<p>The inaugural <a href="https://responsiblecomputing.org/">conference on the foundations of responsible computing</a> will take place in less than two weeks (June 1-2). Registration is FREE but you need to register by May 28.</p>



<p></p>



<p>The <a href="https://responsiblecomputing.org/program/">program</a> looks fantastic, and includes keynotes by <strong>Adrian Weller</strong>, <strong>Rakesh Vohra</strong>, <strong>Patricia Williams</strong>, and <strong>Jon Kleinberg</strong>,  as well as a set of (very interesting, judging by the titles) contributed talks.</p>



<p></p>



<p></p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/05/17/foundations-of-responsible-computing/"><span class="datestr">at May 17, 2020 01:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17042">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/16/the-2020-y-prize/">The 2020 Y Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Can one maintain privacy while publicly congratulating?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/05/x-facemaskman.jpg"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2020/05/x-facemaskman.jpg?w=150&amp;h=129" class="alignright size-thumbnail wp-image-17044" height="129" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from X Facemask <a href="https://www.anime-remix.com/product/face-mask-cotton-cloth-designer-graphic-x/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
X, who shall go nameless here, recently won a prize for research. </p>
<p>
Today we congratulate X and wish to talk a bit about their work while safeguarding X’s identity.</p>
<p>
X also won another prize Z—but we will not mention which one. In both cases the honor is about the creation of a new field rather than the solution of a long-standing open problem. Her—OK, X is a she—brilliant work has allowed many others to write papers in these areas. And it has led to countless conferences, meetings, and talks. </p>
<p>
Oops, we used the word “brilliant.” That has already leaked her identity, given where you are reading this. Just type it into our search box.</p>
<p>
We assume you get the point. One can leak information about people without mentioning them directly. </p>
<p></p><p>
<br /><br />
<br /><br />
</p><h2> The Real Start of the Post</h2><p></p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/05/dworkaf.jpg"><img width="180" alt="" src="https://rjlipton.files.wordpress.com/2020/05/dworkaf.jpg?w=180&amp;h=138" class="alignright wp-image-17045" height="138" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Her <a href="https://royalsociety.org/science-events-and-lectures/2018/06/you-and-ai-fairness/">work</a> in Algorithmic Fairness too</font><br />
</td></tr></tbody>
</table>
<p>
Cynthia Dwork is the <a href="http://www.sigact.org/prizes/knuth/citation2020.pdf">winner</a> of the 2020 Donald E. Knuth Prize. The prize is awarded jointly by ACM SIGACT and the IEEE Computer Society Technical Committee. The ceremony will include a lecture by Cynthia at <a href="https://focs2020.cs.duke.edu/">FOCS 2020</a>, which is still projected to meet in-person at Duke next November 16–19.</p>
<p>
Today we congratulate Cynthia and talk a little about her work.</p>
<p>
I, Dick, have known her for many years. I am excited to see that she has won the Knuth prize. </p>
<p>
She has always been a top researcher and a terrific speaker. I fondly recall some of her earlier talks at Princeton and elsewhere. There is something about her ability to explain complex ideas in a lucid manner. I have often thought that this ability must be one of the reasons she has been so successful in her research. </p>
<p>
We will talk a little about <em>differential privacy</em> (DP) and then her other joint work which led to another prize. We should be quick to <a href="https://rjlipton.wordpress.com/2015/02/06/cynthia-dwork-and-a-brilliant-idea/">mention</a> that <a href="https://rjlipton.wordpress.com/2015/02/07/still-a-brilliant-idea/">others</a> have been instrumental in both developments.</p>
<p>
</p><p></p><h2> Quantifying an Idea </h2><p></p>
<p></p><p>
DP might have been appreciated long before there were large databases and computer systems. The genius is not just the simplicity of the idea but the development of mathematical tools to govern its application.</p>
<p>
For a simple example, suppose a professor has a general policy of posting exam average scores in her class of 100. Suppose she posts that the average is 73.3 after 98 students have taken the exam but two need to make it up a week later. Then suppose she re-posts the average after they take it and it is 72.5. The difference may not seem like much. But the class learns two things: </p>
<ul>
<li>
Both students had their makeup scores included in the average. (If the 73.3 is exact then it is not even possible to get a rounded 72.5 with just one score of zero, pending the other.) <p></p>
</li><li>
The two students collectively bombed the exam. (Just possibly one got a zero while the other did OK, but even then the other’s score was below average.)
</li></ul>
<p>
Your first thought might be that the fault is in the excessive precision in the grade reporting. If she had rounded to the nearest integer then both averages would have been reported as “73” and there would be no problem. But rounding is accidental. The new average could have been 72.49, which would be rounded down to “72,” when the set of possible interpretations is worse than before. </p>
<p>
In wide-scale situations where queries for multiple averages at different times are allowed, effects of rounding may cancel so that a large statistical analysis can subtract them out. Thus passively reducing precision is not enough. The key is to affect the precision actively by adding a random <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm \epsilon}" class="latex" title="{\pm \epsilon}" /> to the reported results, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> is judiciously chosen. The point is:</p>
<blockquote><p><b> </b> <em> Adding <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\pm \epsilon}" class="latex" title="{\pm \epsilon}" />, better than rounding, makes inference much harder. </em>
</p></blockquote>
<p>In the exam-score example, randomly adding or subtracting a number between 0 and 2 changes the calculus so that seeing “73.3” and then “72.5” does not allow a high-confidence inference about the makeup students’ scores. One could reason that the first number could easily have been 72.3 and the second 73.5, in line with interpretations where the makeup students upped the average. Actually, a jump from 72.3 to 73.5 in the <em>true</em> averages is impossible from just two new scores, so the class could deduce that the reported averages are fudged. But that is OK. Two basic purposes of reporting the averages are met despite the fudging: </p>
<ul>
<li>
to show the exam was consistent with past years—especially good to know this year; <p></p>
</li><li>
to give you some idea (knowing your own score exactly) where you stand relative to the class.
</li></ul>
<p>
In related situations, one could argue that a mean of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 1}" class="latex" title="{\pm 1}" /> for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> is not enough. If the exam has 100 one-point questions each with a 73.3% expectation of getting it right for everyone, then the standard deviation about the mean is almost <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. Using a mean change of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 2}" class="latex" title="{\pm 2}" /> seems also fine in this case, whereas rounding to the nearest 05 (i.e., reporting “70” or “75” as the average) might be too coarse.</p>
<p>
We do not intend our example to be definitive, only that it conveys relevant issues. The example is enough to convey that DP leads into deep matters in statistics and <em>what can be inferred by statistical methods</em>. The latter melds the complexity-theoretic notion of knowledge gain from interactive protocols with statistical (machine) learning. And as with quantum computing, theorems and methods from the subfield have come back to inform fundamental questions about complexity (of course, this is true of crypto in general).</p>
<p>
</p><p></p><h2> Privacy and Online Chess </h2><p></p>
<p></p><p>
I, Ken, am the lead on this post, and have had data-privacy affect me personally. The pandemic has forced the cancellation of all over-the-board (OTB) tournaments, some as far ahead as August. Chess has all moved online, as heralded by <a href="https://www.nytimes.com/2020/05/08/sports/coronavirus-chess-online-tournament.html">two</a> recent <a href="https://www.nytimes.com/2020/04/23/sports/chess-cheating-south-dakota-races.html">articles</a> in the New York Times and one in the Wall Street Journal <a href="https://www.wsj.com/articles/forget-sourdough-everyone-is-playing-chess-now-11588845643">proclaiming</a>, “Sports: Chess Is the New King of the Pandemic.” </p>
<p>
As these articles also note, cheating has long been a major concern for online chess. I have been a recourse for second-opinions in online cheating cases but have not tried to be <em>primary</em> in this domain. A great conversation in November 2006 with Daniel Sleator (who—besides being famous for splay trees and amortized analysis—<a href="https://en.wikipedia.org/wiki/Internet_Chess_Club#History">co-founded</a> the Internet Chess Club) told me how online cheating detection avails much information about the manner of play that is not applicable or reliably available in OTB chess. Also for academic reasons, I chose a minimal approach of using no information besides the moves of the games and the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo ratings</a> of the players. An unexpected advantage of that choice is exemplified by an e-mail I received—fifteen minutes before a conference call on Wednesday with representatives of the International Chess Federation (FIDE) and other online platforms—from a different chess organizer requesting a statement that:</p>
<blockquote><p><b> </b> <em> …you will only use the data of the games, and not any personal data that might be obtained from the [tournaments] for your research, such as player name etc. … in writing as a standard precaution … to make sure that there isn’t third party usage of the data or usage outside of its intent. </em>
</p></blockquote>
<p></p><p>
Some recent legal decisions have confirmed that the moves of the games are public data and cannot be copyrighted. The e-mail did not mention the players’ ratings. As used by FIDE and the US and most online platforms (<a href="https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/">and</a> <a href="https://en.wikipedia.org/wiki/World_Football_Elo_Ratings">much</a> <a href="https://dotesports.com/general/news/elo-ratings-explained-20565">more</a> <a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">widely</a>), those are 4-digit numbers, and they are public. Knowing all four digits might be enough to pinpoint the identity of a player. </p>
<p>
Thus I suggested rounding the rating to the nearest 10 or even 20. This makes little difference to my statistical cheating tests, which already give 25 Elo points benefit-of-doubt for uncertainty about the rating. Tournament officials I’ve served have done this in a couple of individual cases but now the need is <em>en masse</em>. </p>
<p>
The question based on the previous section above is whether I should instead (or also) suggest that they implement full DP by adding <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm \epsilon}" class="latex" title="{\pm \epsilon}" /> with a mean of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 10}" class="latex" title="{\pm 10}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+20%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm 20}" class="latex" title="{\pm 20}" /> or so, with-or-without rounding. A caveat is that it is easy for someone with a tournament spreadsheet to round a column of figures before exporting text to me, but applying a random function is less familiar and prone to major mess-ups.</p>
<p>
</p><p></p><h2> The Other Prize </h2><p></p>
<p></p><p>
The Knuth Prize is not Cynthia’s only win for 2020. In December she was awarded the 2020 IEEE Richard W. Hamming Medal. A <a href="https://ieeetv.ieee.org/ieeetv-specials/honors-2020-cynthia-dwork-wins-the-ieee-richard-w-hamming-medal">video</a> gives a one-minute summary of differential privacy and her work on <em>non-malleable</em> cryptosystems via lattice bases. The latter stems from her 2000 <a href="https://www.cs.huji.ac.il/~dolev/pubs/nmc.pdf">paper</a> with Danny Dolev and Moni Naor (DDN).</p>
<p>
We have not covered this crypto topic before. It involves chosen-ciphertext attacks where Eve, the evesdropper, first gets hold of a transmitted ciphertext <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. The cryptosystem is <em>malleable</em> if Eve can know enough about the structure of <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> to make a legal ciphertext <img src="https://s0.wp.com/latex.php?latex=%7By%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y'}" class="latex" title="{y'}" />, one that may decode to a plaintext <img src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x'}" class="latex" title="{x'}" /> that advances her interests compared to the intended <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Wikipedia’s <a href="https://en.wikipedia.org/wiki/Malleability_(cryptography)">article</a> gives an example where <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> embeds a numerical field <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> that Eve can recognize and alter en-route. The paper gives a similar example where Eve cannot disrupt the transmission of <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> but can separately send her own <img src="https://s0.wp.com/latex.php?latex=%7By%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y'}" class="latex" title="{y'}" /> with a value <img src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r'}" class="latex" title="{r'}" /> that gives her advantage. This may be so even without being able to decode or gain any information about the plain value of <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />, except that <img src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r'}" class="latex" title="{r'}" /> is different in a meaningful way. For instance, <img src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r'}" class="latex" title="{r'}" /> could be a lower bid on a contract or higher bid in a secure auction.</p>
<p>
One would think that <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> should just be encoded via an error-correcting code whose range is secure so that an attacker would be overwhelmingly unlikely to find another value <img src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r'}" class="latex" title="{r'}" /> in the legal range of the code. However, this clashes with the goal of <em>homomorphic encryption</em> which allows numerical operations on encoded values. Or the technology used may be too simple, as in the paper’s mention of submitting bids to a fax number. </p>
<p>
Note: both fax machines and the paper date to the previous millennium. But the paper next gives an example that is staying with us in this millennium, where <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> is a proof of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P \neq NP}}" class="latex" title="{\mathsf{P \neq NP}}" />. Suppose Alice arranges to transmit <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> to a referee Ralph using a zero-knowledge protocol, but Eve is able to intercept their messages. Eve can impersonate Ralph to Alice and impersonate Alice to Ralph as Ralph asks questions in the protocol. It may seem impossible to forestall such a “man-in-the-middle” <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">attack</a> without extrinsic considerations of hardware and timing, but DDN design an ingenious scheme for <em>non-malleable zero-knowledge proofs</em>. </p>
<p>
</p><p></p><h2> A Video </h2><p></p>
<p></p><p>
<em>En-passant</em> of this, we want to note a great <a href="https://www.youtube.com/watch?v=4Wl-3kadvgw">video</a> that was made for next month’s Women in Theory <a href="https://womenintheory.wordpress.com/">workshop</a>. It has an all-female cast and lyrics by Avi Wigderson parodying the song “I Will Survive.” Cynthia is not in the video, but she likewise has been a great spokesperson for the experiences of women in our professional life.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Our congratulations again to Cynthia—by name—on the Knuth and Hamming prizes.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/05/16/the-2020-y-prize/"><span class="datestr">at May 16, 2020 08:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3483">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/05/16/foundations-of-responsible-computing-is-virtual/">The First Symposium on the Foundations of Responsible Computing Is Virtual [June 1st-2nd]</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://responsiblecomputing.org/">First Symposium on Foundations of Responsible Computing (FORC 2020)</a> is a forum for mathematical research in computation and society.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>
<p>Lead by program chairs <a href="https://www.microsoft.com/en-us/research/people/dwork/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fpeople%2Fdwork%2F">Cynthia Dwork</a>, <a href="https://engineering.stanford.edu/people/omer-reingold">Omer Reingold</a>, and <a href="https://www.cis.upenn.edu/~aaroth/">Aaron</a><a href="https://www.cis.upenn.edu/~aaroth/"> Roth</a>; the virtual symposium is on June 1st and 2nd, 2020.  The program consists of keynote talks by <a href="http://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>, <a href="https://sites.google.com/site/quaerereverum9/">Rakesh Vohra</a>, <a href="https://www.law.columbia.edu/faculty/patricia-j-williams">Patricia Williams</a>, and <a href="http://www.cs.cornell.edu/home/kleinber/">Jon Kleinberg</a>; and 21 contributed papers.  The conference is free but <a href="https://forms.gle/vgTzkMS6jLN9xPud8">registration is required</a>.  The <a href="https://responsiblecomputing.org/program/">full program</a> is available on the <a href="https://responsiblecomputing.org/">conference webpage</a>.</p>
<p> </p>
<p> </p></div>







<p class="date">
by Jason Hartline <a href="https://agtb.wordpress.com/2020/05/16/foundations-of-responsible-computing-is-virtual/"><span class="datestr">at May 16, 2020 07:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-4659191208065423467">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2020/05/forc-2020-program.html">FORC 2020 Program</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The FORC 2020 program is now available here: <a href="https://responsiblecomputing.org/program/">https://responsiblecomputing.org/program/</a> (and reproduced below) Note the <i>terrific</i> set of contributed talks at keynotes, and note that registration is <i>free</i>! There will also be junior/senior breakout sessions into small groups during the lunch breaks to facilitate informal conversation and networking. Please consider attending, and help to spread the word!<br /><br /><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">FORC 2020 will take place virtually (on Zoom) on June 1st and 2nd, 2020. All times below are in Eastern Daylight Time (i.e. Boston/New York/Philadelphia time).</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;"><br /></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">Registration is FREE but mandatory (Zoom links will be sent to registered email addresses). Register here before May 28: <a style="color: #99cc33;" href="https://forms.gle/vgTzkMS6jLN9xPud8">https://forms.gle/vgTzkMS6jLN9xPud8</a></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;"><br /></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px; text-align: center;"><span style="font-weight: 700;">Day 1: June 1, 2020. </span></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">10:20-10:30 Opening Remarks</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">10:30-11:20 <span style="font-weight: 700;">Keynote Talk</span>: <span style="font-weight: 700;">Adrian Weller</span>, University of Cambridge (Session Chair: <em>Suresh Venkatasubramanian</em>)</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">11:20-12:00 Session 1 (Session Chair: <em>Adam Smith)</em></div><ol style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin: 24px 0px; padding: 0px 0px 0px 12px;"><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Aloni Cohen and Kobbi Nissim. <span style="font-weight: 700;">Towards Formalizing the GDPR’s Notion of Singling Out</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Charlotte Peale, Omer Reingold and Katrina Ligett. <span style="font-weight: 700;">Bounded-Leakage Differential Privacy</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Moni Naor and Neil Vexler.<span style="font-weight: 700;"> Can Two Walk Together: Privacy Enhancing Methods and Preventing Tracking of Users</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Badih Ghazi, Ravi Kumar, Pasin Manurangsi and Rasmus Pagh. <span style="font-weight: 700;">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Badih Ghazi, Noah Golowich, Ravi Kumar, Rasmus Pagh and Ameya Velingker. <span style="font-weight: 700;">On the Power of Multiple Anonymous Messages</span></li></ol><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">12:00-1:30 Virtual Lunch/Informal Discussion/Networking</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">1:30-2:20 <span style="font-weight: 700;">Keynote Talk: Rakesh Vohra</span>, University of Pennsylvania (Session Chair: <em>Rachel Cummings</em>)</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">2:20-3:05 Session 2 (Session Chair: <em>Nicole Immorlica</em>)</div><ol style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin: 24px 0px; padding: 0px 0px 0px 12px;"><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Lee Cohen, Zachary Lipton and Yishay Mansour. <span style="font-weight: 700;">Efficient candidate screening under multiple tests and implications for fairness</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Cynthia Dwork, Christina Ilvento, Guy Rothblum and Pragya Sur.<span style="font-weight: 700;"> Abstracting Fairness: Oracles, Metrics, and Interpretability</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Christina Ilvento.<span style="font-weight: 700;"> Metric Learning for Individual Fairness</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Kevin Stangl and Avrim Blum.<span style="font-weight: 700;"> Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?</span></li></ol><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px; text-align: center;"><span style="font-weight: 700;">End Day 1</span></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px; text-align: center;"><span style="font-weight: 700;">Day 2: June 2, 2020.</span></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">10:30-11:20 <span style="font-weight: 700;">Keynote Talk</span>: <span style="font-weight: 700;">Patricia Williams, </span>Northeastern University (Session Chair: <em>Cynthia Dwork</em>)</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">11:20-12:00 Session 3 (Session Chair: <em>Steven Wu)</em></div><ol style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin: 24px 0px; padding: 0px 0px 0px 12px;"><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Arpita Biswas, Siddharth Barman, Amit Deshpande and Amit Sharma. <span style="font-weight: 700;">Inframarginality Audit of Group-Fairness</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Christopher Jung, Sampath Kannan and Neil Lutz. <span style="font-weight: 700;">Service in Your Neighborhood: Fairness in Center Location</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Cynthia Dwork, Christina Ilvento and Meena Jagadeesan. <span style="font-weight: 700;">Individual Fairness in Pipelines</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Hao Wang, Hsiang Hsu, Mario Diaz and Flavio Calmon. <span style="font-weight: 700;">To Split or Not to Split: The Impact of Disparate Treatment in Classification</span></li></ol><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">12:00-1:30 Virtual Lunch/Informal Discussion/Networking</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">1:30-2:20 <span style="font-weight: 700;">Keynote Talk: Jon Kleinberg</span>, Cornell University. (Session Chair: <em>Michael Kearns</em><span style="font-weight: 700;">)</span></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">2:20-3:05 Session 4 (Session Chair: <em>Guy Rothblum)</em></div><ol style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin: 24px 0px; padding: 0px 0px 0px 12px;"><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Ashesh Rambachan and Jonathan Roth. <span style="font-weight: 700;">Bias In, Bias Out? Evaluating the Folk Wisdom</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Mark Braverman and Sumegha Garg. <span style="font-weight: 700;">The Role of Randomness and Noise in Strategic Classification</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Sergei Mikhalishchev and Andrei Matveenko. <span style="font-weight: 700;">Attentional Role of Quota Implementation</span></li><li style="margin: 6px 0px; padding: 0px 0px 0px 12px;">Roy Dong, Erik Miehling and Cedric Langbort. <span style="font-weight: 700;">Protecting Consumers Against Personalized Pricing: A Stopping Time Approach</span></li></ol><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px;">3:05-3:15 Closing Remarks.</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; margin-bottom: 24px; margin-top: 24px; padding: 0px; text-align: center;"><span style="font-weight: 700;">End of Conference</span></div><div style="clear: both; color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding-top: 0.5em;" class="sharedaddy sd-like-enabled sd-sharing-enabled" id="jp-post-flair"><div style="clear: both;" class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3></div></div></div></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2020/05/forc-2020-program.html"><span class="datestr">at May 16, 2020 06:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-3933333869822440139">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2020/05/full-professor-position-in-machine.html">Full professor position in Machine Learning at the Scuola Normale Superiore, Pisa</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br /><div style="line-height: 100%; margin-bottom: 0.19in; margin-top: 0.19in;"><span style="font-family: Times New Roman, serif;"><span style="font-family: .HelveticaNeueDeskInterface, serif;"><span style="font-size: x-small;"><span lang="en-US">The Scuola Normale Superiore is issuing a call for applications for a Full Professorship in Computer Science, with an emphasis on the fundamental aspects of Machine Learning including mathematical foundations, computational aspects, and applications. The appointment will be made within the framework of the ``Department of Excellence” initiative in the Faculty of Science, which puts a strong emphasis on interdisciplinary collaboration among the existing groups in mathematics, physics, chemistry and biology. </span></span></span></span></div><div style="line-height: 100%; margin-bottom: 0.19in; margin-top: 0.19in;"><span style="font-family: Times New Roman, serif;"><span style="font-family: .HelveticaNeueDeskInterface, serif;"><span style="font-size: x-small;"><span lang="en-US">The ideal candidate is expected to have an outstanding research and teaching record, with a high profile at the international level, and to be able to fruitfully interact with the existing research groups at the Scuola Normale.</span></span></span></span></div><div style="line-height: 100%; margin-bottom: 0.19in; margin-top: 0.19in;"><span style="font-family: Times New Roman, serif;"><span style="font-family: .HelveticaNeueDeskInterface, serif;"><span style="font-size: x-small;"><span lang="en-US">The Scuola Normale Superiore has a long tradition of excellence in the exact sciences. Its students are selected through an extremely rigorous, strictly merit-based admission process and are among the best anywhere in the world. Complementing its traditional strength in pure mathematics, the Scuola Normale Superiore has recently opened up new research directions in fields like Quantitative Finance, Probability Theory, and Numerical Analysis. There is also significant activity in the development of computational methods for the Physical and Life Sciences.  The successful candidate will also have the opportunity to interact closely with researchers at the nearby University of Pisa (which has a strong tradition in Computer Science and was the first university in Italy to pioneer Computer Science as an academic discipline) and at  the Scuola di Studi Superiori Sant’Anna, as well as with various research groups working in the Consiglio Nazionale delle Ricerche.</span></span></span></span></div><div style="line-height: 100%; margin-bottom: 0.19in; margin-top: 0.19in;"><span style="font-family: Times New Roman, serif;"><span style="font-family: .HelveticaNeueDeskInterface, serif;"><span style="font-size: x-small;"><span lang="en-US">The application deadline is June 19, 2020.  For additional information and instructions on how to apply, see the following link, both in Italian and English:</span></span></span></span></div><div style="line-height: 100%; margin-bottom: 0in;"><a href="https://www.blogger.com/null" name="_GoBack"></a><a href="https://wwwold.sns.it/bando/professore-universitario-di-prima-fascia-ssd-inf01-n1-permanent-position-full-professor-academic-field-01b1-academic-discipline" target="_blank"><span style="color: blue;"><span style="font-family: Times New Roman, serif;"><span lang="en-US"><u>https://wwwold.sns.it/bando/professore-universitario-di-prima-fascia-ssd-inf01-n1-permanent-position-full-professor-academic-field-01b1-academic-discipline</u></span></span></span></a><span style="font-family: Times New Roman, serif;"><span lang="en-US">  </span></span></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2020/05/full-professor-position-in-machine.html"><span class="datestr">at May 16, 2020 09:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/05/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/05/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.nytimes.com/2020/04/30/books/celebrity-bookshelves-tv-coronavirus.html">Famous people’s bookshelves, visible as they teleconference from home</a> (<a href="https://mathstodon.xyz/@11011110/104097353853629567"></a>, <a href="https://www.metafilter.com/186820/Famous-Peoples-Bookshelves">via</a>). The story calls this inadvertent, but that seems dubious. Much of my teleconferencing has books behind me, deliberately, mostly as a clean background in a part of the house where it’s convenient to sit and lighting is good, but also because very few views in my house avoid books. On the other hand, at least one colleague has substituted a fake background from a library…</p>
  </li>
  <li>
    <p><a href="http://faculty.smcm.edu/sgoldstine/pinecones.html">Susan Goldstine paints numbers onto the scales of pinecones</a> (<a href="https://mathstodon.xyz/@11011110/104102680813443034"></a>) to show how phyllotaxis causes the Fibonacci numbers to line up.</p>
  </li>
  <li>
    <p>In a post on <a href="https://www.flyingcoloursmaths.co.uk/dictionary-of-mathematical-eponymy-the-quine-mccluskey-algorithm/">the Quine–McCluskey algorithm for minimizing Boolean functions</a> (<a href="https://mathstodon.xyz/@11011110/104114018052115751"></a>), Colin Beveridge suggests that all mathematical Q-eponyms are named for Dan Quillen or Willard Quine. But even if you discount Quasi-abelian categories, Quasi-Hopf algebras, and Quasi-Newton methods (named after Quasi-Abel, Quasi-Hopf, and Quasi-Newton) there’s also <a href="https://en.wikipedia.org/wiki/Qvist%27s_theorem">Qvist’s theorem</a> in finite geometry, from Bertil Qvist.</p>
  </li>
  <li>
    <p><a href="https://www.math.columbia.edu/~woit/wordpress/?p=11723">Why the Szpiro conjecture is still a conjecture</a> (<a href="https://mathstodon.xyz/@11011110/104120078920669171"></a>). See especially the linked collection of comments from the preceding post (with significant contributions from Fields medalist Peter Scholze) for details of why Scholze thinks Mochizuki’s claimed proof not only doesn’t work, but can’t work.</p>
  </li>
  <li>
    <p>Colin (not to be confused with Colin, above) <a href="https://mathstodon.xyz/@ColinTheMathmo/104123191826114771">asks for self-intersecting polyhedra with only hexagonal faces</a>. I described <a href="https://11011110.github.io/blog/2009/09/18/not-nauru-graph.html">a non-self-intersecting toroidal polyhedron with L-shaped hexagonal faces</a> in an earlier post, but I think the <a href="https://en.wikipedia.org/wiki/Small_triambic_icosahedron">small triambic icosahedron</a> (<a href="https://digital.lib.washington.edu/researchworks/handle/1773/4593">conjectured by Grünbaum to be the only face-symmetric polyhedron with more than five sides per face</a>) is closer to what he is asking for.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html">Bill Gasarch asks: why is there no grid for Hilbert’s 10th?</a> (<a href="https://mathstodon.xyz/@11011110/104131498905150965"></a>). What he wants to know is, for which pairs  can we algorithmically find integer solutions to degree- -variable polynomial equations, and for which pairs is it undecidable? The answer seems to be: we can solve them when , we can’t solve them for some pairs of larger numbers, and there’s a big gap of unknown pairs.</p>
  </li>
  <li>
    <p><a href="https://euro-math-soc.eu/news/20/05/8/prize-winners-announced">EMS Prize, Klein Prize, and Neugebauer Prize</a> (<a href="https://mathstodon.xyz/@11011110/104133715419191450"></a>). Winners: Karim Adiprasito, Ana Caraiani, Alexander Efimov, Simion Filip, Aleksandr Logunov, Kaisa Matomäki, Phan Thành Nam, Joaquim Serra, Jack Thorne, and Maryna Viazovska; Arnulf Jentzen; Karine Chemla.</p>
  </li>
  <li>
    <p><a href="https://cstheory.stackexchange.com/q/46746/95">On the complexity of a “list” datastructure in the RAM model</a> (<a href="https://mathstodon.xyz/@11011110/104142162759402806"></a>). Linked lists can insert and delete at arbitrary positions. Arrays can find the value at position . Binary trees can do both, but with log time per operation. Combining methods for maintaining order in a list and integer ranking/unranking instead gives . This showed up just in time for me to add some of the ideas to the syllabus for my ongoing online graduate data structures class.</p>
  </li>
  <li>
    <p>Some reading on “git pull –rebase”: <a href="https://medium.com/@DGabeau/git-pull-rebase-vs-git-pull-c2b352fe53aa">Gabeau/medium</a>, <a href="https://coderwall.com/p/7aymfa/please-oh-please-use-git-pull-rebase">Hasiński/coderwall</a>, <a href="https://stackoverflow.com/questions/2472254/when-should-i-use-git-pull-rebase">Shved &amp; Mortensen/stackoverflow</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/104145964667479373"></a>).</span> I have been using this in situations with multiple authors simultaneously working on a paper, to keep my contributions to the edit history linear (avoiding edit conflicts and merge bubbles in the history). It’s been working well for me but I realize it might be controversial…</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Klaus_Roth">Klaus Roth (1925–2015)</a>, Fields medalist who made important contributions to Diophantine approximation, arithmetic combinatorics, and discrepancy theory (<a href="https://mathstodon.xyz/@11011110/104151822927869863"></a>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=4794">Scott Aaronson discusses four new preprints in quantum computing</a> (<a href="https://mathstodon.xyz/@11011110/104164952895881843"></a>).</p>
  </li>
  <li>
    <p><a href="https://thmatters.wordpress.com/2020/05/14/call-for-nominations-for-talg-new-editor-in-chief/">Call for nominations for new editor-in-chief of <em>ACM Transactions on Algorithms</em></a> (<a href="https://mathstodon.xyz/@11011110/104168138945005653"></a>). Nominations are due <span style="white-space: nowrap;">June 8.</span></p>
  </li>
  <li>
    <p>My campus computing support people have decided that the middle of a very work-intensive term is the time to make me choose between stopping making illustrations or giving up access to the scientific literature (<a href="https://mathstodon.xyz/@11011110/104174466523993151"></a>). The connection is the VPN I use to access campus journal and database subscriptions. They want to upgrade to a version incompatible with OS X 10.12. Upgrading OS X would be incompatible with my purchased Adobe software and make me pay money I don’t have for a subscription instead.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/05/15/linkage.html"><span class="datestr">at May 15, 2020 04:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=435">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/05/14/tcs-talk-wednesday-may-20-mark-bun-boston-university/">TCS+ talk: Wednesday, May 20 — Mark Bun, Boston University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Mark Bun</strong> from Boston University will speak about “<em>An Equivalence between Private Classification and Online Predictability</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We prove that every concept class with finite Littlestone dimension can be learned by an (approximate) differentially-private algorithm. The converse direction was shown in recent work of Alon, Livni, Malliaris, and Moran, STOC ’19. Together these two results show that a class of functions is privately learnable if and only if it is learnable in the mistake-bound model of online learning. To establish our result, we introduce “global stability,” a new notion of algorithmic stability for learning algorithms that we show can always be satisfied when learning classes of finite Littlestone dimension.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/05/14/tcs-talk-wednesday-may-20-mark-bun-boston-university/"><span class="datestr">at May 14, 2020 06:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-05-14-streamlet/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-05-14-streamlet/">Streamlet: A Simple Textbook Blockchain Protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Benjamin Chan and Elaine Shi In this post, we describe an extraordinarily simple blockchain protocol called Streamlet. Consensus is a complex problem and has been studied since the 1980s. More recently, blockchain research has spawned many new works aiming for performance and ease-of-implementation. However, simple, understandable protocols...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-05-14-streamlet/"><span class="datestr">at May 14, 2020 05:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1688">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/05/14/pride-and-prejudice-from-research-to-practice/">Pride and Prejudice: From Research to Practice</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Despite (or because of) being a devoted theoretician, I truly enjoy every occasion when theory influences practice. I therefore felt quite a bit of satisfaction when our <a href="https://arxiv.org/abs/1711.08513">rather recent work</a>, in the context of algorithmic fairness (with Úrsula Hébert-Johnson, Michael P. Kim and Guy N. Rothblum), found an application for predicting COVID-19 complications. Researchers in Israel, in collaboration with Israel’s biggest health-care provider, adapted a refined model for predicting flu complications to a model for predicting COVID-19 complications.  At the time, only very limited data from China were available (marginal statistics).  This is where our work came in (following several past empiric studies of the method):  the team applied our algorithm to improve the accuracy of predictions across various subpopulations (as part of an immense research and engineering effort). Now that there is (unfortunately) more data, it seems that the predictor exhibited surprisingly good performance (surprising, due to the poor training data).  See a <a href="https://www.medrxiv.org/content/10.1101/2020.04.23.20076976v1">manuscript</a> here, an interview <a href="https://www.youtube.com/watch?v=r_alyuZULYI">here</a> and a more technical talk <a href="https://www.youtube.com/watch?v=weaRmSVA3yM">here</a> (starting at minute 36 roughly). The predictor was applied with the appropriate cautiousness to inform and advise patients.</p>
<p>But this is also an example of the gravity of decisions by researchers and software developers. Taking it to extreme, imagine a predictor that is used to determine which patients are denied treatment in an overwhelmed hospital. The booming research area of algorithmic fairness sees a very short turnover from research ideas (in many areas) to deployment. In an ideal world, it would have been much better to first have a couple of decades to develop the computational foundations of algorithmic fairness, before the practical need arose. But in the real world, the huge scale of algorithmic decision making creates immense demand for solutions. Industry, as well as policy and law makers are unlikely to wait decades or even years, nor is it clear that they should. From my perspective, this reality underscores the urgency for <em>principled</em> and <em>deliberate</em> research – rather than <em>hasty</em> research – continuously developing the foundations of algorithmic fairness and offering answers to real-world challenges.</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/05/14/pride-and-prejudice-from-research-to-practice/"><span class="datestr">at May 14, 2020 05:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1307">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/05/14/cra-and-ccc-announce-computing-innovation-fellows-2020/">CRA and CCC announce Computing Innovation Fellows 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computing Research Association (CRA) and Computing Community Consortium (CCC) have announced a new CI Fellows program that will offer 2 year postdoctoral opportunities in computing starting Fall’20.</p>
<p>The deadline for application is yet to be announced but will be around <strong>mid-June 2020</strong>, with decisions being made around mid-July 2020 for positions beginning this fall or winter. We will update this post once a deadline is announced. In the meantime, further details can be found at <a href="https://cifellows2020.org/" rel="nofollow">https://cifellows2020.org/</a>.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/05/14/cra-and-ccc-announce-computing-innovation-fellows-2020/"><span class="datestr">at May 14, 2020 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1304">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/05/14/call-for-nominations-for-talg-new-editor-in-chief/">Call for Nominations for TALG new Editor-in-Chief</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Prof. <span class="il">David</span> <span class="il">Shmoys</span> is chairing the committee to identify a new Editor-in-Chief for ACM Trans. on Algorithms.  The deadline for nominations is <strong>Jun 8th</strong>. Please see details below.</p>
<p>ACM TALG (webpage: <a href="http://talg.acm.org/" rel="nofollow">http://talg.acm.org/</a>) publishes original research of the highest quality dealing with algorithms. It is a peer-reviewed journal, appearing quarterly. Specific areas of computation covered by the journal are listed at <a href="http://talg.acm.org/Aims.html" rel="nofollow">http://talg.acm.org/Aims.html</a>.</p>
<p>We are looking for a well-established person with a strong record of research achievements and service, and with a vision for the future of the field. The term of appointment is three years, to begin late summer 2020, with the possibility of renewal for a second term. The editor-in-chief is responsible for faithfully executing the editorial charter of the journal yet should be proactive in adapting the journal and its charter to changes in the field. A description of the duties of the EiC and evaluation criteria can be found at <a href="http://www.acm.org/publications/policies/evaluation" rel="nofollow">http://www.acm.org/publications/policies/evaluation</a>.</p>
<p>The Search Committee members are:<br />
David Eppstein – University of California, Irvine<br />
Anna Karlin – University of Washington<br />
Chris Hankin – Imperial College London – ACM Publications Board Liaison<br />
Dana Randall – Georgia Institute of Technology<br />
David Shmoys – Cornell University – Committee Chair.</p>
<p>All nominees, including self-nominees, should send a CV and a Vision Statement for TALG (at least one page), with subject header “EiC nomination” to the committee chair – david.shmoys@cornell.edu .</p>
<p>The deadline for nominations is Monday, June 8, 2020 at 11:59 p.m. (EST).</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/05/14/call-for-nominations-for-talg-new-editor-in-chief/"><span class="datestr">at May 14, 2020 04:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/05/14/spectral-sparsification-of-hypergraphs/">Spectral Sparsification of Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 In this post we will construct a “spectral sparsifier” of a given hypergraph in a way that is similar to how Spielman and Srivastava construct spectral graph sparsifiers. We will assign a probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> to each hyperedge, we will sample each hyperedge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" />, and we will weigh it by <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/p_e}" class="latex" title="{1/p_e}" /> if selected. We will then bound the “spectral error” of this construction in terms of the supremum of a Gaussian process using Talagrand’s comparison inequality and finally bound the supremum of the Gaussian process (which will involve matrices) using matrix Chernoff bounds. This is <a href="https://arxiv.org/abs/1905.01495">joint work with Nikhil Bansal and Ola Svensson</a>.</p>
<p>
<span id="more-4389"></span></p>
<p>
</p><p><b>1. Hypergraph Sparsifiers </b></p>
<p></p><p>
An (undirected) hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H= (V,E)}" class="latex" title="{H= (V,E)}" /> is a collection <img src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E}" class="latex" title="{E}" /> of subsets <img src="https://s0.wp.com/latex.php?latex=%7Be+%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e \subseteq V}" class="latex" title="{e \subseteq V}" />, called hyperedges. A graph is the special case in which each set <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> has cardinality 2. For simplicity we will talk only about <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />-uniform hypergraphs, that is hypergraphs in which all hyperedges have the same cardinality <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> (the arguments below would work also in the non-uniform case in which all hyperedges have cardinality <em>at most</em> <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />). Hyperedges may have weights.</p>
<p>
If <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> is a subset of vertices, a hyperedge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is <em>cut</em> by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> if <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> has non-empty intersection with both <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BV-S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V-S}" class="latex" title="{V-S}" />. We call <img src="https://s0.wp.com/latex.php?latex=%7Bcut_H%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{cut_H(S)}" class="latex" title="{cut_H(S)}" /> the number of hyperedges of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> cut by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, or the total weight of such hyperedges if <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is a weighted hypergraph.</p>
<p>
We can then generalize the notion of Benczur-Karger sparsification, and say that <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> is a cut sparsifier of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> with error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> if <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> have the same vertex set <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> and</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+S%5Csubseteq+V%3A+%5C+%5C+%5C+1-%5Cepsilon+%5Cleq+%5Cfrac%7Bcut_%7BH%27%7D%28S%29%7D%7Bcut_%7BH%7D%28S%29%7D+%5Cleq+1+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall S\subseteq V: \ \ \ 1-\epsilon \leq \frac{cut_{H'}(S)}{cut_{H}(S)} \leq 1 + \epsilon " class="latex" title="\displaystyle  \forall S\subseteq V: \ \ \ 1-\epsilon \leq \frac{cut_{H'}(S)}{cut_{H}(S)} \leq 1 + \epsilon " /></p>
<p>
Kogan and Krauthgamer show that every <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />-uniform hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> admits a cut sparsifier of error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> with only <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n+%28r+%2B+%5Clog+n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} n (r + \log n))}" class="latex" title="{O(\epsilon^{-2} n (r + \log n))}" /> weighted hyperedges. They are able to extend the argument of Benczur and Karger to hypergraphs, including arguing that there are few sparse cuts. They assign a probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> to each hyperedge, sample it with that probability, weighing it <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/p_e}" class="latex" title="{1/p_e}" /> if selected, and then use a union bound and Chernoff bounds to argue that all cuts are preserved.</p>
<p>
Anand Louis has introduced a <a href="https://dl.acm.org/doi/abs/10.1145/2746539.2746555">notion of hypergraph Laplacian</a> in the following way. The Laplacian quadratic form of a hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is a function <img src="https://s0.wp.com/latex.php?latex=%7BQ_H+%3A+%7B%5Cmathbb+R%7D%5EV+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_H : {\mathbb R}^V \rightarrow {\mathbb R}}" class="latex" title="{Q_H : {\mathbb R}^V \rightarrow {\mathbb R}}" /> defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_H%28x%29+%3D+%5Csum_%7Be%5Cin+E%7D+w%28e%29+%5C+%5Cmax_%7Ba%2Cb%5Cin+e%7D+%5C+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q_H(x) = \sum_{e\in E} w(e) \ \max_{a,b\in e} \ (x_a - x_b)^2 " class="latex" title="\displaystyle  Q_H(x) = \sum_{e\in E} w(e) \ \max_{a,b\in e} \ (x_a - x_b)^2 " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bw%28e%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w(e)}" class="latex" title="{w(e)}" /> is the weight of the hyperedge <img src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E}" class="latex" title="{E}" />, or 1 if the hypergraph is unweighted.</p>
<p>
This definition has the motivation that it recovers the Laplacian quadratic form <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29+%3D+x%5ET+L_Gx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x) = x^T L_Gx}" class="latex" title="{Q_G(x) = x^T L_Gx}" /> in the case of graphs, and that if <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is the indicator of a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> then <img src="https://s0.wp.com/latex.php?latex=%7BQ_H%28x%29+%3D+cut_H%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_H(x) = cut_H(S)}" class="latex" title="{Q_H(x) = cut_H(S)}" />. Furthermore, one can define “eigenvalues” and “eigenvectors” of this hypergraph Laplacian and recover a Cheeger inequality and even <a href="https://dl.acm.org/doi/abs/10.1145/3178123">higher-order Cheeger inequalities</a>.</p>
<p>
So it seems interesting to consider the following notion of sparsification: a hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> is a spectral sparsifier of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> with error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> if <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> have the same vertex set and</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%5Cin+%7B%5Cmathbb+R%7D%5EV+%5C+%5C+%5C+1-%5Cepsilon+%5Cleq+%5Cfrac%7BQ_%7BH%27%7D%28x%29%7D%7BQ_%7BH%7D%28x%29%7D+%5Cleq+1+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x\in {\mathbb R}^V \ \ \ 1-\epsilon \leq \frac{Q_{H'}(x)}{Q_{H}(x)} \leq 1 + \epsilon " class="latex" title="\displaystyle  \forall x\in {\mathbb R}^V \ \ \ 1-\epsilon \leq \frac{Q_{H'}(x)}{Q_{H}(x)} \leq 1 + \epsilon " /></p>
<p> where, as before, the convention is that <img src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0/0 = 1}" class="latex" title="{0/0 = 1}" />.</p>
<p>
Soma and Yoshida <a href="https://dl.acm.org/doi/10.5555/3310435.3310594">studied this question</a> and gave a construction with <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n%5E3%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} n^3\log n)}" class="latex" title="{O(\epsilon^{-2} n^3\log n)}" /> hyperedges. In the rest of this post we will discuss the construction by Nikhil Bansal, Ola Svensson and me, which uses <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} r^3 n \log n)}" class="latex" title="{O(\epsilon^{-2} r^3 n \log n)}" /> hyperedges.</p>
<p>
</p><p><b>2. Choosing Probabilities </b></p>
<p></p><p>
Given a hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H=(V,E)}" class="latex" title="{H=(V,E)}" />, we can construct a multigraph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> by taking each hyperedge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> and then constructing, in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, a clique between the vertices of <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />. Thus, in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, the edge <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a,b)}" class="latex" title="{(a,b)}" /> is repeated as many times (possibly, zero times) as the number of hyperedges <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> that contain both <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />. Another way to think about it is that the Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is given by</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_G+%3D+%5Csum_%7Be%5Cin+E%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+L_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L_G = \sum_{e\in E} \sum_{a,b\in e} L_{a,b} " class="latex" title="\displaystyle  L_G = \sum_{e\in E} \sum_{a,b\in e} L_{a,b} " /></p>
<p>
where the inner sum is over the <img src="https://s0.wp.com/latex.php?latex=%7B%7Br+%5Cchoose+2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{r \choose 2}}" class="latex" title="{{r \choose 2}}" /> unordered pairs <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Ba%2Cb%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{a,b\}}" class="latex" title="{\{a,b\}}" />. This graph relates in several interesting ways with <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />. For example, if <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> is a subset of vertices and <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is a hyperedge that is cut by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, then between <img src="https://s0.wp.com/latex.php?latex=%7Br-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r-1}" class="latex" title="{r-1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7Br+%5Cchoose+2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{r \choose 2}}" class="latex" title="{{r \choose 2}}" /> of the edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> derived from <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> are cut by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />. If <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is not cut by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, then none of the edges derived from <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is cut by <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, so we have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+S%5Csubseteq+V%3A+%5C+%5C+%5C+%28r-1%29+%5C+cut_H%28S%29+%5Cleq+cut_G%28S%29+%5Cleq+%7Br+%5Cchoose+2%7D+cut_H%28S%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall S\subseteq V: \ \ \ (r-1) \ cut_H(S) \leq cut_G(S) \leq {r \choose 2} cut_H(S) " class="latex" title="\displaystyle  \forall S\subseteq V: \ \ \ (r-1) \ cut_H(S) \leq cut_G(S) \leq {r \choose 2} cut_H(S) " /></p>
<p>
and, with a bit more work, it is possible to prove similar bounds for the Laplacian quadratic forms</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%5Cin+%7B%5Cmathbb+R%7D%5E+V%3A+%5C+%5C+%5C+%5Cfrac+r2+%5C+Q_H%28x%29+%5Cleq+x%5ET+L_G+x+%5Cleq+%7Br+%5Cchoose+2%7D+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x\in {\mathbb R}^ V: \ \ \ \frac r2 \ Q_H(x) \leq x^T L_G x \leq {r \choose 2} Q_H(x) " class="latex" title="\displaystyle  \forall x\in {\mathbb R}^ V: \ \ \ \frac r2 \ Q_H(x) \leq x^T L_G x \leq {r \choose 2} Q_H(x) " /></p>
<p> This means that if we sparsify <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, say with the Spielman-Srivastava construction, we obtain information about <img src="https://s0.wp.com/latex.php?latex=%7BQ_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_H}" class="latex" title="{Q_H}" />, up to multiplicative error <img src="https://s0.wp.com/latex.php?latex=%7BO%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(r)}" class="latex" title="{O(r)}" />. Now suppose that, as we sample edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> to sparsify it in the Spielman-Srivastava way, we will also pick hyperedges of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> (for example we pick <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> if at least one of its corresponding edges in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is picked), and we weigh them by the inverse of the probability of being selected. Then we may hope that if the Spielman-Srivastava sparsification of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is tuned to achieve error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon/r^2}" class="latex" title="{\epsilon/r^2}" />, the hypergraph that we obtain will have error at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />. Indeed, this is roughly what happens, and we will be able to prove it by showing that the error in the hypergraph sparsification is dominated by the error in the “Gaussian version” of Spielman-Srivastava described in the previous post.</p>
<p>
So we are going to assign to each hyperedge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> a probability </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_e+%3D+%5Cmin+%5Cleft%5C%7B+1+%2C+B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+%7C%7C+L%5E%7B-1%2F2%7D_G+L_%7Ba%2Cb%7D+L%5E%7B-1%2F2%7D+%7C%7C+%5Cright%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  p_e = \min \left\{ 1 , B \cdot \sum_{a,b \in e} || L^{-1/2}_G L_{a,b} L^{-1/2} || \right\} " class="latex" title="\displaystyle  p_e = \min \left\{ 1 , B \cdot \sum_{a,b \in e} || L^{-1/2}_G L_{a,b} L^{-1/2} || \right\} " /></p>
<p> where the factor <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> will be chosen later to get the construction to work and <img src="https://s0.wp.com/latex.php?latex=%7BR_%7Ba%2Cb%7D+%3A%3D+%7C%7C+L%5E%7B-1%2F2%7D_G+L_%7Ba%2Cb%7D+L%5E%7B-1%2F2%7D+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R_{a,b} := || L^{-1/2}_G L_{a,b} L^{-1/2} ||}" class="latex" title="{R_{a,b} := || L^{-1/2}_G L_{a,b} L^{-1/2} ||}" /> is the effective resistance of the edge <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a,b)}" class="latex" title="{(a,b)}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.</p>
<p>
 In fact, as before, it will be helpful to have probabilities that are non-positive powers of two, so we will choose <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> to be a power of two such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin+%5Cleft%5C%7B+1+%2C+B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cright%5C%7D+%5Cleq+p_e+%5Cleq+%5Cmin+%5Cleft%5C%7B+1+%2C+2B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cright%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min \left\{ 1 , B \cdot \sum_{a,b \in e} R_{a,b} \right\} \leq p_e \leq \min \left\{ 1 , 2B \cdot \sum_{a,b \in e} R_{a,b} \right\} " class="latex" title="\displaystyle  \min \left\{ 1 , B \cdot \sum_{a,b \in e} R_{a,b} \right\} \leq p_e \leq \min \left\{ 1 , 2B \cdot \sum_{a,b \in e} R_{a,b} \right\} " /></p>
<p>
We have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_e+p_e+%5Cleq+2+B+%5Csum_%7Be%5Cin+E%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cleq+2B+%5Ccdot+%28n-1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_e p_e \leq 2 B \sum_{e\in E} \sum_{a,b\in e} R_{a,b} \leq 2B \cdot (n-1) " class="latex" title="\displaystyle  \sum_e p_e \leq 2 B \sum_{e\in E} \sum_{a,b\in e} R_{a,b} \leq 2B \cdot (n-1) " /></p>
<p>
Another fact that will become useful later (it will save us a factor of the order of <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> in the number of hyperedges in the construction) is that</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cleq+%5Cfrac+2r+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \max_{a,b \in e} R_{a,b} \leq \frac 2r \sum_{a,b \in e} R_{a,b} " class="latex" title="\displaystyle  \max_{a,b \in e} R_{a,b} \leq \frac 2r \sum_{a,b \in e} R_{a,b} " /></p>
<p>
</p><p><b>3. A Discrete Random Process </b></p>
<p></p><p>
Our construction of a hypergraph sparsifier <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> will be to select each <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> independently with probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> and, if selected, weigh it by <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/p_e}" class="latex" title="{1/p_e}" />. Our goal is to find an upper bound in probability, or even in expectation, on </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cleft+%7C+1+-+%5Cfrac%7BQ_%7BH%27%7D%28x%29%7D%7BQ_H%28x%29%7D+%5Cright+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x \in {\mathbb R}^n} \ \ \left | 1 - \frac{Q_{H'}(x)}{Q_H(x)} \right | " class="latex" title="\displaystyle  \sup_{x \in {\mathbb R}^n} \ \ \left | 1 - \frac{Q_{H'}(x)}{Q_H(x)} \right | " /></p>
<p> Recalling that <img src="https://s0.wp.com/latex.php?latex=%7Bx%5ET+L_G+x+%5Cleq+%5Cfrac%7Br%5E2%7D2+Q_H%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^T L_G x \leq \frac{r^2}2 Q_H(x)}" class="latex" title="{x^T L_G x \leq \frac{r^2}2 Q_H(x)}" />, we will study</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%3A+x%5ET+L_G+x+%3D+1%7D+%7C+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x : x^T L_G x = 1} | Q_H(x) - Q_{H'} (x) | " class="latex" title="\displaystyle  \sup_{x : x^T L_G x = 1} | Q_H(x) - Q_{H'} (x) | " /></p>
<p> Because if we can show that the above quantity is at most <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon+%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\epsilon /r^2}" class="latex" title="{2\epsilon /r^2}" />, then, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+%5Cleq+%5Cfrac%7B2%5Cepsilon%7D%7Br%5E2%7D+%5C+x%5ET+L_G+x+%5Cleq+%5Cepsilon+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  | Q_H(x) - Q_{H'} (x) | \leq \frac{2\epsilon}{r^2} \ x^T L_G x \leq \epsilon Q_H(x) " class="latex" title="\displaystyle  | Q_H(x) - Q_{H'} (x) | \leq \frac{2\epsilon}{r^2} \ x^T L_G x \leq \epsilon Q_H(x) " /></p>
<p> as desired.</p>
<p>
If we define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%3D+%5Cmax_%7Ba%2Cb+%5Cin+E%7D+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q_e(x) = \max_{a,b \in E} x^T L_{a,b} x " class="latex" title="\displaystyle  Q_e(x) = \max_{a,b \in E} x^T L_{a,b} x " /></p>
<p> then we are interested in the supremum in <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x+%3A+x%5ET+L_G+x+%3D+1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = \{ x : x^T L_G x = 1 \}}" class="latex" title="{T = \{ x : x^T L_G x = 1 \}}" /> of </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+%3D+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%3D+%5Csum_e+z_e+Q_e+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x) = Q_H(x) - Q_{H'} (x) = \sum_e z_e Q_e (x) " class="latex" title="\displaystyle  F(x) = Q_H(x) - Q_{H'} (x) = \sum_e z_e Q_e (x) " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z_e}" class="latex" title="{z_e}" /> is a random variable that is equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/p_e-1}" class="latex" title="{1/p_e-1}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> and is equal to <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-1}" class="latex" title="{-1}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-p_e}" class="latex" title="{1-p_e}" />.</p>
<p>
As before, we will do the construction in rounds. If the smallest probability <img src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e}" class="latex" title="{p_e}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{-\ell}}" class="latex" title="{2^{-\ell}}" />, then we will have <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> rounds. We start with <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%280%29%7D+%3D+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(0)} = H}" class="latex" title="{H^{(0)} = H}" /> and then, at round <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />, we take all hyperedges <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e \leq 2^{k-1-\ell}}" class="latex" title="{p_e \leq 2^{k-1-\ell}}" /> and, independently for each such hyperedge, we either delete it or we double its weight (with probability 1/2 in each case). The final hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28%5Cell%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(\ell)}}" class="latex" title="{H^{(\ell)}}" /> is distributed like <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" />. We have the processes</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%5E%7B%28k%29%7D%28x%29+%3D+Q_%7BH%5E%7B%28k-1%29%7D%7D%28x%29+-+Q_%7BH%5E%7B%28k%29%7D%7D+%28x%29+%3D+%5Csum_%7Be%5Cin+E_k%7D+r%5E%7B%28k%29%7D_e+w%5E%7B%28k-1%29%7D_e+Q_e+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F^{(k)}(x) = Q_{H^{(k-1)}}(x) - Q_{H^{(k)}} (x) = \sum_{e\in E_k} r^{(k)}_e w^{(k-1)}_e Q_e (x) " class="latex" title="\displaystyle  F^{(k)}(x) = Q_{H^{(k-1)}}(x) - Q_{H^{(k)}} (x) = \sum_{e\in E_k} r^{(k)}_e w^{(k-1)}_e Q_e (x) " /></p>
<p> where the random variables <img src="https://s0.wp.com/latex.php?latex=%7Br%5E%7B%28k%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r^{(k)}_e}" class="latex" title="{r^{(k)}_e}" /> are Rademacher, the weights <img src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w^{(k-1)}_e}" class="latex" title="{w^{(k-1)}_e}" /> are either 0 or <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1-%5Cell%7D+%2F+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k-1-\ell} / p_e}" class="latex" title="{2^{k-1-\ell} / p_e}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BE_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_k}" class="latex" title="{E_k}" /> is the set of edges that are “active” in round <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />, that is, the set of hyperedges <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e \leq 2^{k-1-\ell}}" class="latex" title="{p_e \leq 2^{k-1-\ell}}" />.</p>
<p>
For each hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(k)}}" class="latex" title="{H^{(k)}}" />, we will also consider its associated graph <img src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G^{(k)}}" class="latex" title="{G^{(k)}}" />, obtained by replacing each hyperedge with a clique. The Laplacian of <img src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G^{(k)}}" class="latex" title="{G^{(k)}}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7BG%5E%7B%28k%29%7D%7D+%3D+%5Csum_e+w%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+L_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L_{G^{(k)}} = \sum_e w^{(k)}_e \sum_{a,b\in e} L_{a,b} " class="latex" title="\displaystyle  L_{G^{(k)}} = \sum_e w^{(k)}_e \sum_{a,b\in e} L_{a,b} " /></p>
<p>
We have the following lemma</p>
<blockquote><p><b>Lemma 1</b> <em> For every outcome of the first <img src="https://s0.wp.com/latex.php?latex=%7Bk-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k-1}" class="latex" title="{k-1}" /> rounds such that <img src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%5E%7B%28k-1%29%7D%7D+%5Cpreceq+2+L_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L_{G^{(k-1)}} \preceq 2 L_G}" class="latex" title="{L_{G^{(k-1)}} \preceq 2 L_G}" />, there is a probability at least <img src="https://s0.wp.com/latex.php?latex=%7B1-1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-1/n}" class="latex" title="{1-1/n}" /> over the randomness of the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-th round that </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7C+F%5E%7B%28k%29%7D+%28x%29+%7C+%5Cleq+O+%5Cleft%28+%5Csqrt%7B2%5E%7Bk-%5Cell%7D+%5Cfrac+%7B+%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x: x^T L_G x = 1} | F^{(k)} (x) | \leq O \left( \sqrt{2^{k-\ell} \frac { \log n}{Br} } \right) " class="latex" title="\displaystyle  \sup_{x: x^T L_G x = 1} | F^{(k)} (x) | \leq O \left( \sqrt{2^{k-\ell} \frac { \log n}{Br} } \right) " /></p>
<p> and that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D+%7D+%5Cpreceq+O+%5Cleft%28+%5Csqrt%7B2%5E%7Bk-%5Cell%7D+%5Cfrac+%7B+%5Clog+n%7D%7BB%7D+%7D+%5C+%5Cright%29+%5Ccdot+L_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L_{G^{(k)}} - L_{G^{(k-1)} } \preceq O \left( \sqrt{2^{k-\ell} \frac { \log n}{B} } \ \right) \cdot L_G " class="latex" title="\displaystyle  L_{G^{(k)}} - L_{G^{(k-1)} } \preceq O \left( \sqrt{2^{k-\ell} \frac { \log n}{B} } \ \right) \cdot L_G " /></p>
</em><p><em> </em></p></blockquote>
<p> This means that we can take <img src="https://s0.wp.com/latex.php?latex=%7BB+%3D+O%28%5Cepsilon%5E%7B-2%7D+r%5E3%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B = O(\epsilon^{-2} r^3\log n)}" class="latex" title="{B = O(\epsilon^{-2} r^3\log n)}" /> and apply the above lemma inductively to argue that we have a high probability that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csup_%7Bx%5Cin+T%7D+%7CF%28x%29+%7C+%5Cleq+2%5Cepsilon+%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sup_{x\in T} |F(x) | \leq 2\epsilon /r^2}" class="latex" title="{\sup_{x\in T} |F(x) | \leq 2\epsilon /r^2}" />, and so we get a hypergraph spectral sparsifier with error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} r^3 n \log n)}" class="latex" title="{O(\epsilon^{-2} r^3 n \log n)}" /> hyperedges. </p>
<p>
To prove the Lemma we will define the Gaussian process</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+F%5E%7B%28k%29%7D+%28x%29+%3D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \hat F^{(k)} (x) = \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ x^T L_{a,b} x " class="latex" title="\displaystyle  \hat F^{(k)} (x) = \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ x^T L_{a,b} x " /></p>
<p> where the <img src="https://s0.wp.com/latex.php?latex=%7Bg%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g^{(k)}_{e,a,b}}" class="latex" title="{g^{(k)}_{e,a,b}}" /> are Gaussian. Notice the two differences between <img src="https://s0.wp.com/latex.php?latex=%7BF%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F^{(k)}}" class="latex" title="{F^{(k)}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F^{(k)}}" class="latex" title="{\hat F^{(k)}}" />: we replaced the Rademacher choices with Gaussian choices, and we replaced </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%3D+%5Cmax+_%7Ba%2Cb%5Cin+e%7D+%5C+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q_e(x) = \max _{a,b\in e} \ (x_a - x_b)^2 " class="latex" title="\displaystyle  Q_e(x) = \max _{a,b\in e} \ (x_a - x_b)^2 " /></p>
<p> with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Ba%2Cb%5Cin+e%7D+x%5ET+L_%7Ba%2Cb%7D+x+%3D+%5Csum_%7Ba%2Cb+%5Cin+e%7D+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{a,b\in e} x^T L_{a,b} x = \sum_{a,b \in e} (x_a - x_b)^2 " class="latex" title="\displaystyle  \sum_{a,b\in e} x^T L_{a,b} x = \sum_{a,b \in e} (x_a - x_b)^2 " /></p>
<p> Furthermore, we are doing a random choice for each pair within each hyperedge instead of just one random choice per hyperedge.</p>
<blockquote><p><b>Fact 2</b> <em> The random processes <img src="https://s0.wp.com/latex.php?latex=%7BF%5E%7B%28k%29%7D+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F^{(k)} (x)}" class="latex" title="{F^{(k)} (x)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B-F%5E%7B%28k%29%7D+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-F^{(k)} (x)}" class="latex" title="{-F^{(k)} (x)}" /> are <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />-dominated by <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat F(x)}" class="latex" title="{\hat F(x)}" />. </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  For every <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%5E%7B%28k%29%7D+%28x%29+-+F%5E%7B%28k%29%7D%28y%29+%7C%7C%5E2_%7B%5CPsi_2%7D+%3D+O%281%29+%5Ccdot+%5Csum_%7Be%5Cin+E_k%7D+%28+w%5E%7B%28k-1%29%7D_e+Q_e+%28x%29+%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || F^{(k)} (x) - F^{(k)}(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e\in E_k} ( w^{(k-1)}_e Q_e (x) )^2 " class="latex" title="\displaystyle  || F^{(k)} (x) - F^{(k)}(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e\in E_k} ( w^{(k-1)}_e Q_e (x) )^2 " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%28%5Chat+F%5E%7B%28k%29%7D+%28x%29+-+%5Chat+F%5E%7B%28k%29%7D+%28y%29%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F^{(k)} (x) - \hat F^{(k)} (y))^2" class="latex" title="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F^{(k)} (x) - \hat F^{(k)} (y))^2" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Be%5Cin+E_k%7D+%28w%5E%7B%28k-1%29%7D_e%29%5E2+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{e\in E_k} (w^{(k-1)}_e)^2 \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2 " class="latex" title="\displaystyle  = \sum_{e\in E_k} (w^{(k-1)}_e)^2 \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2 " /></p>
<p> To complete the proof, we argue that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e+%28x%29+-+Q_e+%28y%29%29%5E2+%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (Q_e (x) - Q_e (y))^2 \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" class="latex" title="\displaystyle  (Q_e (x) - Q_e (y))^2 \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" /></p>
<p> To verify the above inequality, assume <img src="https://s0.wp.com/latex.php?latex=%7BQ_e%28x%29+%5Cgeq+Q_e%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_e(x) \geq Q_e(y)}" class="latex" title="{Q_e(x) \geq Q_e(y)}" />, otherwise the argument will be symmetric, and call <img src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a',b'}" class="latex" title="{a',b'}" /> the maximizer for <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Ba%27%27%2Cb%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a'',b''}" class="latex" title="{a'',b''}" /> the maximizer for <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e+%28x%29+-+Q_e+%28y%29%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (Q_e (x) - Q_e (y))^2" class="latex" title="\displaystyle  (Q_e (x) - Q_e (y))^2" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%28%28x_%7Ba%27%7D+-+x_%7Bb%27%7D%29%5E2+-+%28y_%7Ba%27%27%7D+-+y_%7Bb%27%27%7D%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = ((x_{a'} - x_{b'})^2 - (y_{a''} - y_{b''})^2 " class="latex" title="\displaystyle  = ((x_{a'} - x_{b'})^2 - (y_{a''} - y_{b''})^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%28%28x_%7Ba%27%7D+-+x_%7Bb%27%7D%29%5E2+-+%28y_%7Ba%27%7D+-+y_%7Bb%27%7D%29%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq ((x_{a'} - x_{b'})^2 - (y_{a'} - y_{b'})^2)^2 " class="latex" title="\displaystyle  \leq ((x_{a'} - x_{b'})^2 - (y_{a'} - y_{b'})^2)^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" class="latex" title="\displaystyle  \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
It remains to estimate the expected sup </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5C+%5Csup_%7Bx%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) " class="latex" title="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) " /></p>
<p> and the diameter </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%5Cin+T%7D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28+%5Chat+F%5E%7B%28k%29%7D+%28x%29+-+%5Chat+F%5E%7B%28k%29%7D+%28y%29%29%5E2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x,y \in T} \sqrt{ \mathop{\mathbb E} ( \hat F^{(k)} (x) - \hat F^{(k)} (y))^2 } " class="latex" title="\displaystyle  \sup_{x,y \in T} \sqrt{ \mathop{\mathbb E} ( \hat F^{(k)} (x) - \hat F^{(k)} (y))^2 } " /></p>
<p> where, recall, <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x+%3A+x%5ET+L_G+x+%3D+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T = \{ x : x^T L_G x = 1\}}" class="latex" title="{T = \{ x : x^T L_G x = 1\}}" />.</p>
<p>
With the usual change of variable we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x \in T} \ \hat F^{(k)} (x)" class="latex" title="\displaystyle  \sup_{x \in T} \ \hat F^{(k)} (x)" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csup_%7By%3A+%7C%7Cy%7C%7C%3D+1%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28L_G%5E%7B-1%2F2%7D+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sup_{y: ||y||= 1} \ \hat F^{(k)} (L_G^{-1/2} y) " class="latex" title="\displaystyle  = \sup_{y: ||y||= 1} \ \hat F^{(k)} (L_G^{-1/2} y) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7Ba%2Cb%7D+L_G%5E%7B-1%2F2%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ L_{G}^{-1/2} L_{a,b} L_G^{-1/2} \right \| " class="latex" title="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ L_{G}^{-1/2} L_{a,b} L_G^{-1/2} \right \| " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " class="latex" title="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " /></p>
<p> where, as before, we use the notation <img src="https://s0.wp.com/latex.php?latex=%7BM_%7Ba%2Cb%7D+%3A%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7Ba%2Cb%7D+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M_{a,b} := L_{G}^{-1/2} L_{a,b} L_G^{-1/2} }" class="latex" title="{M_{a,b} := L_{G}^{-1/2} L_{a,b} L_G^{-1/2} }" />, <img src="https://s0.wp.com/latex.php?latex=%7BM%5E%7B%28k%29%7D+%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7BG%5E%7B%28k%29%7D+%7D+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M^{(k)} = L_{G}^{-1/2} L_{G^{(k)} } L_G^{-1/2} }" class="latex" title="{M^{(k)} = L_{G}^{-1/2} L_{G^{(k)} } L_G^{-1/2} }" /> and <img src="https://s0.wp.com/latex.php?latex=%7BM+%3A%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_G+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M := L_{G}^{-1/2} L_G L_G^{-1/2} }" class="latex" title="{M := L_{G}^{-1/2} L_G L_G^{-1/2} }" />.</p>
<p>
By matrix Chernoff bounds for Gaussian sums of matrices,</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " class="latex" title="\displaystyle  \mathop{\mathbb E} \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " /></p>
<p> <a name="eq.after.chernoff"></a></p><a name="eq.after.chernoff">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cleq+O%28%5Csqrt%7B%5Clog+n%7D%29+%5Ccdot+%5Csqrt%7B%5Cmax_%7Be%5Cin+E_k%2C+a%2Cb%5Cin+e%7D+%5Cleft%5C%7C+w%5E%7B%28k-1%29%7D_e+M_%7Ba%2Cb%7D+%5Cright%5C%7C%7D+%5Csqrt%7B+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+w%5E%7B%28k-1%29%7D_e+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   \leq O(\sqrt{\log n}) \cdot \sqrt{\max_{e\in E_k, a,b\in e} \left\| w^{(k-1)}_e M_{a,b} \right\|} \sqrt{ \left\| \sum_{e\in E_k} \sum_{a,b\in e} w^{(k-1)}_e \ M_{a,b} \right \| } \ \ \ \ \ (1)" class="latex" title="\displaystyle   \leq O(\sqrt{\log n}) \cdot \sqrt{\max_{e\in E_k, a,b\in e} \left\| w^{(k-1)}_e M_{a,b} \right\|} \sqrt{ \left\| \sum_{e\in E_k} \sum_{a,b\in e} w^{(k-1)}_e \ M_{a,b} \right \| } \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.after.chernoff"></a></p>
<p>
Recall that if <img src="https://s0.wp.com/latex.php?latex=%7Be%5Cin+E_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e\in E_k}" class="latex" title="{e\in E_k}" /> is a hyperedge that is active at round <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />, then <img src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_e \leq 2^{k-1-\ell}}" class="latex" title="{p_e \leq 2^{k-1-\ell}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D+%2F+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w^{(k-1)}_e \leq 2^{k-1-\ell} / p_e}" class="latex" title="{w^{(k-1)}_e \leq 2^{k-1-\ell} / p_e}" />, and we also have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_e+%5Cgeq+B+%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C+%5Cgeq+B+%5Cfrac+r2+%5Cmax_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  p_e \geq B \sum_{a,b\in e} ||M_{a,b}|| \geq B \frac r2 \max_{a,b\in e} ||M_{a,b}||" class="latex" title="\displaystyle  p_e \geq B \sum_{a,b\in e} ||M_{a,b}|| \geq B \frac r2 \max_{a,b\in e} ||M_{a,b}||" /></p>
<p> so that we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7Be%5Cin+E_k%2C+a%2Cb%5Cin+e%7D+w%5E%7B%28k-1%29%7D_e+%7C%7CM_%7Ba%2Cb%7D+%7C%7C+%5Cleq+%5Cfrac%7B2%5E%7Bk-1-%5Cell%7D%7D+%7BBr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \max_{e\in E_k, a,b\in e} w^{(k-1)}_e ||M_{a,b} || \leq \frac{2^{k-1-\ell}} {Br}" class="latex" title="\displaystyle  \max_{e\in E_k, a,b\in e} w^{(k-1)}_e ||M_{a,b} || \leq \frac{2^{k-1-\ell}} {Br}" /></p>
<p> The last term of <a href="https://lucatrevisan.wordpress.com/feed/#eq.after.chernoff">(1)</a> is the spectral norm of <img src="https://s0.wp.com/latex.php?latex=%7BM%5E%7B%28k-1%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M^{(k-1)}}" class="latex" title="{M^{(k-1)}}" />, which is at most 2 by the assumption of the Lemma.</p>
<p>
Collecting all the pieces, we have proved that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5C+%5Csup_%7Bx%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7B%5Clog+n+%5Ccdot+%5Cfrac+%7B2%5E%7Bk-%5Cell%7D%7D%7BB+r%7D%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) \leq O \left( \sqrt{\log n \cdot \frac {2^{k-\ell}}{B r}}\right) " class="latex" title="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) \leq O \left( \sqrt{\log n \cdot \frac {2^{k-\ell}}{B r}}\right) " /></p>
<p>
We also need to bound the diameter of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, that is</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%5Cin+T%7D+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%28%5Chat+F%5E%7B%28k%29%7D%28x%29+-%5Chat+F%5E%7B%28k%29%7D+%28y%29+%29%5E2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x,y \in T} \sqrt{\mathop{\mathbb E} (\hat F^{(k)}(x) -\hat F^{(k)} (y) )^2 } " class="latex" title="\displaystyle  \sup_{x,y \in T} \sqrt{\mathop{\mathbb E} (\hat F^{(k)}(x) -\hat F^{(k)} (y) )^2 } " /></p>
<p> under the usual change of basis, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y}" class="latex" title="{x,y}" /> of length 1 we want to bound the square root of </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D%5Cleft+%28+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%7D+g_%7Ba%2Ce%2Cb%7D+x%5ET+%28M_%7Ba%2Cb%7D+x+-+y%5ET+M_%7Ba%2Cb%7D+y%29%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}\left ( \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} g_{a,e,b} x^T (M_{a,b} x - y^T M_{a,b} y)\right)^2 " class="latex" title="\displaystyle  \mathop{\mathbb E}\left ( \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} g_{a,e,b} x^T (M_{a,b} x - y^T M_{a,b} y)\right)^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+-+y%5ET+M_%7Ba%2Cb%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x - y^T M_{a,b} y)^2 " class="latex" title="\displaystyle  = \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x - y^T M_{a,b} y)^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x%29%5E2+%2B+%28y%5ET+M_%7Ba%2Cb%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x)^2 + (y^T M_{a,b} y)^2 " class="latex" title="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x)^2 + (y^T M_{a,b} y)^2 " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Cmax_%7Ba%2Cb+%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D+%7C%7C+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+%2B+y%5ET+M_%7Ba%2Cb%7D+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \max_{a,b \in e} ||M_{a,b} || \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y) " class="latex" title="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \max_{a,b \in e} ||M_{a,b} || \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y) " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w^{(k-1)}_e}" class="latex" title="{w^{(k-1)}_e}" /> is either zero or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%5E%7Bk-%5Cell-1%7D%7D%7BB+%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_e%7C%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{2^{k-\ell-1}}{B \sum_{a,b\in e} ||M_e||}}" class="latex" title="{\frac{2^{k-\ell-1}}{B \sum_{a,b\in e} ||M_e||}}" /> and we can continue our chain of inequalities with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+%2B+y%5ET+M_%7Ba%2Cb%7D+y+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \frac{2^{k-\ell}}{Br} \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y )" class="latex" title="\displaystyle  \leq \frac{2^{k-\ell}}{Br} \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y )" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+%28x%5ET+M%5E%7B%28k-1%29%7D+x+%2B+y%5ET+M%5E%7B%28k-1%29%7D+y+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \frac{2^{k-\ell}}{Br} (x^T M^{(k-1)} x + y^T M^{(k-1)} y ) " class="latex" title="\displaystyle  = \frac{2^{k-\ell}}{Br} (x^T M^{(k-1)} x + y^T M^{(k-1)} y ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+4+%5Ccdot+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq 4 \cdot \frac{2^{k-\ell}}{Br} " class="latex" title="\displaystyle  \leq 4 \cdot \frac{2^{k-\ell}}{Br} " /></p>
<p> where we used again the assumption of the Lemma <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+M%5E%7B%28k-1%29%7D+%7C%7C+%5Cleq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|| M^{(k-1)} || \leq 2}" class="latex" title="{|| M^{(k-1)} || \leq 2}" />.</p>
<p>
To prove the last part of the lemma, we have to prove that with high probability</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M%5E%7B%28k%29%7D+-+M%5E%7B%28k-1%29%7D+%7C%7C+%5Cleq+O%5Cleft%28+%5Csqrt%7B+%5Cfrac+%7B2%5E%7Bk-%5Cell%7D+%5Clog+n%7D%7BB%7D%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  || M^{(k)} - M^{(k-1)} || \leq O\left( \sqrt{ \frac {2^{k-\ell} \log n}{B}} \right) " class="latex" title="\displaystyle  || M^{(k)} - M^{(k-1)} || \leq O\left( \sqrt{ \frac {2^{k-\ell} \log n}{B}} \right) " /></p>
<p> We see that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%5E%7B%28k%29%7D+-+M%5E%7B%28k-1%29%7D+%3D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+r%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e%7D+M_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  M^{(k)} - M^{(k-1)} = \sum_{e\in E_k} w^{(k-1)}_e r^{(k)}_e \sum_{a,b \in e} M_{a,b} " class="latex" title="\displaystyle  M^{(k)} - M^{(k-1)} = \sum_{e\in E_k} w^{(k-1)}_e r^{(k)}_e \sum_{a,b \in e} M_{a,b} " /></p>
<p> as noted before, each <img src="https://s0.wp.com/latex.php?latex=%7B+w%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ w^{(k-1)}_e}" class="latex" title="{ w^{(k-1)}_e}" /> is either zero or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+%7B2%5E%7Bk-1-%5Cell%7D%7D%7BB%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac {2^{k-1-\ell}}{B\sum_{a,b\in e} ||M_{a,b}||}}" class="latex" title="{\frac {2^{k-1-\ell}}{B\sum_{a,b\in e} ||M_{a,b}||}}" />, so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%5C%7C+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e%7D+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+%5Cleq+%5Cfrac+%7B2%5E%7Bk-1-%5Cell%7D%7D%7BB%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left \| w^{(k-1)}_e \sum_{a,b \in e} M_{a,b} \right \| \leq \frac {2^{k-1-\ell}}{B} " class="latex" title="\displaystyle  \left \| w^{(k-1)}_e \sum_{a,b \in e} M_{a,b} \right \| \leq \frac {2^{k-1-\ell}}{B} " /></p>
<p> and the final claim follows from matrix Chernoff bounds.</p>
<p>
Now we can put everything together. Applying our lemma inductively, we can say that with high probability</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%28x%29%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%5E%7B%28k%29%7D+%28x%29%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+O%5Cleft%28+%5Csqrt%7B+2%5E%7Bk-%5Cell%7D+%5Cfrac%7B%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7B+%5Cfrac%7B%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1} |F^{(k)} (x)| \leq \sum_{k=1}^\ell O\left( \sqrt{ 2^{k-\ell} \frac{\log n}{Br} } \right) \leq O \left( \sqrt{ \frac{\log n}{Br} } \right) " class="latex" title="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1} |F^{(k)} (x)| \leq \sum_{k=1}^\ell O\left( \sqrt{ 2^{k-\ell} \frac{\log n}{Br} } \right) \leq O \left( \sqrt{ \frac{\log n}{Br} } \right) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+k%3A+L%5E%7B%28k%29%7D+_G+%5Cpreceq+O+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B+2%5E%7Bi-%5Cell%7D+%5Cfrac%7B%5Clog+n%7D%7BB%7D+%7D+%5Cright%29+%5Ccdot+L_G+%5Cpreceq+2+L_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall k: L^{(k)} _G \preceq O \left( \sum_{i=1}^k \sqrt{ 2^{i-\ell} \frac{\log n}{B} } \right) \cdot L_G \preceq 2 L_G " class="latex" title="\displaystyle  \forall k: L^{(k)} _G \preceq O \left( \sum_{i=1}^k \sqrt{ 2^{i-\ell} \frac{\log n}{B} } \right) \cdot L_G \preceq 2 L_G " /></p>
<p> provided that we choose <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> at least at absolute constant times <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log n}" class="latex" title="{\log n}" />.</p>
<p>
In particular, we can choose <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> to be an absolute constant times <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%5E%7B-2%7D+r%5E3+%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon^{-2} r^3 \log n}" class="latex" title="{\epsilon^{-2} r^3 \log n}" /> and have</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%28x%29%7C+%5Cleq+%5Cfrac%7B2%7D%7Br%5E2%7D+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \frac{2}{r^2} \epsilon " class="latex" title="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \frac{2}{r^2} \epsilon " /></p>
<p> which is the same as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%3A+%5C+%5C+%7CQ_%7BH%7D+%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+%5Cleq+%5Cfrac%7B2%7D%7Br%5E2%7D+%5Cepsilon+%5Ccdot+x%5ET+L_G+x+%5Cleq+%5Cepsilon%5C+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall x: \ \ |Q_{H} (x) - Q_{H'} (x) | \leq \frac{2}{r^2} \epsilon \cdot x^T L_G x \leq \epsilon\ Q_H(x) " class="latex" title="\displaystyle  \forall x: \ \ |Q_{H} (x) - Q_{H'} (x) | \leq \frac{2}{r^2} \epsilon \cdot x^T L_G x \leq \epsilon\ Q_H(x) " /></p>
<p>
So, with high probability, <img src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H'}" class="latex" title="{H'}" /> is a spectral sparsifier of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> with error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />, and it has <img src="https://s0.wp.com/latex.php?latex=%7BO%28Bn%29+%3D+O%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(Bn) = O(\epsilon^{-2} r^3 n \log n)}" class="latex" title="{O(Bn) = O(\epsilon^{-2} r^3 n \log n)}" /> hyperedges</p>
<p>
</p><p><b>4. Some Additional Thoughts </b></p>
<p></p><p>
The dependence on <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is definitely not optimal, particularly because when <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is order of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> we know from the work of Soma and Yoshida that we can do better. One place where we seem to lose is that, although we know</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%5Cleq+O+%5Cleft%28+%5Cfrac+1r+%5Cright%29+%5C+%5Csum_%7Ba%2Cb%5Cin+%7D+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q_e(x) \leq O \left( \frac 1r \right) \ \sum_{a,b\in } x^T L_{a,b} x " class="latex" title="\displaystyle  Q_e(x) \leq O \left( \frac 1r \right) \ \sum_{a,b\in } x^T L_{a,b} x " /></p>
<p> we are only able to show that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Be%5Cin+E_k%7D+r%5E%7B%28k%29%7D_e+w%5E%7B%28k%29%7D_e+Q_e%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{e\in E_k} r^{(k)}_e w^{(k)}_e Q_e(x) " class="latex" title="\displaystyle  \sum_{e\in E_k} r^{(k)}_e w^{(k)}_e Q_e(x) " /></p>
<p> is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1)}" class="latex" title="{O(1)}" />-dominated by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Be%5Cin+E_k%7D+g%5E%7B%28k%29%7D_e+w%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e+%7D+x%5ET+L_%7Bab%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{e\in E_k} g^{(k)}_e w^{(k)}_e \sum_{a,b \in e } x^T L_{ab} x " class="latex" title="\displaystyle  \sum_{e\in E_k} g^{(k)}_e w^{(k)}_e \sum_{a,b \in e } x^T L_{ab} x " /></p>
<p> rather than, as we could have hoped, <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2Fr%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(1/r)}" class="latex" title="{O(1/r)}" />-dominated. The difficulty is that the bound</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e%28x%29+-+Q_e%28y%29%29%5E2+%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28x%5ET+L_%7Bab%7D+x+-+y%5ET+L_%7Bab%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (Q_e(x) - Q_e(y))^2 \leq \sum_{a,b\in e} (x^T L_{ab} x - y^T L_{ab} y)^2 " class="latex" title="\displaystyle  (Q_e(x) - Q_e(y))^2 \leq \sum_{a,b\in e} (x^T L_{ab} x - y^T L_{ab} y)^2 " /></p>
<p> is sometimes tight. In order to do better, it seems necessary to do something a bit differently from what we do here. </p>
<p>
The effective resistance of an edge <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a,b)}" class="latex" title="{(a,b)}" /> in a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> can be written as</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cfrac%7Bx%5ET+L_%7Ba%2Cb%7D+x%7D%7Bx%5ET+L_G+x%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{x^T L_{a,b} x}{x^T L_G x} " class="latex" title="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{x^T L_{a,b} x}{x^T L_G x} " /></p>
<p>
with the convention that <img src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0/0 = 0}" class="latex" title="{0/0 = 0}" />. So it seems reasonable that a good definition of effective resistance for an hyperedge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> in a hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> would be</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cfrac%7BQ_e%28x%29%7D%7BQ_H%28x%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{Q_e(x)}{Q_H(x) } " class="latex" title="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{Q_e(x)}{Q_H(x) } " /></p>
<p> One can argue that these “effective resistances” add up to <img src="https://s0.wp.com/latex.php?latex=%7BO%28nr%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(nr)}" class="latex" title="{O(nr)}" />, but perhaps they add up to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n)}" class="latex" title="{O(n)}" />? If we sample according to those “effective resitances,” can we apply generic chaining directly to <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k%29%7D_e+g%5E%7B%28k%29%7D_e+Q_e%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{e\in E_k} w^{(k)}_e g^{(k)}_e Q_e(x)}" class="latex" title="{\sum_{e\in E_k} w^{(k)}_e g^{(k)}_e Q_e(x)}" /> without having to rely on a Gaussian process on matrices, for which we have Chernoff bounds?</p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/05/14/spectral-sparsification-of-hypergraphs/"><span class="datestr">at May 14, 2020 04:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1082301538511422857">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/05/awesome-video-from-women-in-theory.html">Awesome Video from Women In Theory!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Below is an <a href="https://www.youtube.com/watch?v=4Wl-3kadvgw">awesome video</a> made by WIT (Women In Theory) on May 10, 2020 to celebrate the women in our field and in place of the Women in Theory Workshop that was supposed to take place<br />
<div>
@Simons in June. ENJOY:</div>
<div>
<br /></div>
<div>
<br /></div>
<div>
<br />

</div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/05/awesome-video-from-women-in-theory.html"><span class="datestr">at May 14, 2020 01:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/05/14/6-year-postdoc-at-tu-wien-vienna-austria-apply-by-may-28-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/05/14/6-year-postdoc-at-tu-wien-vienna-austria-apply-by-may-28-2020/">6-year postdoc at TU Wien, Vienna, Austria (apply by May 28, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A 6-Year Postdoc Position in Algorithms is available at TU Wien, Vienna, Austria. Research experience in one of the following areas is of advantage: parameterized complexity, algorithmic applications of graph decompositions, SAT and CSP.</p>
<p>Website: <a href="https://www.ac.tuwien.ac.at/jobs/#junprof">https://www.ac.tuwien.ac.at/jobs/#junprof</a><br />
Email: sz@ac.tuwien.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/05/14/6-year-postdoc-at-tu-wien-vienna-austria-apply-by-may-28-2020/"><span class="datestr">at May 14, 2020 10:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
