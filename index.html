<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 26, 2021 04:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/25/multiple-open-rank-tenure-track-faculty-positions-at-university-of-illinois-urbana-champaign-uiuc-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/25/multiple-open-rank-tenure-track-faculty-positions-at-university-of-illinois-urbana-champaign-uiuc-apply-by-december-1-2021/">Multiple open rank tenure track faculty positions  at University of Illinois, Urbana-Champaign (UIUC) (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>While we welcome applications from exceptional candidates in all areas, we particularly encourage applications from candidates working in quantum computing, cloud computing and data centers, interactive computing (HCI, HRI, wearable computing, AR/VR, graphics), the intersection of systems/architecture and artificial intelligence/machine learning, and the social impacts of computing.</p>
<p>Website: <a href="https://cs.illinois.edu/about/positions/faculty-positions">https://cs.illinois.edu/about/positions/faculty-positions</a><br />
Email: chekuri@illinois.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/25/multiple-open-rank-tenure-track-faculty-positions-at-university-of-illinois-urbana-champaign-uiuc-apply-by-december-1-2021/"><span class="datestr">at October 25, 2021 07:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">How to deploy machine learning with differential privacy?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In many applications of machine learning, such as machine learning for medical diagnosis, we would like to have machine learning algorithms that do not memorize sensitive information about the training set, such as the specific medical histories of individual patients. Differential privacy is a notion that allows quantifying the degree of privacy protection provided by an algorithm on the underlying (sensitive) data set it operates on. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data.</p>

<h2 id="why-do-we-need-private-machine-learning-algorithms">Why do we need private machine learning algorithms?</h2>

<p>Machine learning algorithms work by studying a lot of data and updating their parameters to encode the relationships in that data. Ideally, we would like the parameters of these machine learning models to encode general patterns (e.g., ‘‘patients who smoke are more likely to have heart disease’’) rather than facts about specific training examples (e.g., “Jane Smith has heart disease”). Unfortunately, machine learning algorithms do not learn to ignore these specifics by default. If we want to use machine learning to solve an important task, like making a cancer diagnosis model, then when we publish that machine learning model (for example, by making an open source cancer diagnosis model for doctors all over the world to use) we might also inadvertently reveal information about the training set. A malicious attacker might be able to inspect the published model’s predictions and learn private information about Jane Smith. For instance, the adversary could mount a membership inference attack to know whether or not Jane Smith contributed her data to the model’s training set [SSS17]. The adversary could also build on membership inference attacks to extract training data by repeatedly guessing possible training points until they result in a sufficiently strong membership signal from the model’s prediction [CTW20]. In many instances, the model itself may be represented by a few of the data samples (e.g., Support Vector Machine in its dual form).</p>

<p>A common misconception is that if a model generalizes (i.e., performs well on the test examples), then it preserves privacy. As mentioned earlier, this is far from being true. One of the main reasons being that generalization is an average case behavior of a model (over the distribution of data samples), whereas privacy must be provided for everyone, including outliers (which may deviate from our distributional assumptions).</p>

<p>Over the years, researchers have proposed various approaches towards protecting privacy in learning algorithms (k-anonymity [SS98], l-diversity [MKG07], m-invariance [XT07], t-closeness [LLV07] etc.). Unfortunately, [GKS08] all these approaches are vulnerable to what are called composition attacks, that use auxiliary information to violate the privacy protection. Famously, this strategy allowed researchers to de-anonymize part of a movie ratings dataset released to participants of the Netflix Prize when the individuals had also shared their movie ratings publicly on the Internet Movie Database (IMDb) [NS08]. If Jane Smith had assigned the same ratings to movies A, B and C in the Netflix Prize dataset and publicly on IMDb at similar times, then researchers could link data corresponding to Jane across both datasets. This would in turn give them the means to recover ratings that were included in the Netflix Prize but not on IMDb. This example shows how difficult it is to define and guarantee privacy because it is hard to estimate the scope of knowledge—about individuals—available to adversaries. While the dataset released by Netflix has since been taken down, it is difficult to ensure that all of its copies have been deleted. In recent years, data sample instance encoding based methods like InstaHide [HSL20], and NeuraCrypt [YEO21] have been demonstrated to be vulnerable to such composition attacks as well.</p>

<p>As a result, the research community has converged on differential privacy [DMNS06], which provides the following semantic guarantee, as opposed to ad-hoc approaches: An adversary learns almost the same information about an individual whether or not they are present in or absent from the training data set. In particular, it provides a condition on the algorithm, independent from who might be attacking it, or the specifics of the data set instantiation.  Put another way, differential privacy is a framework for evaluating the guarantees provided by a system that was designed to protect privacy. Such systems can be applied directly to “raw” data which potentially still contains sensitive information, altogether removing the need for procedures that sanitize or anonymize data and are prone to the failures described previously. That said, minimizing data collection in the first place remains a good practice to limit other forms of privacy risk.</p>

<h2 id="designing-private-machine-learning-algorithms-via-differential-privacy">Designing Private Machine Learning Algorithms via Differential Privacy</h2>

<p>Differential privacy [DMNS06] is a semantic notion of privacy that addresses a lot of the limitations of previous approaches like k-anonymity. The basic idea is to randomize part of the mechanism’s behavior to provide privacy. In our case, the mechanism considered is a learning algorithm, but the differential privacy framework can be applied to study any algorithm.</p>

<p>The intuition for why we introduce randomness into the learning algorithm is that it obscures the contribution of an individual, but does not obscure important statistical patterns. Without randomness, we would be able to ask questions like: “What parameters does the learning algorithm choose when we train it on this specific dataset?” With randomness in the learning algorithm, we instead ask questions like: “What is the probability that the learning algorithm will choose parameters in this set of possible parameters, when we train it on this specific dataset?”</p>

<p>We use a version of differential privacy which requires (in our use case of machine learning) that the probability of learning any particular set of parameters stays roughly the same if we change a single data record in the training set. A data record can be a single training example from an individual, or the collection of all the training examples provided by an individual. The former is often referred to as example level/item level privacy, and the latter is referred to as user level differential privacy. While user level privacy provides stronger semantics, it may be harder to achieve. For a more thorough discussion about the taxonomy of these notions, see [DNPR10, JTT18, HR12, HR13]. In this document, for the ease of exposition of the technical results, we focus on the example level notion. This could mean to add a training example, remove a training example, or change the values within one training example. The intuition is that if a single patient (Jane Smith) does not affect the outcome of learning much, then that patient’s records cannot be memorized and her privacy is respected. In the rest of this post, how much a single record can affect the outcome of learning is called the sensitivity of an algorithm.</p>

<p>The guarantee of differential privacy is that the adversary is not able to distinguish the answers produced by the randomized algorithm based on the data of two of the three users from the answers returned by the same algorithm based on the data of all three users. We also refer to the degree of indistinguishability as the privacy loss. Smaller privacy loss corresponds to a stronger privacy guarantee.</p>

<p>It is often thought that privacy is a fundamental bottleneck in obtaining good prediction accuracy/generalization for machine learning algorithms. In fact, recent research has shown that in many instances it actually helps in designing algorithms with strong generalization ability. Some of the examples where DP has resulted in designing better learning algorithms are Online linear predictions [KV05] and online PCA [DTTZ13]. Notably, [DFH15] formally showed that generalization for any DP learning algorithm comes for free. More concretely, if a DP learning algorithm has good training accuracy, it is guaranteed to have good test accuracy.
This is true because differential privacy itself acts as a very strong form of regularization.</p>

<p>One might argue that the generalization guarantee which a DP algorithm can achieve may be sub-par to that of its non-private baselines. For a large class of learning tasks, one can show that asymptotically DP does not introduce any further error beyond the inherent statistical error [SSTT21]. [ACG16,BFTT19] highlights that in the presence of enough data, a DP algorithm can get arbitrarily close to the inherent statistical error, even under strong privacy parameters.</p>

<h2 id="private-empirical-risk-minimization">Private Empirical Risk Minimization</h2>

<p>Before we go into the design of specific differentially private learning algorithms, we first formalize the problem setup, and standardize some notation. Consider a training data set \(D={(x_1,y_1),…,(x_n,y_n)}\) drawn i.i.d. from some fixed (unknown) distribution \(\Pi\), with the feature vector being \(x_i\) and label/response being \(y_i\). We define the training loss at any model \(\theta\) as \(L_{train} (\theta, D) = \frac{1}{n} \sum_{i=1}^{n} l(\theta; (x_i, y_i))\), and the corresponding test loss as \(L_{test} (\theta) = E_{(x,y) \sim \Pi} l(\theta;(x,y)) \).
We will design DP algorithms to output models that approximately minimize the test loss while having access only to the training loss.</p>

<p>In the literature, there are a variety of approaches towards designing these DP learning algorithms [CMS11, KST12, BST14, PAE16, BTT18]. One can categorize them broadly as: i) algorithms that assume that the individual loss function \(l(\theta;\cdot) \) is convex in the model parameter  to ensure differential privacy, ii) algorithms that are differentially private even when the loss function is non-convex in nature (e.g., deep learning models), and iii) model agnostic algorithms, that do not require any information about the representation of the model \(\theta\), or the loss function \(l(\theta;\cdot) \). In our current discussion, we will only focus on designing algorithms for (ii), and (iii). This is because it turns out that the best known algorithms for (ii) are already competitive to algorithms that are specific for (i) [INS19].</p>

<h2 id="private-algorithms-for-training-deep-learning-models">Private Algorithms for Training Deep Learning Models</h2>

<p>The first approach, due to SCS13, BST14, and ACG16, is named differentially private stochastic gradient descent (DP-SGD). It proposes to modify the model updates computed by the most common optimizer used in deep learning: stochastic gradient descent (SGD). Typically, stochastic gradient descent trains iteratively. At each iteration, a small number of training examples (a “minibatch”) are sampled from the training set. The optimizer computes the average model error on these examples, and then differentiates this average error with respect to each of the model parameters to obtain a gradient vector. Finally, the model parameters (\(\theta_t\)) are updated by subtracting this gradient (\(\nabla_t\)) multiplied by a small constant \(\eta\) (the learning rate controls how quickly the optimizer updates the model’s parameters). At a high level, two modifications are made by DP-SGD to obtain differential privacy: gradients, which are computed on a per-example basis (rather than averaged over multiple examples), are first clipped to control their sensitivity, and, second, spherical Gaussian noise \(b_t\) is added to their sum to obtain the indistinguishability needed for DP. Succinctly, the update step can be written as follows: \(\theta_{t+1} \leftarrow \theta_t - \eta \cdot (\nabla_t + b_t)\).</p>

<p>Let us take the example of a hospital training a model to predict whether patients will be readmitted after being released. To train the model, the hospital uses information from patient records, such as demographic variables and admission variables (e.g., age, ethnicity, insurance type, type of Intensive Care Unit admitted to) but also time-varying vitals and labs (e.g., heart rate, blood pressure, white blood cell counts) [JPS16]. The modifications made by DP-SGD ensure that if (1) Jane Smith’s individual patient record contained unusual features, e.g., her insurance provider was uncommon for people of her age or her heart rate followed an unusual pattern, the resulting signal will have a bounded impact on our model updates, and (2) the model’s final parameters would be essentially identical should Jane Smith have chosen to not contribute (i.e., opt-out) her patient record to the training set. Stronger differential privacy is achieved when one is able to introduce more noise (i.e., sample noise with larger standard deviation) and train for as few iterations as possible.</p>

<p>Two main components in the above DP-SGD algorithm that distinguishes itself from traditional SGD are: i) per-example clipping and ii) Gaussian noise addition. In addition, for the analysis to hold, DP-SGD requires that sub-sampling of mini batches is uniform at random from the training data set. While this is not a requirement of DP-SGD per se, in practice many implementations of SGD do not satisfy this requirement and instead analyze different permutations of the data at each epoch of training.</p>

<p>While gradient clipping is common in deep learning, often used as a form of regularization, it differs from that in DP-SGD as follows: The average gradient over the minibatch is clipped, as opposed to clipping the gradient of individual examples (i.e., \(l(\theta_t;(x,y)) \) before averaging. It is an ongoing research direction to both understand the effect of per-example clipping in DP-SGD in model training [SSTT21], and also effective ways to mitigate its impact both in terms of accuracy [PTS21], and training time [ZHS19].</p>

<p>In standard stochastic gradient descent, subsampling is usually used either as a way to speed up the training process [CAR16], or as a a form of regularization [RCR15]. In DP-SGD, the randomness in the subsampling of the minibatch is used to guarantee DP. The technical component for this sort of privacy analysis is called privacy amplification by subsampling [KLNRS08,BBG18]. Since the sampling randomness is used to guarantee DP, it is crucial that the uniformity in the sampling step is of cryptographic strength. Another, (possibly) counterintuitive feature of DP-SGD is that for best privacy/utility trade-off it is in general better to have larger batch sizes. In fact, full-batch DP-gradient descent may provide the best privacy/utility trade-offs, albeit at the expense of computational feasibility.</p>

<p>For a fixed DP guarantee, the magnitude of the Gaussian noise that gets added to the gradient updates in each step in DP-SGD is proportional to \(\sqrt{the\ number\ of\ steps}\) the model is trained for. As a result, it is important to tune the number of training steps for best privacy/utility trade-offs.</p>

<p>In the <a href="https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial.py">following tutorial</a>, we provide a small code snippet to train a model with DP-SGD.</p>

<h2 id="model-agnostic-private-learning">Model Agnostic Private Learning</h2>

<p>The Sample and Aggregate framework [NRS07] is a generic method to add differential privacy to a non-private algorithm without caring about the internal workings of it, a.k.a. model agnostic. In the context of machine learning, one can state the main idea as follows: Consider a multi-class classification problem. Take the training data, and split into k disjoint subsets of equal size. Train independent models \(\theta_1, \theta_2, …, \theta_k \) on the disjoint subsets. In order to predict on an test example x, first, compute a private histogram over the set of k predictions \(\theta_1(x), \theta_2(x), …, \theta_k(x) \). Then, select and output the bin in the histogram based on the highest count, after adding a small amount of Laplace/Gaussian noise to the counts. In the context of DP learning, this particular approach was used in two different lines of work: i) PATE [PAE16], and ii) Model agnostic private learning [BTT18]. While the latter focussed on obtaining theoretical privacy/utility trade-offs for a class of learning tasks (e.g., agnostic PAC learning), the PATE approach focuses on practical deployment. Both these lines of work make one common observation. If the predictions from \(\theta_1(x), \theta_2(x), …, \theta_k(x) \) are fairly consistent, then the privacy cost in terms of DP is very small. Hence, one can run a large number of prediction queries, without violating DP constraints. In the following, we describe the PATE approach in detail.</p>

<p>The private aggregation of teacher ensembles (PATE) demonstrated in particular that this approach allows one to learn deep neural networks with differential privacy. It proposes to have an ensemble of models trained without privacy predict with differential privacy by having these models predict in aggregate rather than revealing their individual predictions. In PATE, we start by partitioning the private dataset into smaller subsets of data. These subsets are partitions, so there is no overlap between the data included in any pair of partitions. If Jane Smith’s record was in our private dataset, then it is included in one of the partitions only. That is, only one of the teachers has analyzed Jane Smith’s record during training. We train a ML model, called a teacher, on each of these partitions. We now have an ensemble of teacher models that were trained independently, but without any guarantees of privacy. How do we use this ensemble to make predictions that respect privacy? In PATE, we add noise while aggregating the predictions made individually by each teacher to form a single common prediction. We count the number of teachers who voted for each class, and then perturb that count by adding random noise sampled from the Laplace or Gaussian distribution. Each label predicted by the noisy aggregation mechanism comes with rigorous differential privacy guarantees that bound the privacy budget spent to label that input. Again, stronger differential privacy is achieved when we are able to introduce more noise in the aggregation and are able to answer as few queries as possible. Let us now come back to our running example. Imagine that we’d like to use the output of PATE to know if Jane likes a particular movie. The only teacher trained on the partition containing Jane Smith’s data—has now learned that a record similar to Jane’s is characteristic of an individual who likes similar movies, and as a consequence changes its prediction on a test input which is similar to Jane’s to predict the movie rating assigned by Jane. However, because the teacher only contributes a single vote to the aggregation, and that the aggregation injects noise, we won’t be able to know whether the teacher changed its prediction to the movie rating assigned by Jane because the teacher indeed trained on Jane’s data or because the noise injected during the aggregation “flipped” that teacher’s vote. The random noise added to vote counts prevents the outcome of aggregation from reflecting the votes of any individual teachers to protect privacy.</p>

<h2 id="practically-deploying-differential-privacy-in-machine-learning">Practically deploying differential privacy in machine learning</h2>

<p>The two approaches we introduced have the advantage of being conceptually simple to understand. Fortunately, there also exist several open-source implementations of these approaches. For instance, DP-SGD is implemented in TensorFlow Privacy, Objax, and Opacus. This means that one is able to take an existing TensorFlow, JAX, or PyTorch pipeline for training a machine learning model and replace a non-private optimizer with DP-SGD. An example implementation of PATE is also available in TensorFlow Privacy. So what are the concrete potential obstacles to deploying machine learning with differential privacy?</p>

<p>The first obstacle is the accuracy of privacy-preserving models. Datasets are often sampled from distribution with heavy tails. For instance, in a medical application, there are typically (and fortunately) fewer patients with a given medical condition than patients without that condition. This means that there are fewer training examples for patients with each medical condition to learn from. Because differential privacy prevents us from learning patterns which are not found generally across the training data, it limits our ability to learn from these patients for which we have very few examples of [SPG]. More generally, there is often a trade-off between the accuracy of a model and the strength of the differential privacy guarantee it was trained with: the smaller the privacy budget is, the larger the impact on accuracy typically is. That said, this tension is not always inevitable and there are instances where privacy and accuracy are synergical because differential privacy implies generalization [DFH15] (but not vice versa).</p>

<p>The second obstacle to deploying differentially private machine learning can be the computational overhead. For instance, in DP-SGD one must compute per-example gradients rather than average gradients. This often means that optimizations implemented in machine learning frameworks to exploit matrix algebra supported by underlying hardware accelerators (e.g., GPUs) are harder to take advantage of. In another example, PATE requires that one train multiple models (the teachers) rather than a single model so this can also introduce overhead in the training procedure. Fortunately, this cost is mostly mitigated in recent implementations of private learning algorithms, in particular in Objax and Opacus.</p>

<p>The third obstacle to deploying differential privacy, in machine learning but more generally in any form of data analysis, is the choice of privacy budget. The smaller the budget, the stronger the guarantee is. This means one can compare two analyses and say which one is “more private”. However, this also means that it is unclear what is “small enough” of a privacy budget. This is particularly problematic given that applications of differential privacy to machine learning often require a privacy budget that provides little theoretical guarantees in order to train a model whose accuracy is large enough to warrant a useful deployment. Thus, it may be interesting for practitioners to evaluate the privacy of their machine learning algorithm by attacking it themselves. Whereas the theoretical analysis of an algorithm’s differential privacy guarantees provides a worst-case guarantee limiting how much private information the algorithm can leak against any adversary, implementing a specific attack can be useful to know how successful a particular adversary or class of adversaries would be. This helps interpret the theoretical guarantee but may not be treated as a direct substitute for it. Open-source implementations of such attacks are increasingly available: e.g., for membership inference <a href="https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack">here</a> and <a href="https://github.com/cchoquette/membership-inference">here</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In the above, we discussed some of the algorithmic approaches towards differentially private model training which have been effective both in theoretical and practical settings. Since it is a rapidly growing field, we could not cover all the important aspects of the research space. Some prominent ones include: i) Choice of the best hyperparameters in the training of DP models.In order to ensure that the overall algorithm preserves differential privacy, one needs to ensure that the choice of hyperparameters itself preserves DP. Recent research has provided algorithms for selecting the best hyperparameters in a differentially private fashion [LT19]. ii) Choice of network architecture: it is not always true that the best known model architectures for non-private model training are indeed the best for training with differential privacy. In particular, we know that the number of model parameters may have adverse effects on the privacy/utility trade-offs [BST14]. Hence, choosing the right model architecture is important for providing a good privacy/utility trade-off [PTS21]. (iii) Training in the federated/distributed setting: in the above exposition, we assumed that the training data lies in a single centralized location. However, in settings like Federated Learning (FL) [MMRHA17], the data records can be highly distributed, e.g., across various mobile devices. Running DP-SGD in the FL setting, which is required for FL to provide privacy guarantees for the training data, raises a series of challenges [KMA19] which are often facilitated by distributed private learning algorithms designed specifically for FL settings [BKMTT20, KMSTTZ21]. Some of the specific challenges in the context of FL include, limited and non-uniform availability of clients (holding individual data records) and unknown (and variable) size of the training data [BKMTT18]. On the other hand, PATE style algorithms lend themselves naturally to the distributed setting once combined with existing cryptographic primitives, as demonstrated by the CaPC protocol [CDD21]. It is an active area of research to address these above challenges.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The authors would like to thank Thomas Steinke and Andreas Terzis for detailed feedback and edit suggestions. Parts of this blog post previously appeared on <a href="https://differentialprivacy.org/www.cleverhans.io">www.cleverhans.io</a>.</p>

<h2 id="citations">Citations</h2>

<p>[ACG16] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., &amp; Zhang, L. (2016, October). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (pp. 308-318). ACM.</p>

<p>[BBG18] Balle, B., Barthe, G., &amp; Gaboardi, M. (2018). Privacy amplification by subsampling: Tight analyses via couplings and divergences. arXiv preprint arXiv:1807.01647.</p>

<p>[BKMTT18] Balle, B., Kairouz P., McMahan M., Thakkar O. &amp; Thakurta A. (2020). Privacy amplification via random check-ins. In NeurIPS.</p>

<p>[MMRHA17] McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR.</p>

<p>[KMSTTZ18] Kairouz P., McMahan M., Song S., Thakkar O., Thakurta A., &amp; Xu Z. (2021). Practical and Private (Deep) Learning without Sampling or Shuffling. In ICML.</p>

<p>[BFTT19] Bassily, R., Feldman, V., Talwar, K., &amp; Thakurta, A. Private Stochastic Convex Optimization with Optimal Rates. In NeurIPS 2019.</p>

<p>[BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science.</p>

<p>[BTT18] Bassily, R., Thakurta, A. G., &amp; Thakkar, O. D. (2018). Model-agnostic private learning. Advances in Neural Information Processing Systems.</p>

<p>[CDD21] Choquette-Choo, C. A., Dullerud, N., Dziedzic, A., Zhang, Y., Jha, S., Papernot, N., &amp; Wang, X. (2021). CaPC Learning: Confidential and Private Collaborative Learning. arXiv preprint arXiv:2102.05188.</p>

<p>[CMS11] Chaudhuri, K., Monteleoni, C., &amp; Sarwate, A. D. (2011). Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3).</p>

<p>[CTW20] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … &amp; Raffel, C. (2020). Extracting training data from large language models. arXiv preprint arXiv:2012.07805.</p>

<p>[DFH15] Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., &amp; Roth, A. (2015). Generalization in adaptive data analysis and holdout reuse. arXiv preprint arXiv:1506.02629.</p>

<p>[DMNS06] Dwork, C., McSherry, F., Nissim, K., &amp; Smith, A. (2006, March). Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference (pp. 265-284). Springer, Berlin, Heidelberg.</p>

<p>[DNPR10] Dwork, C., Naor, M., Pitassi, T., &amp; Rothblum, G. N. (2010, June). Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing (pp. 715-724).</p>

<p>[DTTZ14] Dwork, C., Talwar, K., Thakurta, A., &amp; Zhang, L. (2014, May). Analyze gauss: optimal bounds for privacy-preserving principal component analysis. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing (pp. 11-20).</p>

<p>[HSL20] Huang, Y., Song, Z., Li, K., &amp; Arora, S. (2020, November). Instahide: Instance-hiding schemes for private distributed learning. In International Conference on Machine Learning (pp. 4507-4518). PMLR.</p>

<p>[HR12] Hardt, M., &amp; Roth, A. (2012, May). Beating randomized response on incoherent matrices. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (pp. 1255-1268).</p>

<p>[HR13] Hardt, M., &amp; Roth, A. (2013, June). Beyond worst-case analysis in private singular vector computation. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing (pp. 331-340).</p>

<p>[JPS16] Johnson, A., Pollard, T., Shen, L. et al. MIMIC-III, a freely accessible critical care database. Sci Data 3, 160035 (2016). https://doi.org/10.1038/sdata.2016.35</p>

<p>[JTT18] Jain, P., Thakkar, O. D., &amp; Thakurta, A. (2018, July). Differentially private matrix completion revisited. In International Conference on Machine Learning (pp. 2215-2224). PMLR.</p>

<p>[INS19] Iyengar, R., Near, J. P., Song, D., Thakkar, O., Thakurta, A., &amp; Wang, L. (2019, May). Towards practical differentially private convex optimization. In 2019 IEEE Symposium on Security and Privacy (SP) (pp. 299-316). IEEE.</p>

<p>[KST12] Kifer, D., Smith, A., &amp; Thakurta, A. (2012, June). Private convex empirical risk minimization and high-dimensional regression. In Conference on Learning Theory (pp. 25-1). JMLR Workshop and Conference Proceedings.</p>

<p>[KMA19] Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., … &amp; Zhao, S. (2019). Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977.</p>

<p>[KV05] Kalai, Adam, and Santosh Vempala. “Efficient algorithms for online decision problems.” Journal of Computer and System Sciences 71.3 (2005): 291-307.</p>

<p>[KLNRS08] Raskhodnikova, S., Smith, A., Lee, H. K., Nissim, K., &amp; Kasiviswanathan, S. P. (2008). What can we learn privately. In Proceedings of the 54th Annual Symposium on Foundations of Computer Science (pp. 531-540).</p>

<p>[LLV07] Li, N., Li, T., &amp; Venkatasubramanian, S. (2007, April). t-closeness: Privacy beyond k-anonymity and l-diversity. In 2007 IEEE 23rd International Conference on Data Engineering (pp. 106-115). IEEE.</p>

<p>[LT19] Liu, J., &amp; Talwar, K. (2019, June). Private selection from private candidates. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (pp. 298-309).</p>

<p>[M17] Mironov, I. (2017, August). Renyi differential privacy. In Computer Security Foundations Symposium (CSF), 2017 IEEE 30th (pp. 263-275). IEEE.</p>

<p>[MKG07] Machanavajjhala, Ashwin; Kifer, Daniel; Gehrke, Johannes; Venkitasubramaniam, Muthuramakrishnan (March 2007). “L-diversity: Privacy Beyond K-anonymity”. ACM Transactions on Knowledge Discovery from Data.</p>

<p>[NRS07] Nissim, K., Raskhodnikova, S., &amp; Smith, A. (2007, June). Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing (pp. 75-84).</p>

<p>[NS08] Narayanan, A., &amp; Shmatikov, V. (2008, May). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.</p>

<p>[PAE16] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., &amp; Talwar, K. (2016). Semi-supervised knowledge transfer for deep learning from private training data. ICLR 2017.</p>

<p>[PTS21] Papernot, N., Thakurta, A., Song, S., Chien, S., &amp; Erlingsson, U. (2020). Tempered sigmoid activations for deep learning with differential privacy. AAAI 2021.</p>

<p>[RCR15] Rudi, A., Camoriano, R., &amp; Rosasco, L. (2015, December). Less is More: Nyström Computational Regularization. In NIPS (pp. 1657-1665).</p>

<p>[SCS13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP ’13, pages 245–248, Washington, DC, USA, 2013. IEEE Computer Society.</p>

<p>[SPG] Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings. Vinith Suriyakumar, Nicolas Papernot, Anna Goldenberg, Marzyeh Ghassemi. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.</p>

<p>[SS98] Samarati, Pierangela; Sweeney, Latanya (1998). “Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression” (PDF). Harvard Data Privacy Lab. Retrieved April 12, 2017</p>

<p>[SSS17] Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017, May). Membership inference attacks against machine learning models. In Security and Privacy (SP), 2017 IEEE Symposium on (pp. 3-18). IEEE.</p>

<p>[SSTT21] Song, S., Thakkar, O., &amp; Thakurta, A. (2020). Evading the Curse of Dimensionality in Unconstrained Private GLMs. In AISTATS 2021.</p>

<p>[XT07] Xiao X, Tao Y (2007) M-invariance: towards privacy preserving re-publication of dynamic datasets. In: SIGMOD conference, Beijing, China, pp 689–700</p>

<p>[YEO21] Yala, A., Esfahanizadeh, H., Oliveira, R. G. D., Duffy, K. R., Ghobadi, M., Jaakkola, T. S., … &amp; Medard, M. (2021). NeuraCrypt: Hiding Private Health Data via Random Neural Networks for Public Training. arXiv preprint arXiv:2106.02484.</p>

<p>[ZHS19] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations, 2019.</p></div>







<p class="date">
by Abhradeep Thakurta <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/"><span class="datestr">at October 25, 2021 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-8628666819453099327">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/10/how-to-send-real-number-using-single.html">How to send a real number using a single bit (and some shared randomness)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this post, we'll look at the natural problem of how to communicate an estimate of a real value in [0,1], using just 1 bit.  The post is based on <a href="https://arxiv.org/abs/2010.02331" target="_blank">this paper</a> (by Ran Ben-Basat of UCL and Shay Vargaftik of VMware Research and myself -- they helped also with the post) that appeared in ICALP last year. </p><p>This question is motivated by various aggregation problems;  multiple sending devices may measure a value, and wish to send the value to an aggregator who will compute something from the received values, such as the average.  In our problem, the senders have a real value x in [0,1] to send, but are constrained to send just a single bit.  Variations of this problem have come up in recent work on distributed/federated learning, where clients compute a gradient vector and send it to a centralized parameter server to update a learning model;  we may want to compress the vector to a small number of bits, or even 1 bit, per coordinate.  (We'll have more to say on the federated learning problem in a future post.)  Of course, it's also just an interesting randomized algorithm problem that seems interesting in its own right.  </p><p>A natural way to look at the problem is as a variation on rounding.  Given a value x in [0,1], one natural approach if limited to one bit is to deterministically round it to X.  But what should the receiver do when they receive the (rounded) bit value X?  It depends on what one's optimization goal is, but to minimize the maximum possible error, the receiver should have their estimate x' take on the value 3/4 when X is 1, 1/4 otherwise.  Note though that deterministic rounding this way is biased -- the expectation E[x'] does not equal x.  Randomized rounding, where the sender sends 1 with probability x and 0 otherwise, and the receiver uses the received bit X as the estimate x', has the property that E[x'] = x.  Unbiased estimators are arguably more natural for many estimation problems.  Here the measure of performance would be the maximum variance for the estimate over all inputs x, so for randomized rounding the cost is 1/4 (when x = 1/2).  </p><p>Can one do better than these schemes?  It turns out that you can, if you have available shared randomness.  An approach that has been known in the engineering world (where it has been used in signal processing) is subtractive dithering:  </p><p>We assume that the sender and receiver have access to shared randomness ℎ∼𝑈[−1/2,1/2].  Given a value x, the sender sends 1 if x+h≥1/2, 0 otherwise.  The receiver estimates x' = X - h.  We leave as an exercise that this is unbiased, which can be shown by deriving the stronger fact that x' is distributed as 𝑈[𝑥−1/2,𝑥+1/2] , and thus Var[𝑥']=1/12.</p><p>Subtractive dithering ignores that generating a shared real number may be more costly or problematic than generating a finite number of shared bits.  So one of the results of our paper is developing a "finite shared random bits" unbiased estimator, that corresponds to randomized rounding with no shared bits and converges to subtractive dithering as the number of shared random bits goes to infinity.  (The approach does allow for generating a private random real value.)  </p><p>Also in our paper, we study biased schemes, aiming to minimize the worst-case expected mean-squared error (where the expectation is over randomness used in the algorithm).  For example, it's very odd in the setting of subtractive dithering that one can obtain estimates smaller than 0 or greater than 1, when the input is restricted to [0,1], but that's a price we pay for having an unbiased estimator.  For  a biased estimator, you might naturally truncate the result from subtractive dithering;  by truncating to [z,1-z] for an appropriate z &gt; 0, you can indeed slightly improve over the worst-case mean-squared error of 1/16 for deterministic rounding.</p><p>We then studied various algorithmic improvements to obtain better biased schemes.  We were able to progress by looking at limited shared randomness, namely finding the best algorithm with s shared bits.  For example, consider the case of just 1 shared random bit h in {0,1}.  The receiver receives 1 bit X from the sender, and thus can have four possible estimates x' depending on X and h.  If the 4 possible estimate values are v0, v1, v2, v3 (all between 0 and 1), then it is possible to show the largest possible expected squared error occurs at one of the five inputs 0, 1, (v0+v1)/2, (v1+v2)/2, (v2+v3)/2.   We can then write a quadratic program to find the values that minimizes the worst-case expected squared error.  The end result is the following rounding algorithm:  given 1 shared random bit h in {0,1} and the value x, let X = 1 if x ≥ 0.4 + 0.2h, and 0 otherwise;  then let the estimate x' = 0.1 + 0.2h + 0.6X.  This has a worst-case expected mean-squared error of 1/20, beating deterministic rounding and truncated subtractive dithering.  Using some additional arguments we can handle more shared random bits;  at 8 bits we improve the worst-case expected squared error to about 0.04599, which is quite close to our lower bound of about 0.0459, and this is better than anything we could come up with analytically.  The optimal solution is still not known (an open question for future work!).  </p><p>Overall there are many variants of the rounding problem, and few tight bounds currently.  So even for simple-seeming problems like rounding, there are still interesting things to do.  </p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/10/how-to-send-real-number-using-single.html"><span class="datestr">at October 25, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/25/three-year-and-tenure-track-positions-at-ttic-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/25/three-year-and-tenure-track-positions-at-ttic-apply-by-december-1-2021/">Three-year and tenure-track positions at TTIC (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship.</p>
<p>Website: <a href="https://www.ttic.edu/faculty-hiring/">https://www.ttic.edu/faculty-hiring/</a><br />
Email: recruiting@ttic.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/25/three-year-and-tenure-track-positions-at-ttic-apply-by-december-1-2021/"><span class="datestr">at October 25, 2021 02:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=936">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/10/25/fibonacci-and-i/">Fibonacci and I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<figure class="wp-block-image size-large"><a href="https://emanueleviola.files.wordpress.com/2021/10/dsc01614.jpg"><img src="https://emanueleviola.files.wordpress.com/2021/10/dsc01614.jpg?w=768" alt="" class="wp-image-945" /></a></figure>



<p>The other day I couldn’t remember Fibonacci’s original motivation/presentation of the sequence now famously named after him.  This had to be corrected immediately, because of the picture above and my <a href="https://www.ccs.neu.edu/home/viola/papers/amigaMagazine.pdf">first publication</a> (1994) which includes a simple algorithm to decompress sounds.  The compression algorithm works by storing rather than the sound data — think of it as the key — the difference between consecutive keys.  The saving comes from not allowing every possible difference, but only those in… the Fibonacci sequence.  Why those differences are the right ones is part of the mystique which makes studying the sequence fun.  For further technical but not mystical details see the paper; an implementation of the decompressor is given in the Motorola 68000 assembly code.</p>



<p>This is me on my way to Fibonacci from Rome, some years ago:</p>



<figure class="wp-block-image size-large"><a href="https://emanueleviola.files.wordpress.com/2021/10/dsc01563-1.jpg"><img src="https://emanueleviola.files.wordpress.com/2021/10/dsc01563-1.jpg?w=1024" alt="" class="wp-image-947" /></a></figure>



<p>I actually find some presentations of the sequence a little hard to grasp, so I came up with a trivially different rendering which now will make it impossible for me to forget:</p>



<p>There are two types of trees: Young and old.  You start with one young tree. In one period, a young tree produces another young tree and becomes old, and an old tree produces a young tree and dies. How many young trees are there after t periods?</p>



<p></p>



<figure class="wp-block-table"><table><tbody><tr><td>Period</td><td>Young trees</td><td>Old trees</td></tr><tr><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>1</td><td>1</td></tr><tr><td>3</td><td>2</td><td>1</td></tr><tr><td>4</td><td>3</td><td>2</td></tr><tr><td>5</td><td>5</td><td>3</td></tr><tr><td>6</td><td>8</td><td>5</td></tr></tbody></table></figure>



<p>I also couldn’t exactly remember the spiral you can make with these numbers.  But you can tile the plane with squares whose sides come from the sequence, if you arrange them in a spiral.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/10/25/fibonacci-and-i/"><span class="datestr">at October 25, 2021 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/25/assistant-associate-professor-theoretical-computer-science-at-university-of-massachusetts-amherst-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/25/assistant-associate-professor-theoretical-computer-science-at-university-of-massachusetts-amherst-apply-by-december-1-2021/">Assistant/Associate Professor – Theoretical Computer Science at University of Massachusetts Amherst (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The College of Information and Computer Sciences (CICS) at the University of Massachusetts Amherst invites applications for tenure-track faculty in Theoretical Computer Science at the Associate and Assistant Professor levels. Exceptional candidates at other ranks may be considered.</p>
<p>Website: <a href="https://careers.umass.edu/amherst/en-us/job/510665/assistantassociate-professor-theoretical-computer-science">https://careers.umass.edu/amherst/en-us/job/510665/assistantassociate-professor-theoretical-computer-science</a><br />
Email: cmusco@cs.umass.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/25/assistant-associate-professor-theoretical-computer-science-at-university-of-massachusetts-amherst-apply-by-december-1-2021/"><span class="datestr">at October 25, 2021 01:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11876">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11876">Tight and Robust Private Mean Estimation with Few Users</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Shyam.html">Shyam Narayanan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11876">PDF</a><br /><b>Abstract: </b>In this work, we study high-dimensional mean estimation under user-level
differential privacy, and attempt to design an
$(\epsilon,\delta)$-differentially private mechanism using as few users as
possible. In particular, we provide a nearly optimal trade-off between the
number of users and the number of samples per user required for private mean
estimation, even when the number of users is as low as
$O(\frac{1}{\epsilon}\log\frac{1}{\delta})$. Interestingly our bound
$O(\frac{1}{\epsilon}\log\frac{1}{\delta})$ on the number of users is
independent of the dimension, unlike the previous work that depends
polynomially on the dimension, solving a problem left open by Amin et
al.~(ICML'2019). Our mechanism enjoys robustness up to the point that even if
the information of $49\%$ of the users are corrupted, our final estimation is
still approximately accurate. Finally, our results also apply to a broader
range of problems such as learning discrete distributions, stochastic convex
optimization, empirical risk minimization, and a variant of stochastic gradient
descent via a reduction to differentially private mean estimation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11876"><span class="datestr">at October 25, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11853">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11853">Polynomial-Time Sum-of-Squares Can Robustly Estimate Mean and Covariance of Gaussians Optimally</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh_K=.html">Pravesh K. Kothari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manohar:Peter.html">Peter Manohar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Brian_Hu.html">Brian Hu Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11853">PDF</a><br /><b>Abstract: </b>In this work, we revisit the problem of estimating the mean and covariance of
an unknown $d$-dimensional Gaussian distribution in the presence of an
$\varepsilon$-fraction of adversarial outliers. The pioneering work of [DKK+16]
gave a polynomial time algorithm for this task with optimal
$\tilde{O}(\varepsilon)$ error using $n = \textrm{poly}(d, 1/\varepsilon)$
samples.
</p>
<p>On the other hand, [KS17b] introduced a general framework for robust moment
estimation via a canonical sum-of-squares relaxation that succeeds for the more
general class of certifiably subgaussian and certifiably hypercontractive
[BK20] distributions. When specialized to Gaussians, this algorithm obtains the
same $\tilde{O}(\varepsilon)$ error guarantee as [DKK+16] but incurs a
super-polynomial sample complexity ($n = d^{O(\log(1/\varepsilon)}$) and
running time ($n^{O(\log(1/\varepsilon))}$). This cost appears inherent to
their analysis as it relies only on sum-of-squares certificates of upper bounds
on directional moments while the analysis in [DKK+16] relies on lower bounds on
directional moments inferred from algebraic relationships between moments of
Gaussian distributions.
</p>
<p>We give a new, simple analysis of the same canonical sum-of-squares
relaxation used in [KS17b, BK20] and show that for Gaussian distributions,
their algorithm achieves the same error, sample complexity and running time
guarantees as of the specialized algorithm in [DKK+16]. Our key innovation is a
new argument that allows using moment lower bounds without having
sum-of-squares certificates for them. We believe that our proof technique will
likely be useful in developing further robust estimation algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11853"><span class="datestr">at October 25, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11851">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11851">Voting algorithms for unique games on complete graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Antoine Méot, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mesmay:Arnaud_de.html">Arnaud de Mesmay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=uuml=hlenthaler:Moritz.html">Moritz Mühlenthaler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Newman:Alantha.html">Alantha Newman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11851">PDF</a><br /><b>Abstract: </b>An approximation algorithm for a Constraint Satisfaction Problem is called
robust if it outputs an assignment satisfying a $(1 - f(\epsilon))$-fraction of
the constraints on any $(1-\epsilon)$-satisfiable instance, where the loss
function $f$ is such that $f(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow
0$. Moreover, the runtime of the algorithm should not depend in any way on
$\epsilon$. In this paper, we present such an algorithm for {\sc
Min-Unique-Games(q)} on complete graphs with $q$ labels. Specifically, the loss
function is $f(\epsilon) = (\epsilon + c_{\epsilon} \epsilon^2)$, where
$c_{\epsilon}$ is a constant depending on $\epsilon$ such that $\lim_{\epsilon
\rightarrow 0} c_{\epsilon} = 16$. The runtime of our algorithm is $O(qn^3)$
(with no dependence on $\epsilon$) and can run in time $O(qn^2)$ using a
randomized implementation with a slightly larger constant $c_{\epsilon}$. Our
algorithm is combinatorial and uses voting to find an assignment. We prove
NP-hardness (using a randomized reduction) for {\sc Min-Unique-Games(q)} on
complete graphs even in the case where the constraints form a cyclic
permutation, which is also known as {\sc Min-Linear-Equations-mod-$q$} on
complete graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11851"><span class="datestr">at October 25, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11836">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11836">The Log-Interleave Bound: Towards the Unification of Sorting and the BST Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Guy Blelloch, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dobson:Magdalen.html">Magdalen Dobson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11836">PDF</a><br /><b>Abstract: </b>We study the connections between sorting and the binary search tree model,
with an aim towards showing that the fields are connected more deeply than is
currently known. The main vehicle of our study is the log-interleave bound, a
measure of the information-theoretic complexity of a permutation $\pi$. When
viewed through the lens of adaptive sorting -- the study of lists which are
nearly sorted according to some measure of disorder -- the log-interleave bound
is comparable to the most powerful known measure of disorder. Many of these
measures of disorder are themselves virtually identical to well-known upper
bounds in the BST model, such as the working set bound or the dynamic finger
bound, suggesting a connection between BSTs and sorting. We present three
results about the log-interleave bound which solidify the aforementioned
connections. The first is a proof that the log-interleave bound is always
within a $\lg \lg n$ multiplicative factor of a known lower bound in the BST
model, meaning that an online BST algorithm matching the log-interleave bound
would perform within the same bounds as the state-of-the-art $\lg \lg
n$-competitive BST. The second result is an offline algorithm in the BST model
which uses $O(\text{LIB}(\pi))$ accesses to search for any permutation $\pi$.
The technique used to design this algorithm also serves as a general way to
show whether a sorting algorithm can be transformed into an offline BST
algorithm. The final result is a mergesort algorithm which performs work within
the log-interleave bound of a permutation $\pi$. This mergesort also happens to
be highly parallel, adding to a line of work in parallel BST operations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11836"><span class="datestr">at October 25, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11723">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11723">A Simple Boosting Framework for Transshipment</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11723">PDF</a><br /><b>Abstract: </b>Transshipment, also known under the names of earth mover's distance,
uncapacitated min-cost flow, or Wasserstein's metric, is an important and
well-studied problem that asks to find a flow of minimum cost that routes a
general demand vector. Adding to its importance, recent advancements in our
understanding of algorithms for transshipment have led to breakthroughs for the
fundamental problem of computing shortest paths. Specifically, the recent
near-optimal $(1+\varepsilon)$-approximate single-source shortest path
algorithms in the parallel and distributed settings crucially solve
transshipment as a central step of their approach.
</p>
<p>The key property that differentiates transshipment from other similar
problems like shortest path is the so-called \emph{boosting}: one can boost a
(bad) approximate solution to a near-optimal $(1 + \varepsilon)$-approximate
solution. This conceptually reduces the problem to finding an approximate
solution. However, not all approximations can be boosted -- there have been
several proposed approaches that were shown to be susceptible to boosting, and
a few others where boosting was left as an open question.
</p>
<p>The main takeaway of our paper is that any black-box $\alpha$-approximate
transshipment solver that computes a \emph{dual} solution can be boosted to an
$(1 + \varepsilon)$-approximate solver. Moreover, we significantly simplify and
decouple previous approaches to transshipment (in sequential, parallel, and
distributed settings) by showing all of them (implicitly) obtain approximate
dual solutions.
</p>
<p>Our analysis is very simple and relies only on the well-known multiplicative
weights framework. Furthermore, to keep the paper completely self-contained, we
provide a new (and arguably much simpler) analysis of multiplicative weights
that leverages well-known optimization tools to bypass the ad-hoc calculations
used in the standard analyses.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11723"><span class="datestr">at October 25, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11721">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11721">Projection-Free Algorithm for Stochastic Bi-level Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akhtar:Zeeshan.html">Zeeshan Akhtar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bedi:Amrit_Singh.html">Amrit Singh Bedi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomdapu:Srujan_Teja.html">Srujan Teja Thomdapu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajawat:Ketan.html">Ketan Rajawat</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11721">PDF</a><br /><b>Abstract: </b>This work presents the first projection-free algorithm to solve stochastic
bi-level optimization problems, where the objective function depends on the
solution of another stochastic optimization problem. The proposed
$\textbf{S}$tochastic $\textbf{Bi}$-level $\textbf{F}$rank-$\textbf{W}$olfe
($\textbf{SBFW}$) algorithm can be applied to streaming settings and does not
make use of large batches or checkpoints. The sample complexity of SBFW is
shown to be $\mathcal{O}(\epsilon^{-3})$ for convex objectives and
$\mathcal{O}(\epsilon^{-4})$ for non-convex objectives. Improved rates are
derived for the stochastic compositional problem, which is a special case of
the bi-level problem, and entails minimizing the composition of two
expected-value functions. The proposed $\textbf{S}$tochastic
$\textbf{C}$ompositional $\textbf{F}$rank-$\textbf{W}$olfe ($\textbf{SCFW}$) is
shown to achieve a sample complexity of $\mathcal{O}(\epsilon^{-2})$ for convex
objectives and $\mathcal{O}(\epsilon^{-3})$ for non-convex objectives, at par
with the state-of-the-art sample complexities for projection-free algorithms
solving single-level problems. We demonstrate the advantage of the proposed
methods by solving the problem of matrix completion with denoising and the
problem of policy value evaluation in reinforcement learning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11721"><span class="datestr">at October 25, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11712">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11712">Incremental SSSP for Sparse Digraphs Beyond the Hopset Barrier</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kyng:Rasmus.html">Rasmus Kyng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meierhans:Simon.html">Simon Meierhans</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gutenberg:Maximilian_Probst.html">Maximilian Probst Gutenberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11712">PDF</a><br /><b>Abstract: </b>Given a directed, weighted graph $G=(V,E)$ undergoing edge insertions, the
incremental single-source shortest paths (SSSP) problem asks for the
maintenance of approximate distances from a dedicated source $s$ while
optimizing the total time required to process the insertion sequence of $m$
edges.
</p>
<p>Recently, Gutenberg, Williams and Wein [STOC'20] introduced a deterministic
$\tilde{O}(n^2)$ algorithm for this problem, achieving near linear time for
very dense graphs. For sparse graphs, Chechik and Zhang [SODA'21] recently
presented a deterministic $\tilde{O}(m^{5/3})$ algorithm, and an adaptive
randomized algorithm with run-time $\tilde{O}(m\sqrt{n} + m^{7/5})$. This
algorithm is remarkable for two reasons: 1) in very spare graphs it reaches the
directed hopset barrier of $\tilde{\Omega}(n^{3/2})$ that applied to all
previous approaches for partially-dynamic SSSP [STOC'14, SODA'20, FOCS'20]
\emph{and} 2) it does not resort to a directed hopset technique itself.
</p>
<p>In this article we introduce \emph{propagation synchronization}, a new
technique for controlling the error build-up on paths throughout batches of
insertions. This leads us to a significant improvement of the approach in
[SODA'21] yielding a \emph{deterministic} $\tilde{O}(m^{3/2})$ algorithm for
the problem. By a very careful combination of our new technique with the
sampling approach from [SODA'21], we further obtain an adaptive randomized
algorithm with total update time $\tilde{O}(m^{4/3})$. This is the first
partially-dynamic SSSP algorithm in sparse graphs to bypass the notorious
directed hopset barrier which is often seen as the fundamental challenge
towards achieving truly near-linear time algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11712"><span class="datestr">at October 25, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11697">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11697">An Efficient Branch-and-Bound Solver for Hitting Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, Tobias Friedrich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stangl:David.html">David Stangl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weyand:Christopher.html">Christopher Weyand</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11697">PDF</a><br /><b>Abstract: </b>The hitting set problem asks for a collection of sets over a universe $U$ to
find a minimum subset of $U$ that intersects each of the given sets. It is
NP-hard and equivalent to the problem set cover. We give a branch-and-bound
algorithm to solve hitting set. Though it requires exponential time in the
worst case, it can solve many practical instances from different domains in
reasonable time. Our algorithm outperforms a modern ILP solver, the
state-of-the-art for hitting set, by at least an order of magnitude on most
instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11697"><span class="datestr">at October 25, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11613">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11613">Pairwise Reachability Oracles and Preservers under Failures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Diptarka.html">Diptarka Chakraborty</a>, Kushagra Chatterjee, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhary:Keerti.html">Keerti Choudhary</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11613">PDF</a><br /><b>Abstract: </b>In this paper, we consider reachability oracles and reachability preservers
for directed graphs/networks prone to edge/node failures. Let $G = (V, E)$ be a
directed graph on $n$-nodes, and $P\subseteq V\times V$ be a set of vertex
pairs in $G$. We present the first non-trivial constructions of single and dual
fault-tolerant pairwise reachability oracle with constant query time.
Furthermore, we provide extremal bounds for sparse fault-tolerant reachability
preservers, resilient to two or more failures. Prior to this work, such oracles
and reachability preservers were widely studied for the special scenario of
single-source and all-pairs settings. However, for the scenario of arbitrary
pairs, no prior (non-trivial) results were known for dual (or more) failures,
except those implied from the single-source setting. One of the main questions
is whether it is possible to beat the $O(n |P|)$ size bound (derived from the
single-source setting) for reachability oracle and preserver for dual failures
(or $O(2^k n|P|)$ bound for $k$ failures). We answer this question
affirmatively.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11613"><span class="datestr">at October 25, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11602">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11602">An O(1) algorithm for implementing the LFU cache eviction scheme</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Dhruv Matani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Ketan.html">Ketan Shah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitra:Anirban.html">Anirban Mitra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11602">PDF</a><br /><b>Abstract: </b>Cache eviction algorithms are used widely in operating systems, databases and
other systems that use caches to speed up execution by caching data that is
used by the application. There are many policies such as MRU (Most Recently
Used), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least
Frequently Used) which each have their advantages and drawbacks and are hence
used in specific scenarios. By far, the most widely used algorithm is LRU, both
for its $O(1)$ speed of operation as well as its close resemblance to the kind
of behaviour that is expected by most applications. The LFU algorithm also has
behaviour desirable by many real world workloads. However, in many places, the
LRU algorithm is is preferred over the LFU algorithm because of its lower run
time complexity of $O(1)$ versus $O(\log n)$. We present here an LFU cache
eviction algorithm that has a runtime complexity of $O(1)$ for all of its
operations, which include insertion, access and deletion(eviction).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11602"><span class="datestr">at October 25, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11585">Monotone edge flips to an orientation of maximum edge-connectivity \`a la Nash-Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ito:Takehiro.html">Takehiro Ito</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iwamasa:Yuni.html">Yuni Iwamasa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kakimura:Naonori.html">Naonori Kakimura</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamiyama:Naoyuki.html">Naoyuki Kamiyama</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yusuke.html">Yusuke Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maezawa:Shun=ichi.html">Shun-ichi Maezawa</a>, Yuta Nozaki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okamoto:Yoshio.html">Yoshio Okamoto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ozeki:Kenta.html">Kenta Ozeki</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11585">PDF</a><br /><b>Abstract: </b>We initiate the study of $k$-edge-connected orientations of undirected graphs
through edge flips for $k \geq 2$. We prove that in every orientation of an
undirected $2k$-edge-connected graph, there exists a sequence of edges such
that flipping their directions one by one does not decrease the
edge-connectivity, and the final orientation is $k$-edge-connected. This yields
an ``edge-flip based'' new proof of Nash-Williams' theorem: an undirected graph
$G$ has a $k$-edge-connected orientation if and only if $G$ is
$2k$-edge-connected. As another consequence of the theorem, we prove that the
edge-flip graph of $k$-edge-connected orientations of an undirected graph $G$
is connected if $G$ is $(2k+2)$-edge-connected. This has been known to be true
only when $k=1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11585"><span class="datestr">at October 25, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11439">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11439">Online Bipartite Matching with Predicted Degrees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Justin_Y=.html">Justin Y. Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11439">PDF</a><br /><b>Abstract: </b>We propose a model for online graph problems where algorithms are given
access to an oracle that predicts the degrees of nodes in the graph (e.g.,
based on past data). Within this model, we study the classic problem of online
bipartite matching. An extensive empirical evaluation shows that a greedy
algorithm called MinPredictedDegree compares favorably to state-of-the-art
online algorithms for this problem. We also initiate the theoretical study of
MinPredictedDegree on a natural random graph model with power law degree
distribution and show that it produces matchings almost as large as the maximum
matching on such graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11439"><span class="datestr">at October 25, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2110.11430">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2110.11430">How can classical multidimensional scaling go wrong?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sonthalia:Rishi.html">Rishi Sonthalia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buskirk:Gregory_Van.html">Gregory Van Buskirk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raichel:Benjamin.html">Benjamin Raichel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gilbert:Anna_C=.html">Anna C. Gilbert</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2110.11430">PDF</a><br /><b>Abstract: </b>Given a matrix $D$ describing the pairwise dissimilarities of a data set, a
common task is to embed the data points into Euclidean space. The classical
multidimensional scaling (cMDS) algorithm is a widespread method to do this.
However, theoretical analysis of the robustness of the algorithm and an
in-depth analysis of its performance on non-Euclidean metrics is lacking.
</p>
<p>In this paper, we derive a formula, based on the eigenvalues of a matrix
obtained from $D$, for the Frobenius norm of the difference between $D$ and the
metric $D_{\text{cmds}}$ returned by cMDS. This error analysis leads us to the
conclusion that when the derived matrix has a significant number of negative
eigenvalues, then $\|D-D_{\text{cmds}}\|_F$, after initially decreasing, will
eventually increase as we increase the dimension. Hence, counterintuitively,
the quality of the embedding degrades as we increase the dimension. We
empirically verify that the Frobenius norm increases as we increase the
dimension for a variety of non-Euclidean metrics. We also show on several
benchmark datasets that this degradation in the embedding results in the
classification accuracy of both simple (e.g., 1-nearest neighbor) and complex
(e.g., multi-layer neural nets) classifiers decreasing as we increase the
embedding dimension.
</p>
<p>Finally, our analysis leads us to a new efficiently computable algorithm that
returns a matrix $D_l$ that is at least as close to the original distances as
$D_t$ (the Euclidean metric closest in $\ell_2$ distance). While $D_l$ is not
metric, when given as input to cMDS instead of $D$, it empirically results in
solutions whose distance to $D$ does not increase when we increase the
dimension and the classification accuracy degrades less than the cMDS solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2110.11430"><span class="datestr">at October 25, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8402768071991338392">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html">Squaring the circle is mentioned in a Gilbert and Sullivan comic Opera.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The problem of <i>squaring the circle</i>: Given a circle, construct (with ruler and compass) a square with the same area. While browsing the web for more information on this problem (for the blog entry on problems that might be similar to P vs NP: <a href="https://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html">here</a>)  I came across the following:</p><p>In the Gilbert and Sullivan comic opera <i>Princess Ida</i>, in the song <i>Gently, Gently</i>  is the line:</p><p>                                <i>    ... and the circle they will square it one fine day.</i></p><p>(To hear the song see <a href="https://www.youtube.com/watch?v=1r2hEcDmeIY">here</a>. The line is towards the end.) </p><p>They lyrics are <a href="https://www.gsarchive.net/princess_ida/webop/pi_12.html">here</a>. That website begins  <i>gsarchive.ne</i>t which made me wonder<i> Did I at one time set</i> u<i>p a website of math refs in Gilbert and Sullivan plays (gsarch is very close to gasarch) ? </i>which IS the kind of thing I would do. The answer is no: <i> gsarch</i> stands for <i>Gilbert and Sullivan archive.</i> They could have called it <i>gasarch </i>if they used the <i>and </i>in <i>Gilbert and Sullivan</i> but abbreviated <i>archive </i>as arch. Then I would have been far more confused. </p><p>Moving on...</p><p>In 1884<i> Princess Ida</i> opened in 1884. For more on this comic opera see <a href="https://en.wikipedia.org/wiki/Princess_Ida">here</a>.</p><p>In 1882 pi was proven  transcendental and hence one cannot square the circle. For more on pi being transcendental see <a href="https://en.wikipedia.org/wiki/Squaring_the_circle">here</a>.</p><p>Kolmogorov Random Thoughts on all of this</p><p>0) The song is sung my three men who are making fun of the notion of a women's college. The song is about all the things the women are trying to do that are absurd such as squaring the circle. They also mention perpetual motion machines. </p><p>1) Did G and S know that the squaring the circle had been proven impossible, or just that it was thought to be impossible, or just that it was thought to be hard?</p><p>2) Was it known that perpetual motion machines were impossible? Or just very hard? </p><p>3) G and S used Mathematics in at least one other song: <i> I am the very model of a modern major general, </i>from<i> The Pirates of Penzance  </i>has the lines:</p><p><br /></p><p>                                       <i>I'm very well acquainted too with matters mathematical</i></p><p><i>                                       I understand equations, both the simple and quadratical,</i></p><p><i>                                       About binomial theorems I'm teeming with the a lot o' news---</i></p><p><i>                                       With many cheerful facts about the square of the hypotenuse</i></p><p><br /></p><p>and later </p><p><i>                                        I'm very good at integral and differential calculus</i></p><p>See <a href="https://naic.edu/~gibson/poems/gilbert1.html">here</a> for all the lyrics. The website mentioned in the next point has a pointer to a YouTube video of people singing it. </p><p>4) There are many parodies of <i>Modern Major General. </i>The earliest ones I know of is Tom Lehrer's  <i>The Elements. </i>Since making a website of them IS the kind of thing I would do,  while writing this post I did it (Are we compelled to do things that fit our image of ourselves? Yup.) The website is <a href="http://www.cs.umd.edu/~gasarch/FUN//modmajgen.html">here</a>. It has 36 parodies (as of Oct 17, 2021 when I wrote this blog--- it may have more if you read this later.) That may seem like a lot, but it pales in comparison  to the most satirized song of all time: <i>The 12 days of Christmas </i>which I did an ugly lyrics-only website for back before html had nice tools, see <a href="http://www.cs.umd.edu/~gasarch/12days.html">here</a>. It has 143 songs on it but I am sure there are many more. (Note to self: redo that website when you have time. Maybe when I retire.) </p><p>4) I suspect that G and S knew more math, or perhaps knew of more math,  than Broadway composers know now. I suspect this is a more general trend: people are more specialized now. Having said that, I need to mention the off-Broadway musical <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Tango">Fermat's last Tango</a> which I liked more than Lance (see his post on it <a href="https://blog.computationalcomplexity.org/2004/02/fermats-last-tango.html">here</a>). </p><p>5) How much math would you need to know in order to insert some into your play or movie? With Wikipedia and other web sources you could find out some things, but you would have to have some idea what you are looking for. And perhaps you would need some math background in order to even want to insert some math into your work in the first place. </p><p>6)  Here's hoping someone will make a musical about William Rowan Hamilton using this song <a href="https://www.youtube.com/watch?v=SZXHoWwBcDc">here</a> as a starting point. I blogged rather optimistically about that possibility <a href="https://blog.computationalcomplexity.org/2017/04/william-rowan-hamilton-musical.html#comment-form">here</a>.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html"><span class="datestr">at October 24, 2021 06:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/24/new-computational-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html">New computational geometry journal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Over the past year or so I’ve been working with Marc van Kreveld and Wolfgang Mulzer to set up a new <a href="https://en.wikipedia.org/wiki/Diamond_open_access">diamond open access</a> computational geometry journal, <em><a href="https://www.cgt-journal.org/index.php/cgt">Computing in Geometry and Topology</a></em>, sponsored by the <a href="https://www.computational-geometry.org/">Society for Computational Geometry</a>, the organization set up to run the annual <em>Symposium on Computational Geometry</em>. It is the second such journal to be established, after the <em><a href="https://jocg.org/index.php/jocg">Journal of Computational Geometry</a></em>. This week the new journal went live and it is now available for submissions. Below is the official announcement we sent out to several mailing lists:</p>

<hr />

<p>Dear all,</p>

<p>As of this week, the new diamond open access journal</p>

<p style="text-align: center;"><em>Computing in Geometry and Topology</em></p>

<p>is welcoming submissions. The website of the journal is <a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<p>Here you can also find the editorial board, the submission guidelines, the scope, and a style file.</p>

<p>If you have any questions about the journal, please send them to info@cgt-journal.org</p>

<p>Best regards,
David Eppstein, Marc van Kreveld, and Wolfgang Mulzer</p>

<hr />

<p><strong>Purpose and scope</strong></p>

<p><em>Computing in Geometry and Topology</em> aims to support the broader computational geometry and topology community by being a peer-reviewed scientific journal that provides diamond open access. <em>Computing in Geometry and Topology</em> is sponsored by the Society for Computational Geometry.</p>

<p>With the broader computational geometry and topology community, we include researchers in discrete and combinatorial geometry, and any application area of computational geometry and topology. We also include algorithm engineering for geometric computations.</p>

<p>The journal publishes two types of papers. Firstly, the journal publishes original research of sufficient depth and interest. Secondly, the journal publishes high-quality survey papers. Every paper has been thoroughly reviewed by experts in the area.</p>

<p>To emphasize the breadth of the interpretation of computational geometry and topology, the editorial board has different sections that represent the algorithmic and mathematical aspects, the applied aspects, and the engineering aspects.</p>

<p><a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<hr />

<p>(<a href="https://mathstodon.xyz/@11011110/107159503144164449">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html"><span class="datestr">at October 24, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/147">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/147">TR21-147 |  Extractors for Sum of Two Sources | 

	Jyun-Jie Liao, 

	Eshan Chattopadhyay</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the problem of extracting randomness from \textit{sumset sources}, a general class of weak sources introduced by Chattopadhyay and Li (STOC, 2016). An $(n,k,C)$-sumset source $\mathbf{X}$ is a distribution on $\{0,1\}^n$ of the form $\mathbf{X}_1 + \mathbf{X}_2 + \ldots + \mathbf{X}_C$, where $\mathbf{X}_i$'s are independent sources on $n$ bits with min-entropy at least $k$. Prior extractors either required the number of sources $C$ to be a large constant or the min-entropy $k$ to be at least $0.51 n$. 

As our main result, we construct an explicit extractor for sumset sources in the setting of $C=2$ for min-entropy $\mathrm{poly}(\log n)$ and polynomially small error. We can further improve the min-entropy requirement  to $(\log n) \cdot (\log \log n)^{1 + o(1)}$ at the expense of worse error parameter of our extractor. We find applications of our sumset extractor for extracting randomness from other well-studied models of weak sources such as affine sources, small-space sources,  and interleaved sources.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/147"><span class="datestr">at October 24, 2021 11:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/">Tenure-Track Faculty Positions at Simon Fraser University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html">https://www.sfu.ca/computing/job-opportunities.html</a><br />
Email: cs_faculty_affairs@sfu.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/23/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-1-2021/"><span class="datestr">at October 23, 2021 07:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/10/23/tcs-talk-wednesday-october-27-shravas-rao-northwestern-university/">TCS+ talk: Wednesday, October 27 — Shravas Rao, Northwestern University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 27th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://cims.nyu.edu/~rao/"><strong>Shravas Rao</strong></a> from Northwestern University will speak about “<em>Degree vs. Approximate Degree and Quantum Implications of Huang’s Sensitivity Theorem</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />,</p>
<ul>
<li>The degree of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is at most quadratic in the approximate degree of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />. This is optimal as witnessed by the OR function.</li>
<li>The deterministic query complexity of f is at most quartic in the quantum query complexity of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" />. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017).</li>
</ul>
<p>We apply these results to resolve the quantum analogue of the Aanderaa–Karp–Rosenberg conjecture. We show that if <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is a nontrivial monotone graph property of an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertex graph specified by its adjacency matrix, then <img src="https://s0.wp.com/latex.php?latex=Q%28f%29%3D%5COmega%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="Q(f)=\Omega(n)" class="latex" />, which is also optimal. We also show that the approximate degree of any read-once formula on <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" /> variables is <img src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Csqrt%7Bn%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\Theta(\sqrt{n})" class="latex" />.</p>
<p>Based on joint work with Scott Aaronson, Shalev Ben-David, Robin Kothari, and Avishay Tal.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/10/23/tcs-talk-wednesday-october-27-shravas-rao-northwestern-university/"><span class="datestr">at October 23, 2021 07:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19240">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/">An Annoying Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>It’s the stupid questions that have some of the most surprising and interesting answers — Cory Doctorow</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/bt/" rel="attachment wp-att-19242"><img width="120" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/bt.jpg?resize=120%2C180&amp;ssl=1" class="alignright wp-image-19242" height="180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">ACM Turing Award <a href="https://amturing.acm.org/photo/tarjan_1092048.cfm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Robert Tarjan is well known to most—a Turing award winner in 1986 with <a href="https://amturing.acm.org/award_winners/tarjan_1092048.cfm">John Hopcroft</a>. Bob is a professor of Computer Science at Princeton University, and one of the world experts on linear time graph algorithms. I always thought if some NP-complete graph problem had a linear time algorithm, then P=NP would have long been solved by Bob. </p>
<p>
Today we talk about a problem that hasn’t been solved by Bob.</p>
<p>
The problem I mean was <em>quasi-</em>solved by Bob. That is, he gave a quasi-polynomial time algorithm. The problem is <em>group isomorphism</em> (GpI). Laci Babai discusses its relation as a special case of <em>graph isomorphism</em> (GI) in section 13.2 of his famous 2016 <a href="https://arxiv.org/pdf/1512.03547.pdf">paper</a> giving a quasi-polynomial time algorithm for GI. He says that the annoyance of GpI should temper expectations for getting GI into polynomial time, because:</p>
<blockquote><p><b> </b> <em> [I]n spite of considerable effort and the availability of powerful algebraic machinery, Group Isomorphism is still not known to be in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />. </em>
</p></blockquote>
<p>
</p><p></p><h2> Bob’s Least Result </h2><p></p>
<p></p><p>
Bob has many great and deep results. For example, Donald Knuth <a href="https://www.informit.com/articles/article.aspx?p=2213858">described</a> Bob’s strong connected component algorithm for directed graphs in these words:</p>
<blockquote><p><b> </b> <em> The data structures that he devised for this problem fit together in an amazingly beautiful way, so that the quantities you need to look at while exploring a directed graph are always magically at your fingertips. </em>
</p></blockquote>
<p></p><p>
Here is Ken’s slight editing of Wikipedia’s <a href="https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm">presentation</a>:</p>
<p><code><br />
<font size="-1"><br />
Input: Graph G = (V,E) whose nodes have fields int index, int lowlink, bool onStack<br />
Output: The set of subsets of V denoting the strongly connected components</font></code></p><font size="-1">
<p>Stack S := empty stack;<br />
int index := 0;  //global variable with invariant: index = smallest unused index</p>
<p>proc strongconnect(v):<br />
   v.index := index; v.lowlink := index; index++;<br />
   S.push(v); v.onStack := true;<br />
   for each w such that (v,w) is in E: </p>
<p>      if w.index is undefined then:  //w not yet visited so recurse on it </p>
<p>         strongconnect(w);<br />
         v.lowlink = min{v.lowlink,w.lowlink}  //lowest-indexed node reached</p>
<p>      else if w.onStack then: </p>
<p>         v.lowlink = min(v.lowlink, w.index);<br />
         //clever: w.index was lowest unused index at the time </p>
<p>      else:<br />
         pass;   //(v,w) goes to already-found component, so ignore.<br />
      end if;<br />
   end for </p>
<p>   if v.lowlink == v.index then: //we've completed a component so pop and output it<br />
      repeat:<br />
         Node w = S.pop();<br />
         w.onStack = false;<br />
         output w as part of the current strongly connected component;<br />
      until w == v;<br />
   end if;<br />
end proc;</p>
</font><p><font size="-1">for each v in V do:<br />
   if v.index is undefined then:<br />
      strongconnect(v);<br />
   end if;<br />
end for;<br />
</font><br />
</p>
<p></p><h2> The Annoying Problem </h2><p></p>
<p></p><p>
Just over ten years ago we talked about: How do we tell if two groups are isomorphic? Precisely on October 8, 2011 we talked about <a href="https://rjlipton.wpcomstaging.com/2011/10/08/an-annoying-open-problem/">group isomorphism</a>. We called it an annoying problem. It remains annoying. </p>
<p>
In the group isomorphism problem (GpI), we are given two finite groups <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> in terms of their multiplication tables and must decide if <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" />.</p>
<p>
Bob Tarjan famously noted that this could be done <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Clog_2+n+%2B+O%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{\log_2 n + O(1)}}" class="latex" /> time. He never published this result and just orally told others about it. The insight was that every group of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> had a generator set of size at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 n}" class="latex" />. Since his insight the group isomorphism problem remains roughly the same complexity. The best is the <a href="https://rjlipton.wpcomstaging.com/2013/05/11/advances-on-group-isomorphism/">improvement</a> due to David Rosenbaum. His algorithm is still of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7Bc+%5Clog+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{c \log n}}" class="latex" />. See <a href="https://arxiv.org/pdf/1304.3935.pdf">here</a> for details. </p>
<blockquote><p><b>Theorem 1</b> <em> General group isomorphism is in <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%281%2F2%29+log_p+n%2BO%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{(1/2) log_p n+O(1)}}" class="latex" /> deterministic time where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> is the smallest prime dividing the order of the group. </em>
</p></blockquote>
<p></p><p>
Better than Bob’s insight, but still not polynomial time. </p>
<p>
</p><p></p><h2> Order Restriction </h2><p></p>
<p></p><p>
The results to date on GpI have relied mainly on basic group theory. Bob’s result uses an elementary fact about finite groups. David uses the same insight and adds a deep fact about isomorphism type problems. Neither use any of the “millions” of theorems known about finite groups. </p>
<p>
I wondered if there is some hope to start to exploit more of the papers on group structure? The trouble I think is that these results are quite difficult for those not in group theory to understand. But perhaps there is some hope. What do you think?</p>
<p>
I still wonder if there is a polynomial time algorithm for the general group isomorphism problem? Can we eliminate the <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log n}" class="latex" /> exponent somehow? This lead us to note the following simple theorem:</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> be finite groups of order <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> that is square free.Then <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" /> can be determined in time polynomial in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> time. </em>
</p></blockquote>
<p></p><p>
That is, <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> has no prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^2}" class="latex" /> divides <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />.</p>
<p>
</p><p></p><h2> But Wait… </h2><p></p>
<p></p><p>
As I worked on this I found a nice <a href="https://arxiv.org/pdf/1806.08872.pdf">paper</a> by Heiko Dietrich and James Wilson, titled “Polynomial Time Isomorphism Tests Of Black-Box Type Groups Of Most Orders.” They reference another <a href="https://arxiv.org/pdf/1810.03467.pdf">paper</a> that proved a stronger theorem than our above one:</p>
<blockquote><p><b>Theorem 3</b> <em> There is an algorithm that given groups <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of permutations on finitely many points, decides whether they are of cube-free order, and if so, decides that <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ccong+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \cong H}" class="latex" /> or constructs an isomorphism <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Crightarrow+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \rightarrow H}" class="latex" />. The algorithm runs in time polynomial in the input size. </em>
</p></blockquote>
<p>
</p><p></p><h2> Proof of Our Theorem </h2><p></p>
<p>
</p><blockquote><p><b>Definition 4</b> <em> A finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is <b>metacyclic</b> provided it has a normal subgroup <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2FN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G/N}" class="latex" /> are both cyclic. </em>
</p></blockquote>
<p></p><p>
Every finite group of square-free order (i.e. the order is not divisible by the square of a natural number) is metacyclic.</p>
<p>
<em>Proof:</em>  We claim that <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> are both metacyclic. This follows from the <a href="https://en.wikipedia.org/wiki/Metacyclic_group">fact</a> that their order is square free. But then we know that both <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> are generated by at most two elements—see <a href="https://math.stackexchange.com/questions/1734406/prove-metacyclic-group-is-generated-by-two-elements">this</a>. This yields an isomorphism algorithm based on the above generator trick. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Box" class="latex" /></p>
<p>
Their stronger proof uses quite a bit more group theory. But our simple proof may give you some intuition why the restriction on the order of the group helps.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
The above result fits well with the belief that the worst case for isomorphism is when the order of the groups is a power of a prime. Some possible conjectures are:</p>
<ul>
<li>
Can we generalize “<img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is square free” to “<img src="https://s0.wp.com/latex.php?latex=%7Bp%5E4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^4}" class="latex" /> does not divide <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />“? Or to “<img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B5%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{5}}" class="latex" /> does not divide <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />?” And so on? <p></p>
</li><li>
Let <img src="https://s0.wp.com/latex.php?latex=%7BZ%28G%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z(G)=1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7CAut%28G%29%7C+%5Cle+%7CG%7C%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|Aut(G)| \le |G|^{O(1)}}" class="latex" />. Then GpI is in polynomial time.
</li></ul>
<blockquote><p><b>Definition 5</b> <em> A finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is <b>metabelian</b> provided it has a normal subgroup <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2FN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G/N}" class="latex" /> are both abelian. </em>
</p></blockquote>
<ul>
<li>
What if we replace metacyclic by metabelian?
</li></ul></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/10/22/an-annoying-problem/"><span class="datestr">at October 22, 2021 08:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6088">An Orthodox rabbi and Steven Weinberg walk into an email exchange…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Ever since I posted my <a href="https://scottaaronson.blog/?p=5566">obituary for the great Steven Weinberg</a> three months ago, I’ve gotten a steady trickle of emails—all of which I’ve appreciated enormously—from people who knew Steve, or were influenced by him, and who wanted to share their own thoughts and memories.  Last week, I was contacted by one Moshe Katz, an Orthodox rabbi, who wanted to share a long email exchange that he’d had with Steve, about Steve’s reasons for rejecting his birth-religion of Judaism (along with every other religion).  Even though Rabbi Katz, rather than Steve, does most of the talking in this exchange, and even though Steve mostly expresses the same views he’d expressed in many of his public writings, I knew immediately on seeing this exchange that it could be of broader interest—so I secured permission to share it here on <em>Shtetl-Optimized</em>, both from Rabbi Katz and from Steve’s widow Louise.</p>



<p>While longtime readers can probably guess what <em>I</em> think about most of the topics discussed, I’ll refrain from any editorial commentary in this post—but of course, feel free to share your own thoughts in the comments, and maybe I’ll join in.  Mostly, reading this exchange reminded me that someone at some point should write a proper book-length biography of Steve, and someone should also curate and publish a selection of his correspondence, much like <a href="https://www.amazon.com/Perfectly-Reasonable-Deviations-Letters-Richard/dp/141934322X"><em>Perfectly Reasonable Deviations from the Beaten Track</em></a> did for Richard Feynman.  There must be a lot more gems to be mined.</p>



<p>Anyway, without further ado, <a href="https://www.scottaaronson.com/weinberg.pdf"><strong>here’s the exchange</strong></a> (10 pages, PDF).</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6088"><span class="datestr">at October 22, 2021 03:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6086">Welcome to scottaaronson.blog !</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If you’ve visited <em>Shtetl-Optimized</em> lately — which, uh, I suppose you have — you may have noticed that your URL was redirected from www.scottaaronson.com/blog to scottaaronson.blog.  That’s because Automattic, makers of <a href="https://wordpress.com">WordPress.com</a>, volunteered to move my blog there from Bluehost, free of charge.  If all goes according to plan, you should notice faster loading times, less downtime, and <em>hopefully</em> nothing else different.  Please let me know if you encounter any problems.  And <em>huge</em> thanks to the WordPress.com Special Projects Team, especially Christopher Jones and Mark Drovdahl, for helping me out with this.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6086"><span class="datestr">at October 21, 2021 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/">Postdoc Positions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The<a href="https://jobmarket.unibocconi.eu/include/dwload.php?a=NTUwXjQ0M2QwY2Y4N2FhMjA2MDcxN2MwMzc3NGZjODM4NGU0Xi9kMC9qb2JtYXJrZXQudW5pYm9jY29uaS5ldS91cGxvYWQvQklEL3Nlc3Npb25fMTM3LTIwMjExMDE4Xmpta19zZXNfZmlsZV5qbWZfXmptZl9maWxlXjM2OQ=="> call is out</a> for two postdoctoral positions at Bocconi to work in my group [<a href="https://jobmarket.unibocconi.eu/application.php?f=MDJkYTc3YjUzZTNlMmNlMGIxMmNjOGQ0ZjRhNTc3MzBeMTM3">application link</a>]. If you are interested and you have any questions, feel free to email me (L.Trevisan at Unibocconi dot it)</p>



<p>The negotiable start date is September 1st, 2022. Each position is for one year, renewable for a second. The positions offer an internationally competitive salary (up to 65,000 Euro per year, tax-free, plus relocation assistance and travel allowance), in a wonderful location that, at long last, is back to more or less normal life. The application deadline is <strong>December 17, 2021</strong>.</p>



<p>Among the topics that I am interested in are spectral graph theory, average-case complexity, “applications” of semidefinite programming, random processes on networks, approximation algorithms, pseudorandomness and combinatorial constructions.</p>



<p>Bocconi Computer Science is building up a theory group: besides me, we have <a href="https://www.alonrosen.net/">Alon Rosen</a>, <a href="https://elias.ba30.eu/">Marek Elias</a>, a tenured person that will join next Fall, and more hires are on the horizon. Now that traveling is ok again, and considering that Alon and I both have ERC grants, we should expect a big stream of theory visitors coming and going through Bocconi from week-long visits to semester or year long sabbaticals.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/"><span class="datestr">at October 21, 2021 05:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/">postdoc at IRIF, DIENS, LIP6 (Paris, France) (apply by November 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking to recruit 1 to 3 postdocs for 1 to 3 years to work on the algorithmic theory of new data models (Theory, Algorithms), at IRIF (Université de Paris), LIP6 (Sorbonne Université), and DIENS (Université PSL) in Paris, France, with three years of funding by the ANR grant Algoridam (<a href="https://www.irif.fr/~algoridam/">https://www.irif.fr/~algoridam/</a>) and a flexible starting date.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc">https://www.irif.fr/postes/postdoc</a><br />
Email: postdoc.algoridam@listes.irif.fr.</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/postdoc-at-irif-diens-lip6-paris-france-apply-by-november-1-2021/"><span class="datestr">at October 20, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/10/20/opportunities-at-harvard/">Opportunities at Harvard!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Computer Science at Harvard, and in particular theoretical computer science and machine learning, is growing fast, see my 21-Tweet thread:</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/21 Banner year for Harvard CS! <br /><br />New hires include Sham Kakade <a href="https://twitter.com/ShamKakade6?ref_src=twsrc%5Etfw">@ShamKakade6</a> and Fernanda Viegas <a href="https://twitter.com/viegasf?ref_src=twsrc%5Etfw">@viegasf</a> (joining <a href="https://twitter.com/wattenberg?ref_src=twsrc%5Etfw">@wattenberg</a>), as well as David Alvarez-Melis, Anurag Anshu <a href="https://twitter.com/AnuragAnshu4?ref_src=twsrc%5Etfw">@AnuragAnshu4</a>, Sitan Chen, and Jonathan Frankle <a href="https://twitter.com/jefrankle?ref_src=twsrc%5Etfw">@jefrankle</a> <a href="https://t.co/mDR4bi0kSD">https://t.co/mDR4bi0kSD</a> <a href="https://t.co/bXwMbQK7s8">pic.twitter.com/bXwMbQK7s8</a></p>— Boaz Barak (@boazbaraktcs) <a href="https://twitter.com/boazbaraktcs/status/1450171218070495238?ref_src=twsrc%5Etfw">October 18, 2021</a></blockquote></div>
</div></figure>



<p>Please consider applying for <a href="https://www.seas.harvard.edu/computer-science/graduate-programs/how-apply"><strong>graduate studies in computer science</strong></a> (or encourage others to apply if like me, your grad-school days are behind you). </p>



<p>In recent years, I’ve taken a special interest in the <a href="https://mltheory.org/"><strong>theory of machine learning</strong></a>, and there are <a href="https://mltheory.org/#opportunities"><strong>several opportunities at Harvard</strong></a> in this area. In particular, I’ve been collaborating with people both in and outside CS, including incoming CS faculty <a href="https://homes.cs.washington.edu/~sham/"><strong>Sham Kakade</strong></a>, as well as <a href="http://www.demba-ba.org/" target="_blank" rel="noreferrer noopener"><strong>Demba Ba</strong></a> (Electrical Engineering and Bioengineering), <a href="http://lucasjanson.fas.harvard.edu/" target="_blank" rel="noreferrer noopener"><strong>Lucas Janson</strong></a> (Statistics), and  <a href="https://pehlevan.seas.harvard.edu/" target="_blank" rel="noreferrer noopener"><strong>Cengiz Pehlevan</strong></a> (Applied Mathematics). I’m very much open to co-advising students outside computer science as well. </p>



<p>Students interested in <a href="https://quantum.harvard.edu/"><strong>quantum computation and information</strong></a> can apply to any of our programs (including computer science, physics, and others) as well as to the new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering"><strong>quantum science and engineering degree</strong></a>. There are a number of Harvard faculty interested in <a href="https://quantum.harvard.edu/hqi-members">quantum computing</a>  including new incoming faculty member <a href="https://people.eecs.berkeley.edu/~anuraganshu/"><strong>Anurag Anshu</strong></a>. In this area as well I am open to co-advising, including students outside CS.</p>



<p>If you are applying to graduate studies and are potentially interested in working with me, you can indicate this in the application form and statement of interest. This is the best way to ensure that I read your application (and in particular better than emailing me separately: for fairness sake I read all the applications together in batch, regardless of whether the candidate emailed me or not).</p>



<p>We are also looking for <strong>postdocs</strong>! We have the general <strong>Rabin postdoc position</strong>, as well as specific positions in <strong>privacy</strong>, <strong>fairness</strong>, and <strong>theory of machine learning</strong>, all <a href="https://academicpositions.harvard.edu/postings/10730"><strong>described in the following ad</strong></a>. There is also a separate <a href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html"><strong>networking+theory</strong></a> postdoc position. Several other machine-learning theory related postdocs are linked in our <a href="https://mltheory.org/#opportunities"><strong>ML theory opportunities page</strong></a>. See also the <strong><a href="https://quantum.harvard.edu/external-candidates">quantum initiative postdoctoral fellowship</a></strong>. There may be more CS, ML and quantum related opportunities that I am missing. If I hear of more relevant opportunities then I will post them here and/or <a href="https://twitter.com/boazbaraktcs"><strong>on Twitter</strong></a>. </p>



<p>Finally, Harvard welcomes applications from candidates of all backgrounds, regardless of disability, gender identity and expression, physical appearance, race, religion, or sexual orientation. I would like to <strong>especially encourage</strong> applications from <strong>members of under-represented groups</strong>. Computer Science, and in particular the areas I work in, have a significant diversity problem. We are trying at Harvard to create an environment that is welcoming and inclusive for people from all backgrounds. I am sincerely grateful and appreciative of people placing their trust in us by applying, and we will do our best to live up to that trust.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/10/20/opportunities-at-harvard/"><span class="datestr">at October 20, 2021 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-3394824620518911343">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html">Networking+Theory Postdoc at Harvard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The last couple of years one aspect of research I've greatly enjoyed is getting back into networking, which is really due to my excellent (and patient) collaborators Ran Ben Basat (now at UCL, was a postdoc at Harvard) and Minlan Yu (at Harvard).  Minlan and I are working to establish a larger-scale Networking+Theory (hopefully broadening to an even larger Systems+Theory) group at Harvard, working on algorithmic problems in the context of real (or at least very real-ish) systems.  We have funding, and are looking for a postdoc, the basic description is below.  Ideally we're looking for people comfortable with the theory side and the systems side.  The website link for applying is <a href="https://academicpositions.harvard.edu/postings/10748">https://academicpositions.harvard.edu/postings/10748</a> .  We have preliminary website for the group at <a href="https://projects.iq.harvard.edu/theosys">https://projects.iq.harvard.edu/theosys</a> (it's just a start, Minlan and I are both on sabbatical, but you can see some of our publications).  We look forward to finding another member of the team!</p><p><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;"><span style="font-family: Avenir-Heavy, Helvetica, Arial, sans-serif; font-weight: bolder;">Networking + Theory Postdoctoral Position</span></span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">The John A. Paulson School of Engineering and Applied Sciences at Harvard University (SEAS) seeks applicants for a postdoctoral position in networking and theory. The postdoc is intended for one year but there will be funding to potentially extend it to a second year. The postdoc will receive a generous salary as well as an allocation for research and travel expenses.</span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">We are looking for junior scientists who are especially interested in working at the intersection of networking and algorithmic theory, in areas such as programmable network architectures, data center network management, cloud computing, and algorithms for the Internet.  Example topics of interest include but are not limited to the design and analysis of sketches and filters for use in real systems, network security, network compression methods, and optimizing network performance for machine learning applications.  The ideal candidate will be interested in both building real systems and either developing algorithms and data structures or using existing, underutilized results from the theoretical literature in system design.  </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">The postdoc is intended to work with closely with Minlan Yu and Michael Mitzenmacher, and others involved in the group focused on Systems + Theory work that they are developing, as well as possibly other Harvard faculty.   </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">Candidates should have backgrounds in networking and/or theoretical computer science.  Candidates should demonstrate experience in working at the intersection of these areas, or otherwise demonstrate how they will be able to contribute at the intersection. The candidate will be expected to publish scholarly papers, attend internal, domestic, and international conferences and meetings, and take on a mentorship role for undergraduate and graduate students.  </span><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><br style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;" /><span style="background-color: white; color: #212529; font-family: AvenirLTStd55Roman, Helvetica, Arial, sans-serif; font-size: 14px;">Harvard SEAS is dedicated to building a diverse community that is welcoming for everyone, regardless of disability, gender identity and expression, physical appearance, race, religion, or sexual orientation.  We strongly encourage applications from members of underrepresented groups.</span></p><p><br /></p><div><br /></div></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/10/networkingtheory-postdoc-at-harvard.html"><span class="datestr">at October 20, 2021 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4572">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Online Optimization Post 6: The Impagliazzo Hard-Core Set Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(This is the sixth in a series of posts on online optimization techniques and their “applications” to complexity theory, combinatorics and pseudorandomness. The plan for this series of posts is to alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application.” The first two posts were about the technique of multiplicative weight updates and its application to “derandomizing” probabilistic arguments based on combining a Chernoff bound and a union bound. The third and fourth post were about the Follow-the-Regularized-Leader framework, and how it unifies multiplicative weights and gradient descent, and a “gradient descent view” of the Frieze-Kannan Weak Regularity Lemma. The fifth post was about the constrained version of the Follow-the-Regularized-Leader framework, and today we shall see how to apply that to a proof of the Impagliazzo Hard-Core Lemma.)</em></p>
<p><span id="more-4572"></span></p>
<p><b>1. The Impagliazzo Hard-Core Lemma </b></p>
<p>The Impagliazzo Hard-Core Lemma is a striking result in the theory of average-case complexity. Roughly speaking, it says that if <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" /> is a function that is “weakly” hard on average for a class <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> of “efficiently computable” functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f}" class="latex" />, that is, if, for some <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta&gt;0}" class="latex" />, we have that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f+%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+%5C%7B+0%2C1%5C%7D%5En%7D+%5Bf%28x%29+%3D+g%28x%29+%5D+%5Cleq+1+-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f \in {\cal F}: \ \ \Pr_{x\sim \{ 0,1\}^n} [f(x) = g(x) ] \leq 1 -\delta " class="latex" /></p>
<p>then there is a subset <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" /> of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+2%5Cdelta+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq 2\delta 2^n}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is “strongly” hard-on-average on <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />, meaning that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f+%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5Bf%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f \in {\cal F}: \ \ \Pr_{x\sim H} [f(x) = g(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
<p>for a small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt;0}" class="latex" />. Thus, the reason why functions from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> make a mistake in predicting <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> at least a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" /> fraction of the times is that there is a “hard-core” set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of inputs such that every function from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> makes a mistake about 1/2 of the times for the <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" /> fraction of inputs coming from <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />.</p>
<p>The result is actually not literally true as stated above, and it is useful to understand a counterexample, in order to motivate the correct statement. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> contains just <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\delta}" class="latex" /> functions, and that each function <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in \cal F}" class="latex" /> differs from <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> in exactly a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" /> fraction of inputs from <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" />, and that the set of mistakes are <em>disjoint</em>. Thus, for every set <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" />, no matter its size, there is a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in \cal F}" class="latex" /> that agrees with <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> on at least a <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1-\delta}" class="latex" /> fraction of inputs from <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />. The reason is that the sets of inputs on which the functions of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> differ from <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> form a partition of <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" />, and so their intersections with <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> form a partition of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />. By an averaging argument, one of those intersections must then contain at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%7CH%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta |H|}" class="latex" /> elements of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />.</p>
<p>In the above example, however, if we choose any three distinct functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2Cf_2%2Cf_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,f_2,f_3}" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+x%5Cin+%5C%7B+0%2C1+%5C%7D%5En%3A+%5C+%5C+%5C+g%28x%29+%3D+%7B%5Crm+majority%7D+%28f_1%28x%29%2C+f_2%28x%29%2Cf_3%28x%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall x\in \{ 0,1 \}^n: \ \ \ g(x) = {\rm majority} (f_1(x), f_2(x),f_3(x)) " class="latex" /></p>
<p>So, although <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is weakly hard on average with respect to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, we have that <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not even worst-case hard for a slight extension of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> in which we allow functions obtained by simple compositions of a small number of functions of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />.</p>
<blockquote><p><b>Theorem 1 (Impagliazzo Hard-Core Lemma)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Then at least one of the following conditions is true: </em></p>
<ul>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not weakly hard-on-average over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ 0,1 \}^n}" class="latex" /> with respect to a slight extension of <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />) There is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in \{ 0,1 \}^n} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</li>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is strongly hard-on-average over a set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of density <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" />) There is a set <img src="https://s0.wp.com/latex.php?latex=%7BH%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H\subseteq \{ 0,1 \}^n}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BH+%5Cgeq+2%5Cdelta+%5Ccdot+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H \geq 2\delta \cdot 2^n}" class="latex" /> and
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Cin+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\in H} [ g(x) = f(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
</li>
</ul>
</blockquote>
<p>Where <img src="https://s0.wp.com/latex.php?latex=%7BI+%5C%7B+%7B%5Crm+boolean%5C+expression%7D+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{I \{ {\rm boolean\ expression} \}}" class="latex" /> is equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> depending on whether the boolean expression is true or false (the letter “<img src="https://s0.wp.com/latex.php?latex=%7BI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{I}" class="latex" />” stands for “indicator” function of the truth of the expression).</p>
<p><b>2. Proving the Lemma </b></p>
<p>Impagliazzo’s proof had <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> polynomial in both <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\epsilon}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/\delta}" class="latex" />, and an alternative proof discovered by Nisan has a stronger bound on <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cepsilon%5E%7B-1%7D+%5Cdelta%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon^{-2} \log \epsilon^{-1} \delta^{-1}}" class="latex" />. The proofs of Impagliazzo and Nisan did not immediately give a set of size <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta2^n}" class="latex" /> (the set had size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta 2^n}" class="latex" />), although this could be achieved by iterating their argument. An idea of Holenstein allows to prove the above statement in a more direct way.</p>
<p>Today we will see how to obtain the Impagliazzo Hard-Core Lemma from online optimization, as done by Barak, Hardt and Kale. Their proof achieves all the parameters claimed above, once combined with Holenstein’s ideas.</p>
<p></p>
<p>We say that a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> (here “<img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />” stands for probability <em>measure</em>; we use this letter since we have already used <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> last time to denote the Bregman divergence) has min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K}" class="latex" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29+%5Cleq+2%5E%7B-K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x) \leq 2^{-K}}" class="latex" />. In other words, the min-entropy of a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> over a sample space <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+H_%7B%5Cinfty%7D+%28M%29+%3A%3D+%5Cmin_%7Bx%5Cin+X%7D+%5Clog_2+%5Cfrac+1+%7BM%28x%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle H_{\infty} (M) := \min_{x\in X} \log_2 \frac 1 {M(x)} " class="latex" /></p>
<p>The uniform distribution over a set <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> has min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CH%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 |H|}" class="latex" />, and all distributions of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K}" class="latex" /> can be realized as a convex combination of distributions that are each uniform over a set of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq K}" class="latex" />, thus uniform distributions over large sets and large-min-entropy distributions are closely-related concepts. We will prove the following version of the hard-core lemma:</p>
<blockquote><p><b>Theorem 2 (Impagliazzo Hard-Core Lemma — Min-Entropy Version)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> be a finite set, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: X \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: X \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Then at least one of the following conditions is true: </em></p>
<ul>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is not weakly hard-on-average over <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with respect to <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />) There is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+X%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in X} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</li>
<li>(<img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g}" class="latex" /> is strongly hard-on-average on a distribution of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta |X|}" class="latex" />) There is a distribution <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%5Cleq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\sim H} [ g(x) = f(x) ] \leq \frac 12 + \epsilon " class="latex" /></p>
</li>
</ul>
</blockquote>
<p>Under minimal assumptions on <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> (that it contains <img src="https://s0.wp.com/latex.php?latex=%7B%3C%3C+2%5E%7B%7CX%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;&lt; 2^{|X|}}" class="latex" /> functions), the min-entropy version implies the set version, and the min-entropy version can be used as-is to derive most of the interesting consequences of the set version.</p>
<p>Let us restate it one more time.</p>
<blockquote><p><b>Theorem 3 (Impagliazzo Hard-Core Lemma — Min-Entropy Version)</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> be a finite set, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" /> be a collection of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f: X \rightarrow \{ 0,1 \}}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g: X \rightarrow \{ 0,1 \}}" class="latex" /> a function, and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt;0}" class="latex" /> be positive reals. Suppose that for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> we have </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%5CPr_%7Bx%5Csim+H%7D+%5B+g%28x%29+%3D+f%28x%29+%5D+%3E+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall f\in {\cal F}: \ \ \Pr_{x\sim H} [ g(x) = f(x) ] &gt; \frac 12 + \epsilon " class="latex" /></p>
<p>Then there is a <img src="https://s0.wp.com/latex.php?latex=%7Bk%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D+%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k= O(\epsilon^{-2} \log \delta^{-1} )}" class="latex" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_k+%5Cin+%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_k \in \cal F}" class="latex" />, such that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I+%5C%7B+f_1%28x%29+%2B+%5Cldots+%2B+f_k%28x%29%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I \{ f_1(x) + \ldots + f_k(x)\geq b \} " class="latex" /></p>
<p>satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Cin+X%7D+%5B+g%28x%29+%3D+h%28x%29+%5D+%5Cgeq+1-%5Cdelta+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\in X} [ g(x) = h(x) ] \geq 1-\delta " class="latex" /></p>
</blockquote>
<p>As in previous posts, we are going to think about a game between a “builder” that works toward the construction of <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> and an “inspector” that looks for defects in the construction. More specifically, at every round <img src="https://s0.wp.com/latex.php?latex=%7Bi+%3D+1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i = 1,\ldots,T}" class="latex" />, the inspector is going to pick a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta|X|}" class="latex" /> and the builder is going to pick a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i\in {\cal F}}" class="latex" />. The loss function, that the inspector wants to minimize, is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i+%28M%29+%3A%3D+%5Cmathop%7B%5Cmathbb+P%7D_%7Bx%5Csim+M%7D+%5Bf_i+%28x%29+%3D+g%28x%29%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i (M) := \mathop{\mathbb P}_{x\sim M} [f_i (x) = g(x)] " class="latex" /></p>
<p>The inspector runs the agile online mirror descent algorithm with the constraint of picking distributions of the required min-entropy, and using the entropy regularizer; the builder always chooses a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i}" class="latex" /> such that that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i+%28M%29+%3A%3D+%5Cmathop%7B%5Cmathbb+P%7D_%7Bx%5Csim+M%7D+%5Bf_i+%28x%29+%3D+g%28x%29%5D+%3E+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i (M) := \mathop{\mathbb P}_{x\sim M} [f_i (x) = g(x)] &gt; \frac 12 + \epsilon " class="latex" /></p>
<p>which is always a possible choice given the assumptions of our theorem.</p>
<p>Just by plugging the above setting into the analysis from the previous post, we get that if we play this online game for <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%28%5Cepsilon%5E%7B-2%7D+%5Clog+%5Cdelta%5E%7B-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T = O(\epsilon^{-2} \log \delta^{-1})}" class="latex" /> steps, the builder picks functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_T}" class="latex" /> such that, <em>for every distribution</em> <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log 2\delta |X|}" class="latex" />, we have</p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p><a name="game"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+%5C+i+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%3E+%5Cfrac+12+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, \ i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] &gt; \frac 12 \ \ \ \ \ (1)" class="latex" /></p>
<p><a name="game"></a><a name="game"></a><a name="game"></a><a name="game"></a></p>
<p>We will prove that <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a> holds in the next section, but we emphasize again that it is just a matter of mechanically using the analysis from the previous post. Impagliazzo’s proof relies, basically, on playing the game using lazy mirror descent with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_2^2}" class="latex" /> regularization, and he obtains a guarantee like the one above after <img src="https://s0.wp.com/latex.php?latex=%7BT%3DO%28%5Cepsilon%5E%7B-2%7D+%5Cdelta%5E%7B-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T=O(\epsilon^{-2} \delta^{-1})}" class="latex" /> steps.</p>
<p>What do we do with <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a>? Impagliazzo’s original reasoning was to define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+majority+%28f_1%28x%29%2C%5Cldots%2Cf_T%28x%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := majority (f_1(x),\ldots,f_T(x)) " class="latex" /></p>
<p>and to consider the set <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> of “bad” inputs <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) \neq g(x)}" class="latex" />. We have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+x+%5Cin+B+%5C+%5C+%5CPr_%7Bi+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall x \in B \ \ \Pr_{i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] \leq \frac 12 " class="latex" /></p>
<p>and so</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+B%2C+%5C+i+%5Csim+%5C%7B+1%2C%5Cldots%2CT+%5C%7D+%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%5Cleq+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim B, \ i \sim \{ 1,\ldots,T \} } [ f_i(x) = g(x) ] \leq \frac 12 " class="latex" /></p>
<p>The min-entropy of the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CB%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 |B|}" class="latex" />, and this needs to be less than <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" />, so we conclude that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) \neq g(x)}" class="latex" /> happens for at most a <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta}" class="latex" /> fraction of elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />.</p>
<p>This is qualitatively what we promised, but it is off by a factor of 2 from what we stated above. The factor of 2 comes from a subsequent idea of Holenstein. In Holenstein’s analysis, we sort elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> according to</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bi+%5Csim+%5C%7B+1%2C%5Cldots%2C+T+%5C%7D%7D+%5B+f_i%28x%29+%3D+g_i+%28x%29+%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{i \sim \{ 1,\ldots, T \}} [ f_i(x) = g_i (x) ] " class="latex" /></p>
<p>and he lets <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> be the set of <img src="https://s0.wp.com/latex.php?latex=%7B2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\delta |X|}" class="latex" /> elements of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> for which the above quantity is smallest, and he shows that if we properly pick an integer <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> and define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28x%29+%3A%3D+I%5C%7B+f_1%28x%29+%2B+%5Ccdots+%2B+f_T%28x%29+%5Cgeq+b+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle h(x) := I\{ f_1(x) + \cdots + f_T(x) \geq b \} " class="latex" /></p>
<p>then <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x)}" class="latex" /> will be equal to <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{g(x)}" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cnot%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\not\in B}" class="latex" /> and also for at least half the <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in B}" class="latex" />, meaning that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%3D+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x) = g(x)}" class="latex" /> for at least a <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1-\delta}" class="latex" /> fraction of the input. Since this is a bit outside the scope of this series of posts, we will not give an exposition of Holenstein’s argument.</p>
<p><b>3. Analysis of the Online Game </b></p>
<p>It remains to show that we can achieve <a href="https://lucatrevisan.wordpress.com/feed/#game">(1)</a> with <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1+%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1 {\delta}}" class="latex" />. As we said, we play a game in which, at every step <img src="https://s0.wp.com/latex.php?latex=%7Bi%3D1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i=1,\ldots,T}" class="latex" /></p>
<ul>
<li>The “inspector” player picks a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> of min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" />, that is, it picks a number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B2%5Cdelta+%7CX%7C%7D+%5Cgeq+M_i%28x%29%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {2\delta |X|} \geq M_i(x)\geq 0}" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x\in X}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+M_i%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_x M_i(x) = 1}" class="latex" />.</li>
<li>The “builder” player picks a function <img src="https://s0.wp.com/latex.php?latex=%7Bf_i+%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i \in {\cal F}}" class="latex" />, whose existence is guaranteed by the assumption of the theorem, such that
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+M_i%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%5Cgeq+%5Cfrac+12+%2B%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim M_i} [f_i(x) = g(x) ] \geq \frac 12 +\epsilon " class="latex" /></p>
<p>and defines the loss function</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_i%28M%29+%3A%3D+%5CPr_%7Bx%5Csim+M_i%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%3D+%5Csum_%7Bx%5Cin+X%7D+M%28x%29+%5Ccdot+I%5C%7B+f_i%28x%29+%3D+g%28x%29+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle L_i(M) := \Pr_{x\sim M_i} [f_i(x) = g(x) ] = \sum_{x\in X} M(x) \cdot I\{ f_i(x) = g(x) \} " class="latex" /></p>
</li>
<li>The “inspector” is charged the loss <img src="https://s0.wp.com/latex.php?latex=%7BL_i%28M_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_i(M_i)}" class="latex" />.</li>
</ul>
<p>We analyze what happens if the inspector plays the strategy defined by agile mirror descent with negative entropy regularizer. Namely, we define the regularizer</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+R%28M%29+%3D+c+%5Csum_x+M%28x%29+%5Clog+M%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle R(M) = c \sum_x M(x) \log M(x) " class="latex" /></p>
<p>for a choice of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> that we will fix later. The corresponding Bregman divergence is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M%2C%5Chat+M%29+%3D+c+KL%28M%2C%5Chat+M%29+%3D+c+%5Ccdot+%5Cleft%28+%5Csum_x+M%28x%29+%5Clog+%5Cfrac+%7BM%28x%29%7D%7B%5Chat+M%28x%29%7D+-+%5Csum_x+M%28x%29+%2B+%5Csum_x+%5Chat+M%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M,\hat M) = c KL(M,\hat M) = c \cdot \left( \sum_x M(x) \log \frac {M(x)}{\hat M(x)} - \sum_x M(x) + \sum_x \hat M(x) \right) " class="latex" /></p>
<p>and we work over the space <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal K}" class="latex" /> of distributions of min-entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \log_2 2\delta |X|}" class="latex" />.</p>
<p>The agile online mirror descent algorithm is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M_1+%3D+%5Carg%5Cmin_%7BM%5Cin+%7B%5Ccal+K%7D%7D+R%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle M_1 = \arg\min_{M\in {\cal K}} R(M) " class="latex" /></p>
<p>so that <img src="https://s0.wp.com/latex.php?latex=%7BM_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_1}" class="latex" /> is the uniform distribution, and for <img src="https://s0.wp.com/latex.php?latex=%7Bi%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i\geq 1}" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat+M_%7Bi%2B1%7D+%3D+%5Carg%5Cmin_%7BM%3A+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D+%5C+%5C+D%28M_i%2CM%29+%2B+L_i+%28M%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \hat M_{i+1} = \arg\min_{M: X \rightarrow {\mathbb R}} \ \ D(M_i,M) + L_i (M) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M_%7Bi%2B1%7D+%3D+%5Carg%5Cmin_%7BM+%5Cin+%7B%5Ccal+K%7D%7D+%5C+%5C+D%28M%2C+%5Chat+M_%7Bi%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle M_{i+1} = \arg\min_{M \in {\cal K}} \ \ D(M, \hat M_{i+1} )" class="latex" /></p>
<p>Solving the first step of agile online mirror descent, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat+M_%7Bi%2B1%7D+%28x%29+%3D+M_i%28x%29+e%5E%7B-%5Cfrac+1c+I+%5C%7B+f%28x%29+%3D+g%28x%29+%5C%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \hat M_{i+1} (x) = M_i(x) e^{-\frac 1c I \{ f(x) = g(x) \} } " class="latex" /></p>
<p>Using the analysis from the previous post, for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal K}" class="latex" />, and every number <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of steps, we have the regret bound</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5ET+L_i%28M_i%29+-+L_i%28M%29+%5Cleq+D%28M%2CM_1%29+%2B+%5Csum_%7Bi%3D1%7D%5ET+D%28M_i%2C+%5Chat+M_%7Bi%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \sum_{i=1}^T L_i(M_i) - L_i(M) \leq D(M,M_1) + \sum_{i=1}^T D(M_i, \hat M_{i+1} ) " class="latex" /></p>
<p>and we can bound</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M%2CM_1%29+%3D+c+%5Csum_x+M%28x%29+%5Ccdot+%5Cln+%7CX%7C+%5Ccdot+M%28x%29+%5Cleq+c+%5Cln+%5Cfrac+1+%7B2%5Cdelta%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M,M_1) = c \sum_x M(x) \cdot \ln |X| \cdot M(x) \leq c \ln \frac 1 {2\delta} " class="latex" /></p>
<p>and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28M_i%2C%5Chat+M_%7Bi%2B1%7D%29+%3D+c%5Ccdot+%5Cleft%28+%5Csum_x+M_i%28x%29+%5Cln+%5Cfrac%7BM_i%28x%29%7D%7B%5Chat+M_%7Bi%2B1%7D+%28x%29+%7D+%2B+%5Csum_x+%5Chat+M_%7Bi%2B1%7D%28x%29+-+M_i%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle D(M_i,\hat M_{i+1}) = c\cdot \left( \sum_x M_i(x) \ln \frac{M_i(x)}{\hat M_{i+1} (x) } + \sum_x \hat M_{i+1}(x) - M_i(x) \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D+c+%5Ccdot+%5Csum_x+M_i%28x%29+%5Ccdot+%5Cleft%28+%5Cfrac+1c+I%5C%7B+f%28x%29+%3D+g%28x%29+%5C%7D+%2B+e%5E%7B-%5Cfrac+1+cI+%5C%7B+f%28x%29+%3D+g%28x%29%5C%7D%7D+-1+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle = c \cdot \sum_x M_i(x) \cdot \left( \frac 1c I\{ f(x) = g(x) \} + e^{-\frac 1 cI \{ f(x) = g(x)\}} -1 \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cleq+O+%5Cleft%28+%5Cfrac+1c+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \leq O \left( \frac 1c \right) " class="latex" /></p>
<p>where, in the last step, we used the fact the quantity in parenthesis is either 0 or <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fc+%2B+e%5E%7B-1%2Fc%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/c + e^{-1/c} - 1}" class="latex" /> which is <img src="https://s0.wp.com/latex.php?latex=%7BO%281%2Fc%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(1/c^2)}" class="latex" />, and that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+M_i%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_x M_i(x) = 1}" class="latex" /> because <img src="https://s0.wp.com/latex.php?latex=%7BM_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M_i}" class="latex" /> is a distribution.</p>
<p>Overall, the regret is bounded by</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5ET+L_i%28M_i%29+-+L_i%28M%29+%5Cleq+O+%5Cleft%28+c%5Clog+%5Cfrac+1%5Cdelta+%2B+%5Cfrac+Tc+%5Cright%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7BT+%5Clog+%5Cfrac+1%5Cdelta%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \sum_{i=1}^T L_i(M_i) - L_i(M) \leq O \left( c\log \frac 1\delta + \frac Tc \right) \leq O \left( \sqrt{T \log \frac 1\delta}\right) " class="latex" /></p>
<p>where the last inequality comes from an optimized choice of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>Recall that we choose the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_i}" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BL_i%28M_i%29+%5Cgeq+1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_i(M_i) \geq 1/2 + \epsilon}" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" />, so for every <img src="https://s0.wp.com/latex.php?latex=%7BM%5Cin+%7B%5Ccal+K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M\in {\cal K}}" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%5Cgeq+%5Cfrac+12+%2B+%5Cepsilon+-+O+%28%5Cleft%28+%5Csqrt%7B%5Cfrac+1+T+%5Clog+%5Cfrac+1%5Cdelta%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \frac 1T \sum_{i=1}^T L_i (M) \geq \frac 12 + \epsilon - O (\left( \sqrt{\frac 1 T \log \frac 1\delta}\right) " class="latex" /></p>
<p>and by choosing <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> of the order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1 \delta}" class="latex" /> we get</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cforall+M+%5Cin+%7B%5Ccal+K%7D+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \forall M \in {\cal K} : \ \ \ \frac 1T \sum_{i=1}^T L_i (M) &gt; \frac 12 " class="latex" /></p>
<p>It remains to observe that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+L_i+%28M%29+%3D+%5Cfrac+1T+%5Csum_%7Bi%3D1%7D%5ET+%5CPr_%7Bx%5Csim+M%7D+%5Bf_i%28x%29+%3D+g%28x%29+%5D+%3D+%5CPr_%7Bi+%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%2C+%5C+x%5Csim+M%7D+%5B+f_i+%28x%29+%3D+g%28x%29+%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \frac 1T \sum_{i=1}^T L_i (M) = \frac 1T \sum_{i=1}^T \Pr_{x\sim M} [f_i(x) = g(x) ] = \Pr_{i \sim \{1,\ldots,T\}, \ x\sim M} [ f_i (x) = g(x) ] " class="latex" /></p>
<p>so we have that for every distribution <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> of min-entropy at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+2%5Cdelta+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 2\delta |X|}" class="latex" /> it holds that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bi+%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%2C+%5C+x%5Csim+M%7D+%5B+f_i+%28x%29+%3D+g%28x%29+%5D%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{i \sim \{1,\ldots,T\}, \ x\sim M} [ f_i (x) = g(x) ]&gt; \frac 12 " class="latex" /></p>
<p>which is the statement that we promised and from which the Impagliazzo Hard-Core Lemma follows.</p>
<p><b>4. Some Final Remarks </b></p>
<p>After Impagliazzo circulated a preliminary version of his paper, Nisan had the following idea: consider the game that we define above, in which a builder picks an <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f\in {\cal F}}" class="latex" />, an inspector picks a distribution <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Cin+%7B%5Ccal+K%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M \in {\cal K}}" class="latex" /> of the prescribed min-entropy, and the loss for the inspector is given by <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr+%5B+f%28x%29+%3D+g%28x%29+%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Pr [ f(x) = g(x) ]}" class="latex" />. We can think of it as a zero-sum game if we also assign a gain <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr+%5B+f%28x%29+%3D+g%28x%29%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Pr [ f(x) = g(x)]}" class="latex" /> to the builder.</p>
<p>If the builder plays second, there is a strategy that guarantees a gain that is at least <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2+%2B+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1/2 + \epsilon}" class="latex" />, and so there must be a mixed strategy, that is, a distribution <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+DF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal DF}}" class="latex" /> over functions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\cal F}" class="latex" />, that guarantees such a gain even if the builder plays first. In other words, for all distributions <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" /> of the prescribed min-entropy we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+f%5Csim+%7B%5Ccal+DF%7D%7D+%5B+f%28x%29+%3D+g%28x%29+%5D+%5Cgeq+%5Cfrac+12+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, f\sim {\cal DF}} [ f(x) = g(x) ] \geq \frac 12 + \epsilon " class="latex" /></p>
<p>Nisan then observes that we can sample <img src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5Cfrac+1%7B%5Cepsilon%5E2%7D+%5Clog+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T = \frac 1{\epsilon^2} \log |X|}" class="latex" /> functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1,\ldots,f_T}" class="latex" /> and have, with high probability</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr_%7Bx%5Csim+H%2C+i%5Csim+%5C%7B1%2C%5Cldots%2CT%5C%7D%7D+%5B+f_i%28x%29+%3D+g%28x%29+%5D+%3E+%5Cfrac+12+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \Pr_{x\sim H, i\sim \{1,\ldots,T\}} [ f_i(x) = g(x) ] &gt; \frac 12 " class="latex" /></p>
<p>and the sampling bound on <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> can be improved to order of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%7B%5Cepsilon%5E2%7D+%5Clog+%5Cfrac+1%7B%5Cepsilon+%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac 1 {\epsilon^2} \log \frac 1{\epsilon \delta}}" class="latex" /> with the same conclusion.</p>
<p>Basically, what we have been doing today is to come up with an algorithm that finds an approximate solution for the LP that defines the optimal mixed strategy for the game, and to design the algorithm is such a way that the solution is very sparse.</p>
<p>This is a common feature of other applications of online optimization techniques to find “sparse approximations”: one sets up an optimization problem whose objective function measures the “approximation error” of a given solution. The object we want to approximate is the optimum of the optimization problem, and we use variants of mirror descent to prove the existence of a sparse solution that is a good approximation.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/"><span class="datestr">at October 20, 2021 04:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/">Department Chair, Computer Science and Engineering at NYU Tandon (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>NYU’s Dept. of Computer Science and Engineering within the Tandon School of Engineering is hiring a department chair. The position is open to all research areas, but applications from theory candidates are very welcome. Our department has a growing Algorithms and Foundations Group (csefoundations.engineering.nyu.edu), with several recent hires working on theory of algorithms and machine learning.</p>
<p>Website: <a href="https://apply.interfolio.com/96495">https://apply.interfolio.com/96495</a><br />
Email: cmusco@nyu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/department-chair-computer-science-and-engineering-at-nyu-tandon-apply-by-december-1-2021/"><span class="datestr">at October 20, 2021 03:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/">Assistant Professor at Georgetown University (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Provost’s Distinguished Faculty Fellow and Assistant Professor of Computer Science position available at Georgetown University. For information on how to apply, please visit <a href="https://cs.georgetown.edu/jobs/">https://cs.georgetown.edu/jobs/</a></p>
<p>Website: <a href="http://apply.interfolio.com/96143">http://apply.interfolio.com/96143</a><br />
Email: nitin.vaidya@georgetown.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/20/assistant-professor-at-georgetown-university-apply-by-december-15-2021/"><span class="datestr">at October 20, 2021 02:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/10/20/highleyman/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/10/20/highleyman/">The Saga of Highleyman's Data.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The first machine learning benchmark dates back to the late 1950s. Few used it and even fewer still remembered it by the time benchmarks became widely used in machine learning in the late 1980s.</p>

<p>In 1959 at Bell Labs, Bill Highleyman and Louis Kamenstky designed a <a href="https://dl.acm.org/doi/10.1145/1457838.1457894">scanner to evaluate character recognition techniques</a>. Their goal was “to facilitate a systematic study of character-recognition techniques and an evaluation of methods prior to actual machine development.” It was not clear at the time which part of the computations should be done in special purpose hardware and which parts should be done with more general computers. Highleyman later <a href="https://patents.google.com/patent/US2978675A/en">patented an OCR scheme</a> that we recognize today as a convolutional neural network with convolutions optically computed as part of the scanning.</p>

<p>Highleyman and Kamentsky used their scanner to create a dataset of 1800 alphanumeric characters. They gathered the 26 letters of the alphabet and 10 digits from 50 different writers. Each character in their corpus was scanned in binary at a resolution of 12 x 12 and stored on punch cards that were compatible with the <a href="https://en.wikipedia.org/wiki/IBM_704">IBM 704</a>, the GPGPU of the era.</p>

<p class="center"><img width="95%" alt="A look at Highleyman’s digits" src="http://www.argmin.net/assets/highleyman-data.png" /></p>

<p>With the data in hand, Highleyman and Kamenstky began studying various proposed techniques for recognition. In particular, they analyzed a method of Woody Bledsoe and published an analysis claiming to be <a href="https://ieeexplore.ieee.org/document/5219829">unable to reproduce Bledsoe’s results</a>. Bledsoe found their numbers to be considerably lower than he had expected, and asked Highleyman to send him the data. Highleyman obliged, mailing the package of punch cards across the country to Sandia Labs.</p>

<p>Upon receiving the data, Bledsoe conducted a new experiment. In what may be the first implementation of a train-test split, he divided the characters up, using 40 writers for training and 10 for testing. By tuning the hyperparameters, <a href="https://ieeexplore.ieee.org/document/5219162">Bledsoe was able to achieve approximately 60% error</a>. Bledsoe also suggested that the high error rates were to be expected as Highleyman’s data was too small. Prophetically, he declared that 1000 alphabets might be needed for good performance.</p>

<p>By this point, Highleyman had also shared his data with Chao Kong “C.K.” Chow at the Burroughs Corporation (a precursor to Unisys). A pioneer of <a href="https://ieeexplore.ieee.org/document/5222035">using decision theory for pattern recognition</a>, Chow built a pattern recognition system for characters. Using the same train-test split as Bledsoe, <a href="https://ieeexplore.ieee.org/document/5219431">Chow obtained an error rate of 41.7%</a> using a convolutional neural network.</p>

<p class="center"><img width="75%" alt="Chow’s architecture" src="http://www.argmin.net/assets/chownet.png" /></p>

<p>Highleyman made at least six additional copies of the data he had sent to Bledsoe and Chow, and many researchers remained interested. He thus decided to <a href="https://ieeexplore.ieee.org/document/4037813">publicly offer to send a copy to anyone</a> willing to pay for the duplication and shipping fees. An interested party would simply have to mail him a request. Of course, the dataset was sent by US Postal Service. Electronic transfer didn’t exist at the time, resulting in sluggish data transfer rates on the order of a few bits per minute.</p>

<p>Highleyman not only created the first machine learning benchmark. He authored the the first formal study of <a href="https://ieeexplore.ieee.org/document/6768949">train-test splits</a> and proposed <a href="https://ieeexplore.ieee.org/document/4066882">empirical risk minimization for pattern classification</a> as part of his 1961 dissertation.
By 1963, however, Highleyman had left his research position at Bell Labs and abandoned pattern recognition research.</p>

<p>We don’t know how many people requested Highleyman’s data. The total number of copies may have been less than twenty. Based on citation surveys, we determined there were at least another six copies made after Highleyman’s public offer for duplication, sent to  <a href="https://ieeexplore.ieee.org/abstract/document/1671536">CMU</a>, <a href="https://ieeexplore.ieee.org/document/1671257">Honeywell</a>, <a href="https://ieeexplore.ieee.org/document/5008873">SUNY Stony Brook</a>, <a href="https://spiral.imperial.ac.uk/bitstream/10044/1/16132/2/Ullmann-JR-1968-PhD-Thesis.pdf">Imperial College</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/0031320371900045">UW Madison</a>, and Stanford Research Institute (SRI).</p>

<p>The SRI team of John Munson, Richard Duda, and Peter Hart performed some of the most <a href="https://ieeexplore.ieee.org/document/1687355">extensive experiments with Highleyman’s data</a>. A 1-nearest-neighbors baseline achieved an error rate of 47.5%. With a more sophisticated approach, they were able to do significantly better. They used a multi-class, piecewise linear model, trained using Kesler’s multi-class version of the perceptron algorithm (what we’d now call “one-versus all classification”). Their feature vectors were 84 simple pooled edge detectors in different regions of the image at different orientations. With these features, they were able to get a test error of 31.7%, 10 percentage points better than Chow. When restricted only to digits, this method recorded 12% error. The authors concluded that they needed more data, and that the error rates were “still far too high to be practical.” They concluded that “larger and higher-quality datasets are needed for work aimed at achieving useful results.” They suggested that such datasets “may contain hundreds, or even thousands, of samples in each class.”</p>

<p>Munson, Duda, and Hart also performed informal experiments with humans to gauge the readability of Highleyman’s characters. On the full set of alphanumeric characters, they found an average error rate of 15.7%, about 2x better than their pattern recognition machine. But this rate was still quite high and suggested the data needed to be of higher quality. They (again prophetically) concluded that “an array size of at least 20X20 is needed, with an optimum size of perhaps 30X30.”</p>

<p>Decades passed until such a dataset appeared. Thirty years later, with 125 times as much training data, 28x28 resolution, and with grayscale scans, a neural net achieved 0.7% test error on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST digit recognition task</a>. In fact, a similar model to Munson’s architecture consisting of kernel ridge regression trained on pooled edged detectors also achieves 0.6% error. Intuition from the 1960s proved right. The resolution was higher and the number of examples per digit was now in the thousands, just as Bledsoe, Munson, Duda, and Hart predicted would be sufficient. Reasoning heuristically that the test error should be inversely proportional to the square root of the number of training examples, we would expect an 11x improvement over Munson’s approaches. The actual recorded improvement from 12% to 0.7% was closer to 17x, not far from what the back of the envelope calculation predicts.</p>

<p>Unlike Highleyman’s data, MNIST featured only digits, no letters. Only recently, in 2017, researchers from Western Sydney University <a href="https://arxiv.org/abs/1702.05373">extracted alphanumeric characters from the NIST-19 repository</a>. The resulting <em>EMNIST_Balanced</em> dataset has 2400 examples in each of the 47 classes, with a class for all upper case letters, all digits, and some of the non-ambiguous lower case letters. Currently, the best performing <a href="https://www.mdpi.com/2076-3417/9/15/3169">model achieves a test error rate of 9.4%</a>. While the dataset is still fairly new, this is only a 3x improvement over the methods of Munson, Duda, and Hart. Applying the same naive scaling argument as above, the increase in dataset size would predict a 7x improvement if such an improvement was achievable. Considering that the SRI team observed a human-error rate of 11% on Highleyman’s data, it is quite possible that an accuracy of 90% is close to the best that we can expect for recognizing handwritten digits without context.</p>

<p>The story of Highleyman’s data foreshadows many of the later waves of machine learning research. A desire for better evaluation inspired the creation of novel data. Dissemination of the experimental results on this data led to sharing in order for researchers to be content that the evaluation was fair. Once the dataset was distributed, others requested the data to prove their methods were superior. And then the dataset itself became enshrined as a benchmark for competitive testing.  Such comparative testing led to innovations in methods, theory, and data collection and curation itself. We have seen this pattern time and time again in machine learning, from <a href="https://archive.ics.uci.edu/ml/index.php">the UCI repository</a>, to <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, to <a href="https://www.image-net.org/">ImageNet</a>, to <a href="https://predictioncenter.org/">CASP</a>. The nearly forgotten history of Highleyman’s data marks the beginning of this pattern recognition research paradigm.</p>

<p><em>We are, as always, deeply indebted to Chris Wiggins for sending us Munson et al.’s paper after watching a talk by BR on the history of ML benchmarking. We also thank Ludwig Schmidt for pointing us to EMNIST.</em></p>

<h2 id="addendum-on-our-protagonist-bill-highleyman">Addendum on our protagonist Bill Highleyman.</h2>

<p>After posting this blog, we found <a href="https://availabilitydigest.com/public_articles/1208/thesis.pdf">some lovely recollections by Bill Highleyman about his thesis</a>. It is remarkable how Bill invented so many powerful machine learning primitives—finding linear functions that minimize empirical risk, gradient descent to minimize the risk, train-test splits, convolutional neural networks—all as part of his PhD dissertation project. That said,
Bill considered the project to be a failure. He (and Bell Labs) realized the computing of 1959 was not up to the task of character recognition.</p>

<p>After he finished his thesis, Bill abandoned pattern recognition and moved on to work on other cool and practical computer engineering projects that interested him, never once looking back. By the mid sixties Bill had immersed himself in data communication and transmission, and patented novel approaches to electrolytic printing and financial transaction hardware. He eventually ended up specializing in high-reliability computing. Though he developed many of the machine learning techniques we use today, he was content to leave the field and work to advance general computing to catch up with his early ideas.</p>

<p>It’s odd but not surprising that while every machine learning class mentions Rosenblatt, Minsky, and Papert, almost everyone we’ve spoken with so far has never heard of Bill Highleyman.</p>

<p>We worry Bill is no longer reachable as he seems to have no online presence after 2019 and would be 88 years old today. If anyone out there on has met Bill, we’d love to hear more about him. Please drop us a note.</p>

<p>And if anyone has any idea of where we can get a copy of his 1800 characters from 1959, please let us know about that too…</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/10/20/highleyman/"><span class="datestr">at October 20, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/146">TR21-146 |  Sample-Based Proofs of Proximity | 

	Guy Goldberg, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose we have random sampling access to a huge object, such as a graph or a database.
Namely, we can observe the values of \emph{random} locations in the object, say random records in the database or random edges in the graph.
We cannot, however, query locations of our choice. Can we verify complex properties of the object using only this restricted sampling access?

In this work, we initiate the study of \emph{sample-based} proof systems, where the verifier is extremely constrained; Given an input, the verifier can only obtain samples of uniformly random and i.i.d. locations in the input string, together with the values at those locations. The goal is verifying complex properties in sublinear time, using only this restricted access.
Following the literature on Property Testing and on Interactive Proofs of Proximity (IPPs), we seek proof systems where the verifier accepts every input that has the property, and with high probability rejects every input that is \emph{far} from the property.

We study both interactive and non-interactive sample-based proof systems, showing:

	- On the positive side, our main result is that rich families of properties / languages have sub-linear sample-based interactive proofs of proximity (SIPPs).
	We show that every language in $\mathcal{NC}$ has a SIPP, where the sample and communication complexities, as well as the verifier's running time, are $\widetilde{O}(\sqrt{n})$, and with polylog(n) communication rounds.
	We also show that every language that can be computed in polynomial-time and bounded-polynomial space has a SIPP, where the sample and communication complexities of the protocol, as well as the verifier's running time are roughly $\sqrt{n}$, and with a constant number of rounds.
	
	This is achieved by constructing a reduction protocol from SIPPs to IPPs.
	With the aid of an untrusted prover, this reduction enables a restricted, sample-based verifier to simulate an execution of a (query-based) IPP, even though it cannot query the input.
	Applying the reduction to known query-based IPPs yields SIPPs for the families described above.
	
	- We show that every language with an adequate (query-based) property tester has a 1-round SIPP with \emph{constant} sample complexity and logarithmic communication complexity.
	One such language is equality testing, for which we give an explicit and simple SIPP.
	
	- On the negative side, we show that \emph{interaction} can be essential:
	we prove that there is no \emph{non}-interactive sample-based proof of proximity for equality testing.
	
	- Finally, we prove that \emph{private coins} can dramatically increase the power of SIPPs.
	We show a strong separation between the power of public-coin SIPPs and private-coin SIPPs for Equality Testing.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/146"><span class="datestr">at October 19, 2021 08:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/">Some Statistical Gamma Fun</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Nothing takes place in the world whose meaning is not that of some maximum or minimum. — Leonhard Euler</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/kaspermueller/" rel="attachment wp-att-19221"><img width="125" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/10/KasperMueller-150x150.jpg?resize=125%2C125&amp;ssl=1" class="alignright wp-image-19221" height="125" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cantor’s Paradise <a href="https://www.cantorsparadise.com/inventing-mathematics-a33cc9d2732b">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Kasper Müller is a mathematics and data science writer for <a href="https://medium.com/">Medium</a>, where he contributes primarily to the blogs <a href="https://www.cantorsparadise.com/">Cantor’s Paradise</a> and <a href="https://towardsdatascience.com/">Towards Data Science</a>. He wrote a nice <a href="https://www.cantorsparadise.com/the-beautiful-gamma-function-and-the-genius-who-discovered-it-8778437565dc">article</a> last April titled, “The Beautiful Gamma Function and the Genius Who Discovered It.”</p>
<p>
Today we discuss the relevance of the gamma function to statistics and use statistics to suggest a new kind of estimate for it.<br />
<span id="more-19219"></span></p>
<p>
The “Genius” that Müller refers to is Leonhard Euler. Euler proved that for all integers <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 0}" class="latex" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%21+%3D+%5Cint_0%5E1+%28-%5Cln+s%29%5En+ds+%3D+%5Cint_0%5E%5Cinfty+t%5En+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n! = \int_0^1 (-\ln s)^n ds = \int_0^\infty t^n e^{-t} dt, " class="latex" /></p>
<p>where the latter equation uses the substitution <img src="https://s0.wp.com/latex.php?latex=%7Bs+%3D+e%5E%7B-t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s = e^{-t}}" class="latex" />. The right-hand side produces a value for any complex number <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+x+%2B+iy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = x + iy}" class="latex" /> in place of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> provided <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3E+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x &gt; -1}" class="latex" />. This leads to the formal definition </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28z%29+%3D+%5Cint_0%5E%5Cinfty+t%5E%7Bz-1%7D+e%5E%7B-t%7D+dt%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma(z) = \int_0^\infty t^{z-1} e^{-t} dt, " class="latex" /></p>
<p>whose analytic extension is defined everywhere except for <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+0%2C+-1%2C+-2%2C+-3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = 0, -1, -2, -3,\dots}" class="latex" />. Because <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> has no zeroes, its reciprocal is an entire function. One neat value is <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" />. We will be mainly concerned with ratios of two values of <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" />.</p>
<p>
</p><p></p><h2> What is Gamma For? </h2><p></p>
<p></p><p>
For all <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" /> except the non-positive integers, <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> obeys the formula </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+1)}{\Gamma(z)} = z. " class="latex" /></p>
<p>Of course, this follows from <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29+%3D+%28z-1%29%21%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z) = (z-1)!}" class="latex" /> for positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" />. Also </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%29%7D+%3D+%5Cfrac%7B%5CGamma%28z%2B2%29%7D%7B%5CGamma%28z%2B1%29%7D%5Ccdot%5Cfrac%7B%5CGamma%28z%2B1%29%7D%7B%5CGamma%28z%29%7D+%3D+%28z%2B1%29z.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+2)}{\Gamma(z)} = \frac{\Gamma(z+2)}{\Gamma(z+1)}\cdot\frac{\Gamma(z+1)}{\Gamma(z)} = (z+1)z. " class="latex" /></p>
<p>In general, for all <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a &gt; 0}" class="latex" />, <a name="approx"></a></p><a name="approx">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28z%2Ba%29%7D%7B%5CGamma%28z%29%7D+%5Csim+z%5Ea+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(z+a)}{\Gamma(z)} \sim z^a \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="approx"></a> but there is a discrepancy. This and the lack of a simple explicit formula for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(z)}" class="latex" /> at all have always made the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function seem opaque to me. Two notable values are <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B1%7D%7B2%7D%29+%3D+%5Csqrt%7B%5Cpi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{1}{2}) = \sqrt{\pi}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28%5Cfrac%7B3%7D%7B2%7D%29+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2}}" class="latex" />.</p>
<p>
The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function is not even the only uniformly continuous interpolation of the factorial function. It is the unique one whose logarithm is a convex function. This is the first of many reasons given in Müller’s article for <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> to be <em>salient</em> and beautiful, culminating in its relation to the Riemann zeta function given by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28%5Cfrac%7Bs%7D%7B2%7D%29%5Czeta%28s%29%7D%7B%5Cpi%5E%7Bs%2F2%7D%7D+%3D+%5Cfrac%7B%5CGamma%28%5Cfrac%7B1-s%7D%7B2%7D%29%5Czeta%281-s%29%7D%7B%5Cpi%5E%7B%281-s%29%2F2%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(\frac{s}{2})\zeta(s)}{\pi^{s/2}} = \frac{\Gamma(\frac{1-s}{2})\zeta(1-s)}{\pi^{(1-s)/2}}. " class="latex" /></p>
<p>Yet the log-convex uniqueness was proved only 99 years ago, and none of these tell me at a flash what the <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function <b>is</b>. </p>
<p>
What is the simplest label for its corner of the sky? The leading example is the formula for the volume of a sphere of radius <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> dimensions: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++V_n+%3D+%5Cfrac%7B%5Cpi%5E%7Bn%2F2%7D%7D%7B%5CGamma%28n+%2B+%5Cfrac%7B1%7D%7B2%7D%29%7Dr%5En.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  V_n = \frac{\pi^{n/2}}{\Gamma(n + \frac{1}{2})}r^n. " class="latex" /></p>
<p>But I wonder whether a different application is more fundamental. Since we are dealing with <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a = \frac{1}{2}}" class="latex" /> already here, let us define the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%3D+%5Cfrac%7B%5CGamma%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D%7B%5CGamma%28z%29%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) = \frac{\Gamma(z+\frac{1}{2})}{\Gamma(z)}. " class="latex" /></p>
<p>Noting <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%29+%5Csim+z%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z) \sim z^{1/2}}" class="latex" /> via (<a href="https://rjlipton.wpcomstaging.com/feed/#approx">1</a>), this is a tweak of the square-root function. Here are some values of it:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5CGamma_%7B1%2F2%7D%281%29+%26%3D%26+%5Cfrac%7B%5CGamma%281.5%29%7D%7B%5CGamma%281%29%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%2F2%7D%7B1%7D+%3D+%5Cfrac%7B%5Csqrt%7B%5Cpi%7D%7D%7B2%7D%5C%5C+%5CGamma_%7B1%2F2%7D%282%29+%26%3D%26+%5Cfrac%7B%5CGamma%282.5%29%7D%7B%5CGamma%282%29%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%2F4%7D%7B1%7D+%3D+%5Cfrac%7B3%5Csqrt%7B%5Cpi%7D%7D%7B4%7D%5C%5C+%5CGamma_%7B1%2F2%7D%283%29+%26%3D%26+%5Cfrac%7B%5CGamma%283.5%29%7D%7B%5CGamma%283%29%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%2F8%7D%7B2%7D+%3D+%5Cfrac%7B15%5Csqrt%7B%5Cpi%7D%7D%7B16%7D%5C%5C+%5CGamma_%7B1%2F2%7D%284%29+%26%3D%26+%5Cfrac%7B%5CGamma%284.5%29%7D%7B%5CGamma%284%29%7D+%3D+%5Cfrac%7B105%5Csqrt%7B%5Cpi%7D%2F16%7D%7B6%7D+%3D+%5Cfrac%7B35%5Csqrt%7B%5Cpi%7D%7D%7B32%7D%5C%5C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  \Gamma_{1/2}(1) &amp;=&amp; \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}/2}{1} = \frac{\sqrt{\pi}}{2}\\ \Gamma_{1/2}(2) &amp;=&amp; \frac{\Gamma(2.5)}{\Gamma(2)} = \frac{3\sqrt{\pi}/4}{1} = \frac{3\sqrt{\pi}}{4}\\ \Gamma_{1/2}(3) &amp;=&amp; \frac{\Gamma(3.5)}{\Gamma(3)} = \frac{15\sqrt{\pi}/8}{2} = \frac{15\sqrt{\pi}}{16}\\ \Gamma_{1/2}(4) &amp;=&amp; \frac{\Gamma(4.5)}{\Gamma(4)} = \frac{105\sqrt{\pi}/16}{6} = \frac{35\sqrt{\pi}}{32}\\ \end{array} " class="latex" /></p>
<p>
Here is the significance:</p>
<blockquote><p><b> </b> <em> For integer <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 1}" class="latex" />, the expected Euclidean norm of a vector of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent samples from the standard Gaussian distribution is <a name="gammahalf"></a></em></p><em><a name="gammahalf">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csqrt%7B2%7D%5Ccdot%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sqrt{2}\cdot\Gamma_{1/2}(\frac{n}{2}). \ \ \ \ \ (2)" class="latex" /></p>
</a></em><p><em><a name="gammahalf"></a> </em>
</p></blockquote>
<p></p><p>
That’s it: Gamma gives the norm of Gaussians. The norm is of order <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{n}}" class="latex" /> but not exactly. The <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> function gives it <em>exactly</em>.</p>
<p>
</p><p></p><h2> An Inferior But Curious Estimate </h2><p></p>
<p></p><p>
The norm of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> independent Gaussians is called the <a href="https://en.wikipedia.org/wiki/Chi_distribution">chi distribution</a>. Its square is the better-known <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>. This idea is used in the statistical <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared</a> test, but what follows is simpler.</p>
<p>
We let <img src="https://s0.wp.com/latex.php?latex=%7BX%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X^2}" class="latex" /> stand for the square norm divided by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> stands for the Euclidean norm divided by <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{n}}" class="latex" />. From (<a href="https://rjlipton.wpcomstaging.com/feed/#gammahalf">2</a>) we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BX%5D+%3D+%5Csqrt%7B%5Cfrac%7B2%7D%7Bn%7D%7D%5CGamma_%7B1%2F2%7D%28%5Cfrac%7Bn%7D%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  E[X] = \sqrt{\frac{2}{n}}\Gamma_{1/2}(\frac{n}{2}). " class="latex" /></p>
<p>
We will estimate <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> a different way and use that to estimate <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}}" class="latex" />. First we note that since the vector entries <img src="https://s0.wp.com/latex.php?latex=%7Bz_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_i}" class="latex" /> are independent and normally distributed, we have the exact values</p>
<p><br /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++E%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+E%5Bz_i%5E2%5D+%3D+1%5C%5C+Var%5BX%5E2%5D+%26%3D%26+%5Cfrac%7B1%7D%7Bn%5E2%7D+%5Csum_%7Bi%3D1%7D%5En+Var%5Bz_i%5E2%5D+%3D+%5Cfrac%7B1%7D%7Bn%5E2%7D%5Csum_%7Bi%3D1%7D%5En+%28E%5Bz_i%5E4%5D+-+E%5Bz_i%5D%5E2%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%283+-+1%29+%3D+%5Cfrac%7B2%7D%7Bn%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  E[X^2] &amp;=&amp; \frac{1}{n}\sum_{i=1}^n E[z_i^2] = 1\\ Var[X^2] &amp;=&amp; \frac{1}{n^2} \sum_{i=1}^n Var[z_i^2] = \frac{1}{n^2}\sum_{i=1}^n (E[z_i^4] - E[z_i]^2) = \frac{1}{n}(3 - 1) = \frac{2}{n}. \end{array} " class="latex" /></p>
<p></p><p><br />
Since we have <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X^2]}" class="latex" />, computing either <img src="https://s0.wp.com/latex.php?latex=%7BE%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{E[X]}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> suffices to get the other, by the relation <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+E%5BX%5E2%5D+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = E[X^2] - E[X]^2}" class="latex" />. Our also having <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5E2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X^2]}" class="latex" /> enables estimating <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X]}" class="latex" /> via the <a href="http://www.phidot.org/software/mark/docs/book/pdf/app_2.pdf">delta</a> <a href="https://en.wikipedia.org/wiki/Delta_method">method</a>, in a particular form I noticed <a href="https://stats.stackexchange.com/questions/10337/operatornamevarx2-if-operatornamevarx-sigma2/383603">here</a>. The derivation requires no special properties of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />: <a name="est2"></a></p><a name="est2">
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Var%5BX%5E2%5D+%5Capprox+4E%5BX%5D%5E2+Var%5BX%5D+-+Var%5BX%5D%5E2.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Var[X^2] \approx 4E[X]^2 Var[X] - Var[X]^2. \ \ \ \ \ (3)" class="latex" /></p>
</a><p><a name="est2"></a></p><p><br />
For our particular <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BVar%5BX%5D+%3D+1+-+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Var[X] = 1 - E[X]^2}" class="latex" />, this yields a quadratic equation in <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+E%5BX%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y = E[X]^2}" class="latex" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D+%5Cdoteq+4y%281+-+y%29+-+%281-y%29%5E2%2C+%5Cquad%5Ctext%7Bso%7D%5Cquad+5y%5E2+-+6y+%2B+1+%2B+%5Cfrac%7B2%7D%7Bn%7D+%3D+0+%5Cquad%5Ctext%7Bso%7D%5Cquad+y+%3D+%5Cfrac%7B6+%2B+%5Csqrt%7B16+-+%5Cfrac%7B40%7D%7Bn%7D%7D%7D%7B10%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n} \doteq 4y(1 - y) - (1-y)^2, \quad\text{so}\quad 5y^2 - 6y + 1 + \frac{2}{n} = 0 \quad\text{so}\quad y = \frac{6 + \sqrt{16 - \frac{40}{n}}}{10}. " class="latex" /></p>
<p>This yields </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2%7D%7Bn%7D%5CGamma_%7B1%2F2%7D%5E2%28%5Cfrac%7Bn%7D%7B2%7D%29+%5Cdoteq+%5Cfrac%7B3%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4%7D%7B25%7D+-+%5Cfrac%7B2%7D%7B5n%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{2}{n}\Gamma_{1/2}^2(\frac{n}{2}) \doteq \frac{3}{5} + \sqrt{\frac{4}{25} - \frac{2}{5n}}. " class="latex" /></p>
<p>
Changing variables to <img src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+%5Cfrac%7Bn%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z = \frac{n}{2}}" class="latex" /> and rearranging, we get the estimate </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma_%7B1%2F2%7D%28z%29+%5Cdoteq+%5Csqrt%7B%5Cfrac%7B3z%7D%7B5%7D+%2B+%5Csqrt%7B%5Cfrac%7B4z%5E2%7D%7B25%7D+-+%5Cfrac%7Bz%7D%7B5%7D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Gamma_{1/2}(z) \doteq \sqrt{\frac{3z}{5} + \sqrt{\frac{4z^2}{25} - \frac{z}{5}}}. " class="latex" /></p>
<p>
It has been traditional to estimate what we would call <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma_%7B1%2F2%7D%28z%2B%5Cfrac%7B1%7D%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma_{1/2}(z+\frac{1}{2})}" class="latex" /> instead, so putting <img src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+z+%2B+%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x = z + \frac{1}{2}}" class="latex" /> we finally get:</p>
<p>
<a name="estimate"></a></p><a name="estimate">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%5CGamma%28x%2B1%29%7D%7B%5CGamma%28x%2B%5Cfrac%7B1%7D%7B2%7D%29%7D+%5Csim+%5Csqrt%7B0.6x+%2B+0.3+%2B+0.2%5Csqrt%7B8x%5E2+-+2x+-+1.5%7D%7D+%7E.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{\Gamma(x+1)}{\Gamma(x+\frac{1}{2})} \sim \sqrt{0.6x + 0.3 + 0.2\sqrt{8x^2 - 2x - 1.5}} ~. \ \ \ \ \ (4)" class="latex" /></p>
</a><p><a name="estimate"></a></p>
<p>
As an estimate, this is barely competitive with the simple <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bx+%2B+0.25%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{x + 0.25}}" class="latex" /> and far inferior to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%5E2+%2B+0.5x+%2B+0.125%29%5E%7B1%2F4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  (x^2 + 0.5x + 0.125)^{1/4}, " class="latex" /></p>
<p>which is the first of several estimates of the form <img src="https://s0.wp.com/latex.php?latex=%7Bp_k%28x%29%5E%7B1%2F2k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p_k(x)^{1/2k}}" class="latex" /> <a href="https://www.sciencedirect.com/science/article/pii/S0895717710001317">given</a> by Cristinel Mortici in 2010. But it is curious that we got a formula with nested radicals and non-dyadic coefficients from a simple statistical estimate. It makes us wonder whether formulas with nested radicals can be tuned for greater accuracy, and whether this might knock back to statistical estimation.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can vectors of Gaussian variables be leveraged to say further interesting things about the gamma function and its applications? What are your favorite properties of the gamma function?</p>
<p>
[fixed missing “ds” in intro, typo n–&gt;sqrt(n) at end of sentence with “divided by”]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/10/19/some-statistical-gamma-fun/"><span class="datestr">at October 19, 2021 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/145">TR21-145 |  Revisiting a Lower Bound on the Redundancy of Linear Batch Codes | 

	Omar Alrabiah, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent work of Li and Wootters (2021) shows a redundancy lower bound of $\Omega(\sqrt{Nk})$ for systematic linear $k$-batch codes of block length $N$ by looking at the $O(k)$ tensor power of the dual code. In this note, we present an alternate proof of their result via a linear independence argument on a collection of polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/145"><span class="datestr">at October 19, 2021 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=102">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/">Thursday Oct 21st — Maxim Raginsky from UIUC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, Oct</strong> <strong>21</strong>st at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong><a href="http://maxim.ece.illinois.edu/" target="_blank" rel="noreferrer noopener">Maxim Raginsky </a></strong>from<strong> University of Illinois, Urbana-Champaign</strong> will speak about “Neural SDEs: Deep Generative Models in the Diffusion Limit”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: In deep generative models, the latent variable is generated by a time-inhomogeneous Markov chain, where at each time step we pass the current state through a parametric nonlinear map, such as a feedforward neural net, and add a small independent Gaussian perturbation. In this talk, based on joint work with Belinda Tzen, I will discuss the diffusion limit of such models, where we increase the number of layers while sending the step size and the noise variance to zero. I will first provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. Then I will show how we can quantify the expressiveness of diffusion-based generative models. Specifically, I will prove that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback-Leibler divergence to the target distribution. Finally, I will briefly discuss a scheme for unbiased, finite-variance simulation in such models. This scheme can be implemented as a deep generative model with a random number of layers.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/10/19/thursday-oct-21st-maxim-raginsky-from-uiuc/"><span class="datestr">at October 19, 2021 02:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/">Theory Group Postdoc at UC Berkeley (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Postdoc inquiries will be viewable by all our group’s faculty. Individual faculty may then reach out in the case of matched interests. Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line.</p>
<p>Website: <a href="http://theory.cs.berkeley.edu/">http://theory.cs.berkeley.edu/</a><br />
Email: tcs-postdoc-inquiries@lists.eecs.berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/18/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2021/"><span class="datestr">at October 18, 2021 09:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
