<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 10, 2021 05:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/">Online Optimization Post 7: Matrix Multiplicative Weights Update</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first two posts were about the technique of <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Multiplicative Weights Updates</a> and its application to <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">“derandomizing” probabilistic arguments</a> based on combining a Chernoff bound and a union bound. The third and fourth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Follow-the-Regularized-Leader</a> framework, which unifies multiplicative weights and gradient descent, and a <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">“gradient descent view” of the Frieze-Kannan Weak Regularity Lemma</a>. The fifth and sixth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">constrained version of the Follow-the-Regularized-Leader</a> framework, and the <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Impagliazzo Hard-Core Set Lemma</a>. Today we shall see the technique of Matrix Multiplicative Weights Updates.</p>
<p><b>1. Matrix Multiplicative Weights Update </b></p>
<p>In this post we consider the following generalization, introduced and studied by <a href="https://dl.acm.org/doi/10.1145/1250790.1250823">Arora and Kale</a>, of the “learning from expert advice” setting and the multiplicative weights update method. In the “experts” model, we have a repeated game in which, at each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, we have the option of following the advice of one of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> experts; if we follow the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, we incur a loss of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_t (i)}" class="latex" />, which is unknown to us (although, at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> we know the loss functions <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%28%5Ccdot%29%2C%5Cldots%2C%5Cell_%7Bt-1%7D%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_1(\cdot),\ldots,\ell_{t-1}(\cdot)}" class="latex" />). We are allowed to choose a probabilistic strategy, whereby we follow the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_t(i)}" class="latex" />, so that our expected loss at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sum_{i=1}^n x_t(i) \ell_t(i)}" class="latex" />.</p>
<p>In the matrix version, instead of choosing an expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> we are allowed to choose a unit <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-dimensional vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t}" class="latex" />, and the loss incurred in choosing the vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bv_t+%5ET+L_t+v_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t ^T L_t v_t}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> is an unknown symmetric <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n\times n}" class="latex" /> matrix. We are also allowed to choose a probabilistic strategy, so that with probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_t(j)}" class="latex" /> we choose the unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t^{(j)}}" class="latex" />, and we incur the expected loss</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_j+x_t+%28j%29+%5Ccdot+%28v_t%5E%7B%28j%29%7D%29%5ET+L_t+v_t%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_j x_t (j) \cdot (v_t^{(j)})^T L_t v_t^{(j)} " class="latex" /></p>
<p><span id="more-4588"></span></p>
<p>The above expression can also be written as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_t+%5Cbullet+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_t \bullet L_t " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7BX_t+%3D+%5Csum_j+x_t%28j%29+v_t%5E%7B%28j%29%7D%28v_t%5E%7B%28j%29%7D%29%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t = \sum_j x_t(j) v_t^{(j)}(v_t^{(j)})^T}" class="latex" /> and we used the Frobenius inner product among square matrices defined as <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A \bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" />. The matrices <img src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t}" class="latex" /> that can be obtained as convex combinations of rank-1 matrices of the form <img src="https://s0.wp.com/latex.php?latex=%7Bvv%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{vv^T}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> is a unit vector are called <em>density matrices</em> and can be characterized as the set of positive semidefinite matrices whose trace is 1.</p>
<p>It is possible to see the above game as the “quantum version” of the experts settings. A choice of a unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_t^{(j)}}" class="latex" /> is a <em>pure quantum state</em>, a probability distribution of pure quantum states, described by a density matrix, is a <em>mixed quantum state</em>. If <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is a density matrix describing a mixed quantum state, <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> is a symmetric matrix, and <img src="https://s0.wp.com/latex.php?latex=%7BL+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L = \sum_i \lambda_i w_i w_i^T}" class="latex" /> is the spectral decomposition of <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L}" class="latex" /> in terms of its eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> and orthonormal eigenvectors <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Cbullet+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \bullet L}" class="latex" /> is the expected outcome of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> in the basis <img src="https://s0.wp.com/latex.php?latex=%7Bw_1%2C%5Cldots%2Cw_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_1,\ldots,w_n}" class="latex" />, and such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> is the value of the measurement if the outcome is <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" />.</p>
<p>If you have no idea what the above paragraph means, that is perfectly ok because this view will not be particularly helpful in motivating the algorithm and analysis that we will describe. (Here I am reminded of the joke about the way people from Naples give directions: “How do I get to the post office?”, “Well, you see that road over there? After the a couple of blocks there is a pharmacy, where my uncle used to work, though now he is retired.” “Ok?” “Now, if you turn left after the pharmacy, after a while you get to a square with a big fountain and the church of St. Anthony where my niece got married. It was a beautiful ceremony, but the food at the reception was not great.” “Yes, I know that square”, “Good, don’t go there, the post office is not that way. Now, if you instead take that other road over there …”)</p>
<p>The main point of the above game, and of the Matrix Multiplicative Weights Update (MMWU) algorithm that plays it with bounded regret, is that it provides useful generalizations of the standard “experts” game and of the Multiplicative Weights Update (MWU) algorithm. For example, as we have already seen, MWU can provide a “derandomization” of the Chernoff bound; we will see that MMWU provides a derandomization of the <em>matrix</em> Chernoff bound. MWU can be used to approximate certain Linear Programming problems; MMWU can be used to approximate certain <em>Semidefinite Programming</em> problems.</p>
<p>To define and analyze the MMWU algorithm, we need to introduce certain operations on matrices. We will always work with real-valued symmetric matrices, but everything generalizes to complex-valued Hermitian matrices. If <img src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M = \sum_i \lambda_i w_i w_i^T}" class="latex" /> is a symmetric matrix, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" /> are the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w_i}" class="latex" /> are corresponding orthonormal eigenvectors, then we will define a number of operations and functions on <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> that operate on the eigenvalues while leaving the eigenvectors unchanged.</p>
<p>The first operation is <em>matrix exponentiation</em>: we define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3A%3D+%5Csum_i+e%5E%7B%5Clambda_i%7D+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X := \sum_i e^{\lambda_i} w_i w_i^T " class="latex" /></p>
<p> The operation always defines a positive definite matrix, and the resulting matrix satisfies a “Taylor expansion”</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3D+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cfrac1+%7Bk%21%7D+X%5Ek+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X = \sum_{k=0}^\infty \frac1 {k!} X^k " class="latex" /></p>
<p> Indeed, it is more common to use the above expansion as the definition of the matrix exponential, and then derive the expression in terms of eigenvalues.</p>
<p>We also have the useful bounds</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Csucceq+I+%2B+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X \succeq I + X" class="latex" /></p>
<p> which is true for every <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Cpreceq+I+%2B+X+%2BX%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  e^X \preceq I + X +X^2 " class="latex" /></p>
<p> which is true for all <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \preceq I}" class="latex" />.</p>
<p>Analogously, if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is positive definite, we can define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+X+%3A%3D+%5Csum_i+%28%5Clog+%5Clambda_i%29+%5Ccdot+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \log X := \sum_i (\log \lambda_i) \cdot w_i w_i^T " class="latex" /></p>
<p>and we have a number of identities like <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+e%5EX+%3D+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log e^X = X}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+X%5Ek+%3D+k+%5Clog+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log X^k = k \log X}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bk+X%7D+%3D+e%5Ek+%5Ccdot+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^{k X} = e^k \cdot X}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> is a scalar. We should be careful, however, not to take the analogy with real numbers too far: for example, if <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> are two symmetric matrices, in general it is not trues that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7BA%2BB%7D+%3D+e%5EA+%5Ccdot+e%5EB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^{A+B} = e^A \cdot e^B}" class="latex" />, in fact the above expression is actually always false except when <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> commute, in which case it is trivially true. We have, however, the following extremely useful fact.</p>
<blockquote><p><b>Theorem 1 (Golden-Thompson Inequality)</b> <em> </em></p>
<p><em></em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7BA%2BB%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5EA+%5Ccdot+e%5EB%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  {\rm tr}(e^{A+B}) \leq {\rm tr}(e^A \cdot e^B) " class="latex" /></p>
</em><p><em></em><em> </em></p></blockquote>
<p>The Golden-Thompson inequality will be all we need to generalize to this matrix setting everything we have proved about multiplicative weights. See <a href="https://terrytao.wordpress.com/2010/07/15/the-golden-thompson-inequality/">this post by Terry Tao</a> for a proof.</p>
<p>The <em>Von Neumann entropy</em> of a density matrix <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> with eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Ccdots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_1,\cdots,\lambda_n}" class="latex" /> is defined as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X%29+%3D+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+1+%7B%5Clambda_i%7D+%3D+-+%7B%5Crm+tr%7D%28X%5Clog+X%29+%3D+-+X+%5Cbullet+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  S(X) = \sum_i \lambda_i \log \frac 1 {\lambda_i} = - {\rm tr}(X\log X) = - X \bullet \log X " class="latex" /></p>
<p> that is, if we view <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" /> as the mixed quantum state in which the pure state <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v_i}" class="latex" /> has probability <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_i}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S(X)}" class="latex" /> is the entropy of the distribution over the pure states. Again, this is not a particularly helpful point of view, and in fact we will be interested in defining <img src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S(X)}" class="latex" /> not just for density matrices <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> but for arbitrary positive definite matrices, and even positive semidefinite (with the convention that <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Clog+0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 \log 0 = 0}" class="latex" />, which is used also in the standard definition of entropy of a distribution).</p>
<p>We will be interested in using Von Neumann entropy as a regularizer, and hence we will want to know what is its Bregman divergence. Some calculations show that the Bregman divergence of the Von Neumann entropy, which is called the quantum relative entropy, is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X_1%7C%7C+X_2%29+%3D+%7B%5Crm+tr%7D+%28X_1+%5Ccdot+%28+%5Clog+X_1+-+%5Clog+X_2%29%29+%2B+%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  S(X_1|| X_2) = {\rm tr} (X_1 \cdot ( \log X_1 - \log X_2)) + {\rm tr}(X_2) - {\rm tr}(X_1) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+X_1+%5Cbullet%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+I+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = X_1 \bullet(\log X_1 - \log X_2) + I \bullet (X_2 - X_1) " class="latex" /></p>
<p> If <img src="https://s0.wp.com/latex.php?latex=%7BX_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_1}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_2}" class="latex" /> are density matrices, the terms <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\rm tr}(X_2) - {\rm tr}(X_1)}" class="latex" /> cancel out; the above definition is valid for arbitrary positive definite matrices.</p>
<p>We will have to study the minima of various functions that take a matrix as an input, so it is good to understand how to compute the gradient of such functions. For example what is the gradient of the function <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\rm tr}(X)}" class="latex" />? Working through the definition we see that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+%7B%5Crm+tr%7D%28X%29+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\nabla {\rm tr}(X) = I}" class="latex" />, and indeed we always have that the gradient of the function <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+A%5Cbullet+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \rightarrow A\bullet X}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> everywhere. Somewhat less obvious is the calculation of the gradient of the Von Neumann entropy, which is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%28X+%5Cbullet+%5Clog+X%29+%3D+I+%2B+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \nabla (X \bullet \log X) = I + \log X " class="latex" /></p>
<p><b>2. Analysis in the Constrained FTRL Framework </b></p>
<p>Suppose that we play that we described above using agile mirror descent and using negative Von Neumann entropy (appropriately scaled) as a regularizer. That is, for some <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> that we will choose later, we use the regularizer</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28X%29+%3D+c+X+%5Cbullet+%5Clog+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  R(X) = c X \bullet \log X" class="latex" /></p>
<p> which has the Bregman divergence</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_1%2CX_2%29+%3D+c+S%28X_1+%7C%7C+X_2%29+%3D+c+X_1+%5Cbullet+%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+cI+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_1,X_2) = c S(X_1 || X_2) = c X_1 \bullet (\log X_1 - \log X_2) + cI \bullet (X_2 - X_1) " class="latex" /></p>
<p> and our feasible set is the set of density matrices</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CDelta+%3A%3D+%5C%7B+X%5Cin+%7B%5Cmathbb+R%7D%5E%7Bn%5Ctimes+n%7D+%3A+X+%5Csucceq+%7B%5Cbf+0%7D+%5Cwedge+%7B%5Crm+tr%7D%28X%29+%3D+1+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Delta := \{ X\in {\mathbb R}^{n\times n} : X \succeq {\bf 0} \wedge {\rm tr}(X) = 1 \} " class="latex" /></p>
<p> To bound the regret, we just have to plug the above definitions into the machinery that we developed <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">in our fifth post</a>.</p>
<p>At time 1, we play the identity matrix scaled by n, which is a density matrix of maximum Von Neumann entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log n}" class="latex" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_1+%3A%3D+%5Carg%5Cmin_%7BX+%5Cin+%5CDelta%7D+R%28X%29+%3D+%5Cfrac+1n+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_1 := \arg\min_{X \in \Delta} R(X) = \frac 1n I " class="latex" /></p>
<p> At time <img src="https://s0.wp.com/latex.php?latex=%7Bt%2B1%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t+1\geq 2}" class="latex" />, we play the matrix <img src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_t}" class="latex" /> obtained as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%7D+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \hat X_{t+1} = \arg\min_{X} D(X,X_{t}) + X\bullet L_{t} " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%5Cin+%5CDelta%7D+D%28X%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  X_{t+1} = \arg\min_{X\in \Delta} D(X,\hat X_{t+1}) " class="latex" /></p>
<p> and recall that we proved that, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> steps,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T%28X%29+%5Cleq+D%28X%2CX_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28X_t%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T(X) \leq D(X,X_1) + \sum_{t=1}^T D(X_t,\hat X_{t+1}) " class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> is a density matrix with eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Cldots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_1,\ldots,\lambda_n}" class="latex" />, then the first term is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D+%5Cleft%28+X%2C+%5Cfrac+1n+I+%5Cright%29+%3D+c+X+%5Cbullet+%28%5Clog+X+-+%5Clog+n%5E%7B-1%7D+I%29+%3D+c+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+n%7B%5Clambda_i%7D+%3D+c+%5Clog+n+-+c+%5Csum_i+%5Clambda_i+%5Cfrac+1+%7B%5Clambda_i%7D+%5Cleq+c%5Clog+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D \left( X, \frac 1n I \right) = c X \bullet (\log X - \log n^{-1} I) = c \sum_i \lambda_i \log \frac n{\lambda_i} = c \log n - c \sum_i \lambda_i \frac 1 {\lambda_i} \leq c\log n " class="latex" /></p>
<p> To complete the analysis we have to understand <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\hat X_{t+1}}" class="latex" />. We need to compute the gradient <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \rightarrow D(X,X_{t}) + X\bullet L_{t} }" class="latex" /> and set it to zero. The gradient of <img src="https://s0.wp.com/latex.php?latex=%7BX%5Cbullet+L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X\bullet L_t}" class="latex" /> is just <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" />. The gradient of <img src="https://s0.wp.com/latex.php?latex=%7BD%28X%2CX_%7Bt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D(X,X_{t})}" class="latex" /> is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+D%28X%2CX_t%29+%3D+c+%5Cnabla+X+%5Cbullet+%5Clog+X+-+c+%5Cnabla+X+%5Cbullet+%5Clog+X_t+-+%5Cnabla+c+X+%5Cbullet+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \nabla D(X,X_t) = c \nabla X \bullet \log X - c \nabla X \bullet \log X_t - \nabla c X \bullet I " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+cI+%2B+c+%5Clog+X+-+c+%5Clog+X_t+-+cI+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = cI + c \log X - c \log X_t - cI " class="latex" /></p>
<p> Meaning that we want to solve for</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%5Clog+X+-+c%5Clog+X_t+%2B+L_t+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  c \log X - c\log X_t + L_t = 0 " class="latex" /></p>
<p> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\hat X_{t+1}}" class="latex" /> satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D+%3D+%5Cfrac+1c+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \log X_t - \log \hat X_{t+1} = \frac 1c L_t " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \hat X_{t+1} = e^{\log X_t - \frac 1c L_t } " class="latex" /></p>
<p> and we can write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%3D+c+%5Ccdot+%5Cleft%28+X_t+%5Cbullet+%28%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D%29%5Cright%29+%2B+c+%7B%5Crm+tr%7D%28+%5Chat+X_%7Bt%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_t,\hat X_{t+1} ) = c \cdot \left( X_t \bullet (\log X_t - \log \hat X_{t+1})\right) + c {\rm tr}( \hat X_{t+1} )" class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%5Ccdot+X_t+%5Cbullet+%5Cfrac+1c+L_t+%2B+c+%5Ccdot+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%2B+c+%7B%5Crm+tr%7D+%5Chat+X_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = c \cdot X_t \bullet \frac 1c L_t + c \cdot {\rm tr}(e^{\log X_t - \frac 1c L_t}) + c {\rm tr} \hat X_{t+1} " class="latex" /></p>
<p> Then we can use Golden-Thompson and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E-%5Cfrac+1c+L_t+%5Cpreceq+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e^-\frac 1c L_t \preceq I - \frac 1c L_t + \frac 1{c^2} L^2_t}" class="latex" />, which holds if <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq cI}" class="latex" />, to write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t%7D+%5Ccdot+e%5E%7B-%5Cfrac+1c+L_t%7D+%29+%3D+X_t+%5Cbullet+e%5E%7B-%5Cfrac+1c+L_t%7D+%5Cleq+X_t+%5Cbullet+%5Cleft%28+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  {\rm tr}(e^{\log X_t - \frac 1c L_t}) \leq {\rm tr}(e^{\log X_t} \cdot e^{-\frac 1c L_t} ) = X_t \bullet e^{-\frac 1c L_t} \leq X_t \bullet \left( I - \frac 1c L_t + \frac 1{c^2} L^2_t \right) " class="latex" /></p>
<p> Combining everything together we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%5Cleq+%5Cfrac+1c+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  D(X_t,\hat X_{t+1} ) \leq \frac 1c X_t \bullet L_t^2 " class="latex" /></p>
<p> and so, provided <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_%7B%5Cmax%7D+%28L_t%29+%5Cleq+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_{\max} (L_t) \leq c}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> This is the best bound we can hope for, and it matches Theorem 1 in <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">our first post</a> about the Xultiplicative Weights Update algorithm.</p>
<p>If we have <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq I}" class="latex" />, we can simplify it to</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+T+c+%3D+2+%5Csqrt%7BT+%5Clog+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T \leq c \log n + \frac T c = 2 \sqrt{T \log n} " class="latex" /></p>
<p> where the last step comes from optimizing <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" />.</p>
<p>We can also write, under the condition <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+c+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq c I}" class="latex" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%28X%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+%28X_t+%5Cbullet+%7CL_t%7C+%29%7C%7CL_t%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Regret_T (X) \leq c \log n + \frac 1c \sum_{t=1}^T (X_t \bullet |L_t| )||L_t|| " class="latex" /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7B%7CL_t%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|L_t|}" class="latex" /> is the “absolute value” of the matrix <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> defined in the following way: if <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" /> is a symmetric matrix, then its absolute value is <img src="https://s0.wp.com/latex.php?latex=%7B%7CX%7C+%3D+%5Csum_i+%7C%5Clambda_i%7C+%5Ccdot+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|X| = \sum_i |\lambda_i| \cdot v_i v_i^T}" class="latex" />. Allen-Zhu, Liao and Orecchia state the analysis in this way in their <a href="https://arxiv.org/abs/1506.04838">on generalizations of Matrix Multiplicative Weights</a>.</p>
<p>Our next post will discuss applications at length, but for now let us gain a bit of intuition about the usefulness of these regret bounds. Recall that, for every symmetric matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_%7B%5Cmin%7D+%28M%29+%3D+%5Cmin_%7BX+%5Crm%5C+density%5C+matrix%7D+%5C+%5C+X+%5Cbullet+M+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lambda_{\min} (M) = \min_{X \rm\ density\ matrix} \ \ X \bullet M " class="latex" /></p>
<p> and so the regret bound can be reintepreted in the following way: if we let <img src="https://s0.wp.com/latex.php?latex=%7BL_1%2C%5Cldots%2CL_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_1,\ldots,L_T}" class="latex" /> be the loss functions used in a game played against a MMWU algorithm, and the algorithm selects density matrices <img src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cldots%2CX_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X_1,\ldots,X_T}" class="latex" />, then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Cmin_%7BX+%5Crm+%5C+density+%5C+matrix%7D+%5C+%5C+X+%5Cbullet+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \min_{X \rm \ density \ matrix} \ \ X \bullet \sum_{t=1}^T L_t \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> that is,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Clambda_%7B%5Cmin%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \lambda_{\min} \left( \sum_{t=1}^T L_t \right) \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" /></p>
<p> provided that <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \preceq cI}" class="latex" />. For example, switching <img src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7B-L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-L_t}" class="latex" />, we have <a name="main"></a></p>
<p><a name="main"></a></p><a name="main">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_%7B%5Cmax%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+%2B+c%5Clog+n+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle   \lambda_{\max} \left( \sum_{t=1}^T L_t \right) \leq \sum_{t=1}^T X_t \bullet L_t + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 + c\log n \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="main"></a><a name="main"></a> provided that <img src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Csucceq+-cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L_t \succeq -cI}" class="latex" />, which means that if we can choose a sequence of loss matrices that make the MMWU have small loss at each step, then we are guaranteed that the sum of such matrices cannot have any large eigenvalue.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/"><span class="datestr">at November 10, 2021 12:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/154">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/154">TR21-154 |  Explicit Binary Tree Codes with Sub-Logarithmic Size Alphabet | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Since they were first introduced by Schulman (STOC 1993), the construction of tree codes remained an elusive open problem. The state-of-the-art construction by Cohen, Haeupler and Schulman (STOC 2018) has constant distance and $(\log n)^{e}$ colors for some constant $e &gt; 1$ that depends on the distance, where $n$ is the depth of the tree. Insisting on a constant number of colors at the expense of having vanishing distance, Gelles, Haeupler, Kol, Ron-Zewi, and Wigderson (SODA 2016) constructed a distance $\Omega(\frac1{\log n})$ tree code.

In this work we improve upon these prior works and construct a distance-$\delta$ tree code with $(\log{n})^{O(\sqrt{\delta})}$ colors. This is the first construction of a constant distance tree code with sub-logarithmic number of colors. Moreover, as a direct corollary we obtain a tree code with a constant number of colors and distance $\Omega\left(\frac1{(\log\log{n})^{2}}\right)$, exponentially improving upon the above-mentioned work by Gelles et al.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/154"><span class="datestr">at November 10, 2021 11:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04332">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04332">Succinct Data Structure for Path Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balakrishnan:Girish.html">Girish Balakrishnan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Sankardeep.html">Sankardeep Chakraborty</a>, N S Narayanaswamy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadakane:Kunihiko.html">Kunihiko Sadakane</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04332">PDF</a><br /><b>Abstract: </b>We consider the problem of designing a succinct data structure for path
graphs (which are a proper subclass of chordal graphs and a proper superclass
of interval graphs) with $n$ vertices while supporting degree, adjacency, and
neighborhood queries efficiently. We provide two solutions for this problem.
Our first data structure is succinct and occupies $n \log n+o(n \log n)$ bits
while answering adjacency query in $O(\log n)$ time, and neighborhood and
degree queries in $O(d \log^2 n)$ time where $d$ is the degree of the queried
vertex. Our second data structure answers adjacency queries faster at the
expense of slightly more space. More specifically, we provide an $O(n \log^2
n)$ bit data structure that supports adjacency query in $O(1)$ time, and the
neighborhood query in $O(d \log n)$ time where $d$ is the degree of the queried
vertex. Central to our data structures is the usage of the classical heavy path
decomposition, followed by a careful bookkeeping using an orthogonal range
search data structure among others, which maybe of independent interest for
designing succinct data structures for other graphs. It is the use of the
results of Acan et al. in the second data structure that permits a simple and
efficient implementation at the expense of more space.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04332"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04295">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04295">The Hardness Analysis of Thompson Sampling for Combinatorial Semi-bandits with Greedy Oracle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kong:Fang.html">Fang Kong</a>, Yueran Yang, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wei.html">Wei Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shuai.html">Shuai Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04295">PDF</a><br /><b>Abstract: </b>Thompson sampling (TS) has attracted a lot of interest in the bandit area. It
was introduced in the 1930s but has not been theoretically proven until recent
years. All of its analysis in the combinatorial multi-armed bandit (CMAB)
setting requires an exact oracle to provide optimal solutions with any input.
However, such an oracle is usually not feasible since many combinatorial
optimization problems are NP-hard and only approximation oracles are available.
An example (Wang and Chen, 2018) has shown the failure of TS to learn with an
approximation oracle. However, this oracle is uncommon and is designed only for
a specific problem instance. It is still an open question whether the
convergence analysis of TS can be extended beyond the exact oracle in CMAB. In
this paper, we study this question under the greedy oracle, which is a common
(approximation) oracle with theoretical guarantees to solve many (offline)
combinatorial optimization problems. We provide a problem-dependent regret
lower bound of order $\Omega(\log T/\Delta^2)$ to quantify the hardness of TS
to solve CMAB problems with greedy oracle, where $T$ is the time horizon and
$\Delta$ is some reward gap. We also provide an almost matching regret upper
bound. These are the first theoretical results for TS to solve CMAB with a
common approximation oracle and break the misconception that TS cannot work
with approximation oracles.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04295"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04265">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04265">Adaptive area-preserving parameterization of open and closed anatomical surfaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choi:Gary_P=_T=.html">Gary P. T. Choi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giri:Amita.html">Amita Giri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Lalan.html">Lalan Kumar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04265">PDF</a><br /><b>Abstract: </b>The parameterization of open and closed anatomical surfaces is of fundamental
importance in many biomedical applications. Spherical harmonics, a set of basis
functions defined on the unit sphere, are widely used for anatomical shape
description. However, establishing a one-to-one correspondence between the
object surface and the entire unit sphere may induce a large geometric
distortion in case the shape of the surface is too different from a perfect
sphere. In this work, we propose adaptive area-preserving parameterization
methods for simply-connected open and closed surfaces with the target of the
parameterization being a spherical cap. Our methods optimize the shape of the
parameter domain along with the mapping from the object surface to the
parameter domain. The object surface will be globally mapped to an optimal
spherical cap region of the unit sphere in an area-preserving manner while also
exhibiting low conformal distortion. We further develop a set of spherical
harmonics-like basis functions defined over the adaptive spherical cap domain,
which we call the adaptive harmonics. Experimental results show that the
proposed parameterization methods outperform the existing methods for both open
and closed anatomical surfaces in terms of area and angle distortion. Surface
description of the object surfaces can be effectively achieved using a novel
combination of the adaptive parameterization and the adaptive harmonics. Our
work provides a novel way of mapping anatomical surfaces with improved accuracy
and greater flexibility. More broadly, the idea of using an adaptive parameter
domain allows easy handling of a wide range of biomedical shapes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04265"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04182">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04182">Parallel Nearest Neighbors in Low Dimensions with Batch Updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dobson:Magdalen.html">Magdalen Dobson</a>, Guy Blelloch <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04182">PDF</a><br /><b>Abstract: </b>We present a set of parallel algorithms for computing exact k-nearest
neighbors in low dimensions. Many k-nearest neighbor algorithms use either a
kd-tree or the Morton ordering of the point set; our algorithms combine these
approaches using a data structure we call the \textit{zd-tree}. We show that
this combination is both theoretically efficient under common assumptions, and
fast in practice. For point sets of size $n$ with bounded expansion constant
and bounded ratio, the zd-tree can be built in $O(n)$ work with
$O(n^{\epsilon})$ span for constant $\epsilon&lt;1$, and searching for the
$k$-nearest neighbors of a point takes expected $O(k\log k)$ time. We benchmark
our k-nearest neighbor algorithms against existing parallel k-nearest neighbor
algorithms, showing that our implementations are generally faster than the
state of the art as well as achieving 75x speedup on 144 hyperthreads.
Furthermore, the zd-tree supports parallel batch-dynamic insertions and
deletions; to our knowledge, it is the first k-nearest neighbor data structure
to support such updates. On point sets with bounded expansion constant and
bounded ratio, a batch-dynamic update of size $k$ requires $O(k \log n/k)$ work
with $O(k^{\epsilon} + \text{polylog}(n))$ span.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04182"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04181">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04181">Interactive Error Correcting Codes Over Binary Erasure Channels Resilient to $&gt;\frac12$ Adversarial Corruption</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Meghal.html">Meghal Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalai:Yael_Tauman.html">Yael Tauman Kalai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Rachel.html">Rachel Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04181">PDF</a><br /><b>Abstract: </b>An error correcting code ($\mathsf{ECC}$) allows a sender to send a message
to a receiver such that even if a constant fraction of the communicated bits
are corrupted, the receiver can still learn the message correctly. Due to their
importance and fundamental nature, $\mathsf{ECC}$s have been extensively
studied, one of the main goals being to maximize the fraction of errors that
the $\mathsf{ECC}$ is resilient to.
</p>
<p>For adversarial erasure errors (over a binary channel) the maximal error
resilience of an $\mathsf{ECC}$ is $\frac12$ of the communicated bits. In this
work, we break this $\frac12$ barrier by introducing the notion of an
interactive error correcting code ($\mathsf{iECC}$) and constructing an
$\mathsf{iECC}$ that is resilient to adversarial erasure of $\frac35$ of the
total communicated bits. We emphasize that the adversary can corrupt both the
sending party and the receiving party, and that both parties' rounds contribute
to the adversary's budget.
</p>
<p>We also prove an impossibility (upper) bound of $\frac23$ on the maximal
resilience of any binary $\mathsf{iECC}$ to adversarial erasures. In the bit
flip setting, we prove an impossibility bound of $\frac27$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04181"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04114">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04114">Formal Barriers to Simple Algorithms for the Matroid Secretary Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bahrani:Maryam.html">Maryam Bahrani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beyhaghi:Hedyeh.html">Hedyeh Beyhaghi</a>, Sahil Singla, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinberg:S=_Matthew.html">S. Matthew Weinberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04114">PDF</a><br /><b>Abstract: </b>Babaioff et al. [BIK2007] introduced the matroid secretary problem in 2007, a
natural extension of the classic single-choice secretary problem to matroids,
and conjectured that a constant-competitive online algorithm exists. The
conjecture still remains open despite substantial partial progress, including
constant-competitive algorithms for numerous special cases of matroids, and an
$O(\log \log \text{rank})$-competitive algorithm in the general case.
</p>
<p>Many of these algorithms follow principled frameworks. The limits of these
frameworks are previously unstudied, and prior work establishes only that a
handful of particular algorithms cannot resolve the matroid secretary
conjecture. We initiate the study of impossibility results for frameworks to
resolve this conjecture. We establish impossibility results for a natural class
of greedy algorithms and for randomized partition algorithms, both of which
contain known algorithms that resolve special cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04114"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04089">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04089">Sampling from Log-Concave Distributions with Infinity-Distance Guarantees and Applications to Differentially Private Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mangoubi:Oren.html">Oren Mangoubi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vishnoi:Nisheeth_K=.html">Nisheeth K. Vishnoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04089">PDF</a><br /><b>Abstract: </b>For a $d$-dimensional log-concave distribution $\pi(\theta)\propto
e^{-f(\theta)}$ on a polytope $K$, we consider the problem of outputting
samples from a distribution $\nu$ which is $O(\varepsilon)$-close in
infinity-distance $\sup_{\theta\in K}|\log\frac{\nu(\theta)}{\pi(\theta)}|$ to
$\pi$. Such samplers with infinity-distance guarantees are specifically desired
for differentially private optimization as traditional sampling algorithms
which come with total-variation distance or KL divergence bounds are
insufficient to guarantee differential privacy. Our main result is an algorithm
that outputs a point from a distribution $O(\varepsilon)$-close to $\pi$ in
infinity-distance and requires
$O((md+dL^2R^2)\times(LR+d\log(\frac{Rd+LRd}{\varepsilon r}))\times
md^{\omega-1})$ arithmetic operations, where $f$ is $L$-Lipschitz, $K$ is
defined by $m$ inequalities, is contained in a ball of radius $R$ and contains
a ball of smaller radius $r$, and $\omega$ is the matrix-multiplication
constant. In particular this runtime is logarithmic in $\frac{1}{\varepsilon}$
and significantly improves on prior works. Technically, we depart from the
prior works that construct Markov chains on a
$\frac{1}{\varepsilon^2}$-discretization of $K$ to achieve a sample with
$O(\varepsilon)$ infinity-distance error, and present a method to convert
continuous samples from $K$ with total-variation bounds to samples with
infinity bounds. To achieve improved dependence on $d$, we present a
"soft-threshold" version of the Dikin walk which may be of independent
interest. Plugging our algorithm into the framework of the exponential
mechanism yields similar improvements in the running time of $\varepsilon$-pure
differentially private algorithms for optimization problems such as empirical
risk minimization of Lipschitz-convex functions and low-rank approximation,
while still achieving the tightest known utility bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04089"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04053">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04053">Registration Techniques for Deformable Objects</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmadi:Alireza.html">Alireza Ahmadi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04053">PDF</a><br /><b>Abstract: </b>In general, the problem of non-rigid registration is about matching two
different scans of a dynamic object taken at two different points in time.
These scans can undergo both rigid motions and non-rigid deformations. Since
new parts of the model may come into view and other parts get occluded in
between two scans, the region of overlap is a subset of both scans. In the most
general setting, no prior template shape is given and no markers or explicit
feature point correspondences are available. So, this case is a partial
matching problem that takes into account the assumption that consequent scans
undergo small deformations while having a significant amount of overlapping
area [28]. The problem which this thesis is addressing is mapping deforming
objects and localizing cameras in the environment at the same time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04053"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04044">Simple Parallel Algorithms for Single-Site Dynamics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Hongyang.html">Hongyang Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yitong.html">Yitong Yin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04044">PDF</a><br /><b>Abstract: </b>The single-site dynamics are a canonical class of Markov chains for sampling
from high-dimensional probability distributions, e.g. the ones represented by
graphical models.
</p>
<p>We give a simple and generic parallel algorithm that can faithfully simulate
single-site dynamics. When the chain asymptotically satisfies the
$\ell_p$-Dobrushin's condition, specifically, when the Dobrushin's influence
matrix has constantly bounded $\ell_p$-induced operator norm for an arbitrary
$p\in[1,\infty]$, the parallel simulation of $N$ steps of single-site updates
succeeds within $O\left({N}/{n}+\log n\right)$ depth of parallel computing
using $\tilde{O}(m)$ processors, where $n$ is the number of sites and $m$ is
the size of graphical model. Since the Dobrushin's condition is almost always
satisfied asymptotically by mixing chains, this parallel simulation algorithm
essentially transforms single-site dynamics with optimal $O(n\log n)$ mixing
time to RNC algorithms for sampling. In particular we obtain RNC samplers, for
the Ising models on general graphs in the uniqueness regime, and for satisfying
solutions of CNF formulas in a local lemma regime. With non-adaptive simulated
annealing, these RNC samplers can be transformed routinely to RNC algorithms
for approximate counting.
</p>
<p>A key step in our parallel simulation algorithm, is a so-called "universal
coupling" procedure, which tries to simultaneously couple all distributions
over the same sample space. We construct such a universal coupling, that for
every pair of distributions the coupled probability is at least their Jaccard
similarity. We also prove this is optimal in the worst case. The universal
coupling and its applications are of independent interests.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04044"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03953">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03953">Frequency Estimation with One-Sided Error</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Shyam.html">Shyam Narayanan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03953">PDF</a><br /><b>Abstract: </b>Frequency estimation is one of the most fundamental problems in streaming
algorithms. Given a stream $S$ of elements from some universe $U=\{1 \ldots
n\}$, the goal is to compute, in a single pass, a short sketch of $S$ so that
for any element $i \in U$, one can estimate the number $x_i$ of times $i$
occurs in $S$ based on the sketch alone. Two state of the art solutions to this
problems are the Count-Min and Count-Sketch algorithms. The frequency estimator
$\tilde{x}$ produced by Count-Min, using $O(1/\varepsilon \cdot \log n)$
dimensions, guarantees that $\|\tilde{x}-x\|_{\infty} \le \varepsilon \|x\|_1$
with high probability, and $\tilde{x} \ge x$ holds deterministically. Also,
Count-Min works under the assumption that $x \ge 0$. On the other hand,
Count-Sketch, using $O(1/\varepsilon^2 \cdot \log n)$ dimensions, guarantees
that $\|\tilde{x}-x\|_{\infty} \le \varepsilon \|x\|_2$ with high probability.
A natural question is whether it is possible to design the best of both worlds
sketching method, with error guarantees depending on the $\ell_2$ norm and
space comparable to Count-Sketch, but (like Count-Min) also has the
no-underestimation property.
</p>
<p>Our main set of results shows that the answer to the above question is
negative. We show this in two incomparable computational models: linear
sketching and streaming algorithms. We also study the complementary problem,
where the sketch is required to not over-estimate, i.e., $\tilde{x} \le x$
should hold always.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03953"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03744">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03744">Hopcroft's Problem, Log-Star Shaving, 2D Fractional Cascading, and Decision Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, Da Wei Zheng <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03744">PDF</a><br /><b>Abstract: </b>We revisit Hopcroft's problem and related fundamental problems about
geometric range searching. Given $n$ points and $n$ lines in the plane, we show
how to count the number of point-line incidence pairs or the number of
point-above-line pairs in $O(n^{4/3})$ time, which matches the conjectured
lower bound and improves the best previous time bound of
$n^{4/3}2^{O(\log^*n)}$ obtained almost 30 years ago by Matou\v{s}ek.
</p>
<p>We describe two interesting and different ways to achieve the result: the
first is randomized and uses a new 2D version of fractional cascading for
arrangements of lines; the second is deterministic and uses decision trees in a
manner inspired by the sorting technique of Fredman (1976). The second approach
extends to any constant dimension.
</p>
<p>Many consequences follow from these new ideas: for example, we obtain an
$O(n^{4/3})$-time algorithm for line segment intersection counting in the
plane, $O(n^{4/3})$-time randomized algorithms for bichromatic closest pair and
Euclidean minimum spanning tree in three or four dimensions, and a randomized
data structure for halfplane range counting in the plane with $O(n^{4/3})$
preprocessing time and space and $O(n^{1/3})$ query time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03744"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03735">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03735">A PTAS for Capacitated Vehicle Routing on Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathieu:Claire.html">Claire Mathieu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Hang.html">Hang Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03735">PDF</a><br /><b>Abstract: </b>We give a polynomial time approximation scheme (PTAS) for the unit demand
capacitated vehicle routing problem (CVRP) on trees, for the entire range of
the tour capacity. The result extends to the splittable CVRP.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03735"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03694">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03694">Metric Distortion Bounds for Randomized Social Choice</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramakrishnan:Prasanna.html">Prasanna Ramakrishnan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03694">PDF</a><br /><b>Abstract: </b>Consider the following social choice problem. Suppose we have a set of $n$
voters and $m$ candidates that lie in a metric space. The goal is to design a
mechanism to choose a candidate whose average distance to the voters is as
small as possible. However, the mechanism does not get direct access to the
metric space. Instead, it gets each voter's ordinal ranking of the candidates
by distance. Given only this partial information, what is the smallest
worst-case approximation ratio (known as the distortion) that a mechanism can
guarantee?
</p>
<p>A simple example shows that no deterministic mechanism can guarantee
distortion better than $3$, and no randomized mechanism can guarantee
distortion better than $2$. It has been conjectured that both of these lower
bounds are optimal, and recently, Gkatzelis, Halpern, and Shah proved this
conjecture for deterministic mechanisms. We disprove the conjecture for
randomized mechanisms for $m \geq 3$ by constructing elections for which no
randomized mechanism can guarantee distortion better than $2.0261$ for $m = 3$,
$2.0496$ for $m = 4$, up to $2.1126$ as $m \to \infty$. We obtain our lower
bounds by identifying a class of simple metrics that appear to capture much of
the hardness of the problem, and we show that any randomized mechanism must
have high distortion on one of these metrics. We provide a nearly matching
upper bound for this restricted class of metrics as well. Finally, we
conjecture that these bounds give the optimal distortion for every $m$, and
provide a proof for $m = 3$, thereby resolving that case.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03694"><span class="datestr">at November 10, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/">Christopher Strachey Professorship of Computing at University of Oxford (apply by February 28, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Strachey Professorship is the oldest chair in the Department of Computer Science, and is named for Christopher Strachey, who founded Oxford’s Programming Research Group in 1965. The Department seeks an internationally recognised research leader who will further the academic and strategic development of the department.</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/1988-full.html">http://www.cs.ox.ac.uk/news/1988-full.html</a><br />
Email: head-of-dept@cs.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/"><span class="datestr">at November 09, 2021 06:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/neurips2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/neurips2021/">Conference Digest - NeurIPS 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The accepted papers for <a href="https://neurips.cc/Conferences/2020">NeurIPS 2021</a> were recently announced, and there’s a huge amount of differential privacy content. 
We found one relevant workshop and 48 papers.
This is up from 31 papers last year, an over 50% increase!
It looks like there’s huge growth in interest on differentially private machine learning.
Impressively, at the time of this writing, all but five papers are already posted on arXiv!
For the full list of accepted papers, see <a href="https://neurips.cc/Conferences/2021/AcceptedPapersInitial">here</a>.
Please let us know if we missed relevant papers on differential privacy!</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li><a href="https://priml2021.github.io/">Privacy in Machine Learning (PriML) 2021</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2103.08721">A Central Limit Theorem for Differentially Private Query Answering</a><br />
Jinshuo Dong, Weijie Su, Linjun Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02391">Adapting to function difficulty and growth conditions in private optimization</a><br />
Hilal Asi, Daniel Levy, John Duchi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.04378">Adaptive Machine Unlearning</a><br />
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, Chris Waites</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.13239">An Uncertainty Principle is a Price of Privacy-Preserving Microdata</a><br />
John Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Daniel Kifer, Philip Leclerc, William Sexton, Ashley Simpson, Christine Task, Pavel Zhuravlev</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03408">Antipodes of Label Differential Privacy: PATE and ALIBI</a><br />
Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, Florian Tramer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13329">Covariance-Aware Private Mean Estimation Without Private Covariance Estimation</a><br />
Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06062">Deep Learning with Label Differential Privacy</a><br />
Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.05855">Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent</a><br />
Rishav Chourasia, Jiayuan Ye, Reza Shokri</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02674">Differentially Private Empirical Risk Minimization under the Fairness Lens</a><br />
Cuong Tran, My Dinh, Ferdinando Fioretto</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.14153">Differentially Private Federated Bayesian Optimization with Distributed Exploration</a><br />
Zhongxiang Dai, Bryan Kian Hsiang Low, Patrick Jaillet</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.03871">Differentially Private Learning with Adaptive Clipping</a><br />
Galen Andrew, Om Thakkar, Swaroop Ramaswamy, Brendan McMahan</p>
  </li>
  <li>
    <p>Differentially Private Model Personalization<br />
Prateek Jain, John Rush, Adam Smith, Shuang Song, Abhradeep Guha Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02900">Differentially Private Multi-Armed Bandits in the Shuffle Model</a><br />
Jay Tenenbaum, Haim Kaplan, Yishay Mansour, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02831">Differentially Private n-gram Extraction</a><br />
Kunho Kim, Sivakanth Gopi, Janardhan Kulkarni, Sergey Yekhanin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02516">Differential Privacy Over Riemannian Manifolds</a><br />
Matthew Reimherr, Karthik Bharath, Carlos Soto</p>
  </li>
  <li>
    <p>Differentially Private Sampling from Distributions<br />
Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith, Marika Swanberg</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.05585">Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings</a><br />
Raef Bassily, Cristóbal Guzmán, Michael Menart</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.01177">Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence</a><br />
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09063">Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization</a><br />
Pranav Subramani, Nicholas Vadivelu, Gautam Kamath</p>
  </li>
  <li>
    <p>Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms<br />
Jeremy Seeman, Matthew Reimherr, Aleksandra Slavković</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.03013">Fast and Memory Efficient Differentially Private-SGD via JL Projections</a><br />
Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Hanwen Shen, Uthaipon Tantipongpipat</p>
  </li>
  <li>
    <p>G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators<br />
Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, Bo Li</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03365">Generalized Linear Bandits with Local Differential Privacy</a><br />
Yuxuan Han, Zhipeng Liang, Yang Wang, Jiheng Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11193">Individual Privacy Accounting via a Rényi Filter</a><br />
Vitaly Feldman, Tijana Zrnic</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.00979">Information-constrained optimization: can adaptive processing of gradients help?</a><br />
Jayadev Acharya, Clement Canonne, Prathamesh Mayekar, Himanshu Tyagi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00463">Instance-optimal Mean Estimation Under Differential Privacy</a><br />
Ziyue Huang, Yuting Liang, Ke Yi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.07153">Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods</a><br />
Terrance Liu, Giuseppe Vietri, Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11845">Learning with User-Level Privacy</a><br />
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13513">Littlestone Classes are Privately Online Learnable</a><br />
Noah Golowich, Roi Livni</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.07778">Local Differential Privacy for Regret Minimization in Reinforcement Learning</a><br />
Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, Matteo Pirotta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.03940">Locally differentially private estimation of functionals of discrete distributions</a><br />
Cristina Butucea, Yann Issartel</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.10675">Locally private online change point detection</a><br />
Tom Berrett, Yi Yu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.10870">Multiclass versus Binary Differentially Private PAC Learning</a><br />
Satchit Sivakumar, Mark Bun, Marco Gaboardi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02848">Numerical Composition of Differential Privacy</a><br />
Sivakanth Gopi, Yin Tat Lee, Lukas Wutschitz</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11526">On the Sample Complexity of Privately Learning Axis-Aligned Rectangles</a><br />
Menachem Sadigurschi, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03645">Photonic Differential Privacy with Direct Feedback Alignment</a><br />
Ruben Ohana, Hamlet Medina, Julien Launay, Alessandro Cappelli, Iacopo Poli, Liva Ralaivola, Alain Rakotomamonjy</p>
  </li>
  <li>
    <p>Private and Non-private Uniformity Testing for Ranking Data<br />
Róbert Busa-Fekete, Dimitris Fotakis, Emmanouil Zampetakis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.07171">Private learning implies quantum stability</a><br />
Yihui Quek, Srinivasan Arunachalam, John A Smolin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.15352">Private Non-smooth ERM and SCO in Subquadratic Steps</a><br />
Janardhan Kulkarni, Yin Tat Lee, Daogao Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02162">Privately Learning Mixtures of Axis-Aligned Gaussians</a><br />
Ishaq Aden-Ali, Hassan Ashtiani, Christopher Liaw</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00001">Privately Learning Subspaces</a><br />
Vikrant Singhal, Thomas Steinke</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02281">Privately Publishable Per-instance Privacy</a><br />
Rachel Redberg, Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2109.06153">Relaxed Marginal Consistency for Differentially Private Query Answering</a><br />
Ryan McKenna, Siddhant Pradhan, Daniel Sheldon, Gerome Miklau</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.03279">Remember What You Want to Forget: Algorithms for Machine Unlearning</a><br />
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.08763">Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning</a><br />
Antonious Girgis, Deepesh Data, Suhas Diggavi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.09159">Robust and differentially private mean estimation</a><br />
Xiyang Liu, Weihao Kong, Sham Kakade, Sewoong Oh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.04995">The Skellam Mechanism for Differentially Private Federated Learning</a><br />
Naman Agarwal, Peter Kairouz, Ken Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.11208">User-Level Differentially Private Learning via Correlated Sampling</a><br />
Badih Ghazi, Ravi Kumar, Pasin Manurangsi</p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/neurips2021/"><span class="datestr">at November 09, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/">Postdoc positions in quantum algorithms at IRIF, CNRS (Paris) (apply by December 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IRIF (Paris, France) is offering multiple postdoc positions to work on the theory of quantum computing. Emphasis is on the development of quantum algorithms for optimization, machine learning, massive data, and cryptography. You will be working with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez, and a strong team of postdocs and PhDs. Starting date: around September 2022.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc#quantum-computing">https://www.irif.fr/postes/postdoc#quantum-computing</a><br />
Email: simon.apers@inria.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/"><span class="datestr">at November 09, 2021 02:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/153">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/153">TR21-153 |  On Hardness Assumptions Needed for &amp;quot;Extreme High-End&amp;quot; PRGs and Fast Derandomization | 

	Ronen Shaltiel, 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The hardness vs.~randomness paradigm aims to explicitly construct pseudorandom generators $G:\{0,1\}^r \to \{0,1\}^m$ that fool circuits of size $m$, assuming the existence of explicit hard functions. A ``high-end PRG'' with seed length $r=O(\log m)$ (implying BPP=P) was achieved in a seminal work of Impagliazzo and Wigderson (STOC 1997), assuming \textsc{the high-end hardness assumption}: there exist constants $0&lt;\beta &lt; 1&lt; B$, and functions computable in time $2^{B \cdot n}$ that cannot be computed by circuits of size $2^{\beta \cdot n}$.

Recently, motivated by fast derandomization of randomized algorithms, Doron et al.~(FOCS 2020) and Chen and Tell (STOC 2021), construct ``extreme high-end PRGs'' with seed length $r=(1+o(1))\cdot \log m$, under qualitatively stronger assumptions.

We study whether extreme high-end PRGs can be constructed from the corresponding hardness assumption in which $\beta=1-o(1)$ and $B=1+o(1)$, which we call \textsc{the extreme high-end hardness assumption}. We give a partial negative answer:

\begin{itemize}
\item The construction of Doron et al. composes a PEG (pseudo-entropy generator) with an extractor.  The PEG is constructed starting from a function that is hard for MA-type circuits.  We show that black-box PEG constructions from \textsc{the extreme high-end hardness assumption} must have large seed length (and so cannot be used to obtain extreme high-end PRGs by applying an extractor).

To prove this, we establish a new property of (general) black-box PRG constructions from hard functions: it is possible to fix many output bits of the construction while fixing few bits of the hard function. This property distinguishes PRG constructions from typical extractor constructions, and this may explain why it is difficult to design PRG constructions.

\item The construction of Chen and Tell composes two PRGs: $G_1:\{0,1\}^{(1+o(1)) \cdot \log m} \to \{0,1\}^{r_2=m^{\Omega(1)}}$ and $G_2:\{0,1\}^{r_2} \to \{0,1\}^m$.  The first PRG is constructed from \textsc{the extreme high-end hardness assumption}, and the second PRG needs to run in time $m^{1+o(1)}$, and is constructed assuming one way functions. We show that in black-box proofs of hardness amplification to $\frac{1}{2}+1/m$, reductions must make $\Omega(m)$ queries, even in the extreme high-end. Known PRG constructions from hard functions are black-box and use (or imply) hardness amplification, and so cannot be used to construct a PRG $G_2$ from \textsc{the extreme high-end hardness assumption}.

The new feature of our hardness amplification result is that it applies even to the extreme high-end setting of parameters, whereas past work does not. Our techniques also improve recent lower bounds of Ron-Zewi, Shaltiel and Varma (ITCS 2021) on the number of queries of local list-decoding algorithms.
\end{itemize}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/153"><span class="datestr">at November 09, 2021 11:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/152">TR21-152 |  Min-Entropic Optimality | 

	Tomer Grossman, 

	Gal Arnon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the notion of \emph{Min-Entropic Optimality} thereby providing a framework for arguing that a given algorithm computes a function better than any other algorithm. An algorithm is $k(n)$ Min-Entropic Optimal if for every distribution $D$ with min-entropy at least $k(n)$, its expected running time when its input is drawn from $D$ is at most a multiplicative constant larger than the expected running time (also with respect to $D$) of any other algorithm that computes the same function. Min-Entropic Optimality is a relaxation of the well established notion of instance optimality (when $k(n) = 0$). Thereby, Min-Entropic Optimality provides a meaningful notion of optimality, even in scenarios where instance optimality is inherently impossible to achieve (for instance, in the super-linear regime).

We analyze basic properties of this notion and prove that for many values of $k(n)$ there exist functions that have Min-Entropic Optimal algorithms. We further show that some natural search problems, such as $k$-sum, are unlikely to have optimal algorithms under this notion.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/152"><span class="datestr">at November 09, 2021 11:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04254">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04254">Alternating Automatic Register Machines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Ziyuan.html">Ziyuan Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Sanjay.html">Sanjay Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zeyong.html">Zeyong Li</a>, Ammar Fathin Sabili, Frank Stephan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04254">PDF</a><br /><b>Abstract: </b>This paper introduces and studies a new model of computation called an
Alternating Automatic Register Machine (AARM). An AARM possesses the basic
features of a conventional register machine and an alternating Turing machine,
but can carry out computations using bounded automatic relations in a single
step. One surprising finding is that an AARM can recognise some NP-complete
problems, including 3SAT (using a particular coding), in O(log^*n) steps. We do
not yet know if every NP-complete problem can be recognised by some AARM in
O(log^*n) steps.
</p>
<p>Furthermore, we study an even more computationally powerful machine, called a
Polynomial-Size Padded Alternating Automatic Register Machine (PAARM), which
allows the input to be padded with a polynomial-size string. It is shown that
each language in the polynomial hierarchy can be recognised by a PAARM in
O(log^*n) steps, while every language recognised by a PAARM in O(log^*(n))
steps belongs to PSPACE. These results illustrate the power of alternation when
combined with computations involving automatic relations, and uncover a finer
gradation between known complexity classes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04254"><span class="datestr">at November 09, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04066">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04066">Fast sampling via spectral independence beyond bounded-degree graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bez=aacute=kov=aacute=:Ivona.html">Ivona Bezáková</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galanis:Andreas.html">Andreas Galanis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldberg:Leslie_Ann.html">Leslie Ann Goldberg</a>, Daniel Štefankovič <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04066">PDF</a><br /><b>Abstract: </b>Spectral independence is a recently-developed framework for obtaining sharp
bounds on the convergence time of the classical Glauber dynamics. This new
framework has yielded optimal $O(n \log n)$ sampling algorithms on
bounded-degree graphs for a large class of problems throughout the so-called
uniqueness regime, including, for example, the problems of sampling independent
sets, matchings, and Ising-model configurations.
</p>
<p>Our main contribution is to relax the bounded-degree assumption that has so
far been important in establishing and applying spectral independence. Previous
methods for avoiding degree bounds rely on using $L^p$-norms to analyse
contraction on graphs with bounded connective constant (Sinclair, Srivastava,
Yin; FOCS'13). The non-linearity of $L^p$-norms is an obstacle to applying
these results to bound spectral independence. Our solution is to capture the
$L^p$-analysis recursively by amortising over the subtrees of the recurrence
used to analyse contraction. Our method generalises previous analyses that
applied only to bounded-degree graphs.
</p>
<p>As a main application of our techniques, we consider the random graph
$G(n,d/n)$, where the previously known algorithms run in time $n^{O(\log d)}$
or applied only to large $d$. We refine these algorithmic bounds significantly,
and develop fast $n^{1+o(1)}$ algorithms based on Glauber dynamics that apply
to all $d$, throughout the uniqueness regime.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04066"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.04011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.04011">$k$ disjoint $st$-paths activation in polynomial time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nutov:Zeev.html">Zeev Nutov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04011">PDF</a><br /><b>Abstract: </b>In activation network design problems we are given an undirected graph
$G=(V,E)$ and a pair of activation costs $\{c_e^u,c_e^v\}$ for each $e=uv \in
E$. The goal is to find an edge set $F \subseteq E$ that satisfies a prescribed
property of minimum activation cost $\tau(F)=\sum_{v \in V} \max \{c_e^v: e \in
F \mbox{ is incident to } v\}$. In the Activation $k$ Disjoint Paths problem we
are given $s,t \in V$ and an integer $k$, and seek an edge set $F \subseteq E$
of $k$ internally disjoint $st$-paths of minimum activation cost. The problem
admits an easy $2$-approximation algorithm. However, it was an open question
whether the problem is in P even for $k=2$ and power activation costs, when
$c_e^u=c_e^v$ for all $e=uv \in E$. Here we will answer this question by giving
a polynomial time algorithm using linear programing. We will also mention
several consequences, among them a polynomial time algorithm for the Activation
2 Edge Disjoint Paths problem, and improved approximation ratios for the
Min-Power $k$-Connected Subgraph problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.04011"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03980">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03980">Dynamic Algorithms Against an Adaptive Adversary: Generic Constructions and Lower Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beimel:Amos.html">Amos Beimel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaplan:Haim.html">Haim Kaplan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mansour:Yishay.html">Yishay Mansour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nissim:Kobbi.html">Kobbi Nissim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stemmer:Uri.html">Uri Stemmer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03980">PDF</a><br /><b>Abstract: </b>A dynamic algorithm against an adaptive adversary is required to be correct
when the adversary chooses the next update after seeing the previous outputs of
the algorithm. We obtain faster dynamic algorithms against an adaptive
adversary and separation results between what is achievable in the oblivious
vs. adaptive settings. To get these results we exploit techniques from
differential privacy, cryptography, and adaptive data analysis.
</p>
<p>We give a general reduction transforming a dynamic algorithm against an
oblivious adversary to a dynamic algorithm robust against an adaptive
adversary. This reduction maintains several copies of the oblivious algorithm
and uses differential privacy to protect their random bits. Using this
reduction we obtain dynamic algorithms against an adaptive adversary with
improved update and query times for global minimum cut, all pairs distances,
and all pairs effective resistance.
</p>
<p>We further improve our update and query times by showing how to maintain a
sparsifier over an expander decomposition that can be refreshed fast. This fast
refresh enables it to be robust against what we call a blinking adversary that
can observe the output of the algorithm only following refreshes. We believe
that these techniques will prove useful for additional problems.
</p>
<p>On the flip side, we specify dynamic problems that, assuming a random oracle,
every dynamic algorithm that solves them against an adaptive adversary must be
polynomially slower than a rather straightforward dynamic algorithm that solves
them against an oblivious adversary. We first show a separation result for a
search problem and then show a separation result for an estimation problem. In
the latter case our separation result draws from lower bounds in adaptive data
analysis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03980"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03968">Improved Approximation Guarantees for Shortest Superstrings using Cycle Classification by Overlap to Length Ratios</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Englert:Matthias.html">Matthias Englert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matsakis:Nicolaos.html">Nicolaos Matsakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vesel=yacute=:Pavel.html">Pavel Veselý</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03968">PDF</a><br /><b>Abstract: </b>In the Shortest Superstring problem, we are given a set of strings and we are
asking for a common superstring, which has the minimum number of characters.
The Shortest Superstring problem is NP-hard and several constant-factor
approximation algorithms are known for it. Of particular interest is the GREEDY
algorithm, which repeatedly merges two strings of maximum overlap until a
single string remains. The GREEDY algorithm, being simpler than other
well-performing approximation algorithms for this problem, has attracted
attention since the 1980s and is commonly used in practical applications.
</p>
<p>Tarhio and Ukkonen (TCS 1988) conjectured that GREEDY gives a
2-approximation. In a seminal work, Blum, Jiang, Li, Tromp, and Yannakakis
(STOC 1991) proved that the superstring computed by GREEDY is a
4-approximation, and this upper bound was improved to 3.5 by Kaplan and Shafrir
(IPL 2005).
</p>
<p>We show that the approximation guarantee of GREEDY is at most
$(13+\sqrt{57})/6 \approx 3.425$, making the first progress on this question
since 2005. Furthermore, we prove that the Shortest Superstring can be
approximated within a factor of $(37+\sqrt{57})/18\approx 2.475$, improving
slightly upon the currently best $2\frac{11}{23}$-approximation algorithm by
Mucha (SODA 2013).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03968"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03898">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03898">Rapid mixing of the hardcore Glauber dynamics and other Markov chains in bounded-treewidth graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eppstein:David.html">David Eppstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frishberg:Daniel.html">Daniel Frishberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03898">PDF</a><br /><b>Abstract: </b>We give a new rapid mixing result for a natural random walk on the
independent sets of an input graph $G$. Rapid mixing is of interest in
approximately sampling a structure, over some underlying set or graph, from
some target distribution. In the case of independent sets, we show that when
$G$ has bounded treewidth, this random walk -- known as the hardcore Glauber
dynamics -- mixes rapidly for all values of the standard parameter $\lambda &gt;
0$, giving a simple alternative to existing sampling algorithms for these
structures.
</p>
<p>We also show rapid mixing for Markov chains on dominating sets and $b$-edge
covers (for fixed $b\geq 1$ and $\lambda &gt; 0$) in the case where treewidth is
bounded, and for Markov chains on the $b$-matchings (for fixed $b \geq 1$ and
$\lambda &gt; 0$), the maximal independent sets, and the maximal $b$-matchings of
a graph (for fixed $b \geq 1$), in the case where carving width is bounded.
</p>
<p>We prove our results by developing a divide-and-conquer framework using the
well-known multicommodity flows technique. Using this technique, we
additionally show that a similar dynamics on the $k$-angulations of a convex
set of $n$ points mixes in quasipolynomial time for all $k \geq 3$. (McShine
and Tetali gave a stronger result in the special case $k = 3$.)
</p>
<p>Our technique also allows us to strengthen existing results by Dyer,
Goldberg, and Jerrum and by Heinrich for the Glauber dynamics on the
$q$-colorings of $G$ on graphs of bounded carving width, when $q \geq \Delta +
2$ is bounded. Specifically, our technique yields an improvement in the
dependence on treewidth when $\Delta &lt; 2t$ or when $q &lt; 4t$ and $\Delta &lt; t^2$.
</p>
<p>We additionally show that the Glauber dynamics on the partial $q$-colorings
of $G$ mix rapidly for all $\lambda &gt; 0$ when $q \geq \Delta + 2$ is bounded.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03898"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03725">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03725">Algorithms and data structures for first-order logic with connectivity under vertex failures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Michał Pilipczuk, Nicole Schirrmacher, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Siebertz:Sebastian.html">Sebastian Siebertz</a>, Szymon Toruńczyk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vigny:Alexandre.html">Alexandre Vigny</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03725">PDF</a><br /><b>Abstract: </b>We introduce a new data structure for answering connectivity queries in
undirected graphs subject to batched vertex failures. Precisely, given any
graph G and integer k, we can in fixed-parameter time construct a data
structure that can later be used to answer queries of the form: ``are vertices
s and t connected via a path that avoids vertices $u_1,..., u_k$?'' in time
$2^{2^{O(k)}}$. In the terminology of the literature on data structures, this
gives the first deterministic data structure for connectivity under vertex
failures where for every fixed number of failures, all operations can be
performed in constant time.
</p>
<p>With the aim to understand the power and the limitations of our new
techniques, we prove an algorithmic meta theorem for the recently introduced
separator logic, which extends first-order logic with atoms for connectivity
under vertex failures. We prove that the model-checking problem for separator
logic is fixed-parameter tractable on every class of graphs that exclude a
fixed topological minor. We also show a weak converse. This implies that from
the point of view of parameterized complexity, under standard complexity
assumptions, the frontier of tractability of separator logic is almost exactly
delimited by classes excluding a fixed topological minor.
</p>
<p>The backbone of our proof relies on a decomposition theorem of Cygan et al.
[SICOMP '19], which provides a tree decomposition of a given graph into bags
that are unbreakable. Crucially, unbreakability allows to reduce separator
logic to plain first-order logic within each bag individually. We design our
model-checking algorithm using dynamic programming over the tree decomposition,
where the transition at each bag amounts to running a suitable model-checking
subprocedure for plain first-order logic. This approach is robust enough to
provide also efficient enumeration of queries expressed in separator logic.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03725"><span class="datestr">at November 09, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/">The Artificial Intelligence Historian</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>The past actually happened but history is only what someone wrote down—Whitney Brown</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/mccorduck-obit-700x700-min/" rel="attachment wp-att-19317"><img width="125" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/mccorduck-obit-700x700-min.jpeg?resize=125%2C156&amp;ssl=1" class="alignright wp-image-19317" height="156" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">CMU <a href="https://www.cmu.edu/news/stories/archives/2021/october/mccorduck-obit.html">tribute</a></font></td>
</tr>
</tbody>
</table>
<p>
Pamela McCorduck passed away last month. The New York Times <a href="https://www.nytimes.com/2021/11/04/technology/pamela-mccorduck-dead.html">obituary</a> notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. </p>
<p>
Today we remember her and talk about AI’s near future.</p>
<p>
I knew McCorduck through her late husband, Joe Traub, who we <a href="https://rjlipton.wpcomstaging.com/2015/08/31/how-joe-traub-beat-the-street/">memorialized</a> in 2015. He became the head of the CS department at Carnegie Mellon University in 1971. She also moved to CMU where she became an English teacher. Per the above quote by Brown, she helped make AI real by writing a number of books on its history. </p>
<p>
The NYT obit quotes something McCorduck wrote in her 2019 <a href="https://press.etc.cmu.edu/index.php/product/this-could-be-important/">memoir</a>, <em>This Could Be Important: My Life and Times With the Artificial Intelligentsia.</em> </p>
<blockquote><p><b> </b> <em> “For 60 years, I’ve lived in AI’s exponential. I’ve watched computers evolve from plodding sorcerer’s apprentices to machines that can best any humans at checkers, then chess, then the guessing game Jeopardy!, and now the deeply complex game of Go.” </em>
</p></blockquote>
<p></p><p>
It is hard to project the future of an exponential, however. The best way I can try is to align it with my own field.</p>
<p>
</p><p></p><h2> AI Movers and Shakers </h2><p></p>
<p></p><p>
At CMU McCorduck got to know the AI pioneers like Turing Award recipients Herbert Simon and Allen Newell and Raj Reddy. She already knew Edward Feigenbaum who said:</p>
<blockquote><p><b> </b> <em> She was dumped into this saturated milieu of the great and greatest in AI at Carnegie Mellon—some of the same people whose papers she’d helped us assemble—and decided to write a history of the field. </em>
</p></blockquote>
<p></p><p>
The <a href="https://www.routledge.com/Machines-Who-Think-A-Personal-Inquiry-into-the-History-and-Prospects-of/McCorduck/p/book/9781568812052">book</a> was <em>Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.</em> Said Simon: </p>
<blockquote><p><b> </b> <em> She was interacting with all the movers and shakers of AI. She was in the middle of it, an eyewitness to history. </em>
</p></blockquote>
<p></p><p>
I wish I knew more of her thoughts on the movers and shakers in Theory. She was well-versed in complexity of the dynamical-systems kind, and her husband’s work bridged to “our kind” of complexity. The title of her third <a href="https://www.amazon.com/Bounded-Rationality-Novel-Pamela-McCorduck/dp/0865348839">novel</a>, <em>Bounded Rationality</em>, speaks to both kinds of complexity from its setting at the Santa Fe Institute. This has led me to musing on the difference between AI and Theory.</p>
<p>
</p><p></p><h2> AI Beats Theory </h2><p></p>
<p></p><p>
I never have worked on AI problems of any kind. The closest I ever came is I took a class at CMU as graduate student from Newell. He was a fun lecturer and the class was interesting. But I always worked on Theory. I must reflect a bit on why AI is so successful and Theory is less so. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/see/" rel="attachment wp-att-19311"><img width="258" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/see.png?resize=258%2C195&amp;ssl=1" class="aligncenter size-full wp-image-19311" height="195" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pinterest <a href="https://www.pinterest.com/pin/55872851604309287/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Let’s start by saying that a field of research is determined not by who works in the field. Not by the tools that the field uses. It is determined by the problems that the field works on. AI is different from Theory because of the problems that it studies. This is the fundamental difference:</p>
<blockquote><p><b> </b> <em> AI looks at whole problems; Theory looks at sub-problems. </em>
</p></blockquote>
<p></p><p>
What do I mean? AI studies problems that are concrete, that are big, that are as close to real problems as possible. For example, how to play Go or how to recognize images of faces. AI looks at problems that humans actually wish to solve: </p>
<blockquote><p><b> </b> <em> What move to make in this Go position? Or is this an image of X or Y? </em>
</p></blockquote>
<p></p><p>
Theory looks at sub-problems. We look at a real problem and then identify some part of the problem that is hard to solve. We then try to invoke clever methods that show that this sub-problem can be done more efficiently that was previous known. This is hard in general. Is fun to work on in general. And leads to a beautiful field of study. One that is deep and rewarding. </p>
<p>
But Theory loses to AI. The issue is that no one really may wish to solve the sub-problem. This is real demand to solve the whole problem, but not the sub-problem. This is the fundamental advantage that AI holds over Theory.</p>
<p>
</p><p></p><h2> AI Future </h2><p></p>
<p></p><p>
Public figures such as Stephen Hawking and Elon Musk have expressed concern that full artificial intelligence (AI) could result in human extinction. The consequences of the technological <a href="https://en.wikipedia.org/wiki/Technological_singularity">singularity</a> and its potential benefit or harm to the human race have been intensely debated.</p>
<p>
For our own part, we have <a href="https://rjlipton.wpcomstaging.com/2011/02/17/are-mathematicians-in-jeopardy/">wondered</a> whether an AI can take over in theory research. This puts a second light on possible meanings of “problems” in another quotation by McCorduck from her memoir, as related <a href="https://computerhistory.org/blog/the-future-humans-and-ai/">here</a>:</p>
<blockquote><p><b> </b> <em> “We can’t now say what living beside other, in some ways superior, intelligences will mean to us. Will it widen and raise our own individual and collective intelligence? In significant ways, it already has. Find solutions to problems we could never solve? Probably. Find solutions to problems we lack the wit even to propose? Maybe. Cause problems? Surely. AI has already shattered some of our fondest myths about ourselves and has shone unwelcome light on others. This will continue.</em></p><em>
<p>
…</p>
</em><p><em>
When people ask me my greatest worry about AI, I say: what we aren’t smart enough even to imagine.” </em>
</p></blockquote>
<p></p><p>
Well, we can only talk about things we can imagine now. We can discuss facets of life that already outsource decisions to technology, such as high-speed stock trading and <a href="https://en.wikipedia.org/wiki/2010_flash_crash">several</a> <a href="https://www.technologyreview.com/2016/10/07/244656/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">flash</a>–<a href="https://www.motherjones.com/politics/2013/02/high-frequency-trading-danger-risk-wall-street/">crashes</a> it has caused. </p>
<p>
But looking ahead, what is one near-term application area as a litmus test for the impact of AI? We think many will agree with our looking to <em>self-driving cars</em>. In taking over driving decisions, the AI expressly aims to reduce the evils of impaired or aggressive drivers. There have been <a href="https://www.washingtonpost.com/technology/2021/11/08/tesla-regulation-elon-musk/">mishaps</a> during development, sure, and the algorithms have not yet demonstrated robustness against possible deceptions. That is to say:</p>
<ul>
<li>
We can already see teething problems with this tech and imagine more along the same lines. <p></p>
</li><li>
But can we project structural problems with the driverless paradigm whose concrete forms we have not imagined?
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your thoughts on the near future of AI? </p>
<p>
Our condolences go out to Pamela’s family and associates.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/"><span class="datestr">at November 08, 2021 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/151">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/151">TR21-151 |  Locally Testable Codes with constant rate, distance, and locality  | 

	Irit Dinur, 

	Shai Evra, 

	Ron Livne, 

	Alexander Lubotzky, 

	Shahar Mozes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A locally testable code (LTC) is an error correcting code that has a property-tester. The tester reads $q$ bits that are randomly chosen, and rejects words with probability proportional to their distance from the code. The parameter $q$ is called the locality of the tester.

LTCs were initially studied as important components of PCPs, and since then the topic has evolved on its own. High rate LTCs could be useful in practice: before attempting to decode a received word, one can save time by first quickly testing if it is close to the code.

An outstanding open question has been whether there exist "$c^3$-LTCs", namely LTCs with *c*onstant rate, *c*onstant distance, and *c*onstant locality.

In this work we construct such codes based on a new two-dimensional complex which we call a left-right Cayley complex. This is essentially a graph which, in addition to vertices and edges, also has squares. Our codes can be viewed as a two-dimensional version of (the one-dimensional) expander codes, where the codewords are functions on the squares rather than on the edges.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/151"><span class="datestr">at November 08, 2021 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8225">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/">New Quantum Science and Engineering PhD Program</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have <a href="https://quantum.harvard.edu/external-candidates">a postdoctoral fellowship in quantum computing</a>. –Boaz]</em></p>



<p>This year Harvard started a new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Ph.D program in Quantum Science and Engineering</a>. We are now accepting application for the first cohort of students which will start in the 2022-2023 academic year.  This program can be an excellent fit for CS majors that are interested in quantum computation and information, and are looking for an interdisciplinary environment, with students from CS, physics, and other backgrounds, and with a curriculum and program that is designed with quantum computing in mind. Students can apply to both the QSE program and the <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science">CS PhD program</a>, in parallel.</p>



<p>Some more information below:</p>



<p>The <strong><a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Harvard Quantum Science and Engineering (QSE) PhD program</a></strong> is designed for students like yours, as well as those studying engineering, physics and chemistry. The program brings these students from diverse undergraduate programs together through a world-class, integrated QSE PhD program that uniquely prepares them to become intellectual leaders and innovators in the burgeoning field of QSE. </p>



<p><strong>The curriculum. </strong>The integrated curriculum provides a shared foundation and QSE language that enables students to make discoveries and collaborate fluently beyond traditional disciplinary boundaries. Students enjoy the freedom to broadly explore their interests, specialize in their area of greatest interest, and can choose PhD advisors from across all quantum departments including computer science, physics, engineering, and chemistry.</p>



<p><strong>The community. </strong>The Harvard QSE PhD program is built around a supportive environment and collaborative research community that helps nurture each student and ensure their success. It’s an unprecedented opportunity to work with world leaders in the field of QSE in state-of-the-art educational and computational facilities while participating in cutting-edge research. </p>



<p><strong>The opportunity. </strong>Graduates of the program will be trained as the next generation of world leaders in the field of QSE. With their broad, yet deep educational foundation, guided by their own interests, they’ll be ready to take on exciting roles in industry, academia, and national laboratories. </p>



<p>Deadline to apply is <strong>December 15, 2021</strong>. Apply via <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering" target="_blank" rel="noreferrer noopener">https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering</a> . Students can also apply in parallel to the CS PhD program via <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science" target="_blank" rel="noreferrer noopener">https://gsas.harvard.edu/programs-of-study/all/computer-science</a> . Email <a href="mailto:qse-admissions@fas.harvard.edu" target="_blank" rel="noreferrer noopener">qse-admissions@fas.harvard.edu</a> with any questions.</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/"><span class="datestr">at November 08, 2021 03:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-6759625364649289630">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/11/postdoc-call-for-fodsi.html">Postdoc call for FODSI</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As a member of FODSI (Foundations of Data Science Institute -- an NSF funded institute with the aim of advancing theoretical foundations for data science), I'm self-interestedly posting the call for postdocs for this year.  Two of the areas are  a) Sketching, Sampling, and Sublinear-Time Algorithms   and b)  Machine Learning for Algorithms (which includes what I call "Algorithms with Predictions.")  I'd be happy to see postdoc applications in those areas from people who want to spend some time at Harvard, for example.... but of course there are lots of other exciting things going on with FODSI too and you should take a look.</p><p>The call is at <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a>  </p><p>Call text below:</p><table align="center" style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); color: #2a2a2a; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 13.3333px; padding: 5px; width: 95%px;" border="1" class="ads"><tbody><tr><td style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); padding: 5px;">The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. It provides a structured environment for exploring interdisciplinary research in foundations of Data Science spanning Mathematics, Statistics, Theoretical Computer Science and other fields.<p>We are looking for multiple postdoctoral team members who will collaborate with <a style="color: #005f6f; font-weight: 700;" href="https://fodsi.us/team.html">FODSI researchers</a> at one or more of the participating institutions. These positions emphasize strong mentorship, flexibility, and breadth of collaboration opportunities with other team members -- senior and junior faculty, postdocs, and graduate students at various nodes around the country. Furthermore, postdoctoral fellows will be able to participate in workshops and other activities organized by FODSI.</p><p>The fellowship is a one-year full-time appointment, with the possibility of renewal for a second year (based upon mutual agreement) either at the same or at another FODSI institution. The start date is flexible, although most appointments are expected to start in summer 2022. Candidates are encouraged to apply to work with more than one faculty mentor <b>at one or more participating institutions</b> (in-person mentoring is preferred, but remote options will be also considered). The applicants should have an excellent theoretical background and a doctorate in a related field, including Mathematics, Statistics, Computer Science, Electrical Engineering or Economics. We particularly encourage applications from women and minority candidates.</p><p>The review process will start on November 15, 2021 and will continue until positions are filled.</p></td></tr></tbody></table></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/11/postdoc-call-for-fodsi.html"><span class="datestr">at November 08, 2021 02:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6799651120968614396">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html">Reflections on Trusting ``Trustlessness'' in the era of ``Crypto'' Blockchains (Guest Post)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> </p><p><i>I trust Evangelos Georgiadis to do a guest post on Trust and Blockchain. </i></p><div>Today we have a guest post by Evangelos Georgiadis on Trust. It was written before Lance's post on trust <a href="https://blog.computationalcomplexity.org/2021/08/trusting-scientists.html">here</a> but it can be viewed as a followup to it. </div><div><br /></div><div>And now, here's E.G:</div><div><div><br /></div><div>==========================================================</div><div><br /></div><div>Trust is a funny concept, particularly in the realm of blockchains and "crypto".</div><div><br /></div><div>Do you trust the consensus mechanism of a public blockchain?</div><div><br /></div><div>Do you trust the architects that engineered the consensus mechanism?</div><div><br /></div><div>Do you trust the software engineers that implemented the code for the consensus mechanism?</div><div><br /></div><div>Do you trust the language that the software engineers used?</div><div><br /></div><div>Do you trust the underlying hardware that that the software is running?</div><div><br /></div><div>Theoretical Computer Science provides tools for some of this. But then the question becomes</div><div>Do you trust the program verifier?</div><div>Do you trust the proof of security?</div><div><br /></div><div>I touch on these issues in: </div><div><br /></div><div>                   <i>Reflections on Trusting ‘Trustlessness’ in the era of ”Crypto”/Blockchains</i></div><div><br /></div><div> which is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/cbit-4-2.pdf">here</a>. Its only 3 pages so enjoy!</div></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html"><span class="datestr">at November 07, 2021 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/150">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/150">TR21-150 |  Extractors: Low Entropy Requirements Colliding With Non-Malleability | 

	Eldon Chung, 

	Maciej Obremski, 

	Divesh Aggarwal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The known constructions of negligible error (non-malleable) two-source extractors can be broadly classified in three categories:

(1) Constructions where one source has min-entropy rate about $1/2$, the other source can have small min-entropy rate, but the extractor doesn't guarantee non-malleability.
(2) Constructions where one source is uniform, and the other can have small min-entropy rate, and the extractor guarantees non-malleability when the uniform source is tampered.
(3) Constructions where both sources have entropy rate very close to $1$ and the extractor guarantees non-malleability against the tampering of both sources. 

We introduce a new notion of collision resistant extractors and in using it we obtain a strong two source non-malleable extractor where we require the first source to have $0.8$ entropy rate and the other source can have min-entropy polylogarithmic in the length of the source.  

We show how the above extractor can be applied to obtain a non-malleable extractor with output rate $\frac 1 2$, which is optimal. We also show how, by using our extractor and extending the known protocol, one can  obtain a privacy amplification secure against memory tampering where the size of the secret output is almost optimal.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/150"><span class="datestr">at November 07, 2021 08:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/149">TR21-149 |  On polynomially many queries to NP or QMA oracles | 

	Dorian Rudolph, 

	Sevag Gharibian</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the complexity of problems solvable in deterministic polynomial time with access to an NP or Quantum Merlin-Arthur (QMA)-oracle, such as $P^{NP}$ and $P^{QMA}$, respectively.
The former allows one to classify problems more finely than the Polynomial-Time Hierarchy (PH), whereas the latter characterizes physically motivated problems such as Approximate Simulation (APX-SIM) [Ambainis, CCC 2014].
In this area, a central role has been played by the classes $P^{NP[\log]}$ and $P^{QMA[\log]}$, defined identically to $P^{NP}$ and $P^{QMA}$, except that only logarithmically many oracle queries are allowed. Here, [Gottlob, FOCS 1993] showed that if the adaptive queries made by a $P^{NP}$ machine have a "query graph" which is a tree, then this computation can be simulated in $P^{NP[\log]}$.

 In this work, we first show that for any verification class $C\in\{NP,MA,QCMA,QMA,QMA(2),NEXP,QMA_{\exp}\}$, any $P^C$ machine with a query graph of "separator number" $s$ can be simulated using deterministic time $\exp(s\log n)$ and $s\log n$ queries to a $C$-oracle.
When $s\in O(1)$ (which includes the case of $O(1)$-treewidth, and thus also of trees), this gives an upper bound of $P^{C[\log]}$, and when $s\in O(\log^k(n))$, this yields bound $QP^{C[\log^{k+1}]}$ (QP meaning quasi-polynomial time).
We next show how to combine Gottlob's "admissible-weighting function" framework with the "flag-qubit" framework of [Watson, Bausch, Gharibian, 2020], obtaining a unified approach for embedding $P^C$ computations directly into APX-SIM instances in a black-box fashion.
Finally, we formalize a simple no-go statement about polynomials (c.f. [Krentel, STOC 1986]): Given a multi-linear polynomial $p$ specified via an arithmetic circuit, if one can "weakly compress" $p$ so that its optimal value requires $m$ bits to represent, then $P^{NP}$ can be decided with only $m$ queries to an NP-oracle.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/149"><span class="datestr">at November 07, 2021 12:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">IDEAL mini-workshop on “New Directions on Robustness in ML”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
November 16, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-new-directions-on-robustness-in-ml/ As machine learning systems are being deployed in almost every aspect of decision-making, it is vital for them to be reliable and secure to adversarial corruptions and perturbations of various kinds. This workshop will explore newer notions of robustness and the different challenges that arise in designing reliable ML algorithms. … <a href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “New Directions on Robustness in ML”</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/"><span class="datestr">at November 07, 2021 04:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1584">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/">News for October 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled sub-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br />(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/"><span class="datestr">at November 07, 2021 02:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br />
Email: See the url</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/"><span class="datestr">at November 05, 2021 10:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br />
Email: bmkapron@uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/"><span class="datestr">at November 05, 2021 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">Assistant Professor at Charles University (apply by January 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered.</p>
<p>Website: <a href="https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022">https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/"><span class="datestr">at November 05, 2021 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">Summer Research Intern at Adobe Research  (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. Interested candidates should send their CV to Zhao.</p>
<p>Website: <a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en</a><br />
Email: zsong@adobe.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/"><span class="datestr">at November 05, 2021 04:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
