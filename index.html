<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 26, 2021 12:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9152128988613149804">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/ferrers-diagrams-can-be-used-to-prove-x.html">Ferrer's Diagrams can be used to prove X theorems about partitions. What is X?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>1978: I took an excellent  ugrad course in combinatorics from James C Frauenthal (he sometimes wrote his name as the biniomial cofficient (J choose F))  and he covered Ferrer's diagrams. They are a nice way to prove equalities about types of partitions.   See <a href="https://www.britannica.com/science/combinatorics/The-Ferrer-diagram">here</a> for a definition and a few examples. I have this (possibly false) memory that there were LOTS of partition theorems proven nicely with Ferrer's diagrams.</p><p>Fast forward to 2021:</p><p>2021: My TA Emily  needs a topic to cover in Honors Discrete Math. I have this memory that there were LOTS of theorems about partitions proven with Ferrer's diagrams. We look at many websites on Ferrer diagrams  and  find only TWO examples:</p><p>The numb of partitions of n into k parts is the numb of partitions of n into parts the largest of which is k.</p><p><br /></p><p>The numb of partitions of n into \le k parts is the numb of partitions of n into parts the largest of which is \le k</p><p>We DO find many theorems about partitions such as this corollary to the Rogers-Ramanujan theorem:</p><p>The numb of partitions of n such that adjacent parts differ by at least 2 is the numb of partitions of n such that each partition is either \equiv 1 mod 5 or \equiv 4 mod 5.</p><p>This is a HARD theorem and there is no Ferrer-diagram or other elementary proof. </p><p>SO, I have one MEMORY but the reality seems different. Possibilities:</p><p>1) My memory is wrong. There really are only 2 examples (or some very small number).</p><p>2) There are other examples but I can't find them on the web. I HOPE this is true--- if someone knows of other ways to use Ferrer diagrams to get partition results, please comment. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/ferrers-diagrams-can-be-used-to-prove-x.html"><span class="datestr">at April 26, 2021 02:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5481">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5481">The easiest exercise in the moral philosophy book</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Peter Singer, in the parable that came to represent his whole worldview and that of the effective altruism movement more generally, asked us to imagine that we could save a drowning child at the cost of jumping into a lake and ruining an expensive new suit.  Assuming we’d do that, he argued that we do in fact face an ethically equivalent choice; if we don’t donate most of our income to save children in the Third World, then we need to answer for why, as surely as the person who walked past the kid thrashing in the water.</p>



<p>In this post, I don’t want to take a position on Singer’s difficult but important hypothetical.  I merely want to say: suppose that to save the child, you didn’t even have to jump in the water.  Suppose you just had to toss a life preserver, one you weren’t using.  Or suppose you just had to assure the child that it was OK to grab your life raft that was already in the water.</p>



<p>That, it seems, is the situation that the US and other rich countries will increasingly face with covid vaccines.  What’s happening in India right now looks on track to become a humanitarian tragedy, if it isn’t already.  Even if, as Indian friends tell me, this was a staggering failure of the Modi government, people shouldn’t pay for it with their lives.  And we in the US now have tens of millions of vaccine doses sitting in warehouses unused, for regulatory and vaccine hesitancy reasons—stupidly, but we do.  We’re past the time, in my opinion, when it’s morally obligatory either to use the doses or to give them away.  Anyone in a position to manufacture more vaccines for distribution to poor countries, should also immediately get the intellectual property rights to do so.</p>



<p>I was glad to read, just this weekend, that the US is finally starting to move in the right direction.  I hope it moves faster.</p>



<p>And I’m sorry that this brief post doesn’t contain any information or insight that you can’t find elsewhere.  It just made me feel better to write it, is all.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5481"><span class="datestr">at April 25, 2021 08:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/060">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/060">TR21-060 |  Optimal Error Resilience of Adaptive Message Exchange | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the error resilience of the message exchange task: Two parties, each holding a private input, want to exchange their inputs. However, the channel connecting them is governed by an adversary that may corrupt a constant fraction of the transmissions. What is the maximum fraction of corruptions that still allows the parties to exchange their inputs? 

For the non-adaptive channel, where the parties must agree in advance on the order in which they communicate, the maximum error resilience was shown to be $\frac{1}{4}$ (see Braverman and Rao, STOC 2011).
The problem was also studied over the adaptive channel, where the order in which the parties communicate may not be predetermined (Ghaffari, Haeupler, and Sudan, STOC 2014; Efremenko, Kol, and Saxena, STOC 2020). These works show that the adaptive channel admits much richer set of protocols but leave open the question of finding its maximum error resilience.

In this work, we show that the maximum error resilience of a protocol for message exchange over the adaptive channel is $\frac{5}{16}$, thereby settling the above question. Our result requires improving both the known upper bounds and the known lower bounds for the problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/060"><span class="datestr">at April 25, 2021 01:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/059">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/059">TR21-059 |  On One-way Functions from NP-Complete Problems | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present the first natural $\NP$-complete problem whose average-case hardness w.r.t. the uniform distribution over instances implies the existence of one-way functions (OWF). In fact, we prove that the existence of OWFs is \emph{equivalent} to mild average-case hardness of this $\NP$-complete problem. The problem, which originated in the 1960s, is the \emph{Conditional Time-Bounded Kolmogorov Complexity Problem}: let $K^t(x \mid z)$ be the length of the shortest ``program'' that, given the ``auxiliary input'' $z$, outputs the string $x$ within time $t(|x|)$, and let $\mcktp[t,\zeta]$ be the set of strings $(x,z,k)$ where $|z| = \zeta(|x|)$, $|k| = \log |x|$ and $K^t(x \mid z)&lt; k$, where, for our purposes, a ``program'' is defined as a RAM machine.

Our main results shows that for every polynomial $t(n)\geq n^2$, there exists some polynomial $\zeta$ such that $\mcktp[t,\zeta]$ is $\NP$-complete. We additionally observe that the result of Liu-Pass (FOCS'20) extends to show that for every polynomial $t(n)\geq 1.1n$, and every polynomial $\zeta(\cdot)$, mild average-case hardness of $\mcktp[t,\zeta]$ is equivalent to the existence of OWFs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/059"><span class="datestr">at April 25, 2021 10:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/058">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/058">TR21-058 |  Average-Case Hardness of NP from Exponential Worst-Case Hardness Assumptions | 

	Shuichi Hirahara</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A long-standing and central open question in the theory of average-case complexity is to base average-case hardness of NP on worst-case hardness of NP.  A frontier question along this line is to prove that PH is hard on average if UP requires (sub-)exponential worst-case complexity.  The difficulty of resolving this question has been discussed from various perspectives based on technical barrier results, such as the limits of black-box reductions and the non-existence of worst-case hardness amplification procedures in PH.

In this paper, we overcome these barriers and resolve the open question by presenting the following main results:

1.  $UP \not\subseteq DTIME(2^{O(n / \log n)})$ implies $DistNP \not\subseteq AvgP$.

2.  $PH \not\subseteq DTIME(2^{O(n / \log n)})$ implies $DistPH \not\subseteq AvgP$.

3.  $NP \not\subseteq DTIME(2^{O(n / \log n)})$ implies $DistNP \not\subseteq Avg_P P$.
Here, $Avg_P P$ denotes P-computable average-case polynomial time, which interpolates average-case polynomial-time and worst-case polynomial-time.  We complement this result by showing that $DistPH \not\subseteq AvgP$ if and only if $DistPH \not\subseteq Avg_P P$.

At the core of all of our results is a new notion of universal heuristic scheme, whose running time is P-computable average-case polynomial time under every polynomial-time samplable distribution.  Our proofs are based on the meta-complexity of time-bounded Kolmogorov complexity: We analyze average-case complexity through the lens of worst-case meta-complexity using a new "algorithmic" proof of language compression and weak symmetry of information for time-bounded Kolmogorov complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/058"><span class="datestr">at April 25, 2021 10:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.11214">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.11214">Topological Simplifications of Hypergraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Youjia.html">Youjia Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rathore:Archit.html">Archit Rathore</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purvine:Emilie.html">Emilie Purvine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Bei.html">Bei Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.11214">PDF</a><br /><b>Abstract: </b>We study hypergraph visualization via its topological simplification. We
explore both vertex simplification and hyperedge simplification of hypergraphs
using tools from topological data analysis. In particular, we transform a
hypergraph to its graph representations known as the line graph and clique
expansion. A topological simplification of such a graph representation induces
a simplification of the hypergraph. In simplifying a hypergraph, we allow
vertices to be combined if they belong to almost the same set of hyperedges,
and hyperedges to be merged if they share almost the same set of vertices. Our
proposed approaches are general, mathematically justifiable, and they put
vertex simplification and hyperedge simplification in a unifying framework.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.11214"><span class="datestr">at April 25, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.11192">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.11192">Affine automata verifiers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khadieva:Aliya.html">Aliya Khadieva</a>, Abuzer Yakaryılmaz <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.11192">PDF</a><br /><b>Abstract: </b>We initiate the study of the verification power of AfAs as part of
Arthur-Merlin (AM) proof systems. We show that every unary language is verified
by a real-valued AfA verifier. Then, we focus on the verifiers restricted to
have only integer-valued or rational-valued transitions. We observe that
rational-valued verifiers can be simulated by integer-valued verifiers, and,
their protocols can be simulated in nondeterministic polynomial time. We show
that this bound tight by presenting an AfA verifier for NP-complete problem
SUBSETSUM. We also show that AfAs can verify certain non-affine and
non-stochastic unary languages.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.11192"><span class="datestr">at April 25, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.11046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.11046">The Density Fingerprint of a Periodic Point Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Edelsbrunner:Herbert.html">Herbert Edelsbrunner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heiss:Teresa.html">Teresa Heiss</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Philip.html">Philip Smith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wintraecken:Mathijs.html">Mathijs Wintraecken</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.11046">PDF</a><br /><b>Abstract: </b>Modeling a crystal as a periodic point set, we present a fingerprint
consisting of density functions that facilitates the efficient search for new
materials and material properties. We prove invariance under isometries,
continuity, and completeness in the generic case, which are necessary features
for the reliable comparison of crystals. The proof of continuity integrates
methods from discrete geometry and lattice theory, while the proof of generic
completeness combines techniques from geometry with analysis. The fingerprint
has a fast algorithm based on Brillouin zones and related inclusion-exclusion
formulae. We have implemented the algorithm and describe its application to
crystal structure prediction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.11046"><span class="datestr">at April 25, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.10939">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.10939">HINT: A Hierarchical Index for Intervals in Main Memory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>George Christodoulou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bouros:Panagiotis.html">Panagiotis Bouros</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mamoulis:Nikos.html">Nikos Mamoulis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.10939">PDF</a><br /><b>Abstract: </b>Indexing intervals is a fundamental problem, finding a wide range of
applications. Recent work on managing large collections of intervals in main
memory focused on overlap joins and temporal aggregation problems. In this
paper, we propose novel and efficient in-memory indexing techniques for
intervals, with a focus on interval range queries, which are a basic component
of many search and analysis tasks. First, we propose an optimized version of a
single-level (flat) domain-partitioning approach, which may have large space
requirements due to excessive replication. Then, we propose a hierarchical
partitioning approach, which assigns each interval to at most two partitions
per level and has controlled space requirements. Novel elements of our
techniques include the division of the intervals at each partition into groups
based on whether they begin inside or before the partition boundaries, reducing
the information stored at each partition to the absolutely necessary, and the
effective handling of data sparsity and skew. Experimental results on real and
synthetic interval sets of different characteristics show that our approaches
are typically one order of magnitude faster than the state-of-the-art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.10939"><span class="datestr">at April 25, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8089">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/24/towards-a-theory-of-generalization-in-reinforcement-learning-guest-lecture-by-sham-kakade/">Towards a Theory of Generalization in Reinforcement Learning: guest lecture by Sham Kakade</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://pehlevan.seas.harvard.edu/people/hamza-chaudhry">Hamza Chaudhry</a> and Zhaolin Ren</em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Natural Language Processing – guest lecture by Sasha Rush</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p>See also <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5d1a4401-6dee-4e49-881c-ad13017f606c">video of lecture</a>. <strong>Lecture slides:</strong> Original form: <a href="http://files.boazbarak.org/misc/mltheory/sham1.pdf">main</a> / <a href="http://files.boazbarak.org/misc/mltheory/sham2.pdf">bandit analysis</a>. Annotated: <a href="http://files.boazbarak.org/misc/mltheory/sham1_ink.pdf">main</a> / <a href="http://files.boazbarak.org/misc/mltheory/sham2_ink.pdf">bandit analysis</a>.</p>



<p><a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a> is a professor in the Department of Computer Science and the Department of Statistics at the University of Washington, as well as a senior principal researcher at Microsoft Research New York City. He works on the mathematical foundations of machine learning and AI. He is the recipient of the several awards, including the ICML Test of Time Award (2020), the IBM Pat Goldberg best paper award (in 2007), and the INFORMS Revenue Management and Pricing Prize (2014).</p>



<p>Sham is writing a <a href="https://rltheorybook.github.io/">book on the theory of reinforcement learning</a> with Agarwal, Jiang and Sun.</p>



<h2>Introduction:</h2>



<p>Reinforcement learning has found success in a great number of fields because it is a very “natural framework” for interactive learning. It is based around the notion of experimenting with different behaviors in one’s environment and learning from mistakes to identify the optimal strategy. However, there is a lack of understanding regarding how to best optimize reinforcement learning algorithms when there is uncertainty about the agent’s environment and potential rewards. Therefore, it is important to develop a theoretical foundation about this to study generalization in reinforcement learning. The primary question these notes will address is as follows:</p>



<p><strong>What are necessary representational and distributional conditions that enable provably sample-efficient reinforcement learning?</strong></p>



<p>We will answer this question in the following parts.</p>



<ul><li><strong>Part I: Bandits &amp; Linear Bandits</strong> “Bandit problems” correspond to RL where the environment is reset in each step (horizon H=1). This captures the aspect of having an unknown reward function of RL, but does not capture the aspect of a changing environment based on agent’s actions. This part will be based on the papers <a href="https://homes.cs.washington.edu/~sham/papers/ml/bandit_rates.pdf">Dani-Hayes-Kakade 08</a> and <a href="https://arxiv.org/abs/0912.3995">Srinivas-Kakade-Krause-Seeger 10</a></li><li><strong>Part II: Lower Bounds</strong> RL is very much <em>not</em> a solved problem in neither theory nor practice. Even the RL analog of linear regression, when the expected reward is a linear function of the actions, is not solved. We will see that this is for a good reason: there is an exponential lower bound on the number of steps it takes to find a nearly-optimal policy in this case. This part is based on the recent paper <a href="https://arxiv.org/abs/2010.01374">Weisz-Amortila-Szepesvári 20</a> and the follow-up <a href="https://arxiv.org/abs/2103.12690">Wang-Wang-Kakade 21</a></li><li><strong>Interlude:</strong> Do these lower bounds matter in practice?</li><li><strong>Part III: Upper Bounds</strong> Given the lower bound, we see that to get positive results (aka <em>upper bounds</em> on the number of steps) we need to make strong assumptions on the structure of reqards. There have been a number of incomparable such assumptions used, and we will see that there is a way to unify them. This part is based on the recent paper <a href="https://arxiv.org/abs/2103.10897">Du-Kakade-Lee-Lovett-Mahajan-Sun-Wang 21</a></li></ul>



<p>Before all of these parts, we will start by introducing the general framework of Markov Decision Processes (MDPs) and do a quick tour of generalization for static learning and RL.</p>



<h3>Markov Decision Processes: A Framework for Reinforcement Learning</h3>



<p>We have an agent in an environment at state <img src="https://s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_t" class="latex" /> that takes some action <img src="https://s0.wp.com/latex.php?latex=a_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_t" class="latex" /> which will observe some reward <img src="https://s0.wp.com/latex.php?latex=r_%7Bt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_{t}" class="latex" /> and update the environment to state <img src="https://s0.wp.com/latex.php?latex=s_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_{t+1}" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2021/04/f7a66-17cuaqjq97x1h_sbieavvzg.png" alt="" /></figure>



<p>The following are some key terms that we will need throughout the rest of the notes:</p>



<ol><li><em>State Space, Action Space, Policy</em>: We denote the state space as <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BS%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{S}" class="latex" />, and the action space as <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{A}" class="latex" />. A policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> is a mapping from states to actions: <img src="https://s0.wp.com/latex.php?latex=%5Cpi%3A+%5Cmathcal%7BS%7D+%5Cto+%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi: \mathcal{S} \to \mathcal{A}" class="latex" /></li><li><strong>Trajectory</strong>: The sequence of states, actions, rewards an agent sees for a horizon of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> timesteps. <img src="https://s0.wp.com/latex.php?latex=%28s_1%2C+a_1%2C+r_1%2C+s_2%2C+a_2%2C+r_2%2C+%5Cdots%2C+s_%7BH%7D%2C+a_%7BH%7D%2C+r_%7BH%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(s_1, a_1, r_1, s_2, a_2, r_2, \dots, s_{H}, a_{H}, r_{H})" class="latex" /></li><li><em>State Value at time <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" /></em>: The expected cumulative reward starting from state <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s" class="latex" /> and using policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> afterwards.</li><li><img src="https://s0.wp.com/latex.php?latex=V_h%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D+%5Cleft%5B+%5Csum_%7Bt%3Dh%7D%5E%7BH%7D+r_t+%5Cvert+s_h+%3D+s+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_h^\pi(s) = \mathbb{E} \left[ \sum_{t=h}^{H} r_t \vert s_h = s \right]" class="latex" /></li><li><em>State Action Value at time <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" /></em>: The expected cumulative reward given a state-action tuple <img src="https://s0.wp.com/latex.php?latex=%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(s,a)" class="latex" /> starting from time <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" /> and using policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> afterwards. <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cpi%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D+%5Cleft%5B+%5Csum_%7Bt%3Dh%7D%5E%7BH%7D+r_t+%5Cvert+s_h+%3D+s%2C+a_h+%3D+a+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\pi(s,a) = \mathbb{E} \left[ \sum_{t=h}^{H} r_t \vert s_h = s, a_h = a \right]" class="latex" /></li><li><em>Optimal value and state-value function</em>: we define an optimal policy by <img src="https://s0.wp.com/latex.php?latex=%5Cpi%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi^\star" class="latex" />, and the associated optimal <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />-function and value function by <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V^\star" class="latex" /> respectively (or equivalently, <img src="https://s0.wp.com/latex.php?latex=Q%5E%7B%5Cpi%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^{\pi^\star}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=V%5E%7B%5Cpi%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V^{\pi^\star}" class="latex" />). Note that <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V^\star" class="latex" /> can be defined via the Bellman optimality equation as follows:</li></ol>



<p><img src="https://s0.wp.com/latex.php?latex=V_h%5E%5Cstar%28s%29+%3D+%5Cmax_%7Ba+%5Cin+%5Cmathcal%7BA%7D%7D+Q_h%5E%5Cstar%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_h^\star(s) = \max_{a \in \mathcal{A}} Q_h^\star(s,a)" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D%5Cleft%5BR_h%28s%2Ca%29+%2B+V_%7Bh%2B1%7D%5E%5Cstar%28s_%7Bh%2B1%7D%29+%5Cmid+s_h+%3D+s%2C+a_h+%3D+a+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(s,a) = \mathbb{E}\left[R_h(s,a) + V_{h+1}^\star(s_{h+1}) \mid s_h = s, a_h = a \right]" class="latex" /></p>



<p>where we additionally define <img src="https://s0.wp.com/latex.php?latex=V_%7BH%2B1%7D%28s%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_{H+1}(s) = 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5Cmathcal%7BS%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s \in \mathcal{S}" class="latex" />.</p>



<ol start="6"><li><strong>Goal:</strong> To find a policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> that maximizes the cumulative H-step reward <img src="https://s0.wp.com/latex.php?latex=V_1%5E%5Cpi%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_1^\pi(s)" class="latex" /> starting from an initial state <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s" class="latex" /> with a horizon <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />. In the episodic setting, one starts at state <img src="https://s0.wp.com/latex.php?latex=s_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1" class="latex" />, acts for <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> steps, and then repeats.</li></ol>



<h3>Challenges in Reinforcement Learning</h3>



<p>There are three main challenges that we face in reinforcement learning</p>



<ol><li><strong>Exploration:</strong> The total size and states of the environment may be unknown.</li><li><em>Credit Assignment:</em> We need to assign rewards to actions even if the rewards are delayed.</li><li><em>Large State/Action Spaces:</em> We face the curse of dimensionality.<ul><li><a href="https://openai.com/blog/learning-dexterity/">OpenAI 2019 Dexterous Robotic Hand Manipulation</a></li></ul><img src="https://i.imgur.com/PIIli4L.gif" alt="" /></li></ol>



<p><strong>We will deal with these problems by framing them in terms of generalization.</strong></p>



<h2>Part 0: A Whirlwind Tour of Generalization</h2>



<h3>Provable Generalization in Supervised Learning</h3>



<p>As we have seen in <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">the first lecture of this course</a>, generalization is possible in the supervised learning setting, when the data follows an i.i.d distribution.<br />Specifically we have the following bound</p>



<blockquote class="wp-block-quote"><p><strong>Occam’s Razor Bound (Finite Hypothesis Class):</strong> To learn a policy that is <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />close to the best policy in a hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" />, we need a number of samples that is <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D+%28%5Clog%28%7C%5Cmathcal%7BF%7D%7C%29+%2F+%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{O} (\log(|\mathcal{F}|) / \epsilon^2)" class="latex" />.</p></blockquote>



<p>This means we can try lots of things on our data to see which hypotheses are <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-best. To handle infinite hypothesis classes, we can replace <img src="https://s0.wp.com/latex.php?latex=%5Clog+%7C%5Cmathcal%7BF%7D%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log |\mathcal{F}|" class="latex" /> with various other “complexity measures” to obtain generalization bounds such as:</p>



<ul><li>VC Dimension: <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D+%28%5Ctext%7BVC%7D%28%5Cmathcal%7BF%7D%29+%2F+%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{O} (\text{VC}(\mathcal{F}) / \epsilon^2)" class="latex" /></li><li>Classification (Margin Bounds): <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D+%28%5Ctext%7Bmargin%7D+%2F+%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{O} (\text{margin} / \epsilon^2)" class="latex" /></li><li>Linear Regression: <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Ctext%7Bdimension%7D%2F%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{O}(\text{dimension}/\epsilon^2)" class="latex" /></li><li>Deep Learning: Algorithm also determines the complexity control</li></ul>



<p>Another way to say this is that in all of these cases, we can bound the generalization gap <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> by a quantity of the form <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bd%2Fn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(\sqrt{d/n})" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" /> is some “complexity measure” of the class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is the number of samples.</p>



<p>One reference for these generalization results in the supervised learning setting is the following <a href="https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf">book</a> by Sanjeev Arora and collaborators.</p>



<p>The key enabler of generalization in supervised learning is <em>data reuse</em>. For a given training set, we can in principle simultaneously evaluate the loss of all hypotheses in our class. For example, given the fixed ImageNet dataset, we can evaluate performance on any classifier. As we will see, this is not a property that will always hold in RL (when it does hold, sample-efficient generalization is likely to follow).</p>



<h3>Sample Efficient RL in the Tabular Case with few states and actions (No Generalization involved):</h3>



<p>Consider a tabular MDP setting where <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> , <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> denote the number of states, number of actions and length of the horizon respectively. Suppose we are operating in a setup where <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> are both small. Suppose also that the MDP is <strong>unknown</strong>.</p>



<p>Our goal in such a setting is to find a <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-optimal policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=V_%7B1%7D%5E%7B%5Cpi%7D%28s_1%29+%5Cgeq+V_1%5E%7B%5Cpi%5E%5Cstar%7D%28s_1%29+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_{1}^{\pi}(s_1) \geq V_1^{\pi^\star}(s_1) - \epsilon" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=s_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1" class="latex" /> is the initial state (for concreteness let’s assume it is deterministic), and <img src="https://s0.wp.com/latex.php?latex=%5Cpi%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi^\star" class="latex" /> is <strong>truly</strong> an optimal policy for this MDP. Since we assume the number of states and actions to be small, it is possible to explore the entire world, and finding such an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-optimal policy is in principle possible. We thus do not have to consider any hypothesis class here (so no generalization involved), and can instead seek to be optimal under all possible mappings from states to actions.</p>



<p>Think for example of the following maze MDP, where the state of the world is the cell the agent is in and the action it can take at each state is a move to each of say 4 neighboring cells. Then, if we are able to get to every state and try every action there, we would have learned the world.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/BxUuayQ.png" alt="" /></figure>



<p>In this particular scenario, randomly exploring the world will allow us to learn the world. However, if we consider a modified random exploration strategy, where the probability of going left is significantly larger (say 5 times larger) than the probability of going right, then it will take exponential time to hit the goal state. In general, even for MDPs with small state and action spaces, a purely random exploration approach may be insufficient, as we may not be exploring the world enough. What alternative approach might we then adopt in order to achieve a sample-efficient learning algorithm?</p>



<blockquote class="wp-block-quote"><p><a href="https://www.cis.upenn.edu/~mkearns/papers/KearnsSinghE3.pdf"><strong>Theorem: (Kearns &amp; Singh ’98)</strong></a>. In the episodic setting, <img src="https://s0.wp.com/latex.php?latex=poly%28S%2CA%2CH%2C1%2F%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="poly(S,A,H,1/\epsilon)" class="latex" /> samples suffice to find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />opt policy, where <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> is the number of states, <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> is the number of actions, and <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is the length of the horizon.</p></blockquote>



<p>The above breakthrough result was the first to demonstrate that learning an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-opt policy is possible using just polynomially many samples. The key idea behind this is optimism and dynamic programming. In proving the result, the authors designed an algorithm called the E<img src="https://s0.wp.com/latex.php?latex=%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="^3" class="latex" /> algorithm (Explicit Explore or Exploit). The E<img src="https://s0.wp.com/latex.php?latex=%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="^3" class="latex" /> algorithm adopts a model-based approach, and relies on a “plan-to-explore” mechanism. As we act randomly, we will learn some part of the state space, and having learned this region well, we can thus accurately plan to escape it. This is where optimism comes in, since we give ourselves a bonus for escaping a region we know well.</p>



<p>Based on the Kearns and Singh result, there has been a number of followup works on the tabular MDP setting. One line of work seeks to improve on the precise factors in the sample complexity.</p>



<p><strong>Improvements on the sample complexity:</strong></p>



<ul><li><a href="https://jmlr.org/papers/volume3/brafman02a/brafman02a.pdf">A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning</a> / Brafman-Tennenholtz 2002</li><li><a href="https://homes.cs.washington.edu/~sham/papers/thesis/sham_thesis.html">On the Sample Complexity of Reinforcement Learning</a> – Kakade 2003 (PhD Thesis).</li><li><a href="https://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf">Near-Optimal Regret Bounds for Reinforcement Learning</a> – Jaksch, Ortner, Auer 2010</li><li><a href="https://arxiv.org/abs/1705.07041">Posterior sampling for reinforcement learning: worst-case regret bounds</a> – Agrawal Jia 2017</li></ul>



<p>Another line of work seeks to show that <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>, a model-free approach, can also achieve similar polynomial complexity, if an appropriate optimism bonus is incorporated.</p>



<p><strong>Provable Q-Learning (+Bonus)</strong></p>



<ul><li><a href="https://cseweb.ucsd.edu/~ewiewior/06efficient.pdf">PAC Model-Free Reinforcement Learning</a> – Strehl-Li-Wiewiora-Langford-Littman 2006</li><li><a href="https://castlelab.princeton.edu/html/ORF544/Readings/Szepesvari%20-%20Algorithms%20for%20reinforcement%20learning.pdf">Algorithms for Reinforcement Learning</a> – lecture by Szepesv´ari 2009</li><li><a href="https://arxiv.org/abs/1807.03765">Is Q-learning Provably Efficient?</a> – Jin Allen-ZhuBubeck Jordan 2018</li></ul>



<p>As the range and technical depth of the above results demonstrate, even in the relatively simple tabular case, the problem is already challenging, and a precise sharp characterization of sample complexity is even more difficult. The chief source of difficulty is the unknown nature of the world (if the world was known, then we can just run dynamic programming).</p>



<h3>Provable Generalization in RL:</h3>



<p>Ultimately, we want to move beyond small tabular MDPs, where a polynomial dependence in the sample complexity on <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> is acceptable, and achieve sample-efficient learning in big problems where the space space could be massive. Think for instance of the game of Go.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/DNHJgnJ.jpg" alt="" /></figure>



<p>In such a setting, requiring polynomially (in <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" />) many samples is clearly unacceptable. This gives rise to the following question.</p>



<p><strong>Question 1: Can we find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />opt policy with no <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> dependence?</strong></p>



<p>In order to do so, it is necessary to reutilize data in some way since we will not be able to see all the possible states in the world. How then might we reuse data to estimate the value of all policies in a policy class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" />? A naive approach is the following:</p>



<ul><li>Idea: Trajectory tree algorithm</li><li>Dataset Collection: Choose actions uniformly at random for all H steps in an episode.</li><li>Estimation: Uses importance sampling to evaluate every <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{F}" class="latex" />.</li></ul>



<blockquote class="wp-block-quote"><p><a href="https://www.cis.upenn.edu/~mkearns/papers/traj.pdf"><strong>Theorem: (Kearns, Mansour, &amp; Ng ’00)</strong></a> To find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />best in class policy, the trajectory tree algorithm uses <img src="https://s0.wp.com/latex.php?latex=O%28A%5EH%5Clog%28%7C%5Cmathcal%7BF%7D%7C%29%2F%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(A^H\log(|\mathcal{F}|)/\epsilon^2)" class="latex" /> samples.</p></blockquote>



<p>Observe that when <img src="https://s0.wp.com/latex.php?latex=H+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H = 1" class="latex" /> (i.e. a contextual bandit) this is exactly the kind of generalization bound we saw in the Occam Razor’s bound for supervised learning. Since there may be stochasticity in the MDP, such that <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> could be infinite or even uncountable, this dependence on <img src="https://s0.wp.com/latex.php?latex=A%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A^H" class="latex" /> is a genuine improvement on the results for the tabular MDP setting which depended polynomially on <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" />. In this sense, this really is a generalization result, since we are learning an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-best in class policy without having seen the entire world (i.e. all the states in the world).<br />We note that the result only has <img src="https://s0.wp.com/latex.php?latex=%5Clog%28%5Cmathcal%7BF%7D%7C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log(\mathcal{F}|)" class="latex" /> dependence on hypothesis class size and similar to the supervised learning setting, there are VC analogues as well. However, we can not avoid the <img src="https://s0.wp.com/latex.php?latex=A%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A^H" class="latex" /> dependence to find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />best-in-class policy agnostically (without assumptions on the MDP). To see why, consider a binary tree with <img src="https://s0.wp.com/latex.php?latex=2%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^H" class="latex" />-policies and a sparse reward at a leaf node.</p>



<p>This <img src="https://s0.wp.com/latex.php?latex=A%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A^H" class="latex" /> dependence, while unavoidable without further assumptions, is clearly undesirable. This brings us to the following question.</p>



<p><strong>Question 2: Can we find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon-" class="latex" />opt policy with no <img src="https://s0.wp.com/latex.php?latex=S%2CA&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S,A" class="latex" /> dependence and <img src="https://s0.wp.com/latex.php?latex=poly%28H%2C1%2F+%5Cepsilon%2C+%5Ctext%7B%22complexity+measure%22%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="poly(H,1/ \epsilon, \text{&quot;complexity measure&quot;})" class="latex" /> samples?</strong></p>



<p>As we just saw, agnostically we cannot learn an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-best-in-class policy without an <img src="https://s0.wp.com/latex.php?latex=A%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A^H" class="latex" /> dependence. However, as we will see it is possible when appropriate assumptions are made. But what is the nature of the assumptions under which this kind of sample-efficient RL generalization is possible (when there is no (or mild) dependence on <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" />)? What assumptions are necessary? What assumptions are sufficient? We will seek to address these questions.</p>



<p>To do so, we start simple, and first look at the bandits and linear bandits problem, where the horizon <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is just 1. Note that this is still an interactive learning problem, just that we reset the episode after one time-step, and that it is an example of a problem with a potentially large action space.</p>



<h2>Part 1: Bandits and Linear Bandits</h2>



<h3>Multi-Armed Bandits:</h3>



<figure class="wp-block-image"><img src="https://i.imgur.com/LHEqOwn.png" alt="" /></figure>



<p>The multi-armed bandits algorithm is intimately interwoven with the theory of reinforcement learning. It is based around the question of how to allocate T tokens to A “arms” to maximize one’s return:</p>



<ul><li><a href="https://www.ams.org/journals/bull/1952-58-05/S0002-9904-1952-09620-8/S0002-9904-1952-09620-8.pdf">Some Aspects of the Sequential Design of Experiments</a> – Robbins 1952</li><li><a href="https://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Gittins:1979.pdf">Bandit Processes and Dynamic Allocations Indices</a> – Gittins 1979</li><li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.1620&amp;rep=rep1&amp;type=pdf">Asymptotically Efficient Adaptive Allocation Rules</a> – Lai and Robbins 1985</li></ul>



<p>It is a very successful algorithm when <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> is small. What can we do when <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> is large?</p>



<h4>Large-Action Case:</h4>



<p>The bandits have to make a decision regarding which arm to pull. There is a widely used linear formulation of this problem that will assist us in understanding generalization. The <a href="http://phillong.info/publications/peval.pdf">linear bandit model</a> is successful in many applications (scheduling, ads, etc.)</p>



<p><strong>Linear (RKHS) Bandits:</strong></p>



<ul><li>Decision: <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t" class="latex" />; Reward: <img src="https://s0.wp.com/latex.php?latex=r_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_t" class="latex" />; Reward model:<br /><img src="https://s0.wp.com/latex.php?latex=r_t+%3D+f%28x_t%29+%2B+%5Ctext%7Bnoise%7D%3B+f%28x%29+%3D+w%5E%5Cstar+%5Ccdot+%5Cphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_t = f(x_t) + \text{noise}; f(x) = w^\star \cdot \phi(x)" class="latex" /></li><li>The hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" /> is a set of linear/RKHS functions (an overview of RKHS, which stands for Reproducing Kernel Hilbert Space, can be found <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">here</a>).</li></ul>



<h4>Linear/Gaussian Process Upper Confidence Bounds (UCB):</h4>



<p>The principle underlying the Linear Bandits algorithm is <strong>optimism in the face of uncertainty</strong>:<br />Pick an input that maximizes the upper confidence bound:</p>



<p><img src="https://s0.wp.com/latex.php?latex=x_t+%3D+%5Ctext%7Barg%7D%5Cmax_%7Bx+%5Cin+D%7D+%5Cmu_%7Bt-1%7D%28x%29+%2B+%5Cbeta_t+%5Csigma_%7Bt-1%7D%28x%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t = \text{arg}\max_{x \in D} \mu_{t-1}(x) + \beta_t \sigma_{t-1}(x)." class="latex" /></p>



<p>Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_{t-1}" class="latex" /> is the best estimate of the ground-truth and <img src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma_{t-1}" class="latex" /> is a standard deviation that we have to estimate. In choosing the term <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t" class="latex" />, we have to navigate a trade-off between exploration and exploitation. As we can see, this algorithm will only pick plausible maximizers.</p>



<h4>Regret of Linear-UCB / Gaussian Process-UCB (Generalization in Action Space)</h4>



<blockquote class="wp-block-quote"><p><strong><a href="http://people.cs.uchicago.edu/~varsha/pubs/stoch_bandit.pdf">Theorem: (Dani, Hayes, &amp; K. ’08)</a>, <a href="https://arxiv.org/pdf/0912.3995.pdf">(Srinivas, Krause, K., &amp; Seeger ’10)</a></strong>. Assuming <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" /> is an RKHS (with bounded norm), if we choose <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t" class="latex" /> “correctly”, then the regret satisfies<br /><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt%3D1%7D%5ET%5Bf%28x%5E%5Cstar%29+-+f%28x_t%29%5D+%3D+%5Ctilde%7B%5Cmathcal%7BO%7D%7D+%5Cleft%28+%5Csqrt%7B%5Cfrac%7B%5Cgamma_T%7D%7BT%7D%7D+%5Cright%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{1}{T} \sum_{t=1}^T[f(x^\star) - f(x_t)] = \tilde{\mathcal{O}} \left( \sqrt{\frac{\gamma_T}{T}} \right)," class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BO%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mathcal{O}}" class="latex" /> hides logarithmic terms, and </p></blockquote>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/image.png"><img src="https://windowsontheory.files.wordpress.com/2021/04/image.png?w=1024" alt="" class="wp-image-8095" /></a></figure>



<p>The key complexity concept here is “Maximum Information Gain”: <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_T" class="latex" />, which one can think of as the “effective dimension,” determines the regret because <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_T+%5Capprox+d+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_T \approx d \log T" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" />. Here are some relevant papers for further understanding regret, which is the difference between the reward of a possible action and the reward of an action that has been taken.</p>



<ul><li><a href="https://homes.di.unimi.it/cesa-bianchi/Pubblicazioni/ml-02.pdf">Finite-time Analysis of the Multiarmed Bandit Problem</a> – Auer Cesa-Bianchi Fischer 2002</li><li><a href="https://papers.nips.cc/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf">Improved Algorithms for Linear Stochastic Bandits</a> – Abbasi-yadkori, Pál, Szepesvári 2011</li></ul>



<h3>Linear Upper Confidence Bound Analysis</h3>



<h4>Handling Large Action Spaces</h4>



<p>On each round, we must choose a decision <img src="https://s0.wp.com/latex.php?latex=x_t+%5Cin+D+%5Csubset+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t \in D \subset R^d" class="latex" />. This yields a reward <img src="https://s0.wp.com/latex.php?latex=r_t+%5Cin+%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_t \in [-1,1]" class="latex" />, where</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Br_t+%7C+x_t+%3D+x%5D+%3D+%5Cmu%5E%5Cstar+%5Ccdot+x+%5Cin+%5B-1%2C1%5D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[r_t | x_t = x] = \mu^\star \cdot x \in [-1,1]." class="latex" /></p>



<p>Above, <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star" class="latex" /> is an unknown weight vector and <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> may be replaced by <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(x)" class="latex" /> if we have access to such a representation. Note that this tells us that the conditional expectation of <img src="https://s0.wp.com/latex.php?latex=r_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_t" class="latex" /> upon <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t" class="latex" /> is linear. We have the a corresponding i.i.d. noise sequence <img src="https://s0.wp.com/latex.php?latex=%5Ceta_t+%3D+r_t+-+%5Cmu%5E%5Cstar+%5Ccdot+x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta_t = r_t - \mu^\star \cdot x_t" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%28x_0%2C+%5Cdots%2C+x_%7BT-1%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_0, \dots, x_{T-1})" class="latex" /> are our decisions, then our <em>cumulative regret</em> in expectation is</p>



<p><img src="https://s0.wp.com/latex.php?latex=R_T+%3D+T%28%5Cmu%5E%5Cstar+%5Ccdot+x%5E%5Cstar%29+-+%5Csum_%7Bt%3D0%7D%5E%7BT+-+1%7D+%5Cmu%5E%5Cstar+%5Ccdot+x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_T = T(\mu^\star \cdot x^\star) - \sum_{t=0}^{T - 1} \mu^\star \cdot x_t" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=x%5E%5Cstar%5Cin+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\star\in D" class="latex" /> is an optimal decision for <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star" class="latex" />, i.e.</p>



<p><img src="https://s0.wp.com/latex.php?latex=x%5E%5Cstar+%5Cin+%5Ctext%7Barg%7D%5Cmax_%7Bx+%5Cin+D%7D+%5Cmu%5E%5Cstar+%5Ccdot+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\star \in \text{arg}\max_{x \in D} \mu^\star \cdot x" class="latex" /></p>



<h4>LinUCB and the Confidence Ball</h4>



<p>After t rounds, we can define our uncertainty region <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_t" class="latex" /> with center <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu_t%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu_t}" class="latex" /> and shape <img src="https://s0.wp.com/latex.php?latex=%5CSigma_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t" class="latex" /> using the <img src="https://s0.wp.com/latex.php?latex=%5Clambda-&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda-" class="latex" />regularized least squares solution:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu_t%7D+%3D+%5Ctext%7Barg%7D%5Cmin_%5Cmu+%5Csum_%7B%5Ctau+%3D+0%7D%5E%7Bt-1%7D+%5Cleft%5C%7C+%5Cmu+%5Ccdot+x_%5Ctau+-+r_%5Ctau+%5Cright%5C%7C_2%5E2+%2B+%5Clambda+%5Cleft+%5C%7C+%5Cmu+%5Cright%5C%7C_2%5E2%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu_t} = \text{arg}\min_\mu \sum_{\tau = 0}^{t-1} \left\| \mu \cdot x_\tau - r_\tau \right\|_2^2 + \lambda \left \| \mu \right\|_2^2," class="latex" /></li><li><img src="https://s0.wp.com/latex.php?latex=%5CSigma_t+%3D+%5Clambda+I+%2B+%5Csum_%7B%5Ctau+%3D+0%7D%5E%7Bt-1%7D+x_%5Ctau+x_%5Ctau%5E%5Ctop+%5Ctext%7B%2C+with+%7D+%5CSigma_0+%3D+%5Clambda+I%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t = \lambda I + \sum_{\tau = 0}^{t-1} x_\tau x_\tau^\top \text{, with } \Sigma_0 = \lambda I," class="latex" /></li><li><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_t+%3D+%5Cleft%5C%7B+%5Cmu%3A+%28%5Chat%7B%5Cmu_t%7D+-+%5Cmu%29%5E%5Ctop+%5CSigma_t+%28%5Chat%7B%5Cmu_t%7D+-+%5Cmu%29+%5Cleq+%5Cbeta_t+%5Cright%5C%7D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_t = \left\{ \mu: (\hat{\mu_t} - \mu)^\top \Sigma_t (\hat{\mu_t} - \mu) \leq \beta_t \right\}," class="latex" /></li><li><img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t" class="latex" /> is a parameter of the algorithm and <img src="https://s0.wp.com/latex.php?latex=%5CSigma_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t" class="latex" /> determines how accurately we know <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu_t%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu_t}" class="latex" /></li></ul>



<p>The LinUCB Algorithm can be understood as follows: For <img src="https://s0.wp.com/latex.php?latex=t+%3D+0%2C1%2C%5Cdots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t = 0,1,\dots" class="latex" /></p>



<ol><li>Execute <img src="https://s0.wp.com/latex.php?latex=x_t+%3D+%5Ctext%7Barg%7D%5Cmax_%7Bx+%5Cin+D%7D+%5Cmax_%7B%5Cmu+%5Cin+%5Ctext%7BBALL%7D_t%7D+%5Cmu+%5Ccdot+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t = \text{arg}\max_{x \in D} \max_{\mu \in \text{BALL}_t} \mu \cdot x" class="latex" /></li><li>Observe the reward <img src="https://s0.wp.com/latex.php?latex=r_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_t" class="latex" /> and update <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_{t+1}" class="latex" /></li></ol>



<h4>LinUCB Regret Bound</h4>



<p>As the following theorem shows, the regret <img src="https://s0.wp.com/latex.php?latex=R_T+%5Cleq+%5Ctilde%7B%5Cmathcal%7BO%7D+%7D%28d%5Csqrt+T%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_T \leq \tilde{\mathcal{O} }(d\sqrt T)" class="latex" /> is sublinear with polynomial dependence on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" /> and no dependence on the cardinality <img src="https://s0.wp.com/latex.php?latex=%7CD%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|D|" class="latex" /> of the decision space <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />.</p>



<blockquote class="wp-block-quote"><p><a href="https://homes.cs.washington.edu/~sham/papers/ml/bandit_linear_long.pdf"><strong>Theorem (regret): (Dani, Hayes, Kakade 2009)</strong></a>. Suppose we have bounded noise <img src="https://s0.wp.com/latex.php?latex=%7C%5Ceta_t%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\eta_t|" class="latex" />; <img src="https://s0.wp.com/latex.php?latex=%7C%7C+%5Cmu%5E%5Cstar+%7C%7C+%5Cleq+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|| \mu^\star || \leq W" class="latex" />; <img src="https://s0.wp.com/latex.php?latex=%7C%7Cx%7C%7C+%5Cleq+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="||x|| \leq B" class="latex" />, for <img src="https://s0.wp.com/latex.php?latex=x%5C+%5Cin+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\ \in D" class="latex" />. Set <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+%5Csigma%5E2%2FW%5E2%2C+%5Cquad+%5Cbeta_t+%3A%3D+c_1+%5Csigma%5E2%5Cleft%28d+%5Clog%5Cleft%281+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%7D%5Cright%29+%2B+%5Clog%281+%2F%5Cdelta%29%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda = \sigma^2/W^2, \quad \beta_t := c_1 \sigma^2\left(d \log\left(1 + \frac{TB^2}{d}\right) + \log(1 /\delta)\right)." class="latex" /> Then, with probability greater than <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \delta" class="latex" />, for all <img src="https://s0.wp.com/latex.php?latex=t+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \geq 0" class="latex" />,<br /><img src="https://s0.wp.com/latex.php?latex=R_T+%5Cleq+c_2+%5Csigma+%5Csqrt%7BT%7D+%5Cleft+%28+d+%5Clog%5Cleft%281+%2B+%5Cfrac%7BTB%5E2%7D%7Bd+%5Csigma%5E2%7D%5Cright%29+%2B+%5Csqrt%7B%5Clog%281+%2F%5Cdelta%29%7D%5Csqrt%7Bd+%5Clog%5Cleft%281+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%7D%5Cright%29%7D+%5Cright%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_T \leq c_2 \sigma \sqrt{T} \left ( d \log\left(1 + \frac{TB^2}{d \sigma^2}\right) + \sqrt{\log(1 /\delta)}\sqrt{d \log\left(1 + \frac{TB^2}{d}\right)} \right)," class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=c_1%2Cc_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c_1,c_2" class="latex" /> are absolute constants.</p></blockquote>



<p>To prove the regret theorem above, we will require the following two lemmas.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma 1 (Confidence):</strong> Let <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta &gt; 0" class="latex" />. We have that <img src="https://s0.wp.com/latex.php?latex=Pr%28%5Cforall+t%2C+%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t%29+%5Cgeq+1+-+%5Cdelta.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Pr(\forall t, \mu^\star \in \text{BALL}_t) \geq 1 - \delta." class="latex" /></p><p><strong>Lemma 2 (Sum of Squares Regret Bound):</strong> Define <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bregret%7D_t+%3D+%5Cmu%5E%5Cstar+%5Ccdot+x%5E%5Cstar+-+%5Cmu%5E%5Cstar+%5Ccdot+x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{regret}_t = \mu^\star \cdot x^\star - \mu^\star \cdot x_t" class="latex" />. Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t \geq 1" class="latex" /> is increasing and that for all <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />, we have <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{BALL}_t" class="latex" />. Then, <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+%5Ctext%7Bregret%7D_t%5E2+%5Cleq+8+%5Cbeta_T+d+%5Clog+%5Cleft+%28+1+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%5Clambda%7D+%5Cright+%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{t=0}^{T-1} \text{regret}_t^2 \leq 8 \beta_T d \log \left ( 1 + \frac{TB^2}{d\lambda} \right )." class="latex" /></p></blockquote>



<p>We note that Lemma 2 actually depends on Lemma 1, since it assumes that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{BALL}_t" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />, a property that Lemma 1 tells us happens with probability at least <img src="https://s0.wp.com/latex.php?latex=1-%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\delta" class="latex" />. We defer the proofs of the two lemmas to later, and first show why they can be used to prove the regret theorem.</p>



<p><strong>Proof of regret theorem:</strong> Using the two lemmas above along with the Cauchy-Schwarz inequality, we have with probability at least <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \delta" class="latex" /> that</p>



<p><img src="https://s0.wp.com/latex.php?latex=R_T+%3D+%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+%5Ctext%7Bregret%7D_t+%5Cleq+%5Csqrt%7BT+%5Csum%7Bt%3D0%7D%5E%7BT-1%7D+%5Ctext%7Bregret%7D_t%5E2%7D+%5Cleq+%5Csqrt%7B8+T+%5Cbeta_T+d+%5Clog+%5Cleft%28+1+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%5Clambda%7D+%5Cright%29%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_T = \sum_{t=0}^{T-1} \text{regret}_t \leq \sqrt{T \sum{t=0}^{T-1} \text{regret}_t^2} \leq \sqrt{8 T \beta_T d \log \left( 1 + \frac{TB^2}{d\lambda} \right)}." class="latex" /></p>



<p>The rest of the proof follows from our chosen value of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_T" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>We now proceed to sketch out the proofs of Lemma 1 (confidence bound) and Lemma 2 (sum of squares regret bound). We begin with showing why Lemma 2 holds.</p>



<h4>Analysis and proof of Lemma 2 (sum of squares regret bound)</h4>



<p>Our first auxilliary result bounds the pointwise width of the confidence ball.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma (pointwise width of confidence ball)</strong>. Let <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in D" class="latex" />. Consider any <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in \text{BALL}_t" class="latex" />. Then,<br /><img src="https://s0.wp.com/latex.php?latex=%7C+%28%5Cmu+-+%5Chat%7B%5Cmu%7D_t%29%5E%5Ctop+x%7C+%5Cleq+%5Csqrt%7B%5Cbeta_t+x%5E%5Ctop+%5CSigma_t%5E%7B-1%7Dx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| (\mu - \hat{\mu}_t)^\top x| \leq \sqrt{\beta_t x^\top \Sigma_t^{-1}x}." class="latex" /></p></blockquote>



<p><em><strong>Proof</strong></em>. We have</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ghNhZRo.png" alt="" /></figure>



<p>where the first inequality follows from Cauchy-Schwarx and the second (i.e last) inequality holds by the definition of <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_t" class="latex" /> and our assumption that <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Ctext%7BBALL%7D_t.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in \text{BALL}_t." class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>Let us now define</p>



<p><img src="https://s0.wp.com/latex.php?latex=w_t+%3A%3D+%5Csqrt%7Bx_t%5E%5Ctop+%5CSigma_t%5E%7B-1%7Dx_t%7D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_t := \sqrt{x_t^\top \Sigma_t^{-1}x_t}," class="latex" /></p>



<p>which we can think of as the “normalized width” at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" /> in the direction of our decision. We have the following bound on the instantaneous regret <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bregret%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{regret}_t" class="latex" />.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma (instantaneous regret lemma)</strong>. Fix <img src="https://s0.wp.com/latex.php?latex=t+%5Cleq+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \leq T" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{BALL}_t" class="latex" />, then<br /><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bregret%7D_t+%5Cleq+2+%5Cmin+%28%5Csqrt%7B%5Cbeta_t%7Dw_t%2C+1%29+%5Cleq+2+%5Csqrt%7B%5Cbeta_t%7D+%5Cmin%28w_t%2C1%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{regret}_t \leq 2 \min (\sqrt{\beta_t}w_t, 1) \leq 2 \sqrt{\beta_t} \min(w_t,1)." class="latex" /></p></blockquote>



<p><em><strong>Proof</strong></em>. The basic idea is to use “optimism”. Let <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmu%7D+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mu} \in \text{BALL}_t" class="latex" /> denote the vector maximizing the dot product <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D%5E%5Ctop+x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu}^\top x_t" class="latex" />. By choice of <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmu%7D%5E%5Ctop+x_t+%3D+%5Cmax%7B%5Cmu+%5Cin+%5Ctext%7BBALL%7D_t%7D+%5Cmax_%7Bx+%5Cin+D%7D%5Cmu%5E%5Ctop+x+%5Cgeq+%28%5Cmu%5E%5Cstar%29%5E%5Ctop+x%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mu}^\top x_t = \max{\mu \in \text{BALL}_t} \max_{x \in D}\mu^\top x \geq (\mu^\star)^\top x^\star" class="latex" />, where the inequality used the hypothesis that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{BALL}_t" class="latex" />. This manifestation of “optimism” is crucial, since it tells us that the “ideal” reward we think we can get at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" /> exceeds the optimal expected reward <img src="https://s0.wp.com/latex.php?latex=%28%5Cmu%5E%5Cstar%29%5E%5Ctop+x%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mu^\star)^\top x^\star" class="latex" />. Hence,</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/CO4lDFc.png" alt="" /></figure>



<p>where the last step follows from the pointwise width lemma (note <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mu}" class="latex" /> is in <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_t" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star" class="latex" /> is assumed to be <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{BALL}_t" class="latex" /> by the hypothesis in Lemma 2). Since in the linear bandits setup we assumed that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Ccdot+x+%5Cin+%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \cdot x \in [-1,1]" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in D" class="latex" />, the simple bound <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bregret%7D_t+%5Cleq+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{regret}_t \leq 2" class="latex" /> holds as well. We may also assume for simplicity that <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t \geq 1" class="latex" />. This then yields the bound in the result. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>In the next two lemmas, we use a geometric potential function argument to bound the sum of widths independently of the choices made by the algorithm (e.g. choice of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_t%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\beta_t}" class="latex" /> sequence).</p>



<blockquote class="wp-block-quote"><p><strong>Geometric Lemma 1</strong>. We have <img src="https://s0.wp.com/latex.php?latex=%5Cdet%28%5CSigma_T%29+%3D+%5Cdet%28%5CSigma_0%29+%5Cprod_%7Bt%3D0%7D%5E%7BT-1%7D+%281%2Bw_t%5E2%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\det(\Sigma_T) = \det(\Sigma_0) \prod_{t=0}^{T-1} (1+w_t^2)." class="latex" /></p></blockquote>



<p><em><strong>Proof</strong></em>. By definition of <img src="https://s0.wp.com/latex.php?latex=%5CSigma_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_{t+1}" class="latex" />, we have</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/qX7Q3Ft.png" alt="" /></figure>



<p>We complete the proof by noting that <img src="https://s0.wp.com/latex.php?latex=w_t%5E2+%3D+x_t%5E%5Ctop+%5CSigma_t%5E%7B-1%7Dx_t+%3D+%7C%5CSigma_t%5E%7B-1%2F2%7Dx_t+%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_t^2 = x_t^\top \Sigma_t^{-1}x_t = |\Sigma_t^{-1/2}x_t |^2" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<blockquote class="wp-block-quote"><p><strong>Geometric Lemma 2</strong>. For any sequence <img src="https://s0.wp.com/latex.php?latex=x_0%2C%5Cdots%2Cx_%7BT-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_0,\dots,x_{T-1}" class="latex" /> such that for <img src="https://s0.wp.com/latex.php?latex=t+%3C+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t &lt; T" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Cx_t%7C_2+%5Cleq+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|x_t|_2 \leq B" class="latex" />, we have<br /><img src="https://s0.wp.com/latex.php?latex=%5Clog%5Cleft%28%5Cdet%28%5CSigma_%7BT-1%7D%29%2F%5Cdet%28%5CSigma_0%29%5Cright%29+%3D+%5Clog+%5Cdet+%5Cleft%28I+%2B+%5Cfrac%7B1%7D%7B%5Cgamma%7D+%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+x_tx_t%5E%5Ctop+%5Cright%29+%5Cleq+d+%5Clog+%5Cleft%281+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%5Clambda%7D%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log\left(\det(\Sigma_{T-1})/\det(\Sigma_0)\right) = \log \det \left(I + \frac{1}{\gamma} \sum_{t=0}^{T-1} x_tx_t^\top \right) \leq d \log \left(1 + \frac{TB^2}{d\lambda}\right)." class="latex" /></p></blockquote>



<p><em><strong>Proof</strong></em>. Denote the eigenvalues of <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D0%7D%5E%7BT-1%7Dx_tx_t%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{t=0}^{T-1}x_tx_t^\top" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Csigma_1%2C%5Cdots%2C%5Csigma_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma_1,\dots,\sigma_d" class="latex" />, and note<br /><img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5Ed+%5Csigma_i+%3D+%5Ctext%7BTrace%7D%5Cleft%28%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+x_t+x_t%5E%5Ctop+%5Cright%29+%3D+%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+%7Cx_t%7C%5E2+%5Cleq+TB%5E2.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{i=1}^d \sigma_i = \text{Trace}\left(\sum_{t=0}^{T-1} x_t x_t^\top \right) = \sum_{t=0}^{T-1} |x_t|^2 \leq TB^2." class="latex" /><br />Using the AM-GM inequality,</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/jQDxDYm.png" alt="" /></figure>



<p>We are now finally ready to prove Lemma 2 (sum of squares regret bound).</p>



<p><strong>Proof of Lemma 2 (sum of squares regret bound)</strong>.<br />Assume <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{BALL}_t" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />. We have<br /><img src="https://i.imgur.com/1rzWWOT.png" alt="" /></p>



<p>where the first inequality follows from the instantaneous regret lemma, the second from that <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t" class="latex" /> is an increasing function of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />, the third uses the fact that for <img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+y+%5Cleq+1%2C+%5Cln%281%2By%29+%5Cgeq+y%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \leq y \leq 1, \ln(1+y) \geq y/2" class="latex" />, the final equality holds by Geometric Lemma 1, and the final inequality follows from Geometric Lemma 2. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>This wraps up our discussion of Lemma 2.</p>



<h4>Analysis of Lemma 1 (Confidence bound)</h4>



<p>Recall that our goal here is to show that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBall%7D_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\star \in \text{Ball}_t" class="latex" /> with high probability. We begin with the following result, which is a general version of the self-normalized sum argument in <a href="https://homes.cs.washington.edu/~sham/papers/ml/bandit_linear_long.pdf">Dani, Hayes, Kakade 2009</a>.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma (Self-normalized bound for vector-valued Martingalues, <a href="https://arxiv.org/abs/1102.2670">Abbasi et al. 2011</a>)</strong>. Suppose <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D_%7Bi%3D1%7D%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\epsilon_i}_{i=1}^{\infty}" class="latex" /> are mean zero random variables (can be generalized to martingalues), and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon_i" class="latex" /> is bounded by <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D_%7Bi%3D1%7D%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{X_i}_{i=1}^{\infty}" class="latex" /> be a stochastic process. Define <img src="https://s0.wp.com/latex.php?latex=%5CSigma_t+%3A%3D+%5CSigma_0+%2B+%5Csum_%7Bi%3D1%7D%5Et+X_i+X_i%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t := \Sigma_0 + \sum_{i=1}^t X_i X_i^\top" class="latex" />. With probability at least <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \delta" class="latex" />, we have for all <img src="https://s0.wp.com/latex.php?latex=t+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \geq 1" class="latex" />,<br /><img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Csum_%7Bt%3D1%7D%5Et+X_i+%5Cepsilon_i+%5Cright%7C_%7B%5CSigma_t%5E%7B-1%7D%7D%5E2+%5Cleq+%5Csigma%5E2+%5Clog+%5Cleft%28%5Cfrac%7B%5Cdet%28%5CSigma_t%29%5Cdet%28%5CSigma_0%29%5E%7B-1%7D%7D%7B%5Cdelta%5E2%7D+%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|\sum_{t=1}^t X_i \epsilon_i \right|_{\Sigma_t^{-1}}^2 \leq \sigma^2 \log \left(\frac{\det(\Sigma_t)\det(\Sigma_0)^{-1}}{\delta^2} \right)." class="latex" /></p></blockquote>



<p>Equipped with the lemma above, we are now ready to prove Lemma 1, which we will restate here again.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma 1 (Confidence):</strong> Let <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta &gt; 0" class="latex" />. We have that <img src="https://s0.wp.com/latex.php?latex=Pr%28%5Cforall+t%2C+%5Cmu%5E%5Cstar+%5Cin+%5Ctext%7BBALL%7D_t%29+%5Cgeq+1+-+%5Cdelta.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Pr(\forall t, \mu^\star \in \text{BALL}_t) \geq 1 - \delta." class="latex" /></p></blockquote>



<p><strong>Proof of Lemma 1</strong>. Since <img src="https://s0.wp.com/latex.php?latex=r_%7B%5Ctau%7D+%3D+x_%7B%5Ctau%7D%5Ccdot+%5Cmu%5E%5Cstar+%2B+%5Ceta_%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_{\tau} = x_{\tau}\cdot \mu^\star + \eta_{\tau}" class="latex" />, we have<br /><img src="https://i.imgur.com/fsVT5QQ.png" alt="" /></p>



<p>To get the last equality, we recall that <img src="https://s0.wp.com/latex.php?latex=%5CSigma_t+%3D+%5Clambda+I+%2B+%5Csum_%7B%5Ctau%3D0%7D%5E%7Bt-1%7D%5Ceta_%7B%5Ctau%7Dx_%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t = \lambda I + \sum_{\tau=0}^{t-1}\eta_{\tau}x_{\tau}" class="latex" />. By the triangle inequality, it follows that we have</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8Pr0zPB.png" alt="" /></figure>



<p>where the last inequality holds with probability at least <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \delta" class="latex" /> for every <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />, using the self-normalized bound above, as well as the fact that <img src="https://s0.wp.com/latex.php?latex=%5CSigma_t%5E%7B-1%2F2%7D+%5Cleq+1%2F%5Csqrt%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Sigma_t^{-1/2} \leq 1/\sqrt{\lambda}" class="latex" />. Since <img src="https://s0.wp.com/latex.php?latex=%28%7Cv_1+%7C+%2B+%7Cv_2%7C%29%5E2+%5Cleq+2%7Cv_1%7C%5E2+%2B+2%7Cv_2%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(|v_1 | + |v_2|)^2 \leq 2|v_1|^2 + 2|v_2|^2" class="latex" /> for any vectors <img src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,v_2" class="latex" />, it follows that with probability at least <img src="https://s0.wp.com/latex.php?latex=1+-+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - \delta" class="latex" />, for every <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />,<br /><img src="https://i.imgur.com/TS18lfK.png" alt="" /></p>



<p>where the final inequality is a consequence of the choice <img src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+%5Csigma%5E2%2FW%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda = \sigma^2/W^2" class="latex" /> in the algorithm (where we recall the upper bound <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%5E%5Cstar%7C+%5Cleq+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mu^\star| \leq W" class="latex" />), as well as Geometric Lemma 2. The result then follows by our choice of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t" class="latex" />, which is</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbeta_t+%3A%3D+c_1+%5Csigma%5E2%5Cleft%28d+%5Clog%5Cleft%281+%2B+%5Cfrac%7BTB%5E2%7D%7Bd%5Clambda%7D%5Cright%29+%2B+%5Clog%281+%2F%5Cdelta%29%5Cright%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta_t := c_1 \sigma^2\left(d \log\left(1 + \frac{TB^2}{d\lambda}\right) + \log(1 /\delta)\right)," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c_1" class="latex" /> is an absolute constant (note in doing so we also subsumed the <img src="https://s0.wp.com/latex.php?latex=2%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2\sigma^2" class="latex" /> term, simplifying the exposition). <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>We move on now to more challenging RL problems where the horizon <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> is larger than 1, and explore lower bounds in this regime.</p>



<h2>Part 2: What are necessary assumptions for generalization in RL?</h2>



<h3>Approximate dynamic programming with linear function approximation</h3>



<p>We begin by considering generalization with a very natural assumption: suppose that the value function <img src="https://s0.wp.com/latex.php?latex=Q%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q(s,a)" class="latex" /> can be approximated by linear basis functions</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28s%2Ca%29+%3D+%28%5Cphi_1%28s%2Ca%29%2C%5Cdots%2C%5Cphi_d%28s%2Ca%29%29%2C+%5Cquad+%5Cphi%28s%2Ca%29+%5Cin+%5Cmathbb%7BR%7D%5Ed.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(s,a) = (\phi_1(s,a),\dots,\phi_d(s,a)), \quad \phi(s,a) \in \mathbb{R}^d." class="latex" /></p>



<p>We assume that the dimension of the representation, <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" />, is low compared the state and action dimensions. The idea of using a linear function approximation in RL and dynamic programming is not new, and had been explored in early works by Shannon (<a href="https://vision.unipv.it/IA1/ProgrammingaComputerforPlayingChess.pdf">“Programming a digital computer for playing chess.”, Philosophical Magazine, 1950</a>) as well as Bellman and Dreyfus (<a href="https://www.rand.org/content/dam/rand/pubs/papers/2006/P1176.pdf">“Functional approximations and dynamic programming”, 1959</a>). There has also since been significant work on this approach, see e.g. <a href="https://dl.acm.org/doi/10.1145/203330.203343">Tesauro 1995</a>, <a href="http://www.mit.edu/~pucci/discountedLP.pdf">de Farias and Van Roy 2003</a>, <a href="https://arxiv.org/abs/1307.4847">Wen and Van Roy 2013</a>.</p>



<p>One natural question that arises is this: <strong>what conditions must the representation <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> satisfy in order for this approach to work?</strong></p>



<p>We proceed by studying the simplest possible case: assuming that the optimal <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />-function <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star" class="latex" /> is linearly realizable.</p>



<h4>RL with linearly realizable <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star" class="latex" /> function approximation: does there exist a sample efficient algorithm?</h4>



<p>Suppose we have access to a feature map <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28s%2Ca%29+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(s,a) \in \mathbb{R}^d" class="latex" />. Concretely, the assumption we consider is the following:</p>



<blockquote class="wp-block-quote"><p><strong>Assumption 1 (Linearly realizable <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star" class="latex" />)</strong>: Assume for all <img src="https://s0.wp.com/latex.php?latex=s%2Ca%2Ch+%5Cin+%5BH%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s,a,h \in [H]" class="latex" /> that there exists <img src="https://s0.wp.com/latex.php?latex=w_1%5E%5Cstar%2C%5Cdots%2Cw_H%5E%5Cstar+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_1^\star,\dots,w_H^\star \in \mathbb{R}^d" class="latex" /> such that<br /><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28s%2Ca%29+%3D+w_h%5E%5Cstar+%5Ccdot+%5Cphi%28s%2Ca%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(s,a) = w_h^\star \cdot \phi(s,a)." class="latex" /></p></blockquote>



<p>As an aside, with Assumption 1, we can consider the problem from a linear programming viewpoint. Note that:</p>



<ul><li>We have an underlying LP with <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" /> variables and <img src="https://s0.wp.com/latex.php?latex=O%28SA%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(SA)" class="latex" /> constraints.</li><li>The LP is specific to the dynamic programming problem at hand (and hence not general) because it encodes the Bellman optimality constraints.</li><li>We have sampling access (in the episodic setting).</li></ul>



<p>It may be tempting to think that Assumption 1 is sufficient to enable a sample-efficient algorithm for RL (if we assume we already know the representation <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" />). However, that is <strong>not</strong> true, as the following theorem from a very recent work demonstrates:</p>



<blockquote class="wp-block-quote"><p><a href="https://arxiv.org/abs/2010.01374"><strong>Theorem 1 (Weisz, Amortila, Szepesvári 2021):</strong></a> There exists an MDP and a representation <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> satisfying Assumption 1, such that any online RL algorithm (with knowledge of <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" />) requires <img src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmin%282%5Ed%2C2%5EH%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Omega(\min(2^d,2^H))" class="latex" /> samples to output the value <img src="https://s0.wp.com/latex.php?latex=V_1%5E%5Cstar%28s_1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_1^\star(s_1)" class="latex" /> up to constant additive error, with probability at least 0.9.</p></blockquote>



<p>While linear realizability alone is insufficient for sample efficiency in online RL, one might consider imposing further assumptions that could suffice for sample-efficient RL. One candidate assumption is to assume that at each state, the optimal action yields significantly more value than the next-best action:</p>



<blockquote class="wp-block-quote"><p><strong>Assumption 2 (Large suboptimality gap)</strong>: Assume for all <img src="https://s0.wp.com/latex.php?latex=a+%5Cneq+%5Cpi%5E%5Cstar%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a \neq \pi^\star(s)" class="latex" />, we have<br /><img src="https://s0.wp.com/latex.php?latex=V_h%5E%5Cstar%28s%29+-+Q_h%5E%5Cstar%28s%2Ca%29+%5Cgeq+%5Cfrac%7B1%7D%7B16%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_h^\star(s) - Q_h^\star(s,a) \geq \frac{1}{16}." class="latex" /></p></blockquote>



<p>Perhaps surprisingly, the following theorem shows that an exponential lower bound for online RL remains under <strong>both</strong> Assumption 1 and Assumption 2.</p>



<blockquote class="wp-block-quote"><p><a href="https://arxiv.org/abs/2103.12690"><strong>Theorem 2 (Wang, Wang, Kakade 2021):</strong></a> There exists an MDP and a representation <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> satisfying both Assumption 1 and Assumption 2, such that any online RL algorithm (with knowledge of <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" />) requires <img src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmin%282%5Ed%2C2%5EH%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Omega(\min(2^d,2^H))" class="latex" /> samples to output the value <img src="https://s0.wp.com/latex.php?latex=V_1%5E%5Cstar%28s_1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_1^\star(s_1)" class="latex" /> up to constant additive error, with probability at least 0.9.</p></blockquote>



<p><em><strong>Remark</strong></em>: We note a subtle distinction between the online RL setting and the simulator access setting. In the online RL setting, during each episode, we start at some state <img src="https://s0.wp.com/latex.php?latex=s_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_0" class="latex" />, and the subsequent states we see are entirely dependent on the policy we choose and the environment dynamics. Meanwhile, in the simulator access setting, at each time-step, we are free to input <strong>any</strong> <img src="https://s0.wp.com/latex.php?latex=%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(s,a)" class="latex" /> pair, and the simulator will return the next state <img src="https://s0.wp.com/latex.php?latex=s%27+%5Csim+P%28%5Ccdot+%5Cmid+s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s' \sim P(\cdot \mid s,a)" class="latex" />, as well as the reward <img src="https://s0.wp.com/latex.php?latex=r%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(s,a)" class="latex" />. While Theorem 1 (when only Assumption 1 is satisfied) holds for both online RL and simulator access, Theorem 2 (when both Assumption 1 and Assumption 2 hold) is valid only in the online RL setting. In the simulator access setting, <a href="https://arxiv.org/abs/1910.03016">Du et al. 2019</a> proved that there exists a sample-efficient approach (i.e. polynomial in all relevant problem parameters) to find an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-optimal policy, when both Assumption 1 and Assumption 2 hold. This demonstrates an <em>exponential separation</em> between the online RL and simulator access settings.</p>



<p>We next introduce the counterexample used to prove Theorem 2 in detail.</p>



<h4>Construction sketch for counterexample in Theorem 2</h4>



<figure class="wp-block-image"><img src="https://i.imgur.com/ReAcF8x.png" alt="" /></figure>



<p>Above, we have a pictorial representation of the MDP family in the counterexample. We first describe its state and action spaces.</p>



<ul><li>The state space is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbar%7B1%7D%2C%5Cdots%2C%5Cbar%7Bm%7D%2C+f%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\bar{1},\dots,\bar{m}, f}" class="latex" />. We use <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m" class="latex" /> to denote an integer, which we set to be approximately <img src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^d" class="latex" />.</li><li>State <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is a special state, which we can think of as a “terminal state”.</li><li>At state <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\bar{i}" class="latex" />, the feasible action set is <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D+%5Csetminus+%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[m] \setminus {i}" class="latex" />. At state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />, the feasible action set is <img src="https://s0.wp.com/latex.php?latex=%5Bm-1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[m-1]" class="latex" />. Hence, there are <img src="https://s0.wp.com/latex.php?latex=m-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m-1" class="latex" /> feasible actions at each state.</li><li>Each MDP in this “hard” family is specified by an index <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar+%5Cin+%5Bm%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star \in [m]" class="latex" /> and denoted by <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D_%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}_{a^\star}" class="latex" />.</li></ul>



<p>Before we proceed, we first recall the Johnson-Lindenstrauss lemma, which states that a set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that the distances between the points are nearly preserved.</p>



<blockquote class="wp-block-quote"><p><a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">Johnson-Lindenstrauss Lemma</a>: Suppose we are given <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cepsilon+%3C+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 &lt; \epsilon &lt; 1" class="latex" />, a set <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m" class="latex" /> points in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^N" class="latex" />, and a number <img src="https://s0.wp.com/latex.php?latex=n+%3E+8+%5Cln+m%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n &gt; 8 \ln m/\epsilon^2" class="latex" />. Then, there is a linear map <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathbb%7BR%7D%5EN+%5Cto+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathbb{R}^N \to \mathbb{R}^n" class="latex" /> such that<br /><img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29%7Cu-v%7C%5E2+%5Cleq+%7Cf%28u%29+-+f%28v%29%7C%5E2+%5Cleq+%281%2B%5Cepsilon%29%7Cu-v%7C%5E2.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(1-\epsilon)|u-v|^2 \leq |f(u) - f(v)|^2 \leq (1+\epsilon)|u-v|^2." class="latex" /></p></blockquote>



<p>Consider a collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{X}" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> orthogonal unit vectors in the high-dimensional space <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^N" class="latex" />. For any two vectors <img src="https://s0.wp.com/latex.php?latex=u%5Cneq+v+%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u\neq v \in \mathcal{X}" class="latex" />, after applying the linear embedding <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />, we observe by Johnson-Lindenstrauss that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cpm%5Cleft+%5Clangle+f%28u%29%2Cf%28v%29%5Cright+%5Crangle%3D+%5Cfrac%7B%7Cf%28u%29+%5Cpm+f%28v%29%7C%5E2+-+%7Cf%28u%29%7C%5E2+-+%7Cf%28v%29%7C%5E2%7D%7B2%7D+%5Cleq+%5Cfrac%7B2%281%2B%5Cepsilon%29+-+2%281-%5Cepsilon%29%7D%7B2%7D+%3D+2%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pm\left \langle f(u),f(v)\right \rangle= \frac{|f(u) \pm f(v)|^2 - |f(u)|^2 - |f(v)|^2}{2} \leq \frac{2(1+\epsilon) - 2(1-\epsilon)}{2} = 2\epsilon" class="latex" /></p>



<p>where we used the fact that <img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29+%5Cleq+%7Cf%28u%29+-+0%7C%5E2+%3D+%7Cf%28u%29%7C%5E2%5Cleq+%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(1-\epsilon) \leq |f(u) - 0|^2 = |f(u)|^2\leq (1+\epsilon)" class="latex" /> holds for any unit <img src="https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u \in \mathcal{X}" class="latex" /> by linearity of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. Hence, we can apply Johnson-Lindenstrauss to derive the following lemma, which will be useful in our construction.</p>



<blockquote class="wp-block-quote"><p><a href="https://arxiv.org/pdf/2103.12690.pdf">Lemma 1 (Johnson-Lindenstrauss)</a>: For any <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma &gt; 0" class="latex" />, there exists <img src="https://s0.wp.com/latex.php?latex=m+%3D+%5Cleft%5Clfloor%5Cexp%5Cleft%28%5Cfrac%7B1%7D%7B8%7D+%5Cgamma%5E2+d%5Cright%29%5Cright%5Crfloor&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m = \left\lfloor\exp\left(\frac{1}{8} \gamma^2 d\right)\right\rfloor" class="latex" /> unit vectors <img src="https://s0.wp.com/latex.php?latex=%7Bv_1%2C%5Cdots%2Cv_m%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{v_1,\dots,v_m}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Cforall+i%2Cj+%5Cin+%5Bm%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall i,j \in [m]" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i \neq j" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Clangle+v_i%2Cv_j+%5Crangle%7C+%5Cleq+%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\langle v_i,v_j \rangle| \leq \gamma" class="latex" />.</p></blockquote>



<p>Throughout our discussion, we will set <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3A%3D+%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma := \frac{1}{4}" class="latex" />.</p>



<p>Equipped with the lemma above, we can now describe the transitions, features and rewards of the constructed MDP family. In the sequel, <img src="https://s0.wp.com/latex.php?latex=a_1+%5Cin+%5Bm%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_1 \in [m]" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=a_2+%5Cin+%5Bm%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2 \in [m]" class="latex" /> represent integers associated with the state and action respectively.</p>



<p><em><strong>Transitions</strong></em>: The initial state <img src="https://s0.wp.com/latex.php?latex=s_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_0" class="latex" /> follows the uniform distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Cmathrm%7BUnif%7D%5Cleft%28%7B%5Cbar%7B1%7D%2C%5Cdots%2C%5Cbar%7Bm%7D%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu = \mathrm{Unif}\left({\bar{1},\dots,\bar{m}}\right)" class="latex" />. The transition probabilities are set as follows:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/image-1.png"><img src="https://windowsontheory.files.wordpress.com/2021/04/image-1.png?w=1024" alt="" class="wp-image-8096" /></a></figure>



<p>After taking action <img src="https://s0.wp.com/latex.php?latex=a_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2" class="latex" />, the next state is either <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba_2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a_2}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. We might observe then that this MDP resembles a “leaking complete graph”. It is possible to visit any other state (except for <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a^\star}" class="latex" />). However, importantly, there is at least <img src="https://s0.wp.com/latex.php?latex=1+-+3%5Cgamma+%3D+%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 - 3\gamma = \frac{1}{4}" class="latex" /> probability of going to the terminal state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. Also, observe that the transition probabilities are indeed valid, since by Lemma 1 above, <img src="https://s0.wp.com/latex.php?latex=0+%3C+%5Cgamma+%5Cleq+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a_2%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5Cleq+3%5Cgamma+%3C+1.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 &lt; \gamma \leq \left\langle v(a_1),v(a_2) \right\rangle + 2\gamma \leq 3\gamma &lt; 1." class="latex" /></p>



<p><em><strong>Features</strong></em>: The feature map, which maps state-action pairs to <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" />-dimensional vectors, is defined as<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3A%3D+%5Cleft%28%5Cleft%5Clangle+v%28a_1%29%2Cv%28a_2%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5Cright%29+v%28a_2%29%2C+%5C%3B+%5Cforall+a_1%2Ca_2+%5Cin+%5Bm%5D%2C+a_1+%5Cneq+a_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(\overline{a_1},a_2) := \left(\left\langle v(a_1),v(a_2) \right\rangle + 2\gamma \right) v(a_2), \; \forall a_1,a_2 \in [m], a_1 \neq a_2" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28f%2C%5Ccdot%29+%3A%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(f,\cdot) := 0" class="latex" /></p>



<p>Note that the feature map is independent of <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" /> and is shared across the MDP family.</p>



<p><em><strong>Rewards</strong></em>: For <img src="https://s0.wp.com/latex.php?latex=1+%5Cleq+h+%5Cleq+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1 \leq h \leq H" class="latex" />, the rewards are defined as</p>



<p><img src="https://s0.wp.com/latex.php?latex=R_h%28%5Coverline%7Ba_1%7D%2Ca%5E%5Cstar%29+%3A%3D+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a%5E%5Cstar%29+%5Cright%5Crangle+%2B+2+%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_h(\overline{a_1},a^\star) := \left\langle v(a_1),v(a^\star) \right\rangle + 2 \gamma" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=R_h%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3A%3D+-2%5Cgamma+%5Cleft%5B%5Cleft%5Clangle+v%28a_1%29%2Cv%28a_2%29+%5Cright%5Crangle+%2B+2%5Cgamma%5Cright%5D%2C+%5C%3B+%28a_2+%5Cneq+a%5E%5Cstar%2C+a_2+%5Cneq+a_1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_h(\overline{a_1},a_2) := -2\gamma \left[\left\langle v(a_1),v(a_2) \right\rangle + 2\gamma\right], \; (a_2 \neq a^\star, a_2 \neq a_1)" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=R_h%28f%2C%5Ccdot%29+%3A%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_h(f,\cdot) := 0" class="latex" /></p>



<p>For <img src="https://s0.wp.com/latex.php?latex=h+%3D+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h = H" class="latex" />, we set <img src="https://s0.wp.com/latex.php?latex=r_H%28s%2Ca%29+%3A%3D+%5Clangle+%5Cphi%28s%2Ca%29%2C+v%28a%5E%5Cstar%29%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r_H(s,a) := \langle \phi(s,a), v(a^\star)\rangle" class="latex" /> for every state-action pair.</p>



<p>We now verify that our construction satisfies both the linear realizability and large suboptimality gap assumptions (Assumption 1 and Assumption 2).</p>



<blockquote class="wp-block-quote"><p><strong>Lemma (Linear realizability)</strong>. For all <img src="https://s0.wp.com/latex.php?latex=%28s%2Ca%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(s,a)" class="latex" />, we have <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28s%2Ca%29+%3D+%5Clangle+%5Cphi%28s%2Ca%29%2C+v%28a%5E%5Cstar%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(s,a) = \langle \phi(s,a), v(a^\star) \rangle" class="latex" />.</p></blockquote>



<p><em><strong>Proof</strong></em>: Throughout, we assume that <img src="https://s0.wp.com/latex.php?latex=a_2+%5Cneq+a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2 \neq a^\star" class="latex" />. We first verify the statement for the terminal state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. At the state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />, regardless of the action taken, the next state is always <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> and the reward is always 0. Hence, <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28f%2C%5Ccdot%29+%3D+V_h%5E%5Cstar%28f%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(f,\cdot) = V_h^\star(f) = 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5BH%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in [H]" class="latex" />. Thus, <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28f%2C%5Ccdot%29+%3D+%5Clangle+%5Cphi%28f%2C%5Ccdot%29%2Cv%28a%5E%5Cstar%29%5Crangle+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(f,\cdot) = \langle \phi(f,\cdot),v(a^\star)\rangle = 0" class="latex" />. We next verify realizability for other states via backwards induction on <img src="https://s0.wp.com/latex.php?latex=h+%3D+H%2C+H-1%2C%5Cdots%2C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h = H, H-1,\dots,1" class="latex" />. The inductive hypothesis is <img src="https://s0.wp.com/latex.php?latex=%5Cforall+a_1+%5Cin+%5Bm%5D%2C+a_2+%5Cneq+a_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall a_1 \in [m], a_2 \neq a_1" class="latex" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3D+%5Cleft%28%5Cleft%5Clangle+v%28a_1%29%2Cv%28a_2%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5Cright%29+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a%5E%5Cstar%29+%5Cright%5Crangle.+%5C%3B%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(\overline{a_1},a_2) = \left(\left\langle v(a_1),v(a_2) \right\rangle + 2\gamma \right) \left\langle v(a_1),v(a^\star) \right\rangle. \;(1)" class="latex" /></p>



<p>and that <img src="https://s0.wp.com/latex.php?latex=%5Cforall+a_1+%5Cneq+a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall a_1 \neq a^\star" class="latex" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=V_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%29+%3D+Q_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%2Ca%5E%5Cstar%29+%3D+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a%5E%5Cstar%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5C%3B+%282%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_h^\star(\overline{a_1}) = Q_h^\star(\overline{a_1},a^\star) = \left\langle v(a_1),v(a^\star) \right\rangle + 2\gamma \; (2)" class="latex" /></p>



<p>When <img src="https://s0.wp.com/latex.php?latex=h+%3D+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h = H" class="latex" />, (1) holds by the definition of the rewards at that level. Next, note that <img src="https://s0.wp.com/latex.php?latex=%5Cforall+h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall h" class="latex" />, (2) follows from (1). This is because for <img src="https://s0.wp.com/latex.php?latex=a_2+%5Cneq+a%5E%5Cstar%2Ca_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2 \neq a^\star,a_1" class="latex" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3D+%5Cleft%28%5Cleft%5Clangle+v%28a_1%29%2Cv%28a_2%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5Cright%29+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a%5E%5Cstar%29+%5Cright%5Crangle+%5Cleq+3%5Cgamma%5E2%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(\overline{a_1},a_2) = \left(\left\langle v(a_1),v(a_2) \right\rangle + 2\gamma \right) \left\langle v(a_1),v(a^\star) \right\rangle \leq 3\gamma^2," class="latex" /></p>



<p>while (recall <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3A%3D+1%2F4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma := 1/4" class="latex" />)</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%2Ca%5E%5Cstar%29+%3D+%5Cleft%5Clangle+v%28a_1%29%2Cv%28a%5E%5Cstar%29+%5Cright%5Crangle+%2B+2%5Cgamma+%5Cgeq+%5Cgamma+%3E+3%5Cgamma%5E2.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(\overline{a_1},a^\star) = \left\langle v(a_1),v(a^\star) \right\rangle + 2\gamma \geq \gamma &gt; 3\gamma^2." class="latex" /></p>



<p>This means that proving (1) suffices to show that <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" /> is always the optimal action. A simple verification via Bellman’s optimality equation suffices to prove the inductive hypothesis for (1) for every <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5BH%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in [H]" class="latex" />, since the base case <img src="https://s0.wp.com/latex.php?latex=h+%3D+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h = H" class="latex" /> holds. Thus, both (1) and (2) hold for all <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5BH%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in [H]" class="latex" />, concluding our proof. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>We next show that the constant suboptimality gap (Assumption 2) is also (approximately) satisfied by our constructed MDP family.</p>



<blockquote class="wp-block-quote"><p><strong>Lemma (Suboptimality gap)</strong>. For all state <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba_1%7D+%5Cneq+%5Coverline%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a_1} \neq \overline{a^\star}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=a_2+%5Cneq+a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2 \neq a^\star" class="latex" />, the suboptimality gap is<br /><img src="https://s0.wp.com/latex.php?latex=%5CDelta_h%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3A%3D+V_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%29+-Q_h%5E%5Cstar%28%5Coverline%7Ba_1%7D%2Ca_2%29+%3E+%5Cgamma+-+3%5Cgamma%5E2+%5Cgeq+%5Cfrac%7B1%7D%7B4%7D+%5Cgamma.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_h(\overline{a_1},a_2) := V_h^\star(\overline{a_1}) -Q_h^\star(\overline{a_1},a_2) &gt; \gamma - 3\gamma^2 \geq \frac{1}{4} \gamma." class="latex" /><br />Hence, in this MDP, Assumption 2 is satisfied with <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7B%5Cmin%7D+%5Cgeq+%5Cfrac%7B1%7D%7B4%7D%5Cgamma+%3D+%5COmega%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{\min} \geq \frac{1}{4}\gamma = \Omega(1)" class="latex" />.</p></blockquote>



<p><em><strong>Remark</strong></em>. Note that that here we ignored the terminal state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> and the essentially unreachable state <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a^\star}" class="latex" /> for simplicity. This seems reasonable intuitively, since reaching <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is effectively the end of the episode, and the state <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a^\star}" class="latex" /> can only be reached with negligible probability(recall that <img src="https://s0.wp.com/latex.php?latex=m+%5Capprox+2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m \approx 2^d" class="latex" /> is exponentially large). For a more rigorous treatment of this issue, refer to Appendix B in <a href="https://arxiv.org/pdf/2103.12690.pdf">Wang, Wang, Kakade 2021</a>.</p>



<p>We can now state and prove the following key technical lemma, which directly implies Theorem 2 in <a href="https://arxiv.org/pdf/2103.12690.pdf">Wang, Wang, Kakade 2021</a>.</p>



<blockquote class="wp-block-quote"><p><em><strong>Lemma</strong></em>. For any algorithm, there exists <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar+%5Cin+%5Bm%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star \in [m]" class="latex" /> such that in order to output <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" /> with<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bs_1+%5Csim+%5Cmu%7D+V_1%5E%7B%5Cpi%7D%28s_0%29+%5Cgeq+%5Cmathbb%7BE%7D_%7Bs_1+%5Csim+%5Cmu%7D+V_1%5E%5Cstar%28s_1%29+-+0.05&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{s_1 \sim \mu} V_1^{\pi}(s_0) \geq \mathbb{E}_{s_1 \sim \mu} V_1^\star(s_1) - 0.05" class="latex" /><br />with probability at least 0.1 for <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D_%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}_{a^\star}" class="latex" />, the number of samples required is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5COmega%28%5Cmin%28d%2CH%29%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^{\Omega(\min(d,H))}" class="latex" />.</p></blockquote>



<p><strong>Proof sketch</strong>. We take an information-theoretic perspective. Observe that the feature map of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D_%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}_{a^\star}" class="latex" /> does not depend on <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />, and that for <img src="https://s0.wp.com/latex.php?latex=h+%3C+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h &lt; H" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=a_2+%5Cneq+a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2 \neq a^\star" class="latex" />, the reward <img src="https://s0.wp.com/latex.php?latex=R_h%28%5Coverline%7Ba_1%7D%2Ca_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R_h(\overline{a_1},a_2)" class="latex" /> also has no information about <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />. The transition probabilities are also independent of <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />, unless the action <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" /> is taken, and the reward at state <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is always 0. Thus, to receive information about the optimal action <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />, the agent either needs to take the action <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />, or be a non-game-over state at the final time step <img src="https://s0.wp.com/latex.php?latex=h%3D+H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h= H" class="latex" /> (i.e <img src="https://s0.wp.com/latex.php?latex=s_H+%5Cneq+f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_H \neq f" class="latex" />.)</p>



<p>However, by the design of the transition probabilities, the probability of remaining at a non-game-over state at the next time step is at most</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Csup_%7Ba_1+%5Cneq+a_2%7D+%5Clangle+v%28a_1%29%2Cv%28a_2%29%5Crangle+%2B+2%5Cgamma+%5Cleq+3%5Cgamma+%5Cleq+%5Cfrac%7B3%7D%7B4%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sup_{a_1 \neq a_2} \langle v(a_1),v(a_2)\rangle + 2\gamma \leq 3\gamma \leq \frac{3}{4}." class="latex" /></p>



<p>Hence, for any algorithm, <img src="https://s0.wp.com/latex.php?latex=P%28s_H+%5Cneq+f%29+%5Cleq+%5Cleft%28%5Cfrac%7B3%7D%7B4%7D%5Cright%29%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(s_H \neq f) \leq \left(\frac{3}{4}\right)^H" class="latex" />, which is exponentially small.</p>



<p>Summarizing, any algorithm that does not know <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" /> either needs to “get lucky” so that <img src="https://s0.wp.com/latex.php?latex=s_H+%3D+f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_H = f" class="latex" />, or take the optimal action <img src="https://s0.wp.com/latex.php?latex=a%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a^\star" class="latex" />. For each episode, the first event happens with probability less than <img src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cfrac%7B3%7D%7B4%7D%5Cright%29%5EH&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left(\frac{3}{4}\right)^H" class="latex" />, and the second event happens with probability less than <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BH%7D%7Bm-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{H}{m-1}" class="latex" />. Since the number of actions is <img src="https://s0.wp.com/latex.php?latex=m-1+%3D+2%5E%7B%5CTheta%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m-1 = 2^{\Theta(d)}" class="latex" />, it follows that neither event can happen with constant probability unless the number of episodes is exponential in <img src="https://s0.wp.com/latex.php?latex=%5Cmin%28d%2CH%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min(d,H)" class="latex" />. This wraps up the sketch. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<p>We note that our construction is not quite rigorous due to the remark earlier that the suboptimality gap assumption does not hold for the states <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Coverline%7Ba%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\overline{a^\star}" class="latex" />. A more rigorous construction can be found in Appendix B of <a href="https://arxiv.org/pdf/2103.12690.pdf">Wang, Wang, Kakade 2021</a>. Note that this lower bound is silent on the dependence of the sample complexity on the size of the action space, giving rise to the following question, which appears to be still unsolved.</p>



<blockquote class="wp-block-quote"><p><strong>Open problem</strong>: Could we get a lower bound that also depends on the action space dimension <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" />, such that the number of samples required to obtain an approximately optimal policy scales with<br /><img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Cmin%28A%2Cd%2CH%29%29%3F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(\min(A,d,H))?" class="latex" /></p></blockquote>



<h2>Interlude: do these lower bounds matter in practice?</h2>



<p>A natural question to ask is this: are these exponential lower bounds in Part 2 (when we only assume linear realizability) actually relevant for practice?</p>



<p>To answer this, we take a brief detour into <em>offline RL</em>. In offline RL (see <a href="https://arxiv.org/abs/2005.01643">Levine et al. 2020</a> for a survey), we assume that the agent has no direct access to the MDP, and is instead provided with a static dataset of transitions, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D+%3D+%7B%7Bs_h%5Ei%2Ca_h%5Ei%2Cr_h%5Ei%2Cs_%7Bh%2B1%7D%5Ei%7D_%7Bh%3D1%7D%5E%7BH%7D%7D_%7Bi%3D1%7D%5Em&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D} = {{s_h^i,a_h^i,r_h^i,s_{h+1}^i}_{h=1}^{H}}_{i=1}^m" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="m" class="latex" /> denotes number of independent episodes in the offline data). The goal here could be to learn a policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi%28a%5Cmid+s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi(a\mid s)" class="latex" /> (based on the static dataset <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />) that attains the largest possible cumulative reward when applied to the MDP, or to evaluate the performance of some target policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi%28a+%5Cmid+s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi(a \mid s)" class="latex" /> based on the offline data. We use <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_{\beta}" class="latex" /> to denote the distribution over states and actions in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" />, such that we assume the state-action tuples <img src="https://s0.wp.com/latex.php?latex=%28s%2Ca%29+%5Cin+%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(s,a) \in \mathcal{D}" class="latex" /> are sampled according to <img src="https://s0.wp.com/latex.php?latex=s+%5Csim+d%5E%7B%5Cpi_%7B%5Cbeta%7D%7D%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s \sim d^{\pi_{\beta}}(s)" class="latex" />, and the actions are sampled according to the behaviour policy, such that <img src="https://s0.wp.com/latex.php?latex=a+%5Csim+%5Cpi_%7B%5Cbeta%7D%28a+%5Cmid+s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a \sim \pi_{\beta}(a \mid s)" class="latex" />.</p>



<p>Analogous to the online RL lower bound, the following theorem shows that linear realizability is also insufficient for sample-efficient evaluation of a target policy using offline data.</p>



<blockquote class="wp-block-quote"><p><strong>Theorem (informal, from <a href="https://arxiv.org/pdf/2010.11895.pdf">Wang, Foster, Kakade 2020</a>))</strong>. In the offline RL setting, suppose the data distributions have (polynomially) lower bounded eigenvalues, and the <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />-functions of every policy are linear with respect to a given feature mapping. Then, any algorithm requires an exponential number of samples in the horizon <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> to output a non-trivially accurate estimate of the value of any given policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi" class="latex" />, with constant probability.</p></blockquote>



<p>Some remarks are in order. First, note that the above hardness result for policy evaluation also holds for finding near-optimal policies using offline data. For a simple reduction, consider an example where at the initial state, one action <img src="https://s0.wp.com/latex.php?latex=a_1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_1" class="latex" /> leads to a fixed reward and another action <img src="https://s0.wp.com/latex.php?latex=a_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_2" class="latex" /> transits us to an instance which is hard to evaluate using offline data. Then, in order to find a good policy, it is necessary for the agent to approximately evaluate the value of the optimal policy in the hard instance. Second, an appropriate eigenvalue lower bound on the offline data distribution ensures that there is sufficient feature coverage in the dataset, without which linear realizability alone is clearly insufficient for sample-efficient estimation. Third, note that the representation condition in the theorem is significantly stronger than assuming than assuming realizability with regards to only a single target policy, and so the result carries over to the latter setting as well. Fourth, the key idea to prove the result is the error amplification (exponential in the horizon <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" />) induced by the <em>distribution shift</em> from the offline policy to the target policy we wish to evaluate.</p>



<p>Empirical work performed in <a href="https://arxiv.org/pdf/2103.04947.pdf">Wang et al. 2021</a> show that the these negative results do manifest themselves in experimental examples. The methology considered by <a href="https://arxiv.org/pdf/2103.04947.pdf">Wang et al. 2021</a> is as follows:</p>



<ol><li>Decide on a target policy to be evaluated, along with a good feature mapping for this policy (could be the last layer of a deep neural network trained to evaluate the policy).</li><li>Collect offline data using trajectories that are a mixture of the target policy and another distribution (perhaps generated by a random policy).</li><li>Run offline RL methods to evaluate the target policy using feature mapping found in Step 1 and the offline data obtained in Step 2.</li></ol>



<p>We note that features extracted from pre-trained deep neural networks should be able to satisfy the linear realizibility assumption approximately (for the target policy). Moreover, the offline dataset is relatively favorable for evaluation of the target policy, since we would not expect realistic offline datasets to have a large number of trajectories from the target policy itself.</p>



<p>However, numerical results show substantial degradation in the accuracy of policy evaluation, even for a relatively mild distribution shift (e.g. where there is a 50/50 split in target policy and random policy in the offline data). As an example, consider the following plot.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/WUpg5nW.png" alt="" /></figure>



<p>The figure above depicts the performance of Fitted Q-Iteration (FQI) on Walker2d-v2, an environment from the <a href="https://github.com/openai/gym">OpenAI gym benchmark suite</a> which has continuous action space. Here, the <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />-axis is the number of rounds of FQI used, and the <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" />-axis is the square root of the mean squared error of the predicted values (smaller is better). The blue line corresponds to performance when the dataset is generated by the target policy itself with 1 million samples, and other lines correspond to the performance when adding more offline data induced by random trajectories. As we can see, adding more random trajectories lead to significant degradation of FQI. See <a href="https://arxiv.org/pdf/2103.04947.pdf">Wang et al. 2021</a> for more such experiments.</p>



<p>These empirical results seem to affirm the hardness results in <a href="https://arxiv.org/pdf/2103.04947.pdf">Wang et al. 2021</a> (offline RL) and <a href="https://arxiv.org/pdf/2103.12690.pdf">Wang, Wang, Kakade 2021</a> (online RL), in that the definition of a good representation in RL is more subtle than in supervised learning, and certainly goes beyond just linear realizibility.</p>



<h2>Part 3: Sufficient conditions for provable generalization in RL</h2>



<p>We have seen from Part 1 and Part 2 that finding an <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" />-optimal policy with mild (e.g. logarithmic) dependence on <img src="https://s0.wp.com/latex.php?latex=S%2CA&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S,A" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=poly%28H%2C1%2F%5Cepsilon%2C%5Cmbox%7B%22complexity+measure%22%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="poly(H,1/\epsilon,\mbox{&quot;complexity measure&quot;})" class="latex" /> samples is <strong>NOT</strong> possible agnostically, or even with linearly realizable <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" />. This leads us to the following question.</p>



<p><strong>Q: What kind of assumptions enable provable generalization in RL?</strong></p>



<p>In fact, under various stronger assumptions, sample-efficient generalization <strong>is</strong> possible in many special cases. Amongst others, these include</p>



<ul><li>Linear Bellman Completion [<a href="https://www.aaai.org/Library/AAAI/2005/aaai05-159.php">Munos 2005</a>, <a href="https://arxiv.org/abs/2003.00153">Zanette et al. 2020</a>]<ul><li>Linear MDPs (low-rank transition matrix) [<a href="https://arxiv.org/abs/1902.04779">Wang and Yang 2018</a>; <a href="https://arxiv.org/abs/1907.05388">Jin et al. 2019</a>]</li><li>Linear Quadratic Regulators (LQR): standard control theory model (see e.g. <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">Wikipedia page for LQR</a>)</li></ul></li><li>FLAMBE/Feature Selection: <a href="https://arxiv.org/abs/2006.10814">Agarwal, Kakade, Krishnamurthy, Sun 2020</a></li><li>Linear Mixture MDPs: [<a href="http://proceedings.mlr.press/v108/modi20a/modi20a.pdf">Modi et al. 2020</a>, <a href="http://proceedings.mlr.press/v119/ayoub20a/ayoub20a.pdf">Ayoub et al. 2020</a>]</li><li>Block MDPs <a href="https://arxiv.org/pdf/1901.09018.pdf">Du et al. 2019</a></li><li>Factored MDPs <a href="https://arxiv.org/abs/1811.08540">Sun et al 2019</a></li><li>Kernelized Nonlinear Regulator <a href="https://arxiv.org/abs/2006.12466">Kakade et al. 2020</a></li></ul>



<p>What structural commonalities are shared between these underlying assumptions and models? To answer this question, we go back to the start, and revisit the case of linear bandits (<img src="https://s0.wp.com/latex.php?latex=H%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H=1" class="latex" /> RL problem) for intuition.</p>



<h3>Intuition from properties satisfied by linear bandits</h3>



<p>We consider linear contextual bandits, where the context is <img src="https://s0.wp.com/latex.php?latex=s+%5Cin%5Cmathcal%7BS%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s \in\mathcal{S}" class="latex" />, the action is <img src="https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a \in \mathcal{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BS%7D%2C+%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{S}, \mathcal{A}" class="latex" /> denote the state (or context) and action space respectively. We assume as before that associated with each state-action pair is a representation <img src="https://s0.wp.com/latex.php?latex=%5Cphi%28s%2Ca%29+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(s,a) \in \mathbb{R}^d" class="latex" />. The observed reward is <img src="https://s0.wp.com/latex.php?latex=r%28s%2Ca%29+%3D+w%5E%5Cstar+%5Ccdot+%5Cphi%28s%2Ca%29+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(s,a) = w^\star \cdot \phi(s,a) + \epsilon" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> is a mean-zero stochastic noise term. The hypothesis class is</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D+%3D+%7Bf%28s%2Ca%29+%3D+w%28f%29+%5Ccdot+%5Cphi%28s%2Ca%29%2C+w%28f%29+%5Cin+%5Cmathcal%7BW%7D%7D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F} = {f(s,a) = w(f) \cdot \phi(s,a), w(f) \in \mathcal{W}}," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{W}" class="latex" /> is a subset of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" />. We let <img src="https://s0.wp.com/latex.php?latex=%5Cpi_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_f" class="latex" /> denote the greedy policy for <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />, i.e.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cpi_f%28s%29+%3D+%5Cmathrm%7Bargmax%7D_%7Ba%7D+f%28s%2Ca%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_f(s) = \mathrm{argmax}_{a} f(s,a)." class="latex" /></p>



<p>An important structural property satisfied by linear contextual bandits is the following: <em>data reuse</em>. Indeed, the difference between any <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{F}" class="latex" /> and the observed reward <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" /> is estimable when we had in fact played <img src="https://s0.wp.com/latex.php?latex=%5Cpi_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_g" class="latex" /> for some hypothesis <img src="https://s0.wp.com/latex.php?latex=g+%5Cneq+f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g \neq f" class="latex" />. Via direct calculation, we see that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%28s%2Ca%29+%5Csim+%5Cpi_g%7D+%5Bf%28s%2Ca%29+-+r%28s%2Ca%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{(s,a) \sim \pi_g} [f(s,a) - r(s,a)]" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=%3D+%5Cmathbb%7BE%7D_%7B%28s%2Ca%29+%5Csim+%5Cpi_g%7D+%5Bw%28f%29+%5Ccdot+%5Cphi%28s%2Ca%29+-+%28w%5E%5Cstar+%5Ccdot+%5Cphi%28s%2Ca%29+%2B+%5Cepsilon%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \mathbb{E}_{(s,a) \sim \pi_g} [w(f) \cdot \phi(s,a) - (w^\star \cdot \phi(s,a) + \epsilon)]" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=%3D+%5Cleft%5Clangle+w%28f%29+-+w%5E%5Cstar%2C+%5Cmathbb%7BE%7D_%7B%28s%2Ca%29+%5Csim+%5Cpi_g%7D+%5B%5Cphi%28s%2Ca%29%5D+%5Cright%5Crangle.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \left\langle w(f) - w^\star, \mathbb{E}_{(s,a) \sim \pi_g} [\phi(s,a)] \right\rangle." class="latex" /></p>



<p>Intuitively, assuming that <img src="https://s0.wp.com/latex.php?latex=%5Cpi_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_g" class="latex" /> induces “sufficient exploration” of the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" />-dimensional representation space, this implies that we can evaluate the quality of any policy/hypothesis <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{F}" class="latex" /> using just the one set of data collected by <img src="https://s0.wp.com/latex.php?latex=%5Cpi_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_g" class="latex" />. This is <strong>precisely</strong> the kind of data reuse property we saw for supervised learning, which enables sample-efficient generalization there. This suggests that to ensure sample-efficient generalization in general RL, it may be fruitful to look for assumptions that enable data reuse. One special case where such data reuse is possible is the class of linear Bellman complete models.</p>



<h3>Special case: linear Bellman complete class</h3>



<p>Let <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> be the length of each episode as before. We recall that a hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" /> is <strong>realizable</strong> for an MDP <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}" class="latex" /> if there exists a hypothesis <img src="https://s0.wp.com/latex.php?latex=f%5E%5Cstar+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f^\star \in \mathcal{H}" class="latex" /> such that</p>



<p><img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar%28s%2Ca%29+%3D+Q_%7Bh%7D%5E%7Bf%5E%5Cstar%7D%28s%2Ca%29+%5Cquad+%5Cforall+h+%5Cin+%5Bh%5D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star(s,a) = Q_{h}^{f^\star}(s,a) \quad \forall h \in [h]," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=Q_h%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^\star" class="latex" /> is the optimal state-action value at time step <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" />. Having defined realizability, we are now ready to define the notion of linear Bellman completeness.</p>



<p>For any <img src="https://s0.wp.com/latex.php?latex=f+%3D+%28%5Ctheta_1%2C%5Cdots%2C%5Ctheta_%7BH%7D%29+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f = (\theta_1,\dots,\theta_{H}) \in \mathcal{H}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%5Cpi_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_f" class="latex" /> be the greedy policy associated with <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. By the definition of a linear Bellman complete class, it follows that given some fixed <img src="https://s0.wp.com/latex.php?latex=g+%3D+%28%5Ctheta_1%5Eg%2C%5Cdots%2C%5Ctheta_%7BH%7D%5Eg%29+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g = (\theta_1^g,\dots,\theta_{H}^g) \in \mathcal{H}" class="latex" />, for any <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" />, we have<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bs_h%2Ca_h%2C+s%7Bh%2B1%7D+%5Csim+%5Cpi_g%7D+%5Cleft%5BQ_%7Bh%7D%5Ef%28s_h%2Ca_h%29+-+r%28s_h%2Ca_h%29+-+V_%7Bh%2B1%7D%5Ef%28s_%7Bh%2B1%7D%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{s_h,a_h, s{h+1} \sim \pi_g} \left[Q_{h}^f(s_h,a_h) - r(s_h,a_h) - V_{h+1}^f(s_{h+1}) \right]" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=%3D+%5Cmathbb%7BE%7D_%7Bs_h%2C+a_h+%5Csim+%5Cpi_g%7D+%5Cleft%5B+%28%5Ctheta_h+-+%5Cmathcal%7BT%7D_h%28%5Ctheta%7Bh%2B1%7D%29+%5Ccdot+%5Cphi%28s_h%2Ca_h%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \mathbb{E}_{s_h, a_h \sim \pi_g} \left[ (\theta_h - \mathcal{T}_h(\theta{h+1}) \cdot \phi(s_h,a_h) \right]" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=%3D+%28%5Ctheta_h+-+%5Cmathcal%7BT%7Dh%28%5Ctheta%7Bh%2B1%7D%29+%5Ccdot+%5Cmathbb%7BE%7D_%7Bs_h%2C+a_h+%5Csim+%5Cpi_g%7D+%5Cleft%5B+%5Cphi%28s_h%2Ca_h%29+%5Cright%5D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= (\theta_h - \mathcal{T}h(\theta{h+1}) \cdot \mathbb{E}_{s_h, a_h \sim \pi_g} \left[ \phi(s_h,a_h) \right]." class="latex" /></p>



<blockquote class="wp-block-quote"><p><strong>Definition (linear Bellman complete)</strong>. A hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" />, with respect to some known feature <img src="https://s0.wp.com/latex.php?latex=%5Cphi%3A%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BA%7D+%5Cmapsto+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi:\mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}^d" class="latex" />, is linear Bellman complete for an MDP <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" /> is realizable and there exists <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D_h%3A+%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}_h: \mathbb{R}^d \to \mathbb{R}^d" class="latex" /> such that for all <img src="https://s0.wp.com/latex.php?latex=%28%5Ctheta_1%2C%5Cdots%2C%5Ctheta%3C_%7BH%7D%29+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\theta_1,\dots,\theta&lt;_{H}) \in \mathcal{H}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5BH%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in [H]" class="latex" />,<br /><img src="https://s0.wp.com/latex.php?latex=T_h%28%5Ctheta_%7Bh%2B1%7D%29+%5Ccdot+%5Cphi%28s%2Ca%29+%3D+r%28s%2Ca%29+%2B+%5Cmathbb%7BE%7D_%7Bs%27+%5Csim+P_h%28s%2Ca%29%7D+%5Cleft%5B%5Cmax_%7Ba%27+%5Cin+%5Cmathcal%7BA%7D%7D+%5Ctheta_%7Bh%2B1%7D%5E%5Ctop+%5Cphi%28s%27%2Ca%27%29+%5Cright%5D%2C+%5Cquad+%5Cforall+%28s%2Ca%29+%5Cin+%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BA%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T_h(\theta_{h+1}) \cdot \phi(s,a) = r(s,a) + \mathbb{E}_{s' \sim P_h(s,a)} \left[\max_{a' \in \mathcal{A}} \theta_{h+1}^\top \phi(s',a') \right], \quad \forall (s,a) \in \mathcal{S} \times \mathcal{A}." class="latex" /></p></blockquote>



<p>This shows that data reuse is possible for any linear Bellman complete class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" />, since any <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{H}" class="latex" /> can be evaluated using offline data collected by some fixed policy <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" />.</p>



<p>As an aside, note that linear Bellman completeness is a very strong condition that can break when new features are added. This is because adding new features expands the hypothesis space (of linear functions), and there is no guarantee that the new hypothesis class will again satisfy linear Bellman completeness.</p>



<p>It turns out that linear Bellman complete classes are just one example of Bilinear Classes (<a href="https://arxiv.org/pdf/2103.10897.pdf">Du et al. 2021</a>), which encompass many RL models in which sample-efficient generalization has been shown to be possible.</p>



<h3>Bilinear Classes: structural properties to enable generalization in RL</h3>



<p>We assume access to a hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+%3D+%5Cmathcal%7BH%7D_1+%5Ctimes+%5Cdots+%5Ctimes+%5Cmathcal%7BH%7D_%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H} = \mathcal{H}_1 \times \dots \times \mathcal{H}_{H}" class="latex" />, which can be abstract sets that permit for both model-based and value-based hypotheses. We assume that for all <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{H}" class="latex" />, there is an associated state-action value function <img src="https://s0.wp.com/latex.php?latex=Q_h%5Ef&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_h^f" class="latex" /> and a value function <img src="https://s0.wp.com/latex.php?latex=V_h%5Ef&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V_h^f" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B1%2C%5Cdots%2CH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in {1,\dots,H}" class="latex" />. As before, let <img src="https://s0.wp.com/latex.php?latex=%5Cpi_%7Bh%7D%5E%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_{h}^{f}" class="latex" /> denote the greedy policy with respect to <img src="https://s0.wp.com/latex.php?latex=Q_%7Bh%7D%5E%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{h}^{f}" class="latex" />, and let <img src="https://s0.wp.com/latex.php?latex=%5Cpi_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pi_f" class="latex" /> denote <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_%7Bh%7D%5E%7Bf%7D%7D_%7Bh%3D1%7D%5E%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\pi_{h}^{f}}_{h=1}^{H}" class="latex" />. We can now introduce the Bilinear Class.</p>



<p><strong>Definition (Bilinear Class)</strong>. Consider an MDP <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}" class="latex" />, a hypothesis class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" />, a discrepancy function <img src="https://s0.wp.com/latex.php?latex=%5Cell_f%3A+%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BA%7D+%5Ctimes+%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BH%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\ell_f: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{H} \to \mathbb{R}" class="latex" /> (defined for each <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{H}" class="latex" />). Suppose <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" /> is realizable in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}" class="latex" /> and that there exists functions <img src="https://s0.wp.com/latex.php?latex=W_h%3A+%5Cmathcal%7BH%7D+%5Cto+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W_h: \mathcal{H} \to \mathbb{R}^d" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X_h%3A+%5Cmathcal%7BH%7D+%5Cto+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X_h: \mathcal{H} \to \mathbb{R}^d" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=d+%5Cin+%5Cmathcal%7BN%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \in \mathcal{N}" class="latex" />. Then, <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathcal%7BH%7D%2C%5Cell_f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mathcal{H},\ell_f)" class="latex" /> forms a Bilinear Class for <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BM%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{M}" class="latex" /> if the following two conditions hold.</p>



<ol><li>Bilinear regret: on-policy difference between claimed reward and true reward satisfies following upper bound,<br /><img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D_%7Ba%7B1%3Ah%7D+%5Csim+%5Cpi_f%7D+%5Cleft%5BQ_%7Bh%7D%5E%7Bf%7D%28s_h%2Ca_h%29+-+r%28s_h%2Ca_h%29+-+V_%7Bh%7D%5E%7Bf%7D%28s_%7Bh%2B1%7D%29+%5Cright%5D+%7C+%5Cleq+%5Clangle+W_h%28f%29++-+W_h%28f%5E%5Cstar%29%2C+X_h%28f%29%5Crangle.f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mathbb{E}_{a{1:h} \sim \pi_f} \left[Q_{h}^{f}(s_h,a_h) - r(s_h,a_h) - V_{h}^{f}(s_{h+1}) \right] | \leq \langle W_h(f)  - W_h(f^\star), X_h(f)\rangle.f" class="latex" /></li><li>Data reuse: <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D_%7Ba%7B1%3Ah%7D+%5Csim+%5Cpi_f%7D%5Cleft%5B%5Cell_f%28s_h%2Ca_h%2Cs_%7Bh%2B1%7D%2Cg%29+%5Cright%5D%7C+%3D+%7C%5Clangle+W_h%28g%29+-+W_h%28f%5E%5Cstar%29%2C+X_h%28f%29%5Crangle+%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mathbb{E}_{a{1:h} \sim \pi_f}\left[\ell_f(s_h,a_h,s_{h+1},g) \right]| = |\langle W_h(g) - W_h(f^\star), X_h(f)\rangle |." class="latex" /></li></ol>



<p>As an example to demonstrate what the choices of <img src="https://s0.wp.com/latex.php?latex=%5Cell_f%2CW_h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\ell_f,W_h" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=X_h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X_h" class="latex" /> might look like, for a linear Bellman complete class <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{H}" class="latex" />, we can choose<br /><img src="https://s0.wp.com/latex.php?latex=%5Cell_f%28s_h%2Ca_h%2Cs_%7Bh%2B1%7D%2Cg%29+%3D+Q_%7Bh%7D%5E%7Bg%7D%28s_h%2Ca_h%29+-+r%28s_h%2Ca_h%29+-+V_%7Bh%2B1%7D%5E%7Bg%7D%28s_%7Bh%2B1%7D%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\ell_f(s_h,a_h,s_{h+1},g) = Q_{h}^{g}(s_h,a_h) - r(s_h,a_h) - V_{h+1}^{g}(s_{h+1})," class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=W_h%28g%29+%3D+%5Ctheta_h+-+%5Cmathcal%7BT%7D_h%28%5Ctheta_%7Bh%2B1%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W_h(g) = \theta_h - \mathcal{T}_h(\theta_{h+1})" class="latex" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=X_h%28f%29+%3D+%5Cmathbb%7BE%7D_%7B%28s_h%2Ca_h%29+%5Csim+%5Cpi_f%7D%5B%5Cphi%28s_h%2Ca_h%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X_h(f) = \mathbb{E}_{(s_h,a_h) \sim \pi_f}[\phi(s_h,a_h)]" class="latex" /></p>



<p>Above, note that <img src="https://s0.wp.com/latex.php?latex=W_h%28f%5E%5Cstar%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W_h(f^\star) = 0" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" /> for linear Bellman complete classes, and that the discrepancy function <img src="https://s0.wp.com/latex.php?latex=%5Cell_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\ell_f" class="latex" /> in this case does not depend on <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />. As demonstrated in <a href="https://arxiv.org/pdf/2103.10897.pdf">Du et al. 2021</a>, the following models (in which sample-efficient generalization is known to be possible) can all be shown to be Bilinear Classes for some discrepancy function <img src="https://s0.wp.com/latex.php?latex=%5Cell_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\ell_f" class="latex" />:</p>



<ul><li>Linear Bellman Completion [<a href="https://www.aaai.org/Library/AAAI/2005/aaai05-159.php">Munos 2005</a>, <a href="https://arxiv.org/abs/2003.00153">Zanette et al. 2020</a>]<ul><li>Linear MDPs (low-rank transition matrix) [<a href="https://arxiv.org/abs/1902.04779">Wang and Yang 2018</a>; <a href="https://arxiv.org/abs/1907.05388">Jin et al. 2019</a>]</li><li>Linear Quadratic Regulators (LQR): standard control theory model (see e.g. <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">Wikipedia page for LQR</a>)</li></ul></li><li>FLAMBE/Feature Selection: <a href="https://arxiv.org/abs/2006.10814">Agarwal, Kakade, Krishnamurthy, Sun 2020</a></li><li>Linear Mixture MDPs: [<a href="http://proceedings.mlr.press/v108/modi20a/modi20a.pdf">Modi et al. 2020</a>, <a href="http://proceedings.mlr.press/v119/ayoub20a/ayoub20a.pdf">Ayoub et al. 2020</a>]</li><li>Block MDPs <a href="https://arxiv.org/pdf/1901.09018.pdf">Du et al. 2019</a></li><li>Factored MDPs <a href="https://arxiv.org/abs/1811.08540">Sun et al 2019</a></li><li>Kernelized Nonlinear Regulator <a href="https://arxiv.org/abs/2006.12466">Kakade et al. 2020</a></li><li>and more (see <a href="https://arxiv.org/pdf/2103.10897.pdf">Du et al. 2021</a> for details.)</li></ul>



<p>Bilinear classes can be seen as a generalization of Bellman rank (<a href="https://arxiv.org/abs/1610.09512">Jiang et al. 2017</a>) and Witness rank (<a href="https://arxiv.org/abs/1811.08540">Wen et al. 2019</a>), which were previous works that sought to identify strucural commonalities between different RL models that enable sample-efficient generalization. That being said, there are still models (with known provable generalization) which Bilinear Classes does not cover. Two such exceptions are the deterministic linear <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" /> (<a href="https://arxiv.org/abs/1307.4847">Wen and Van Roy 2013</a>) model and the <img src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q^\star" class="latex" />-state aggregation model (<a href="https://arxiv.org/abs/1912.06366">Dong et al. 2020</a>). On a heuristic level, the structural commonalities identified by the Bilinear Classes show that to a large extent, most RL models known to enable sample-efficient generalization resemble linear bandits, in that data reuse is possible. In this sense, understanding why generalization is possible in the linear bandit case gives one intuition for why generalization is possible in these other cases as well. On some level, this may be disappointing since we might hope to capture richer phenomenon than just linear bandits, but promisingly, there is a rich class of RL models which share these structural commonalities that enable generalization in RL (as the examples encompassed by the Bilinear Classes demonstrate).</p>



<h2>Conclusion</h2>



<p>From the discussion above, we see that a generalization theory for RL, while significantly distinct from that for supervised learning, is still possible. However, natural assumptions that might seem adequate, such as linear realizability, are in fact insufficient, and much stronger assumptions are required. One such example of sufficient assumptions is the Bilinear Class, which covers a rich set of models. Moreover, as the empirical results we saw in the interlude show, these representational issues identified by theory are relevant for practice. For more on the theory of RL, see the following <a href="https://rltheorybook.github.io">forthcoming book</a>.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/24/towards-a-theory-of-generalization-in-reinforcement-learning-guest-lecture-by-sham-kakade/"><span class="datestr">at April 24, 2021 02:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/23/tcs-women-rising-star-nominations/">TCS Women Rising star nominations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(Guest post by Virginia Vassilevska Williams) <br /></em><br /></p>



<p>Dear colleagues</p>



<p>We invite you to nominate speakers for our TCS Women Rising Star talks at the TCS Women Spotlight Workshop at STOC 2021. To be eligible, your nominee has to be a theoretical computer science researcher (all topics represented at STOC are welcome) who is female or an underrepresented minority, and is a graduating PhD student or a postdoc. You can make your nomination by filling this form by May 15th:  <a href="https://forms.gle/g4mTS2MJzkenKrry6" target="_blank" rel="noreferrer noopener">https://forms.gle/g4mTS2MJzkenKrry6</a></p>



<p>The TCS Women Spotlight workshop at STOC 2021 will take place virtually between June 21st and June 25th (most likely on Tuesday, June 22<sup>nd</sup>, to be confirmed later on).</p>



<p>You can see the list of speakers from last year here: <a href="https://sigact.org/tcswomen/3rd-tcs-women-meeting/tcs-women-2020/">https://sigact.org/tcswomen/3rd-tcs-women-meeting/tcs-women-2020/</a></p>



<p></p>



<p>Looking forward to your nominations and to seeing you at the TCS Women Spotlight Workshop!</p>



<p>Virginia Vassilevska Williams, Barna Saha, Sofya Raskhodnikova, Mary Wootters and Elena Grigorescu</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/23/tcs-women-rising-star-nominations/"><span class="datestr">at April 23, 2021 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/057">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/057">TR21-057 |  Hardness of KT Characterizes Parallel Cryptography | 

	Hanlin Ren, 

	Rahul Santhanam</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent breakthrough of Liu and Pass (FOCS'20) shows that one-way functions exist if and only if the (polynomial-)time-bounded Kolmogorov complexity K^t is bounded-error hard on average to compute. In this paper, we strengthen this result and extend it to other complexity measures:

1. We show, perhaps surprisingly, that the KT complexity is bounded-error average-case hard if and only if there exist one-way functions in constant parallel time (i.e. NC^0). This result crucially relies on the idea of randomized encodings. Previously, a seminal work of Applebaum, Ishai and Kushilevitz (FOCS'04; SICOMP'06) used the same idea to show that NC^0-computable one-way functions exist if and only if logspace-computable one-way functions exist.
	
2. Inspired by the above result, we present randomized average-case reductions among the NC^1-versions and logspace-versions of K^t complexity, and KT complexity. Our reductions preserve both bounded-error average-case hardness and zero-error average-case hardness. To the best of our knowledge, this is the first reduction between KT complexity and a variant of K^t complexity.

3. We prove tight connections between the hardness of K^t complexity and the hardness of (the hardest) one-way functions. In analogy with the Exponential-Time Hypothesis and its variants, we define and motivate the Perebor Hypotheses for complexity measures such as K^t and KT. We show that a Strong Perebor Hypothesis for K^t implies the existence of (weak) one-way functions of near-optimal hardness 2^{n-o(n)}. To the best of our knowledge, this is the first construction of one-way functions of near-optimal hardness based on a natural complexity assumption about a search problem.

4. We show that a Weak Perebor Hypothesis for MCSP implies the existence of one-way functions, and establish a partial converse. This is the first unconditional construction of one-way functions from hardness of MCSP over a natural distribution.

5. Finally, we study the average-case hardness of MKtP. We show that it characterizes cryptographic pseudorandomness in one natural regime of parameters, and complexity-theoretic pseudorandomness in another natural regime.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/057"><span class="datestr">at April 23, 2021 01:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/056">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/056">TR21-056 |  On the Possibility of Basing Cryptography on $\EXP \neq \BPP$ | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Liu and Pass (FOCS'20) recently demonstrated an equivalence between the existence of one-way functions (OWFs) and mild average-case hardness of the time-bounded Kolmogorov complexity problem. In this work, we establish a similar equivalence but to a different form of time-bounded Kolmogorov Complexity---namely, Levin's notion of Kolmogorov Complexity---whose hardness is closely related to the problem of whether $\EXP \neq \BPP$. In more detail, let $Kt(x)$ denote the Levin-Kolmogorov Complexity of the string $x$; that is, $Kt(x) = \min_{\desc \in \bitset^*, t \in \N}\{|\desc| + \lceil \log t \rceil: U(\desc, 1^t) = x\}$, where $U$ is a universal Turing machine, and $U(\desc,1^t)$ denotes the output of the program $\Pi$ after $t$ steps, and let $\mktp$ denote the language of pairs $(x,k)$ having the property that $Kt(x) \leq k$.
We demonstrate that:
- $\mktp \notin \HeurpBPP$ (i.e., $\mktp$ is infinitely-often \emph{two-sided error} mildly average-case hard) iff infinititely-often OWFs exist.
- $\mktp \notin \AvgpBPP$ (i.e., $\mktp$ is infinitely-often \emph{errorless} mildly average-case hard) iff $\EXP \neq \BPP$.
Thus, the only ``gap'' towards getting (infinitely-often) OWFs from the assumption that $\EXP \neq \BPP$ is the seemingly ``minor'' technical gap between two-sided error and errorless average-case hardness of the $\mktp$ problem. As a corollary of this result, we additionally demonstrate that any reduction from errorless to two-sided error average-case hardness for $\mktp$ implies (unconditionally) that $\NP \neq \P$. 

We finally consider other alternative notions of Kolmogorov complexity---including space-bounded Kolmogorov complexity and conditional Kolmogorov complexity---and show how average-case hardness of problems related to them characterize log-space computable OWFs, or OWFs in $\NC^0$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/056"><span class="datestr">at April 23, 2021 09:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1199907254169036789">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/the-million-dollar-sermon.html">The Million Dollar Sermon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Illinois Tech has one of the greatest origin stories for a university. In 1890 Frank Gunsaulus, a pastor on the south side of Chicago, gave a sermon where he said "If I had a million dollars I would build a school to provide students from all backgrounds meaningful roles in a changing industrial society". Philip Armour, a wealthy industrialist in the congregation, went up to Gunsaulus after the sermon and said, "if you give me five years for this school, I'll give you the million dollars". Thus started the Armour Institute of Technology which after some mergers became the Illinois Institute of Technology.</p><p>The "million dollar sermon" really happened, though the exact wording and even the exact year are lost to posterity or, as speculated, hidden in a cornerstone of one of the original campus buildings. </p><p>In 1890 we were in the beginnings of the second industrial revolution, a revolution of communication, transportation, electrification and soon the assembly line. The first revolution happened a century earlier with mechanization and the steam engine. The third revolution was computers and automation, and we are now in the early parts of the fourth industrial revolution, one based on data and information. </p><p>There are many parallels between 1890 and today. Like 1890, the private economy is dominated by a small number of large companies that have an outsized influence in our society. Like 1890, technology is changing faster than we can manage it. Like 1890, many workers are finding their skills quickly outdated.</p><p>Today Gunsaulus's words ring truer than ever. We more than ever need to provide students of all backgrounds meaningful roles in a changing technological society. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/the-million-dollar-sermon.html"><span class="datestr">at April 22, 2021 02:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18622">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/">Ken Turns 40</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Every chess master was once a beginner—Irving Chernev</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/ken2/" rel="attachment wp-att-18624"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ken2.png?w=150&amp;ssl=1" alt="" class="alignright  wp-image-18624" /></a></p>
<p>
Ken Regan started his Ph.D. 40 years ago from Oxford, in complexity theory, advised by Dominic <a href="https://en.wikipedia.org/wiki/Dominic_Welsh">Welsh</a>. Ken continues to be a researcher in complexity theory, a teacher of computer science, a mentor of graduate students, a writer of books and more. He has long been on the faculty of the <a href="https://cse.buffalo.edu/~regan/">Department of Computer Science</a> and Engineering at Buffalo. </p>
<p>
Today I thought I would thank Ken for his many wonderful accomplishments. </p>
<p>
Ken is a great friend and long time co-author of mine. We also write this blog together. He has done many things—one colorful thing is the <a href="https://cse.buffalo.edu/~regan/papers/ComplexityPoster.jpg">poster</a>. Ken is a strong chess player—a chess International Master just below the grandmaster level and rated at 2372—roughly. </p>
<p>
We will highlight his continued work in complexity theory and his work in detecting chess cheating.</p>
<p></p><h2> Complexity Theory </h2><p></p>
<p></p><p>
Ken and I have just released a second edition of our book on quantum algorithms: <a href="https://www.thriftbooks.com/w/introduction-to-quantum-algorithms-via-linear-algebra-second-edition_kenneth-w-regan_richard-j-lipton/26771326/item/44480377/?gclid=Cj0KCQjw1PSDBhDbARIsAPeTqrd5GXdcE5MG1a_Ue1T4vDhz4_adZNPdNAslCRxsyFUroKj6AgxRGlkaAulSEALw_wcB#idiq=44480377&amp;edition=41576984"> Introduction to Quantum Algorithms Via Linear Algebra, Second Edition</a> </p>
<blockquote><p><b> </b> <em> Quantum computing explained in terms of elementary linear algebra, emphasizing computation and algorithms and requiring no background in physics. This introduction to quantum algorithms is concise but comprehensive, covering many key algorithms. It is mathematically rigorous but requires minimal background and assumes no knowledge of quantum theory or quantum mechanics. </em>
</p></blockquote>
<p>I must add that Ken did the lion share of the work on writing this second edition.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/quantum/" rel="attachment wp-att-18625"><img width="201" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/quantum.jpg?resize=201%2C300&amp;ssl=1" class="aligncenter size-medium wp-image-18625" height="300" /></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>Please note a special offer: Buy one, get another for full price. </p>
<p>
In another part of complexity theory, Ken has just had a paper accepted: <i>Multi-Structural Games and Number of Quantifiers</i> with Ronald Fagin, Jonathan Lenchner, and Nikhil Vyas. This will appear at <a href="http://easyconferences.eu/lics2021/">LICS 2021</a> this summer.</p>
<p>
The paper defines a new type of game that captures the number of quantifies needed for certain properties. This is neat—see the paper for details.</p>
<p>
</p><p></p><h2> Chess Cheating—How? </h2><p></p>
<p></p><p>
The area of detecting chess cheating is one that Ken works in. Moreover he is perhaps the leader in the world. That is the leader in detecting when a player cheated at playing chess. </p>
<p>
What is this? In games like poker, games that rely on cards, it is long been an issue that people can <a href="https://en.wikipedia.org/wiki/Cheating_in_poker">cheat</a>. Cards can be marked for example. The dealer can try and control what cards a player gets. These are ancient issues for card games. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/card/" rel="attachment wp-att-18626"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/card.png?resize=300%2C182&amp;ssl=1" class="aligncenter size-medium wp-image-18626" height="182" /></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>
What is this for <a href="https://en.wikipedia.org/wiki/Cheating_in_chess">chess</a>? The pieces cannot be marked to any advantage: a king is king, a pawn is a pawn and so on. But a player can cheat in a simple manner. They can use the moves of a computer program instead of their own. The problem in chess is that computer programs currently play at a level vastly higher than even a strong player. Stockfish, a top program, plays at around <a href="https://ccrl.chessdom.com/ccrl/4040/">3551</a>. </p>
<p>
Recall Ken is around 2372. That places him via the Elo <a href="https://en.wikipedia.org/wiki/Chess_rating_system">rating</a> at IM: </p>
<ul>
<li>
2500-2700	most Grandmasters (GM) <p></p>
</li><li>
2400-2500	most International Masters (IM) and some Grandmasters (GM) <p></p>
</li><li>
2300-2400	most FIDE Masters (FM) and some International Masters (IM) <p></p>
</li><li>
2200-2300	FIDE Candidate Masters (CM), most national masters
</li></ul>
<p>Note: the computer therefore plays much better than all known GM’s. </p>
<p>
</p><p></p><h2> Chess Cheating—Detecting? </h2><p></p>
<p></p><p>
Enter Ken. He has been studying how to detect whether or not a player is using a computer’s moves rather than their own moves. He has worked on this for many years, with good success. Note: this is a real <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">problem</a>: </p>
<blockquote><p><b> </b> <em> In one chess tournament, five of the top six were disqualified for cheating. In another, the doting parents of 10-year-old competitors furiously rejected evidence that their darlings were playing at the level of the world No 1. </em>
</p></blockquote>
<p></p><p>
An obvious idea is to not let a player have access to any computer. This is naive for several reasons: </p>
<ol>
<li>
A player can use their cellphone secretly—in the toilet? <p></p>
</li><li>
A player could conceal the computer on themselves—computers are small? <p></p>
</li><li>
A player could be playing chess online—impossible to stop them?
</li></ol>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/heel/" rel="attachment wp-att-18627"><img width="300" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/heel.png?resize=300%2C169&amp;ssl=1" class="aligncenter size-medium wp-image-18627" height="169" /></a>
</td>
</tr>
<tr>
</tr>
</tbody></table>
<p>
</p><p></p><h2> Chess Cheating—How to Detect </h2><p></p>
<p></p><p>
This led Ken to consider the following problem: Suppose that some player makes a series of moves during a game. Check how well their moves agree with Stockfish or some other computer program. Then claim the player cheated provided they agree too often with the program. </p>
<p>
This sounds simple. Compare the computer’s move and the player’s moves. If these agree too often then say the player has cheated.</p>
<p>
Simple. Unfortunately not. There are many complications: </p>
<ol>
<li>
In chess sometimes moves are “in book”. Especially at the beginning the best move may be well known. <p></p>
</li><li>
In chess sometimes moves are “forced”. There may be a unique legal move. <p></p>
</li><li>
A player may only use the computer when the position is “hard”. <p></p>
</li><li>
The computer may suggest several moves. These moves may all be about the same quality.
</li></ol>
<p>Clearly all these make the problem—did the player cheat—complex. Difficult. A further complicating factor is that even a weak player will sometimes randomly agree with the computer. That is even a weaker player can make a great move. </p>
<p>
Perhaps the shock is that Ken has been able to build software that is able to correctly decide when a player cheated or not. His method is good enough that it is used in practice on real players. It has led to actual penalties on players, and has saved others from false claims. </p>
<p>
I must add that his software is quite impressive. The test to see if a player cheated involves the running of chess computer programs. These programs like <a href="https://stockfishchess.org">Stockfish</a> were made to be used by one player. They were not designed to be used as a subroutine in an automated search. Perhaps for a complexity theorist Ken might have a top ranking for writing the most real programs. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
Correction: Ken is 40th from Princeton as an undergrad. Ken is 35th from Oxford for Ph.D. See Ken on a 40th Reunion for Princeton Class of ’81 <a href="https://www.youtube.com/watch?v=S1LDJZglNUg">panel</a>   [<b>Ken:</b> A one-word fix did the trick, changing “obtained” to “started” in the first sentence.  It’s a Trans-Pond-er difference: I count as ’81 both at Princeton and Oxford (Merton College) because the latter goes by the starting year.  The photo is from Glenveagh Castle, Ireland, on a June 2019 trip with my family.]</p>
<p>
Ken <a href="https://www.newser.com/story/297646/pandemic-has-caused-cheating-crisis-in-chess.html">says</a>: “The pandemic has brought me as much work in a single day as I have had in a year previously,” said Prof Kenneth Regan, an international chess master and computer scientist whose model is relied on by the sport’s governing body, FIDE, to detect suspicious patterns of play. “It has ruined my sabbatical.”</p>
<p>
See this <a href="https://cse.buffalo.edu/~regan/personal/JuneCLarticleKWR.pdf">article</a> for more detail on Ken’s work. This is by Howard Goldowsky.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/"><span class="datestr">at April 22, 2021 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/04/22/postdoc-in-algorithms-at-university-of-vienna-apply-by-may-16-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/04/22/postdoc-in-algorithms-at-university-of-vienna-apply-by-may-16-2021/">Postdoc in Algorithms at University of Vienna (apply by May 16, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are looking for an excellent researcher to join our project on “Fast Algorithms for a Reactive Network Layer” funded by the Austrian NSF. It will lay the algorithmic foundations of more adaptive networks, using algorithmic tools from many areas of algorithms research. This is a collaboration between the research groups of Monika Henzinger at U Vienna and of Stefan Schmid at TU Berlin.</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br />
Email: applications.taa@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/04/22/postdoc-in-algorithms-at-university-of-vienna-apply-by-may-16-2021/"><span class="datestr">at April 22, 2021 09:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/04/21/postdoc-at-university-of-birmingham-uk-apply-by-may-20-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/04/21/postdoc-at-university-of-birmingham-uk-apply-by-may-20-2021/">Postdoc at University of Birmingham, UK (apply by May 20, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We seek a Research Fellow (3 yr) to work on an UKRI-funded project on co-evolution. Co-evolutionary algorithms mimick natural selection, e.g. for minmax optimisation or learning game playing strategies.</p>
<p>The postdoc will develop time-complexity analysis of co-evolutionary algorithms, and should have a track record in theory of evolutionary computation, algorithmic game theory, and/or algorithms.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CFK453/research-fellow-in-theory-of-co-evolutionary-computation">https://www.jobs.ac.uk/job/CFK453/research-fellow-in-theory-of-co-evolutionary-computation</a><br />
Email: p.k.lehre@cs.bham.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/04/21/postdoc-at-university-of-birmingham-uk-apply-by-may-20-2021/"><span class="datestr">at April 21, 2021 02:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/04/21/tcs-talk-wednesday-april-28-ronen-eldan-weizmann-institute/">TCS+ talk: Wednesday, April 28 — Ronen Eldan, Weizmann Institute</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://www.wisdom.weizmann.ac.il/~ronene/"><strong>Ronen Eldan</strong></a> from the Weizmann Institute will speak about “<em>Localization, stochastic localization, and Chen’s recent breakthrough on the Kannan-Lovasz-Simonivits conjecture</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The Kannan-Lovasz and Simonovits (KLS) conjecture considers the following isoperimetric problem on high-dimensional convex bodies: Given a convex body <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="K" class="latex" />, consider the optimal way to partition it into two pieces of equal volume so as to minimize their interface. Is it true that up to a universal constant, the minimal partition is attained via a hyperplane cut? Roughly speaking, this question can be thought of as asking “to what extent is a convex set a good expander”?</p>
<p>In analogy to expander graphs, such lower bounds on the capacity would imply bounds on mixing times of Markov chains associated with the convex set, and so this question has direct implications on the complexity of many computational problems on convex sets. Moreover, it was shown that a positive answer would imply Bourgain’s slicing conjecture.</p>
<p>Very recently, Yuansi Chen obtained a striking breakthrough, nearly solving this conjecture. In this talk, we will overview some of the central ideas used in the proof. We will start with the classical concept of “localization” (a very useful tool to prove concentration inequalities) and its extension, stochastic localization – the main technique used in the proof.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/04/21/tcs-talk-wednesday-april-28-ronen-eldan-weizmann-institute/"><span class="datestr">at April 21, 2021 04:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5460">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5460">Doubts about teapot supremacy: my reply to Richard Borcherds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://en.wikipedia.org/wiki/Richard_Borcherds">Richard Borcherds</a> is a British mathematician at Berkeley, who won the 1998 Fields Medal for the proof of the <a href="https://en.wikipedia.org/wiki/Monstrous_moonshine">monstrous moonshine conjecture</a> among many other contributions.  A couple months ago, Borcherds posted on YouTube a <a href="https://www.youtube.com/watch?v=sFhhQRxWTIM"><strong>self-described “rant” about quantum computing</strong></a>, which was recently making the rounds on Facebook and which I found highly entertaining.</p>



<p>Borcherds points out that the term “quantum supremacy” means only that quantum computers can outperform existing classical computers on <em>some</em> benchmark, which can be chosen to show maximum advantage for the quantum computer.  He allows that <a href="https://en.wikipedia.org/wiki/Boson_sampling">BosonSampling</a> could have some value, for example in calibrating quantum computers or in comparing one quantum computer to another, but he decries the popular conflation of quantum supremacy with the actual construction of a scalable quantum computer able (for example) to run Shor’s algorithm to break RSA.</p>



<p>Borcherds also proposes a “teapot test,” according to which any claim about quantum computers can be dismissed if an analogous claim would hold for a teapot (which he brandishes for the camera).  For example, there are many claims to solve practical optimization and machine learning problems by “quantum/classical hybrid algorithms,” wherein a classical computer does most of the work but a quantum computer is somehow involved.  Borcherds points out that, at least as things stand in early 2021, in most or all such cases, the classical computer could’ve probably done as well entirely on its own.  So then if you put a teapot on top of your classical computer while it ran, you could equally say you used a “classical/teapot hybrid approach.”</p>



<p>Needless to say, Borcherds is correct about all of this.  I’ve made similar points on this blog for 15 years, although less Britishly.  I’m delighted to have such serious new firepower on the scoffing-at-QC-hype team.</p>



<p>I do, however, have one substantive disagreement.  At one point, Borcherds argues that sampling-based quantum supremacy itself fails his teapot test.  For consider the computational problem of predicting how many pieces a teapot will break into if it’s dropped on the ground.  Clearly, he says, the teapot itself will outperform any simulation running on any existing classical computer at that task, and will therefore achieve “teapot supremacy.”  But who cares??</p>



<p>I’m glad that Borcherds has set out, rather crisply, an objection that’s been put to me many times over the past decade.  The response is simple: <em>I don’t believe the teapot really does achieve teapot supremacy on the stated task!  At the least, I’d need to be shown why.  You can’t just assert it without serious argument.</em></p>



<p>If we want to mirror the existing quantum supremacy experiments, then the teapot computational problem, properly formulated, should be: given as input a description of a teapot’s construction, the height from which it’s dropped, etc., <em>output a sample from the probability distribution</em> over the number of shards that the teapot will break into when it hits the floor.</p>



<p>If so, though, then clearly a classical computer can easily sample from the same distribution!  Why?  Because presumably we agree that there’s a negligible probability of more than (say) 1000 shards.  So the distribution is characterized by a list of at most 1000 probabilities, which can be estimated empirically (at the cost of a small warehouse of smashed teapots) and thereafter used to generate samples.  In the plausible event that the distribution is (say) a Gaussian, it’s even easier: just estimate the mean and variance.</p>



<p>A couple days ago, I was curious what the distribution looked like, so I decided to order some teapots from Amazon and check.  Unfortunately, real porcelain teapots are <em>expensive</em>, and it seemed vaguely horrific to order dozens (as would be needed to get reasonable data) for the sole purpose of smashing them on my driveway.  So I hit on what seemed like a perfect solution: I ordered <em>toy</em> teapots, which were much smaller and cheaper.  Alas, when my toy “porcelain” teapots arrived yesterday, they turned out (unsurprisingly in retrospect for a children’s toy) to be some sort of plastic or composite material, meaning that they <em>didn’t</em> break unless one propelled them downward forcefully.  So, while I can report that they tended to break into one or two large pieces along with two or three smaller shards, I found it impossible to get better data.  (There’s a reason why I became a <em>theoretical</em> computer scientist…)</p>



<figure class="wp-block-image size-large"><img src="https://www.scottaaronson.com/teapot.jpg" alt="" /></figure>



<p>The good news is that my 4-year-old son had an absolute <em>blast</em> smashing toy teapots with me on our driveway, while my 8-year-old daughter was thrilled to take the remaining, unbroken teapots for her dollhouse.  I apologize if this fails to defy gender stereotypes.</p>



<p>Anyway, it might be retorted that it’s not good enough to sample from a probability distribution: what’s wanted, rather, is to calculate how many pieces this <em>specific</em> teapot will break into, given all the microscopic details of it and its environment.  Aha, this brings us to a crucial conceptual point: in order for something to count as an “input” to a computer, <em>you need to be able to set it freely</em>.  Certainly, at the least, you need to be able to measure and record the input in its entirety, so that someone trying to reproduce your computation on a standard silicon computer would know exactly which computation to do.  You don’t get to claim computational supremacy based on a problem with secret inputs: that’s like failing someone on a math test without having fully told them the problems.</p>



<p>Ability to set and know the inputs is <em>the</em> key property that’s satisfied by Google’s quantum supremacy experiment, and to a lesser extent by the USTC BosonSampling experiment, but that’s not satisfied at all by the “smash a teapot on the floor” experiment.  Or perhaps it’s better to say: influences on a computation that vary uncontrollably and chaotically, like gusts of air hitting the teapot as it falls to the floor, shouldn’t be called “inputs” at all; they’re simply <em>noise sources</em>.  And what one does with noise sources is to try to estimate their distribution and average over them—but in that case, as I said, there’s no teapot supremacy.</p>



<p>A Facebook friend said to me: that’s well and good, but surely we could change Borcherds’s teapot experiment to address this worry?  For example: add a computer-controlled lathe (or even a 3D printer), with which you can build a teapot in an arbitrary shape of your choice.  Then consider the problem of sampling from the probability distribution over how many pieces <em>that</em> teapot will smash into, when it’s dropped from some standard height onto some standard surface.  I replied that this is indeed more interesting—in fact, it already seems more like what engineers do in practice (still, sometimes!) when building wind tunnels, than like a silly reductio ad absurdum of quantum supremacy experiments.  On the other hand, <em>if</em> you believe the <a href="https://inst.eecs.berkeley.edu/~cs191/fa08/lectures/lecture17.pdf">Extended Church-Turing Thesis</a>, then as long as your analog computer is governed by classical physics, it’s presumably inherently limited to an <a href="https://en.wikipedia.org/wiki/Avogadro_constant">Avogadro’s number</a> type speedup over a standard digital computer, whereas with a quantum computer, you’re limited only by the exponential dimensionality of Hilbert space, which seems more interesting.</p>



<p>Or maybe I’m wrong—in which case, I look forward to the first practical demonstration of teapot supremacy!  Just like with quantum supremacy, though, it’s not enough to <em>assert</em> it; you need to … put the tea where your mouth is.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> On the suggestion of <a href="https://cs.nyu.edu/davise/">Ernest Davis</a>, who I can now reveal as the Facebook friend mentioned above, I just ordered some <a href="https://www.ssww.com/item/terra-cotta-pots-3-1-8-PL807/index.php?cid=3358&amp;gclid=Cj0KCQjw9_mDBhCGARIsAN3PaFOKHOxEAHqA-WjzGS5lFgass_iNz27V88AZdGe5e26GTsrJUX0ZNL4aAqOzEALw_wcB&amp;fbclid=IwAR1eBc9xXODJ3a15ukebh1loN9XNpl7aD2Xiihsb2H7JvZVBKLKC10OwIys">terra cotta flower pots</a>, which look cheap, easily smashable, and environmentally friendly, and which will hopefully be acceptable substitutes for porcelain teapots in a new experiment.  (Not that my main arguments in this post hinge on the results of such an experiment!  That’s the power of theory.)</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> Some of you might enjoy <a href="https://www.scientificamerican.com/article/will-quantum-computing-ever-live-up-to-its-hype/">John Horgan’s <em>Scientific American</em> column</a> on reality vs. hype in quantum computing, based on conversations with me and with Terry Rudolph of PsiQuantum.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5460"><span class="datestr">at April 20, 2021 06:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=39">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Introducing ALT Highlights 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The 32nd International Conference on Algorithmic Learning Theory (<a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>) just wrapped up, featuring a wide selection of exciting results at the frontiers of learning theory. The <a href="http://proceedings.mlr.press/v132/">proceedings</a> and all <a href="https://www.youtube.com/channel/UC7wMo5OivSnsQJNfZm8zmJQ/videos">talk recordings</a> are available online for perusal. </p>



<p>Did you miss out on the conference? Don’t have time to go through all the proceedings? Fear not, the <a href="https://let-all.com/">Learning Theory Alliance</a> is pleased to bring you ALT Highlights, a series of blog posts spotlighting various happenings at ALT, including plenary talks, tutorials, trends in learning theory, and more!</p>



<p>In order to reach a broad audience in learning theory, we’ll be releasing these posts across a number of different blogs. All content will be linked from this post, so be sure to bookmark this post so you don’t miss anything!</p>



<p>ALT Highlights will be brought to you by an amazing team of junior researchers, written by <a href="https://people.eecs.berkeley.edu/~kush/">Kush Bhatia</a>, <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>, <a href="https://web.stanford.edu/~mglasgow/">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>, <a href="https://www.ttic.edu/students/">Keziah Naggita</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/">Cyrus Rashtchian</a>, and overseen and edited by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. </p>



<p>Links to articles:<br />1. <a href="https://hunch.net/?p=13762948">An Interview with Joelle Pineau</a></p>



<p>Next article coming April 26!</p>
<p>The post <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" rel="nofollow">Introducing ALT Highlights 2021</a> appeared first on <a href="https://www.let-all.com/blog" rel="nofollow">The Learning Theory Alliance Blog</a>.</p></div>







<p class="date">
by admin <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/"><span class="datestr">at April 20, 2021 04:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/04/20/early-career-scientists-at-amazon-apply-by-may-14-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/04/20/early-career-scientists-at-amazon-apply-by-may-14-2021/">Early career scientists at Amazon (apply by May 14, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Amazon Advertising is launching a new, 2-year program for recent PhDs. It offers full-time two-year positions, aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. Key areas include but are not limited to: machine learning, economics, marketing, operations research, and statistics. The application deadline is May 14.</p>
<p>Website: <a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists">https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists</a><br />
Email: advertising-es-program@amazon.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/04/20/early-career-scientists-at-amazon-apply-by-may-14-2021/"><span class="datestr">at April 20, 2021 03:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/055">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/055">TR21-055 |  Cryptography from Sublinear-Time Average-Case Hardness of Time-Bounded Kolmogorov Complexity | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Let $\mktp[s]$ be the set of strings $x$ such that $K^t(x) \leq s(|x|)$, where $K^t(x)$ denotes the $t$-bounded Kolmogorov complexity of the truthtable described by $x$. Our main theorem shows that for an appropriate notion of mild average-case hardness, for every $\varepsilon&gt;0$, polynomial $t(n) \geq (1+\varepsilon)n$, and every ``nice'' class $\F$ of super-polynomial functions, the following are equivalent:
- the existence of some function $T \in \F$ such that $T$-hard one-way functions (OWF) exists (with non-uniform security);
- the existence of some function $T \in \F$ such that $\mktp[T^{-1}]$ is mildly average-case hard with respect to sublinear-time non-uniform algorithms (with running-time $n^{\delta}$ for some $0&lt;\delta&lt;1$).
For instance, existence of subexponentially-hard (resp. quasi-polynomially-hard) OWFs is equivalent to mild average-case hardness of $\mktp[\poly\log n]$ (resp. $\mktp[2^{O(\sqrt{\log n})})]$) w.r.t. sublinear-time non-uniform algorithms.

We additionally note that if we want to deduce $T$-hard OWFs where security holds w.r.t. \emph{uniform} $T$-time probabilistic attackers (i.e., uniformly-secure OWFs), it suffices to assume sublinear time hardness of $\mktp$ w.r.t. uniform probabilistic sublinear-time attackers. We complement this result by proving lower bounds that come surprisingly close to what is required to unconditionally deduce the existence of (uniformly-secure) OWFs: $\mktp[\poly\log n]$ is worst-case hard w.r.t. uniform probabilistic sublinear-time algorithms, and $\mktp[n-\log n]$ is mildly average-case hard for all $O(t(n)/n^3)$-time deterministic algorithms.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/055"><span class="datestr">at April 20, 2021 11:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3517">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2021/04/20/the-stony-brook-international-conference-on-game-theory-july-5-8-2021/">The Stony Brook International Conference on Game Theory, July 5-8, 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Call for Participation and Poster submission:</p>



<p><a href="http://www.gtcenter.org/?page=Conference.html" target="_blank" rel="noreferrer noopener">The 32nd annual Stony Brook International Conference on Game Theory </a> will be held online July 5 – 8, 2021. This year the conference will emphasize recent research at the intersection of computer science and economics. We will also celebrate the recent Nobel prize to Paul Milgrom and Robert Wilson, who will both deliver an extended version of their Nobel prize speech.</p>



<p>The conference will begin and end with a reception in honor of Paul Milgrom and Robert Wilson. The schedule is available <a href="http://www.gtcenter.org/index.php?page=Downloads/ConfSchedule.html" target="_blank" rel="noreferrer noopener">here</a>. The main program will consist of invited papers and talks.</p>



<p>We invite graduate students and junior faculty to submit recent work for <a href="https://docs.google.com/forms/d/1I4aipMKrhlmthfF8W0QeclGIJgF9oLa5xb5O-tJA7PA/viewform?edit_requested=true" target="_blank" rel="noreferrer noopener">poster sessions</a> which will be an integral part of the conference. The deadline for submission to the poster session is Friday, April 30<sup>th</sup>, 5pm est. The program for the poster session will be announced, Monday, May 17<sup>th</sup>.</p>



<p>If you would like to attend any of the sessions, please <a href="http://www.gtcenter.org/index.php?page=php/Login.php" target="_blank" rel="noreferrer noopener">Login</a> or <a href="http://www.gtcenter.org/index.php?page=php/Regform.php" target="_blank" rel="noreferrer noopener">Create Account</a> and RSVP for the events you would like to attend. As the conference date approaches, you will receive an email with more details on accessing and navigating Virtual Chair, the online, interactive event space that will host the conference.</p>



<p>Please feel free to share this information broadly. We look forward to your participation!</p>



<p><a href="https://campuspress.yale.edu/dirkbergemann/">Dirk Bergemann</a> and <a href="https://cs.tau.ac.il/~mfeldman">Michal Feldman</a></p>



<p>Organizers</p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2021/04/20/the-stony-brook-international-conference-on-game-theory-july-5-8-2021/"><span class="datestr">at April 20, 2021 10:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-628795981625377168">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/an-investment-puzzle-and-speculation-as.html">An investment puzzle and speculation as to why some think its hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> This is a Guest Post by David Marcus. He gives a puzzle and its solution, which is interesting, and then speculates as to why some people get it wrong. </p><p>---------------------------------------------------------------------------</p><p>THE PROBLEM:</p><p>Investing Puzzle or Arithmetic Can Be Useful</p><p><br /></p><p>The following is an example of investment results that I saw in an</p><p>investment newsletter. There are two portfolios that use different</p><p>strategies. Both portfolios start with $1 million twenty years ago and</p><p>withdraw 5% each year. The idea is that you are retired and withdrawing</p><p>money to spend. Not all years are shown in the tables.</p><p><br /></p><p>Portfolio A</p><p><br /></p><p>Year   Return  Withdrawal    Balance</p><p>2000   15.31%      57,655  1,095,445</p><p>2005    1.81%      59,962  1,139,273</p><p>2008  -12.65%      51,000    969,004</p><p>2009   34.26%      65,049  1,235,936</p><p>2010   11.94%      69,175  1,314,331</p><p>2015   -2.48%      64,935  1,233,764</p><p>2020   10.27%      66,935  1,271,765</p><p>Total Withdrawal: 1,685,190</p><p>Change in Balance: 27.18%</p><p>======</p><p>Portfolio B</p><p>Year   Return  Withdrawal    Balance</p><p>2000   -0.95%      49,524    940,956</p><p>2005    3.80%      44,534    846,154</p><p>2008  -20.11%      35,922    682,523</p><p>2009   18.27%      40,360    766,833</p><p>2010   11.57%      42,777    812,764</p><p>2015    0.99%      50,767    964,567</p><p>2020   13.35%      65,602  1,246,433</p><p><br /></p><p>Total Withdrawal: 1,425,573</p><p>Change in Balance: 24.64%</p><p><br /></p><p>Portfolio A has a final balance that is 25,000 more than Portfolio B's and</p><p>had about 260,000 more in withdrawals. Does the example lend credence to</p><p>the Portfolio A strategy being better than the Portfolio B strategy?</p><p>---------------------------------------------------------------------------</p><p>THE ANSWER:</p><p>Investing Puzzle or Arithmetic Can Be Useful: Analysis</p><p><br /></p><p>Summary: The two portfolios have about the same performance over the 20</p><p>years. The difference is mainly due to Portfolio A having a good year or</p><p>years near the beginning before much money was withdrawn. The example</p><p>merely shows that it is better to withdraw money after a gain rather than</p><p>before.</p><p><br /></p><p>Detailed Analysis:</p><p><br /></p><p>The scenario is: Start with X = $1 million. Withdraw 5% a year.</p><p><br /></p><p>Define "gain factor" to be 1 plus the percentage return. For example, if a</p><p>portfolio returns 5%, then the gain factor is 1.05.</p><p><br /></p><p>Let A_j, j = 1, ..., 20, be the gain factors each year for portfolio A.</p><p><br /></p><p>Let B_j, j = 1, ..., 20 be the gain factors each year for portfolio B.</p><p><br /></p><p>The final amount in portfolio A is</p><p><br /></p><p>   F = X * A_1 * 0.95 * A_2 * 0.95 * ... * A_20 * 0.95 .</p><p><br /></p><p>The final amount in portfolio B is</p><p><br /></p><p>   G = X * B_1 * 0.95 * B_2 * 0.95 * ... * B_20 * 0.95 .</p><p><br /></p><p>From the "Change in Balance" values or the balances for year 2020, we see</p><p>that F and G are almost the same:</p><p><br /></p><p>   F = 1.271865 * X,</p><p>   G = 1.246433 * X.</p><p><br /></p><p>But, as we learned in elementary school, multiplication is commutative, so</p><p><br /></p><p>   F = X * 0.95^20 * \prod_{j=1}^20 A_j,</p><p>   G = X * 0.95^20 * \prod_{j=1}^20 B_j.</p><p><br /></p><p>Since F and G are almost the same, the total gains (product of the gain</p><p>factors) for the two portfolios are almost the same, i.e.,</p><p><br /></p><p>   \prod_{j=1}^20 A_j \approx \prod_{j=1}^20 B_j.</p><p><br /></p><p>Then what accounts for the big difference in the amounts withdrawn?</p><p>Portfolio A must have had some good years near the beginning. (We see in</p><p>the tables that Portfolio A did better in 2000 than Portfolio B.) So, all</p><p>the example shows is that it is better to withdraw your money after your</p><p>gains rather than before.</p><p><br /></p><p>To take an extreme example, suppose an investment is going to go up 100%</p><p>this year. It is better to take your money out at the end of the year</p><p>(after the gain) than at the beginning of the year (before the gain). This</p><p>is a triviality.</p><p><br /></p><p>The example tells us nothing useful about the two strategies.</p><p><br /></p><p>Note: The total gains aren't exactly the same, but the timing of the yearly</p><p>gains is what is driving the results. We have (rounding off)</p><p>   ( F - G ) / 0.95^20 = 70942.81 .</p><p>So, if there had been no withdrawals, the difference in the portfolio</p><p>balances would have been about $71,000, much less than the $260,000 +</p><p>$25,000 difference with the withdrawals.</p><p>---------------------------------------------------------</p><p>WHY IS THIS HARD FOR PEOPLE?</p><p>Many people have trouble with this puzzle. The difficulty may be that such</p><p>people don't make a mental model (or a written model) of the process that</p><p>is producing the balance. If you write down (or have in your head) a</p><p>formula for the balance, then you see that the gain factors are independent</p><p>of the withdrawal factors. That is, we could withdraw more or less money,</p><p>or even deposit money, without affecting the gain factors we would use in</p><p>the model. This then leads us to consider the gain factors on their own,</p><p>and to recognize that the gain factors are the true measures of how the</p><p>strategies perform.</p><p><br /></p><p><br /></p><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/an-investment-puzzle-and-speculation-as.html"><span class="datestr">at April 19, 2021 02:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18580">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/">Summing Up the Primes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Given the millennia that people have contemplated prime numbers, our continuing ignorance concerning the primes is stultifying—Richard Crandall and Carl Pomerance</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/lola-2/" rel="attachment wp-att-18596"><img width="165" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/lola.png?resize=165%2C175&amp;ssl=1" class="alignright wp-image-18596" height="175" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sources: <a href="http://www.lolathompson.com/">her site</a>, Oberlin <a href="https://www.oberlin.edu/news/conversation-lola-thompson">interview</a></font></td>
</tr>
</tbody>
</table>
<p>
Lola Thompson is an Associate Professor of Mathematics at Utrecht University. She did her 2012 PhD <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/thesis.pdf">thesis</a>, <i>Products of Distinct Cyclotomic Polynomials</i>, under Pomerance at Dartmouth. Of course, she works in number theory.  Her thesis is numbered <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> on her publications <a href="http://www.lolathompson.com/research.html">page</a>.  </p>
<p>
Today we thought we would highlight a recent result of hers and its connections to complexity theory—indeed, to the realm of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> vs. <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> and beyond.</p>
<p>The interview linked below her photo at right includes these words:</p>
<blockquote><p>
Even [as an undergrad math major], I didn’t see myself having a future as a mathematician. I think that part of the issue was that I had never seen a female mathematician (none of my math professors at UChicago were women). A major turning point came when my department sent me to the Nebraska Conference for Undergraduate Women In Mathematics (NCUWM). While at NCUWM, I received a huge amount of encouragement. The idea of going to graduate school and becoming a mathematics professor had never occurred to me. It was also extremely helpful for me to see examples of female mathematicians. For the first time, I could imagine myself as a mathematician!
</p></blockquote>
<p>
She is paying that forward in mentoring women. See this <a href="https://women-in-numbers-europe-4.sites.uu.nl/wp-content/uploads/sites/658/2021/04/Poster-WIN.pdf">poster</a> for Utrecht’s hosting of next year’s 4th Women in Numbers – Europe <a href="https://women-in-numbers-europe-4.sites.uu.nl/">conference</a>, which is affiliated to the Women in Number Theory <a href="http://womeninnumbertheory.org/">network</a>. I also like her comments on undergraduate education <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/jmm_2020_special_session_talk.pdf">here</a>. </p>
<p>
The <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/mertensnew.pdf">paper</a> is joint with Harald Helfgott. The key “players” are the Möbius function and the Mertens function—named for August Möbius and for Franz Mertens. </p>
<p>
</p><h2> The Key Players </h2><p></p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/mm-2/" rel="attachment wp-att-18590"><img width="300" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/mm.png?resize=300%2C140&amp;ssl=1" class="aligncenter size-medium wp-image-18590" height="140" /></a></p>
<p>
The Möbius <a href="https://en.wikipedia.org/wiki/Mobius_function">function</a> was invented by Möbius in 1832. It is an important function throughout number theory. It is denoted by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n)}" class="latex" />: </p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n) = 1}" class="latex" /> if n is a square-free positive integer with an even number of prime factors.
</li><li><img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n) = -1}" class="latex" /> if n is a square-free positive integer with an odd number of prime factors.
</li><li><img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n) = 0}" class="latex" /> if n has a squared prime factor.
</li></ul>
<p>It is multiplicative: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%281%29+%3D+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mu(1) = 1 " class="latex" /></p>
<p>and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28ab%29+%3D+%5Cmu%28a%29%5Cmu%28b%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mu(ab) = \mu(a)\mu(b) " class="latex" /></p>
<p>when <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> are co-prime. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/val/" rel="attachment wp-att-18586"><img width="300" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/val.png?resize=300%2C78&amp;ssl=1" class="aligncenter size-medium wp-image-18586" height="78" /></a></p>
<p>
Think of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n)}" class="latex" /> as a function that returns <img src="https://s0.wp.com/latex.php?latex=%7B-1%2C0%2C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1,0,+1}" class="latex" />, which reveals important information about <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. The <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> part is the <a href="https://en.wikipedia.org/wiki/Mertens_function">Mertens</a> function, which gives average-like information on the value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n)}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cle+n+%5Cle+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0 \le n \le x}" class="latex" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%29+%3D+%5Csum_%7Bn+%5Cle+x%7D+%5Cmu%28n%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  M(x) = \sum_{n \le x} \mu(n). " class="latex" /></p>
<p>Of course <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%2Fx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)/x}" class="latex" /> is exactly the average value. 	 </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/m/" rel="attachment wp-att-18587"><img width="300" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/M.png?resize=300%2C240&amp;ssl=1" class="aligncenter size-medium wp-image-18587" height="240" /></a></p>
<p>
A key property of the Möbius function is: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bd+%7C+n%7D+%5Cmu%28d%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{d | n} \mu(d) = 0 " class="latex" /></p>
<p>unless <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=1}" class="latex" /> then the sum is <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />.</p>
<p>
</p><h2> Computing <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" />—the easy way </h2><p></p>
<p>
Computing <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(n)}" class="latex" /> for one value <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> seems to rely on the factorization of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. Thus even computing it at a single value could be hard. That is it could require super-polynomial time in worst case. Clearly computing <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> is even harder: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%2B1%29-M%28x%29+%3D+%5Cmu%28x%2B1%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  M(x+1)-M(x) = \mu(x+1). " class="latex" /></p>
<p>	 One way to compute <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> is via complexity theory. This is <i>not</i> what Thompson does. </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+%5C%23P%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = \#P}}" class="latex" />. Then we can compute <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> in time polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log(x)}" class="latex" />. </em>
</p></blockquote>
<p>
This would be an exponential improvement over Thompson’s result. It probably shows how unlikely it is to get this collapse. But this is still open.</p>
<p>
Recall a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> is in <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> provided given <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> one can decide whether <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is in the set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> in time polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log x}" class="latex" />. Recall <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> computes the function that counts how many <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cle+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \le N}" class="latex" /> are in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> also in time polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log x}" class="latex" />. Certainly counting is at least as hard as deciding membership in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />. We believe that in general it is much harder. Assuming they are equal means that any predicate that can be computed in polynomial time can also be exactly counted. For example: </p>
<ol>
<li>Given a graph: One can count the number of spanning trees.
</li><li>Given a graph: One can count the number of three colorings of the graph.
</li><li>Given a polynomial equation: One can count the number of solutions.
</li><li>And so on.
</li></ol>
<p>The conjecture is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> is not equal to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = \#P}}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P=NP}}" class="latex" /> and more. Thus it probably is the case that they are not equal. But like much of complexity theory, this is wide open. </p>
<p>
</p><h2> Computing <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" />—the hard way </h2><p></p>
<p>
Another way to compute <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> is via number theory not via complexity theory. This is what Thompson does—with no unproved assumption about <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%23P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\#P}" class="latex" />.</p>
<p>
She proves this main result in her <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/mertensnew.pdf">paper</a>—this is the paper we previously cited.</p>
<blockquote><p><b>Theorem 2</b> <em> There is an algorithm that computes <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> in time 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O_%7B%5Cepsilon%7D%28x%5E%7B%5Cdelta%7D+%5Clog%5E%7B%5Cdelta%2B%5Cepsilon%7D+x%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  O_{\epsilon}(x^{\delta} \log^{\delta+\epsilon} x). " class="latex" /></p>
</em><p><em>Here <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%3D3%2F5%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta=3/5}" class="latex" /> and the constant in <img src="https://s0.wp.com/latex.php?latex=%7BO_%7B%5Cepsilon%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O_{\epsilon}}" class="latex" /> depends on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" />. </em>
</p></blockquote>
<p>
Note the time used is <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> to some power. Before on the assumption that <img src="https://s0.wp.com/latex.php?latex=%7BP%3D%5C%23P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P=\#P}" class="latex" /> we get that the time is polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log x}" class="latex" />.</p>
<p>
The proof is non-trivial. In order to give some idea we will give a few comments. Standard identities allow one to write, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%28x%29+%3D+2M%28%5Csqrt%7Bx%7D%29+-+%5Csum+%5Cmu%28m_1%29%5Cmu%28m_2%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  M(x) = 2M(\sqrt{x}) - \sum \mu(m_1)\mu(m_2), " class="latex" /></p>
<p>over 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m_1%2Cm_2+%5Cle+%5Csqrt%7Bx%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  m_1,m_2 \le \sqrt{x}. " class="latex" /></p>
<p>The insight is to look hardest at those values	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m_1%2Cm_2+%5Cle+x%5E%7B2%2F5%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  m_1,m_2 \le x^{2/5}. " class="latex" /></p>
<p>Their summary of what happens next:</p>
<blockquote><p><b> </b> <em> Our approach in Section 4 roughly amounts to analyzing the difference between reality and a model that we obtain via Diophantine approximation, in that we show that this difference has a simple description in terms of congruence classes and segments. This description allows us to compute the difference quickly, in part by means of table lookups. </em>
</p></blockquote>
<p></p><p>
As often, we say to see the full paper for the details. Or you can follow her talk either in <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/laten_charla.pdf">Spanish</a> or in <a href="http://www.lolathompson.com/uploads/1/1/0/6/110629329/copy_of_jmm_2021_talk.pdf">English</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/mertens/" rel="attachment wp-att-18615"><img width="480" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/Mertens.jpg?resize=480%2C308&amp;ssl=1" class="aligncenter wp-image-18615" height="308" /></a></p>
<p></p><p><br />
The proof is clever and there seems to be no way to improve it to anything near what we get via complexity theory. Of course we can only do that in the presence of a very strong conjecture: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BP+%3D+%5C%23P%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathsf{P = \#P}. " class="latex" /></p>
<p>
But what she could do was to implement the algorithm in her joint paper. The computation ran on a many-core machine for weeks. It gets <img src="https://s0.wp.com/latex.php?latex=%7BM%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M(x)}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cle+10%5E%7B22%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \le 10^{22}}" class="latex" />. </p>
<p>
This is one advantage of algorithms that do not rely on unproved conjectures. They can actually be implemented and executed. </p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>
Is there a richer fount for open problems than number theory?</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/18/summing-up-the-primes/"><span class="datestr">at April 18, 2021 01:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/04/17/picks-shoelaces">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/04/17/picks-shoelaces.html">Pick’s shoelaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Two important methods for computing area of polygons in the plane are <a href="https://en.wikipedia.org/wiki/Pick%27s_theorem">Pick’s theorem</a> and the <a href="https://en.wikipedia.org/wiki/Shoelace_formula">shoelace formula</a>. For a simple lattice polygon (a polygon with a single non-crossing boundary cycle, all of whose vertex coordinates are integers) with \(i\) integer points in its interior and \(b\) on the boundary, Pick’s theorem computes the area as</p>

\[A=i+b/2-1.\]

<p>The shoelace formula has various formulations, but in the version I’m going to use, it is a sum over oriented edges of the polygon. Let \((x,y)\to (x',y')\) denote an edge of the polygon that connects the two points \((x,y)\) and \((x',y')\), oriented in the clockwise direction around the polygon so that if you travel from \((x,y)\) to \((x',y')\) along this line segment, then the interior of the polygon will be on your right. Then, according to the shoelace formula, the area is</p>

\[A= \frac{1}{2}\sum_{(x,y)\to (x',y')}(x'-x)(y'+y).\]

<p>The shoelace formula looks messier than Pick’s formula but it’s much easier for computers to evaluate, both because polygons are generally represented in computers by their boundary rather than by the points they contain, and because there are often many fewer boundary edges than interior lattice points. It also doesn’t need the coordinates to be integers. But since these two formulas produce the same value, it would be interesting to see a more direct relation between them, explaining one formula in terms of the other.</p>

<h1 id="proof-of-the-shoelace-formula">Proof of the shoelace formula</h1>

<p>It is convenient to shift the polygon to lie entirely above the \(x\)-axis, without changing its area or number of grid points. After this shift, the nonzero terms \(\tfrac{1}{2}(x'-x)(y'+y)\) of the shoelace formula can be interpreted as the signed areas of trapezoids, extending vertically down from each non-vertical edge \((x,y)\to (x',y')\) to the \(x\)-axis. The reason for doing this shift is to orient all of the trapezoids the same way, downwards from their edges, and to avoid complications with edges that cross the \(x\)-axis.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pick-trapezoids.svg" alt="Trapezoids extending downward from each edge of a polygon" /></p>

<p>Now consider what you would see from a generic point in the upper half-plane, if you looked straight upwards. (By “generic”, I mean that the point isn’t on an edge or directly below a vertex, because these would introduce additional and unimportant cases to our analysis.) Your line of site would pass through a sequence of zero or more polygon edges, whose trapezoids have positive sign when the line passes from inside the polygon to outside and negative sign when the line passes from outside to inside. Because of this alternation between inside and outside, between positive and negative, a point outside the polygon sees equally many edges of each sign, but a point in the polygon sees one more positive edge than negative.</p>

<p>Another way of expressing the same counts of signed edges above each point involves the <em>characteristic functions</em> of the polygon and the trapezoids, functions that are 1 inside each shape and zero outside it. The area of a shape is just the integral of its characteristic function. We’ll leave these functions undefined on the boundary of the shapes, but that’s ok because the boundary points contribute nothing to the total area. Then because of the cancellation between positive and negative signs, at the generic points where all of these functions are defined, the characteristic function of the polygon equals the signed sum of  characteristic functions of trapezoids. By the sum rule for integrals, the area of the polygon equals the signed sum of trapezoid areas. With a little more complication, the same argument can also be made to work directly for unshifted polygons.</p>

<h1 id="counting-lattice-points-in-trapezoids">Counting lattice points in trapezoids</h1>

<p>Can we use the same argument to prove Pick’s theorem, in the form that the shoelace formula equals Pick’s formula?  We’d like to interpret each term of the shoelace formula as a count over grid points, decompose the polygon in the same way into a sum of trapezoids, and argue that counting points in each trapezoid and then summing produces the same result as summing the trapezoids and then counting points. The difficulty is that now we can’t ignore the points on the axis or on boundary of the trapezoids, because their contribution to the count is nonzero.</p>

<p>First, let’s see how we can interpret each shoelace term as a grid point count.
Rotating a trapezoid around the midpoint of its defining edge produces another trapezoid whose union with the first trapezoid is an axis-aligned rectangle. Pick’s theorem is easy to see for these rectangles, in the simplified form that the area is the sum of one unit for integer points inside the rectangle, half a unit for integer points on its edges, and a quarter of a unit for integer points at its vertices. These fractional numbers of units are merely the amounts of rectangle area nearest to each point.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pick-rectangle.svg" alt="The area of a lattice trapezoid equals the sum of units of lattice points: a whole unit for points in the trapezoid, half a unit on the boundary, and a quarter unit for the four corners" /></p>

<p>For the trapezoids we will use the same assignment of units to lattice points: one unit for points in the trapezoid, half on its boundary, and a quarter at its vertices, regardless of the actual trapezoid angles at those vertices. The figure above shows each point decorated in blue with its number of units.
The equality of area with total units still holds true, because both the area of the trapezoid and the total number of units for its integer points are half that of the rectangle. Each point off the edge that contributes its units to the trapezoid has a reflection that does not contribute its units. And each point on the edge contributes half its units to the trapezoid and half to the reflection. So the same terms \(\tfrac{1}{2}(x'-x)(y'+y)\) of the shoelace formula also count the contributions of units in each trapezoid to Pick’s formula.</p>

<h1 id="picks-formula-from-sums-of-trapezoids">Pick’s formula from sums of trapezoids</h1>

<p>We know how much each lattice point contributes to Pick’s formula: one unit if it is inside the polygon, half if it is on the boundary, or zero if it is outside. We also know how much each lattice point contributes to each trapezoid of the shoelace formula: one unit if it is interior to the trapezoid, half if it is on an edge, a quarter if it is on a corner, and none if it is exterior. In order to prove that Pick’s formula and the shoelace formula are equal, we want to show that all points make equal contributions. And for most points, this turns out to be true.</p>

<p>The vertical ray from any point \(p\) may pass through the boundary of the polygon in multiple ways: \(p\) itself may lie on the boundary, the ray may cross an edge of the boundary, it may pass through a vertex of the boundary that has edges to its left and right, or it may brush past the boundary at a vertex that has edges only to the left or only to the right. We can skip over these last “brush past” cases, because the two trapezoids from these edges have opposite signs and cancel each other in the total trapezoid contribution of \(p\). If \(p\) lies on an edge, then it gets a (positive or negative) contribution of \(1/2\), and when \(p\) is a vertex of the polygon with edges extending left and right, then it gets the same contribution in two quarters. The remaining cases make a contribution of \(\pm 1\), and as for the area calculation they alternate in sign. Canceling these alternating contributions shows that \(p\) always has a total contribution from its trapezoids equal to its contribution to Pick’s formula.</p>

<p>I thought at first that the points on the \(x\)-axis might need a different calculation, because they get only half as much from each trapezoid. But half zero is still zero; they contribute nothing to the sum of trapezoids and nothing to the Pick formula. The points that do need special treatment are the polygon vertices at which the two incident edges do not extend to the left and right, either because one is vertical or because both extend in the same direction. The contribution to Pick’s formula for these vertices is still \(1/2\), but the total contribution from the trapezoids is either an integer (if the incident edges extend in the same direction) or a quarter-integer (if one edges is vertical). So we’ll have to count how much we’ve missed at those points.</p>

<p>Consider driving around the polygon clockwise, in the same direction that we oriented the edges. As you drive, you can make a right turn (from rightward to vertically down, vertically down to leftward, etc), a double-right u-turn, a left turn, or a double-left turn. Points at which the polygon makes an angle but continues in the same general left-to-right or right-to-left direction don’t count as turns. Then at each right turn Pick’s formula will run a deficit of 1/4 unit in total contributions, compared to the sum of trapezoids. A double-right u-turn gives a deficit of 1/2 unit, the same as two right turns. A left turn gives 1/4 more unit to Pick than to the sum of trapezoids, and a double-left gives 1/2 more unit. To return to your starting direction, you must make four more right turns than left, so the total difference in contributions from these terms comes out to exactly the \(-1\) correction term in Pick’s formula.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pick-deficits.svg" alt="Differences in contributions at turning points of the e polygon" /></p>

<p>In summary, when we sum contributions over all points, the points inside the polygon contribute one unit to Pick’s formula and one unit to the sum of trapezoids. The points outside the polygon contribute zero to both. The points on the boundary that are not turns contribute \(1/2\) to both. And the points on the boundary that are turns contribute different amounts to Pick’s formula and to the sum of trapezoids, with the total of these differences equalling the \(-1\) correction term in Pick’s formula. Therefore, the overall value of Pick’s formula equals the sum of point contributions in trapezoids, which equals the signed sum of trapezoid areas, which equals the polygon area.</p>

<h1 id="generalizations-of-picks-formula">Generalizations of Pick’s formula</h1>

<p>The same idea lets us generalize Pick’s formula to a polygon with holes, defined as a connected region of the plane whose boundary is a disjoint union of simple polygons. The shoelace formula for area in the version we’re using here, and its proof, need no change for this generalization. For each hole, we should drive counterclockwise rather than clockwise, to stay consistent with the orientation we have given the edges; the same argument shows that each hole produces a positive, rather than negative, total difference between the contributions to Pick’s formula and to the shoelace formula. Therefore, for a polygon with \(h\) holes, the total area becomes</p>

\[A=i+b/2+h-1.\]

<p>The basic idea for all this, by the way, comes from the paper “Pick’s theorem”, by Branko Grünbaum and G. C. Shephard (<a href="https://doi.org/10.2307/2323771"><em>Amer. Math. Monthly</em> 1993</a>). Rather than using the trapezoid version of the shoelace formula, Grünbaum and Shephard use a version of the formula that sums, over each edge of the polygon, the signed area of the triangle formed by each edge plus the origin. I think this makes the analysis a little messier, but it’s otherwise much the same as the analysis here. The formula for polygons with holes can be found in “On the compactness of subsets of digital pictures” by Sankar and Krishnamurthy (<a href="https://doi.org/10.1016/s0146-664x(78)80021-5"><em>CGIP</em> 1978</a>) but without the careful definition of polygons with holes that is necessary to avoid problems here.</p>

<p>Grünbaum and Shephard also generalize Pick’s formula in a slightly different direction: rather than allowing separate holes in their polygons, they define polygons to have only one boundary polygon, but they allow that polygon to cross itself. Their version of Pick’s theorem for this kind of generalized polygons sums a certain half-integral index of each lattice point (essentially, the winding number of the polygon around that point, averaged between open and closed versions of the polygon), with a correction term coming from the turning number of the whole polygon. It should be possible to combine both generalizations, and allow polygon boundaries that both cross and have multiple components. There’s a little ambiguity about which way the boundary is connected at vertices of degree higher than two, though, which would need to be resolved somehow if such vertices are to be allowed, because different choices are likely to cause pieces of the boundary to have different orientations leading to different areas.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106084926459254807">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/04/17/picks-shoelaces.html"><span class="datestr">at April 17, 2021 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/04/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/04/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Keller%27s_conjecture">Keller’s conjecture</a> (<a href="https://mathstodon.xyz/@11011110/105994491983819823">\(\mathbb{M}\)</a>), another new Good Article on Wikipedia. The conjecture was falsified in 1992 with all remaining cases solved by 2019, but the name stuck. It’s about tilings of \(n\)-space by unit cubes, and pairs of cubes that share \((n-1)\)-faces. In 2d, all squares share an edge with a neighbor, but a 3d tiling derived from <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix</a> has many cubes with no face-to-face neighbor. Up to 7d, some cubes must be face-to-face, but tilings in eight or more dimensions can have no face-to-face pair.</p>
  </li>
  <li>
    <p><a href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/">Italians and bibliometrics</a> (<a href="https://mathstodon.xyz/@11011110/105996551762053680">\(\mathbb{M}\)</a>): Luca Trevisan (a leading theorist with 7 SODA papers, 2 FOCS papers, a JACM paper and a SICOMP paper in the last four years) gets dinged for poor productivity as the Italian system only counts journal papers that do not match conference papers. The fact that these are all in top venues is irrelevant, and the conference papers count only negatively against matching journal papers. Comments discuss similar problems in other countries.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5402">What is the computational complexity of dinosaur train tracks?</a> (<a href="https://mathstodon.xyz/@11011110/106009429519024889">\(\mathbb{M}\)</a>).  Answer: not very high, because the only usable junction, a Y that remembers which way you came through it and sends you the same way if you come back through the other direction, is just not powerful enough to do much.</p>
  </li>
  <li>
    <p>Congratulations to Martín Farach-Colton, Shang-Hua Teng, and all of the other <a href="https://sinews.siam.org/Details-Page/siam-announces-class-of-2021-fellows">new SIAM Fellows</a> (<a href="https://mathstodon.xyz/@11011110/106016827432463028">\(\mathbb{M}\)</a>)!</p>
  </li>
  <li>
    <p><a href="https://writings.stephenwolfram.com/2021/03/a-little-closer-to-finding-what-became-of-moses-schonfinkel-inventor-of-combinators/">Stephen Wolfram tries to track down</a> what happened to logician <a href="https://en.wikipedia.org/wiki/Moses_Sch%C3%B6nfinkel">Moses Schönfinkel</a> (<a href="https://mathstodon.xyz/@11011110/106025064389253132">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26694685">via</a>), who worked in Göttingen from 1914 to 1924, returned to Moscow, and then “basically vanished”. Wikipedia has more detail about what happened after (mental health issues, death around 1942), but Wolfram says the evidence for all that is weak. He doesn’t make direct progress on Schönfinkel himself but does find some relatives.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2021/04/my-robot-draws-tex/">How Christian Lawson-Perfect got a pen plotter to draw mathematical notation using TeX</a> (<a href="https://mathstodon.xyz/@christianp/106030474747952758">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.improbable.com/2021/04/08/a-look-way-back-at-some-bearded-mathematicians/">When mathematicians wore geometric beards</a> (<a href="https://mathstodon.xyz/@11011110/106034106736789855">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5437">Aaronson on politicization of research prizes</a> (<a href="https://mathstodon.xyz/@11011110/106039607813461033">\(\mathbb{M}\)</a>). Jeff Ullman won the Turing Award despite deplorable (some say racist) treatment of grad applicants for the crime of being Iranian, and Oded Goldreich was blocked from the Israel Prize for anti-settlement politics. Politicization is two-edged. I’d rather see Ullman awarded for his worthy contributions, and use the opportunity to decry his abhorrent actions and statements, than subject prizes to litmus tests from all sides.</p>
  </li>
  <li>
    <p><a href="https://thatsmaths.com/2021/04/08/circles-polygons-and-the-kepler-bouwkamp-constant/">Circles, polygons and the Kepler-Bouwkamp constant</a> (<a href="https://mathstodon.xyz/@11011110/106045568373359503">\(\mathbb{M}\)</a>). On the limiting behavior of infinitely-nested shapes alternating between circles and polygons with increasing numbers of sides.</p>
  </li>
  <li>
    <p><a href="https://www.technologyreview.com/2021/04/09/1022217/facebook-ad-algorithm-sex-discrimination">Continuing gender bias in who sees job-opening ads on Facebook</a> (<a href="https://mathstodon.xyz/@11011110/106051162687419819">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26760790">via</a>): if an employer or industry has historically skewed male or female, Facebook replicates that bias, even for pairs of ads with identical qualifications. This is illegal, but Facebook appears unable to find a technical fix and unwilling to apply the obvious fix of not targeting its ads even when that targeting is illegal. As usual for Hacker News via links on topics related to social justice, don’t read the comments there.</p>
  </li>
  <li>
    <p>Amusing quote from McLarty’s 2003 “<a href="http://www.landsburg.com/grothendieck/mclarty1.pdf">Grothendieck on simplicity and generality</a>” (<a href="https://mathstodon.xyz/@11011110/106059214777848356">\(\mathbb{M}\)</a>, <a href="https://golem.ph.utexas.edu/category/2021/04/algebraic_closure.html">via</a>): “Serre created a series of concise elegant tools which Grothendieck and coworkers simplified into thousands of pages of category theory.” Nowadays I guess the people doing this sort of simplification are the ones formulating machine-verifiable proofs…</p>
  </li>
  <li>
    <p><a href="https://projects.cs.dal.ca/wads2021/wads-2021-accepted-papers/">WADS 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/106062479142848045">\(\mathbb{M}\)</a>). 
The biennial Algorithms and Data Structures Symposium is usually in Canada, and this time was supposed to be in Halifax, but is looking very likely to be completely online, this August. I have one paper on the list; I’ll write more about it later when I have a preprint version ready to share.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=S5fPwE7GQOA">A “confounding topological curiosity”</a> (<a href="https://mathstodon.xyz/@11011110/106068163014347342">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2021/04/13/heres-a-confounding-topological-curiosity.html">via</a>): a double torus with a line through one of its holes can be continuously transformed so that the line instead goes through both holes.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11818">Strange inverses in the group rings of torsion-free groups</a> (<a href="https://mathstodon.xyz/@11011110/106072203556292078">\(\mathbb{M}\)</a>, <a href="https://www.quantamagazine.org/mathematician-disproves-group-algebra-unit-conjecture-20210412/">via</a>, <a href="https://www.uni-muenster.de/MathematicsMuenster/news/artikel/2021-03-04.shtml">see also</a>). This result of Giles Gardam disproves the strongest of the three <a href="https://en.wikipedia.org/wiki/Kaplansky%27s_conjectures">Kaplansky conjectures on group rings</a>. It’s just an isolated example at this point but it does show that group rings are less well-behaved than had been hoped.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/04/15/linkage.html"><span class="datestr">at April 15, 2021 10:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/">TOC: a Personal Perspective (2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Editorial note</strong>: this post has been written in celebration of 25 years for “<a href="http://www.wisdom.weizmann.ac.il/~oded/toc-sp.html">TOC: a Scientific Perspective (1996),</a>” by <a href="http://www.wisdom.weizmann.ac.il/~oded/">Oded Goldreich</a> and <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>. In the process, I have been made aware of a Facebook discussion from a few weeks ago (which I don’t know how to link to), and to <a href="https://simons.berkeley.edu/talks/tbd-271">Avi Wenderson’s recent talk</a> that addresses this discussion (and more). I will not attribute statements from the Facebook discussion to individuals (nor will I specify its initiator), as they may not identify with the statements, when taken out of their original context or in the editorialized version here. In any case, this specific discussion is not the point. Feel free to claim ownership in comments .</p>



<p>———————————–</p>



<p>I’m sure that, like me, you read on social media: “TOC is in crisis, scratch that, it is lost! We do not have agreed-upon challenges (that are not way out of reach) and do not know how to evaluate papers. Therefore our conferences are favoring progress in techniques and favor complicated papers on obscure problems over progress on important problems (in fact, there is shortage of interesting work on relevant problems). Our flagship conferences are broken and newer conferences, that aimed to do better, have become just more of the same. This is not TCS as we remember it!”</p>



<p>There are many points with which I agree. Like others, I have been critical at times about the way we do some things, mourning papers and subfields that our conferences have missed. On several occasions, I have been pushing for change. At times I was successful but in other instances <a href="https://windowsontheory.org/2015/06/08/can-we-get-serious/">my suggestions </a>were deemed radical by the powers that be (and more modest/timid suggestions were adopted). <strong><em>But did TOC really lose its way?</em> </strong> </p>



<p>One of the commenter was saying “I have been hearing these complaints about focs and Stoc for more than ten years. They have come from powerful people that sit on committees. … So why nothing changes?“ This comment strikes a chord with me, but let me revise it and say that I have been hearing such complaints for the last 25 years (since attending my first conference), and it is almost always stated by the powerful people, those that have the responsibility to shape the field.</p>



<p>Following the continuous self-criticism, we are likely to assume that TOC is a dysfunctional field and has been so for many years. But if we look at the research achievements of TOC in the last quarter century, we must conclude that this was a glorious period. And the contributions of TCS were on different fronts. Contributions to applied CS and industry, growing contributions outside of CS as well as progress on fundamental questions within TOC. Said progress was obtained by simple papers and by complex and long papers. By papers developing new techniques, by papers making progress on known problems and by papers that introduced new problems, models or even papers that initiated new subfields. They have been made by breakthrough papers and by long sequences of modest papers. By papers in FOCS/STOC and papers in other conferences. So <strong>if TOC is in a continuous crisis, it is the most wonderful crisis possible</strong>.</p>



<p>During my studies (ages ago), I was intensely attracted to TOC. But at the same time, I felt that the field is under constant external attack. It was claimed that we are not as deep as Math and not as useful as CS. Many fewer universities than today have been hiring theoreticians. The field was grossly underfunded (still underfunded but less grossly) but still calls have been made to reduce funding to any area in which TOC is not directly serving other, more applied areas. The dissonance between my intuitive attraction and external criticism could have deterred me from TOC, but there were incredible leaders of TOC that effectively defended the field and shouted – look, something amazing is happening here. “TOC: a Scientific Perspective (1996),” by Oded Goldreich and Avi Wigderson gave me courage to continue. 25 years later, the case they once made in defense of TOC is so much easier to support (and they kept on making this case throughout the years in essays and <a href="https://www.math.ias.edu/avi/book">books</a>). <a href="https://simons.berkeley.edu/talks/tbd-271">As Avi argued</a>, TOC’s success have brought growth and diversification, influx of young talent, scientific respect, industrial respect, and societal respect. <strong>We should do better on self-respect.</strong></p>



<p>I am of course not advocating resting on our laurels’. Like in Alice’s adventures, in our fast field, it takes all the running we can do, to keep in the same place. If we want to get somewhere else, we must run at least twice as fast as that! Constructive criticism is a good thing but our tendency for alarmist/defeatist cries is not serving us well. As someone who grew (scientifically) in an atmosphere of struggle, I am grateful for the progress made in establishing our field by the generations that preceded me. We shouldn’t take for granted how easy we have it. But more importantly, confidence in our field and optimism towards our future are important for our impact on the world (I discussed one aspect of this <a href="https://theorydish.blog/2019/06/24/on-the-importance-of-disciplinary-pride-for-multidisciplinary-collaboration/">here</a>). Finally, thinking of students interested in TOC today and hearing the most powerful people in the field announcing that it is lost. I ask myself, who are the Avi and Oded that will give them the needed courage and optimism? The answer is still that their Avi and Oded are the very same Avi and Oded from my student years. But isn’t it about time that we lend a hand?</p>



<p></p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/"><span class="datestr">at April 15, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18549">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/">Scott Wins a Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Quantum mechanics makes absolutely no sense—Roger Penrose</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/aaronsonhorgan/" rel="attachment wp-att-18565"><img width="160" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/AaronsonHorgan.jpg?resize=160%2C170&amp;ssl=1" class="alignright size-full wp-image-18565" height="170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Scott Answers Big Questions <a href="https://blogs.scientificamerican.com/cross-check/scott-aaronson-answers-every-ridiculously-big-question-i-throw-at-him/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Scott Aaronson has just been named the 2020 ACM <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">Prize</a> in Computing for groundbreaking contributions to quantum computing, Penrose’s comment notwithstanding.  The prize <a href="https://awards.acm.org/about/2020-acm-prize">citation</a> also credits Scott’s multifaceted public outreach for making our fields accessible to many.</p>
<p>
Today Ken and I send congrats to Scott for this singular honor.<br />
<span id="more-18549"></span></p>
<p>
The ACM Prize was founded in 2007.  It does not have the age limit of a Fields Medal but is similarly positioned.  Scott joins a impressive list of winners: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/who-2/" rel="attachment wp-att-18562"><img width="550" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/who-1.png?resize=550%2C625&amp;ssl=1" class="aligncenter wp-image-18562" height="625" /></a></p>
<p></p><p><br />
</p><h2> Not Why He Did Win? </h2><p></p>
<p>
Scott perhaps could have won for the following three accomplishments:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His wonderful <a href="https://www.scottaaronson.com/blog/">blog</a>. About the prize, he <a href="https://www.scottaaronson.com/blog/?p=5448">says</a> there: </p>
<blockquote><p><b> </b> <em> Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently. Am I in trouble? … Luckily, Dina only wanted to tell me that I’d been selected to receive the 2020 ACM Prize in Computing, a mid-career award founded in 2007 that comes with $250,000 from Infosys. Not the Turing Award but I’d happily take it! And I could even look back on 2020 fondly for something. </em>
</p></blockquote>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">book</a> on quantum: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/book-6/" rel="attachment wp-att-18554"><img width="160" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/book-1.jpg?resize=160%2C240&amp;ssl=1" class="aligncenter wp-image-18554" height="240" /></a></p>
<p>
Democritus was known as “the laughing philosopher,” though some other depictions of the laugh range from world-weary to pained.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His sense of <a href="https://www.scottaaronson.com/blog/?p=62">humor</a>. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/kl2/" rel="attachment wp-att-18563"><img width="600" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/kl2.png?resize=600%2C351&amp;ssl=1" class="aligncenter size-large wp-image-18563" height="351" /></a></p>
<p>
This is my returning thanks, in a way. Ken says that what impresses him is not the floor-length garment but the floor-length blackboard. Both of us hope that after handling “For All” and “Exists,” he went on to solve the problem of placing “Not” in English—for instance, “Why He Did Not Win?” is more grammatically natural but wrong.</p>
<p></p><h2> Why He Did Win? </h2><p></p>
<p>
Scott perhaps did win for the following four accomplishments:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> The theoretical foundations of the sampling-based quantum supremacy <a href="https://arxiv.org/pdf/1612.05903.pdf">experiments</a>—joint with Lijie Chen. Ken adds that this paper is also known for the “Schrödinger” and “Feynman” nomenclature for simulating quantum classically, and the idea of hybridizing those approaches.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> The algebrization <a href="https://www.scottaaronson.com/papers/alg.pdf">barrier</a> in complexity theory—joint with Avi Wigderson.  About Avi’s talk on this at my 60th birthday workshop, Ken <a href="https://blog.computationalcomplexity.org/2008/05/report-on-sym-for-liptons-60th-bday.html">wrote</a> that it “planted a Monty Python foot on further progress” on lower bounds.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> Limitations on quantum computers—work on the quantum lower bound for the <a href="https://www.scottaaronson.com/papers/collision.ps">collision problem</a>.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /> The opposite of limitations—getting quantum advantage from the simplest of components in linear <a href="https://dl.acm.org/doi/10.1145/1993636.1993682">optics</a>. And <a href="https://arxiv.org/abs/1309.7460">this</a>, likewise joint with Alex Arkhipov. This approach has been followed by many, notably last <a href="https://science.sciencemag.org/content/370/6523/1460">December</a> by a large team in China.  We just <a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/">wrote</a> about envy of big experiments, but Scott has arguably done the most of anyone in our field to launch them.</p>
<p>
There are many more. But maybe the one that will get us all rich is Scott’s <a href="https://arxiv.org/abs/1110.5353">work</a> on <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>. Qubitcoin, anyone?</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
Wonderful to add Scott to the list of winners of this award. Congrats again.</p>
<p>
We also congratulate Paul Beame on the SIGACT Distinguished Service Award.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/"><span class="datestr">at April 15, 2021 01:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3988126252359540708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html">Ordering Beauty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>First, congratulations to fellow complexity theorist and <a href="https://www.scottaaronson.com/blog/">blogger</a> Scott Aaronson for <a href="https://awards.acm.org/about/2020-acm-prize">receiving the 2020 ACM Prize in Computing</a> for "groundbreaking contributions to quantum computing". The prize is ACM's highest honor for mid-career researchers. Well deserved! </p><p>Now back to our regularly scheduled post...</p><p>Every freshman at Cornell back in 1981 had to take two seminar courses, basically one-shot courses in an usually humanities area which required no prerequisites but lots of writing. I took my first course in philosophy. The instructor, a PhD student, at one point described his research, a philosophical argument that there is an intrinsic total ordering of beauty, say that Beethoven would always sit above the Beatles, no matter the beholder. I didn't believe him then and still don't today. A few months ago the Washington Post ran a story with the same theme entitled <a href="https://www.washingtonpost.com/entertainment/maradona-messi-ronaldo-zlatan-shakespeare-beatles/2020/12/23/27654712-38a9-11eb-9276-ae0ca72729be_story.html">Maradona was great, and maybe the greatest. Can we make similar claims about artists?</a></p><p>Somehow we have this belief when it comes to conference submissions, that there is some perfect ordering of the submissions and a good PC can suss it out. That's not really how it works. Let's say a conference has an accept rate of 30%. Typically 10% of the submissions are strong and will be accepted by any committee. About half the submissions are just okay or worse and would be rejected. The other 40% of the submissions will be chosen seemingly randomly based on the tastes of the specific members of the program committee. Experiments in the NeurIPS and ESA conferences have bourn this out. </p><p>Why not make the randomness explicit instead of implicit? Have the PC divide the papers into three piles, definite accepts, definite rejects and the middle. Take the third group and randomly choose which ones to accept. It will create a more interesting program. Also randomness removes biases, randomness doesn't care about gender, race and nationality or whether the authors are at senior professors at MIT or first year grad students at Southern North Dakota. </p><p>We put far too much weight on getting accepted into a conference given the implicit randomness of a PC. If we make the randomness explicit that would devalue that weight. We would have to judge researchers on the quality of their research instead of their luck in conferences.</p><p>Given that conferences, especially the virtual ones, have no real limits on the number of papers and talks, you might say why not just accept all the papers in the middle. Works for me.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html"><span class="datestr">at April 15, 2021 12:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/054">TR21-054 |  Encodings and the Tree Evaluation Problem | 

	Ian Mertz, 

	James  Cook</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that the Tree Evaluation Problem with alphabet size $k$ and height $h$ can be solved by branching programs of size $k^{O(h/\log h)} + 2^{O(h)}$. This answers a longstanding challenge of Cook et al. (2009) and gives the first general upper bound since the problem's inception.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/054"><span class="datestr">at April 14, 2021 07:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5448">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5448">The ACM Prize thing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently.  <em>Am I in trouble?  For what, though??  I haven’t even worked at MIT for five years!</em></p>



<p>Luckily, Dina only wanted to tell me that I’d been <a href="https://awards.acm.org/about/2020-acm-prize">selected</a> to receive the 2020 <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">ACM Prize in Computing</a>, a mid-career award founded in 2007 that comes with $250,000 from Infosys.  Not the Turing Award but I’d happily take it!  And I could even look back on 2020 fondly for something.</p>



<p>I was utterly humbled to see the <a href="https://awards.acm.org/acm-prize/award-winners">list</a> of past ACM Prize recipients, which includes amazing computer scientists I’ve been privileged to know and learn from (like Jon Kleinberg, Sanjeev Arora, and Dan Boneh) and others who I’ve admired from afar (like Daphne Koller, Jeff Dean and Sanjay Ghemawat of Google MapReduce, and David Silver of AlphaGo and AlphaZero).</p>



<p>I was even more humbled, later, to read my <a href="https://awards.acm.org/award_winners/aaronson_9555914">prize citation</a>, which focuses on four things:</p>



<ol><li>The theoretical foundations of the sampling-based quantum supremacy experiments now being carried out (and in particular, my and Alex Arkhipov’s <a href="https://www.theoryofcomputing.org/articles/v009a004/">2011 paper on BosonSampling</a>);</li><li>My and Avi Wigderson’s <a href="https://www.scottaaronson.com/papers/alg.pdf">2008 paper</a> on the algebrization barrier in complexity theory;</li><li>Work on the limitations of quantum computers (in particular, the 2002 <a href="https://www.scottaaronson.com/papers/collision.pdf">quantum lower bound for the collision problem</a>); and</li><li>Public outreach about quantum computing, including through <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">QCSD</a>, popular talks and articles, and this blog.</li></ol>



<p>I don’t know if I’m worthy of such a prize—but I know that if I am, then it’s mainly for work I did between roughly 2001 and 2012.  This honor inspires me to want to be more like I was back then, when I was driven, non-jaded, and obsessed with figuring out the contours of BQP and efficient computation in the physical universe.  It makes me want to justify the ACM’s faith in me.</p>



<p>I’m grateful to the committee and nominators, and more broadly, to the whole quantum computing and theoretical computer science communities—which I “joined” in some sense around age 16, and which were the first communities where I ever felt like I belonged.  I’m grateful to the mentors who made me what I am, especially Chris Lynch, Bart Selman, Lov Grover, Umesh Vazirani, Avi Wigderson, and (if he’ll allow me to include him) John Preskill.  I’m grateful to the slightly older quantum computer scientists who I looked up to and tried to emulate, like Dorit Aharonov, Andris Ambainis, Ronald de Wolf, and John Watrous.  I’m grateful to my wonderful colleagues at UT Austin, in the CS department and beyond.  I’m grateful to my students and postdocs, the pride of my professional life.  I’m grateful, of course, to my wife, parents, and kids.</p>



<p>By coincidence, my <a href="https://www.scottaaronson.com/blog/?p=5437">last post</a> was also about prizes to theoretical computer scientists—in that case, two prizes that attracted controversy because of the recipient’s (or would-be recipient’s) political actions or views.  It would understate matters to point out that not everyone has always agreed with everything I’ve said on this blog.  I’m <em>ridiculously</em> lucky, and I know it, that even living through this polarized and tumultuous era, I never felt forced to choose between academic success and the freedom to speak my conscience in public under my real name.  If there’s been one constant in my public stands, I’d like to think that—inspired by memories of my own years as an unknown, awkward, self-conscious teenager—it’s been my determination to nurture and protect talented young scientists, whatever they look like and wherever they come from.  And I’ve tried to live up to that ideal in real life, and I welcome anyone’s scrutiny as to how well I’ve done.</p>



<p>What should I do with the prize money? I confess that my first instinct was to donate it, in its entirety, to some suitable charity—specifically, something that would make all the strangers who’ve attacked me on Twitter, Reddit, and so forth over the years realize that I’m fundamentally a good person.  But I was talked out of this plan by my family, who pointed out that<br />(1) in all likelihood, <em>nothing</em> will make online strangers stop hating me,<br />(2) in any case this seems like a poor basis for making decisions, and<br />(3) if I really want to give others a say in what to do with the winnings, then why not everyone who’s stood by me and supported me?</p>



<p>So, beloved commenters!  Please mention your favorite charitable causes below, especially weird ones that I wouldn’t have heard of otherwise.  If I support their values, I’ll make a small donation from my prize winnings.  Or a larger donation, especially if you donate yourself and challenge me to match.  Whatever’s left after I get tired of donating will probably go to my kids’ college fund.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> And by an amusing coincidence, today is apparently <a href="https://worldquantumday.org/">“World Quantum Day”</a>!  I hope your Quantum Day is as pleasant as mine (and stable and coherent).</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5448"><span class="datestr">at April 14, 2021 03:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/053">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/053">TR21-053 |  Information in propositional proofs and algorithmic proof search | 

	Jan  Krajicek</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study from the proof complexity perspective the (informal) proof search problem:
Is there an optimal way to search for propositional proofs?
We note that for any fixed proof system there exists a time-optimal proof search algorithm. Using classical proof complexity results about reflection principles we prove that a time-optimal proof search algorithm exists w.r.t. all proof systems iff a p-optimal proof system exists.
To characterize precisely the time proof search algorithms need for individual formulas we introduce a new proof complexity measure based on algorithmic information concepts. In particular, to a proof system P we attach {\bf information-efficiency function} $i_P(\tau)$ assigning to a tautology a natural number, and we show that:
- $i_P(\tau)$ characterizes time any $P$-proof search algorithm has to use on $\tau$ and that for a fixed $P$ there is such an information-optimal algorithm,
- a proof system is information-efficiency optimal iff it is p-optimal,
- for non-automatizable systems $P$ there are formulas $\tau$ with short proofs but having large information measure $i_P(\tau)$.
We isolate and motivate the problem to establish {\em unconditional} super-logarithmic lower bounds for $i_P(\tau)$ where no super-polynomial size lower bounds are known. We also point out connections of the new measure with some topics in proof complexity other than proof search.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/053"><span class="datestr">at April 13, 2021 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18514">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/">Wobble in the Standard Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Prediction is very difficult, especially if it’s about the future—Niels Bohr</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randalluncertainty/" rel="attachment wp-att-18526"><img width="153" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallUncertainty.jpg?resize=153%2C210&amp;ssl=1" class="alignright wp-image-18526" height="210" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Boston Globe “Uncertainty” <a href="https://www.bostonglobe.com/ideas/2011/10/22/lisa-randall-physics-universe-uncertainty/BfSYjipZy7HkQPmmRs8ZRI/story.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written popular books about her work and science in general. Thus she has a handle on aspects of science that overlap my expertise—not to mention those of her sister Dana Randall, whom I have known as a colleague for many years.</p>
<p>
Today, Ken and I thought we would talk about recent developments in particle physics, and their connection to two topics dear to us.<br />
<span id="more-18514"></span></p>
<p>
Randall’s most recent popular book is <a href="https://en.wikipedia.org/wiki/Dark_Matter_and_the_Dinosaurs">Dark Matter and the Dinosaurs</a>. The idea she advances is that the periodic extinctions in Earth’s history may have been caused when the solar system passes through a plane of dark matter within our galaxy. But dark matter and also dark energy have come under <a href="https://www.sciencenews.org/article/dark-matter-mystery-deepens-demise-reported-detection">increasing</a> <a href="https://phys.org/news/2021-03-composition-percent-universe.html">recent</a> <a href="https://www.nbcnews.com/science/space/maybe-dark-matter-doesn-t-exist-after-all-new-research-n1252995">doubt</a>, even from their original <a href="https://www.newscientist.com/article/mg24632851-400-why-the-universe-i-invented-is-right-but-still-not-the-final-answer/">formulator</a>. Maybe Niels Bohr’s quote should also say: </p>
<blockquote><p><b> </b> <em> <i>Prediction is very difficult, especially if it’s about the past.</i> </em>
</p></blockquote>
<p>
Randall’s previous book, <a href="https://en.wikipedia.org/wiki/Knocking_on_Heaven's_Door_(book)">Knocking on Heaven’s Door</a>, is most relevant to this post. The 1973 Bob Dylan <a href="https://en.wikipedia.org/wiki/Knockin'_on_Heaven's_Door">song</a> title it pinches describes the feeling of doing frontier physical science. Insofar as her own work is mostly theoretical, much of it connects to feelings we have in computer science—especially complexity lower bounds where the door mostly feels slammed shut. </p>
<p>
But the book is also about the practice of experimental science—not only how to gather knowledge but when and how we can have confidence in it. Its long middle part is titled, “Machines, Measurements, and Probability.” All three elements are foremost in considering a new development that involves two measurements taken 20 years apart.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randallbooks/" rel="attachment wp-att-18528"><img width="406" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallBooks.jpg?resize=406%2C254&amp;ssl=1" class="aligncenter wp-image-18528" height="254" /></a></p>
<p></p><h2> Muons </h2><p></p>
<p>
Last Tuesday’s New York Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">highlighted</a> a potential discovery in particle physics. It was in their Tuesday science section. </p>
<p>
The result is an experimental discovery that could show that the current model of matter is wrong. </p>
<blockquote><p><b> </b> <em> “This is our Mars rover landing moment,” said Chris Polly, a physicist at the Fermi National Accelerator Laboratory, or Fermilab, in Batavia, Ill., who has been working toward this finding for most of his career. </em>
</p></blockquote>
<p>
Indeed. It is a Mars landing moment. They both involved many people, lots of exotic machinery, lots of money, many years. Say three billion dollars or so for Mars. Say nearly the same amount for muons—the annual budget for Fermilab is over one half billion dollars. It was certainly enough to reassemble and upgrade a huge accelerator ring that was <a href="https://www.bnl.gov/newsroom/news.php?a=112259">first used</a> at Brookhaven National Lab on Long Island in 2001:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/fermilabring/" rel="attachment wp-att-18539"><img width="450" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/FermilabRing.jpg?resize=450%2C301&amp;ssl=1" class="aligncenter wp-image-18539" height="301" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NY Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">src</a></font>
</td>
</tr>
</tbody></table>
<p>
The study used the muon to probe the Standard Model of physics. Muons are useful because they are charged like an electron, which helps control them in an accelerator. Yet their mass is roughly 207 times larger than an electron. The same charge helps control their motion and the large mass makes collisions more interesting. As Polly stated in Natalie Wolchover’s <a href="https://www.quantamagazine.org/muon-g-2-experiment-at-fermilab-finds-hint-of-new-particles-20210407/">story</a> for <em>Quanta</em>:</p>
<blockquote><p><b> </b> <em> “[I]f you’re looking for particles that could explain the missing mass of the universe—dark matter—or you’re looking for [supersymmetry], that’s where the muon has a unique role.” </em>
</p></blockquote>
<p></p><h2> Theory and Jealousy </h2><p></p>
<p>
Computer science theory is so different from high end physics. We are closer to the type of research that Randall does. We involve few people, no exotic machinery, and small amounts of money. Maybe the closest attribute we have to high-end research is we also take years and years.</p>
<p>
Perhaps we are also jealous of high-end physics. Not just for money, but for the ability of particle physicists to get announcements into the New York Times. Polly said the <a href="https://www.energy.gov/science/articles/first-person-science-chris-polly-muon-physics">following</a> about the ending day of the muon experiment two decades ago:</p>
<blockquote><p><b> </b> <em> When we revealed the results, people from all over the world flew in to visit the lab. These experiments take decades to build and analyze, so you don’t get to go to very many of these events. We did a little “Drumroll, please” and then had the postdoc managing the spreadsheet hit the button to show it on the projector. Lo and behold, you could see that there was still a three-sigma discrepancy! </em>
</p></blockquote>
<p>
At the time he was a graduate assistant assigned to machinery for measuring particle energies. He had fixed a problem where someone had touched a component with bare hands and thereby ruined its sheathing. All such problems were meant to be ironed-out by the drum-roll event. But all this raises two further interesting issues that connect the muon results with issues we think about in computer science. Let’s look at them next.</p>
<p></p><h2> Three to Four Sigma </h2><p></p>
<p>
In an experimental science one must be aware that results are not exact. They are samples from some random process. Flip a coin 10 times in a row. If they all come up heads what does that mean? Could be the coin is fair but this happens about one time in a thousand. Or the coin is biased. Or something else. </p>
<p>
Flip a muon many times. That is sample some muon experiment. The outcome is from a random process. Some of it comes from properties of the natural processes themselves and others from incidentals of the measurement apparatus. How do we decide if the experiment means what we think it does?</p>
<p>
The theory developed by Carl Gauss and others before and after to delineate the normal distribution was largely prompted by analysis of measurement errors <a href="https://www.maa.org/sites/default/files/images/upload_library/22/Allendoerfer/stahl96.pdf">to begin with</a>. This yields the “<a href="https://en.wikipedia.org/wiki/68-95-99.7_rule">rule of three</a>,” about the percentage of values that naturally lie within an interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively.</p>
<p>
The question is, how to assess cases where the measurement result is well outside these intervals—when can we conclude it is more than a deviation by natural chance? In social sciences a result is “significant” provided it lies outside two-sigma. In particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a discovery. No Nobel prize for less. </p>
<p>
The situation with the muons has an extra factor of repeated measurements—but there have been only two measurements so far:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/muoncharts/" rel="attachment wp-att-18531"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/MuonCharts.png?resize=550%2C247&amp;ssl=1" class="aligncenter wp-image-18531" height="247" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://news.fnal.gov/2021/04/first-results-from-fermilabs-muon-g-2-experiment-strengthen-evidence-of-new-physics/">src1</a>, <a href="https://www.sciencemag.org/news/2021/04/particle-mystery-deepens-physicists-confirm-muon-more-magnetic-predicted">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The blue line in the left figure is the original Brookhaven measurement; the red is the new one. There is also <a href="https://4gravitons.com/2021/04/09/theoretical-uncertainty-and-uncertain-theory/">theoretical uncertainty</a> in the calculation of the Standard Model prediction, and that combines with the measurement error bars to give the sigma baseline. The chart at right normalizes the deviation to parts per billion—the measurements need to be incredibly fine. This scale appears to be about 25% under the current <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma}" class="latex" />-scale (it shows about <img src="https://s0.wp.com/latex.php?latex=%7B2.8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.8}" class="latex" /> for Brookhaven compared to its <img src="https://s0.wp.com/latex.php?latex=%7B3.7%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3.7\sigma}" class="latex" /> after <a href="https://arxiv.org/pdf/2006.04822.pdf">revised</a> uncertainty) but it is close enough to get the picture.</p>
<p>
Although the new Fermilab result by itself deviates slightly less from the Standard Model, it corroborates the earlier measurement. It is not fully independent from it, but the combination is enough to raise the current claimed deviation to about <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> sigmas. This is well above social science level but below Nobel level. This is with respect to the probability that the effects are real. </p>
<p>
The social significance of <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> is that it is above the “<img src="https://s0.wp.com/latex.php?latex=%7B3%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3+}" class="latex" />” level where hoped-for anomalies have subsequently disappeared for reasons chalked up to natural chance. This is because physicists around the world do many a hundredfold amount of hopeful measurements. Some measurements get initial “bumps” up just because of the numbers. But <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> reduces the natural frequency under 1-in-40,000. This is why reproducing measurements is so important, why the new Fermilab team devoted all the expense and effort. A more independent measurement on other machines could give a higher boost that might get over the <img src="https://s0.wp.com/latex.php?latex=%7B5.0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{5.0}" class="latex" /> line. Time to break out the wallets and hammers?</p>
<p>
The <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> is not, however, beyond the realm of <a href="https://en.wikipedia.org/wiki/Faster-than-light_neutrino_anomaly">recent</a> <a href="https://physics.aps.org/articles/v11/s78">experience</a> with apparatus faults and modeling error. On the latter, there is <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12292">still</a> some <a href="http://resonaances.blogspot.com/2021/04/why-is-it-when-something-happens-it-is.html">doubt</a> about the theoretical prediction for the muon’s magnetic moment. In any event, the muon results are exciting but still below what is required for a true discovery. Time will tell.</p>
<p>
The second factor we draw attention to concerns the human “hoping” directly.</p>
<p></p><h2> Blinding </h2><p></p>
<p>
In any experimental science one must also be aware that people are not unbiased. Scientists have much invested in the outcome of their experiments. Think jobs, tenure even, funding, and more. So a big physics experiment like the muon one must be careful. They follow standard practice to perform <a href="https://en.wikipedia.org/wiki/blinded_experiment">blinded</a> data analysis. </p>
<p>
This surprised me. This blinding is a crypto-type protocol, which is something we computer scientists study. The muon team performed a protocol that protected against cheating. Here is how they did it:</p>
<blockquote><p><b> </b> <em> In this case, the master clock that keeps track of the muons’ wobble had been set to a rate unknown to the researchers. The figure was sealed in envelopes that were locked in the offices at Fermilab and the University of Washington in Seattle.</em></p><em>
<p>
In a ceremony on Feb. 25 that was recorded on video and watched around the world on Zoom, Dr. Polly opened the Fermilab envelope and David Hertzog from the University of Washington opened the Seattle envelope. The number inside was entered into a spreadsheet, providing a key to all the data, and the result popped out to a chorus of wows.</p>
</em><p><em>
“That really led to a really exciting moment, because nobody on the collaboration knew the answer until the same moment,” said Saskia Charity, a Fermilab postdoctoral fellow who has been working remotely from Liverpool, England, during the pandemic. </em>
</p></blockquote>
<p>
This mechanism for blinding suggests possible crypto questions. They hid the master clock rate. Can this be modeled as one of our crypto problems? Can we prove some security bounds? If they claim that hiding the rate protects against cheating then they should be able to make this claim precise. The <a href="https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves">discovery</a> of gravitational waves used a <a href="https://www.ligo.org/news/blind-injection.php">blind injection</a> scheme tailored for that experiment. How can this be generalized?</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
We have discussed two aspects that involve soft numbers rather than hard machines and hard-shelled particles. Perhaps they are interesting new problems for us? What do you think?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/"><span class="datestr">at April 12, 2021 11:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/052">TR21-052 |  Upslices, Downslices, and Secret-Sharing with Complexity of $1.5^n$ | 

	Oded Nir, 

	Benny Applebaum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A secret-sharing scheme allows to distribute a secret $s$ among $n$ parties such that only some predefined ``authorized'' sets of parties can reconstruct the secret, and all other ``unauthorized'' sets learn nothing about $s$. 
The collection of authorized/unauthorized sets can be captured by a monotone function $f:\{0,1\}^n\rightarrow \{0,1\}$. 
In this paper, we focus on monotone functions that all their min-terms are sets of size $a$, and on their duals -- monotone functions whose max-terms are of size $b$. We refer to these classes as $(a,n)$-upslices and $(b,n)$-downslices, and note that these natural families correspond to monotone $a$-regular DNFs and monotone $(n-b)$-regular CNFs. We derive the following results.

1. (General downslices) Every downslice can be realized with total share size of $1.5^{n+o(n)}&lt;2^{0.585 n}$. Since every monotone function can be cheaply decomposed into $n$ downslices, we obtain a similar result for general access structures improving the previously known $2^{0.637n+o(n)}$ complexity of Applebaum, Beimel, Nir and Peter (STOC 2020). We also achieve a minor improvement in the exponent of linear secrets sharing schemes. 

2. (Random mixture of upslices) Following Beimel and Farras (TCC 2020) who studied the complexity of random DNFs with constant-size terms, we consider the following general distribution $F$ over monotone DNFs: For each width value $a\in [n]$, uniformly sample $k_a$ monotone terms of size $a$, where $k=(k_1,\ldots,k_n)$ is an arbitrary vector of non-negative integers. We show that, except with exponentially small probability, $F$ can be realized with share size of $2^{0.5 n+o(n)}$ and
    can be linearly realized with an exponent strictly smaller than $2/3$. Our proof also provides a candidate distribution for ``exponentially-hard'' access structure. 
    
We use our results to explore connections between several seemingly unrelated questions about the complexity of secret-sharing schemes such as worst-case vs. average-case, linear vs. non-linear and primal vs. dual access structures. We prove that, in at least one of these settings, there is a significant gap in secret-sharing complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/052"><span class="datestr">at April 12, 2021 05:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8147147831052480535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html">Is the following reaction to getting the first COVID shot logical?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Alice works at a charity that puts together bag and box lunches for children.</p><p><br />They all wear masks and they are 12 feet apart and very careful, and nobody there has gotten COVID.</p><p>Then Alice gets here first COVID shot and says:</p><p><br /></p><p><i>I am not going to work for that charity until I have had my second shot and waited  4 weeks so I am immune. </i></p><p><i><br /></i></p><p>She is really scared of getting COVID NOW that  she is on the verge of being immune. </p><p><br /></p><p>Is that logical? She was not scared before. So does it make sense to be scared now? I see where she is coming from emotionally, but is there a logical argument for her viewpoint? I ask nonrhetorically.</p><p><br /></p><p>bill g. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html"><span class="datestr">at April 12, 2021 04:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5437">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5437">Just some prizes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://en.wikipedia.org/wiki/Oded_Goldreich">Oded Goldreich</a> is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel.  He’s best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the <a href="https://en.wikipedia.org/wiki/Hard-core_predicate">Goldreich-Levin Theorem</a> (every one-way function can be modified to have a hard-core predicate), the <a href="https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How%20To%20Construct%20Random%20Functions.pdf">Goldreich-Goldwasser-Micali Theorem</a> (every pseudorandom generator can be made into a pseudorandom function), and the <a href="https://www.cs.purdue.edu/homes/hmaji/teaching/Fall%202017/lectures/39.pdf">Goldreich-Micali-Wigderson protocol</a> for secure multi-party computation.  I first met Oded more than 20 years ago, when he lectured at a summer school at the Institute for Advanced Study in Princeton, barefoot and wearing a tank top and what looked like pajama pants.  It was a bracing introduction to complexity-theoretic cryptography.  Since then, I’ve interacted with Oded from time to time, partly around his <a href="http://www.wisdom.weizmann.ac.il/~oded/on-qc.html">firm belief</a> that quantum computing is impossible.</p>



<p>Last month a committee in Israel voted to award Goldreich the <a href="https://en.wikipedia.org/wiki/Israel_Prize">Israel Prize</a> (roughly analogous to the US National Medal of Science), for which I’d say Goldreich had been a plausible candidate for decades.  But alas, Yoav Gallant, Netanyahu’s Education Minister, then rather <a href="https://www.jpost.com/israel-news/high-court-revokes-israel-prize-in-math-to-pro-bds-professor-664538">non-gallantly blocked the award</a>, solely because he objected to Goldreich’s far-left political views (and apparently because of various statements Goldreich signed, including in support of a boycott of Ariel University, which is in the West Bank).  The case went all the way to the Israeli Supreme Court (!), which <a href="https://www.washingtonpost.com/world/middle_east/israeli-academic-wont-receive-prize-after-signing-petition/2021/04/08/d1e987ca-987b-11eb-8f0a-3384cf4fb399_story.html">ruled two days ago</a> in Gallant’s favor: he gets to “delay” the award to investigate the matter further, and in the meantime has apparently sent out invitations for an award ceremony next week that doesn’t include Goldreich.  Some are now calling for the other winners to boycott the prize in solidarity until this is righted.</p>



<p>I doubt readers of this blog need convincing that this is a travesty and an embarrassment, a <em><a href="https://en.wiktionary.org/wiki/shanda#:~:text=shanda%20(uncountable),(Jewish)%20shame%3B%20disgrace.">shanda</a></em>, for the Netanyahu government itself.  That I disagree with Goldreich’s far-left views (or <em>might</em> disagree, if I knew in any detail what they were) is totally immaterial to that judgment.  In my opinion, not even Goldreich’s belief in the impossibility of quantum computers should affect his eligibility for the prize. <img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /></p>



<p>Maybe it would be better to say that, as far as his academic colleagues in Israel and beyond are concerned, Goldreich <em>has</em> won the Israel Prize; it’s only some irrelevant external agent who’s blocking his receipt of it.  Ironically, though, among Goldreich’s many heterodox beliefs is a <a href="http://www.wisdom.weizmann.ac.il/~oded/on-awards1.html">total rejection of the value of scientific prizes</a> (although Goldreich has also said he wouldn’t refuse the Israel Prize if offered it!).</p>



<p></p><hr /><p></p>



<p>In unrelated news, the 2020 Turing Award has been given to <a href="https://en.wikipedia.org/wiki/Alfred_Aho">Al Aho</a> and <a href="https://en.wikipedia.org/wiki/Jeffrey_Ullman">Jeff Ullman</a>.  Aho and Ullman have both been celebrated leaders in CS for half a century, having laid many of the foundations of formal languages and compilers, and having coauthored one of CS’s <a href="https://www.amazon.com/Design-Analysis-Computer-Algorithms/dp/0201000296/ref=pd_lpo_14_t_1/140-9181226-0879049?_encoding=UTF8&amp;pd_rd_i=0201000296&amp;pd_rd_r=4c6cd308-4669-45ee-ad4d-292ee24e043f&amp;pd_rd_w=WxNWv&amp;pd_rd_wg=AAt3G&amp;pf_rd_p=337be819-13af-4fb9-8b3e-a5291c097ebb&amp;pf_rd_r=T95EJ78DHVE1V1G604D1&amp;psc=1&amp;refRID=T95EJ78DHVE1V1G604D1">defining textbooks</a> with <a href="https://en.wikipedia.org/wiki/John_Hopcroft">John Hopcroft</a> (who already received a different Turing Award).</p>



<p>But again there’s a controversy.  <a href="https://lobelog.com/niac-calls-out-anti-iranian-stanford-professor/">Apparently</a>, in 2011, Ullman wrote to an Iranian student who wanted to work with him, saying that as “a matter of principle,” he would not accept Iranian students until the Iranian government recognized Israel.  Maybe I should say that I, like Ullman, am both a Jew and a Zionist, but I find it hard to imagine the state of mind that would cause me to hold some hapless student responsible for the misdeeds of their birth-country’s government.  Ironically, this is a mirror-image of the <a href="https://en.wikipedia.org/wiki/Academic_boycott_of_Israel#Mona_Baker,_Miriam_Shlesinger_and_Gideon_Toury">tactics</a> that the BDS movement has wielded against Israeli academics.  Unlike Goldreich, though, Ullman seems to have gone beyond merely expressing his beliefs, actually turning them into a one-man foreign policy.</p>



<p>I’m <a href="https://www.scottaaronson.com/blog/?p=3167">proud</a> of the Iranian students I’ve mentored and hope to mentor more.  While I don’t think this issue should affect Ullman’s Turing Award (and I haven’t seen anyone claim that it should), I do think it’s appropriate to use the occasion to express our opposition to all forms of discrimination.  I fully endorse Shafi Goldwasser’s <a href="https://www.facebook.com/SimonsInstitute/">response</a> in her capacity as Director of the Simons Institute for Theory of Computing in Berkeley:</p>



<blockquote class="wp-block-quote"><p>As a senior member of the computer science community and an American-Israeli, I stand with our Iranian students and scholars and outright reject any notion by which admission, support, or promotion of individuals in academic settings should be impeded by national origin or politics. Individuals should not be conflated with the countries or institutions they come from. Statements and actions to the contrary have no place in our computer science community. Anyone experiencing such behavior will find a committed ally in me.</p></blockquote>



<p>As for Al Aho?  I knew him fifteen years ago, when he became interested in quantum computing, in part due to his then-student <a href="https://www.microsoft.com/en-us/research/people/ksvore/">Krysta Svore</a> (who’s now the head of Microsoft’s quantum computing efforts).  Al struck me as not only a famous scientist but a gentleman who radiated kindness everywhere.  I’m not aware of any controversies he’s been involved in and never heard anyone say a bad word about him.</p>



<p>Anyway, this seems like a good occasion to recognize some foundational achievements in computer science, as well as the complex human beings who produce them!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5437"><span class="datestr">at April 09, 2021 06:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/051">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/051">TR21-051 |  Binary Interactive Error Resilience Beyond $1/8$ (or why $(1/2)^3 &amp;gt; 1/8$) | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Interactive error correcting codes are codes that encode a two party communication protocol to an error-resilient protocol that succeeds even if a constant fraction of the communicated symbols are adversarially corrupted, at the cost of increasing the communication by a constant factor. What is the largest fraction of corruptions that such codes can protect against? 

If the error-resilient protocol is allowed to communicate large (constant sized) symbols, Braverman and Rao  (STOC, 2011) show that the maximum rate of corruptions that can be tolerated is $1/4$. They also give a binary interactive error correcting protocol that only communicates bits and is resilient to $1/8$ fraction of errors, but leave the optimality of this scheme as an open problem.

We answer this question in the negative, breaking the $1/8$ barrier. Specifically, we give a binary interactive error correcting scheme that is resilient to $5/39 &gt; 1/8$ fraction of adversarial errors. Our scheme builds upon a novel construction of binary list-decodable interactive codes with small list size.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/051"><span class="datestr">at April 09, 2021 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-709704913623244646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">Quantum Stories</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Scott Aaronson <a href="https://www.scottaaronson.com/blog/?p=5387">wrote last month</a> about the hype over quantum computing. I'd thought I'd drop a few stories.</p><p>I was once asked to review a grant proposal (outside the US) that claimed it would find a quantum algorithm for NP-hard problems. I wrote a scathing review but the grant was funded because I failed to prove that it was impossible. I replied that they should fund my research to teleport people from Chicago to Paris because they couldn't prove I couldn't do it. I never got a response.</p><div>I was at an NSF sponsored meeting on quantum computing. I suggested, as a complexity theorist, that we need to explore the limits of quantum computing. A senior researcher said we shouldn't mention that in the report or it might hurt our chances of funding the field if they think quantum computing might not be a complete success.</div><p>I went to a Microsoft Faculty Research Summit which had a big focus on quantum computing. I complained of the quantum computing hype. My friends in the field denied the hype. Later at the summit a research head said that Microsoft will solve world hunger with quantum computing.</p><p>I was meeting with a congressional staffer who had worked on the National Quantum Initiative which coincidentally was being announced that day. I said something about high risk, high reward. He looked shocked--nobody had told him before that quantum computing is a speculative technology.</p><p>Quantum computing has generated a large number of beautiful and challenging scientific questions. Thinking about quantum has helped generate classical complexity and algorithmic results. But quantum computing having a real-world impact in the near or mid-term is unlikely. Most scientists I know working directly in quantum research are honest about the limitations and challenges in quantum computing. But somehow that message is not often getting to the next layers up, the policy makers, the research managers, the university administrators, the media and the venture capitalists. </p><p>But who knows, maybe some quantum heuristic that doesn't need much entanglement will change the world tomorrow. I can't prove it's impossible.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html"><span class="datestr">at April 08, 2021 12:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-21129445.post-1274107059735040105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/pizza.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html">Postdoctoral openings at Amazon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists" target="_blank">Amazon Advertising opens applications for early career scientists.</a> The new program, which offers full-time two-year positions, is aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. The application deadline is May 14.</p></div>







<p class="date">
by metoo (noreply@blogger.com) <a href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html"><span class="datestr">at April 08, 2021 04:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
