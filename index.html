<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 12, 2021 05:38 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/069">TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/069"><span class="datestr">at May 12, 2021 08:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/glm_saga/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/glm_saga/">Debuggable Deep Networks: Sparse Linear Models (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/intro.png" alt="Overview" /></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png" alt="Examples of feature visualization" /></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png" alt="Examples of word cloud visualization" /></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png" alt="Feature visualization dump" /></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas width="400" id="ablation_dense_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img src="https://gradientscience.org/feed.xml" id="reg" />
        <div id="reg_slider"></div>
        <div class="quarterblock"> </div>
        <div style="text-align: center;" class="quarterblock">Accuracy: <span id="reg_accuracy"></span>%</div>
        <div style="text-align: center;" class="quarterblock">Non-zero: <span id="reg_sparsity"></span>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas width="400" id="ablation_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
    </div>
    <div id="zoom">
        <img src="https://gradientscience.org/feed.xml" id="feature_big" />
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>







<p class="date">
<a href="https://gradientscience.org/glm_saga/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/debugging/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/debugging/">Debuggable Deep Networks: Usage and Evaluation (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png" height="350" /></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png" alt="Wordcloud visualization of feature used in unbiased BERT" /></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" value="" id="toxic_">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="christianity" id="toxic_christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="African" id="toxic_African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="Catholic" id="toxic_Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"></span>
        <canvas width="400" id="toxic" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png" height="350" /></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"></div>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>

</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas width="400" id="sst_canvas" height="200"></canvas>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png" alt="ImageNet counterfactuals" /></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/problematic.png" alt="Problematic features" /></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png" height="350" /></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"></div>
  <div style="float: none;" class="blocktxt" id="mislabels"> </div>
  <div style="clear: both;" id="misimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"></span><br /><span style="color: red;" class="choices_info_text"><b></b></span><br /><span style="color: green;" class="choices_info_text"><b></b></span><br /><hr /><h3 style="text-align: center;"><h3><div style="text-align: center; font-weight: 300; margin: 0.75em auto;" class="sp_txt"></div><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png" /><hr style="margin: 0.75em auto;" /><div style="text-align: center; font-weight: 200;" class="sp_txt"><span></span></div><hr style="margin: 0.3em auto;" /><h3 style="text-align: center;"><h3><div style="text-align: center;" class="sp_txt"><span style="font-weight: 200;"></span></div><br /><span></span></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"></span><span class="choices_info_text"></span><br /><span class="choices_info_text"></span><div class="mis_txt blocktxt thirdblock"><br /><span class="widgetheading"></span></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;" /></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div></div></h3></h3></div>







<p class="date">
<a href="https://gradientscience.org/debugging/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05130">Towards a Model for LSH</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Li.html">Li Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05130">PDF</a><br /><b>Abstract: </b>As data volumes continue to grow, clustering and outlier detection algorithms
are becoming increasingly time-consuming. Classical index structures for
neighbor search are no longer sustainable due to the "curse of dimensionality".
Instead, approximated index structures offer a good opportunity to
significantly accelerate the neighbor search for clustering and outlier
detection and to have the lowest possible error rate in the results of the
algorithms. Locality-sensitive hashing is one of those. We indicate directions
to model the properties of LSH.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05130"><span class="datestr">at May 12, 2021 01:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05062">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05062">Current Algorithms for Detecting Subgraphs of Bounded Treewidth are Probably Optimal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, Jasper Slusallek <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05062">PDF</a><br /><b>Abstract: </b>The Subgraph Isomorphism problem is of considerable importance in computer
science. We examine the problem when the pattern graph H is of bounded
treewidth, as occurs in a variety of applications. This problem has a
well-known algorithm via color-coding that runs in time $O(n^{tw(H)+1})$ [Alon,
Yuster, Zwick'95], where $n$ is the number of vertices of the host graph $G$.
While there are pattern graphs known for which Subgraph Isomorphism can be
solved in an improved running time of $O(n^{tw(H)+1-\varepsilon})$ or even
faster (e.g. for $k$-cliques), it is not known whether such improvements are
possible for all patterns. The only known lower bound rules out time
$n^{o(tw(H) / \log(tw(H)))}$ for any class of patterns of unbounded treewidth
assuming the Exponential Time Hypothesis [Marx'07].
</p>
<p>In this paper, we demonstrate the existence of maximally hard pattern graphs
$H$ that require time $n^{tw(H)+1-o(1)}$. Specifically, under the Strong
Exponential Time Hypothesis (SETH), a standard assumption from fine-grained
complexity theory, we prove the following asymptotic statement for large
treewidth $t$: For any $\varepsilon &gt; 0$ there exists $t \ge 3$ and a pattern
graph $H$ of treewidth $t$ such that Subgraph Isomorphism on pattern $H$ has no
algorithm running in time $O(n^{t+1-\varepsilon})$.
</p>
<p>Under the more recent 3-uniform Hyperclique hypothesis, we even obtain tight
lower bounds for each specific treewidth $t \ge 3$: For any $t \ge 3$ there
exists a pattern graph $H$ of treewidth $t$ such that for any $\varepsilon&gt;0$
Subgraph Isomorphism on pattern $H$ has no algorithm running in time
$O(n^{t+1-\varepsilon})$.
</p>
<p>In addition to these main results, we explore (1) colored and uncolored
problem variants (and why they are equivalent for most cases), (2) Subgraph
Isomorphism for $tw &lt; 3$, (3) Subgraph Isomorphism parameterized by pathwidth,
and (4) a weighted problem variant.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05062"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04993">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04993">Constrained Consensus Sequence Algorithm for DNA Archiving</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lavenier:Dominique.html">Dominique Lavenier</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04993">PDF</a><br /><b>Abstract: </b>The paper describes an algorithm to compute a consensus sequence from a set
of DNA sequences of approximatively identical length generated by 3rd
sequencing generation technologies. Its purpose targets DNA storage and is
guided by specific features that cannot be exhibited from biological data such
as the exact length of the consensus sequences, the precise location of known
patterns, the kmer composition, etc.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04993"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04965">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04965">Compact Euler Tours of Trees with Small Maximum Degree</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gagie:Travis.html">Travis Gagie</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04965">PDF</a><br /><b>Abstract: </b>We show how an Euler tour for a tree on $n$ vertices with maximum degree $d$
can be stored in $2 n + o (n)$ bits such that queries take $O (\log n)$ time
and updates take $O (d \log^{1 + \epsilon} n)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04965"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04856">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04856">Parameterized Algorithms for Diverse Multistage Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kellerhals:Leon.html">Leon Kellerhals</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renken:Malte.html">Malte Renken</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04856">PDF</a><br /><b>Abstract: </b>The world is rarely static -- many problems need not only be solved once but
repeatedly, under changing conditions. This setting is addressed by the
"multistage" view on computational problems. We study the "diverse multistage"
variant, where consecutive solutions of large variety are preferable to similar
ones, e.g. for reasons of fairness or wear minimization. While some aspects of
this model have been tackled before, we introduce a framework allowing us to
prove that a number of diverse multistage problems are fixed-parameter
tractable by diversity, namely Perfect Matching, s-t Path, Matroid Independent
Set, and Plurality Voting. This is achieved by first solving special, colored
variants of these problems, which might also be of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04856"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04847">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04847">Improved LCAs for constructing spanners</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Rubi Arviv, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Reut.html">Reut Levi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04847">PDF</a><br /><b>Abstract: </b>In this paper we study the problem of constructing spanners in a local
manner, specifically in the Local Computation Model proposed by Rubinfeld et
al. (ICS 2011).
</p>
<p>We provide an LCA for constructing $(2r-1)$-spanners with
$\widetilde{O}(n^{1+1/r})$ edges and probe complexity of
$\widetilde{O}(n^{1-1/r})$ $r \in \{2,3\}$, where $n$ denotes the number of
vertices in the input graph. Up to polylogarithmic factors, in both cases the
stretch factor is optimal (for the respective number of edges). In addition,
our probe complexity for $r=2$, i.e., for constructing $3$-spanner is optimal
up to polylogarithmic factors. Our result improves over the probe complexity of
Parter et al. (ITCS 2019) that is $\widetilde{O}(n^{1-1/2r})$ for $r \in
\{2,3\}$.
</p>
<p>For general $k\geq 1$, we provide an LCA for constructing $O(k^2)$-spanners
with $\tilde{O}(n^{1+1/k})$ edges on expectation whose probe complexity is
$O(n^{2/3}\Delta^2)$. This improves over the probe complexity of Parter et al.
that is $O(n^{2/3}\Delta^4)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04847"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04809">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04809">Testing Triangle Freeness in the General Model in Graphs with Arboricity $O(\sqrt{n})$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Reut.html">Reut Levi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04809">PDF</a><br /><b>Abstract: </b>We study the problem of testing triangle freeness in the general graph model.
This problem was first studied in the general graph model by Alon et al. (SIAM
J. Discret. Math. 2008) who provided both lower bounds and upper bounds that
depend on the number of vertices and the average degree of the graph. Their
bounds are tight only when $d_{\rm max} = O(d)$ and $\bar{d} \leq \sqrt{n}$ or
when $\bar{d} = \Theta(1)$, where $d_{\rm max}$ denotes the maximum degree and
$\bar{d}$ denotes the average degree of the graph. In this paper we provide
bounds that depend on the arboricity of the graph and the average degree. As in
Alon et al., the parameters of our tester is the number of vertices, $n$, the
number of edges, $m$, and the proximity parameter $\epsilon$ (the arboricity of
the graph is not a parameter of the algorithm). The query complexity of our
tester is $\tilde{O}(\Gamma/\bar{d} + \Gamma)\cdot poly(1/\epsilon)$ on
expectation, where $\Gamma$ denotes the arboricity of the input graph (we use
$\tilde{O}(\cdot)$ to suppress $O(\log \log n)$ factors). We show that for
graphs with arboricity $O(\sqrt{n})$ this upper bound is tight in the following
sense. For any $\Gamma \in [s]$ where $s= \Theta(\sqrt{n})$ there exists a
family of graphs with arboricity $\Gamma$ and average degree $\bar{d}$ such
that $\Omega(\Gamma/\bar{d} + \Gamma)$ queries are required for testing
triangle freeness on this family of graphs. Moreover, this lower bound holds
for any such $\Gamma$ and for a large range of feasible average degrees.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04809"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04802">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04802">Tree Edit Distance with Variables. Measuring the Similarity between Mathematical Formulas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akutsu:Tatsuya.html">Tatsuya Akutsu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mori:Tomoya.html">Tomoya Mori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakamura:Naotoshi.html">Naotoshi Nakamura</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozawa:Satoshi.html">Satoshi Kozawa</a>, Yuhei Ueno, Thomas N. Sato <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04802">PDF</a><br /><b>Abstract: </b>In this article, we propose tree edit distance with variables, which is an
extension of the tree edit distance to handle trees with variables and has a
potential application to measuring the similarity between mathematical
formulas, especially, those appearing in mathematical models of biological
systems. We analyze the computational complexities of several variants of this
new model. In particular, we show that the problem is NP-complete for ordered
trees. We also show for unordered trees that the problem of deciding whether or
not the distance is 0 is graph isomorphism complete but can be solved in
polynomial time if the maximum outdegree of input trees is bounded by a
constant. This distance model is then extended for measuring the
difference/similarity between two systems of differential equations, for which
results of preliminary computational experiments using biological models are
provided.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04802"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04735">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04735">A 3-approximation list scheduling algorithm for a single-machine scheduling problem with a non-renewable resource and total weighted completion time criterion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Susumu Hashimoto, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mizuno:Shinji.html">Shinji Mizuno</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04735">PDF</a><br /><b>Abstract: </b>This paper studies a single-machine scheduling problem with a non-renewable
resource (NR-SSP) and total weighted completion time criterion. The
non-renewable resource is consumed when the machine starts processing a job. We
consider the case where each job's weight in the objective function is
proportional to its resource consumption amount. The problem is known to be
NP-hard in this case. We propose a 3-approximation list scheduling algorithm
for this problem. Besides, we show that the approximation ratio 3 is tight for
the algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04735"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04712">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04712">Near Neighbor Search via Efficient Average Distortion Embeddings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kush:Deepanshu.html">Deepanshu Kush</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolov:Aleksandar.html">Aleksandar Nikolov</a>, Haohua Tang <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04712">PDF</a><br /><b>Abstract: </b>A recent series of papers by Andoni, Naor, Nikolov, Razenshteyn, and
Waingarten (STOC 2018, FOCS 2018) has given approximate near neighbour search
(NNS) data structures for a wide class of distance metrics, including all
norms. In particular, these data structures achieve approximation on the order
of $p$ for $\ell_p^d$ norms with space complexity nearly linear in the dataset
size $n$ and polynomial in the dimension $d$, and query time sub-linear in $n$
and polynomial in $d$. The main shortcoming is the exponential in $d$
pre-processing time required for their construction.
</p>
<p>In this paper, we describe a more direct framework for constructing NNS data
structures for general norms. More specifically, we show via an algorithmic
reduction that an efficient NNS data structure for a given metric is implied by
an efficient average distortion embedding of it into $\ell_1$ or into Euclidean
space. In particular, the resulting data structures require only polynomial
pre-processing time, as long as the embedding can be computed in polynomial
time. As a concrete instantiation of this framework, we give an NNS data
structure for $\ell_p$ with efficient pre-processing that matches the
approximation factor, space and query complexity of the aforementioned data
structure of Andoni et al. On the way, we resolve a question of Naor (Analysis
and Geometry in Metric Spaces, 2014) and provide an explicit, efficiently
computable embedding of $\ell_p$, for $p \ge 2$, into $\ell_2$ with (quadratic)
average distortion on the order of $p$. We expect our approach to pave the way
for constructing efficient NNS data structures for all norms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04712"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04702">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04702">ppsim: A software package for efficiently simulating and visualizing population protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Doty:David.html">David Doty</a>, Eric Severson <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04702">PDF</a><br /><b>Abstract: </b>We introduce ppsim, a software package for efficiently simulating population
protocols, a widely-studied subclass of chemical reaction networks (CRNs) in
which all reactions have two reactants and two products. Each step in the
dynamics involves picking a uniform random pair from a population of $n$
molecules to collide and have a (potentially null) reaction. In a recent
breakthrough, Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020]
discovered a population protocol simulation algorithm quadratically faster than
the naive algorithm, simulating $\Theta(\sqrt{n})$ reactions in *constant*
time, while preserving the *exact* stochastic dynamics.
</p>
<p>ppsim implements this algorithm, with a tightly optimized Cython
implementation that can exactly simulate hundreds of billions of reactions in
seconds. It dynamically switches to the CRN Gillespie algorithm for efficiency
gains when the number of applicable reactions in a configuration becomes small.
As a Python library, ppsim also includes many useful tools for data
visualization in Jupyter notebooks, allowing robust visualization of time
dynamics such as histogram plots at time snapshots and averaging repeated
trials.
</p>
<p>Finally, we give a framework that takes any CRN with only bimolecular (2
reactant, 2 product) or unimolecular (1 reactant, 1 product) reactions, with
arbitrary rate constants, and compiles it into a continuous-time population
protocol. This lets ppsim exactly sample from the chemical master equation
(unlike approximate heuristics such as tau-leaping or LNA), while achieving
asymptotic gains in running time. In linked Jupyter notebooks, we demonstrate
the efficacy of the tool on some protocols of interest in molecular
programming, including the approximate majority CRN and CRN models of DNA
strand displacement reactions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04702"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04700">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04700">Ultrafast Distributed Coloring of High Degree Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Halld=oacute=rsson:Magn=uacute=s_M=.html">Magnús M. Halldórsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nolin:Alexandre.html">Alexandre Nolin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tonoyan:Tigran.html">Tigran Tonoyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04700">PDF</a><br /><b>Abstract: </b>We give a new randomized distributed algorithm for the $\Delta+1$-list
coloring problem. The algorithm and its analysis dramatically simplify the
previous best result known of Chang, Li, and Pettie [SICOMP 2020]. This allows
for numerous refinements, and in particular, we can color all $n$-node graphs
of maximum degree $\Delta \ge \log^{2+\Omega(1)} n$ in $O(\log^* n)$ rounds.
The algorithm works in the CONGEST model, i.e., it uses only $O(\log n)$ bits
per message for communication. On low-degree graphs, the algorithm shatters the
graph into components of size $\operatorname{poly}(\log n)$ in $O(\log^*
\Delta)$ rounds, showing that the randomized complexity of $\Delta+1$-list
coloring in CONGEST depends inherently on the deterministic complexity of
related coloring problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04700"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04660">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04660">Parameterized Complexity of Deletion to Scattered Graph Classes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jacob:Ashwin.html">Ashwin Jacob</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kroon:Jari_J=_H=_de.html">Jari J. H. de Kroon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Majumdar:Diptapriyo.html">Diptapriyo Majumdar</a>, Venkatesh Raman <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04660">PDF</a><br /><b>Abstract: </b>Graph-modification problems, where we add/delete a small number of
vertices/edges to make the given graph to belong to a simpler graph class, is a
well-studied optimization problem in all algorithmic paradigms including
classical, approximation and parameterized complexity. Specifically,
graph-deletion problems, where one needs to delete at most $k$ vertices to
place it in a given non-trivial hereditary (closed under induced subgraphs)
graph class, captures several well-studied problems including {\sc Vertex
Cover}, {\sc Feedback Vertex Set}, {\sc Odd Cycle Transveral}, {\sc Cluster
Vertex Deletion}, and {\sc Perfect Deletion}. Investigation into these problems
in parameterized complexity has given rise to powerful tools and techniques.
While a precise characterization of the graph classes for which the problem is
{\it fixed-parameter tractable} (FPT) is elusive, it has long been known that
if the graph class is characterized by a {\it finite} set of forbidden graphs,
then the problem is FPT.
</p>
<p>In this paper, we initiate a study of a natural variation of the problem of
deletion to {\it scattered graph classes} where we need to delete at most $k$
vertices so that in the resulting graph, each connected component belongs to
one of a constant number of graph classes. A simple hitting set based approach
is no longer feasible even if each of the graph classes is characterized by
finite forbidden sets.
</p>
<p>As our main result, we show that this problem is fixed-parameter tractable
(FPT) when the deletion problem corresponding to each of the finite classes is
known to be FPT and the properties that a graph belongs to each of the classes
is expressible in CMSO logic.
</p>
<p>When each graph class has a finite forbidden set, we give a faster FPT
algorithm using the well-known techniques of iterative compression and
important separators.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04660"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04588">Partitioning H-Free Graphs of Bounded Diameter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brause:Christoph.html">Christoph Brause</a>, Petr Golovach, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paulusma:Dani=euml=l.html">Daniël Paulusma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Siani.html">Siani Smith</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04588">PDF</a><br /><b>Abstract: </b>A natural way of increasing our understanding of NP-complete graph problems
is to restrict the input to a special graph class. Classes of $H$-free graphs,
that is, graphs that do not contain some graph $H$ as an induced subgraph, have
proven to be an ideal testbed for such a complexity study. However, if the
forbidden graph $H$ contains a cycle or claw, then these problems often stay
NP-complete. A recent complexity study on the $k$-Colouring problem shows that
we may still obtain tractable results if we also bound the diameter of the
$H$-free input graph. We continue this line of research by initiating a
complexity study on the impact of bounding the diameter for a variety of
classical vertex partitioning problems restricted to $H$-free graphs. We prove
that bounding the diameter does not help for Independent Set, but leads to new
tractable cases for problems closely related to 3-Colouring. That is, we show
that Near-Bipartiteness, Independent Feedback Vertex Set, Independent Odd Cycle
Transversal, Acyclic 3-Colouring and Star 3-Colouring are all polynomial-time
solvable for chair-free graphs of bounded diameter. To obtain these results we
exploit a new structural property of 3-colourable chair-free graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04588"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04035">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04035">Knapsack and Subset Sum with Small Items</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Adam Polak, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, Karol Węgrzycki <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04035">PDF</a><br /><b>Abstract: </b>Knapsack and Subset Sum are fundamental NP-hard problems in combinatorial
optimization. Recently there has been a growing interest in understanding the
best possible pseudopolynomial running times for these problems with respect to
various parameters.
</p>
<p>In this paper we focus on the maximum item size $s$ and the maximum item
value $v$. We give algorithms that run in time $O(n + s^3)$ and $O(n + v^3)$
for the Knapsack problem, and in time $\tilde{O}(n + s^{5/3})$ for the Subset
Sum problem.
</p>
<p>Our algorithms work for the more general problem variants with
multiplicities, where each input item comes with a (binary encoded)
multiplicity, which succinctly describes how many times the item appears in the
instance. In these variants $n$ denotes the (possibly much smaller) number of
distinct items.
</p>
<p>Our results follow from combining and optimizing several diverse lines of
research, notably proximity arguments for integer programming due to Eisenbrand
and Weismantel (TALG 2019), fast structured $(\min,+)$-convolution by Kellerer
and Pferschy (J. Comb. Optim. 2004), and additive combinatorics methods
originating from Galil and Margalit (SICOMP 1991).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04035"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04023">Fast and Error-Adaptive Influence Maximization based on Count-Distinct Sketches</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Gokhan Gokturk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaya:Kamer.html">Kamer Kaya</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04023">PDF</a><br /><b>Abstract: </b>Influence maximization (IM) is the problem of finding a seed vertex set that
maximizes the expected number of vertices influenced under a given diffusion
model. Due to the NP-Hardness of finding an optimal seed set, approximation
algorithms are frequently used for IM. In this work, we describe a fast,
error-adaptive approach that leverages Count-Distinct sketches and hash-based
fused sampling. To estimate the number of influenced vertices throughout a
diffusion, we use per-vertex Flajolet-Martin sketches where each sketch
corresponds to a sampled subgraph. To efficiently simulate the diffusions, the
reach-set cardinalities of a single vertex are stored in memory in a
consecutive fashion. This allows the proposed algorithm to estimate the number
of influenced vertices in a single step for simulations at once. For a faster
IM kernel, we rebuild the sketches in parallel only after observing estimation
errors above a given threshold. Our experimental results show that the proposed
algorithm yields high-quality seed sets while being up to 119x faster than a
state-of-the-art approximation algorithm. In addition, it is up to 62x faster
than a sketch-based approach while producing seed sets with 3%-12% better
influence scores
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04023"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03968">Fast $n$-fold Boolean Convolution via Additive Combinatorics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03968">PDF</a><br /><b>Abstract: </b>We consider the problem of computing the Boolean convolution (with
wraparound) of $n$~vectors of dimension $m$, or, equivalently, the problem of
computing the sumset $A_1+A_2+\ldots+A_n$ for $A_1,\ldots,A_n \subseteq
\mathbb{Z}_m$. Boolean convolution formalizes the frequent task of combining
two subproblems, where the whole problem has a solution of size $k$ if for some
$i$ the first subproblem has a solution of size~$i$ and the second subproblem
has a solution of size $k-i$. Our problem formalizes a natural generalization,
namely combining solutions of $n$ subproblems subject to a modular constraint.
This simultaneously generalises Modular Subset Sum and Boolean Convolution
(Sumset Computation). Although nearly optimal algorithms are known for special
cases of this problem, not even tiny improvements are known for the general
case.
</p>
<p>We almost resolve the computational complexity of this problem, shaving
essentially a factor of $n$ from the running time of previous algorithms.
Specifically, we present a \emph{deterministic} algorithm running in
\emph{almost} linear time with respect to the input plus output size $k$. We
also present a \emph{Las Vegas} algorithm running in \emph{nearly} linear
expected time with respect to the input plus output size $k$. Previously, no
deterministic or randomized $o(nk)$ algorithm was known.
</p>
<p>At the heart of our approach lies a careful usage of Kneser's theorem from
Additive Combinatorics, and a new deterministic almost linear output-sensitive
algorithm for non-negative sparse convolution. In total, our work builds a
solid toolbox that could be of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03968"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03782">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03782">Construction of Sparse Suffix Trees and LCE Indexes in Optimal Time and Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kosolobov:Dmitry.html">Dmitry Kosolobov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sivukhin:Nikita.html">Nikita Sivukhin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03782">PDF</a><br /><b>Abstract: </b>The notions of synchronizing and partitioning sets are recently introduced
variants of locally consistent parsings with great potential in
problem-solving. In this paper we propose a deterministic algorithm that
constructs for a given readonly string of length $n$ over the alphabet
$\{0,1,\ldots,n^{\mathcal{O}(1)}\}$ a version of $\tau$-partitioning set with
size $\mathcal{O}(b)$ and $\tau = \frac{n}{b}$ using $\mathcal{O}(b)$ space and
$\mathcal{O}(\frac{1}{\epsilon}n)$ time provided $b \ge n^\epsilon$, for
$\epsilon &gt; 0$. As a corollary, for $b \ge n^\epsilon$ and constant $\epsilon &gt;
0$, we obtain linear construction algorithms with $\mathcal{O}(b)$ space on top
of the string for two major small-space indexes: a sparse suffix tree, which is
a compacted trie built on $b$ chosen suffixes of the string, and a longest
common extension (LCE) index, which occupies $\mathcal{O}(b)$ space and allows
us to compute the longest common prefix for any pair of substrings in
$\mathcal{O}(n/b)$ time. For both, the $\mathcal{O}(b)$ construction storage is
asymptotically optimal since the tree itself takes $\mathcal{O}(b)$ space and
any LCE index with $\mathcal{O}(n/b)$ query time must occupy at least
$\mathcal{O}(b)$ space by a known trade-off (at least for $b \ge \Omega(n /
\log n)$). In case of arbitrary $b \ge \Omega(\log^2 n)$, we present
construction algorithms for the partitioning set, sparse suffix tree, and LCE
index with $\mathcal{O}(n\log_b n)$ running time and $\mathcal{O}(b)$ space,
thus also improving the state of the art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03782"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03773">Separations for Estimating Large Frequency Moments on Data Streams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03773">PDF</a><br /><b>Abstract: </b>We study the classical problem of moment estimation of an underlying vector
whose $n$ coordinates are implicitly defined through a series of updates in a
data stream. We show that if the updates to the vector arrive in the
random-order insertion-only model, then there exist space efficient algorithms
with improved dependencies on the approximation parameter $\varepsilon$. In
particular, for any real $p &gt; 2$, we first obtain an algorithm for $F_p$ moment
estimation using $\tilde{\mathcal{O}}\left(\frac{1}{\varepsilon^{4/p}}\cdot
n^{1-2/p}\right)$ bits of memory. Our techniques also give algorithms for $F_p$
moment estimation with $p&gt;2$ on arbitrary order insertion-only and turnstile
streams, using $\tilde{\mathcal{O}}\left(\frac{1}{\varepsilon^{4/p}}\cdot
n^{1-2/p}\right)$ bits of space and two passes, which is the first optimal
multi-pass $F_p$ estimation algorithm up to $\log n$ factors. Finally, we give
an improved lower bound of $\Omega\left(\frac{1}{\varepsilon^2}\cdot
n^{1-2/p}\right)$ for one-pass insertion-only streams. Our results separate the
complexity of this problem both between random and non-random orders, as well
as one-pass and multi-pass streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03773"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03753">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03753">Parameterized Complexity of Feature Selection for Categorical Data Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simonov:Kirill.html">Kirill Simonov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03753">PDF</a><br /><b>Abstract: </b>We develop new algorithmic methods with provable guarantees for feature
selection in regard to categorical data clustering. While feature selection is
one of the most common approaches to reduce dimensionality in practice, most of
the known feature selection methods are heuristics. We study the following
mathematical model. We assume that there are some inadvertent (or undesirable)
features of the input data that unnecessarily increase the cost of clustering.
Consequently, we want to select a subset of the original features from the data
such that there is a small-cost clustering on the selected features. More
precisely, for given integers $\ell$ (the number of irrelevant features) and
$k$ (the number of clusters), budget $B$, and a set of $n$ categorical data
points (represented by $m$-dimensional vectors whose elements belong to a
finite set of values $\Sigma$), we want to select $m-\ell$ relevant features
such that the cost of any optimal $k$-clustering on these features does not
exceed $B$. Here the cost of a cluster is the sum of Hamming distances
($\ell_0$-distances) between the selected features of the elements of the
cluster and its center. The clustering cost is the total sum of the costs of
the clusters. We use the framework of parameterized complexity to identify how
the complexity of the problem depends on parameters $k$, $B$, and $|\Sigma|$.
Our main result is an algorithm that solves the Feature Selection problem in
time $f(k,B,|\Sigma|)\cdot m^{g(k,|\Sigma|)}\cdot n^2$ for some functions $f$
and $g$. In other words, the problem is fixed-parameter tractable parameterized
by $B$ when $|\Sigma|$ and $k$ are constants. Our algorithm is based on a
solution to a more general problem, Constrained Clustering with Outliers. We
also complement our algorithmic findings with complexity lower bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03753"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03594">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03594">Learning stochastic decision trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03594">PDF</a><br /><b>Abstract: </b>We give a quasipolynomial-time algorithm for learning stochastic decision
trees that is optimally resilient to adversarial noise. Given an
$\eta$-corrupted set of uniform random samples labeled by a size-$s$ stochastic
decision tree, our algorithm runs in time
$n^{O(\log(s/\varepsilon)/\varepsilon^2)}$ and returns a hypothesis with error
within an additive $2\eta + \varepsilon$ of the Bayes optimal. An additive
$2\eta$ is the information-theoretic minimum.
</p>
<p>Previously no non-trivial algorithm with a guarantee of $O(\eta) +
\varepsilon$ was known, even for weaker noise models. Our algorithm is
furthermore proper, returning a hypothesis that is itself a decision tree;
previously no such algorithm was known even in the noiseless setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03594"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">STOC Test of time award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br /><br />The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br /><br />Please nominate the papers you think have most influenced our field!</li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/"><span class="datestr">at May 11, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03831">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03831">Super Solutions of the Model RB</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Guangyan.html">Guangyan Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Wei.html">Wei Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03831">PDF</a><br /><b>Abstract: </b>The concept of super solution is a special type of generalized solutions with
certain degree of robustness and stability. In this paper we consider the
$(1,1)$-super solutions of the model RB. Using the first moment method, we
establish a "threshold" such that as the constraint density crosses this value,
the expected number of $(1,1)$-super solutions goes from $0$ to infinity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03831"><span class="datestr">at May 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03697">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03697">Quantum Proofs of Proximity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Marcel Dall'Agnol, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gur:Tom.html">Tom Gur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moulik:Subhayan_Roy.html">Subhayan Roy Moulik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thaler:Justin.html">Justin Thaler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03697">PDF</a><br /><b>Abstract: </b>We initiate the systematic study of QMA algorithms in the setting of property
testing, to which we refer as QMA proofs of proximity (QMAPs). These are
quantum query algorithms that receive explicit access to a sublinear-size
untrusted proof and are required to accept inputs having a property $\Pi$ and
reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a
minuscule portion of their input. Our algorithmic results include a
general-purpose theorem that enables quantum speedups for testing an expressive
class of properties, namely, those that are succinctly decomposable.
Furthermore, we show quantum speedups for properties that lie outside of this
family, such as graph bipartitneness. We also investigate the complexity
landscape of this model, showing that QMAPs can be exponentially stronger than
both classical proofs of proximity and quantum testers. To this end, we extend
the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to
prove quantum property testing lower bounds via reductions from communication
complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory
of Computing, 2016).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03697"><span class="datestr">at May 11, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03531">On the Complexity of Verification of Time-Sensitive Distributed Systems: Technical Report</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Max Kanovich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kirigin:Tajana_Ban.html">Tajana Ban Kirigin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nigam:Vivek.html">Vivek Nigam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scedrov:Andre.html">Andre Scedrov</a>, Carolyn Talcott <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03531">PDF</a><br /><b>Abstract: </b>Time-Sensitive Distributed Systems (TSDS), such as applications using
autonomous drones, achieve goals under possible environment interference (e.g.,
winds). Goals are often specified using explicit time constraints, and,
moreover, goals must be satisfied by the system perpetually. For example,
drones carrying out the surveillance of some area must always have recent
pictures, i.e., at most M time units old, of some strategic locations. This
paper proposes a Multiset Rewriting language with explicit time for specifying
and analyzing TSDSes. We introduce new properties, such as realizability (there
exists a good trace), survivability (where, in addition, all admissible traces
are good), recoverability (all compliant traces do not reach
points-of-no-return), and reliability (system can always continue functioning
using a good trace). A good trace is an infinite trace in which goals are
perpetually satisfied. We propose a class of systems called Progressing Timed
Systems (PTS), where intuitively only a finite number of actions can be carried
out in a bounded time period. We prove that for this class of systems the
problems of realizability, recoverability, reliability, and survivability are
PSPACE-complete. Furthermore, if we impose a bound on time (as in bounded
model-checking), we show that for PTS, realizability becomes NP-complete, while
survivability and reliability problems are in the $\Delta_2^p$ class of the
polynomial hierarchy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03531"><span class="datestr">at May 11, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br />
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/"><span class="datestr">at May 10, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5486">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br /></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br /></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5486"><span class="datestr">at May 10, 2021 05:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8609684815037895352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html">Trump, Facebook, and ComplexityBlog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html"><span class="datestr">at May 10, 2021 12:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">Arc-triangle tilings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg" alt="Tiling by ordinary triangles" /></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg" alt="Tiling by arc-triangles with two curved sides" /></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg" alt="Tiling by arc-triangles with three curved sides" /></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pinwheels.svg" alt="Tiling by arc-triangles with two curved sides, without reflection" /></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html"><span class="datestr">at May 09, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1512">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1512">News for April 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper.<em> (We may have missed some; if so, please let us know in the comments!)</em></p>



<p><strong>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</strong>, by Sepehr Assadi and Vishvajeet N (<a href="https://arxiv.org/abs/2104.04908">arXiv</a>). This paper establishes space vs. pass trade-offs lower bounds for streaming algorithms, for a variety of graph tasks: that is, of the sort “any \(m\)-pass-streaming algorithm for task \(\mathcal{T}\) must use memory at least \(f(m)\).” The tasks considered include graph property estimation (size of the maximum matching, of the max cut, of the  weight of the MST) and property testing for sparse graphs (connectivity, bipartiteness, and cycle-freeness). The authors obtained exponentially improved lower bounds for those, via reductions to a relatively standard problem, (noisy) gap cycle counting, for which they establish their main lower bound. As a key component of their proof, they prove a general direct product result (XOR lemma) for the streaming setting, showing that the advantage for solving the XOR of \(\ell\) copies of a streaming predicate \(f\) decreases exponentially with \(\ell\). </p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1512"><span class="datestr">at May 08, 2021 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/068">TR21-068 |  Quantum Proofs of Proximity | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Subhayan Roy Moulik, 

	Justin Thaler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the systematic study of QMA algorithms in the setting of property testing, to which we refer as QMA proofs of proximity (QMAPs). These are quantum query algorithms that receive explicit access to a sublinear-size untrusted proof and are required to accept inputs having a property $\Pi$ and reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a minuscule portion of their input.

Our algorithmic results include a general-purpose theorem that enables quantum speedups for testing an expressive class of properties, namely, those that are succinctly decomposable. Furthermore, we show quantum speedups for properties that lie outside of this family, such as graph bipartitneness.

We also investigate the complexity landscape of this model, showing that QMAPs can be exponentially stronger than both classical proofs of proximity and quantum testers. To this end, we extend the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to prove quantum property testing lower bounds via reductions from communication complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory of Computing, 2016).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/068"><span class="datestr">at May 08, 2021 11:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/067">TR21-067 |  Variety Evasive Subspace Families | 

	Zeyu Guo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of constructing explicit variety evasive subspace families. Given a family $\mathcal{F}$ of subvarieties of a projective or affine space, a collection $\mathcal{H}$ of projective or affine $k$-subspaces is $(\mathcal{F},\epsilon)$-evasive if for every $\mathcal{V}\in\mathcal{F}$, all but at most $\epsilon$-fraction of $W\in\mathcal{H}$ intersect every irreducible component of $\mathcal{V}$ with (at most) the expected dimension. The problem of constructing such an explicit subspace family generalizes both deterministic black-box polynomial identity testing (PIT) and the problem of constructing explicit (weak) lossless rank condensers. 

Using Chow forms, we construct explicit $k$-subspace families of polynomial size that are evasive for all varieties of bounded degree in a projective or affine $n$-space. As one application, we obtain a complete derandomization of Noether's normalization lemma for varieties of bounded degree in a projective or affine $n$-space. In another application, we obtain a simple polynomial-time black-box PIT algorithm for depth-4 arithmetic circuits with bounded top fan-in and bottom fan-in that are not in the Sylvester-Gallai configuration, improving and simplifying a result of Gupta (ECCC TR 14-130).

As a complement of our explicit construction, we prove a lower bound for the size of $k$-subspace families that are evasive for degree-$d$ varieties in a projective $n$-space. When $n-k=\Omega(n)$, the lower bound is superpolynomial unless $d$ is bounded. The proof uses a dimension-counting argument on Chow varieties that parametrize projective subvarieties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/067"><span class="datestr">at May 08, 2021 06:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html">Congratulations, Dr. Matias!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://pmatias.com/">Pedro Ascensao Ferreira Matias</a>, one of the students working with Mike Goodrich in the UC Irvine <a href="https://www.ics.uci.edu/~theory/">Center for Algorithms and Theory of Computation</a>, passed his Ph.D. defense today.</p>

<p>Pedro is Portuguese, and came to UCI after a bachelor’s degree from the University of Coimbra in Portugal and a master’s degree from Chalmers University of Technology in Sweden.</p>

<p>The general topic of Pedro’s research is “exact learning”, the inference of structured information from queries or other smaller pieces of data. I’ve written here before about my work with Matias on <a href="https://11011110.github.io/blog/2019/02/21/mutual-nearest-neighbors.html">nearest-neighbor chains</a> and on <a href="https://11011110.github.io/blog/2019/08/17/footprints-in-snow.html">tracking paths in planar graphs</a>, the problem of placing sensors on a small subset of vertices so that, by detecting the order in which a path reaches each sensor, you can uniquely determine the whole path. His dissertation combines the tracking paths work with a second paper on tracking paths (“How to Catch Marathon Cheaters: New Approximation Algorithms for Tracking Paths”, <a href="https://arxiv.org/abs/2104.12337">arXiv:2104.12337</a>, to appear at WADS 2021), and a paper on reconstructing periodic and near-periodic strings from sublinear numbers of queries (“Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors and Jumbled-Index Queries in String Reconstruction”, <a href="https://arxiv.org/abs/2007.08787">arXiv:2007.08787</a>, in SPIRE 2020). He also has recent papers on reconstructing trees in SPAA 2020 and ESA 2020.</p>

<p>After finishing his doctorate, Pedro’s next position will be working for Facebook.</p>

<p>Congratulations, Pedro!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106196168129163033">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html"><span class="datestr">at May 07, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">Faculty Full Professor at University of Bamberg, Germany (apply by June 18, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems.</p>
<p>Website: <a href="https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/">https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/</a><br />
Email: michael.mendler@uni-bamberg.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/"><span class="datestr">at May 07, 2021 02:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1659">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/">Fair Clustering with Probabilistic Group Membership</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br /><br />Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h \in {\cal H}" class="latex" /> to designate its group membership and we were to consider the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="l_h" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="u_h" class="latex" /> are the lower and upper pre-set proportionality bounds for color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C_i" class="latex" /> denotes the points in cluster <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="i" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C^h_i" class="latex" /> denotes the subset of those points with color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br /></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" class="wp-image-1735" height="151" />Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br /></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" class="wp-image-1737" height="210" />Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-center, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-median, and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p></p></div>







<p class="date">
by seyed2357 <a href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/"><span class="datestr">at May 07, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/">Ph.D. Student at IDSIA, USI-SUPSI, Lugano, Switzerland (apply by June 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms.<br />
The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas.<br />
The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and contact details of 2-3 references.</p>
<p>Website: <a href="https://people.idsia.ch//~grandoni/">https://people.idsia.ch//~grandoni/</a><br />
Email: fabrizio@idsia.ch</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/"><span class="datestr">at May 07, 2021 09:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21716">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/">Alef Corner: ICM2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img width="2100" alt="icm2022" src="https://gilkalai.files.wordpress.com/2021/05/icm2022.jpg" class="alignnone size-full wp-image-21717" height="2100" /><strong><span style="color: #ff0000;">Alef’s new piece for ICM 2022 will surely cheer you up!</span> </strong></p>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/"><span class="datestr">at May 06, 2021 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
