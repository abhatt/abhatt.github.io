<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 12, 2021 12:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/100">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/100">TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/100"><span class="datestr">at July 12, 2021 07:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6138760775046652444">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html">Would you take this bet (Part 2) ?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br /></p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br /></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>The expected value is infinity.</p><p><br /></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below. </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i> might be a paradox since most people would say NO even though the expected value is infinity.  See <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a> for more background.</p><p>Shapley (see <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would not  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it. </p><p><br /></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars. </p><p>The amount of money in the world is, according to <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a> around 1.2 quadrillion dollars  which is roughly 2^{40} dollars. </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play. </p><p><br /></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20. </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win. </p><p>Does Shapley's discussions  <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then Shapley  resolves the paradox  by contrasting the real world to the math world. </p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html"><span class="datestr">at July 12, 2021 12:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/099">TR21-099 |  Improved Product-Based High-Dimensional Expanders | 

	Louis Golowich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
High-dimensional expanders generalize the notion of expander graphs to higher-dimensional simplicial complexes. In contrast to expander graphs, only a handful of high-dimensional expander constructions have been proposed, and no elementary combinatorial construction with near-optimal expansion is known. In this paper, we introduce an improved combinatorial high-dimensional expander construction, by modifying a previous construction of Liu, Mohanty, and Yang (ITCS 2020), which is based on a high-dimensional variant of a tensor product. Our construction achieves a spectral gap of $\Omega(\frac{1}{k^2})$ for random walks on the $k$-dimensional faces, which is only quadratically worse than the optimal bound of $\Theta(\frac{1}{k})$. Previous combinatorial constructions, including that of Liu, Mohanty, and Yang, only achieved a spectral gap that is exponentially small in $k$. We also present reasoning that suggests our construction is optimal among similar product-based constructions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/099"><span class="datestr">at July 11, 2021 07:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/098">TR21-098 |  On the Probabilistic Degree of an $n$-variate Boolean Function | 

	Srikanth Srinivasan, 

	S Venkitesh</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Nisan and Szegedy (CC 1994) showed that any Boolean function $f:\{0,1\}^n\to\{0,1\}$ that depends on all its input variables, when represented as a real-valued multivariate polynomial $P(x_1,\ldots,x_n)$, has degree at least $\log n - O(\log \log n)$. This was improved to a tight $(\log n - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar statements are also known for other Boolean function complexity measures such as Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate degree (Ambainis and de Wolf (CC 2014)). 

In this paper, we address this question for \emph{Probabilistic degree}. The function $f$ has probabilistic degree at most $d$ if there is a random real-valued polynomial of degree at most $d$ that agrees with $f$ at each input with high probability. Our understanding of this complexity measure is significantly weaker than those above: for instance, we do not even know the probabilistic degree of the OR function, the best-known bounds put it between $(\log n)^{1/2-o(1)}$ and $O(\log n)$ (Beigel, Reingold, Spielman (STOC 1991); Tarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).

Here we can give a near-optimal understanding of the probabilistic degree of $n$-variate functions $f$, \emph{modulo} our lack of understanding of the probabilistic degree of OR. We show that if the probabilistic degree of OR is $(\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is at least $(\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\log n)^{o(1)}$ factors.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/098"><span class="datestr">at July 11, 2021 04:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03932">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03932">Perfect Sampling for (Atomic) Lov\'asz Local Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Kun.html">Kun He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaoming.html">Xiaoming Sun</a>, Kewen Wu <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03932">PDF</a><br /><b>Abstract: </b>We give a Markov chain based perfect sampler for uniform sampling solutions
of constraint satisfaction problems (CSP). Under some mild Lov\'asz local lemma
conditions where each constraint of the CSP has a small number of forbidden
local configurations, our algorithm is accurate and efficient: it outputs a
perfect uniform random solution and its expected running time is quasilinear in
the number of variables. Prior to our work, perfect samplers are only shown to
exist for CSPs under much more restrictive conditions (Guo, Jerrum, and Liu,
JACM'19).
</p>
<p>Our algorithm has two components:
</p>
<p>1. A simple perfect sampling algorithm using bounding chains (Huber, STOC'98;
Haggstrom and Nelander, Scandinavian Journal of Statistics'99). This sampler is
efficient if each variable domain is small.
</p>
<p>2. A simple but powerful state tensorization trick to reduce large domains to
smaller ones. This trick is a generalization of state compression (Feng, He,
and Yin, STOC'21).
</p>
<p>The crux of our analysis is a simple information percolation argument which
allows us to achieve bounds even beyond current best approximate samplers
(Jain, Pham, and Vuong, ArXiv'21).
</p>
<p>Previous related works either use intricate algorithms or need sophisticated
analysis or even both. Thus we view the simplicity of both our algorithm and
analysis as a strength of our work.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03932"><span class="datestr">at July 11, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03916">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03916">Balanced Allocations with Incomplete Information: The Power of Two Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Dimitrios Los, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sauerwald:Thomas.html">Thomas Sauerwald</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03916">PDF</a><br /><b>Abstract: </b>We consider the problem of allocating $m$ balls into $n$ bins with incomplete
information. In the classical two-choice process, a ball first queries the load
of $\textit{two}$ randomly chosen bins and is then placed in the least loaded
bin. In our setting, each ball also samples two random bins but can only
estimate a bin's load by sending $\textit{binary queries}$ of the form "Is the
load at least the median?" or "Is the load at least $100$?".
</p>
<p>For the lightly loaded case $m=O(n)$, one can achieve an $O(\sqrt{\log n/\log
\log n})$ maximum load with one query per chosen bin using an oblivious
strategy, as shown by Feldheim and Gurel-Gurevich (2018). For the case
$m=\Omega(n)$, the authors conjectured that the same strategy achieves a
maximum load of $m/n+O(\sqrt{\log n/\log \log n})$. In this work, we disprove
this conjecture by showing a lower bound of $m/n+\Omega( \sqrt{\log n})$ for a
fixed $m=\Theta(n \sqrt{\log n})$, and a lower bound of $m/n+\Omega(\log
n/\log\log n)$ for some $m$ depending on the used strategy. Surprisingly, these
lower bounds hold even for any $\textit{adaptive strategy}$ with one query,
i.e., queries may depend on the full history of the process.
</p>
<p>We complement this negative result by proving a positive result for multiple
queries. In particular, we show that with only two binary queries per chosen
bin, there is an oblivious strategy which ensures a maximum load of
$m/n+O(\sqrt{\log n})$ whp for any $m \geq 1$. For any $k=O(\log \log n)$
binary queries, the upper bound on the maximum load improves to $m/n+O(k(\log
n)^{1/k})$ whp for any $m \geq 1$. Hence for $k=\Theta(\log\log n)$, we recover
the two-choice result up to a constant multiplicative factor, including the
heavily loaded case where $m=\Omega(n)$. One novel aspect of our proof
techniques is the use of multiple super-exponential potential functions, which
might be of use in future work.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03916"><span class="datestr">at July 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03885">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03885">Keep That Card in Mind: Card Guessing with Limited Memory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Menuhin:Boaz.html">Boaz Menuhin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naor:Moni.html">Moni Naor</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03885">PDF</a><br /><b>Abstract: </b>A card guessing game is played between two players, Guesser and Dealer. At
the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1,
..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser
guesses which card was drawn, and then the card is discarded from the deck. The
Guesser receives a point for each correctly guessed card. With perfect memory,
a Guesser can keep track of all cards that were played so far and pick at
random a card that has not appeared so far, yielding in expectation $\ln n$
correct guesses. With no memory, the best a Guesser can do will result in a
single guess in expectation. We consider the case of a memory bounded Guesser
that has $m &lt; n$ memory bits. We show that the performance of such a memory
bounded Guesser depends much on the behavior of the Dealer. In more detail, we
show that there is a gap between the static case, where the Dealer draws cards
from a properly shuffled deck or a prearranged one, and the adaptive case,
where the Dealer draws cards thoughtfully, in an adversarial manner.
Specifically:
</p>
<p>1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near
optimal result against any static Dealer.
</p>
<p>2. We show that no Guesser with $m$ bits of memory can score better than
$O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min
\{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal.
</p>
<p>3. We show an efficient adaptive Dealer against which no Guesser with $m$
memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in
expectation.
</p>
<p>These results are (almost) tight, and we prove them using compression
arguments that harness the guessing strategy for encoding.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03885"><span class="datestr">at July 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03861">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03861">Feedback Vertex Set on Geometric Intersection Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/An:Shinwoo.html">Shinwoo An</a>, Eunjin Oh <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03861">PDF</a><br /><b>Abstract: </b>In this paper, we present an algorithm for computing a feedback vertex set of
a unit disk graph of size $k$, if it exists, which runs in time
$2^{O(\sqrt{k})}(n+m)$, where $n$ and $m$ denote the numbers of vertices and
edges, respectively. This improves the $2^{O(\sqrt{k}\log k)}n^{O(1)}$-time
algorithm for this problem on unit disk graphs by Fomin et al. [ICALP 2017].
Moreover, our algorithm is optimal assuming the exponential-time hypothesis.
Also, our algorithm can be extended to handle geometric intersection graphs of
similarly sized fat objects without increasing the running time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03861"><span class="datestr">at July 11, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03821">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03821">Sampling and Counting Edges via Vertex Accesses</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jakub Tětek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thorup:Mikkel.html">Mikkel Thorup</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03821">PDF</a><br /><b>Abstract: </b>We consider the problems of sampling and counting edges from a graph on $n$
vertices where our basic access is via uniformly sampled vertices. When we have
a vertex, we can see its degree, and access its neighbors. Eden and Rosenbaum
[SOSA 2018] have shown it is possible to sample an edge $\epsilon$-uniformly in
$O(\sqrt{1/\epsilon}\frac{n}{\sqrt{m}})$ vertex accesses. Here, we get down to
expected $O(\log(1/\epsilon)\frac{n}{\sqrt{m}})$ vertex accesses. Next, we
consider the problem of sampling $s&gt;1$ edges. For this we introduce a model
that we call hash-based neighbor access. We show that, w.h.p, we can sample $s$
edges exactly uniformly at random, with or without replacement, in
$\tilde{O}(\sqrt{s} \frac{n}{\sqrt{m}} + s)$ vertex accesses. We present a
matching lower bound of $\Omega(\sqrt{s} \frac{n}{\sqrt{m}} + s)$ which holds
for $\epsilon$-uniform edge multi-sampling with some constant $\epsilon&gt;0$ even
though our positive result has $\epsilon=0$.
</p>
<p>We then give an algorithm for edge counting. W.h.p., we count the number of
edges to within error $\epsilon$ in time $\tilde{O}(\frac{n}{\epsilon\sqrt{m}}
+ \frac{1}{\epsilon^2})$. When $\epsilon$ is not too small (for $\epsilon \geq
\frac{\sqrt m}{n}$), we present a near-matching lower-bound of
$\Omega(\frac{n}{\epsilon \sqrt{m}})$. In the same range, the previous best
upper and lower bounds were polynomially worse in $\epsilon$.
</p>
<p>Finally, we give an algorithm that instead of hash-based neighbor access uses
the more standard pair queries (``are vertices $u$ and $v$ adjacent''). W.h.p.
it returns $1+\epsilon$ approximation of the number of edges and runs in
expected time $\tilde{O}(\frac{n}{\epsilon \sqrt{m}} + \frac{1}{\epsilon^4})$.
This matches our lower bound when $\epsilon$ is not too small, specifically for
$\epsilon \geq \frac{m^{1/6}}{n^{1/3}}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03821"><span class="datestr">at July 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03795">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03795">An Efficient Reduction of a Gammoid to a Partition Matroid</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leichter:Marilena.html">Marilena Leichter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moseley:Benjamin.html">Benjamin Moseley</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pruhs:Kirk.html">Kirk Pruhs</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03795">PDF</a><br /><b>Abstract: </b>Our main contribution is a polynomial-time algorithm to reduce a
$k$-colorable gammoid to a $(2k-2)$-colorable partition matroid. It is known
that there are gammoids that can not be reduced to any $(2k-3)$-colorable
partition matroid, so this result is tight. We then discuss how such a
reduction can be used to obtain polynomial-time algorithms with better
approximation ratios for various natural problems related to coloring and list
coloring the intersection of matroids.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03795"><span class="datestr">at July 11, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03778">Choiceless Polynomial Time, Symmetric Circuits and Cai-F\"urer-Immerman Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pago:Benedikt.html">Benedikt Pago</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03778">PDF</a><br /><b>Abstract: </b>Choiceless Polynomial Time (CPT) is currently the only candidate logic for
capturing PTIME (that is, it is contained in PTIME and has not been separated
from it). A prominent example of a decision problem in PTIME that is not known
to be CPT-definable is the isomorphism problem on unordered
Cai-F\"urer-Immerman graphs (the CFI-query). We study the expressive power of
CPT with respect to this problem and develop a partial characterisation of
solvable instances in terms of properties of symmetric XOR-circuits over the
CFI-graphs: The CFI-query is CPT-definable on a given class of graphs only if:
For each graph $G$, there exists an XOR-circuit $C$, whose input gates are
labelled with edges of $G$, such that $C$ is sufficiently symmetric with
respect to the automorphisms of $G$ and satisfies certain other circuit
properties. We also give a sufficient condition for CFI being solvable in CPT
and develop a new CPT-algorithm for the CFI-query. It takes as input structures
which contain, along with the CFI-graph, an XOR-circuit with suitable
properties. The strongest known CPT-algorithm for this problem can solve
instances equipped with a preorder with colour classes of logarithmic size. Our
result implicitly extends this to preorders with colour classes of
polylogarithmic size (plus some unordered additional structure). Finally, our
work provides new insights regarding a much more general problem: The existence
of a solution to an unordered linear equation system $A \cdot x = b$ over a
finite field is CPT-definable if the matrix $A$ has at most logarithmic rank
(with respect to the size of the structure that encodes the equation system).
This is another example that separates CPT from fixed-point logic with
counting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03778"><span class="datestr">at July 11, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03662">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03662">On Submodular Prophet Inequalities and Correlation Gap</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chekuri:Chandra.html">Chandra Chekuri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livanos:Vasilis.html">Vasilis Livanos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03662">PDF</a><br /><b>Abstract: </b>Prophet inequalities and secretary problems have been extensively studied in
recent years due to their elegance, connections to online algorithms,
stochastic optimization, and mechanism design problems in game theoretic
settings. Rubinstein and Singla developed a notion of combinatorial prophet
inequalities in order to generalize the standard prophet inequality setting to
combinatorial valuation functions such as submodular and subadditive functions.
For non-negative submodular functions they demonstrated a constant factor
prophet inequality for matroid constraints. Along the way they showed a variant
of the correlation gap for non-negative submodular functions.
</p>
<p>In this paper we revisit their notion of correlation gap as well as the
standard notion of correlation gap and prove much tighter and cleaner bounds.
Via these bounds and other insights we obtain substantially improved constant
factor combinatorial prophet inequalities for both monotone and non-monotone
submodular functions over any constraint that admits an Online Contention
Resolution Scheme. In addition to improved bounds we describe efficient
polynomial-time algorithms that achieve these bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03662"><span class="datestr">at July 11, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03615">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03615">Angles of Arc-Polygons and Lombardi Drawings of Cacti</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eppstein:David.html">David Eppstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frishberg:Daniel.html">Daniel Frishberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Osegueda:Martha_C=.html">Martha C. Osegueda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03615">PDF</a><br /><b>Abstract: </b>We characterize the triples of interior angles that are possible in
non-self-crossing triangles with circular-arc sides, and we prove that a given
cyclic sequence of angles can be realized by a non-self-crossing polygon with
circular-arc sides whenever all angles are at most pi. As a consequence of
these results, we prove that every cactus has a planar Lombardi drawing (a
drawing with edges depicted as circular arcs, meeting at equal angles at each
vertex) for its natural embedding in which every cycle of the cactus is a face
of the drawing. However, there exist planar embeddings of cacti that do not
have planar Lombardi drawings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03615"><span class="datestr">at July 11, 2021 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03570">Boosting Method in Approximately Solving Linear Programming with Fast Online Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Wenzhi.html">Wenzhi Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Chunlin.html">Chunlin Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Yuyang.html">Yuyang Ye</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Yinyu.html">Yinyu Ye</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03570">PDF</a><br /><b>Abstract: </b>In this paper, we develop a new algorithm combining the idea of ``boosting''
with the first-order algorithm to approximately solve a class of (Integer)
Linear programs(LPs) arisen in general resource allocation problems. Not only
can this algorithm solve LPs directly, but also can be applied to accelerate
the Column Generation method. As a direct solver, our algorithm achieves a
provable $O(\sqrt{n/K})$ optimality gap, where $n$ is the number of variables
and $K$ is the number of data duplication bearing the same intuition as the
boosting algorithm. We use numerical experiments to demonstrate the
effectiveness of our algorithm and several variants.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03570"><span class="datestr">at July 11, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03569">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03569">Dynamic Data-Race Detection through the Fine-Grained Lens</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulkarni:Rucha.html">Rucha Kulkarni</a>, Umang Mathur, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavlogiannis:Andreas.html">Andreas Pavlogiannis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03569">PDF</a><br /><b>Abstract: </b>Data races are among the most common bugs in concurrency. The standard
approach to data-race detection is via dynamic analyses, which work over
executions of concurrent programs, instead of the program source code. The rich
literature on the topic has created various notions of dynamic data races,
which are known to be detected efficiently when certain parameters (e.g.,
number of threads) are small. However, the \emph{fine-grained} complexity of
all these notions of races has remained elusive, making it impossible to
characterize their trade-offs between precision and efficiency.
</p>
<p>In this work we establish several fine-grained separations between many
popular notions of dynamic data races. The input is an execution trace with $N$
events, $T$ threads and $L$ locks. Our main results are as follows. First, we
show that happens-before (HB) races can be detected in $O(N\cdot \min(T, L))$
time, improving over the standard $O(N\cdot T)$ bound when $L=o(T)$. Moreover,
we show that even reporting an HB race that involves a read access is hard for
2-orthogonal vectors (2-OV). This is the first rigorous proof of the
conjectured quadratic lower-bound in detecting HB races. Second, we show that
the recently introduced synchronization-preserving races are hard to detect for
OV-3 and thus have a cubic lower bound, when $T=\Omega(N)$. This establishes a
complexity separation from HB races which are known to be less expressive.
Third, we show that lock-cover races are hard for 2-OV, and thus have a
quadratic lower-bound, even when $T=2$ and $L = \omega(\log N)$. The similar
notion of lock-set races is known to be detectable in $O(N\cdot L)$ time, and
thus we achieve a complexity separation between the two. Moreover, we show that
lock-set races become hitting-set (HS)-hard when $L=\Theta(N)$, and thus also
have a quadratic lower bound, when the input is sufficiently complex.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03569"><span class="datestr">at July 11, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03485">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03485">Space-Efficient Fault-Tolerant Diameter Oracles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bil=ograve=:Davide.html">Davide Bilò</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Sarel.html">Sarel Cohen</a>, Tobias Friedrich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schirneck:Martin.html">Martin Schirneck</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03485">PDF</a><br /><b>Abstract: </b>We design $f$-edge fault-tolerant diameter oracles ($f$-FDOs). We preprocess
a given graph $G$ on $n$ vertices and $m$ edges, and a positive integer $f$, to
construct a data structure that, when queried with a set $F$ of $|F| \leq f$
edges, returns the diameter of $G-F$.
</p>
<p>For a single failure ($f=1$) in an unweighted directed graph of diameter $D$,
there exists an approximate FDO by Henzinger et al. [ITCS 2017] with stretch
$(1+\varepsilon)$, constant query time, space $O(m)$, and a combinatorial
preprocessing time of $\widetilde{O}(mn + n^{1.5} \sqrt{Dm/\varepsilon})$.We
present an FDO for directed graphs with the same stretch, query time, and
space. It has a preprocessing time of $\widetilde{O}(mn + n^2/\varepsilon)$.
The preprocessing time nearly matches a conditional lower bound for
combinatorial algorithms, also by Henzinger et al. With fast matrix
multiplication, we achieve a preprocessing time of $\widetilde{O}(n^{2.5794} +
n^2/\varepsilon)$. We further prove an information-theoretic lower bound
showing that any FDO with stretch better than $3/2$ requires $\Omega(m)$ bits
of space.
</p>
<p>For multiple failures ($f&gt;1$) in undirected graphs with non-negative edge
weights, we give an $f$-FDO with stretch $(f+2)$, query time $O(f^2\log^2{n})$,
$\widetilde{O}(fn)$ space, and preprocessing time $\widetilde{O}(fm)$. We
complement this with a lower bound excluding any finite stretch in $o(fn)$
space. We show that for unweighted graphs with polylogarithmic diameter and up
to $f = o(\log n/ \log\log n)$ failures, one can swap approximation for query
time and space. We present an exact combinatorial $f$-FDO with preprocessing
time $mn^{1+o(1)}$, query time $n^{o(1)}$, and space $n^{2+o(1)}$. When using
fast matrix multiplication instead, the preprocessing time can be improved to
$n^{\omega+o(1)}$, where $\omega &lt; 2.373$ is the matrix multiplication
exponent.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03485"><span class="datestr">at July 11, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03460">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03460">Geometric averages of partitioned datasets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Needham:Tom.html">Tom Needham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weighill:Thomas.html">Thomas Weighill</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03460">PDF</a><br /><b>Abstract: </b>We introduce a method for jointly registering ensembles of partitioned
datasets in a way which is both geometrically coherent and partition-aware.
Once such a registration has been defined, one can group partition blocks
across datasets in order to extract summary statistics, generalizing the
commonly used order statistics for scalar-valued data. By modeling a
partitioned dataset as an unordered $k$-tuple of points in a Wasserstein space,
we are able to draw from techniques in optimal transport. More generally, our
method is developed using the formalism of local Fr\'{e}chet means in symmetric
products of metric spaces. We establish basic theory in this general setting,
including Alexandrov curvature bounds and a verifiable characterization of
local means. Our method is demonstrated on ensembles of political redistricting
plans to extract and visualize basic properties of the space of plans for a
particular state, using North Carolina as our main example.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03460"><span class="datestr">at July 11, 2021 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html">Angles of arc-triangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the <a href="https://en.wikipedia.org/wiki/Lune_of_Hippocrates">lune of Hippocrates</a>, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a>, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</p>

<p>I looked at one of these properties, <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">the ability of arc-triangles to tile the plane</a>, in an earlier post. Another of these properties involves the feasible combinations of angles of these shapes. As is well known, in a straight-sided triangle in the plane, the three interior angles always sum to exactly \(\pi\), and any three positive angles summing to \(\pi\) are possible. Let \(T\) be the set of triples of angles \((\theta_1,\theta_2,\theta_3)\) from triangles, and reinterpret these triples as coordinates of points in Euclidean space. Then \(T\) is itself an equilateral triangle, with corners at the three points \((\pi,0,0)\), \((0,\pi,0)\), and \((0,0\pi)\). (More precisely, it’s the relative interior of this triangle.)</p>

<p>What about arc-triangles? Are their angles similarly constrained? What shape do their triples of angles make? First of all, their angles don’t have a fixed sum (except for the tilers, for which this sum is again \(\pi\)). The arbelos has three interior angles that are all zero, summing to zero. The Reuleux triangle has three angles of \(2\pi/3\), summing to \(2\pi\). <a href="https://11011110.github.io/blog/2021/05/15/linkage.html">Boscovitch’s cardioid</a>, below, uses three semicircles like the arbelos, but with one interior angle of \(2\pi\) and two others equal to \(\pi\), summing to \(4\pi\). The <a href="https://en.wikipedia.org/wiki/Trefoil">trefoil</a>, a common architectural motif, bulges outward from its three corners, forming interior angles that are much larger, up to \(2\pi\) each for a trefoil made from three \(5/6\)-circle arcs, for a total interior angle of \(6\pi\).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/boscovich.svg" alt="Boscovich's cardioid" /></p>

<p>Nevertheless, for a non-self-crossing arc-triangle, not all combinations of angles are possible. For instance, it’s not possible to have one angle that is zero and two that are \(2\pi\). My new preprint, “Angles of arc-polygons and lombardi drawings of cacti” (<a href="https://arxiv.org/abs/2107.03615">arXiv:2107.03615
</a>, with UCI students Daniel Frishberg and Martha Osegueda, to appear at CCCG) proves a precise characterization: beyond the obvious requirement that each angle \(\theta_i\) be in the range \(0\le\theta_i\le 2\pi\), we have only the additional inequalities</p>

\[-\pi &lt; \frac{\pi - \theta_i + \theta_{i+1} - \theta_{i+2}}{2} &lt; \pi\]

<p>where the index arithmetic is done modulo three. The formula in the middle of each of these inequalities is itself an angle, the angle of incidence between one of the circular arcs of the arc-triangle and the circle through its three corners. Where the straight triangles had an equilateral-triangle feasible region, the arc-triangles have a more complicated shape. The obvious constraints \(0\le\theta_i\le 2\pi\) would produce a cubical feasible region \([0,2\pi]^3\), but the additional inequalities above cut off six corners of the cube, leaving a feasible region looking like this:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/feasible-arc-triangles.svg" alt="The feasible region for triples of angles of arc-triangles" /></p>

<p>The motivating application for all of this is graph drawing, and more specifically Lombardi drawing, in which edges are circular arcs meeting at equal angles at each vertex. Using our new understanding of arc-polygons, we prove that every <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus graph</a> has a planar Lombardi drawing for its natural embedding (the one in which each cycle of the cactus forms a face) but might not for some other embeddings, including the one below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/badhat.svg" alt="An embedded cactus that has no planar Lombardi drawing" /></p>

<p>But beyond graph drawing, I think that the long history and many applications of arc-polygons justifies more study of their general properties. For instance, what about arc-polygons with more than three sides? What can their angles be? Our paper has a partial answer, enough to answer the questions we asked in our Lombardi drawing application, but a complete characterization for arc-polygons of more than three sides is still open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106557711895868173">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html"><span class="datestr">at July 10, 2021 11:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/097">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/097">TR21-097 |  Number of Variables for Graph Identification and the Resolution of GI Formulas | 

	Jacobo Toran, 

	Florian Wörz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that the number of variables and the quantifier depth needed to distinguish a pair of graphs by first-order logic sentences exactly match the complexity measures of clause width and positive depth needed to refute the corresponding graph isomorphism formula in propositional narrow resolution. 

Using this connection, we obtain upper and lower bounds for refuting graph  isomorphism formulas in (normal) resolution. In particular, we show that if $k$ is the minimum number of variables needed to distinguish two graphs with $n$ vertices each, then there is an $n^{\mathrm{O}(k)}$ resolution refutation size upper bound for the corresponding isomorphism formula, as well as lower bounds of $2^{k-1}$ and $k$ for the tree-like resolution size and resolution clause space for this formula. We also show a resolution size lower bound of ${\exp} \big( \Omega(k^2/n) \big)$ for the case of colored graphs with constant color class size.

Applying these results, we prove the first exponential lower bound for graph isomorphism formulas in the proof system SRC-1, a system that extends resolution with a global symmetry rule, thereby answering an open question posed by Schweitzer and Seebach.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/097"><span class="datestr">at July 09, 2021 03:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball.html">Spanners for unit ball graphs in doubling spaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators</a>. Now we have another spanner preprint: <a href="https://arxiv.org/abs/2106.15234">“Optimal spanners for unit ball graphs in doubling metrics”, arXiv:2106.15234</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Doubling_space">Doubling metrics</a> are a generalization of Euclidean spaces. Like Euclidean spaces, they have a dimension, the <em>doubling dimension</em>, but it might not be an integer. Even the doubling dimension of the Euclidean plane itself is \(\log_2 7\approx 2.807355\); this means that every circular disk of radius \(r\) in the plane can be covered by seven closed disks of radius \(r/2\).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/doubling-dim.svg" alt="Seven disks of radius $$r/2$$ cover a single disk of radius $$r$$" /></p>

<p>Analogously, the <em>doubling constant</em> of a metric space (if it exists) is the smallest number \(c\) such that every closed metric ball (the set of points within some radius \(r\) of a fixed point) can be covered by \(c\) balls of half the radius. The <em>doubling dimension</em> is the binary logarithm of the doubling constant. A metric space is a <em>doubling metric</em> or <em>doubling space</em> if it has a doubling constant and doubling dimension. This is true for all Euclidean spaces: for instance, if you cover a ball with a grid of hypercubes, sized small enough that their long diagonal has length at most the radius of the ball, you obtain doubling constant at most \(\lceil 2\sqrt{d}\rceil^d\), although it seems difficult to compute the precise doubling constant in general.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/grid-doubling.svg" alt="Nine disks of radius $$r/2$$ cover a square grid that covers a single disk of radius $$r$$" /></p>

<p>The hyperbolic plane provides a natural example of a space that is not a doubling space: arbitrarily large-radius disks require arbitrarily many half-radius disks to cover them.</p>

<p>Many results in computational geometry can be generalized to doubling metrics, but not always, and sometimes with difficulty. That includes results on spanners, as we consider in our paper. A spanner of a weighted graph is a subgraph whose shortest path distances approximate the distances in the full graph, and often for spanners of metric spaces one uses the complete graph, weighted by the metric distance between each pair of points. But here, we are using a different graph, the unit ball graph. The unit ball graph, for points in a continuous space, has an edge whenever two unit balls centered at two of the points have a nonempty intersection, and it can be extended to discrete point sets by instead including an edge whenever two points are at distance at most 2 (or, as in our paper, distance at most 1; scaling doesn’t really change anything). The weights are the same as in the complete graph. If a spanner accurately approximates all edge weights, it approximates all paths.</p>

<p>The unit ball graph has fewer edges to approximate than the complete graph. But that actually makes it harder to approximate, because by the same token there are fewer edges that can be used in the spanner. Despite that, the greedy spanner algorithm still produces a spanner, but one of its key properties is lost when going from Euclidean to doubling spaces: Euclidean greedy spanners have bounded degree, but greedy spanners in doubling spaces do not. Instead, our paper provides different spanner algorithms that apply to unit ball graphs, approximate paths in these graphs arbitrarily well, have bounded degree, have total weight a constant times that of the minimum spanning tree, and can be constructed efficiently in a distributed model of computing. I think the details are too technical to go into here, so see the paper for more.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106551534535329957">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball.html"><span class="datestr">at July 09, 2021 09:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/096">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/096">TR21-096 |  Keep That Card in Mind: Card Guessing with Limited Memory | 

	Boaz Menuhin, 

	Moni Naor</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A card guessing game is played between two players, Guesser and Dealer. At the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1, ..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser guesses which card was drawn, and then the card is discarded from the deck. The Guesser receives a point for each correctly guessed card. 

With perfect memory, a Guesser can keep track of all cards that were played so far and pick at random a card that has not appeared so far, yielding in expectation $\ln n$ correct guesses. With no memory, the best a Guesser can do will result in a single guess in expectation. 

We consider the case of a memory bounded Guesser that has $m &lt; n$ memory bits. We show that the performance of such a memory bounded Guesser depends much on the behavior of the Dealer. In more detail, we show that there is a gap between the static case, where the Dealer draws cards from a properly shuffled deck or a prearranged one, and the adaptive case, where the Dealer draws cards thoughtfully, in an adversarial manner. Specifically: 

1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near optimal result against any static Dealer. 

2. We show that no Guesser with $m$ bits of memory can score better than $O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min \{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal. 

3. We show an efficient adaptive Dealer against which no Guesser with $m$ memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in expectation. 

These results are (almost) tight, and we prove them using compression arguments that harness the guessing strategy for encoding.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/096"><span class="datestr">at July 08, 2021 09:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/07/08/imp-reg-tf/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/07/08/imp-reg-tf/">Implicit Regularization in Tensor Factorization&amp;#58; Can Tensor Rank Shed Light on Generalization in Deep Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In effort to understand implicit regularization in deep learning, a lot of theoretical focus is being directed at matrix factorization, which can be seen as linear neural networks.
This post is based on our <a href="https://arxiv.org/pdf/2102.09972.pdf">recent paper</a> (to appear at ICML 2021), where we take a step towards practical deep learning, by investigating <em>tensor factorization</em> — a model equivalent to a certain type of non-linear neural networks.
It is well known that <a href="https://arxiv.org/pdf/0911.1393.pdf">most tensor problems are NP-hard</a>, and accordingly, the common sentiment is that working with tensors (in both theory and practice) entails extreme difficulties.
However, by adopting a dynamical systems view, we manage to avoid such difficulties, and establish an implicit regularization towards low <em>tensor rank</em>.
Our results suggest that tensor rank may shed light on generalization in deep learning.</p>

<h2 id="challenge-finding-a-right-measure-of-complexity">Challenge: finding a right measure of complexity</h2>

<p>Overparameterized neural networks are mysteriously able to generalize even when trained without any explicit regularization.
Per conventional wisdom, this generalization stems from an <em>implicit regularization</em> — a tendency of gradient-based optimization to fit training examples with predictors of minimal ‘‘complexity.’’
A major challenge in translating this intuition to provable guarantees is that we lack measures for predictor complexity that are quantitative (admit generalization bounds), and at the same time, capture the essence of natural data (images, audio, text etc.), in the sense that it can be fit with predictors of low complexity.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/imp_reg_tf_data_complexity.png" style="width: 500px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 1:</b> 
To explain generalization in deep learning, a complexity <br />
measure must allow the fit of natural data with low complexity. On the <br />
other hand, when fitting data which does not admit generalization, <br />
e.g. random data, the complexity should be high. 
</i>
</div>
<p><br /></p>

<h2 id="a-common-testbed-matrix-factorization">A common testbed: matrix factorization</h2>

<p>Without a clear complexity measure for practical neural networks, existing analyses usually focus on simple settings where a notion of complexity is obvious. 
A common example of such a setting is <em>matrix factorization</em> — matrix completion via linear neural networks. 
This model was discussed pretty extensively in previous posts (see <a href="http://www.offconvex.org/2019/06/03/trajectories/">one</a> by Sanjeev, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">one</a> by Nadav and Wei and <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">another one</a> by Nadav), but for completeness we present it again here.</p>

<p>In <em>matrix completion</em> we’re given a subset of entries from an unknown matrix $W^* \in \mathbb{R}^{d, d’}$, and our goal is to predict the unobserved entries. 
This can be viewed as a supervised learning problem with $2$-dimensional inputs, where the label of the input $( i , j )$ is $( W^* )_{i,j}$.
Under such a viewpoint, the observed entries are the training set, and the average reconstruction error over unobserved entries is the test error, quantifying generalization.
A predictor can then be thought of as a matrix, and a natural notion of complexity is its <em>rank</em>.
Indeed, in many real-world scenarios (a famous example is the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>) one is interested in <a href="https://arxiv.org/pdf/1601.06422.pdf">recovering a low rank matrix from incomplete observations</a>.</p>

<p>A ‘‘deep learning approach’’ to matrix completion is matrix factorization, where the idea is to use a linear neural network (fully connected neural network with no non-linearity), and fit observations via gradient descent (GD). 
This amounts to optimizing the following objective:</p>

<div style="text-align: center;">
\[
 \min\nolimits_{W_1 , \ldots , W_L} ~ \sum\nolimits_{(i,j) \in observations} \big[ ( W_L \cdots W_1 )_{i , j} - (W^*)_{i,j} \big]^2 ~.
\]
</div>

<p>It is obviously possible to constrain the rank of the produced solution by limiting the shared dimensions of the weight matrices $\{ W_j \}_j$. 
However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained and the factorization can express any matrix. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>As it turns out, in practice, matrix factorization with near-zero initialization and small step size tends to accurately recover low rank matrices.
This phenomenon (first identified in <a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>) manifests some kind of implicit regularization, whose mathematical characterization drew a lot of interest.
It was initially conjectured that matrix factorization implicitly minimizes nuclear norm (<a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>), but recent evidence points to implicit rank minimization, stemming from incremental learning dynamics (see <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a>; <a href="https://papers.nips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf">Razin &amp; Cohen 2020</a>; <a href="https://openreview.net/pdf/e29b53584bc9017cb15b9394735cd51b56c32446.pdf">Li et al. 2021</a>). 
Today, it seems we have a relatively firm understanding of generalization in matrix factorization.
There is a complexity measure for predictors — matrix rank — by which implicit regularization strives to lower complexity, and the data itself is of low complexity (i.e. can be fit with low complexity). 
Jointly, these two conditions lead to generalization.</p>

<h2 id="beyond-matrix-factorization-tensor-factorization">Beyond matrix factorization: tensor factorization</h2>

<p>Matrix factorization is interesting on its own behalf, but as a theoretical surrogate for deep learning it is limited.
First, it corresponds to <em>linear</em> neural networks, and thus misses the crucial aspect of non-linearity. 
Second, viewing matrix completion as a prediction problem, it doesn’t capture tasks with more than two input variables.
As we now discuss, both of these limitations can be lifted if instead of matrices one considers tensors.</p>

<p>A tensor can be thought of as a multi-dimensional array.
The number of axes in a tensor is called its <em>order</em>.
In the task of <em>tensor completion</em>, a subset of entries from an unknown tensor $\mathcal{W}^* \in \mathbb{R}^{d_1, \ldots, d_N}$ are given, and the goal is to predict the unobserved entries. 
Analogously to how matrix completion can be viewed as a prediction problem over two input variables, order-$N$ tensor completion can be seen as a prediction problem over $N$ input variables (each corresponding to a different axis).
In fact, any multi-dimensional prediction task with discrete inputs and scalar output can be formulated as a tensor completion problem.
Consider for example the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, and for simplicity assume that image pixels hold one of two values, i.e. are either black or white. 
The task of predicting labels for the $28$-by-$28$ binary images can be seen as an order-$784$ (one axis for each pixel) tensor completion problem, where all axes are of length $2$ (corresponding to the number of values a pixel can take). 
For further details on how general prediction tasks map to tensor completion problems see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/pred_prob_to_tensor_comp.png" style="width: 550px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 2:</b> 
Prediction tasks can be viewed as tensor completion problems. <br />
For example, predicting labels for input images with $3$ pixels, each taking <br />
one of $5$ grayscale values, corresponds to completing a $5 \times 5 \times 5$ tensor.
</i>
</div>
<p><br />
Like matrices, tensors can be factorized. 
The most basic scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for information on this scheme, as well as others, see the <a href="http://www.kolda.net/publication/TensorReview.pdf">excellent survey</a> of Kolda and Bader).
In <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> and this post, we use the term <em>tensor factorization</em> to refer to solving tensor completion by fitting observations via GD over CP parameterization, i.e. over the following objective ($\otimes$ here stands for outer product):</p>

<div style="text-align: center;">
\[
\min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \sum\nolimits_{ (i_1 , ... , i_N) \in observations } \big[ \big(  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big)_{i_1 , \ldots , i_N} - (\mathcal{W}^*)_{i_1 , \ldots , i_N} \big]^2 ~.
\]
</div>

<p>The concept of rank naturally extends from matrices to tensors.
The <em>tensor rank</em> of a given tensor $\mathcal{W}$ is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that for order-$2$ tensors, i.e. for matrices, this exactly coincides with matrix rank.
We can explicitly constrain the tensor rank of solutions found by tensor factorization via limiting the number of components $R$. 
However, since our interest lies on implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>By now you might be wondering what does tensor factorization have to do with deep learning.
Apparently, as Nadav mentioned in an <a href="http://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>, analogously to how matrix factorization is equivalent to matrix completion (two-dimensional prediction) via linear neural networks, tensor factorization is equivalent to tensor completion (multi-dimensional prediction) with a certain type of <em>non-linear</em> neural networks (for the exact details behind the latter equivalence see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>). 
It therefore represents a setting one step closer to practical neural networks.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/mf_lnn_tf_nonlinear.png" style="width: 900px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 3:</b> 
While matrix factorization corresponds to a linear neural network, <br />
tensor factorization corresponds to a certain non-linear neural network.
</i>
</div>
<p><br />
As a final piece of the analogy between matrix and tensor factorizations, in a <a href="https://arxiv.org/pdf/2005.06398.pdf">previous paper</a> (described in an <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>) Noam and Nadav demonstrated empirically that (similarly to the phenomenon discussed above for matrices) tensor factorization with near-zero initialization and small step size tends to accurately recover low rank tensors.
Our goal in the <a href="https://arxiv.org/pdf/2102.09972.pdf">current paper</a> was to mathematically explain this finding. 
To avoid the <a href="https://arxiv.org/pdf/0911.1393.pdf">notorious difficulty of tensor problems</a>, we chose to adopt a dynamical systems view, and analyze directly the trajectories induced by GD.</p>

<h2 id="dynamical-analysis-implicit-tensor-rank-minimization">Dynamical analysis: implicit tensor rank minimization</h2>

<p>So what can we say about the implicit regularization in tensor factorization? 
At the core of our analysis is the following dynamical characterization of component norms:</p>

<blockquote>
  <p><strong>Theorem:</strong>
Running gradient flow (GD with infinitesimal step size) over a tensor factorization with near-zero initialization leads component norms to evolve by:
[ \frac{d}{dt} || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) || \propto \color{brown}{|| \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||^{2 - 2/N}} ~,
]
where $\mathbf{w}_r^1 (t), \ldots, \mathbf{w}_r^N (t)$ denote the weight vectors at time $t \geq 0$.</p>
</blockquote>

<p>According to the theorem above, component norms evolve at a rate proportional to their size exponentiated by $\color{brown}{2 - 2 / N}$ (recall that $N$ is the order of the tensor to complete).
Consequently, they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
This suggests that when initialized near zero, components tend to remain close to the origin, and then, after passing a critical threshold, quickly grow until convergence. 
Intuitively, these dynamics induce an incremental process where components are learned one after the other, leading to solutions with a few large components and many small ones, i.e. to (approximately) low tensor rank solutions!</p>

<p>We empirically verified the incremental learning of components in many settings. 
Here is a representative example from one of our experiments (see <a href="https://arxiv.org/pdf/2102.09972.pdf">the paper</a> for more):</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/tf_dyn_exps.png" style="width: 800px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 4:</b> 
Dynamics of component norms during GD over tensor factorization. <br />
An incremental learning effect is enhanced as initialization scale decreases, <br />
leading to accurate completion of a low rank tensor.
</i>
</div>
<p><br />
Using our dynamical characterization of component norms, we were able to prove that with sufficiently small initialization, tensor factorization (approximately) follows a trajectory of rank one tensors for an arbitrary amount of time. 
This leads to:</p>

<blockquote>
  <p><strong>Theorem:</strong>
If tensor completion has a rank one solution, then under certain technical conditions, tensor factorization will reach it.</p>
</blockquote>

<p>It’s worth mentioning that, in a way, our results extend to tensor factorization the incremental rank learning dynamics known for matrix factorization (cf. <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a> and <a href="https://arxiv.org/pdf/2012.09839v1.pdf">Li et al. 2021</a>). 
As typical when transitioning from matrices to tensors, this extension entailed various challenges that necessitated use of different techniques.</p>

<h2 id="tensor-rank-as-measure-of-complexity">Tensor rank as measure of complexity</h2>

<p>Going back to the beginning of the post, recall that a major challenge towards understanding implicit regularization in deep learning is that we lack measures for predictor complexity that capture natural data. 
Now, let us recap what we have seen thus far:
$(1)$ tensor completion is equivalent to multi-dimensional prediction; 
$(2)$ tensor factorization corresponds to solving the prediction task with certain non-linear neural networks; 
and 
$(3)$ the implicit regularization of these non-linear networks, i.e. of tensor factorization, minimizes tensor rank.
Motivated by these findings, we ask the following:</p>

<blockquote>
  <p><strong>Question:</strong> 
Can tensor rank serve as a measure of predictor complexity?</p>
</blockquote>

<p>We empirically explored this prospect by evaluating the extent to which tensor rank captures natural data, i.e. to which natural data can be fit with predictors of low tensor rank.
As testbeds we used <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> and <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> datasets, comparing the resulting errors against those obtained when fitting two randomized variants: one generated via shuffling labels (‘‘rand label’’), and the other by replacing inputs with noise (‘‘rand image’’).</p>

<p>The following plot, displaying results for Fashion-MNIST (those for MNIST are similar), shows that with predictors of low tensor rank the original data is fit way more accurately than the randomized datasets. 
Specifically, even with tensor rank as low as one the original data is fit relatively well, while the error in fitting random data is close to trivial (variance of the label). 
This suggests that tensor rank as a measure of predictor complexity has potential to capture aspects of natural data! 
Note also that an accurate fit with low tensor rank coincides with low test error, which is not surprising given that low tensor rank predictors can be described with a small number of parameters.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/exp_complexity_fmnist.png" style="width: 600px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 5:</b> 
Evaluation of tensor rank as a measure of complexity — standard datasets <br />
can be fit accurately with predictors of low tensor rank (far beneath what is required by <br />
random datasets), suggesting it may capture aspects of natural data. Plot shows mean <br />
error of predictors with low tensor rank over Fashion-MNIST. Markers correspond <br />
to separate runs differing in the explicit constraint on the tensor rank.
</i>
</div>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>Overall, <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> shows that tensor rank captures both the implicit regularization of a certain type of non-linear neural networks, and aspects of natural data. 
In light of this, we believe tensor rank (or more advanced notions such as hierarchical tensor rank) might pave way to explaining both implicit regularization in more practical neural networks, and the properties of real-world data translating this implicit regularization to generalization.</p>

<p><a href="https://noamrazin.github.io/">Noam Razin</a>, <a href="https://asafmaman101.github.io/">Asaf Maman</a>, <a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/07/08/imp-reg-tf/"><span class="datestr">at July 08, 2021 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/095">TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/095"><span class="datestr">at July 07, 2021 09:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/open-problem-optimal-query-release/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/open-problem-optimal-query-release/">Open Problem - Optimal Query Release for Pure Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br />error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br />error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br /> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/open-problem-optimal-query-release/"><span class="datestr">at July 07, 2021 05:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8892467103290123546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">Would you take this bet (Part 1) ?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox. </p><p>Here it is:</p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br /></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>etc. </p><p>1) Expected value: </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value is </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br /></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html"><span class="datestr">at July 07, 2021 12:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=879">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/">Windows never changes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some things I didn’t want to remove, often with a lot of effort, because space was so tight that Windows didn’t even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program — a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/"><span class="datestr">at July 06, 2021 04:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1546">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1546">News for June 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A quieter month after last month’s bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. <em>(Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, Clément!)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>Learning-based Support Estimation in Sublinear Time</strong> by Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner (<a href="https://arxiv.org/abs/2106.08396">arXiv</a>). A classic problem in distribution testing is that of estimating the support size \(n\) of an unknown distribution \(\mathcal{D}\). (Assume that all elements in the support have probability at least \(1/n\).) A fundamental result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a> (2011)  proves that the sample complexity of this problem is \(\Theta(n/\log n)\). A line of work has emerged in trying to reduce this complexity, with additional sources of information. <a href="http://dspace.mit.edu/bitstream/handle/1721.1/101001/Rubinfeld_Testing%20probability.pdf;sequence=1">Canonne-Rubinfeld (2014)</a> showed that, if one can query the exact probabilities of elements, then the complexity can be made independent of \(n\). This paper studies a robust version of this assumption: suppose, we can get constant factor approximations to the probabilities. Then, the main result is that we can get a query complexity of \(n^{1-1/\log(\varepsilon^{-1})} \ll n/\log n\) (where the constant \(\varepsilon\) denotes the additive approximation to the support size). This paper also does empirical experiments to show that the new algorithm is indeed better in practice. Moreover, it shows that existing methods degraded rapidly with poorer probability estimates, while the new algorithm maintains its accuracy even with such estimates. </p>



<p><strong>The Price of Tolerance in Distribution Testing</strong> by Clément L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li (<a href="https://arxiv.org/abs/2106.13414">arXiv</a>). While we have seen many results in distribution testing, the subject of tolerance is one that hasn’t received as much attention. Consider the problem of testing if unknown distribution \(\mathcal{p}\) (over domain \([n]\)) is the same as known distribution \(\mathcal{q}\). We wish to distinguish \(\varepsilon_1\)-close from \(\varepsilon_2\)-far, under total variation distance. When \(\varepsilon_1\) is zero, this is the standard property testing setting, and classic results yield \(\Theta(\sqrt{n})\) sample complexity. If \(\varepsilon_1 = \varepsilon_2/2\), then we are looking for a constant factor approximation to the distance. And the complexity is \(\Theta(n/\log n)\). Surprisingly, nothing was known in better. Until this paper, that is. The main result gives a complete characterization of sample complexity (up to log factors), for all values of \(\varepsilon_1, \varepsilon_2\). Remarkably, the sample complexity has an additive term \((n/\log n) \cdot (\varepsilon_1/\varepsilon^2_2)\). Thus, when \(\varepsilon_1 &gt; \sqrt{\varepsilon_2}\), the sample complexity is \(\Theta(n/\log n)\). When \(\varepsilon_1\) is smaller, the main result gives a smooth dependence on the sample complexity. One the main challenges is that existing results use two very different techniques for the property testing vs constant-factor approximation regimes. The former uses simpler \(\ell_2\)-statistics (e.g. collision counting), while the latter is based on polynomial approximations (estimating moments). The upper bound in this paper shows that simpler statistics based on just the first two moments suffice to getting results for all regimes of \(\varepsilon_1, \varepsilon_2\).</p>



<p><strong>Open Problems in Property Testing of Graphs</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/088/">ECCC</a>). As the title clearly states, this is a survey covering a number of open problems in graph property testing. The broad division is based on the query model: dense graphs, bounded degree graphs, and general graphs. A reader will see statements of various classic open problems, such as the complexity of testing triangle freeness for dense graphs and characterizing properties that can be tested in \(poly(\varepsilon^{-1})\) queries. Arguably, there are more open problems (and fewer results) for testing in bounded degree graphs, where we lack broad characterizations of testable properties. An important, though less famous (?), open problem is that of the complexity of testing isomorphism. It would appear that the setting of general graphs, where we know the least, may be the next frontier for graph property testing. A problem that really caught my eye: can we transform testers that work for bounded degree graphs into those that work for bounded arboricity graphs? The latter is a generalization of bounded degree that has appeared in a number of recent results on sublinear graph algorithms.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1546"><span class="datestr">at July 06, 2021 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/094">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/094">TR21-094 |  New Non-FPT Lower Bounds for Some Arithmetic Formulas | 

	Sébastien Tavenas, 

	Nutan Limaye, 

	Srikanth Srinivasan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An Algebraic Formula for a polynomial $P\in F[x_1,\ldots,x_N]$ is an algebraic expression for $P(x_1,\ldots,x_N)$ using variables, field constants, additions and multiplications.  Such formulas capture an algebraic analog of the Boolean complexity class $\mathrm{NC}^1.$ Proving lower bounds against this model is thus an important problem.


It is known that, to prove superpolynomial lower bounds against algebraic formulas, it suffices to prove good enough lower bounds against restricted kinds of formulas known as Set-Multilinear formulas, for computing a polynomial $P(x_1,...,x_N)$ of degree $O(\log N/\log \log N)$. In the past, many superpolynomial lower bounds were found, but they are of the form $\Omega(f(d) poly(N))$ (where $f$ is typically a subexponential function) which is insufficient to get lower bounds for general formulas. Recently, the authors proved the first non-FPT lower bounds, i.e., a lower bound of the form $N^{\Omega{(f(d)})}$, against small-depth set-multilinear formulas (and also for circuits). In this work, we extend this result in two directions. 

1) Large-depth set-multilinear formulas. In the setting of general set-multilinear formulas, we prove a lower bound of $(\log n)^{\Omega(\log d)}$ for computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}.$ In particular, this implies the first superpolynomial lower bound against unbounded-depth set-multilinear formulas computing $IMM_{n,n}.$ 

As a corollary, this also resolves the homogeneous version of a question of Nisan (STOC 1991) regarding the relative power of Algebraic formulas and Branching programs in the non-commutative setting.

2) Stronger bounds for homogeneous non-commutative small-depth circuits. In the small-depth homogeneous non-commutative case, we prove  a lower bound of $n^{d^{1/\Delta}/2^{O(\Delta)}}$, which yields non-FPT bounds for depths up to $o(\sqrt{\log d}).$ In comparison, the previous bound works in the harder commutative set-multilinear setting, but only up to depths $o(\log \log d)$. 
Moreover, our lower bound holds for all values of $d$, as opposed to the previous set-multilinear lower bound, which holds as long as $d$ is small, i.e., $d = O(\log n)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/094"><span class="datestr">at July 06, 2021 02:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5554">Open thread on new quantum supremacy claims</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Happy 4th to those in the US!</p>



<p>The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a <em>serious</em> quantum supremacy tear lately.  Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photons—the second claim of sampling-based quantum supremacy, after Google’s in Fall 2019.  However, skeptics then poked holes in the USTC claim, showing how they could spoof the results with a classical computer, basically by reproducing the k-photon correlations for relatively small values of k.  Debate over the details continues, but the Chinese group seeks to render the debate largely moot with a <a href="https://arxiv.org/abs/2106.15534">new and better Gaussian BosonSampling experiment</a>, with 144 modes and up to 113 detected photons.  They say they were able to measure k-photon correlations for k up to about 19, which if true would constitute a serious obstacle to the classical simulation strategies that people discussed for the previous experiment.</p>



<p>In the meantime, though, an overlapping group of authors had <a href="https://arxiv.org/abs/2106.14734">put out another paper the day before</a> (!) reporting a sampling-based quantum supremacy experiment using superconducting qubits—extremely similar to what Google did (the same circuit depth and everything), except now with 56 qubits rather than 53.</p>



<p>I confess that I haven’t yet studied either paper in detail—among other reasons, because I’m on vacation with my family at the beach, and because I’m trying to spend what work-time I have on my own projects.  But anyone who <em>has</em> read them, please use the comments of this post to discuss!  Hopefully I’ll learn something.</p>



<p>To confine myself to some general comments: since Google’s announcement in Fall 2019, I’ve consistently said that sampling-based quantum supremacy is <em>not</em> yet a done deal.  I’ve said that quantum supremacy seems important enough to want independent replications, and demonstrations in other hardware platforms like ion traps and photonics, and better gate fidelity, and better classical hardness, and better verification protocols.  Most of all, I’ve said that we needed a genuine dialogue between the “quantum supremacists” and the classical skeptics: the former doing experiments and releasing all their data, the latter trying to design efficient classical simulations for those experiments, and so on in an iterative process.  Just like in applied cryptography, we’d only have real confidence in a quantum supremacy claim once it had survived at least a few years of attacks by skeptics.  So I’m delighted that this is precisely what’s now happening.  USTC’s papers are two new volleys in this back-and-forth; we all eagerly await the next volley, whichever side it comes from.</p>



<p>While I’ve been trying for years to move away from the expectation that I blog about each and every QC announcement that someone messages me about, maybe I’ll also say a word about the recent announcement by IBM of a quantum advantage in space complexity (see <a href="https://www.zdnet.com/article/ibm-researchers-demonstrate-the-advantage-that-quantum-computers-have-over-classical-computers/?fbclid=IwAR0KHVoI8W83Kwmeq9XwmuLHknlKeHjxWcvIjrqjH0QUtLZ8kaIWi6z42yk">here</a> for popular article and <a href="https://arxiv.org/abs/2008.06478">here</a> for arXiv preprint).  There appears to be a nice theoretical result here, about the ability to evaluate any symmetric Boolean function with a single qubit in a branching-program-like model.  I’d love to understand that result better.  But to answer the question I received, this is another case where, once you know the protocol, you know both that the experiment can be done <em>and</em> exactly what its result will be (namely, the thing predicted by QM).  So I think the interest is almost entirely in the protocol itself.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5554"><span class="datestr">at July 04, 2021 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/093">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/093">TR21-093 |  Bounded Indistinguishability for Simple Sources | 

	Yuval Filmus, 

	Andrej Bogdanov, 

	Yuval Ishai, 

	Akshayaram Srinivasan, 

	Krishnamoorthy Dinesh, 

	Avi Kaplan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A pair of sources $\mathbf{X},\mathbf{Y}$ over $\{0,1\}^n$ are $k$-indistinguishable if their projections to any $k$ coordinates are identically distributed. Can some $\mathit{AC^0}$ function distinguish between two such sources when $k$ is big, say $k=n^{0.1}$? Braverman's theorem (Commun. ACM 2011) implies a negative answer when $\mathbf{X}$ is uniform, whereas Bogdanov et al. (Crypto 2016) observe that this is not the case in general.

We initiate a systematic study of this question for natural classes of low-complexity sources, including ones that arise in cryptographic applications, obtaining positive results, negative results, and barriers. In particular: 

– There exist $\Omega(\sqrt{n})$-indistinguishable $\mathbf{X},\mathbf{Y}$, samplable by degree $O(\log n)$ polynomial maps (over $\mathbb{F}_2$) and by $\mathit{poly}(n)$-size decision trees, that are $\Omega(1)$-distinguishable by OR.

– There exists a function $f$ such that all $f(d, \epsilon)$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by degree-$d$ polynomial maps are $\epsilon$-indistinguishable by OR for all sufficiently large $n$.  Moreover, $f(1, \epsilon) = \lceil\log(1/\epsilon)\rceil + 1$ and $f(2, \epsilon) = O(\log^{10}(1/\epsilon))$. 

– Extending (weaker versions of) the above negative results to $\mathit{AC^0}$ distinguishers would require  settling a conjecture of Servedio and Viola (ECCC 2012).
Concretely, if every pair of $n^{0.9}$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by linear maps is $\epsilon$-indistinguishable by $\mathit{AC^0}$ circuits, then the binary inner product function can have at most an $\epsilon$-correlation with $\mathit{AC^0}\circ\oplus$ circuits.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/093"><span class="datestr">at July 04, 2021 02:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">ADFOCS 2021: Convex Optimization and Graph Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 26 – August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this year’s edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to … <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and Graph Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/"><span class="datestr">at July 03, 2021 01:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/">ITC 2021: Call for participation (guest post by Benny Applebaum)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The second edition of the recently created conference on <em>Information-Theoretic Cryptography (ITC 2021)</em> will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark Zhandry.</p>



<p>Registration to the conference is free (but required!)</p>



<p>Visit the webpage <a href="https://itcrypto.github.io/2021/index.html" target="_blank" rel="noreferrer noopener">https://itcrypto.github.io/2021/index.html</a> for more information and for the full program.  </p>



<p>Hope to see you there!</p>



<p>– The Organising Committee</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/"><span class="datestr">at July 02, 2021 01:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1542">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1542">Looking back at WOLA’21: Videos available</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The fifth <a href="http://www.local-algorithms.com/">Workshop on Local Algorithms</a> (WOLA’21) took place <a href="https://ptreview.sublinear.info/?p=1517">earlier this month</a>, and the recordings of the invited talks are now <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm">available on YouTube</a>. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule):</p>



<ul><li>James Aspnes (Yale) on <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=2"><em>Population Protocols</em></a></li><li>Uri Stemmer (Ben-Gurion University) on <em><a href="https://www.youtube.com/watch?v=yxOmND76k4M&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=4">The Local Model of Differential Privacy: A Survey</a></em></li><li>Mary Wootters (Stanford University) on <em><a href="https://www.youtube.com/watch?v=oHGvGgSdvAc&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=7">Lifted Codes and Disjoint Repair Groups</a></em></li><li>Christian Sohler (University of Cologne) on <em><a href="https://www.youtube.com/watch?v=DuJQe0pv224&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=6">Property Testing in Planar Graphs</a></em></li><li>Elaine Shi (Carnegie Mellon University) on <em><a href="https://www.youtube.com/watch?v=jq5MuJJLnDk&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=5">Game-Theoretically Secure Protocols Inspired by Blockchains</a></em></li><li>Jelani Nelson (UC Berkeley) on <em><a href="https://www.youtube.com/watch?v=ZQ1ekannlw0&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=3">Optimal bounds for approximate counting</a></em></li></ul>



<p>Thanks again to the speakers and organizers, and looking forward to WOLA’22!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1542"><span class="datestr">at July 02, 2021 04:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/092">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/092">TR21-092 |  A Note on One-way Functions and Sparse Languages | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show equivalence between the existence of one-way
functions and the existence of a \emph{sparse} language that is
hard-on-average w.r.t. some efficiently samplable ``high-entropy''
distribution.
In more detail, the following are equivalent:
  - The existentence of a $S(\cdot)$-sparse language $L$ that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq 4\log n$;
  - The existentence of a $S(\cdot)$-sparse language $L \in
    \NP$, that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq n/3$;
  - The existence of one-way functions.

Our results are insipired by, and generalize, the recent elegant paper by Ilango,
Ren and Santhanam (ECCC'21), which presents similar characterizations for
concrete sparse languages.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/092"><span class="datestr">at July 02, 2021 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4580221543262282386">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html">Intersecting Classes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>If you have two complexity classes that have complete sets, the intersection might not, for example NP ∩ co-NP. The world of total-function classes acts differently.</p><p>Christos Papadimitriou and others defined a <a href="https://en.wikipedia.org/wiki/TFNP">number of classes</a> based on finding solutions to problems where solutions are known to exists for some combinatorial reason. While TFNP, the set of all such problems, might not have complete sets, all the other classes are defined basically based on the complete search problem for the class, such as <a href="https://en.wikipedia.org/wiki/TFNP#PLS">PLS</a>, finding a local minimum. If you have two such classes <b><span style="font-family: Cedarville Cursive;">A</span></b> and <b><span style="font-family: Cedarville Cursive;">B</span></b> with complete search problems A and B define the search problem D as </p><p style="text-align: center;">D(x,y): Find a solution to either A(x) or B(y)</p><p>D is complete for the intersection of <b><span style="font-family: Cedarville Cursive;">A</span></b> and <span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span style="font-family: inherit;">:</span><span style="font-family: inherit;"><span style="font-family: inherit;"> </span>First the </span><span style="font-family: inherit;">pr</span>oblem D is in <b><span style="font-family: Cedarville Cursive;">A</span></b> since you can reduce the problem of finding a solution to either A or B to finding a solution to A. Likewise D is in <span style="font-family: Cedarville Cursive;"><b>B</b></span>.</p><p>Suppose you have a problem Z in <b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span>.<span style="font-family: inherit;"> Then since Z is in </span><b><span style="font-family: Cedarville Cursive;">A</span></b><span style="font-family: inherit;"> and A is complete for </span><span style="font-family: Cedarville Cursive;">A</span><span style="font-family: inherit;">, finding a solution to Z(u) reduces a finding a solution of A(x) where x is easily computed from u. Likewise Z(u) reduces to finding a solution of B(y). So whatever solution D(x,y) gives you, it allows you to find a solution to Z(u). Thus D is complete for </span><b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span>.</span></p><p>Some people nerd out to <a href="https://mars.nasa.gov/technology/helicopter">helicopters on mars</a>. I nerd out to the complexity of complete sets. </p><p>I learned about complete sets of intersections of total function classes from the talk by one of <a href="http://acm-stoc.org/stoc2021/STOCprogram.html">last week's STOC</a> best paper awardees, <a href="https://doi.org/10.1145/3406325.3451052">The Complexity of Gradient Descent</a> by John Fearnley, Paul W. Goldberg, Alexandros Hollender and Rahul Savani. The part above was well known but the paper goes much further.</p><p>Consider <a href="https://blog.computationalcomplexity.org/2005/12/what-is-ppad.html">PPAD</a> famously with Nash Equilibrium as a complete problem and PLS. PPAD ∩ PLS has complete sets by the argument above. But we can go further.</p><p>The class <a href="https://en.wikipedia.org/wiki/TFNP#CLS">CLS</a> is a variation of PLS where you find a local minimum in a continuous domain under some Lipschitz conditions and is known to sit in the intersection of PPAD and PLS. Fearnley et al. look at finding a minimum using gradient descent (the main tool for deep learning), and showing not only is it CLS-compete but complete for PPAD ∩ PLS. As a consequence CLS = PPAD ∩ PLS. Pretty cool stuff.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html"><span class="datestr">at July 01, 2021 02:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://maxoffsky.com/code-blog/flow-lines/">Flow lines</a> (<a href="https://mathstodon.xyz/@11011110/106428733923482392">\(\mathbb{M}\)</a>). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</p>
  </li>
  <li>
    <p><a href="https://daily.jstor.org/the-soap-bubble-trope/">The soap bubble trope</a> (<a href="https://mathstodon.xyz/@11011110/106430498906810187">\(\mathbb{M}\)</a>, <a href="https://3quarksdaily.com/3quarksdaily/2021/06/soap-bubbles.html">via</a>). Soap bubbles as a recurring theme in art, literature, and popular culture, including “the roof of the Munich Olympic Stadium, Glinda the Good Witch, the first viral ad campaign of the late Victorian era, and morose Dutch still-life paintings”.</p>
  </li>
  <li>
    <p><a href="http://gosper.org/homeplate.html">Officially, home plate doesn’t exist</a> (<a href="https://mathstodon.xyz/@esoterica/106435964222477352">\(\mathbb{M}\)</a>). The rules of baseball define it as a 90-45-90-90-45 pentagon with two 12” sides at one of the right angles and a 17” side between the other two, not possible.</p>
  </li>
  <li>
    <p><a href="https://www.natureindex.com/news-blog/microsoft-academic-graph-discontinued-whats-next">Microsoft Academic Graph being discontinued</a> (<a href="https://mathstodon.xyz/@11011110/106438809410223323">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/19/weekend-reads-biotech-ceo-on-leave-after-allegations-on-pubpeer-a-researcher-disavows-his-own-paper-plagiarism-here-there-and-everywhere/">via</a>). I didn’t much use that one but I live in fear that one day Google will do the same thing to Google Scholar, as they have to so many other useful but nonprofitable Google services.</p>
  </li>
  <li>
    <p>My current workflow for preparing technical talk videos (<a href="https://mathstodon.xyz/@11011110/106444988062063872">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p>Use LaTeX+beamer (169 option) to make pdf talk slides</p>
      </li>
      <li>
        <p>For each slide, print open-in-Preview with custom 16x9 zero-margin layout then export to png</p>
      </li>
      <li>
        <p>Write a script and use quicktime to record 1-2 minute voiceover clips</p>
      </li>
      <li>
        <p>Compose slides and audio in iMovie, export to a huge mp4</p>
      </li>
      <li>
        <p>Use Handbrake to convert to reasonably-sized mp4</p>
      </li>
    </ul>

    <p>It works, but is a bit tedious and produces very dry results. The discussion includes suggestion of alternatives.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/7vEgc7cNarI">The points rotated, and the lines danced</a> (<a href="https://mastodon.social/@sarielhp/106439137323288004">\(\mathbb{M}\)</a>). Video illustrating point-line duality by Sariel Har-Peled.</p>
  </li>
  <li>
    <p><a href="https://www.bbc.com/future/article/20210616-how-the-forgotten-tricks-of-letterlocking-shaped-history">Letterlocking</a> (<a href="https://mathstodon.xyz/@11011110/106458633659042049">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27549256">via</a>, <a href="https://en.wikipedia.org/wiki/Letterlocking">see also</a>): the art of folding your letters so intricately that readers will be forced to tear the paper to unfold and read them.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Bill Gasarch summarizes an online debate</a> (<a href="https://mathstodon.xyz/@11011110/106463907058053228">\(\mathbb{M}\)</a>) with Richard DeMillo and Richard Lipton, moderated by Harry Lewis, looking back at the idea of proving programs correct and at <a href="https://doi.org/10.1145/359104.359106">a classic 1979 paper by DeMillo, Lipton, and Perlis</a> arguing that this idea was already problematic.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/X_%2B_Y_sorting">\(X+Y\) sorting</a> (<a href="https://mathstodon.xyz/@11011110/106468019908924208">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. This is on an old open problem in comparison sorting: can you sort pairs of elements from two sets by their sums, faster than unstructured data of the same length? It’s still an active topic of research; see e.g. <a href="https://doi.org/10.1145%2F3285953">Kane, Lovett, and Moran, “Near-optimal linear decision trees for \(k\)-sum and related problems”, <em>JACM</em> 2019</a>.</p>

    <p>It was not easy to persuade the GA reviewer that this article was as accessible as it could be. I have hopes of <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a> also becoming a Good Article but its “Realizability” section is far more advanced.</p>
  </li>
  <li>
    <p>This week I participated in the International Workshop on Graph-Theoretic Concepts in Computer Science, WG (<a href="https://mathstodon.xyz/@11011110/106474212936524737">\(\mathbb{M}\)</a>). The 9-hour time difference made live participation awkward for me, but fortunately prerecorded contributed talks and the three invited talks (Dujmović on product structures, Samotij on independent set numeration, and Bonnet on twin-width) are linked from <a href="https://wg2021.mimuw.edu.pl/program/">the conference program</a>. The proceedings is not yet out but many preprints of papers are also linked.</p>
  </li>
  <li>
    <p><a href="https://theintercept.com/2021/06/23/anming-hu-trial-fbi-china/">“A juror says the FBI owes an apology to University of Tennessee scientist Anming Hu”</a> (<a href="https://mathstodon.xyz/@11011110/106481625501499687">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/25/weekend-reads-the-obesity-wars-and-the-education-of-a-researcher-zombie-research-hijacked-journals/">via</a>) after putting Hu on trial for allegedly hiding ties to China despite his repeated disclosures of those ties and possibly in retaliation for his refusal to become a spy in China for the FBI. Beyond hurting US research both directly and by motivating good people to go elsewhere, this racist witch hunt has provided fuel for Chinese propaganda.</p>
  </li>
  <li>
    <p>The Wikipedia “Book:” namespace for curated collections of articles is being killed off (<a href="https://mathstodon.xyz/@11011110/106484876399456966">\(\mathbb{M}\)</a>) after the software to collate them into pdfs stopped working. I created five of these, and used two as readings for my courses. All five have moved to my user space:</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Fundamental_Data_Structures"><em>Fundamental Data Structures</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Algorithms"><em>Graph Algorithms</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Drawing"><em>Graph Drawing</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Matroid_Theory"><em>Matroid Theory</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Perfect_Graphs"><em>Perfect Graphs</em></a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://danilafe.com/blog/math_rendering_is_wrong/">Math rendering is wrong</a> (<a href="https://mathstodon.xyz/@11011110/106492765314826160">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27656446">via</a>). This blog post from a year ago argues that, for the same reasons one might write a web site in a markup language before compiling it to html, we should also compile LaTeX to html at that time rather than using browser-side scripts (as in most deploys of MathJax or KaTeX) or conversion to images (Wikipedia). It doesn’t present a solution, but is more a call for that solution to be made.</p>
  </li>
  <li>
    <p><a href="https://kleinbottle.com/#AMAZON%20BRAND%20HIJACKING">Amazon stands by and does nothing as Chinese scammers hijack Cliff Stoll’s Klein bottle business to usurp its positive reviews</a> (<a href="https://mathstodon.xyz/@11011110/106500881370367192">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27684807">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/30/linkage.html"><span class="datestr">at June 30, 2021 10:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18933">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/">Scaling and Fame</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Scaling the pandemic is different from scaling the US budget</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/tt/" rel="attachment wp-att-18935"><img width="225" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/tt.png?resize=225%2C126&amp;ssl=1" class="alignright wp-image-18935" height="126" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sydney Morning Herald interview <a href="https://www.smh.com.au/lifestyle/terence-tao-the-mozart-of-maths-20150216-13fwcv.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Terence Tao is now “properly” famous. He was cited earlier this month in the NYT <a href="https://www.nytimes.com/2021/06/17/science/math-numbers-federal-budget-tao.html">science</a> section for help in explaining large numbers. Numbers such as the US federal budget.</p>
<p>
Today we discuss caveats on such explanations, after a riff on the popular explanation of mathematics.</p>
<p>
Regarding that, let us forget Tao’s work on primes in progressions with Ben Green, forget the Erdős discrepancy problem, and forget his almost-resolution of the Collatz conjecture. Forget it all. Better than a headline, he got his name embedded into the NYT article’s URL. This was for something much less deep that he wrote in 2009—as a blogger. </p>
<p>
<em>En passant</em>, we mention that Ken has an event tomorrow (Wed. 6/30) at 3:30 ET. It is a webinar hosted by Marc Rotenberg, who heads the Washington-based Center for AI and Digital Policy (<a href="https://www.caidp.org">CAIDP</a>), on “Chess and AI: The Role of Transparency.” Registration is free at this <a href="https://www.caidp.org/events/chess/">link</a>. One aspect of transparency in Ken’s work is that he writes about his model’s methodology here—as a blogger.</p>
<p>
</p><p></p><h2> Rescaling the Budget </h2><p></p>
<p></p><p>
Tao was referenced for a <a href="https://terrytao.wordpress.com/2009/05/04/the-federal-budget-rescaled/">post</a> he wrote in May 2009 when Barack Obama was working on his first budget as President. The ratio of $100 million to $3 that he used scaled the budget income to about $75,000. </p>
<p>
With Joe Biden engaged in budget deliberations, Aiyana Green and Steven Strogatz wrote the NYT <a href="https://www.human.cornell.edu/pam/news/aiyana_green_nyt">article</a> on explaining the US federal budget. Green is a student at Cornell: she just completed her junior year in the Department of Policy Analysis and Management. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/agss/" rel="attachment wp-att-18936"><img width="355" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/AGSS.png?resize=355%2C200&amp;ssl=1" class="aligncenter wp-image-18936" height="200" /></a></p>
<p></p><p><br />
They updated Tao’s post to scale the income to $100,000. Besides being a round number, this is close to the estimated <a href="https://www.in2013dollars.com/us/inflation/2009">inflation since 2009</a>. Here is the NYT graphic of the numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/bud-2/" rel="attachment wp-att-18938"><img width="545" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/bud.png?resize=545%2C500&amp;ssl=1" class="aligncenter wp-image-18938" height="500" /></a></p>
<p>
</p><p></p><h2> A Scaling Caveat </h2><p></p>
<p></p><p>
It is attractive to apply this scaling trick elsewhere, even to grim subjects like the coronavirus pandemic. But there we find an element that does not scale.</p>
<p>
Suppose we use the same figure of 100,000 to scale down the world’s population. Besides its famous pandemic numbers <a href="https://www.worldometers.info/coronavirus/">pages</a>, Worldometer also keeps a running <a href="https://www.worldometers.info/world-population/">estimate</a> of the total world population, now nearing 7.9 billion. Scaling down means multiplying every ther human number by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\gamma =}" class="latex" /> 0.000012697445274. </p>
<p>
We can think of 100,000 people as a city that is not a metropolis. Scaling down the current pandemic figures, we get:</p>
<ul>
<li>
182,000,000 total cases become <b>2,309</b>. <p></p>
</li><li>
11,496,147 active cases become <b>145</b>. <p></p>
</li><li>
4 million total deaths (the numbers are approaching that millstone as we write) become <b>50</b> deaths. <p></p>
</li><li>
80,346 currently listed in critical or serious condition become <b>exactly one</b>.
</li></ul>
<p>
These numbers are not at all unusual for our size of city if one considers all kinds of illness and mortality. The scaling trick may seem to have reduced the scope of the pandemic, as opposed to statements such as 182 million being over half the US population. Yet in terms of the raw numbers it preserves the proportions.</p>
<p>
What the scaling doesn’t preserve is the proportion of <em>relations</em>. The number of possible binary relations—person <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> knows person <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" />—is quadratic in the number <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> of people. Suppose the number of pairs who know each other is <img src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^2}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> is a small but fixed constant. (Note: we will redo this with something more reasonable below.) If we then scale <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> down to <img src="https://s0.wp.com/latex.php?latex=%7Bn%27+%3D+n%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n' = n\gamma}" class="latex" />, the situation becomes:</p>
<ul>
<li>
If we estimate the relatedness of our city, we get <img src="https://s0.wp.com/latex.php?latex=%7Ban%27%5E2+%3D+an%5E2%5Cgamma%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an'^2 = an^2\gamma^2}" class="latex" />. <p></p>
</li><li>
But if we scaled down the number of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" />-knows-<img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> relations directly we would get <img src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^2\gamma}" class="latex" />, which is substantially bigger by a factor of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{\gamma}}" class="latex" />.
</li></ul>
<p>
Thus what the scaling really underestimates is the impact of how people are affected by their loved ones being among the 182 million (or the worse numbers). The underestimation logic applies to any form <img src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^c}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &gt; 1}" class="latex" />. This is not an issue with the budget because dollar bills don’t feel relatedness—at least not so much, even absent a line-item veto. Now we will make values of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> approaching <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> more reasonable.</p>
<p>
</p><p></p><h2> Small World: Tao and Strogatz Again </h2><p></p>
<p></p><p>
Let’s consider people <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" /> who have <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> degrees of separation in the graph of who-knows-who. Then there are <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> who know each other such that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> knows <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> knows <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" />. If something strikes <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> will feel a deep connection by that impact. The feeling is amplified if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" /> have multiple pairs <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> that make a path. This quantifies the relation that Tao calls “awareness” in his “Lecture Notes 3 FOR 254A” course <a href="https://rjlipton.wpcomstaging.com/feed/LECTURE NOTES 3 FOR 254A">notes</a> (pages 51–57 overall). A fact way more basic than what Tao is actually talking about in those notes is the following:</p>
<blockquote><p><b> </b> <em> The sum over pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(X,Y)}" class="latex" /> of the number of pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(A,B)}" class="latex" /> between them who would be affected equals the sum over edges <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(A,B)}" class="latex" /> of the number of pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(X,Y)}" class="latex" /> they can be struck by: both are equal to the number of paths of length <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> in the graph. </em>
</p></blockquote>
<p></p><p>
That number of paths is what we say is reasonable to model by a function <img src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^c}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgg+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c \gg 1}" class="latex" />. This is the simplest way of approaching why we feel that treating the pandemic like the US budget underestimates its human effect. One can still rebut that other kinds of illness and death have the same scaling properties, but ultimately we are talking about the <em>excess</em> caused by the pandemic—the effect on top of everything else.</p>
<p>
We have not tried to make this analysis become rigorous using more-realistic models of human networks. Perhaps our readers can point us to such analysis. But one inkling of why we expect our point to be borne out comes from a key conclusion of the famous 1998 <a href="https://www.nature.com/articles/30918">paper</a> of Strogatz with Duncan Watts on ‘small-world’ networks: The phase transition from a lattice network with large average distances to a small-world network with small distances takes place in a range where small clusters cannot recognize it happening locally. </p>
<p>
Thus, if our scaling carried the intuitive picture of an isolated city, it would miss the expanding sphere of relations. At the opposite extreme would be taking the union of Monaco and central Venice, which sum to 100,000 people who fan out mightily. There is also the argument that while small-world networks are held tight by “<a href="https://sociology.stanford.edu/sites/g/files/sbiybj9501/f/publications/the_strength_of_weak_ties_and_exch_w-gans.pdf">weak ties</a>,” the shared knowledge of misery is something that most tends to strengthen ties. And of course, our point about relations extends to many other activities impacted by the pandemic.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What should govern the appropriateness of scaling down?</p>
<p>
It should be noted that if we scale down the US to 100,000 people, the numbers are appreciably higher:</p>
<ul>
<li>
34.5 million total cases become <b>10,366</b>. <p></p>
</li><li>
4,928,564 active cases become <b>1,480</b>. <p></p>
</li><li>
620,000 total deaths become <b>186</b> deaths. <p></p>
</li><li>
3,833 currently listed in critical or serious condition still become <b>exactly one</b>.
</li></ul></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/"><span class="datestr">at June 29, 2021 11:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/06/29/trace-reconstruction/">Trace Reconstruction from Complex Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> is an unknown binary string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. We are asked to recover <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> from its <em>traces</em>, and each <em>trace</em> <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is a random subsequence obtained by deleting the bits of <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> independently with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />.</p>



<p>More formally, let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> denote the distribution of traces obtained from string <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" />. For example, when <img src="https://s0.wp.com/latex.php?latex=s+%3D+110&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = 110" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> assigns a probability mass of <img src="https://s0.wp.com/latex.php?latex=1%2F8&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/8" class="latex" /> to each element in the multiset<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Ctext%7Bempty%7D%2C+1%2C+1%2C+0%2C+11%2C+10%2C+10%2C+110%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{\text{empty}, 1, 1, 0, 11, 10, 10, 110\}." class="latex" /><br /></p>



<p>We then ask: what is the smallest number <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> such that we can recover any <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" /> (say, with probability <img src="https://s0.wp.com/latex.php?latex=%5Cge+0.99&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\ge 0.99" class="latex" />) given <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> independent samples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" />?</p>



<p>This <em>trace reconstruction</em> problem was first formulated by <a href="https://people.cs.umass.edu/~mcgregor/papers/04-soda.pdf" target="_blank" rel="noreferrer noopener">Batu-Kannan-Khanna-McGregor</a> in 2004, and their central motivation is from the <a href="https://en.wikipedia.org/wiki/Multiple_sequence_alignment" target="_blank" rel="noreferrer noopener">multiple sequence alignment</a> problem in computational biology. Trace reconstruction is also a fundamental problem related to the <a href="https://en.wikipedia.org/wiki/Deletion_channel" target="_blank" rel="noreferrer noopener">deletion channel</a> in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />. Then the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> tells us the number of independent copies that need to be sent for the receiver to determine the original message.</p>



<p>Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the computational complexity). In 2017, <a href="https://arxiv.org/pdf/1612.03148.pdf" target="_blank" rel="noreferrer noopener">De-O’Donnell-Servedio</a> and <a href="https://arxiv.org/pdf/1612.03599.pdf" target="_blank" rel="noreferrer noopener">Nazarov-Peres</a> independently proved <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Cle+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \le \exp(O(n^{1/3}))" class="latex" />. A very recent breakthrough due to <a href="https://arxiv.org/pdf/2009.03296.pdf" target="_blank" rel="noreferrer noopener">Chase</a> further improved this bound to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Ctilde+O%28n%5E%7B1%2F5%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(\tilde O(n^{1/5}))" class="latex" />, which is still super-polynomial. On the other hand, the best known sample complexity lower bound is merely <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%5COmega%28n%5E%7B3%2F2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde\Omega(n^{3/2})" class="latex" />, proved by <a href="https://arxiv.org/pdf/1905.03031.pdf" target="_blank" rel="noreferrer noopener">Chase</a> in another recent work.</p>



<p>In this blog post, I will explain why this problem is much more non-trivial than it might appear at first glance. I will also give an overview on the work of [DOS17, NP17], which, interestingly, reduces this seemingly combinatorial problem to complex analysis.</p>



<p><strong>Observation: reconstruction <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> TV-distance.</strong> Let us start with a natural first attempt at the problem. Define <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> as the minimum statistical distance between the trace distribution of two different length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> strings:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n+%3D+%5Cmin_%7Bx+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En%7Dd_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n = \min_{x \ne y \in \{0, 1\}^n}d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)." class="latex" /><br /></p>



<p>It is not hard to show that <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n" class="latex" /> bounds <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> on both sides, up to a polynomial factor:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n+%5Clesssim+M%28n%29+%5Clesssim+n%2F%5Cdelta_n%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n \lesssim M(n) \lesssim n/\delta_n^2." class="latex" /><br /></p>



<p class="has-black-color has-text-color">The lower bound holds because any trace reconstruction algorithm must be able to distinguish <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for every pair of different strings <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" />, and this requires <img src="https://s0.wp.com/latex.php?latex=%5COmega%281+%2F+%5Cdelta_n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(1 / \delta_n)" class="latex" /> samples for the minimizer <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> in the definition of <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />. For the upper bound, we note that every pair <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> can be distinguished with an <img src="https://s0.wp.com/latex.php?latex=o%282%5E%7B-n%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="o(2^{-n})" class="latex" /> error probability using <img src="https://s0.wp.com/latex.php?latex=O%28n%2F%5Cdelta_n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n/\delta_n^2)" class="latex" /> samples. We say that string <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> “beats” <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />, if the distinguisher for <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> decides that “<img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" />“. By a union bound, the correct answer <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> “beats” every other string with high probability. We can then obtain a reconstruction algorithm by running the distinguisher for every string pair, and outputting the unique string that “beats” the other <img src="https://s0.wp.com/latex.php?latex=2%5En-1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^n-1" class="latex" /> strings.</p>



<p>Thus, to determine whether the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> is polynomial in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />, it suffices to determine whether <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> scales as <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\mathrm{poly}(n)" class="latex" /> or is much smaller. Unfortunately, it turns out to be highly non-trivial to bound <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />, and even bounding <img src="https://s0.wp.com/latex.php?latex=d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)" class="latex" /> for “simple” <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> can be hard. Imagine that we try to reason about the distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" />: the probability mass that <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> assigns to string <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is proportional to the number of times that <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> appears as a subsequence in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, but this count is already hard to express or control, unless <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> has a very simple pattern. This is roughly where this natural attempt gets stuck.</p>



<p><strong>Distinguishing using estimators.</strong> Now we turn to a different approach that underlies the recent breakthrough on trace reconstruction algorithms. As discussed earlier, we can focus on the problem of distinguishing the case <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for fixed strings <img src="https://s0.wp.com/latex.php?latex=x+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x \ne y \in \{0, 1\}^n" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f: \{0, 1\}^{\le n} \to \mathbb{R}" class="latex" /> be a function defined over all possible traces from a length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> string. We consider the following algorithm for distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" />:<br /><br /><strong>Step 1.</strong> Given traces <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s_1%2C+%5Ctilde+s_2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s_1, \tilde s_2, \ldots" class="latex" />, compute the average of <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s_1%29%2C+f%28%5Ctilde+s_2%29%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s_1), f(\tilde s_2), \ldots" class="latex" /><br /><strong>Step 2.</strong> Output <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> if the average is closer to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+x+%5Csim+%5Cmathcal%7BD%7D_x%7D%5Bf%28%5Ctilde+x%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde x \sim \mathcal{D}_x}[f(\tilde x)]" class="latex" /> than to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+y+%5Csim+%5Cmathcal%7BD%7D_y%7D%5Bf%28%5Ctilde+y%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde y \sim \mathcal{D}_y}[f(\tilde y)]" class="latex" />, and output <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> otherwise.<br /></p>



<p>For the above to succeed, <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> needs to satisfy the following two conditions:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" /> are well-separated under <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, namely <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+x%29%5D+-+%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+y%29%5D%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\mathbb{E}[f(\tilde x)] - \mathbb{E}[f(\tilde y)]| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon &gt; 0" class="latex" />.<br /><strong>(Boundedness)</strong> Expectation of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can be efficiently estimated. This can be guaranteed if for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Cf%28%5Ctilde+s%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|f(\tilde s)| \le B" class="latex" /> holds for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%5Cin+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s \in \{0, 1\}^{\le n}" class="latex" />.<br /></p>



<p>Assuming the above, a standard concentration argument shows that we can distinguish <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> using <img src="https://s0.wp.com/latex.php?latex=O%28%28B%2F%5Cepsilon%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O((B/\epsilon)^2)" class="latex" /> samples, and thus <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Clesssim+n+%5Ccdot+%28B%2F%5Cepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \lesssim n \cdot (B/\epsilon)^2" class="latex" />.</p>



<p>In principle, a near-optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> would be setting <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s)" class="latex" /> to be the indicator of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x%28%5Ctilde+s%29+%5Cge+%5Cmathcal%7BD%7D_y%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x(\tilde s) \ge \mathcal{D}_y(\tilde s)" class="latex" />, which gives boundedness <img src="https://s0.wp.com/latex.php?latex=B+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = 1" class="latex" /> and separation <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29+%5Cge+%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y) \ge \delta_n" class="latex" />. However, as argued above, this optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can still be hard to analyze. Instead, we focus on choosing a simpler function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, which potentially gives a suboptimal <img src="https://s0.wp.com/latex.php?latex=B%2F%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B/\epsilon" class="latex" /> but admits simple analyses.</p>



<p><strong>Sketch of the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> Upper Bound.</strong> The main results of [DOS17] and [NP17] follow from choosing <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> to be a linear function. Given a trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%3D+%5Ctilde+s_0+%5Ctilde+s_1+%5Ctilde+s_2+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s = \tilde s_0 \tilde s_1 \tilde s_2 \cdots" class="latex" />, we consider the following polynomial with coefficients being the bits of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29+%3D+%5Csum_%7Bk%7D%5Ctilde+s_k+z%5Ek+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z) = \sum_{k}\tilde s_k z^k = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots." class="latex" /><br /></p>



<p>For some number <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> to be determined later, we consider the estimator <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29+%3D+%5Ctilde+S%28z%29+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s) = \tilde S(z) = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots" class="latex" />, which is indeed a linear function in the trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />.</p>



<p>At first glance, it might be unclear why we choose the coefficients to be powers of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" />. The following fact justifies this choice by showing that polynomials interact with the deletion channel very nicely: The expectation of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z)" class="latex" /> is exactly the evaluation of the polynomial <img src="https://s0.wp.com/latex.php?latex=S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S(\cdot)" class="latex" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=s_0%2C+s_1%2C+%5Cldots%2C+s_%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s_0, s_1, \ldots, s_{n-1}" class="latex" />, but at a slightly different point.<br /></p>



<p><strong>Fact:</strong> For any string <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+s+%5Csim+%5Cmathcal%7BD%7D_s%7D%5Cleft%5B%5Ctilde+S%28z%29%5Cright%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%5Cleft%28%5Cfrac%7Bz%2B1%7D%7B2%7D%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde s \sim \mathcal{D}_s}\left[\tilde S(z)\right] = \frac{1}{2}S\left(\frac{z+1}{2}\right)." class="latex" /><br /></p>



<p>Equivalently, we have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+S%282z-1%29%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}[\tilde S(2z-1)] = \frac{1}{2}S(z)" class="latex" />, which allows us to rephrase our requirements on the choice of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> as follows:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%7CX%28z%29+-+Y%28z%29%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|X(z) - Y(z)| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> that is not too small.<br /><strong>(Boundedness)</strong> <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)| \le B" class="latex" /> for all possible trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />, for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" /> that is not too large.<br /></p>



<p>After some thought, it is beneficial to choose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> such that both <img src="https://s0.wp.com/latex.php?latex=%7Cz%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|z|" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1|" class="latex" /> are close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />, since this ensures that different bits in the string are assigned weights of similar magnitudes in the polynomials above. These two conditions hold if and only if <img src="https://s0.wp.com/latex.php?latex=z+%5Capprox+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \approx 1" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="512" alt="" src="https://theorydish.files.wordpress.com/2021/06/plan.png?w=1024" class="wp-image-2442" height="230" />The overall plan for distinguishing strings <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />.</figure></div>



<p>The crucial idea in both papers [DOS17, NP17] is to consider the polynomials in the complex plane <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{C}" class="latex" /> instead of on the real line. Fortunately, all the previous discussion still holds for complex numbers, with <img src="https://s0.wp.com/latex.php?latex=%7C%5Ccdot%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\cdot|" class="latex" /> interpreted as modulus instead of absolute value. In [NP17], the authors chose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> from a small arc of the unit circle:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=A_L+%3D+%5C%7Be%5E%7Bi%5Ctheta%7D%3A+%5Ctheta%5Cin%5B-%5Cpi%2FL%2C+%5Cpi%2FL%5D%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L = \{e^{i\theta}: \theta\in[-\pi/L, \pi/L]\}" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="311" alt="" src="https://theorydish.files.wordpress.com/2021/06/a_l.png?w=622" class="wp-image-2445" height="262" />Arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" /> marked by the red box and dotted lines.</figure></div>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" />, it is easy to upper bound <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)|" class="latex" />: it follows from simple calculus that <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C+%5Cle+1+%2B+O%28L%5E%7B-2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1| \le 1 + O(L^{-2})" class="latex" />, and thus for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctilde+S%282z-1%29%5Cright%7C+%5Cle+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D%5Cleft%7C%282z-1%29%5Ek%5Cright%7C+%5Cle+n+%5Ccdot+%5Cleft%5B1+%2B+O%28L%5E%7B-2%7D%29%5Cright%5D%5En+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\left|\tilde S(2z-1)\right| \le \sum_{k=0}^{n-1}\left|(2z-1)^k\right| \le n \cdot \left[1 + O(L^{-2})\right]^n = e^{O(n/L^2)}" class="latex" />.</p>



<p>To lower bound <img src="https://s0.wp.com/latex.php?latex=X%28z%29+-+Y%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X(z) - Y(z)" class="latex" />, note that since both <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> are binary, all the coefficients of <img src="https://s0.wp.com/latex.php?latex=X-Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X-Y" class="latex" /> are in <img src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+0%2C+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{-1, 0, 1\}" class="latex" />. Such polynomials are known as <em>Littlewood polynomials</em>. The separation condition is then reduced to the following claim in complex analysis:</p>



<p class="has-text-align-center"><em><strong>Littlewood polynomials cannot to be too “flat” over a short arc around 1.</strong></em></p>



<p>This is indeed the case:</p>



<p id="lemma-1"><strong>Lemma 1.</strong> (<a href="https://www.jstor.org/stable/pdf/24899666.pdf" target="_blank" rel="noreferrer noopener">[Borwein and Erdélyi, 1997]</a>) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge e^{-O(L)}" class="latex" />.<br /></p>



<p>By <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a>, there exists <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" /> such that the separation condition holds for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = e^{-O(L)}" class="latex" />. Recall that the boundedness holds for <img src="https://s0.wp.com/latex.php?latex=B+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = e^{O(n/L^2)}" class="latex" />. This shows that the sample complexity is upper bounded by <img src="https://s0.wp.com/latex.php?latex=%28B%2F%5Cepsilon%29%5E2+%3D+%5Cexp%28O%28n%2FL%5E2+%2B+L%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(B/\epsilon)^2 = \exp(O(n/L^2 + L))" class="latex" />, which is minimized at <img src="https://s0.wp.com/latex.php?latex=L+%3D+n%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L = n^{1/3}" class="latex" />. This proves the upper bound <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%3D+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) = \exp(O(n^{1/3}))" class="latex" />.</p>



<p><strong>Proof of a weaker lemma.</strong> While the original proof of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> is a bit technical, [NP17] presented a beautiful and much simpler proof of the following weaker result, in which the <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="e^{-O(L)}" class="latex" /> lower bound is replaced by <img src="https://s0.wp.com/latex.php?latex=n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{-O(L)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is the degree of the Littlewood polynomial.</p>



<p><strong>Lemma 2.</strong> ([Lemma 3.1, NP17]) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> of degree <img src="https://s0.wp.com/latex.php?latex=%3C+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt; n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge n^{-O(L)}" class="latex" />.<br /></p>



<p><strong>Proof.</strong> Without loss of generality, we assume that the constant term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />. Suppose otherwise, that the lowest order term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z^m" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=m+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m \ge 1" class="latex" />. We may consider the polynomial <img src="https://s0.wp.com/latex.php?latex=p%28z%29+%2F+z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z) / z^m" class="latex" /> instead.</p>



<p>Let <img src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M = \max_{z \in A_L}|p(z)|" class="latex" /> be the maximum modulus of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> over arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+e%5E%7B2%5Cpi+i%2FL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega = e^{2\pi i/L}" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />-th root of unity. Consider the following polynomial:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=q%28z%29+%3D+%5Cprod_%7Bj%3D0%7D%5E%7BL-1%7Dp%28%5Comega%5Ej+z%29+%3D+p%28z%29+%5Ccdot+p%28%5Comega+z%29+%5Ccdot+%5Ccdots+%5Ccdot+p%28%5Comega%5E%7BL-1%7D+z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z) = \prod_{j=0}^{L-1}p(\omega^j z) = p(z) \cdot p(\omega z) \cdot \cdots \cdot p(\omega^{L-1} z)" class="latex" />.</p>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, at least one point <img src="https://s0.wp.com/latex.php?latex=%5Comega%5Ej+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^j z" class="latex" /> falls into the arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7Cp%28%5Comega%5Ej+z%29%7C+%5Cle+M&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|p(\omega^j z)| \le M" class="latex" />. (See <a href="https://theorydish.blog/feed/#windmill">figure below</a> for a proof by picture.) The moduli of the remaining <img src="https://s0.wp.com/latex.php?latex=L+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L - 1" class="latex" /> factors are trivially bounded by <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. Thus, <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cle+M%5Ccdot+n%5E%7BL-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \le M\cdot n^{L-1}" class="latex" />.</p>



<p>On the other hand, we note <img src="https://s0.wp.com/latex.php?latex=q%280%29+%3D+%5Bp%280%29%5D%5EL+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(0) = [p(0)]^L = 1" class="latex" />. The <a href="https://en.wikipedia.org/wiki/Maximum_modulus_principle" target="_blank" rel="noreferrer noopener">maximum modulus principle</a> implies that for some <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, we have <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \ge 1" class="latex" />. Therefore, we must have <img src="https://s0.wp.com/latex.php?latex=M+%5Ccdot+n%5E%7BL+-+1%7D+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \cdot n^{L - 1} \ge 1" class="latex" />, which implies <img src="https://s0.wp.com/latex.php?latex=M+%5Cge+n%5E%7B-%28L+-+1%29%7D+%3D+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \ge n^{-(L - 1)} = n^{-O(L)}" class="latex" /> and completes the proof. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<div class="wp-block-image" id="windmill"><figure class="aligncenter size-large is-resized"><img width="462" alt="" src="https://theorydish.files.wordpress.com/2021/06/example.png?w=924" class="wp-image-2449" height="261" />Points involved in the definition of <img src="https://s0.wp.com/latex.php?latex=q%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=L%3D4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L=4" class="latex" />.<br />In this case, <img src="https://s0.wp.com/latex.php?latex=%5Comega%5E3+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^3 z" class="latex" /> is inside arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, which is marked by the red lines.</figure></div>



<p><strong>Beyond linear estimators?</strong> The work of [DOS17, NP17] not only proved the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> upper bound, but also showed that this is the best sample complexity we can get from linear estimator <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />. The recent work of [Cha20] goes beyond these linear estimators by taking the higher moments of the trace into account. The idea turns out to be a natural one: consider the “<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-grams” (i.e., length-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> substrings) of the string for some <img src="https://s0.wp.com/latex.php?latex=k+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k &gt; 1" class="latex" />. In more detail, we fix some string <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5C%7B0%2C+1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w \in \{0, 1\}^k" class="latex" />, and consider the binary polynomial with coefficients corresponding to the positions at which <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> appears as a (contiguous) substring. The key observation is that there exists a choice of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> such that the resulting polynomial is sparse (in the sense that the degrees of the nonzero monomials are far away). The technical part of [Cha20] is then devoted to proving an analogue of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> that is specialized to these “sparse” Littlewood polynomials.</p>



<p><strong>Acknowledgments.</strong> I would like to thank Moses Charikar, Li-Yang Tan and Gregory Valiant for being on my quals committee and for helpful discussions about this problem.</p></div>







<p class="date">
by Mingda Qiao <a href="https://theorydish.blog/2021/06/29/trace-reconstruction/"><span class="datestr">at June 29, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html">Greedy orderings with transposition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I’m a big fan of using <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a> to model vertex-ordering processes in graphs such as the construction of <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological orderings</a> in <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> and perfect elimination orderings in <a href="https://en.wikipedia.org/wiki/Chordal_graph">chordal graphs</a>. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</p>

<p>But there are some greedy vertex-ordering processes that do not form antimatroids, even though they do have the same inability to make mistakes. Two of these are the dismantling orders of <a href="https://en.wikipedia.org/wiki/Cop-win_graph">cop-win graphs</a> and the reverse construction orders of <a href="https://en.wikipedia.org/wiki/Distance-hereditary_graph">distance-hereditary graphs</a>. I wrote about cop-win graphs <a href="https://11011110.github.io/blog/2016/08/18/game-of-cop.html">here in 2016</a>; a graph is cop-win if a cop can always land on the same vertex as a robber when they take turns either moving from a vertex to a neighboring vertex or staying put. In distance-hereditary graphs, all induced subgraphs have the same distances; <a href="https://11011110.github.io/blog/2005/10/11/delta-confluent-drawing-paper.html">these graphs also have nice confluent drawings</a>. Both of these classes of graphs can be recognized by greedy algorithms that remove one vertex at a time until either getting stuck (for graphs not in the class) or succeeding by reaching a single-vertex graph. But although the conditions for removing vertices in these algorithms are local, they are not antimatroidal.</p>

<h1 id="an-example-graph">An example graph</h1>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/Ptolemaic.svg" alt="A six-vertex graph with six vertices A, B, C, D, E, and F, and seven edges AD, BC, BD, BE, CE, and EF" /></p>

<p>The graph shown above happens to be chordal, distance-hereditary, and cop-win, making it a convenient example both of how to order the vertices of these graph classes and of why the distance-hereditary and cop-win orderings are not antimatroidal.</p>

<ul>
  <li>
    <p>In chordal graphs, a perfect elimination ordering can be constructed by repeatedly removing <em>simplicial vertices</em>, vertices whose neighborhoods form a clique. For an elimination ordering of the example graph, vertices \(A\), \(C\), and \(F\) are already available to be listed: \(A\) and \(F\) only have one neighbor (automatically a clique), and \(C\) has two neighbors forming a two-vertex clique. The other vertices will become available later in the removal process, once enough of their neighbors have been removed and all remaining vertices become adjacent. For instance, once \(A\) has been removed, \(D\) will become available, and once \(C\) has been removed, \(B\) will become available. Once this removal process makes a vertex simplicial, it remains simplicial until removed, so elimination orderings form an antimatroid.</p>
  </li>
  <li>
    <p>Distance-hereditary graphs can be constructed from a single vertex by repeatedly adding leaf vertices (with one neighbor connecting to previous vertices) or twins (duplicates of previous vertices). Reversing this process, these graphs can be deconstructed by repeatedly removing leaves or twins. The graph above has no twins, but \(A\) and \(F\) are leaves, and can be removed immediately. If \(A\) is removed, \(C\) and \(D\) become false twins (not adjacent to each other), and either of them can be removed. Similarly, if \(F\) is removed, \(B\) and \(E\) become true twins (adjacent to each other), after which one can be removed, but not both: after removing \(F\) and \(B\), \(E\) is no longer a leaf or a twin (because its twin, \(B\), has gone), and must remain until later steps. Because the removal orders can start \(FB\) or \(FE\) but not \(FBE\), they are not described by an antimatroid.</p>
  </li>
  <li>
    <p>Similarly, cop-win graphs can be dismantled by repeatedly removing a vertex \(v\) that is dominated by another vertex \(w\), meaning that the neighborhood of \(v\) (including \(v\) itself) is a subset of the neighborhoood of \(w\). In the given graph, \(A\) is dominated by \(D\), \(B\) is dominated by \(E\), \(C\) is dominated by both \(B\) and \(E\), and \(F\) is dominated by \(E\). So any one of these four dominated vertices starts out as removable. But if we remove first \(F\) and then \(E\) (dominated by \(B\) after the removal of \(F\)) we can no longer remove \(B\). So because the ability to be removed can go away before the removal happens, we do not have an antimatroid.</p>
  </li>
</ul>

<p>There’s another complication here as well. For both distance-hereditary graphs and cop-win graphs, removing leaves and twins or dominated vertices will never eliminate all graph vertices. Instead, both removal processes stop when we reach a single remaining vertex. But this is different from antimatroids, where all elements must be included in all orderings.</p>

<h1 id="some-axiomatics">Some axiomatics</h1>

<p>To understand why greedy orderings still work in these cases, I think it’s helpful to start by understanding why they work for antimatroids, as a general class of structures. The following is not quite the usual system of axioms for antimatroids, but they can be defined as non-empty formal languages (that is, sets of strings over a finite alphabet) with the following properties:</p>

<dl>
  <dt>Hereditary:</dt>
  <dd>
    <p>Every prefix of a string in the language is also in the language. Thinking about this in the other direction: every string in the language can be built up by adding one character at a time, starting from the empty string, at all times remaining within the language.</p>
  </dd>
  <dt>Normal:</dt>
  <dd>
    <p>Every character occurs at most once in any string in the language. An element can only be added to the sequence of elements once. Because we are assuming the alphabet to be finite, this means that the language itself is also finite.</p>
  </dd>
  <dt>Oblivious:</dt>
  <dd>
    <p>If \(S\) and \(T\) are permutations of each other in the language, then for every character \(x\), \(Sx\) is in the language if and only if \(Tx\) is in the language. This means that what can be added next depends only on the set of characters that have been added already, forgetting about the order in which they were added.</p>
  </dd>
  <dt>Anti-exchange:</dt>
  <dd>
    <p>If \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, then so does \(Sxy\). Adding \(x\) doesn’t prevent \(y\) from being added later. This is the key property of an antimatroid and the one that is violated by the distance-hereditary and cop-win orderings.</p>
  </dd>
</dl>

<p>Usually a stronger version of obliviousness is used, stating that when \(S\) and a permutation of \(Sx\) are in the language, then \(Sx\) is in the language, but it’s not immediately obvious why this should be true for the vertex-ordering processes I’m considering here, so I’ve gone with a weaker version. We’ll see later that the stronger version is implied by a combination of this and other properties. It is standard to also require that all characters be usable, but I haven’t done this, because I want to understand the behavior of antimatroidal greedy algorithms on graphs not in the given graph class, for which they get stuck before ordering the whole graph. But this is not important, because one could instead redefine the alphabet to consist only of usable characters.</p>

<p>Given a language that satisfies all of these properties, one can show that all non-extendable strings are equally long and use the same alphabet as each other. For, if we have two different non-extendable strings \(S\) and \(T\), we can morph \(S\) into \(T\) one step at a time, never shortening it or changing its character set, by finding the first position at which \(S\) and \(T\) differ, finding the character \(t\) that \(T\) has at that position (necessarily also used later in \(S\) because it was usable at that position and would have remained usable until it was used), and repeatedly using the anti-exchange axiom to swap \(t\) for the previous character in \(S\) until it has been swapped into a match with \(T\). The oblivious property ensures that the part of the string after the swap remains valid. So \(S\) cannot be shorter than or miss any characters from \(T\), nor vice versa.</p>

<p>Instead of the anti-exchange axiom, the distance-hereditary and cop-win orderings satisfy a weaker property, based on the notion of swapping two characters.</p>

<dl>
  <dt>Transposition:</dt>
  <dd>
    <p>Suppose \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, but \(Sxy\) does not. Then for all \(T\) not containing \(x\) or \(y\), \(SxT\) is in the language if and only if \(SyT\) is also in the language, and \(SxTy\) is in the language if and only if \(SyTx\) is also in the language.</p>
  </dd>
</dl>

<p>The last part of the transposition property, about \(SxTy\) and \(SyTx\), is only included because we used a weak version of obliviousness; if we used the stronger version, it would follow from the earlier part of the transposition property.</p>

<h1 id="cop-win-and-distance-hereditary-orderings-have-the-transposition-property">Cop-win and distance-hereditary orderings have the transposition property</h1>

<p>Let’s suppose we’re trying to dismantle a cop-win graph by repeatedly removing dominated vertices, in the hope of getting down to a single vertex. After we’ve removed some vertices already in a sequence \(S\), two vertices \(x\) and \(y\) might become included in the set of dominated vertices. This can happen in several different ways:</p>

<ul>
  <li>It might be the case that \(x\) is dominated by a vertex that is not \(y\), and that \(y\) is dominated by a vertex that is not \(x\). When this happens, they can be removed in either order: removing one won’t change the fact that the other is dominated by whatever other vertex dominated it already.</li>
  <li>It might be the case that \(x\) is dominated by \(y\), and \(y\) is dominated by a third vertex \(z\). But then \(x\) is also dominated by \(z\), and again they can be removed in either order. When one is removed, the other is still dominated by \(z\).</li>
  <li>The only remaining case is that \(x\) and \(y\) are each dominated only by the other of these two vertices. In this case, we can remove one or the other but not both. But if \(x\) and \(y\) dominate each other, they have the same neighbors (they are twins), and there is a symmetry of the remaining subgraph swapping \(x\) and \(y\). So in this case, any continuation of the removal sequence \(S\) can have \(x\) replaced by \(y\) and vice versa, and still be a valid continuation. This is exactly what the transposition property states.</li>
</ul>

<p>The argument for distance-hereditary orderings is even easier. If \(x\) and \(y\) are not twins of each other, then removing one won’t affect the removability of the other. If they are twins, then they are symmetric and any continuation of the removal sequence can exchange \(x\) for \(y\) without changing its validity.</p>

<h1 id="orderings-with-transposition-form-greedoids">Orderings with transposition form greedoids</h1>

<p>If we can’t obtain an antimatroid from the cop-win or distance-hereditary graphs, we might at least hope for a more general structure, a greedoid. The key property of greedoids (viewed as hereditary normal languages rather than their usual definition as set systems) is the following axiom:</p>

<dl>
  <dt>Exchange:</dt>
  <dd>If \(S\) is a longer string in the language of a greedoid, and \(T\) is a longer string in the same language, then there is a character \(x\) in \(T\) such that \(Sx\) is a string in the language.</dd>
</dl>

<p>This implies that all maximal strings in the language have the same length, and in the cop-win and distance-hereditary cases it implies that all greedy dismantling or deconstruction sequences reach a single vertex without getting stuck along the way. The greedoid exchange property also immediately implies the strong version of the obliviousness property, by plugging in a permutation of \(Sx\) as the string \(T\) in the exchange property.</p>

<p>To prove that indistinguishability implies the exchange property, let \(S\) be any string in an indistinguishable (hereditary normal oblivious) language, and let \(T\) be a longer string, which we might as well assume to be maximal. If \(S\) is a prefix of \(T\), then obviously we can satisfy the exchange property: just take the prefix of \(T\) that has one more character.</p>

<p>Otherwise, I claim that we can replace \(T\) by a different string \(T'\) of the same length that agrees with \(S\) for more positions. To find \(T'\), let \(y\) be the first character of \(S\) that differs from the corresponding character of \(T\); this must exist by the assumption that \(S\) is not a prefix of \(T\). Obviously, at the position of \(y\) in \(S\), we could have added it to \(T\), but instead some other character was chosen. Maybe, \(y\) remained available to be chosen throughout the remaining positions of \(T\), until it actually was chosen. If so, just as in the antimatroid case, we could repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position. Alternatively, maybe at some point during the construction of sequence \(T\), we chose a character \(z\) causing \(y\) to become unavailable. In this case, by the transposition property, we can swap \(y\) for \(z\) in \(T\) and then as before repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position.</p>

<p>By repeatedly replacing \(T\) by equally long strings that agree with more and more positions of \(S\), we eventually reach a string for which \(S\) is a prefix, and can append one more character. This construction of \(T'\) from \(T\) does not include any new characters that weren’t already in \(S\) or \(T\), so the appended character must have come from \(T\), proving the exchange axiom.</p>

<p>The use of the transposition property to form greedoids is standard; these greedoids are called transposition greedoids, and are described e.g. by Björner and Ziegler in their introduction to greedoids in the book <em>Matroid Applications</em>. Another <a href="https://doi.org/10.1007/978-3-642-58191-5_10">book chapter on transposition greedoids</a>, in the book <em>Greedoids</em> by Korte, Schrader, and Lovász, includes another graph-theoretic example where the elements are edges of series-parallel graphs. The part that appears to be less standard is the use of this property to explain the ability of greedy algorithms to recognize cop-win and distance-hereditary graphs. I looked, but was unable to find publications observing that these two classes of graphs lead to greedoids or transposition greedoids, despite some suspiciously-similar terminology (“twins”, “dismantling”) on both sides. If anyone knows of such publications, I’d appreciate hearing of them, so that I could add this connection to their Wikipedia articles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106495619434427598">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html"><span class="datestr">at June 29, 2021 12:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/091">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/091">TR21-091 |  Expander Random Walks: The General Case and Limitations | 

	Gil Cohen, 

	Dor Minzer, 

	Shir Peleg, 

	Aaron Potechin, 

	Amnon Ta-Shma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Cohen, Peri and Ta-Shma (STOC'21) considered the following question: Assume the vertices of an expander graph are labelled by $\pm 1$. What "test" functions $f : \{\pm 1\}^t \to \{\pm1 \}$ can or cannot distinguish $t$ independent samples from those obtained by a random walk? [CPTS'21] considered only balanced labelling, and proved that all symmetric functions are fooled by random walks on expanders with constant spectral gap. Furthermore, it was shown that functions computable by $\mathbf{AC}^0$ circuits are fooled by expanders with vanishing spectral expansion. 

We continue the study of this question and, in particular, resolve all open problems raised by [CPTS'21]. First, we generalize the result to all labelling, not merely balanced. In doing so, we improve the known bound for symmetric functions and prove that the bound we obtain is optimal (up to a multiplicative constant). Furthermore, we prove that a random walk on expanders with constant spectral gap does not fool $\mathbf{AC}^0$. In fact, we prove that the bound obtained by [CPTS'21] for $\mathbf{AC}^0$ circuits is optimal up to a polynomial factor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/091"><span class="datestr">at June 29, 2021 07:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
