<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 04, 2020 02:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02408">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02408">Dynamic Longest Common Substring in Polylogarithmic Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charalampopoulos:Panagiotis.html">Panagiotis Charalampopoulos</a>, Paweł Gawrychowski, Karol Pokorski <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02408">PDF</a><br /><b>Abstract: </b>The longest common substring problem consists in finding a longest string
that appears as a (contiguous) substring of two input strings. We consider the
dynamic variant of this problem, in which we are to maintain two dynamic
strings $S$ and $T$, each of length at most $n$, that undergo substitutions of
letters, in order to be able to return a longest common substring after each
substitution. Recently, Amir et al. [ESA 2019] presented a solution for this
problem that needs only $\tilde{\mathcal{O}}(n^{2/3})$ time per update. This
brought the challenge of determining whether there exists a faster solution
with polylogarithmic update time, or (as is the case for other dynamic
problems), we should expect a polynomial (conditional) lower bound. We answer
this question by designing a significantly faster algorithm that processes each
substitution in amortized $\log^{\mathcal{O}(1)} n$ time with high probability.
Our solution relies on exploiting the local consistency of the parsing of a
collection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on
maintaining two dynamic trees with labeled bicolored leaves, so that after each
update we can report a pair of nodes, one from each tree, of maximum combined
weight, which have at least one common leaf-descendant of each color. We
complement this with a lower bound of $\Omega(\log n/ \log\log n)$ for the
update time of any polynomial-size data structure that maintains the LCS of two
dynamic strings, and the same lower bound for the update time of any data
structure of size $\tilde{\mathcal{O}}(n)$ that maintains the LCS of a static
and a dynamic string. Both lower bounds hold even allowing amortization and
randomization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02408"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02399">ExKMC: Expanding Explainable $k$-Means Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frost:Nave.html">Nave Frost</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Michal.html">Michal Moshkovitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02399">PDF</a><br /><b>Abstract: </b>Despite the popularity of explainable AI, there is limited work on effective
methods for unsupervised learning. We study algorithms for $k$-means
clustering, focusing on a trade-off between explainability and accuracy.
Following prior work, we use a small decision tree to partition a dataset into
$k$ clusters. This enables us to explain each cluster assignment by a short
sequence of single-feature thresholds. While larger trees produce more accurate
clusterings, they also require more complex explanations. To allow flexibility,
we develop a new explainable $k$-means clustering algorithm, ExKMC, that takes
an additional parameter $k' \geq k$ and outputs a decision tree with $k'$
leaves. We use a new surrogate cost to efficiently expand the tree and to label
the leaves with one of $k$ clusters. We prove that as $k'$ increases, the
surrogate cost is non-increasing, and hence, we trade explainability for
accuracy. Empirically, we validate that ExKMC produces a low cost clustering,
outperforming both standard decision tree methods and other algorithms for
explainable clustering. Implementation of ExKMC available at
https://github.com/navefr/ExKMC.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02399"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02374">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02374">On tensor rank and commuting matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koiran:Pascal.html">Pascal Koiran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02374">PDF</a><br /><b>Abstract: </b>Obtaining superlinear lower bounds on tensor rank is a major open problem in
complexity theory. In this paper we propose a generalization of the approach
used by Strassen in the proof of his $3n/2$ border rank lower bound. Our
approach revolves around a problem on commuting matrices:
</p>
<p>Given matrices $Z_1,...,Z_p$ of size $n$ and an integer $r&gt;n$, are there
commuting matrices $Z'_1,...,Z'_p$ of size $r$ such that every $Z_k$ is
embedded as a submatrix in the top-left corner of $Z'_k$?
</p>
<p>As one of our main results, we show that this question always has a positive
answer for $r$ larger than $rank(T)+n$, where $T$ denotes the tensor with
slices $Z_1,..,Z_p$. Taking the contrapositive, if one can show for some
specific matrices $Z_1,...,Z_p$ and a specific integer $r$ that this question
has a negative answer, this yields the lower bound $rank(T) &gt; r-n$. There is a
little bit of slack in the above $rank(T)+n$ bound, but we also provide a
number of exact characterizations of tensor rank and symmetric rank, for
ordinary and symmetric tensors, over the fields of real and complex numbers.
Each of these characterizations points to a corresponding variation on the
above approach. In order to explain how Strassen's theorem fits within this
framework we also provide a self-contained proof of his lower bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02374"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02249">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02249">Complete Characterization of Incorrect Orthology Assignments in Best Match Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>David Schaller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gei=szlig=:Manuela.html">Manuela Geiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02249">PDF</a><br /><b>Abstract: </b>Genome-scale orthology assignments are usually based on reciprocal best
matches. In the absence of horizontal gene transfer (HGT), every pair of
orthologs forms a reciprocal best match. Incorrect orthology assignments
therefore are always false positives in the Reciprocal Best Match Graph. We
consider duplication/loss scenarios and characterize unambiguous false-positive
(u-fp) orthology assignments, that is, edges in the Best Match Graphs (BMGs)
that cannot correspond to orthologs for any gene tree that explains the BMG. We
characterize u-fp edges in terms of subgraphs of the BMG and show that, given a
BMG, there is a unique "augmented tree" that explains the BMG and identifies
all u-fp edges in terms of overlapping sets of species in certain subtrees. The
augmented tree can be constructed as a refinement of the unique least resolved
tree of the BMG in polynomial time. Removal of the u-fp edges from the
reciprocal best matches results in a unique orthology assignment.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02249"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02219">LCP-Aware Parallel String Sorting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ellert:Jonas.html">Jonas Ellert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Johannes.html">Johannes Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sitchinava:Nodari.html">Nodari Sitchinava</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02219">PDF</a><br /><b>Abstract: </b>When lexicographically sorting strings, it is not always necessary to inspect
all symbols. For example, the lexicographical rank of "europar" amongst the
strings "eureka", "eurasia", and "excells" only depends on its so called
relevant prefix "euro". The distinguishing prefix size $D$ of a set of strings
is the number of symbols that actually need to be inspected to establish the
lexicographical ordering of all strings. Efficient string sorters should be
$D$-aware, i.e. their complexity should depend on $D$ rather than on the total
number $N$ of all symbols in all strings. While there are many $D$-aware
sorters in the sequential setting, there appear to be no such results in the
PRAM model. We propose a framework yielding a $D$-aware modification of any
existing PRAM string sorter. The derived algorithms are work-optimal with
respect to their original counterpart: If the original algorithm requires
$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time
increases only by a small factor that is logarithmic in the length of the
longest relevant prefix. Our framework universally works for deterministic and
randomized algorithms in all variations of the PRAM model, such that future
improvements in ($D$-unaware) parallel string sorting will directly result in
improvements in $D$-aware parallel string sorting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02219"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02134">Computing Palindromic Trees for a Sliding Window and Its Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mieno:Takuya.html">Takuya Mieno</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watanabe:Kiichi.html">Kiichi Watanabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02134">PDF</a><br /><b>Abstract: </b>The palindromic tree (a.k.a. eertree) for a string $S$ of length $n$ is a
tree-like data structure that represents the set of all distinct palindromic
substrings of $S$, using $O(n)$ space [Rubinchik and Shur, 2018]. It is known
that, when $S$ is over an alphabet of size $\sigma$ and is given in an online
manner, then the palindromic tree of $S$ can be constructed in $O(n\log\sigma)$
time with $O(n)$ space. In this paper, we consider the sliding window version
of the problem: For a fixed window length $d$, we propose two algorithms to
maintain the palindromic tree of size $O(d)$ for every sliding window
$S[i..i+d-1]$ over $S$, one running in $O(n\log\sigma')$ time with $O(d)$ space
where $\sigma' \leq d$ is the maximum number of distinct characters in the
windows, and the other running in $O(n + d\sigma)$ time with $d\sigma + O(d)$
space. We also present applications of our algorithms for computing minimal
unique palindromic substrings (MUPS) and for computing minimal absent
palindromic words (MAPW) for a sliding window.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02134"><span class="datestr">at June 04, 2020 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.02027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.02027">Sampling-Based Motion Planning on Manifold Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Englert:Peter.html">Peter Englert</a>, Isabel M. Rayas Fernández, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramachandran:Ragesh_K=.html">Ragesh K. Ramachandran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sukhatme:Gaurav_S=.html">Gaurav S. Sukhatme</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.02027">PDF</a><br /><b>Abstract: </b>We address the problem of planning robot motions in constrained configuration
spaces where the constraints change throughout the motion. A novel problem
formulation is introduced that describes a task as a sequence of intersecting
manifolds, which the robot needs to traverse in order to solve the task. We
specify a class of sequential motion planning problems that fulfill a
particular property of the change in the free configuration space when
transitioning between manifolds. For this problem class, a sequential motion
planning algorithm SMP is developed that searches for optimal intersection
points between manifolds by using RRT* in an inner loop with a novel steering
strategy. We provide a theoretical analysis regarding its probabilistic
completeness and demonstrate its performance on kinematic planning problems
where the constraints are represented as geometric primitives. Further, we show
its capabilities on solving multi-robot object transportation tasks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.02027"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01975">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01975">Sparsification of Balanced Directed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Yu.html">Yu Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Kevin.html">Kevin Sun</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01975">PDF</a><br /><b>Abstract: </b>Sparsification, where the cut values of an input graph are approximately
preserved by a sparse graph (called a cut sparsifier) or a succinct data
structure (called a cut sketch), has been an influential tool in graph
algorithms. But, this tool is restricted to undirected graphs, because some
directed graphs are known to not admit sparsification. Such examples, however,
are structurally very dissimilar to undirected graphs in that they exhibit
highly unbalanced cuts. This motivates us to ask: can we sparsify a balanced
digraph?
</p>
<p>To make this question concrete, we define balance $\beta$ of a digraph as the
maximum ratio of the cut value in the two directions (Ene et al., STOC 2016).
We show the following results:
</p>
<p>For-All Sparsification: If all cut values need to be simultaneously preserved
(cf. Bencz\'ur and Karger, STOC 1996), then we show that the size of the
sparsifier (or even cut sketch) must scale linearly with $\beta$. The upper
bound is a simple extension of sparsification of undirected graphs (formally
stated recently in Ikeda and Tanigawa (WAOA 2018)), so our main contribution
here is to show a matching lower bound.
</p>
<p>For-Each Sparsification: If each cut value needs to be individually preserved
(Andoni et al., ITCS 2016), then the situation is more interesting. Here, we
give a cut sketch whose size scales with $\sqrt{\beta}$, thereby beating the
linear lower bound above. We also show that this result is tight by exhibiting
a matching lower bound of $\sqrt{\beta}$ on "for-each" cut sketches.
</p>
<p>Our upper bounds work for general weighted graphs, while the lower bounds
even hold for unweighted graphs with no parallel edges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01975"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01944">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01944">Designing Differentially Private Estimators in High Dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dhar:Aditya.html">Aditya Dhar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jason.html">Jason Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01944">PDF</a><br /><b>Abstract: </b>We study differentially private mean estimation in a high-dimensional
setting. Existing differential privacy techniques applied to large dimensions
lead to computationally intractable problems or estimators with excessive
privacy loss. Recent work in high-dimensional robust statistics has identified
computationally tractable mean estimation algorithms with asymptotic
dimension-independent error guarantees. We incorporate these results to develop
a strict bound on the global sensitivity of the robust mean estimator. This
yields a computationally tractable algorithm for differentially private mean
estimation in high dimensions with dimension-independent privacy loss. Finally,
we show on synthetic data that our algorithm significantly outperforms classic
differential privacy methods, overcoming barriers to high-dimensional
differential privacy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01944"><span class="datestr">at June 04, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01933">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01933">Hierarchical Clustering: a 0.585 Revenue Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alon:Noga.html">Noga Alon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Azar:Yossi.html">Yossi Azar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vainstein:Danny.html">Danny Vainstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01933">PDF</a><br /><b>Abstract: </b>Hierarchical Clustering trees have been widely accepted as a useful form of
clustering data, resulting in a prevalence of adopting fields including
phylogenetics, image analysis, bioinformatics and more. Recently, Dasgupta
(STOC 16') initiated the analysis of these types of algorithms through the
lenses of approximation. Later, the dual problem was considered by Moseley and
Wang (NIPS 17') dubbing it the Revenue goal function. In this problem, given a
nonnegative weight $w_{ij}$ for each pair $i,j \in [n]=\{1,2, \ldots ,n\}$, the
objective is to find a tree $T$ whose set of leaves is $[n]$ that maximizes the
function $\sum_{i&lt;j \in [n]} w_{ij} (n -|T_{ij}|)$, where $|T_{ij}|$ is the
number of leaves in the subtree rooted at the least common ancestor of $i$ and
$j$.
</p>
<p>In our work we consider the revenue goal function and prove the following
results. First, we prove the existence of a bisection (i.e., a tree of depth 2
in which the root has two children, each being a parent of $n/2$ leaves) which
approximates the general optimal tree solution up to a factor of $\frac{1}{2}$
(which is tight). Second, we apply this result in order to prove a
$\frac{2}{3}p$ approximation for the general revenue problem, where $p$ is
defined as the approximation ratio of the Max-Uncut Bisection problem. Since
$p$ is known to be at least 0.8776 (Wu et al., 2015, Austrin et al., 2016), we
get a 0.585 approximation algorithm for the revenue problem. This improves a
sequence of earlier results which culminated in an 0.4246-approximation
guarantee (Ahmadian et al., 2019).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01933"><span class="datestr">at June 04, 2020 01:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01903">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01903">On a Class of Constrained Synchronization Problems in NP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoffmann:Stefan.html">Stefan Hoffmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01903">PDF</a><br /><b>Abstract: </b>The class of known constraint automata for which the constrained
synchronization problem is in NP all admit a special form. In this work, we
take a closer look at them. We characterize a wider class of constraint
automata that give constrained synchronization problems in NP, which
encompasses all known problems in NP. We call these automata polycyclic
automata. The corresponding language class of polycyclic languages is
introduced. We show various characterizations and closure properties for this
new language class. We then give a criterion for NP-completeness and a
criterion for polynomial time solvability for polycyclic constraint languages.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01903"><span class="datestr">at June 04, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/084">TR20-084 |  Rate Amplification and Query-Efficient Distance Amplification for Locally Decodable Codes | 

	Gil Cohen, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In a seminal work, Kopparty et al. (J. ACM 2017) constructed asymptotically good $n$-bit locally decodable codes (LDC) with $2^{\widetilde{O}(\sqrt{\log{n}})}$ queries. A key ingredient in their construction is a distance amplification procedure by Alon et al. (FOCS 1995) which amplifies the distance $\delta$ of a code to a constant at a  $\mathrm{poly}(1/\delta)$ multiplicative cost in query complexity. Given the pivotal role of the AEL distance amplification procedure in the state-of-the-art constructions of LDC as well as LCC and LTC, one is prompt to ask whether the $\mathrm{poly}(1/\delta)$ factor in query complexity can be reduced.

Our first contribution is a significantly improved distance amplification procedure for LDC. The cost is reduced from $\mathrm{poly}(1/\delta)$ to, roughly, the query complexity of a length $1/\delta$ asymptotically good LDC. We derive several applications, one of which allows us to convert a $q$-query LDC with extremely poor distance $\delta = n^{-(1-o(1))}$ to a constant distance LDC with $q^{\mathrm{poly}(\log\log{n})}$ queries. As another example, amplifying distance $\delta = 2^{-(\log{n})^\alpha}$, for any constant $\alpha &lt; 1$, will require $q^{O(\log\log\log{n})}$ queries using our procedure.

Motivated by the fruitfulness of distance amplification,  we investigate the natural question of rate amplification. Our second contribution is identifying a rich and natural class of LDC and devise two (incomparable) rate amplification procedures for it. These, however, deteriorate the distance, at which point a distance amplification procedure is invoked. Combined, the procedures convert any $q$-query LDC in our class, having rate $\rho$ and, say, constant distance, to an asymptotically good LDC with $q^{\mathrm{poly}(1/\rho)}$ queries.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/084"><span class="datestr">at June 03, 2020 01:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6696939180299352624">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html">How to handle grades during the Pandemic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In March many Colleges sent students home and the rest of the semester was online. This was quite disruptive for the students. Schools, quite reasonably, wanted to make it less traumatic for students.<br />
<br />
So what to do about grades? There are two issues. I state the options I have heard.<br />
<br />
<br />
ISSUE ONE  If P/F How to Got About it?<br />
<br />
1) Grade as usual.<br />
<br />
2) Make all classes P/F.<br />
<br />
PRO: Much less pressure on students.<br />
<br />
CON: Might be demoralizing for the good students.<br />
<br />
3) Make all classes P/F but allow students to opt for letter grades BUT they must decide before the last day of class. Hence teachers must post cutoffs before the final is graded<br />
<br />
CON: Complicated and puts (a) teachers in an awkward position of having to post cutoffs before the final, and (b) puts students in an awkward position of having to predict how well they would do.<br />
<br />
CON: A student can blow off a final knowing they will still get a D (passing) in the course.<br />
<br />
PRO: Good students can still get their A's<br />
<br />
CAVEAT: A transcript might look very strange. Say I was looking at a graduate school applicant and I see<br />
<br />
Operating Systems: A<br />
<br />
Theory of Computation: P<br />
<br />
I would likely assume that the Theory course the student got a C. And that might be unfair.<br />
<br />
3) Make all classes P/F but allow students to opt for letter grades AFTER seeing their letter grades. <br />
<br />
PRO: Less complicated an awkard<br />
<br />
PRO: A students blah blah<br />
<br />
CAVEAT above still applies.<br />
<br />
ISSUE TWO If P/F what about a D in the major<br />
<br />
At UMCP COMP SCI (and I expect other depts)<br />
<br />
a D is a passing grade for the University<br />
<br />
but<br />
<br />
a D is not a passing grade for the Major.<br />
<br />
So if a s CS Major gets a D in Discrete Math that does not count for the major--- they have to take it over again.<br />
<br />
But if classes are P/F what do do about that.<br />
<br />
Options<br />
<br />
1) Students have to take classes in their major for a letter grade.<br />
<br />
CON: The whole point of the P/F is to relieve pressure on the students in these hard times.<br />
<br />
PRO: None.<br />
<br />
2) Students taking a course in their major who get a D will still get a P on the transcript but will be told that they have to take the class over again.<br />
<br />
3) Do nothing, but tell the students<br />
<br />
IF you got a D in a course in your major and you are taking a sequel, STUDY HARD OVER THE SUMMER!<br />
<br />
4) Do nothing, but tell the teachers<br />
<br />
Students in the Fall may have a weak background. Just teach the bare minimum of what they need for the major.<br />
<br />
(Could do both 3 and 4)<br />
<br />
SO- what is your school doing and how is it working?</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/06/how-to-handle-grades-during-pandemic.html"><span class="datestr">at June 03, 2020 04:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01825">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01825">Efficient tree-structured categorical retrieval</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belazzougui:Djamal.html">Djamal Belazzougui</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kucherov:Gregory.html">Gregory Kucherov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01825">PDF</a><br /><b>Abstract: </b>We study a document retrieval problem in the new framework where $D$ text
documents are organized in a {\em category tree} with a pre-defined number $h$
of categories. This situation occurs e.g. with taxomonic trees in biology or
subject classification systems for scientific literature. Given a string
pattern $p$ and a category (level in the category tree), we wish to efficiently
retrieve the $t$ \emph{categorical units} containing this pattern and belonging
to the category. We propose several efficient solutions for this problem. One
of them uses $n(\log\sigma(1+o(1))+\log D+O(h)) + O(\Delta)$ bits of space and
$O(|p|+t)$ query time, where $n$ is the total length of the documents, $\sigma$
the size of the alphabet used in the documents and $\Delta$ is the total number
of nodes in the category tree. Another solution uses
$n(\log\sigma(1+o(1))+O(\log D))+O(\Delta)+O(D\log n)$ bits of space and
$O(|p|+t\log D)$ query time. We finally propose other solutions which are more
space-efficient at the expense of a slight increase in query time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01825"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01598">A Novel Approach to Solve K-Center Problems with Geographical Placement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hillmann:Peter.html">Peter Hillmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uhlig:Tobias.html">Tobias Uhlig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodosek:Gabi_Dreo.html">Gabi Dreo Rodosek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rose:Oliver.html">Oliver Rose</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01598">PDF</a><br /><b>Abstract: </b>The facility location problem is a well-known challenge in logistics that is
proven to be NP-hard. In this paper we specifically simulate the geographical
placement of facilities to provide adequate service to customers. Determining
reasonable center locations is an important challenge for a management since it
directly effects future service costs. Generally, the objective is to place the
central nodes such that all customers have convenient access to them. We
analyze the problem and compare different placement strategies and evaluate the
number of required centers. We use several existing approaches and propose a
new heuristic for the problem. For our experiments we consider various
scenarios and employ simulation to evaluate the performance of the optimization
algorithms. Our new optimization approach shows a significant improvement. The
presented results are generally applicable to many domains, e.g., the placement
of military bases, the planning of content delivery networks, or the placement
of warehouses.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01598"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01588">Fast Algorithms for Join Operations on Tree Decompositions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rooij:Johan_M=_M=_van.html">Johan M. M. van Rooij</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01588">PDF</a><br /><b>Abstract: </b>Treewidth is a measure of how tree-like a graph is. It has many important
algorithmic applications because many NP-hard problems on general graphs become
tractable when restricted to graphs of bounded treewidth. Algorithms for
problems on graphs of bounded treewidth mostly are dynamic programming
algorithms using the structure of a tree decomposition of the graph. The
bottleneck in the worst-case run time of these algorithms often is the
computations for the so called join nodes in the associated nice tree
decomposition.
</p>
<p>In this paper, we review two different approaches that have appeared in the
literature about computations for the join nodes: one using fast zeta and
M\"obius transforms and one using fast Fourier transforms. We combine these
approaches to obtain new, faster algorithms for a broad class of vertex subset
problems known as the [\sigma,\rho]-domination problems. Our main result is
that we show how to solve [\sigma,\rho]-domination problems in $O(s^{t+2} t n^2
(t\log(s)+\log(n)))$ arithmetic operations. Here, t is the treewidth, s is the
(fixed) number of states required to represent partial solutions of the
specific [\sigma,\rho]-domination problem, and n is the number of vertices in
the graph. This reduces the polynomial factors involved compared to the
previously best time bound (van Rooij, Bodlaender, Rossmanith, ESA 2009) of $O(
s^{t+2} (st)^{2(s-2)} n^3 )$ arithmetic operations. In particular, this removes
the dependence of the degree of the polynomial on the fixed number of
states~$s$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01588"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01570">CNNs on Surfaces using Rotation-Equivariant Features</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiersma:Ruben.html">Ruben Wiersma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eisemann:Elmar.html">Elmar Eisemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hildebrandt:Klaus.html">Klaus Hildebrandt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01570">PDF</a><br /><b>Abstract: </b>This paper is concerned with a fundamental problem in geometric deep learning
that arises in the construction of convolutional neural networks on surfaces.
Due to curvature, the transport of filter kernels on surfaces results in a
rotational ambiguity, which prevents a uniform alignment of these kernels on
the surface. We propose a network architecture for surfaces that consists of
vector-valued, rotation-equivariant features. The equivariance property makes
it possible to locally align features, which were computed in arbitrary
coordinate systems, when aggregating features in a convolution layer. The
resulting network is agnostic to the choices of coordinate systems for the
tangent spaces on the surface. We implement our approach for triangle meshes.
Based on circular harmonic functions, we introduce convolution filters for
meshes that are rotation-equivariant at the discrete level. We evaluate the
resulting networks on shape correspondence and shape classifications tasks and
compare their performance to other approaches.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01570"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01491">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01491">The Fine-Grained Complexity of Andersen's Pointer Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavlogiannis:Andreas.html">Andreas Pavlogiannis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01491">PDF</a><br /><b>Abstract: </b>Pointer analysis is one of the fundamental problems in static program
analysis. Given a set of pointers, the task is to produce a useful
over-approximation of the memory locations that each pointer may point-to at
runtime. The most common formulation is Andersen's Pointer Analysis (APA),
defined as an inclusion-based set of $m$ pointer constraints over a set of $n$
pointers. Existing algorithms solve APA in $O(n^2\cdot m)$ time, while it has
been conjectured that the problem has no truly sub-cubic algorithm, with a
proof so far having remained elusive. It is also well-known that $APA$ can be
solved in $O(n^2)$ time under certain sparsity conditions that hold naturally
in some settings. Besides these simple bounds, the complexity of the problem
has remained poorly understood.
</p>
<p>In this work we draw a rich fine-grained complexity landscape of APA, and
present upper and lower bounds. First, we establish an $O(n^3)$ upper-bound for
general APA, improving over $O(n^2\cdot m)$ as $n=O(m)$. Second, we show that
sparse instances can be solved in $O(n^{3/2})$ time, improving the current
$O(n^2)$ bound. Third, we show that even on-demand APA ("may a specific pointer
$a$ point to a specific location $b$?") has an $\Omega(n^3)$ (combinatorial)
lower bound under standard complexity-theoretic hypotheses. This formally
establishes the long-conjectured "cubic bottleneck" of APA, and shows that our
$O(n^3)$-time algorithm is optimal. Fourth, we show that under mild
restrictions, APA is solvable in $\tilde{O}(n^{\omega})$ time, where
$\omega&lt;2.373$ is the matrix-multiplication coefficient. It is believed that
$\omega=2+o(1)$, in which case this bound becomes quadratic. Fifth, we show
that even under such restrictions, even the on-demand} problem has an
$\Omega(n^2)$ lower bound under standard complexity-theoretic hypotheses, and
hence our algorithm is optimal when $\omega=2+o(1)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01491"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01428">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01428">Zone Theorem for Arrangements in three dimensions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Sanjeev.html">Sanjeev Saxena</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01428">PDF</a><br /><b>Abstract: </b>In this note, a simple description of zone theorem in three dimensions is
given.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01428"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01400">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01400">Approximation Guarantees of Local Search Algorithms via Localizability of Set Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujii:Kaito.html">Kaito Fujii</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01400">PDF</a><br /><b>Abstract: </b>This paper proposes a new framework for providing approximation guarantees of
local search algorithms. Local search is a basic algorithm design technique and
is widely used for various combinatorial optimization problems. To analyze
local search algorithms for set function maximization, we propose a new notion
called localizability of set functions, which measures how effective local
improvement is. Moreover, we provide approximation guarantees of standard local
search algorithms under various combinatorial constraints in terms of
localizability. The main application of our framework is sparse optimization,
for which we show that restricted strong concavity and restricted smoothness of
the objective function imply localizability, and further develop accelerated
versions of local search algorithms. We conduct experiments in sparse
regression and structure learning of graphical models to confirm the practical
efficiency of the proposed local search algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01400"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01256">Walking through Doors is Hard, even without Staircases: Proving PSPACE-hardness via Planar Assemblies of Door Gadgets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ani:Joshua.html">Joshua Ani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bosboom:Jeffrey.html">Jeffrey Bosboom</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, Yevhenii Diomidov, Dylan Hendrickson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01256">PDF</a><br /><b>Abstract: </b>A door gadget has two states and three tunnels that can be traversed by an
agent (player, robot, etc.): the "open" and "close" tunnel sets the gadget's
state to open and closed, respectively, while the "traverse" tunnel can be
traversed if and only if the door is in the open state. We prove that it is
PSPACE-complete to decide whether an agent can move from one location to
another through a planar assembly of such door gadgets, removing the
traditional need for crossover gadgets and thereby simplifying past
PSPACE-hardness proofs of Lemmings and Nintendo games Super Mario Bros., Legend
of Zelda, and Donkey Kong Country. Our result holds in all but one of the
possible local planar embedding of the open, close, and traverse tunnels within
a door gadget; in the one remaining case, we prove NP-hardness.
</p>
<p>We also introduce and analyze a simpler type of door gadget, called the
self-closing door. This gadget has two states and only two tunnels, similar to
the "open" and "traverse" tunnels of doors, except that traversing the traverse
tunnel also closes the door. In a variant called the symmetric self-closing
door, the "open" tunnel can be traversed if and only if the door is closed. We
prove that it is PSPACE-complete to decide whether an agent can move from one
location to another through a planar assembly of either type of self-closing
door. Then we apply this framework to prove new PSPACE-hardness results for
eight different 3D Mario games and Sokobond.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01256"><span class="datestr">at June 03, 2020 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01225">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01225">Streaming Coresets for Symmetric Tensor Factorization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Rachit Chhaya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhari:Jayesh.html">Jayesh Choudhari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dasgupta:Anirban.html">Anirban Dasgupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shit:Supratim.html">Supratim Shit</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01225">PDF</a><br /><b>Abstract: </b>Factorizing tensors has recently become an important optimization module in a
number of machine learning pipelines, especially in latent variable models. We
show how to do this efficiently in the streaming setting. Given a set of $n$
vectors, each in $\mathbb{R}^d$, we present algorithms to select a sublinear
number of these vectors as coreset, while guaranteeing that the CP
decomposition of the $p$-moment tensor of the coreset approximates the
corresponding decomposition of the $p$-moment tensor computed from the full
data. We introduce two novel algorithmic techniques: online filtering and
kernelization. Using these two, we present four algorithms that achieve
different tradeoffs of coreset size, update time and working space, beating or
matching various state of the art algorithms. In case of matrices (2-ordered
tensor) our online row sampling algorithm guarantees $(1 \pm \epsilon)$
relative error spectral approximation. We show applications of our algorithms
in learning single topic modeling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01225"><span class="datestr">at June 03, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2006.01202">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2006.01202">Negative Instance for the Edge Patrolling Beacon Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abel:Zachary.html">Zachary Abel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akitaya:Hugo_A=.html">Hugo A. Akitaya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Martin_L=.html">Martin L. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hesterberg:Adam.html">Adam Hesterberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Matias.html">Matias Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ku:Jason_S=.html">Jason S. Ku</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2006.01202">PDF</a><br /><b>Abstract: </b>Can an infinite-strength magnetic beacon always ``catch'' an iron ball, when
the beacon is a point required to be remain nonstrictly outside a polygon, and
the ball is a point always moving instantaneously and maximally toward the
beacon subject to staying nonstrictly within the same polygon? Kouhestani and
Rappaport [JCDCG 2017] gave an algorithm for determining whether a
ball-capturing beacon strategy exists, while conjecturing that such a strategy
always exists. We disprove this conjecture by constructing orthogonal and
general-position polygons in which the ball and the beacon can never be united.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2006.01202"><span class="datestr">at June 03, 2020 11:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/">Post-Doctoral Research Fellowship at All Souls College, University of Oxford (apply by September 11, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>All Souls is primarily a research institution with particular strengths in the Humanities and Social and Theoretical Sciences, and with strong connections to public life. It is strongly committed to supporting early career scholars. The Fellowships are intended to offer opportunities for outstanding early career researchers to establish a record of independent research.</p>
<p>Website: <a href="https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars">https://www.asc.ox.ac.uk/post-doctoral-research-fellowships-2020-further-particulars</a><br />
Email: pdrf.admin@all-souls.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/06/02/post-doctoral-research-fellowship-at-all-souls-college-university-of-oxford-apply-by-september-11-2020/"><span class="datestr">at June 02, 2020 11:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4830">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4830">The US might die, but P and PSPACE are forever</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Today, I interrupt the news of the rapid disintegration of the United States of America, on every possible front at once (medical, economic, social…), to bring you something far more important: a long-planned <a href="https://www.preposterousuniverse.com/podcast/2020/06/01/99-scott-aaronson-on-complexity-computation-and-quantum-gravity/">two-hour podcast</a>, where theoretical physicist and longtime friend-of-the-blog <a href="https://www.preposterousuniverse.com/">Sean Carroll</a> interviews yours truly about complexity theory!  Here’s Sean’s description of this historic event:</p>



<blockquote class="wp-block-quote"><p>There are some problems for which it’s very hard to find the answer, but very easy to check the answer if someone gives it to you. At least, we think there are such problems; whether or not they really exist is the famous <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">P vs NP problem</a>, and actually proving it will win you <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">a million dollars</a>. This kind of question falls under the rubric of “computational complexity theory,” which formalizes how hard it is to computationally attack a well-posed problem. Scott Aaronson is one of the world’s leading thinkers in computational complexity, especially the wrinkles that enter once we consider quantum computers as well as classical ones. We talk about how we quantify complexity, and how that relates to ideas as disparate as creativity, knowledge vs. proof, and what all this has to do with black holes and quantum gravity.</p></blockquote>



<p>So, OK, I guess I should also comment on the national disintegration thing.  As someone who was once himself the victim of a crazy police overreaction (albeit, trivial compared to what African-Americans regularly deal with), I was moved by the scenes of police chiefs in several American towns taking off their helmets and joining protesters to cheers.  Not only is that a deeply moral thing to do, but it serves a practical purpose of quickly defusing the protests.  Right now, of course, is an <em>even worse time than usual</em> for chaos in the streets, with a lethal virus still spreading that doesn’t care whether people are congregating for good or for ill.  If rational discussion of policy still matters, I support the current push to end the “qualified immunity” doctrine, end the provision of military training and equipment to police, and generally spur the nation’s police to rein in their psychopath minority.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4830"><span class="datestr">at June 01, 2020 07:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7741">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/">Theory of Machine Learning summer seminar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[Note: While I and many others are fortunate to be able to go on with our work, deadlines, and (as mentioned in this post) seminars, this is not the case for many in the U.S. following yet another demonstration that black lives don’t matter as much as they should in this country. I would like to relay <a href="https://twitter.com/red_abebe/status/1267145593991946245">Rediet Abebe’s call</a> to support local organizations. As Rediet says “These problems have been and will be here for a very long time. We’re not solving racism this month.”. –Boaz]</em></p>



<p>For the last year, I have been co organizing a <a href="https://mltheory.org/#talks">theory of machine learning seminar</a> at Harvard. Following the format of our prior Harvard/MSR/MIT theory reading group, these have been extended blackboard talks with plenty of audience interaction.</p>



<p>Following COVID-19, the last three talks in the semester (by Moritz Hardt,  Zico Kolter, and Anima Anandkumar)  were given virtually. Frankly, I was at first unsure whether these seminars can work in the virtual format but was pleasantly surprised. Talks have been very interactive, with plenty of audience participation in the chat channel. In fact, the virtual format has some <em>advantages </em>over physical talks. Sometimes a question will be asked and answered by a co-author over chat, without the speaker needing to interrupt the talk.</p>



<p>Since the seminars were so successful, we decided to continue holding them over the summer. We have an exciting line up of confirmed speakers, and more will come soon. See <a href="https://mltheory.org/#talks">our webpage</a> for more details, which also contains a google calendar and a mailing list you can sign up for to get the Zoom link.</p>



<p>Confirmed speakers so far include:</p>



<ul><li><a href="http://www.sohldickstein.com/">Jascha Sohl-Dickstein</a> – June 11</li><li><a href="https://www.neyshabur.net/">Behnam Neyshabur</a> – June 18</li><li><a href="https://users.ece.utexas.edu/~dimakis/">Alex Dimakis</a> – July 9</li><li><a href="https://www.cohennadav.com/">Nadav Cohen</a> – August 6</li><li><a href="https://maithraraghu.com/">Maithra Raghu</a> – date tbd</li><li><a href="https://research.google/people/HanieSedghi/">Hanie Sedghi</a> – date tbd</li></ul>



<p>More should be confirmed soon – join our <a href="https://groups.google.com/a/seas.harvard.edu/forum/#!forum/ml-theory-seminar/join">mailing list</a> to get updates.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/06/01/theory-of-machine-learning-summer-seminar/"><span class="datestr">at June 01, 2020 04:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2566">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">Gradient descent for wide two-layer neural networks – I : Global convergence</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Supervised learning methods come in a variety of flavors. While local averaging techniques such as <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">nearest-neighbors</a> or <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a> are often used with low-dimensional inputs where they can adapt to any potentially non-linear relationship between inputs and outputs, methods based on empirical risk minimization are the most commonly used in high-dimensional settings. Their principle is simple: optimize the (potentially regularized) risk on training data over prediction functions in a pre-defined set of functions.</p>



<p class="justify-text">When the set of a functions is a convex subset of a vector space with a finite-dimensional representation, with standard assumptions, the corresponding optimization problem is convex. This has the benefits of allowing a thorough theoretical understanding of the computational and statistical properties of learning methods, which often come with strong theoretical guarantees, in terms of running time [<a href="https://arxiv.org/pdf/1405.4980">1</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">2</a>, <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">3</a>] or prediction performance on unseen data [<a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">4</a>, <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">5</a>]. In particular, the linear parameterization can be done either explicitly by building a typically large finite set of features, or implicitly through the use of kernel methods and then a series of dedicated algorithms and theories can be leveraged for efficient non-linear predictions [6, <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">7</a>, <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">8</a>].</p>



<p class="justify-text">However, linearly-parameterized sets of functions do not include neural networks, which lead to state-of-the-art performance in most learning tasks in computer vision, natural language processing, speech processing, in particular through the use of deep and convolutional neural networks [<a href="https://www.deeplearningbook.org/">9</a>].</p>



<h2>Two-layer neural networks with “relu” activations</h2>



<p class="justify-text">The goal of this blog post is to provide some understanding of why supervised machine learning work for the simplest form of such models: $$ h(x) = \frac{1}{m} \sum_{i=1}^m a_i ( b_i^\top x)_+ = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\},$$ where the input \(x\) is a vector in \(\mathbb{R}^d\), and \(m\) is the number of hidden neurons. The weights \(a_i \in \mathbb{R}\), \(i=1,\dots,n\), are the <em>output weights</em>, while the weights \(b_i \in \mathbb{R}^d\), \(i=1,\dots,n\), are the <em>input weights</em>. The rectified linear unit (“relu”) [<a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">10</a>] activation is used, and our results will depend heavily on its positive homogeneity (that is, for \(\lambda &gt; 0\), \((\lambda u)_+ = \lambda u_+\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="407" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/nn_single_blog.png" class="wp-image-3829" height="292" />Two-layer neural network in dimension \(d = 6\) with \(m=4\) hidden neurons, and a single output.</figure></div>



<p class="justify-text">Note that this is an idealized and much simplified set-up for deep learning, as there is a single hidden-layer, no convolutions, no pooling, etc. As I will show below, this simple set-up is already complex to understand, and I believe it captures some of the core difficulties associated with non-convexity.</p>



<p class="justify-text">The first question that one may come to after decades of research in learning theory is: <em>why is it so hard to analyze?</em>  </p>



<p class="justify-text">There are at least two major difficulties:</p>



<ul class="justify-text"><li><strong>Non-linearity</strong>: the dependence on the input weights \(b_i\)’s is non-linear because of the activation function, typically leading to non-convex optimization problems.</li><li><strong>Overparameterization</strong>: The number \(m\) of hidden neurons is very large (often so large that the number of parameters \(m(d+1)\) exceeds the number of observations), which is hard in terms of optimization and potentially generalization to unseen data. </li></ul>



<p class="justify-text">In this blog post, we will leverage the overparameterization and take \(m\) tending to infinity (without any dependence on the number of observations), which will allow us to derive theoretical results. We will leverage two key properties of the problem:</p>



<ul class="justify-text"><li><strong>Separability</strong> of the model in \(w_i = (a_i,b_i)\), that is, the prediction function \(h(x)\) is the sum of terms which are independently parameterized, as \(h = \frac{1}{m} \sum_{i=1}^m \Phi(w_i)\), where \(\Phi: \mathbb{R}^p \to \mathcal{F}\), with \(\mathcal{F}\) a space of functions. In our situation, \(p = d+1\) and: $$ \Phi(w)(x) = a (b^\top x)_+. $$ In other words,  there is no parameter sharing among hidden neurons. Unfortunately, this does not generalize to more than a single hidden layer.</li><li><strong>Homogeneity</strong>: the relu activation is positively homogeneous so that as as function of \(w = (a,b) \in \mathbb{R} \times \mathbb{R}^d\), \(\Phi(w)(x) = a (b^\top x)_+\) is positively 2-homogeneous, that is, \(\Phi(\lambda w) = \lambda^2 \Phi(w)\) for \(\lambda &gt; 0\).</li></ul>



<p class="justify-text">In this sequence of two blog posts, following a recent trend in optimization and machine learning theory [<a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">11</a>, <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">12</a>], optimization and statistics cannot be separated and need to be tackled together. I will focus on gradient flows on empirical or expected risks.</p>



<p class="justify-text">In this blog post, I will cover optimization and how over-parameterization leads to global convergence for 2-homogeneous models, a recent result obtained two years ago with <a href="https://lchizat.github.io/">Lénaïc Chizat</a> [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]. This requires tools from optimal transport which I will briefly describe (for more details, see, e.g., [<a href="https://arxiv.org/abs/1803.00567">14</a>]).</p>



<p class="justify-text">Next month, I will focus on generalization capabilities and the several implicit biases associated with gradient descent in this context [<a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">15</a>, <a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<h2>Infinitely wide limit and probability measures</h2>



<p class="justify-text">Following the standard learning set-up, our goal will be to minimize with respect to the prediction function \(h\) the functional \(R\) defined as $$ R(h) = \mathbb{E}_{p(x,y)} \big[ \ell( y, h(x) ) \big],$$ where \(\ell(y,h(x))\) is the loss incurred by outputting \(h(x)\) when \(y\) is the true label. Even within deep learning, this loss is most often convex in its second argument, such as for least-squares or <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">logistic</a> losses. Thus, I will assume that \(R\) is convex.</p>



<p>The expectation can be considered in two scenarios:</p>



<ul class="justify-text"><li><strong>Empirical risk</strong>: this corresponds to the situation where we have observations \((x_j,y_j)\), \(j=1,\dots,n\), coming from some joint distribution on \((x,y) \in \mathbb{R}^d \times \mathbb{R}\). Minimizing \(R\) then may not lead to any guarantee on unseen data unless some explicit or implicit regularization is used. In next blog post, I will consider the implicit regularization effect of gradient-based algorithms.</li><li><strong>Expected risk (or generalization performance)</strong>: The expectation is taken with respect to unseen data, and thus its value (or a gradient) cannot be computed. However, any training observation \((x_j,y_j)\) can lead to an unbiased estimate, and if single pass stochastic gradient is used, our guarantees will be on the expected risk.</li></ul>



<p class="justify-text">The main and very classical idea is to consider the minimization of $$ G(W) = G(w_1,\dots,w_m) = R \Big( \frac{1}{m} \sum_{i=1}^m \Phi(w_i) \Big),$$ and see it as the minimization of $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ with respect to a probability measure \(\mu\), with the equivalence for $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ where \(\delta(w_i)\) is the Dirac measure at \(w_i\). See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="615" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/diracs_measures-1-1024x156.png" class="wp-image-3852" height="93" />Left: discrete probability measure. Right: measure with density.</figure></div>



<p class="justify-text">When \(m\) is large, we can represent any measure in the <a href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak sense</a> (that is, expectations of any continuous and bounded functions can be approximated). The benefits of considering the space of all measures instead of discrete measures have been used already in variety of contexts in machine learning, statistics and signal processing [<a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">17</a>, <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">18</a>, <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">19</a>]. In this blog post, the key benefit is that the set of measures in convex and \(h = \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \) is linear in the measure \(\mu\), so that our optimization problem has become convex.</p>



<p class="justify-text">However, (1) It does not buy much in practice, as the set of probability measures is infinite-dimensional. <a href="https://en.wikipedia.org/wiki/Frank–Wolfe_algorithm">Frank-Wolfe algorithms</a> can be used, but the choice of new neurons is a difficult optimization problem, NP-hard for the threshold activation function [<a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">20</a>], with polynomial potentially high complexity for the relu activation [<a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">21</a>], and (2) this is not what is used in practice, which is (stochastic) gradient descent.</p>



<h2>Finite-dimensional gradient flow</h2>



<p class="justify-text">In this post, I will consider the gradient flow $$\dot{W} = \ – m \nabla G(W),$$ (where \(m\) is added as a normalization factor to allow a well-defined limit when \(m\) tends to infinity). This is still not exactly what is used in practice, but, as explained in <a href="https://francisbach.com/gradient-flows/">last month post</a>, this is a good approximation of gradient descent (if using the empirical risk, then leading to guarantees of global convergence on the empirical risk only), or stochastic gradient descent (if doing a single pass on the data, then leading to guarantees of global convergence on unseen data). This is a non-convex dynamics, with stationary points and local minima, even when \(m\) is large (see, e.g., [<a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">27</a>]).</p>



<p class="justify-text">Two main questions arise: (1) what does the gradient flow dynamics converge to when the number of neurons \(m\) tends to infinity, and (2) can we get any global convergence guarantees for the limiting dynamics?</p>



<h2>Mean-field limit and Wasserstein gradient flows</h2>



<p class="justify-text">When \(m\) tends to infinity, since we want to use the measure representation, we need to understand the effect of performing the gradient flow jointly on \(w_1,\dots,w_m\) on the measure $$ \mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i),$$ and see if we can take a limit when \(m\) tends to infinity. This process of taking limits is common in physics and often referred to as the “mean-field” limit, and has been considered in a series of recent works [<a href="https://arxiv.org/pdf/1712.05438">22</a>, <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>, <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">23</a>, <a href="https://arxiv.org/pdf/1805.00915">24</a>]. To avoid too much technicality, I will assume that the map \(\Phi\) is sufficiently differentiable, which unfortunately exclude the relu activation; for dedicated results, see [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>].</p>



<p class="justify-text"><strong>Gradient flows on metric spaces.</strong> In order to understand the dynamics in the space of probability measures, we need to take a step backward and realize that gradient flows can be defined for many functions \(f\) on any metric space \(\mathcal{X}\). Indeed, it can be seen as the limit of taking infinitesimal steps of length \(\gamma\), where each new iterate \(x_{k+1}\) (corresponding to the value at time \(k\gamma\)) is defined recursively from \(x_k\) as $$x_{k+1} \in \arg\min_{x \in \mathcal{X}}\  f(x) + \frac{1}{2\gamma} d(x,x_k)^2.$$ As shown in [<a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">25</a>, <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">26</a>], with some form of interpolation, this defines a curve with prescribed values \(x_k\) at each \(\gamma k\), and when the step-size \(\gamma\) goes to zero, this curves “converges” to the gradient flow.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="419" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/euler-1024x520.png" class="wp-image-3960" height="212" />Gradient flow in bold black, with interpolating curve in red from 14 points \(x_0,\dots,x_{13}\).</figure></div>



<p class="justify-text">For the space \(\mathcal{X} = \mathbb{R}^d\) with the Euclidean distance and a continuously differentiable function \(f\), we obtain that $$x_{k+1} = x_k – \gamma f'(x_k) + o(\gamma),$$ and we get the usual gradient flow associated to \(f\), and the scheme above is nothing less than <a href="https://en.wikipedia.org/wiki/Euler_method">Euler discretization</a> that was described <a href="https://francisbach.com/gradient-flows/">last month</a>.</p>



<p class="justify-text"><strong>Vector space gradient flows on probability measures.</strong> Probability measures are a convex subset of measures with finite <a href="https://en.wikipedia.org/wiki/Total_variation#Total_variation_of_probability_measures">total variation</a>, which is equal to the \(\ell_1\)-norm between densities when the two probability measures have densities with respect to the same base measure. It is a normed vector space for which we could derive our first type of gradient flow, which can be seen as a continuous version of Frank-Wolfe algorithm, where atoms are added one by one, until convergence.  </p>



<p class="justify-text">As mentioned above, the fact that atoms are created sequentially seems attractive computationally. However, (1) deciding which one to add is a computationally hard problem, and (2) the flow on measures cannot be approximated by a finite evolving set of “particles” (here hidden neurons each defined by a vector \(w \in \mathbb{R}^{d+1}\)).</p>



<p class="justify-text"><strong>Wasserstein gradient flows on probability measures.</strong> There is another natural distance here, namely the <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> (sometimes called the Kantorovich–Rubinstein distance). In order to remain short, I will only define it between empirical measures $$\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i) \mbox{ and } \nu = \frac{1}{m} \sum_{i=1}^m \delta(v_i)$$ with the same number of points. The squared 2-Wasserstein distance is obtained by minimizing $$\frac{1}{m} \sum_{i=1}^m \| w_j – v_{\sigma(j)} \|_2^2$$ over all permutations \(\sigma: \{1,\dots,m\} \to \{1,\dots,m\}\). See illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="573" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein-2-1024x307.png" class="wp-image-3886" height="171" />Wasserstein distance between two empirical measures: (left) original observations of two empirical measures with \(m = 11\) points, (right) assigning all black points to red points by minimizing the sum of squared distances between assigned points.</figure></div>



<p class="justify-text">This can be extended to any pair of probability measures, and used within gradient flows, it has a very natural decoupling property: if \(\mu\) is fixed, and \(\nu\) is within a small distance of \(\mu\) in Wasserstein distance, then the optimal permutation above will always be the same, that is, locally, the Wasserstein distance is a sum of squared Euclidean distances. Then, the Wasserstein gradient flow will lead to \(m\) independent local regular Euclidean gradient flows, which interact through the gradient term as: $$ \dot{w}_i = \ –  \nabla \Phi(w_i)  \nabla R\Big(\int_{\mathbb{R}^p} \Phi d\mu \Big),$$ where the Jacobian \(\nabla \Phi(w_i)\) is a linear operator from \(\mathcal{F}\) to \(\mathbb{R}^p\), and \( \nabla R: \mathbb{R}^p \to \mathcal{F}\) the gradient operator of \(R\). Since \(\mu = \frac{1}{m} \sum_{i=1}^m \delta(w_i)\), the dynamics of each particle interacts through the gradient of \(R\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="456" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/Wasserstein_flows-1024x508.png" class="wp-image-3890" height="225" />Gradient flow for \(m=7\) interacting particles.</figure></div>



<p class="justify-text">The intuitive reasoning above is behind the formal result for the function $$ F(\mu) = R \Big( \int_{\mathbb{R}^p} \Phi(w) d\mu(w) \Big),$$ that the limit of the Euclidean gradient flow on each particle when \(m\) tends to infinity, is exactly the Wasserstein gradient flow of \(F\). While I have proposed an intuitive explanation, this can be made more formal in particular through the use of partial differential equations on the density of the measure [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>] (see also nice <a href="http://web.math.ucsb.edu/~kcraig/math/curriculum_vitae_files/NIPS_120917.pdf">slides</a> from Katy Craig on Wasserstein gradient flows).</p>



<p class="justify-text"><strong>Stationary points.</strong> Since \(R\) is assumed convex over the convex set of probability measures, all local minima of \(R\) are global, and we should expect the gradient flow to converge to global optimum from any initial measure. This is true for the gradient flow associated with the total variation metric. However this is not true for the Wasserstein gradient flow, for which stationary points which are not global minimizers exist (given that for discrete measures this corresponds to classical backpropagation, this is well known to anybody who has ever trained a neural network). Note that there exists a notion of convexity for Wasserstein gradient flows, namely <a href="https://en.wikipedia.org/wiki/Geodesic_convexity">geodesic convexity</a>, but the function \(F\) is not geodesically convex in general.</p>



<h2>Global convergence</h2>



<p class="justify-text">We can now describe the main result from our recent work with Lénaïc [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">13</a>]: under assumptions described below, for the function \(F\) defined above, if the Wasserstein gradient flow converges to a measure, this measure has to be a global minimum of \(F\) (note that we cannot prove it is always convergent).</p>



<p class="justify-text">On top of technical regularity assumptions that I will not describe here, we need two crucial broad assumptions:</p>



<ul class="justify-text"><li><strong>Homogeneity</strong> of the function \(\Phi: \mathbb{R}^{d+1} \to \mathcal{F}\). We need a condition of this form, since if \(R\) is a linear function, then \(F(\mu)\) is of the form \(F(\mu) = \int_{\mathbb{R}^p} \psi(w)d\mu(w)\) with \(\psi(w) = R(\Phi(w))\), and the Wasserstein gradient flow will converge to a weighted some of Diracs at all local minimizers of \(\psi\), which is typically not a global minimizer.</li><li><strong>Initialization with positive mass in all directions</strong>. That is, \(w_i\)’s are uniformly distributed on the sphere or Gaussian, which is the de facto choice in practice. </li></ul>



<p class="justify-text"><strong>Illustration</strong>. We illustrate the result above by considering \(R\) as the square loss and \(y\) being generated from \(x\) through a neural network with \(m_0=5\) neurons. When running the gradient flow above, as soon as \(m \geqslant 5\), the model is sufficiently flexible to attain zero loss, which is thus the global optimum of the cost function. However, the gradient flow may not reach it, as it gets trapped in a local optimum. Our theoretical result suggests that when \(m\) is large, we should converge to the original neurons, which we see below. The surprising (and still unexplained) phenomenon is that \(m\) does not need to be much larger than \(m_0\) to see practical global convergence.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="389" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m5-1024x683.png" class="wp-image-3942" height="259" />Position of \(m = 5\) neurons, plotted as \(|a_i| b_i \in \mathbb{R}^2\) for a two-dimensional problem. The five dotted lines are the directions of the generating neurons. Although \(m\) is large enough to lead to the global optimum, the flow gets stuck in a local optimum.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="365" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m10-1024x683.png" class="wp-image-3943" height="243" />Position of \(m = 10\) neurons; same setting as above. The flow converges to the global optimum, although \(m\) is not large.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="379" alt="" src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100-1024x683.png" class="wp-image-3944" height="252" />Position of \(m = 100\) neurons; same setting as above. The flow converges to the global optimum, with \(m\) large. See video below.</figure></div>



<figure class="wp-block-video aligncenter justify-text"><video src="https://francisbach.com/wp-content/uploads/2020/05/ReLU_m100.mp4"></video>Position of \(m = 100\) neurons; exact same setting as above.</figure>



<h2>Discussion and open problems</h2>



<p class="justify-text">In this blog post, I described theoretical results showing the benefits of overparameterization: when the number of hidden neurons \(m\) tends to infinity, then the corresponding gradient flow converges to the global optimum of the cost function. The proof relies notably on homogeneity properties of the relu activation. </p>



<p class="justify-text">The main weakness of  this result is that is only <em>qualitative</em>: we cannot quantify how big \(m\) need to be to be close to the infinite width limit, or how fast the gradient flow converges to the global optimum. These are still open problems. Additional interesting areas of research are to extend these results to convolutional and/or deep networks.</p>



<p class="justify-text">Now that we know that we can obtain global convergence, I will describe next month the generalization properties when interpolating the training data with an overparameterized relu network [<a href="https://arxiv.org/pdf/2002.04486">16</a>].</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat for producing the nice figures and video of neural networks, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980">Convex Optimization: Algorithms and Complexity</a>. <em>Foundations and Trends in Machine Learning</em>, <em>8</em>(3-4), 231-357, 2015.<br />[2] Léon Bottou, Frank E. Curtis, Jorge Nocedal. <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1080173">Optimization methods for large-scale machine learning</a>. SIAM Review, 60(2):223-311, 2018.<br />[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski. <a href="http://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. <em>Foundations and Trends in Machine Learning, </em>4(1):1-106, 2012.<br />[4] Shai Shalev-Shwartz, Shai Ben-David. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding machine learning: From theory to algorithms</a>. Cambridge University Press, 2014.<br />[5] Larry Wasserman. <a href="http://static.stevereads.com/papers_to_read/all_of_statistics.pdf">All of statistics: a concise course in statistical inference</a>. Springer Science &amp; Business Media, 2013.<br />[6] Bernhard Schölkopf, Alexander J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.<br />[7] Ali Rahimi and Benjamin Recht. <a href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines</a>. <em>Advances in neural information processing systems</em>, 2008.<br />[8] Alessandro Rudi, Luigi Carratino, Lorenzo Rosasco. <a href="http://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf">Falkon: An optimal large scale kernel method</a>. <em>Advances in Neural Information Processing Systems</em>, 2017.<br />[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville. <a href="https://www.deeplearningbook.org/">Deep learning</a>. MIT Press, 2016.<br />[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. <a href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks</a>. <em>International Conference on Artificial Intelligence and Statistics</em>, 2011.<br />[11] Francis Bach and Eric Moulines. <a href="http://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)</a>. <em>Advances in Neural Information Processing Systems</em>, 2013.<br />[12] MIkhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off</a>. <em>Proceedings of the National Academy of Sciences</em>, 116(32), 15849-15854, 2019.<br />[13] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018.<br />[14] Gabriel Peyré, Marco Cututi. <em><a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a></em>. Foundations and Trends in Machine Learning, 51(1):1–44, 2019.<br />[15] Lénaïc Chizat, Edouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On Lazy Training in Differentiable Programming</a>. <em>Advances in Neural Information Processing Systems</em>, 2019.<br />[16] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. Technical report, arXiv:2002.04486, 2020.<br />[17] Andrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function</a>. <em>IEEE Transactions on Information theory</em>, <em>39</em>(3), 930-945, 1993.<br />[18] Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte. <a href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">Convex neural networks</a>. <em>Advances in neural information processing systems</em>, 2006.<br />[19] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a>.<strong> </strong><em>Journal of Machine Learning Research</em>, 18(19):1-53, 2017.<br />[20] Venkatesan Guruswami, Prasad Raghavendra. <a href="https://epubs.siam.org/doi/pdf/10.1137/070685798">Hardness of learning halfspaces with noise</a>. <em>SIAM Journal on Computing</em>, 39(2):742-765, 2009.<br />[21] Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler. <a href="http://proceedings.mlr.press/v65/goel17a/goel17a.pdf">Reliably Learning the ReLU in Polynomial Time</a>. <em>Conference on Learning Theory</em>, 2017.<br />[22] Atsushi Nitanda, Taiji Suzuki. <a href="https://arxiv.org/pdf/1712.05438">Stochastic particle gradient descent for infinite ensembles</a>. Technical report, arXiv:1712.05438, 2017.<br />[23] Song Mei, Andrea Montanari, Phan-Minh Nguyen. <a href="https://www.pnas.org/content/pnas/115/33/E7665.full.pdf">A mean field view of the landscape of two-layer neural networks</a>. <em>Proceedings of the National Academy of Sciences</em> 115(33):E7665-E7671, 2018.<br />[24] Grant M. Rotskoff, Eric Vanden-Eijnden. <a href="https://arxiv.org/pdf/1805.00915">Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error</a>. Technical report, arXiv:1805.00915, 2018.<br />[25] Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré. <a href="http://www2.stat.duke.edu/~sayan/ambrosio.pdf">Gradient flows: in metric spaces and in the space of probability measures</a>. Springer Science &amp; Business Media, 2008<br />[26] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br />[27] Itay Safran, Ohad Shamir. <a href="http://proceedings.mlr.press/v80/safran18a/safran18a.pdf">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a>. <em>International Conference on Machine Learning</em>, 2018.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/"><span class="datestr">at June 01, 2020 07:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/diana-davis-beautiful-pentagons/">Diana Davis’s beautiful pentagons</a> (<a href="https://mathstodon.xyz/@11011110/104182521046531449"></a>). I briefly mentioned her regular-pentagon billiards-trajectory art in <a href="https://11011110.github.io/blog/2019/02/15/linkage.html">an earlier post</a> but now Evelyn Lamb has a much more detailed column on her and her work.</p>
  </li>
  <li>
    <p><a href="https://inference-review.com/article/points-and-lines">Points and lines</a> (<a href="https://mathstodon.xyz/@11011110/104190473820256744"></a>). A new review of my book <em><a href="https://www.ics.uci.edu/~eppstein/forbidden/">Forbidden Configurations in Discrete Geometry</a></em>, by Daniel Kleitman, in <em>Inference</em>.</p>
  </li>
  <li>
    <p><a href="https://www.mathi.uni-heidelberg.de/~roquette/noetherphoto-engl.pdf">About photos of Emmy Noether</a> (<a href="https://mathstodon.xyz/@11011110/104193371530927420"></a>), in which Peter Roquette apologizes for having indirectly caused <a href="https://books.google.com/books/about/Emmy_Noether.html?id=IePqBgAAQBAJ">Margaret Tent’s young-adult historical-fiction about Noether</a> to have a photo of someone else on its cover. Via MarkH<sub>21</sub> on Wikipedia, in the context of <a href="https://commons.wikimedia.org/wiki/Commons:Deletion_requests/File:Noether.jpg">a discussion of the provenance of a different photo of Noether</a>.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.4007/annals.2020.191.2.5">The Conway knot is not slice</a> (<a href="https://mathstodon.xyz/@11011110/104196369094406901"></a>). Newly published result by Lisa Piccirillo in <em>Ann. Math.</em>, with <a href="https://www.quantamagazine.org/graduate-student-solves-decades-old-conway-knot-problem-20200519/">an overview of the significance of the result (if not much of its detail) in <em>Quanta</em></a>.</p>
  </li>
  <li>
    <p><a href="https://gowers.wordpress.com/2020/05/20/mathematical-research-reports-a-new-mathematics-journal-is-launched/">Tim Gowers relates the convoluted history of a mathematical announcements journal</a> (<a href="https://mathstodon.xyz/@11011110/104205046534922072"></a>). <em>Electronic Research Announcements of the AMS</em> (founded 1995) moved to the American Inst. of Mathematical Sciences in 2007, but recent heavyhanded moves by the publisher led the editorial board to quit, and its name changed to <em>Electronic Research Archive</em>. In the meantime the old editorial board have a new journal: <em><a href="https://mrr.centre-mersenne.org/">Mathematical Research Reports</a></em>. See the Gowers link for details.</p>
  </li>
  <li>
    <p>In preparation for time travel week in graduate data structures, here’s <a href="http://www.tasteofcinema.com/2014/the-20-best-time-travel-movies-of-all-time/">a listicle of 20 time travel movies</a> (<a href="https://mathstodon.xyz/@11011110/104208167329418315"></a>). There are any number of these lists but for me they should include at least <em>12 Monkeys</em>, <em>Primer</em>, <em>Safety Not Guaranteed</em>, <em>Donnie Darko</em>, and <em>Time Bandits</em>. This one adds <em>The Girl Who Leapt Through Time</em>, also good. I’d have thrown in <em>The Infinite Man</em> but it’s obscure enough that I’m not offended by its absence. The relevant one for my class, <em>Retroactive</em>, can stay off.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2020/05/xavier-puente-vilardell-wood-sculpture/">Spirals and loops twist through wooden sculptures by Xavier Puente Vilardell</a> (<a href="https://mathstodon.xyz/@11011110/104216153044887740"></a>).</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/a/361166/440">Dmitri Panov constructs infinitely many convex 4-polytopes with no triangle or quadrilateral 2-faces</a> (<a href="https://mathstodon.xyz/@11011110/104222101065596394"></a>). The construction is pretty: take some 120-cells (all 2-faces regular pentagons and all 3-faces regular dodecahedra), embedded into hyperbolic space so all dihedrals are right angles, and glue them together on shared facets. If at most two touch at each ridge, the result is convex. Some pairs of pentagons merge into hexagons, but you still have no triangles or quads.</p>
  </li>
  <li>
    <p><a href="https://cp4space.wordpress.com/2013/09/06/ten-things-you-possibly-didnt-know-about-the-petersen-graph/">Ten things you (possibly) didn’t know about the Petersen graph</a> (<a href="https://mathstodon.xyz/@11011110/104226455130356772"></a>). Found while making <a href="https://en.wikipedia.org/wiki/The_Petersen_Graph">a Wikipedia article on the book <em>The Petersen Graph</em></a> (for the graph itself, see <a href="https://en.wikipedia.org/wiki/Petersen_graph">its Wikipedia article</a>).</p>
  </li>
  <li>
    <p><a href="https://www.taylorfrancis.com/books/9781351247771">Godfried Toussaint’s book <em>The Geometry of Musical Rhythm:
What Makes a “Good” Rhythm Good?</em>, on the mathematical analysis of drumming patterns, has a new expanded posthumous 2nd edition</a> (<a href="https://mathstodon.xyz/@11011110/104233265094045761"></a>). I was able to download free from there but that may be via a campus subscription; your access may vary. For a description of the 1st edition of the book (probably undetailed enough that it can describe the 2nd as well) see <a href="https://en.wikipedia.org/wiki/The_Geometry_of_Musical_Rhythm">its Wikipedia article</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=CfRSVPhzN5M">French video about self-replicating patterns in cellular automata, with English subtitles</a> (<a href="https://mathstodon.xyz/@11011110/104239007149697974"></a>, <a href="https://cp4space.wordpress.com/2019/06/11/self-replicator-caught-on-video/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@erou/104245194532017940">@erou visualizes the complexity of Karatsuba’s algorithm for integer multiplication as a Sierpiński triangle</a>, inside a square, with a number of dark pixels proportional to the steps of the algorithm. The square itself counts in the same way the complexity of the naive algorithm.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/quicktakes/2020/05/28/proposed-legislation-would-bar-chinese-stem-graduate-students">Republicans propose legislation to bar Chinese from science</a> (<a href="https://mathstodon.xyz/@11011110/104248070190238732"></a>). I’m having difficulty distinguishing this sort of move from “<a href="https://en.wikipedia.org/wiki/Nuremberg_Laws">Nazis propose legislation to bar Jews from science</a>”.</p>
  </li>
  <li>
    <p><a href="https://wrog.dreamwidth.org/63735.html">Arthur C. Clarke and the projective plane</a> (<a href="https://mathstodon.xyz/@ColinTheMathmo/104253017415627794"></a>). Wrog beanplates Clarke’s “The Wall of Darkness” (1949).</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1112/blms/16.3.278">Volumes of projections of unit cubes</a> (<a href="https://mathstodon.xyz/@11011110/104261517104618440"></a>), Peter McMullen, <em>Bull LMS</em> 1984. A cute theorem that deserves to be better known: if you hold a unit cube in the noonday sun, at any angle, its shadow’s area equals its height (elevation difference between lowest and highest point). It follows immediately that the biggest possible shadow is a hexagon with area = long diagonal length = , and the smallest shadow is a unit square. Similar things happen in higher dimensions.</p>
  </li>
  <li>
    <p><a href="http://roberthodgin.com/project/meander">Meander, a procedural system for generating historical maps of rivers that never existed</a> (<a href="https://mathstodon.xyz/@11011110/104265448606504741"></a>, <a href="https://www.metafilter.com/187292/Meander-generating-historical-maps-of-rivers-that-never-existed">via</a>). The way this system models the motion of river beds over time looks a lot like the time-reversal of the curve-shortening flow, but with added tangential motion that causes bends to flow downstream (and maybe helps maintain smoothness) and with shortcutting of oxbows.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/05/31/linkage.html"><span class="datestr">at May 31, 2020 04:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/083">TR20-083 |  Proximity Gaps for Reed-Solomon Codes | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Yuval Ishai, 

	Swastik Kopparty, 

	Shubhangi Saraf</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A collection of sets displays a proximity gap with respect to some property if for every set in the collection, either (i) all members are $\delta$-close to the property in relative Hamming distance or (ii) only a tiny fraction of members are $\delta$-close to the property. In particular, no set in the collection has roughly half of its members $\delta$-close to the property and the others $\delta$-far from it.

We show that the collection of affine spaces displays a proximity gap with respect to Reed-Solomon (RS) codes, even over small fields, of size polynomial in the dimension of the code, and the gap applies to any $\delta$ smaller than the Johnson/Guruswami-Sudan list-decoding bound of the RS code. We also show near-optimal gap results, over fields of (at least) linear size in the RS code dimension, for $\delta$ smaller than the unique decoding radius. Finally, we discuss several applications of our proximity gap results to distributed storage, multi-party cryptographic protocols, and concretely efficient proof systems.

We prove the proximity gap results by analyzing the execution of classical algebraic decoding algorithms for Reed-Solomon codes (due to Berlekamp-Welch and Guruswami-Sudan) on a formal element of an affine space. This involves working with Reed-Solomon codes whose base field is an (infinite) rational function field. Our proofs are obtained by developing an extension (to function fields) of a strategy of Arora and Sudan for analyzing low-degree tests.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/083"><span class="datestr">at May 30, 2020 07:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17089">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/">Just Arvind</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Theory and practice</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/unknown-142/" rel="attachment wp-att-17098"><img width="180" alt="" class="alignright  wp-image-17098" src="https://rjlipton.files.wordpress.com/2020/05/unknown-2.jpeg?w=180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ MIT ]</font></td>
</tr>
</tbody>
</table>
<p>
Arvind Mithal—almost always referred to as Arvind—is now the head of the faculty of computer science at a Boston trade school. The school, also known as MIT, is of course one of the top places for all things computer science. From education to service to startups to research, MIT is perhaps the best in the world.</p>
<p>
Today I thought we might discuss one of Arvind’s main research themes.</p>
<p>
Although this work started in the 1980’s I believe that it underscores an important point. This insight might apply to current research topics like <a href="https://en.wikipedia.org/wiki/Timeline_of_quantum_computing">quantum computers</a>.</p>
<p>
But first I cannot resist saying something personal about him. Arvind is a long time friend of mine, and someone who gets the <i>stuck in elevator measure</i> of many hours. This measure is one I made up, but I hope you get the idea.</p>
<p>
Arvind is the only name I have ever known for Arvind. I was a bit surprised to see that the Wikipedia reference for <a href="https://en.wikipedia.org/wiki/Arvind_(computer_scientist)">him</a> states his “full” name. He told me that he found having one name—not two—was a challenge. For example, he sometimes had to explain when arriving at a check-in desk at a conference that he only had one name. His name tag often became: </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/tag/" rel="attachment wp-att-17092"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/tag.png?w=300&amp;h=222" class="aligncenter size-medium wp-image-17092" height="222" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>because the conference software could not handle people with one name. </p>
<p>
</p><p></p><h2> Dataflow—The prehistory </h2><p></p>
<p></p><p>
About forty years ago one of the major open problem in CS was how to make computers go faster. It is still an issue, but in the 1980’s this problem was one of all-hands-on-deck. It was worked on by software engineers, by electrical engineers, by researchers of all kinds including complexity theorists. Conferences like FOCS and STOC—hardcore theory conferences—often contained papers on how to speed up computations. </p>
<p>
Two examples come to mind:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The idea of <a href="https://en.wikipedia.org/wiki/Systolic_array">systolic</a> arrays led by Hsiang-Tsung Kung then at CMU. Also his students, especially Charles Leiserson, made important contributions. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The idea of the <a href="https://en.wikipedia.org/wiki/Ultracomputer">Ultracomputer</a> led by Jacob Schwartz at NYU. An ultracomputer has N processors, N memories and an N log N message-passing switch connecting them. Note, the use of “N” so you can <a href="https://rjlipton.wordpress.com/2020/05/22/math-tells/">tell</a> that it came from a theorist. </p>
<p>
Arvind tried to make computers faster by inventing a new type of computer architecture. Computers then were based on the classic von Neumann architecture or control flow architecture. He and his colleagues worked for many years trying to replace this architecture by <a href="https://en.wikipedia.org/wiki/Dataflow#Hardware_architecture">dataflow</a>. </p>
<p>
A bottleneck in von Neumann style machines is caused by the program counter fetch cycle. Each cycle the program counter decides which instruction to get, and thus which data to get. These use the same hardware channel, which causes the famous <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">von Neumann bottleneck</a>. We have modified Wikipedia’s graphic to make the bottleneck aspect clearer:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/neck2/" rel="attachment wp-att-17111"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/neck2.png?w=300&amp;h=204" class="aligncenter size-medium wp-image-17111" height="204" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p></p><h2> Dataflow—The promise </h2><p></p>
<p></p><p>
Arvind is usually identified with the invention of <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a191029.pdf">dataflow</a> computer architecture. The key idea of this architecture is to avoid the above bottleneck by eliminating the program counter. If there are no instructions to fetch, then it would seem that we can beat the bottleneck. Great idea.</p>
<p>
Dataflow architectures do not have a program counter, and so data is king. Roughly data objects move around in such a machine, and they eventually appear at computational units. Roughly these machines operate, at a high level, like a directed graph from complexity theory. Data moves along edges to nodes that compute values. Moreover, nodes compute as soon as all the input data is present. After the node computes the value, the new data is sent to the next node; and so on. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/flow-2/" rel="attachment wp-att-17094"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/flow.png?w=300&amp;h=229" class="aligncenter size-medium wp-image-17094" height="229" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The hope was that this would yield a way to increase the performance of computers. No program counter, no instruction fetching, could make dataflow machines faster. </p>
<p>
</p><p></p><h2> Dataflow—The actual </h2><p></p>
<p></p><p>
The dataflow idea is clever. The difficulty is while dataflow can work, classic von Neumann machines have been augmented in various ways. The competition rarely stays put. These updates did not eliminate the von Neumann bottleneck, but they reduce the cost of it, and let von Neumann machines continue to get faster. Caches are one example of how they avoid the bottleneck, thus making the dataflow machines less attractive. Thus, a cache for program instructions makes it likely that the program fetch step does not actually happen. This is an attack on the main advantage of dataflow machines. </p>
<p>
The <a href="http://www.cs.cornell.edu/tve/papers-ucb/limits.pdf">paper</a> by David Culler, Klaus Schauser, and Thorsten von Eicken titled <i>Two Fundamental Limits on Dataflow Multiprocessing?</i> discusses these issues in detail. They start by saying: </p>
<blockquote><p><b> </b> <em> The advantages of dataflow architectures were argued persuasively in a seminal 1983 paper by Arvind and Iannucci and in a 1987 revision entitled “Two Fundamental Issues in Multiprocessing”. However, reality has proved less favorable to this approach than their arguments would suggest. This motivates us to examine the line of reasoning that has driven dataflow architectures and fine-grain multithreading to understand where the argument went awry. </em>
</p></blockquote>
<p>
</p><p></p><h2> Dataflow—Lessons </h2><p></p>
<p></p><p>
What are the lessons? Is there one? </p>
<p>
I claim there are lessons. The work of Arvind on dataflow was and is important. It did not lead to the demise of von Neumann machines. I am using one right now to write this.</p>
<p>
Dataflow did lead to insights on programming that have many applications. The dataflow idea may yet impact special computational situations: there is interest in using them for data intensive applications.</p>
<p>Ken adds that dataflow has been realized in other ways. Caches and pipes and subsequent architecture innovations profit from designs that enhance <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality</a>. The <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a> programming model gives a general framework that fits this well. The paradigm of <a href="http://\href{https://en.wikipedia.org/wiki/Stream_processing">streaming</a> is even a better fit. Note that Wikipedia says both that it is “equivalent to dataflow programming” and “was explored within dataflow programming”—so perhaps the parent lost out to the wider adaptability of her children.</p>
<p>
I think there are lessons: Practical goals like making hardware go faster, are complex and many faceted. A pure theory approach such as systolic arrays or ultra computers or dataflow machines is unlikely to suffice. Also the existing technology like von Neumann machines will continue to evolve. </p>
<p>
A question is: Could quantum computers be subject to the same lesson? Will non-quantum machines continue to evolve in a way to make the quantum advantage less than we think? Or is this type of new architecture different? Could non-quantum machines incorporate tricks from quantum and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Speaking about theory and practice: Noga Alon, Phillip Gibbons, Yossi Matias, and Mario Szegedy are the winners of the 2019 ACM Paris Kanellakis Theory and Practice <a href="https://awards.acm.org/kanellakis">Award</a>. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/paris/" rel="attachment wp-att-17096"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/05/paris.png?w=300&amp;h=169" class="aligncenter size-medium wp-image-17096" height="169" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>We applaud them on receiving this award. Sadly the Paris award reminds us that Paris Kanellakis died in the terrible plane crash of American Airlines Flight 965 on December 20, 1995. Also on that flight were his wife, Maria Otoya, and their two children, Alexandra and Stephanos.</p>
<p>
The citation for the award says: </p>
<blockquote><p><b> </b> <em> They pioneered a framework for algorithmic treatment of streaming massive datasets, and today their sketching and streaming algorithms remain the core approach for streaming big data and constitute an entire subarea of the field of algorithms. </em>
</p></blockquote>
<p></p><p>
In short they invented streaming. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/05/29/just-arvind/"><span class="datestr">at May 29, 2020 09:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/05/29/liberation-from-grades/">Liberation from grades</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This semester, like many other universities, Harvard switched to a pass/fail grade model. (In typical Harvard style, we give them different names – “Emergency Satisfactory” and “Emergency Unsatisfactory” –  but that doesn’t matter much).</p>



<p>One unexpected but happy consequence of this policy is that even though I already submitted the grades for my <a href="https://cs127.boazbarak.org/schedule/">crypto course</a>, I can now take the time and send students detailed feedback on their final projects. Typically,  both students and faculty tend to be focused on the “bottom line” of exams or papers – what is the final grade. The comments are viewed as of marginal importance and only serve to justify why points have been deducted.</p>



<p>Now that there is no grade, I am actually giving many more comments on the write ups, trying to focus on giving students feedback on writing and presentation that will be useful for them later on. I benefited immensely from the extensive comments on my writing that I received from my advisor Oded Goldreich. While I will never match Oded’s thoroughness and dedication, I try to at least provide some of this to my students (though unlike Oded, I use blue and not red ink, and also do not intersperse the comments with Hebrew curses for emphasis <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> )</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/05/29/liberation-from-grades/"><span class="datestr">at May 29, 2020 04:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=439">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/05/28/tcs-talk-wednesday-june-3-michael-p-kim-stanford-university/">TCS+ talk: Wednesday, June 3 — Michael P. Kim, Stanford University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk (and penultimate of the season!) will take place this coming Wednesday, June 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Michael Kim</strong> from Stanford University will speak about “<em>Learning from Outcomes:  Evidence-Based Rankings</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br />
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In this work, we address the task of ranking members of a population according to their qualifications based on a training set of binary outcome data. A natural approach for ranking is to reduce to prediction: first learn to predict individuals’ “probability” of success; then rank individuals in the order specified by the predictions. A concern with this approach is that such rankings may be vulnerable to manipulation. The rank of an individual depends not only on their own qualification but also on every other individuals’ qualifications, so small inaccuracies in prediction may result in a highly inaccurate and unfair induced ranking. We show how to obtain rankings that satisfy a number of desirable accuracy and fairness criteria, despite the coarseness of binary outcome data.<br />
We develop two parallel definitions of evidence-based rankings. First, we study a semantic notion of <em>domination-compatibility</em>: if the training data suggest that members of a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> are on-average more qualified than the members of <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" />, then a ranking that favors <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" /> over <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> (i.e., where <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" alt="T" class="latex" title="T" /> dominates <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />) is blatantly inconsistent with the data, and likely to be discriminatory. Our definition asks for domination-compatibility, not just for a pair of sets (e.g., majority and minority populations), but rather for every pair of sets from a rich collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> of subpopulations. The second notion—evidence-consistency—aims at precluding even more general forms of discrimination: the ranking must be justified on the basis of consistency with the expectations for every set in the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" />. Somewhat surprisingly, while evidence-consistency is a strictly stronger notion than domination-compatibility when the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> is predefined, the two notions are equivalent when the collection <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\mathcal{C}" class="latex" title="\mathcal{C}" /> may depend on the ranking itself. Finally, we show a tight connection between evidence-based rankings and multi-calibrated predictors [HKRR’18]. This connection establishes a way to reduce the task of ranking to prediction that ensures strong guarantees of fairness in the resulting ranking.<br />
Joint work with Cynthia Dwork, Omer Reingold, Guy N. Rothblum, and Gal Yona. Appeared at FOCS 2019.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/05/28/tcs-talk-wednesday-june-3-michael-p-kim-stanford-university/"><span class="datestr">at May 28, 2020 11:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/05/28/polymath-reu/">Polymath REU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I am excited to announce a new program for undergraduates: The Polymath REU will run during the summer of 2020. Due to the pandemic, many students are stuck at home without a summer program. The aim of the Polymath REU program is to provide research opportunities for such students. The program consists of research projects […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/05/28/polymath-reu/"><span class="datestr">at May 28, 2020 04:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=749">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/05/26/fast-stuff/">Fast stuff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>5-second UFC <a href="https://www.youtube.com/watch?v=_rCTmFxQ4pM">knockout</a></p>



<p><a href="https://www.youtube.com/watch?v=gqyAc4VnObc">Bullet chess</a></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/05/26/fast-stuff/"><span class="datestr">at May 26, 2020 12:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4816">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4816">The Collapsing Leviathan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I was seriously depressed for the last week, by noticeably more than my baseline amount for the new pandemic-ravaged world.  The depression seems to have been triggered by two pieces of news:</p>



<ol><li>The US Food and Drug Administration—yes, the same FDA whose failure to approve covid tests in February infamously set the stage for the deaths of 100,000 Americans—has now <em>also</em> <a href="https://www.nytimes.com/2020/05/15/us/coronavirus-testing-seattle-bill-gates.html">banned</a> the Gates Foundation’s program for at-home covid testing.  This, it seems to me, is not the sort of thing that could happen in a still-functioning society, one where people valued their own and their neighbors’ physical survival, and viewed rules and regulations as merely instruments to that end.  It’s the sort of thing that one imagines in the waning years of a doomed empire, when no one pretends anymore that they can fix or improve the Leviathan; they’re all just scurrying to flee the Leviathan as it collapses with a thud.  More broadly, I <em>still</em> don’t think that the depth of America’s humiliation and downfall has sunk in to most Americans.  For me, it starts and ends with a single observation: <strong>where fifty years ago we landed humans on the moon, today we can no longer make or distribute paper masks, even when hundreds of thousands of lives depend on it.</strong>  Look, there are many countries, like Taiwan and New Zealand, that managed to protect both their economies <em>and</em> their vulnerable citizens’ lives, by crushing the virus early.  Then there are countries that waited, until they faced an excruciating choice between the two.  But here in the US, we’ve somehow achieved the worst of both worlds—triggering a second Great Depression while <em>also</em> utterly failing to control the virus.  Can we abandon the charade of treating this as a legible “policy choice,” to be debated in earnest thinkpieces?  To me, it just feels like the death-spasm of a collapsing Leviathan.</li><li>Something that, at first glance, might seem trivial by comparison, but isn’t: the University of California system—ignoring the <a href="https://senate.universityofcalifornia.edu/_files/reports/kkb-jn-standardized-testing.pdf">advice</a> of its own Academic Senate, and at the apparent insistence of its chancellor Janet Napolitano—will now <a href="https://timesofsandiego.com/education/2020/05/24/the-story-behind-university-of-californias-landmark-decision-to-ditch-the-sat/">permanently end</a> the use of the SAT and ACT in undergraduate admissions.  This is widely expected, probably correctly, to trigger a chain reaction, whereby one US university after the next will abandon standardized tests.  As a result, admissions to the top US universities—and hence, most chances for social advancement in the US—will henceforth be based <em>entirely</em> on shifting and nebulous criteria that rich, well-connected kids and their parents spend most of their lives figuring out, rather than merely <em>mostly</em> based on such criteria.  The last side door for smart noncomformist kids is now being slammed shut.  From now on, in the US, the <em>only</em> paths to success that clearly delineate their rules will be sports, gambling, reality TV, and the like.  In case it matters to anyone reading this, I feel certain that a 15-year-old me wouldn’t stand a chance in the emerging regime—any more than nerdy Jewish kids did in the USSR of the 1970s, or the US of the 1920s.  (As I’ve <a href="https://www.scottaaronson.com/blog/?p=2003">previously recounted</a> on this blog, the US’s “holistic” college admissions system, with its baffling-to-foreigners emphasis on “character,” “leadership,” “well-roundedness,” etc. rather than test scores, originated in a successful push a century ago by the presidents of Harvard, Princeton, and Yale to keep Jewish enrollments down.  Today the system fulfills precisely the same function, except against Asian-Americans rather than Jews.)  Ironically but predictably, the death of the SAT—i.e., of one of the most fearsome weapons against entrenched wealth and power ever devised—is being <em>celebrated</em> by the self-described champions of the underdog.  I have one question for those champions: do you not understand what your system will <em>actually</em> do to society’s underdogs?  Or do you understand perfectly well, and approve?</li></ol>



<p>To put it bluntly—since events like these leave no room for euphemism—a hundred thousand Americans are now dead from covid, and hundreds of thousands more are poised to die, because smart people are no longer in charge.  And the death of the SAT will help ensure that smart people will never be <em>back</em> in charge.  Obama might be remembered by history as America’s last smart-person-in-charge, its last competent technocrat—but one man couldn’t stop a tidal wave of stupid.</p>



<p>I know from experience what many will readers will say to all this: “instead of wallowing in gloom, Scott, why don’t you just make falsifiable predictions about the bad outcomes you expect from these developments, and then score yourself later?”</p>



<p>So here’s the thing about that.</p>



<p>Shortly after Trump was elected, I changed this blog’s background to black, as a small way to mourn the United States that I’d grown up thinking that I lived in, the one that had at least some ideals.  Today, with four years of hindsight, my thinking then feels <em>overly optimistic</em>: why plain black?  Why not, like, images of rotting corpses in a pit?</p>



<p>And yet, were I foolish enough to register predictions in 2016, I would’ve said that within one year, Trump’s staggering incompetence would <em>surely</em> cause some catastrophe or other to grip the country—a really obvious one, with mass death and even Trump’s beloved stock market cratering.</p>



<p>And then after a year, commenters would ridicule me, because none of that had happened.  After two years, they’d ridicule me again because it <em>still</em> hadn’t happened, and after three years they’d ridicule me a third time.</p>



<p>Now it’s happened.</p>



<p>America, we now know, is like the cartoon character who runs off a cliff: it dangled in midair for three years, defying physics, before it finally looked down.</p>



<p>Look, I’m a theoretical computer scientist.  By training, I deal in asymptotics, not in constant factors.  I don’t often make predictions with deadlines; when I do, I often regret it.  It’s a good thing that I became an academic rather than an investor!  For I’ve learned that the only “oracular power” I have is to make statements like:</p>



<blockquote class="wp-block-quote"><p>My eyes, my brain, and the pit of my stomach are all blaring at me that the asymptotics of this situation just took a sharp turn for the worse.  Sure, for an unknown length of time, noise and constant factors could mask the effects.  But eventually, either (1) society will need to reverse what it just did, or else (2) terrible effects will spring from it, or else (3) the entire universe no longer makes sense.</p></blockquote>



<p>When I’ve felt this way in the past, option (3) rarely turned out to be the right answer.</p>



<p>So, what can anyone say that will make me less depressed?  Thanks in advance!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (May 30): </span></strong>Woohoo!!  Avoiding yet another tragedy, after years of setbacks and struggles, it looks like today the US has finally launched humans into orbit, thereby recapitulating a technological achievement from 1961 that the US had already vastly surpassed by 1969.  I hereby retract the pessimism of this post.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4816"><span class="datestr">at May 26, 2020 12:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3492">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/05/25/guide-to-virtual-ec-2020/">Guide to Virtual EC 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="font-weight: 400;"><i>From the Virtual Transition Team for EC 2020:</i> Due to concerns regarding the novel coronavirus COVID-19, the 2020 ACM Conference on Economics and Computation (EC 2020) will be held virtually.  This change of format offers exciting new opportunities. This guide can also be found on the </span><a href="http://ec20.sigecom.org/participation/covid/"><span style="font-weight: 400;">EC 2020 webpage</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Overview</span></h2>
<p><b>June 15 – 19:</b><span style="font-weight: 400;"> Mentoring Workshop and Live Tutorial Pre-recording Sessions.</span><br />
<b>June 22 – July 3:</b><span style="font-weight: 400;"> Live EC Paper Pre-recording Plenary Sessions.</span><br />
<b>July 13:</b><span style="font-weight: 400;"> Tutorial Watch Parties, Business Meeting and Poster Session</span><br />
<b>July 14 – 16:</b><span style="font-weight: 400;"> EC Conference (Paper Watch Parties, Paper Poster Sessions, and Plenaries).</span><br />
<b>July 17 – 22:</b><span style="font-weight: 400;"> Workshops.</span></p>
<h2><span style="font-weight: 400;">Philosophy</span></h2>
<p><span style="font-weight: 400;">The planning committee has been hard at work revisioning EC 2020 as a virtual event.  The event aims to emphasize opportunities afforded by the virtual format with activities that are intractable in the physical format, but not to recreate aspects of the physical conference experience that are difficult virtually.  Some aspects of the event will look similar to a physical conference, while some will be quite different.  The desiderata for the selected format are:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize exposure for EC papers.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Maximize interaction between community members.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Allow broad global accessibility.</span></li>
</ul>
<p><span style="font-weight: 400;">The organizational team studied the results of many virtual conferences that have already taken place (</span><a href="https://aamas2020.conference.auckland.ac.nz/"><span style="font-weight: 400;">AAMAS</span></a><span style="font-weight: 400;">, </span><a href="https://cacm.acm.org/blogs/blog-cacm/243882-the-asplos-2020-online-conference-experience/fulltext"><span style="font-weight: 400;">ASPLOS</span></a><span style="font-weight: 400;">, </span><a href="https://www.eurosys2020.org/"><span style="font-weight: 400;">EuroSys</span></a><span style="font-weight: 400;">, </span><a href="https://iccp2020.engr.wustl.edu/vsetup.html?fbclid=IwAR3ONLEftxPsTmW4b1oTyCjvAFWWOxWRWS4bV6kTox1yP8AUpgSAbkL76Qc"><span style="font-weight: 400;">ICCP</span></a><span style="font-weight: 400;">, </span><a href="https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3"><span style="font-weight: 400;">ICLR</span></a><span style="font-weight: 400;">, </span><a href="https://www.daniellitt.com/blog/2020/4/20/wagon-lessons-learned"><span style="font-weight: 400;">WAGON</span></a><span style="font-weight: 400;">, among others, as well as the </span><a href="https://people.clarkson.edu/~jmatthew/acm/VirtualConferences_GuideToBestPractices_CURRENT.pdf"><span style="font-weight: 400;">ACM guidelines</span></a><span style="font-weight: 400;">).  Some general rules of thumb are (a) prerecording talks, (b) emphasizing posters, (c) expecting about four hours a day of participation, (d) prime time is 11am-3pm ET.  </span></p>
<p><span style="font-weight: 400;">The planning committee adopted a strategy that allows the minimal commitment level for authors and attendees of presenting and viewing 18 minute talks and attending activities only during the main EC week of July 13-17.  However, official EC events will run June 15 to July 24 and the planning committee highly encourages members of the community to enjoy a more relaxed pace where all programming is plenary.</span></p>
<h2><span style="font-weight: 400;">Registration</span></h2>
<p><span style="font-weight: 400;">Registration will be mandatory but complementary with ACM SIGecom membership ($10 Professional / $5 Student) which can easily be completed online.  Registration will open on June 1.</span></p>
<h2><span style="font-weight: 400;">Mentoring Workshop and Live Tutorial Pre-recording Sessions</span></h2>
<p><b>June 15-19:</b><span style="font-weight: 400;"> To give junior researchers plenty of time to prepare for the EC conference, the mentoring workshops and tutorial live pre-recording sessions will be held the week of June 15.  In addition to the usual activities, junior students will be paired with senior students who will share their EC itinerary and be available for discussions of papers and events in online chat throughout the duration of the EC events.  There will be three tutorials, each broken into four 45 minute segments and recorded over several days and with live audiences of EC participants.  Students will be able to work together on exercises in between sessions.</span></p>
<h2><span style="font-weight: 400;">Live Paper Pre-recording Plenary Sessions</span></h2>
<p><b>June 22 – July 3:</b><span style="font-weight: 400;">  EC papers will keep the usual 18-minute format.  The planning committee highly encourages authors to choose to pre-record their EC talks in live pre-recording sessions.  Each session comprises three papers and is followed by a virtual coffee break for discussion between the speakers, coauthors, and attendees.  Speakers and attendees are recommended to schedule for 2 hours.  These sessions are all plenary and will be scheduled for synergies in topic and preferred timing of the speakers.  These sessions begin at regular times:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Eastern/China 9:00, 13:00, 17:00, 21:00, 1:00, 5:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Central/Israel  8:00, 12:00, 16:00, 20:00, 24:00, 4:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Mountain/Europe 7:00, 11:00, 15:00, 19:00, 23:00, 3:00.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">US Pacific/London  6:00, 10:00, 14:00, 18:00, 22:00, 2:00.</span></li>
</ul>
<p><span style="font-weight: 400;">We expect about three sessions a day over the ten weekdays between June 22 and July 3.  (Pre-recording sessions are optional but highly encouraged for authors of EC papers.)</span></p>
<h2><span style="font-weight: 400;">EC Conference</span></h2>
<p><b>July 13-16: </b><span style="font-weight: 400;">July 13 is tutorial day.  It will include the EC business meeting and a contributed poster session for breaking results and results from other venues.  The main conference runs July 14-July 16 with programming from 9am-5pm Eastern.  This programming includes pre-recorded talk watch parties, poster discussion sessions for the papers, live plenary talks, and live highlights beyond EC.  The paper and poster discussion sessions will run in two-hour blocks as follows:</span></p>
<p><span style="font-weight: 400;">Hour 1: 3-5 parallel tracks of watch parties (3 papers each), with realtime chat with the authors.</span></p>
<p><span style="font-weight: 400;">Hour 2: plenary 1-minute lightning talks for all papers, breakout room for discussion for each paper with poster (a poster is 1-4 slides in PDF). (The lightning talks and poster discussion rooms are optional but highly encouraged for authors of the session’s papers.)</span></p>
<p><span style="font-weight: 400;">As these watch parties will be in parallel sessions and some may be at times that are hard for some members of the community to attend, the planning committee highly encourages participation in the live pre-recording sessions (June 22-July 3) and all talks will be available for individual viewing in advance of the conference. </span></p>
<h2><span style="font-weight: 400;">EC Workshops</span></h2>
<p><b>July 17-22:</b><span style="font-weight: 400;"> Three workshops will take place on the Friday of EC and the following week.  Details for these events are still being worked out by the organizers.</span></p>
<h2><span style="font-weight: 400;">Best Presentation by Student or Postdoctoral Research Award.</span></h2>
<p><span style="font-weight: 400;">The</span><a href="http://www.sigecom.org/award-presentation.html"><span style="font-weight: 400;"> Best Presentation by a Student or Postdoctoral Researcher Award</span></a><span style="font-weight: 400;"> is designed to encourage and acknowledge excellence in oral presentations by students and recent graduates. In addition to an honorarium, this year’s winner will be invited to present in a special session at WINE 2020 with expenses covered by SIGecom. </span><span style="font-weight: 400;">To be considered for the award, the presenter must participate in a live pre-recording session. </span><span style="font-weight: 400;"> This year will feature several “fun” categories for video presentations. Audience nominations are requested for creative and fun videos in the categories of: Best Video, Best Cameo by a Child or Pet, Best Special Effects, Best Soundtrack, or Best [Insert Your Nomination Here]. Submit nominations </span><a href="https://forms.gle/bfWEqDwseGHnciUP6"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Virtual Transition Team</span></h2>
<p><span style="font-weight: 400;">Virtual aspects of the conference are being coordinated by a Virtual Transition Team appointed by the SIGecom Executive Committee, that in addition to the </span><a href="http://sigecom.org/officers.html"><span style="font-weight: 400;">Executive Committee</span></a><span style="font-weight: 400;"> and the </span><a href="http://ec20.sigecom.org/committees-acm/organizing-committee/"><span style="font-weight: 400;">EC 2020 organizing committee</span></a><span style="font-weight: 400;"> includes the following officers:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual General Chair: Jason Hartline</span></li>
</ul>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Local Chair: Yannai Gonczarowski</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Virtual Global Outreach Chairs: Rediet Abebe and Eric Sodomka</span></li>
</ul>
<p><span style="font-weight: 400;">The team is looking forward to interacting with everyone in the community at an outstanding conference!</span></p></div>







<p class="date">
by Jason Hartline <a href="https://agtb.wordpress.com/2020/05/25/guide-to-virtual-ec-2020/"><span class="datestr">at May 25, 2020 09:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1531592307894359428">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html">Oldest Living Baseball Players- can you estimate...</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
(The Baseball season is delayed or cancelled, so I post about baseball instead.)<br />
<br />
This post is going to ask a question that you could look up on the web. But what fun with that be?<br />
<br />
The following statements are true<br />
<br />
1) Don Larsen, a professional baseball player who played from 1953 to 1967, is still alive. He is 90 years old (or perhaps 90 years young---I don't know the state of his health).  He was born Aug 7, 1929. He is best know for pitching a perfect game in the World Series in 1956, pitching for the Yankees. He played for several other teams as well, via trades (this was before free agency).<br />
(CORRECTION- I wrote this post a while back, and Don Larsen has died since then.)<br />
<br />
<br />
2) Whitey Ford, a professional baseball player who played from 1950 to 1967, is still alive. He is 91 years old (or perhaps 91 years young---I don't know the state of his health).  He was born Oct 21,  1928. He had many great seasons and is in the hall of fame. He played for the New York Yankees and no other team.<br />
<br />
3) From 1900 (or so) until 1962 there were 16 professional baseball teams which had 25 people each. From 1962 until 1969 there were 20 teams which had 25 people each. There were also many minor league teams.<br />
<br />
4) The youngest ballplayers are usually around 20. The oldest around 35. These are not exact numbers<br />
<br />
SO here is my question: Try to estimate<br />
<br />
1) How many LIVING  retired major league baseball players are there now who are older than Don Larsen?<br />
<br />
2) How many LIVING retired major league baseball players are of an age between Don and Whitey?<br />
<br />
3) How  many LIVING retired major league baseball players are older than Whitey Ford?<br />
<br />
Give your REASONING for your answer.<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/05/oldest-living-baseball-players-can-you.html"><span class="datestr">at May 25, 2020 04:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/">Proof of the Diagonal Lemma in Logic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Why is the proof so short yet so difficult?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/saeed_salehi4/" rel="attachment wp-att-17078"><img width="200" alt="" class="alignright  wp-image-17078" src="https://rjlipton.files.wordpress.com/2020/05/saeed_salehi4.jpg?w=200" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Saeed Salehi is a logician at the University of Tabriz in Iran. Three years ago he gave a <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> at a Moscow workshop on proofs of the diagonal lemma.</p>
<p>
Today I thought I would discuss the famous <a href="https://en.wikipedia.org/wiki/Diagonal_lemma">diagonal lemma</a>. </p>
<p>
The lemma is related to Georg Cantor’s famous diagonal argument yet is different. The logical version imposes requirements on when the argument applies, and requires that it be expressible within a formal system. </p>
<p>
The lemma underpins Kurt Gödel’s famous 1931 <a href="http://www.w-k-essler.de/pdfs/goedel.pdf">proof</a> that arithmetic is incomplete. However, Gödel did not state it as a lemma or proposition or theorem or anything else. Instead, he focused his attention on what we now call Gödel numbering. We consider this today as “obvious” but his paper’s title ended with “Part I”. And he had readied a “Part II” with over 100 pages of calculations should people question that his numbering scheme was expressible within the logic. </p>
<p>
Only after his proof was understood did people realize that one part, perhaps the trickiest part, could be abstracted into a powerful lemma. The tricky part is <em>not</em> the Gödel numbering. People granted that it can be brought within the logic once they saw enough of Gödel’s evidence, and so we may write <img src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ulcorner \phi \urcorner}" class="latex" title="{\ulcorner \phi \urcorner}" /> for the function giving the Gödel number of any formula <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> and use that in other formulas. The hard part is what one <em>does</em> with such expressions. </p>
<p>
This is what we will try to motivate.</p>
<p>
</p><p></p><h2> Tracing the Lemma </h2><p></p>
<p></p><p>
Rudolf Carnap is often credited with the first formal statement, in 1934, for instance by Eliott Mendelson in his famous <a href="https://www.goodreads.com/book/show/250868.Introduction_to_Mathematical_Logic">textbook</a> on logic. Carnap was a <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap">member</a> of the <a href="https://en.wikipedia.org/wiki/Vienna_Circle">Vienna Circle</a>, which Gödel frequented, and Carnap is <a href="http://texts.cdlib.org/view?docId=hb6h4nb3q7&amp;doc.view=frames&amp;chunk.id=div00004&amp;toc.depth=1&amp;toc.id=">considered</a> a giant among twentieth-century philosophers. He worked on sweeping grand problems of philosophy, including logical positivism and analysis of human language via syntax before semantics. Yet it strikes us with irony that his work on the lemma may be the best remembered.</p>
<p>
Who did the lemma first? Let’s leave that for others and move on to the mystery of how to prove the lemma once it is stated. I must say the lemma is easy to state, easy to remember, and has a short proof. But I believe that the proof is not easy to remember or even follow. </p>
<p>
Salehi’s <a href="http://wrm17.mi.ras.ru/slides/Salehi.pdf">presentation</a> quotes others’ opinions about the proof:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> Sam Buss: “Its proof [is] quite simple but rather tricky and difficult to conceptualize.”</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> György Serény (we jump to Serény’s <a href="https://arxiv.org/pdf/math/0606425.pdf">paper</a>): “The proof of the lemma as it is presented in textbooks on logic is not self-evident to say the least.”</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> Wayne Wasserman: “It is `Pulling a Rabbit Out of the Hat’—Typical Diagonal Lemma Proofs Beg the Question.”</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/hat/" rel="attachment wp-att-17079"><img src="https://rjlipton.files.wordpress.com/2020/05/hat.png?w=600" alt="" class="aligncenter size-full wp-image-17079" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
So I am not alone, and I thought it might be useful to try and unravel its proof. This exercise helped me and maybe it will help you.</p>
<p>
Here goes. </p>
<p>
</p><p></p><h2> Stating the Lemma </h2><p></p>
<p></p><p>
Let <img src="https://s0.wp.com/latex.php?latex=%7BS%28w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S(w)}" class="latex" title="{S(w)}" /> be a formula in Peano Arithmetic (<img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" />). We claim that there is some sentence <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> so that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " /></p>
<p>Formally, </p>
<blockquote><p><b>Lemma 1</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S(x)}" class="latex" title="{S(x)}" /> is some formula in <img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" />. Then there is a sentence <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> so that 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++PA+%5Cvdash+%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " class="latex" title="\displaystyle  PA \vdash \phi \iff S(\ulcorner \phi \urcorner). " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
The beauty of this lemma is that it was used by Gödel and others to prove various powerful theorems. For example, the lemma quickly proves this result of Alfred Tarski:</p>
<blockquote><p><b>Theorem 2</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" /> is consistent. Then <i>truth</i> cannot be defined in <img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" />. That is there is <b>no</b> formula <img src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Tr(x)}" class="latex" title="{Tr(x)}" /> so that for all sentences <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> <img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" /> proves 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). " class="latex" title="\displaystyle  \phi \iff Tr(\ulcorner \phi \urcorner). " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
The proof is this. Assume there is such a formula <img src="https://s0.wp.com/latex.php?latex=%7BTr%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Tr(x)}" class="latex" title="{Tr(x)}" />. Then use the diagonal lemma and get 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)." class="latex" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner)." /></p>
<p>This shows that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+%5Cneg+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29+%5Ciff+Tr%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). " class="latex" title="\displaystyle  \phi \iff \neg Tr(\ulcorner \phi \urcorner) \iff Tr(\ulcorner \phi \urcorner). " /></p>
<p>This is a contradiction. A short proof. </p>
<p>
</p><p></p><h2> The Proof </h2><p></p>
<p></p><p>
The key is to define the function <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(n)}" class="latex" title="{F(n)}" /> as follows: Suppose that <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is the Gödel number of a formula of the form <img src="https://s0.wp.com/latex.php?latex=%7BA%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A(x)}" class="latex" title="{A(x)}" /> for some variable <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28n%29+%3D+%5Culcorner+A%28%5Culcorner+A%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. " class="latex" title="\displaystyle  F(n) = \ulcorner A(\ulcorner A(x) \urcorner) \urcorner. " /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is not of this form then define <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(n)=0}" class="latex" title="{F(n)=0}" />. This is a strange function, a clever function, but a perfectly fine function, It certainly maps numbers to numbers. It is certainly recursive, actually it is clearly computable in polynomial time for any reasonable Gödel numbering. Note: the function <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> does depend on the choice of the variable <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Thus, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+y%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+y%3D0+%5Curcorner%29%3D0+%5Curcorner%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, " class="latex" title="\displaystyle  F(\ulcorner y=0 \urcorner) = \ulcorner (\ulcorner y=0 \urcorner)=0 \urcorner, " /></p>
<p>and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+x%3D0+%5Curcorner%29+%3D+%5Culcorner+%28%5Culcorner+x%3D0+%5Curcorner%29%3D0+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. " class="latex" title="\displaystyle  F(\ulcorner x=0 \urcorner) = \ulcorner (\ulcorner x=0 \urcorner)=0 \urcorner. " /></p>
<p>	          Now we make two definitions:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " /></p>
<p>
Now we compute just using the definitions of <img src="https://s0.wp.com/latex.php?latex=%7BF%2C+g%2C+%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F, g, \phi}" class="latex" title="{F, g, \phi}" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29+%5C%5C+++++++++++%26%3D%26+S%28%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%5C%5C+++++++++++++++%26%3D%26+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)) \\           &amp;=&amp; S(\ulcorner g(\ulcorner g(x) \urcorner) \urcorner) \\               &amp;=&amp; S(\ulcorner \phi \urcorner). \end{array} " /></p>
<p>We are done.</p>
<p>
</p><p></p><h2> But … </h2><p></p>
<p></p><p>
Where did this proof come from? Suppose that you forgot the proof but remember the statement of the lemma. I claim that we can then reconstruct the proof. </p>
<p>
First let’s ask: Where did the definition of the function <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> come from? Let’s see. Imagine we defined </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++g%28w%29+%26%5Cequiv%26+S%28F%28w%29%29+%5C%5C++++++++%5Cphi+%26%5Cequiv%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}        g(w) &amp;\equiv&amp; S(F(w)) \\        \phi &amp;\equiv&amp; g(\ulcorner g(x) \urcorner). \end{array} " /></p>
<p>But left <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> undefined for now. Then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++%5Cphi+%26%3D%26+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5C%5C+++++++++++++++++%26%3D%26+S%28F%28%5Culcorner+g%28x%29+%5Curcorner%29%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}        \phi &amp;=&amp; g(\ulcorner g(x) \urcorner) \\                 &amp;=&amp; S(F(\ulcorner g(x) \urcorner)). \end{array} " /></p>
<p>But we want <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi+%3D+S%28%5Culcorner+%5Cphi+%5Curcorner%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi = S(\ulcorner \phi \urcorner)}" class="latex" title="{\phi = S(\ulcorner \phi \urcorner)}" /> that happens provided:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner%29+%3D+F%28%5Culcorner+g%28x%29+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). " class="latex" title="\displaystyle  \ulcorner g(\ulcorner g(x) \urcorner) \urcorner) = F(\ulcorner g(x) \urcorner). " /></p>
<p>This essentially gives the definition of the function <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. Pretty neat.</p>
<p>
</p><p></p><h2> But but … </h2><p></p>
<p></p><p>
Okay where did the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> come from? It is reasonable to define 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28w%29+%5Cequiv+S%28F%28w%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(w) \equiv S(F(w)), " class="latex" title="\displaystyle  g(w) \equiv S(F(w)), " /></p>
<p>for some <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. We cannot change <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> but we can control the input to the formula <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, so let’s put a function there. Hence the definition for <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> is not unreasonable. </p>
<p>
Okay how about the definition of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" />? Well we could argue that this is the magic step. If we are given this definition then <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> follows, by the above. I would argue that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> is not completely surprising. The name of the lemma is after all the “diagonal” lemma. So defining <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> as the application of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to itself is plausible.</p>
<p>
</p><p></p><h2> Taking an Exam </h2><p></p>
<p></p><p>
Another way to think about the diagonal lemma is imagine you are taking an exam in logic. The first question is: </p>
<blockquote><p><b> </b> <em> Prove in <img src="https://s0.wp.com/latex.php?latex=%7BPA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{PA}" class="latex" title="{PA}" /> that for any <img src="https://s0.wp.com/latex.php?latex=%7BS%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S(x)}" class="latex" title="{S(x)}" /> there is a sentence <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> so that 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%5Ciff+S%28%5Culcorner+%5Cphi+%5Curcorner%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). " class="latex" title="\displaystyle  \phi \iff S(\ulcorner \phi \urcorner). " /></p>
</em><p><em></em>
</p></blockquote>
<p>You read the question again and think: “I wish I had studied harder, I should have not have checked Facebook last night. And then went out and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" />” But you think let’s not panic, let’s think.</p>
<p>
Here is what you do. You say let me define 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+S%28F%28x%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x) = S(F(x)), " class="latex" title="\displaystyle  g(x) = S(F(x)), " /></p>
<p>for some <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. You recall there was a function that depends on <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, and changing the input from <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x)}" class="latex" title="{F(x)}" /> seems to be safe. Okay you say, now what? I need the definition of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. Hmmm let me wait on that. I recall vaguely that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> had a strange definition. I cannot recall it, so let me leave it for now.</p>
<p>
But you think: I need a sentence <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" />. A sentence cannot have an unbound variable. So <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi}" class="latex" title="{\phi}" /> cannot be <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(x)}" class="latex" title="{g(x)}" />. It could be <img src="https://s0.wp.com/latex.php?latex=%7Bg%28m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(m)}" class="latex" title="{g(m)}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" />. But what could <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> be? How about <img src="https://s0.wp.com/latex.php?latex=%7B%5Culcorner+%5Cphi+%5Curcorner%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ulcorner \phi \urcorner}" class="latex" title="{\ulcorner \phi \urcorner}" />. This makes 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%3D+g%28%5Culcorner+g+%5Curcorner%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi = g(\ulcorner g \urcorner). " class="latex" title="\displaystyle  \phi = g(\ulcorner g \urcorner). " /></p>
<p>It is after all the diagonal lemma. Hmmm does this work. Let’s see if this works. Wait as above I get that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is now forced to satisfy 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28%5Culcorner+g%28x%29+%5Curcorner%29+%3D+%5Culcorner+g%28%5Culcorner+g%28x%29+%5Curcorner%29+%5Curcorner.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. " class="latex" title="\displaystyle  F(\ulcorner g(x) \urcorner) = \ulcorner g(\ulcorner g(x) \urcorner) \urcorner. " /></p>
<p>Great this works. I think this is the proof. Wonderful. Got the first question. </p>
<p>
Let’s look at the next exam question. Oh no <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Does this help? Does this unravel the mystery of the proof? Or is it still magic?</p>
<p></p><p><br />
[Fixed equation formatting]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/05/25/proof-of-the-diagonal-lemma-in-logic/"><span class="datestr">at May 25, 2020 03:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/benchmarks/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/benchmarks/">From ImageNet to Image Classification</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2005.11295" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/ImageNetMultiLabel" class="bbutton">
<i class="fab fa-github"></i>
   Data
</a>
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2005.11295">new paper</a>, we explore how closely
the ImageNet benchmark aligns with the object recognition task it serves as a
proxy for. We find pervasive and systematic deviations of ImageNet annotations
from the ground truth, which can often be attributed to specific design choices
in the data collection pipeline. These issues indicate that ImageNet accuracy
alone might be insufficient to effectively gauge real model performance.  </i></p>

<h2 id="contextualizing-progress-on-benchmarks">Contextualizing Progress on Benchmarks</h2>

<p>Large-scale benchmarks are central to machine learning—they serve both
as concrete targets for model development, and as proxies for assessing model
performance on real-world tasks we actually care about. However, few benchmarks
are perfect, and so as our models get increasingly better at them, we must also
ask ourselves: <i>to what extent is performance on existing benchmarks
indicative of progress on the real-world tasks that motivate them?</i></p>

<p>In this post, we will explore this question of <i>benchmark-task alignment</i>
in the context of the popular
<a href="http://image-net.org/challenges/LSVRC/2012/">ImageNet object recognition dataset</a>.
Specifically, our goal is to understand how well the underlying ground truth is
captured by the dataset itself—this dataset is, after all, what we consider
to be the gold standard during model training and evaluation.</p>

<h3 id="a-sneak-peak-into-imagenet">A sneak peak into ImageNet</h3>
<p>The ImageNet dataset contains over a million images of objects from a thousand,
quite diverse classes. Like many other benchmarks of that scale, ImageNet was
not carefully curated by experts, but instead created via crowd-sourcing,
without perfect quality control. So what does ImageNet data look like? Here are
a few image-label pairs from the dataset:</p>

<div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/0.jpg" />
      <div class="fake_label">missile</div>
      <div class="true_label">projectile</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/1.jpg" />
      <div class="fake_label">stage</div>
      <div class="true_label">acoustic guitar</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/2.jpg" />
      <div class="fake_label">monastery</div>
      <div class="true_label">church</div>
    </div>
    <div class="fake_img"> 
      <img src="https://gradientscience.org/assets/multilabel/anec/3.jpg" />
      <div class="fake_label">Norwich terrier</div>
      <div class="true_label">Norfolk terrier</div>
    </div>
</div>
<p><br /></p>

<p>These samples appear pretty reasonable…but are they? Actually, while these
are indeed <i>images</i> from the dataset, the labels shown above are
<i>not</i> their actual ImageNet labels!
<a href="https://gradientscience.org/" id="reveal">[Click to see the actual ImageNet labels]</a>
Still, even though not “correct” from the point of view of the ImageNet
dataset, these labels <i>do</i> correspond to actual ImageNet classes, and
appear plausible when you see them in isolation. This shows that for ImageNet
images, which capture objects in diverse real-world conditions, the ImageNet
label may not properly reflect the ground truth.</p>

<p>In our work, we dive into examining how this label misalignment actually
impacts ImageNet: how often do ImageNet labels deviate from the ground truth?
And how do shortcomings in these labels impact ImageNet-trained models?</p>

<h3 id="revisiting-the-imagenet-collection-pipeline">Revisiting the ImageNet collection pipeline</h3>
<p>Before going further, let’s take a look at how ImageNet was created. To build
such a large dataset, the creators of ImageNet had to leverage scalable methods
like automated data collection and crowd-sourcing. That is, they first selected
a set of object classes (using the <a href="https://wordnet.princeton.edu">WordNet</a>
hierarchy), and queried various search engines to obtain a pool of candidate
images. These candidate images were then verified by annotators on <a href="https://www.mturk.com/">Mechanical
Turk (MTurk)</a> using (what we will refer to as) the
<span class="sc">Contains</span> task: annotators were shown images retrieved
for a specific class
label, and were subsequently asked to select the ones that actually contain an
object of this class. Only images that multiple annotators validated ended up
in the final dataset.</p>

<div>
  <div class="stages block">
      <div class="stage rbutton block clicked" id="selection">Class Selection</div>
      <div class="stage rbutton block" id="retrieval">Image Retrieval</div>
      <div class="stage rbutton block" id="filtering">Label Validation</div>
  </div>
  <img src="https://gradientscience.org/assets/multilabel/pipeline/selection.jpg" id="stage_img" />
</div>
<div class="footnote"> <strong>Imagenet collection pipeline:</strong> Click on a
stage at the top for an illustration. </div>

<p>While this is a natural approach to scalably annotate data (and, in fact, is
commonly used to create large-scale benchmarks—e.g.,
<a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>,
<a href="http://cocodataset.org/#home">COCO</a>, <a href="http://places.csail.mit.edu/">places</a>),
it has an important caveat. Namely, this process has an inherent bias: the
annotation task itself is phrased as a leading question. ImageNet annotators
were not asked to provide an image label, but instead only to verify if a
<i>specific</i> label (predetermined by the image retrieval process) was
<i>contained</i> in an image. Annotators had no knowledge of what the other
classes in the dataset even were, or the granularity at which they were
required to make distinctions. In fact, they were explicitly instructed to
ignore clutter and obstructions.</p>

<p>Looking back at the ImageNet samples shown above, one can see how this setup
could lead to imperfect annotations. For instance, it is unclear if the average
annotator knows the differences between a “Norwich terrier” and a “Norfolk
terrier”, especially if they don’t even know that both of these (as well as 22
other terrier breeds) are valid ImageNet classes. Also, the
<span class="sc">Contains</span> task itself might be ill-suited for annotating
multi-object images—the answer to the <span class="sc">Contains</span> question would be yes for any
object in the image that corresponds to an ImageNet class. It is not unthinkable
that the same images could have made it into ImageNet under the labels “stage”
and “Norwich terrier” had they come up in the search results for those classes
instead.</p>

<p>Overall, this suggests that the labeling issues in ImageNet may go beyond just
occasional annotator mistakes—the <i>design</i> of the data collection
pipeline itself could have caused these labels to systematically deviate from
the ground truth.</p>

<h3 id="diagnosing-benchmark-task-misalignment">Diagnosing benchmark-task misalignment</h3>

<p>To characterize how wide-spread these deviations are, we first need to get a
better grasp of the ground truth for ImageNet data. In order to do this at
scale, we still need to rely on crowd-sourcing. However, in contrast to the
original label validation setup, we design a new annotation task based directly
on <i>image classification</i>. Namely, we present annotators with a set of
possible labels for a single image <i>simultaneously</i>. We then ask them to
assign one label to every object in the image, and identify what they believe
to be the main object. (Note that we intentionally ask for such fine-grained
image annotations since, as we saw before, a single label might be inherently
insufficient to capture the ground truth.)</p>

<p>Of course, we need to ensure that annotators can meaningfully perform this
task. To this end we  devise a way to narrow down the label choices they are
presented with (all thousand ImageNet classes would be nearly impossible for a
worker to choose between!). Specifically, for each image, we identify the most
relevant labels by pooling together the top-5 predictions of a diverse set of
ImageNet models and filtering them via the <span class="sc">Contains</span>
task. Note that, by doing so, we are effectively bootstrapping the existing
ImageNet labels by first using them to train models and then using model
predictions to get better annotation candidates.</p>

<p>This is what our resulting annotation task looks like:</p>

<p><img src="https://gradientscience.org/assets/multilabel/main_task.jpg" /></p>

<p>We aggregate the responses from multiple annotators to get per-image estimates
of the number of objects in the image (along with their corresponding labels),
as well as which object humans tend to view as the main one.</p>

<p>We collect such <a href="https://github.com/MadryLab/ImageNetMultiLabel">annotations for 10k images from the ImageNet validation
set</a>. With these more
fine-grained and accurate annotations in hand, we now examine where the
original ImageNet labels may fall short.</p>

<h3 id="multi-object-images">Multi-object images</h3>
<p>The simplest way in which ImageNet labels could deviate from the ground truth
is if the image contains multiple objects. So, the first thing we want to
understand is: how many ImageNet images contain objects from more than one
valid class?</p>

<p>It turns out: quite a few! Indeed, more than <b>20%</b> of the images contain
more than one ImageNet object. Examples:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="multi"> </div>
  <div class="choices_img widgetheading"> <img id="multi1" /> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="multiclass"> </div>
  </div>
</div>
<div style="clear: both;"></div>
<div class="footnote"> <strong>Multi-object images:</strong> Choose an image on
the left to see the annotations we obtain for it.  </div>

<p>Looking at some of these images, it is clear that the problem is not just
natural image clutter but also the fact that certain objects are quite likely
to co-occur in the real-world—e.g., “table lamp” and “lamp shade”. This means
that choosing classes which in principle correspond to distinct objects (e.g.,
using WordNet) is not enough to guarantee that the corresponding images have
unambiguous labels. For example, see if you can guess the ImageNet label for
the samples below:</p>

<div class="widget">
  <span class="widgetheading" id="coclass">Chosen class pair</span>
  <div class="choices_one_full" id="co">
  <div class="show_labels rbutton block" id="cooc">Show Labels</div>
  </div>
  <div style="border-right: 10px white solid;" id="coimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Co-occurring objects:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h4 id="model-performance-on-multi-object-images">Model performance on multi-object images</h4>

<p>So, how do models deal with images that contain multiple objects? To understand
this, we evaluate a number of models (from AlexNet to EfficientNet-B7), and
measure their accuracy (w.r.t. the ImageNet labels) on such images. We plot
these accuracies below (as a function of their full test accuracy):</p>

<canvas width="400" id="multi_drop" height="200"></canvas>
<div class="footnote">
<strong>Performance on multi-object images:</strong>
Model accuracy variation as a function of the number of objects in the image.
Hover over each data point to see the corresponding model name.
</div>

<p>Across the board, in comparison to their performance on single-object images,
models suffer around a 10% accuracy drop on multi-object ones. At the same
time, this drop more-or-less disappears if we consider a model prediction to be
correct if it matches the label of <i>any</i> valid object in the image (see
the <a href="https://arxiv.org/abs/2005.11295">paper</a> for specifics).</p>

<div class="footnote">
    <strong>Aside:</strong> The original motivation of top-5 accuracy was
    exactly to accommodate such multi-label images. However, we find that, while
    it does largely account for these multi-object confusions, it also seems to
    overestimate accuracy on single-object images.
</div>

<p>Still, even though models seem to struggle with multi-object images, they
perform much better than chance (i.e., better than what one would get if they
were picking the label of an object in the image at random). This makes sense
when the image has a single prominent object that also matches the ImageNet
label. However, for a third of all multi-object images the ImageNet label does
not even match what annotators deem to be the main object in the image. Yet,
even in these cases, models still successfully predict the ImageNet label
(instead of what humans consider to be the right label for the image)!</p>

<canvas width="400" id="overfitting" height="200"></canvas>
<div class="footnote">
<strong>Performance on images with annotator-label disagreement:</strong>
Model accuracy on images where annotators do not consider the ImageNet label to
be the main object. Baseline: accuracy of picking the label of a random
prominent object in the image.
</div>

<p>Here, models seem to base their predictions on biases in the dataset which
humans do not find salient. For instance, models get high accuracy on the class
“pickelhaube”, even though, pickelhaubes are usually present in images with
other, more salient objects, such as “military uniforms”, suggesting that
ImageNet models may be overly sensitive to the presence of distinctive objects
in the image. While exploiting such biases would improve ImageNet accuracy,
this strategy might not translate to improved performance on object recognition
in the wild. Here are a few examples that seem to exhibit a similar mismatch:</p>

<div class="widget">
  <div class="choices_one">
    <span class="widgetheading">Inspect Image</span>
  </div>
  <div class="choices_img">
    <span class="widgetheading">Image</span>
  </div>
  <div class="choices_info block">
    <span class="widgetheading">Annotation</span>
  </div>
  <div class="choices_one" id="main"> </div>
  <div class="choices_img widgetheading"> <img id="main1" /> </div>
  <div class="choices_info block">
    <div class="choices_info_text" id="mainclass"> </div>
  </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Annotator-label disagreement:</strong> Select an image on the left to
see the annotations we obtain for it. Notice that the ImageNet label is
different from the (annotator-selected) "main label".
</div>

<h3 id="biases-in-label-validation">Biases in label validation</h3>

<p>Let us now turn our attention to the ImageNet data filtering process. Recall
that each class in ImageNet was constructed by automatically retrieving many
images and filtering them (via the <span class="sc">Contains</span> task described above). How likely
were annotators to filter out mislabeled images under this setup?</p>

<p>To understand this, we replicate the original filtering process on the existing
ImageNet images. But this time, instead of only asking annotators to check if
the image is valid with respect to its ImageNet label (i.e., the search query),
we also try several other labels (each in isolation, with different sets of
annotators).</p>

<p>We find that annotators frequently deem an image to be valid for <i>many</i>
different labels—even when only one object is present. Typically, this occurs
when the image is ambiguous and lacks enough context (e.g. “seashore” or
“lakeshore”), or annotators are likely confused between different semantically
similar labels (e.g., “assault rifle” vs. “rifle”, dog breeds). It turns out
that this confusion, at least partly, stems from the one-sidedness of the
<span class="sc">Contains</span> task—i.e., asking annotators to ascertain the validity of a specific
label without them knowing about any other options. If instead we present
annotators with all the relevant labels simultaneously and ask them to choose
one (as we did in our annotation setup), this kind of label confusion is
alleviated: annotators select significantly fewer labels in total (see our
<a href="https://arxiv.org/abs/2005.11295">paper</a> for details). So, even putting
annotator’s expertise aside, the specific annotation task setup itself
drastically affects the quality of the resulting dataset labels.</p>

<div>
  <div class="dropdown">
    <div class="rates block" id="dropdownMenuButton">
      Choose annotator threshold
    </div>
    <div class="rates">
      <div class="block rbutton clicked">10%</div>
      <div class="block rbutton">30%</div>
      <div class="block rbutton">50%</div>
    </div>
  </div>
  <img src="https://gradientscience.org/assets/multilabel/filtering/filtering_0.1.jpg" id="filtering_plot" />
</div>
<div class="footnote">
<strong>Number of valid labels:</strong> Distribution of labels annotators deem
valid (based on an agreement threshold) for single-object images in the <span class="sc">Contains</span>
task. Click the percentages on the top to change the annotator agreement
threshold.
</div>

<p>Going back to ImageNet, our findings give us reason to believe that annotators
may have had a rather limited ability to correct errors in labeling. Thus, in
certain cases, ImageNet labels were largely determined by the automated image
retrieval process—propagating any biases or mixups this process might
introduce to the final dataset.</p>

<p>In fact, we can actually see direct evidence of that in the ImageNet
dataset—there are pairs of classes that appear to be <i>inherently
ambiguous</i> (e.g., “laptop computer” and “notebook computer”) and neither
human annotators, nor models, can tell the corresponding images apart (see
below). If such class pairs actually overlap in terms of their ImageNet images,
it is unclear how models can learn to separate them without memorizing specific
validation examples.</p>

<div class="widget">
  <span class="widgetheading" id="ambclass">Chosen class pair</span>
  <div class="choices_one_full" id="am">
  <div class="show_labels rbutton block" id="amb">Show Labels</div>
  </div>
  <div style="border-right: 10px white solid;" id="ambimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Ambiguous class pairs:</strong> Select a class pair on the top and see if
you can identify which images correspond to each class (click on "Show Labels"
to reveal the answers).
</div>

<h3 id="beyond-test-accuracy-human-centric-model-evaluation">Beyond test accuracy: human-centric model evaluation</h3>

<p>Performance of ImageNet-trained models is typically judged based on their
ability to predict the dataset labels—yet, as we saw above, these labels may
not fully capture the ground truth. Hence, ImageNet accuracy may not reflect
properly model performance—for instance, measuring accuracy alone could
unfairly penalize models for certain correct predictions on  multi-object
images. So, how can we better assess model performance?</p>

<p>One approach is to measure model-human alignment directly—we present model
predictions to annotators and ask them to gauge their validity:</p>

<canvas width="400" id="limit" height="200"></canvas>
<div class="footnote">
<strong>Human-based model evaluation:</strong> Fraction of annotators that
select a label as valid in the <span class="sc">Contains</span> task (i.e.,
selection frequency) for ImageNet labels and model predictions. Baseline:
(number of correct predictions) × (average selection frequency of the
ImageNet label).
</div>

<p>Surprisingly, we find that for state-of-the-art models, annotators actually
deem the prediction that models make to be valid about as often as the ImageNet
label (even when the two <i> do not</i> match). Thus, recent models may be
better at predicting the ground truth than their top-1 accuracy (w.r.t. the
ImageNet label) would indicate.</p>

<p>However, this does not imply that improving ImageNet accuracy is meaningless.
For instance, non-expert annotators may not be able to tell apart certain
fine-grained class differences (e.g., dog breeds) and for some of these images
the ImageNet label may actually match the ground truth. What it does indicate,
though, is that we are at a point where it may be hard to gauge if better
performance on ImageNet corresponds to actual progress or merely to exploiting
idiosyncrasies of the dataset.</p>

<p>For further experimental details and additional results (e.g., human confusion
matrices), take a look at <a href="https://arxiv.org/abs/2005.11295">our paper</a>!</p>

<h3 id="conclusions">Conclusions</h3>

<p>We took a closer look at how well ImageNet aligns with the real-world object
recognition task—even though ImageNet is used extensively, we rarely question
whether its labels actually reflect the ground truth. We saw that oftentimes
ImageNet labels do not fully capture image content—e.g., many images have
multiple (ImageNet) objects and there are classes that are inherently
ambiguous. As a result, models trained using these labels as ground truth end
up learning unintended biases and confusions.</p>

<p>Our analysis indicates that when creating datasets we must be aware of (and try
to mitigate) ways in which scalable data collection practices can skew the
corresponding annotations (see our
<a href="https://gradientscience.org/data_rep_bias">previous post</a> for another
example of such a skew). Finally, given that such imperfections in our datasets
could be inevitable, we also need to think about how to reliably assess model
performance in their presence.</p>





















<b></b><br /><hr /><b></b><br /><br /><br /><b></b><br /><br /><div class="cooc_img block"><div class="image_label label_cooc"><br /></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg" /></div><div class="amb_img block"><div class="image_label label_amb"><br /></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg" /></div><div class="bias_img block"><div class="image_label label_bias"><br /></div><img src="https://gradientscience.org/&quot; + base +                         pair + &quot;_&quot; + i + &quot;_dst.jpg" /></div></div>







<p class="date">
<a href="https://gradientscience.org/benchmarks/"><span class="datestr">at May 25, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/">Postdoc at University of Vienna, TU Vienna, IST Austria, WU Vienna (apply by June 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Vienna Graduate School on Computational Optimization (VGSCO) is a research and training program funded by the Austrian Science Funds (FWF). The VGSCO offers lecture series given by international experts in optimization related fields, organizes research seminars, retreats, soft skills courses, scientific workshops, and social events, provides travel grants, and supports research stays abroad.</p>
<p>Website: <a href="http://vgsco.univie.ac.at/positions">http://vgsco.univie.ac.at/positions</a><br />
Email: vgsco@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/05/24/postdoc-at-university-of-vienna-tu-vienna-ist-austria-wu-vienna-apply-by-june-15-2020/"><span class="datestr">at May 24, 2020 08:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
