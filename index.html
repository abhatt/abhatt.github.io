<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 26, 2019 09:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Online Optimization Post 2: Constructing Pseudorandom Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an “almost pairwise independent” pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a “builder” against an “inspector,” in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the “inspector” is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the “offline optimum” that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the “builder” that runs the algorithm and the “inspector” who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the “dual view” of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" title="{{\cal C} = \{ C_1,\ldots,C_N \}}" /> is a collection of boolean functions <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" />, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S\subseteq \{ 0,1 \}^n}" class="latex" title="{S\subseteq \{ 0,1 \}^n}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i \in \cal C}" class="latex" title="{C_i \in \cal C}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " /></p>
<p> That is, sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, which we can do with <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log_2 |S|}" class="latex" title="{\log_2 |S|}" /> random bits, is as good as sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" />, which requires <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits, as far as the functions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N/\epsilon^2)}" class="latex" title="{O(N/\epsilon^2)}" />, so that we can sample from it using only <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" title="{\log N + 2\log \frac 1 \epsilon + O(1)}" /> random bits.</p>
<p>
We will prove this result (while also providing an “algorithm” for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"></span></p>
<p>
First of all, possibly by changing <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2N}" class="latex" title="{2N}" />, we may assume that for every function <img src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C \in {\cal C}}" class="latex" title="{C \in {\cal C}}" /> the function <img src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-C}" class="latex" title="{1-C}" /> is also in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " /></p>
<p>
We will make up an “experts” setup in which there is an expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> for each function <img src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i}" class="latex" title="{C_i}" />. Thus, the algorithm, at each step, comes up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> over the functions, which we can think of as a “probabilistic function.” At time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, the adversary chooses a string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t \in \{ 0,1 \}^n}" class="latex" title="{s_t \in \{ 0,1 \}^n}" /> and defines the cost function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " /></p>
<p> where the adversary chooses an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) \geq 0}" class="latex" title="{f_t(x_t) \geq 0}" />. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> is always possible;
</li><li> That the cost function is of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" title="{f_t(x) = \langle \ell_t , x\rangle}" />, where the loss vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\ell_t (i) | \leq 1}" class="latex" title="{|\ell_t (i) | \leq 1}" />, so that the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq 2 \sqrt{T \ln N}}" class="latex" title="{\leq 2 \sqrt{T \ln N}}" />;
</li><li> That the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_1,\ldots,s_T}" class="latex" title="{s_1,\ldots,s_T}" /> of choices by the adversary determines a <img src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" title="{2\sqrt {\frac {\ln N}{T}}}" />-pseudorandom multiset for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />, and, in particular, we get an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom multiset of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" title="{4 \frac {\ln N}{\epsilon^2}}" />
</li></ol>
<p> For the first point, note that for a random <img src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s \sim \{ 0,1 \}^n}" class="latex" title="{s \sim \{ 0,1 \}^n}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " /></p>
<p> so there is an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " /></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " /></p>
<p> so the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " /></p>
<p> which, after dividing by <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p> Consider now the application of constructing a small-support distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" /> that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-almost-pairwise-independent, meaning that if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> is a random string sampled according to this distribution, then, for every <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j}" class="latex" title="{i,j}" />, the marginal <img src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(s_i,s_j)}" class="latex" title="{(s_i,s_j)}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^2}" class="latex" title="{\{ 0,1 \}^2}" /> in total variation distance. This is the same thing as asking for a small-support distribution that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for all functions <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" /> that depend on only two input variables. There are only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} \ln n)}" class="latex" title="{O(\epsilon^{-2} \ln n)}" />, meaning that the distribution can be sampled using <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" /> random bits. Furthermore the algorithm can be implemented to run in time <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{O(1)} / \epsilon^2}" class="latex" title="{n^{O(1)} / \epsilon^2}" />. The only tricky step is how to find the string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> at each step. For a string <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, the loss <img src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f (x_t)}" class="latex" title="{f (x_t)}" /> obtained by choosing <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> as the “reference string” is a polynomial of degree 2 in the bits of <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, so we can find a no-worse-than-average <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> of the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-th string in the sample space can be generated in time <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\log n)^{O(1)}}" class="latex" title="{(\log n)^{O(1)}}" />. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma’s construction of small-bias generators we would not get the correct dependency on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />. This framework can “derandomize Chernoff bounds” in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of “pessimistic estimators” described in the Motwani-Raghavan book on randomized algorithms. </p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"><span class="datestr">at April 26, 2019 01:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11446">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11446">Quantum Walk Sampling by Growing Seed Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11446">PDF</a><br /><b>Abstract: </b>This work describes a new algorithm for creating a superposition over the
edge set of a graph, encoding a quantum sample of the random walk stationary
distribution. The algorithm requires a number of quantum walk steps scaling as
$\widetilde{O}(m^{1/3} \delta^{-1/3})$, with $m$ the number of edges and
$\delta$ the random walk spectral gap. This improves on existing strategies by
initially growing a classical seed set in the graph, from which a quantum walk
is then run. The algorithm leads to a number of improvements: (i) it provides a
new bound on the setup cost of quantum walk search algorithms, (ii) it yields a
new algorithm for $st$-connectivity, and (iii) it allows to create a
superposition over the isomorphisms of an $n$-node graph in time
$\widetilde{O}(2^{n/3})$, surpassing the $\Omega(2^{n/2})$ barrier set by index
erasure.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11446"><span class="datestr">at April 26, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11440">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11440">Distributed Detection of Cliques in Dynamic Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Matthias Bonne, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11440">PDF</a><br /><b>Abstract: </b>This paper provides an in-depth study of the fundamental problems of finding
small subgraphs in distributed dynamic networks. While some problems are
trivially easy to handle, such as detecting a triangle that emerges after an
edge insertion, we show that, perhaps somewhat surprisingly, other problems
exhibit a wide range of complexities in terms of the trade-offs between their
round and bandwidth complexities. In the case of triangles, which are only
affected by the topology of the immediate neighborhood, some end results are:
</p>
<p>\begin{itemize}
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle detection or
listing is $\Theta(1)$.
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle membership
listing is $\Theta(1)$ for node/edge deletions, $\Theta(n^{1/2})$ for edge
insertions, and $\Theta(n)$ for node insertions.
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle membership
detection is $\Theta(1)$ for node/edge deletions, $O(\log n)$ for edge
insertions, and $\Theta(n)$ for node insertions.
</p>
<p>\end{itemize}
</p>
<p>Most of our upper and lower bounds are \emph{tight}. Additionally, we provide
almost always tight upper and lower bounds for larger cliques.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11440"><span class="datestr">at April 26, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11395">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11395">On the Complexity of Local Graph Transformations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheideler:Christian.html">Christian Scheideler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Setzer:Alexander.html">Alexander Setzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11395">PDF</a><br /><b>Abstract: </b>We consider the problem of transforming a given graph $G_s$ into a desired
graph $G_t$ by applying a minimum number primitives from a particular set of
local graph transformation primitives. These primitives are local in the sense
that each node can apply them based on local knowledge and by affecting only
its $1$-neighborhood. Although the specific set of primitives we consider makes
it possible to transform any (weakly) connected graph into any other (weakly)
connected graph consisting of the same nodes, they cannot disconnect the graph
or introduce new nodes into the graph, making them ideal in the context of
supervised overlay network transformations. We prove that computing a minimum
sequence of primitive applications (even centralized) for arbitrary $G_s$ and
$G_t$ is NP-hard, which we conjecture to hold for any set of local graph
transformation primitives satisfying the aforementioned properties. On the
other hand, we show that this problem admits a polynomial time algorithm with a
constant approximation ratio.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11395"><span class="datestr">at April 26, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11337">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11337">A multi-start local search algorithm for the Hamiltonian completion problem on undirected graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jorik Jooken, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leyman:Pieter.html">Pieter Leyman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Causmaecker:Patrick_De.html">Patrick De Causmaecker</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11337">PDF</a><br /><b>Abstract: </b>This paper proposes a local search algorithm for a specific combinatorial
optimisation problem in graph theory: the Hamiltonian Completion Problem (HCP)
on undirected graphs. In this problem, the objective is to add as few edges as
possible to a given undirected graph in order to obtain a Hamiltonian graph.
This problem has mainly been studied in the context of various specific kinds
of undirected graphs (e.g. trees, unicyclic graphs and series-parallel graphs).
The proposed algorithm, however, concentrates on solving HCP for general
undirected graphs. It can be considered to belong to the category of
matheuristics, because it integrates an exact linear time solution for trees
into a local search algorithm for general graphs. This integration makes use of
the close relation between HCP and the minimum path partition problem, which
makes the algorithm equally useful for solving the latter problem. Furthermore,
a benchmark set of problem instances is constructed for demonstrating the
quality of the proposed algorithm. A comparison with state-of-the-art solvers
indicates that the proposed algorithm is able to achieve high-quality results.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11337"><span class="datestr">at April 26, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11323">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11323">Performance Prediction for Coarse-Grained Locking</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Vitaly Aksenov Dan Alistarh Petr Kuznetsov <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11323">PDF</a><br /><b>Abstract: </b>A standard design pattern found in many concurrent data structures, such as
hash tables or ordered containers, is an alternation of parallelizable sections
that incur no data conflicts and critical sections that must run sequentially
and are protected with locks. A lock can be viewed as a queue that arbitrates
the order in which the critical sections are executed, and a natural question
is whether we can use stochastic analysis to predict the resulting throughput.
As a preliminary evidence to the affirmative, we describe a simple model that
can be used to predict the throughput of coarse-grained lock-based algorithms.
We show that our model works well for CLH lock, and we expect it to work for
other popular lock designs such as TTAS, MCS, etc.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11323"><span class="datestr">at April 26, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11285">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11285">Detecting and Counting Small Patterns in Planar Graphs in Subexponential Parameterized Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11285">PDF</a><br /><b>Abstract: </b>We present an algorithm that takes as input an $n$-vertex planar graph $G$
and a $k$-vertex pattern graph $P$, and computes the number of (induced) copies
of $P$ in $G$ in $2^{O(k/\log k)}n^{O(1)}$ time. If $P$ is a matching,
independent set, or connected bounded maximum degree graph, the runtime reduces
to $2^{\tilde{O}(\sqrt{k})}n^{O(1)}$.
</p>
<p>While our algorithm counts all copies of $P$, it also improves the fastest
algorithms that only detect copies of $P$. Before our work, no $2^{O(k/\log
k)}n^{O(1)}$ time algorithms for detecting unrestricted patterns $P$ were
known, and by a result of Bodlaender et al. [ICALP 2016] a $2^{o(k/\log
k)}n^{O(1)}$ time algorithm would violate the Exponential Time Hypothesis
(ETH). Furthermore, it was only known how to detect copies of a fixed connected
bounded maximum degree pattern $P$ in $2^{\tilde{O}(\sqrt{k})}n^{O(1)}$ time
probabilistically. For counting problems, it was a repeatedly asked open
question whether $2^{o(k)}n^{O(1)}$ time algorithms exist that count even
special patterns such as independent sets, matchings and paths in planar
graphs. The above results resolve this question in a strong sense by giving
algorithms for counting versions of problems with running times equal to the
ETH lower bounds for their decision versions.
</p>
<p>Generally speaking, our algorithm counts copies of $P$ in time proportional
to its number of non-isomorphic separations of order $\tilde{O}(\sqrt{k})$. The
algorithm introduces a new recursive approach to construct families of balanced
cycle separators in planar graphs that have limited overlap inspired by methods
from Fomin et al. [FOCS 2016], a new `efficient' inclusion-exclusion based
argument and uses methods from Bodlaender et al. [ICALP 2016].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11285"><span class="datestr">at April 26, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11263">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11263">Stochastic rounding and reduced-precision fixed-point arithmetic for solving neural ODEs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Michael.html">Michael Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mikaitis:Mantas.html">Mantas Mikaitis</a>, Dave R. Lester, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furber:Steve.html">Steve Furber</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11263">PDF</a><br /><b>Abstract: </b>Although double-precision floating-point arithmetic currently dominates
high-performance computing, there is increasing interest in smaller and simpler
arithmetic types. The main reasons are potential improvements in energy
efficiency and memory footprint and bandwidth. However, simply switching to
lower-precision types typically results in increased numerical errors. We
investigate approaches to improving the accuracy of lower-precision arithmetic
types, using examples in an important domain for numerical computation in
neuroscience: the solution of Ordinary Differential Equations (ODEs). The
Izhikevich neuron model is used to demonstrate that rounding has an important
role in producing accurate spike timings from explicit ODE solution algorithms.
In particular, stochastic rounding consistently results in smaller errors
compared to single-precision floatingpoint and fixed-point arithmetic with
round-tonearest across a range of neuron behaviours and ODE solvers. A
computationally much cheaper alternative is also investigated, inspired by the
concept of dither that is a widely understood mechanism for providing
resolution below the LSB in digital signal processing. These results will have
implications for the solution of ODEs in other subject areas, and should also
be directly relevant to the huge range of practical problems that are
represented by Partial Differential Equations (PDEs).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11263"><span class="datestr">at April 26, 2019 01:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11244">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11244">On adaptive algorithms for maximum matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Falko Hegerfeld, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11244">PDF</a><br /><b>Abstract: </b>In the fundamental Maximum Matching problem the task is to find a maximum
cardinality set of pairwise disjoint edges in a given undirected graph. The
fastest algorithm for this problem, due to Micali and Vazirani, runs in time
$\mathcal{O}(\sqrt{n}m)$ and stands unbeaten since 1980. It is complemented by
faster, often linear-time, algorithms for various special graph classes.
Moreover, there are fast parameterized algorithms, e.g., time
$\mathcal{O}(km\log n)$ relative to tree-width $k$, which outperform
$\mathcal{O}(\sqrt{n}m)$ when the parameter is sufficiently small.
</p>
<p>We show that the Micali-Vazirani algorithm, and in fact any algorithm
following the phase framework of Hopcroft and Karp, is adaptive to beneficial
input structure. We exhibit several graph classes for which such algorithms run
in linear time $\mathcal{O}(n+m)$. More strongly, we show that they run in time
$\mathcal{O}(\sqrt{k}m)$ for graphs that are $k$ vertex deletions away from any
of several such classes, without explicitly computing an optimal or approximate
deletion set; before, most such bounds were at least $\Omega(km)$. Thus, any
phase-based matching algorithm with linear-time phases obliviously interpolates
between linear time for $k=\mathcal{O}(1)$ and the worst case of
$\mathcal{O}(\sqrt{n}m)$ when $k=\Theta(n)$. We complement our findings by
proving that the phase framework by itself still allows $\Omega(\sqrt{n})$
phases, and hence time $\Omega(\sqrt{n}m)$, even on paths, cographs, and
bipartite chain graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11244"><span class="datestr">at April 26, 2019 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11229">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11229">Finding Hexahedrizations for Small Quadrangulations of the Sphere</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verhetsel:Kilian.html">Kilian Verhetsel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pellerin:Jeanne.html">Jeanne Pellerin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Remacle:Jean=Fran=ccedil=ois.html">Jean-François Remacle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11229">PDF</a><br /><b>Abstract: </b>This paper tackles the challenging problem of constrained hexahedral meshing.
An algorithm is introduced to build combinatorial hexahedral meshes whose
boundary facets exactly match a given quadrangulation of the topological
sphere. This algorithm is the first practical solution to the problem. It is
able to compute small hexahedral meshes of quadrangulations for which the
previously known best solutions could only be built by hand or contained
thousands of hexahedra. These challenging quadrangulations include the
boundaries of transition templates that are critical for the success of general
hexahedral meshing algorithms.
</p>
<p>The algorithm proposed in this paper is dedicated to building combinatorial
hexahedral meshes of small quadrangulations and ignores the geometrical
problem. The key idea of the method is to exploit the equivalence between quad
flips in the boundary and the insertion of hexahedra glued to this boundary.
The tree of all sequences of flipping operations is explored, searching for a
path that transforms the input quadrangulation Q into a new quadrangulation for
which a hexahedral mesh is known. When a small hexahedral mesh exists, a
sequence transforming Q into the boundary of a cube is found; otherwise, a set
of pre-computed hexahedral meshes is used.
</p>
<p>A novel approach to deal with the large number of problem symmetries is
proposed. Combined with an efficient backtracking search, it allows small
shellable hexahedral meshes to be found for all even quadrangulations with up
to 20 quadrangles. All 54,943 such quadrangulations were meshed using no more
than 72 hexahedra. This algorithm is also used to find a construction to fill
arbitrary domains, thereby proving that any ball-shaped domain bounded by n
quadrangles can be meshed with no more than 78 n hexahedra. This very
significantly lowers the previous upper bound of 5396 n.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11229"><span class="datestr">at April 26, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11134">Tell Me What I Need to Know: Succinctly Summarizing Data with Itemsets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mampaey:Michael.html">Michael Mampaey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vreeken:Jilles.html">Jilles Vreeken</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11134">PDF</a><br /><b>Abstract: </b>Data analysis is an inherently iterative process. That is, what we know about
the data greatly determines our expectations, and hence, what result we would
find the most interesting. With this in mind, we introduce a well-founded
approach for succinctly summarizing data with a collection of itemsets; using a
probabilistic maximum entropy model, we iteratively find the most interesting
itemset, and in turn update our model of the data accordingly. As we only
include itemsets that are surprising with regard to the current model, the
summary is guaranteed to be both descriptive and non-redundant. The algorithm
that we present can either mine the top-$k$ most interesting itemsets, or use
the Bayesian Information Criterion to automatically identify the model
containing only the itemsets most important for describing the data. Or, in
other words, it will `tell you what you need to know'. Experiments on synthetic
and benchmark data show that the discovered summaries are succinct, and
correctly identify the key patterns in the data. The models they form attain
high likelihoods, and inspection shows that they summarize the data well with
increasingly specific, yet non-redundant itemsets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11134"><span class="datestr">at April 26, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11037">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11037">Counting the Number of Crossings in Geometric Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duque:Frank.html">Frank Duque</a>, Ruy Fabila-Monroy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hern=aacute=ndez=V=eacute=lez:C=eacute=sar.html">César Hernández-Vélez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hidalgo=Toscano:Carlos.html">Carlos Hidalgo-Toscano</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11037">PDF</a><br /><b>Abstract: </b>A geometric graph is a graph whose vertices are points in general position in
the plane and its edges are straight line segments joining these points. In
this paper we give an $O(n^2 \log n)$ algorithm to compute the number of pairs
of edges that cross in a geometric graph on $n$ points. For layered, and convex
geometric graphs the algorithm takes $O(n^2)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11037"><span class="datestr">at April 26, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.11026">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.11026">Efficient Nearest-Neighbor Query and Clustering of Planar Curves</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aronov:Boris.html">Boris Aronov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horton:Michael.html">Michael Horton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katz:Matthew_J=.html">Matthew J. Katz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sheikhan:Khadijeh.html">Khadijeh Sheikhan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11026">PDF</a><br /><b>Abstract: </b>We study two fundamental problems dealing with curves in the plane, namely,
the nearest-neighbor problem and the center problem. Let $\mathcal{C}$ be a set
of $n$ polygonal curves, each of size $m$. In the nearest-neighbor problem, the
goal is to construct a compact data structure over $\mathcal{C}$, such that,
given a query curve $Q$, one can efficiently find the curve in $\mathcal{C}$
closest to $Q$. In the center problem, the goal is to find a curve $Q$, such
that the maximum distance between $Q$ and the curves in $\mathcal{C}$ is
minimized. We use the well-known discrete Frechet distance function, both
under~$L_\infty$ and under $L_2$, to measure the distance between two curves.
</p>
<p>For the nearest-neighbor problem, despite discouraging previous results, we
identify two important cases for which it is possible to obtain practical
bounds, even when $m$ and $n$ are large. In these cases, either $Q$ is a line
segment or $\mathcal{C}$ consists of line segments, and the bounds on the size
of the data structure and query time are nearly linear in the size of the input
and query curve, respectively. The returned answer is either exact under
$L_\infty$, or approximated to within a factor of $1+\varepsilon$ under~$L_2$.
We also consider the variants in which the location of the input curves is only
fixed up to translation, and obtain similar bounds, under $L_\infty$.
</p>
<p>As for the center problem, we study the case where the center is a line
segment, i.e., we seek the line segment that represents the given set as well
as possible. We present near-linear time exact algorithms under $L_\infty$,
even when the location of the input curves is only fixed up to translation.
Under $L_2$, we present a roughly $O(n^2m^3)$-time exact algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.11026"><span class="datestr">at April 26, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10454">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10454">Normalizers and permutational isomorphisms in simply-exponential time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiebking:Daniel.html">Daniel Wiebking</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10454">PDF</a><br /><b>Abstract: </b>We show that normalizers and permutational isomorphisms of permutation groups
given by generating sets can be computed in time simply exponential in the
degree of the groups. The result is obtained by exploiting canonical forms for
permutation groups (up to permutational isomorphism).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10454"><span class="datestr">at April 26, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=351">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/">TCS+ talk: Wednesday, May 1st — Chris Peikert, University of Michigan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about “<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat—Shamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on “exotic” assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> circuits, for any polynomially bounded <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible “modes,” yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" target="_blank" rel="noopener">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/"><span class="datestr">at April 25, 2019 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1084">Call for Participation: HALG 2019 (Highlights of Algorithms)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>-------------------------------------------------------------------<br />
4rd Highlights of Algorithms conference (HALG 2019)<br />
Copenhagen, June 14-16, 2019<br />
<a href="http://highlightsofalgorithms.org/" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br />
highlights of recent developments in algorithms and for discussing<br />
potential further advances in this area. The conference will provide a<br />
broad picture of the latest research in algorithms through a series of<br />
invited talks, as well as the possibility for all researchers and<br />
students to present their recent results through a series of short<br />
talks and poster presentations. Attending the Highlights of Algorithms<br />
conference will also be an opportunity for networking and meeting<br />
leading researchers in algorithms.<br />
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br />
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br />
accepted short contributions can be found at:<br />
<a href="http://2018.highlightsofalgorithms.org/programme" target="_blank" rel="noopener noreferrer">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br />
     <a href="http://highlightsofalgorithms.org/registration" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/registration</a><br />
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br />
- academic rate (incl. postdocs): 160€<br />
- student rate: 115€</p>
<p>Regular registration will be 50€ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. Ørsted Institute of the<br />
University of Copenhagen.<br />
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br />
Monika Henzinger (University of Vienna)<br />
Thomas Vidick (California Institute of Technology)<br />
Laszlo Vegh (London School of Economics)<br />
James Lee (University of Washington)<br />
Timothy Chan (University of Illinois at Urbana-Champaign)<br />
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br />
Martin Grohe (RWTH Aachen University)<br />
Josh Alman (MIT)<br />
Nima Anari (Stanford University)<br />
Michal Koucký (Charles University)<br />
Naveen Garg (IIT Delhi)<br />
Vera Traub (University of Bonn)<br />
Rico Zenklusen (ETH Zurich)<br />
Shayan Oveis Gharan (University of Washington)<br />
Greg Bodwin (MIT)<br />
Cliff Stein (Columbia University)<br />
Sungjin Im (University of California at Merced)<br />
C. Seshadhriy (University of California, Santa Cruz)<br />
Shay Moran (Technion)<br />
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br />
Sebastien Bubeck (Microsoft Research, Redmond)<br />
Sushant Sachdeva (University of Toronto)<br />
Kunal Talwar (Google Brain)<br />
Moses Charikar (Stanford University)<br />
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1084"><span class="datestr">at April 25, 2019 12:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8078353386966881451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html">Geo-Centric Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br />
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (László Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk. </li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html"><span class="datestr">at April 25, 2019 12:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Are Natural Mathematical Problems Bad Problems?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to  one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p></p>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that  the densest packing problem in <img src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R^3" class="latex" title="R^3" /> is not interesting.  In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes’s earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" alt="" width="640" class="alignnone size-full wp-image-17390" height="361" /></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov’s proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.  He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and  Günter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/"><span class="datestr">at April 25, 2019 09:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4236">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Online Optimization Post 1: Multiplicative Weights</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> self-described experts. At each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,2,\ldots}" class="latex" title="{t=1,2,\ldots}" />, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" title="{x_t = (x_t(1),\ldots,x_t(n))}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) \geq 0}" class="latex" title="{x_t (i) \geq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" title="{\sum_{i=1}^n x_t(i)=1}" />. After the algorithm makes this choice, it is revealed that following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> leads to loss <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t (i)}" class="latex" title="{\ell_t (i)}" />, so that the expected loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}" />. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm “regrets” that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " /></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" title="{\Delta \subseteq {\mathbb R}^n}" /> of probability distributions over the sample space <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots,n\}}" class="latex" title="{\{ 1,\ldots,n\}}" /> and in which the loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x)}" class="latex" title="{f_t (x)}" /> are linear functions of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" title="{f_t (x) = \sum_i x(i) \ell_t (i)}" />. In order to bound the regret, we also have to bound the “magnitude” of the loss functions, so in the following we will assume that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and all <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t (i) | \leq 1}" class="latex" title="{| \ell_t (i) | \leq 1}" />, and otherwise we can scale everything by a known upper bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{t,i} |\ell_t |}" class="latex" title="{\max_{t,i} |\ell_t |}" />.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> a vector of <em>weights</em> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" title="{w_t = (w_t(1),\ldots,w_t(n))}" /> which is initialized as <img src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_1 := (1,\ldots,1)}" class="latex" title="{w_1 := (1,\ldots,1)}" />. The algorithm performs the following operations at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" />
</li></ul>
<p>
That is, the weight of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" />, and the probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t(i)}" class="latex" title="{x_t(i)}" /> of following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is proportional to the weight. The parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t(i) | \leq 1}" class="latex" title="{| \ell_t(i) | \leq 1}" />, for every <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" title="{0 &lt; \epsilon &lt; 1/2}" />, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " /></p>
<p> In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T &gt; 4 \ln n}" class="latex" title="{T &gt; 4 \ln n}" />, by setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}" /> we achieve a regret bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<span id="more-4236"></span></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, define the quantity</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " /></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, and that if <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " /></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^*}" class="latex" title="{L^*}" /> is large)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> be an index such that <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" title="{L^* = \sum_{t=1}^T \ell_t (j)}" />. Then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t^2}" class="latex" title="{\ell_t^2}" /> is the vector whose <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />-th coordinate is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left( \ell_t (i)\right)^2 }" class="latex" title="{\left( \ell_t (i)\right)^2 }" /> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Since we know that <img src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_1 = n}" class="latex" title="{W_1 = n}" />, it is enough to prove that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,\ldots, T}" class="latex" title="{t=1,\ldots, T}" />, we have <a name="eq.lemma.two"></a></p><a name="eq.lemma.two">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.lemma.two"></a> And we see that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " /></p>
<p> where we used the definitions of our quantities and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-z} \leq 1-z+z^2}" class="latex" title="{e^{-z} \leq 1-z+z^2}" /> for <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1/2}" class="latex" title="{|z| \leq 1/2}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Using the fact that <img src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-z \leq e^{-z}}" class="latex" title="{1-z \leq e^{-z}}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1}" class="latex" title="{|z| \leq 1}" />, the above lemmas can be restated as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " /></p>
<p> which together imply </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " /></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let’s call the loss caused by expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> after <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps the <em>energy</em> of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " /></p>
<p> Note that we have defined it in such a way that the algorithm knows <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. Our offline optimum is the energy of the lowest energy expert at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />, that, is, the energy of the <em>ground state</em> at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />. When we have a collection of numbers <img src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(1),\ldots, E_t(n)}" class="latex" title="{E_t(1),\ldots, E_t(n)}" />, a nice lower bound to their minimum is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> which is true for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" />. The right-hand side above is the <em>free energy</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1 \epsilon}" class="latex" title="{\frac 1 \epsilon}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. This seems like the kind of expression that we could use to bound the offline optimum, so let’s give it a name </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> are the losses of the experts at times <img src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1,\ldots,t-1}" class="latex" title="{1,\ldots,t-1}" />. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on experts that have done well in the first <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps, that is <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on “lower-energy” experts. When we have a system in which, at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, state <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> has energy <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" />, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " /></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " /></p>
<p> So far we have “rediscovered” our multiplicative weights algorithm, and the quantity <img src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_t}" class="latex" title="{W_t}" /> that we had in our analysis gets interpreted as the partition function <img src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_t}" class="latex" title="{Z_t}" />. The fact that <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{T+1}}" class="latex" title="{\Phi_{T+1}}" /> bounds the offline optimum suggests that we should use <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_t}" class="latex" title="{\Phi_t}" /> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " /></p>
<p> which telescopes to give </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> Recalling that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " /></p>
<p> we have again </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"><span class="datestr">at April 25, 2019 06:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10850">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10850">Building a Nest by an Automaton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Czyzowicz:Jurek.html">Jurek Czyzowicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dereniowski:Dariusz.html">Dariusz Dereniowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pelc:Andrzej.html">Andrzej Pelc</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10850">PDF</a><br /><b>Abstract: </b>A robot modeled as a deterministic finite automaton has to build a structure
from material available to it. The robot navigates in the infinite oriented
grid $\mathbb{Z} \times \mathbb{Z}$. Some cells of the grid are full (contain a
brick) and others are empty. The subgraph of the grid induced by full cells,
called the field, is initially connected. The (Manhattan) distance between the
farthest cells of the field is called its span. The robot starts at a full
cell. It can carry at most one brick at a time. At each step it can pick a
brick from a full cell, move to an adjacent cell and drop a brick at an empty
cell. The aim of the robot is to construct the most compact possible structure
composed of all bricks, i.e., a nest. That is, the robot has to move all bricks
in such a way that the span of the resulting field be the smallest. Our main
result is the design of a deterministic finite automaton that accomplishes this
task and subsequently stops, for every initially connected field, in time
$O(sz)$, where $s$ is the span of the initial field and $z$ is the number of
bricks. We show that this complexity is optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10850"><span class="datestr">at April 25, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10748">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10748">Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy with Adaptive Submodularity Ratio</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujii:Kaito.html">Kaito Fujii</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakaue:Shinsaku.html">Shinsaku Sakaue</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10748">PDF</a><br /><b>Abstract: </b>We propose a new concept named adaptive submodularity ratio to study the
greedy policy for sequential decision making. While the greedy policy is known
to perform well for a wide variety of adaptive stochastic optimization problems
in practice, its theoretical properties have been analyzed only for a limited
class of problems. We narrow the gap between theory and practice by using
adaptive submodularity ratio, which enables us to prove approximation
guarantees of the greedy policy for a substantially wider class of problems.
Examples of newly analyzed problems include important applications such as
adaptive influence maximization and adaptive feature selection. Our adaptive
submodularity ratio also provides bounds of adaptivity gaps. Experiments
confirm that the greedy policy performs well with the applications being
considered compared to standard heuristics.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10748"><span class="datestr">at April 25, 2019 11:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10719">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10719">Reoptimization of Path Vertex Cover Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Mehul.html">Mehul Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rangan:C=_Pandu.html">C. Pandu Rangan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10719">PDF</a><br /><b>Abstract: </b>Most optimization problems are notoriously hard. Considerable efforts must be
spent in obtaining an optimal solution to certain instances that we encounter
in the real world scenarios. Often it turns out that input instances get
modified locally in some small ways due to changes in the application world.
The natural question here is, given an optimal solution for an old instance
$I_O$, can we construct an optimal solution for the new instance $I_N$, where
$I_N$ is the instance $I_O$ with some local modifications. Reoptimization of
NP-hard optimization problem precisely addresses this concern. It turns out
that for some reoptimization versions of the NP-hard problems, we may only hope
to obtain an approximate solution to a new instance. In this paper, we
specifically address the reoptimization of path vertex cover problem. The
objective in $k$-$path$ vertex cover problem is to compute a minimum subset $S$
of the vertices in a graph $G$ such that after removal of $S$ from $G$ there is
no path with $k$ vertices in the graph. We show that when a constant number of
vertices are inserted, reoptimizing unweighted $k$-$path$ vertex cover problem
admits a PTAS. For weighted $3$-$path$ vertex cover problem, we show that when
a constant number of vertices are inserted, the reoptimization algorithm
achieves an approximation factor of $1.5$, hence an improvement from known
$2$-approximation algorithm for the optimization version. We provide
reoptimization algorithm for weighted $k$-$path$ vertex cover problem $(k \geq
4)$ on bounded degree graphs, which is also an NP-hard problem. Given a
$\rho$-approximation algorithm for $k$-$path$ vertex cover problem on bounded
degree graphs, we show that it can be reoptimized within an approximation
factor of $(2-\frac{1}{\rho})$ under constant number of vertex insertions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10719"><span class="datestr">at April 25, 2019 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10701">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10701">Faster Algorithms for All Pairs Non-decreasing Paths Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duan:Ran.html">Ran Duan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ce.html">Ce Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Hongxun.html">Hongxun Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10701">PDF</a><br /><b>Abstract: </b>In this paper, we present an improved algorithm for the All Pairs
Non-decreasing Paths (APNP) problem on weighted simple digraphs, which has
running time $\tilde{O}(n^{\frac{3 + \omega}{2}}) = \tilde{O}(n^{2.686})$. Here
$n$ is the number of vertices, and $\omega &lt; 2.373$ is the exponent of time
complexity of fast matrix multiplication [Williams 2012, Le Gall 2014]. This
matches the current best upper bound for $(\max, \min)$-matrix product [Duan,
Pettie 2009] which is reducible to APNP. Thus, further improvement for APNP
will imply a faster algorithm for $(\max, \min)$-matrix product. The previous
best upper bound for APNP on weighted digraphs was $\tilde{O}(n^{\frac{1}{2}(3
+ \frac{3 - \omega}{\omega + 1} + \omega)}) = \tilde{O}(n^{2.78})$ [Duan, Gu,
Zhang 2018]. We also show an $\tilde{O}(n^2)$ time algorithm for APNP in
undirected graphs which also reaches optimal within logarithmic factors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10701"><span class="datestr">at April 25, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10680">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10680">A Polynomial-Time Approximation Scheme for Facility Location on Planar Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen=Addad:Vincent.html">Vincent Cohen-Addad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, Michał Pilipczuk <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10680">PDF</a><br /><b>Abstract: </b>We consider the classic Facility Location problem on planar graphs
(non-uniform, uncapacitated). Given an edge-weighted planar graph $G$, a set of
clients $C\subseteq V(G)$, a set of facilities $F\subseteq V(G)$, and opening
costs $\mathsf{open} \colon F \to \mathbb{R}_{\geq 0}$, the goal is to find a
subset $D$ of $F$ that minimizes $\sum_{c \in C} \min_{f \in D}
\mathrm{dist}(c,f) + \sum_{f \in D} \mathsf{open}(f)$.
</p>
<p>The Facility Location problem remains one of the most classic and fundamental
optimization problem for which it is not known whether it admits a
polynomial-time approximation scheme (PTAS) on planar graphs despite
significant effort for obtaining one. We solve this open problem by giving an
algorithm that for any $\varepsilon&gt;0$, computes a solution of cost at most
$(1+\varepsilon)$ times the optimum in time $n^{2^{O(\varepsilon^{-2} \log
(1/\varepsilon))}}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10680"><span class="datestr">at April 25, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10493">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10493">Counting perfect matchings and the eight-vertex model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Jin=Yi.html">Jin-Yi Cai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Tianyu.html">Tianyu Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10493">PDF</a><br /><b>Abstract: </b>We study the approximation complexity of the partition function of the
eight-vertex model on general 4-regular graphs. For the first time, we relate
the approximability of the eight-vertex model to the complexity of
approximately counting perfect matchings, a central open problem in this field.
Our results extend those in <a href="http://export.arxiv.org/abs/1811.03126">arXiv:1811.03126</a> [cs.CC].
</p>
<p>In a region of the parameter space where no previous approximation complexity
was known, we show that approximating the partition function is at least as
hard as approximately counting perfect matchings via approximation-preserving
reductions. In another region of the parameter space which is larger than the
previously known FPRASable region, we show that computing the partition
function can be reduced to (with or without approximation) counting perfect
matchings. Moreover, we give a complete characterization of nonnegatively
weighted (not necessarily planar) 4-ary matchgates, which has been open for
several years. The key ingredient of our proof is a geometric lemma.
</p>
<p>We also identify a region of the parameter space where approximating the
partition function on planar 4-regular graphs is feasible but on general
4-regular graphs is equivalent to approximately counting perfect matchings. To
our best knowledge, these are the first problems of this kind.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10493"><span class="datestr">at April 25, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.10479">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.10479">Counting Induced Subgraphs: An Algebraic Approach to #W[1]-hardness</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=ouml=rfler:Julian.html">Julian Dörfler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Marc.html">Marc Roth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmitt:Johannes.html">Johannes Schmitt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellnitz:Philip.html">Philip Wellnitz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10479">PDF</a><br /><b>Abstract: </b>We study the problem #IndSub(P) of counting all induced subgraphs of size k
in a graph G that satisfy the property P. This problem was introduced by Jerrum
and Meeks and shown to be #W[1]-hard when parameterized by k for some families
of properties P including, among others, connectivity [JCSS 15] and even- or
oddness of the number of edges [Combinatorica 17]. Very recently [IPEC 18], two
of the authors introduced a novel technique for the complexity analysis of
#IndSub(P), inspired by the "topological approach to evasiveness" of Kahn, Saks
and Sturtevant [FOCS 83] and the framework of graph motif parameters due to
Curticapean, Dell and Marx [STOC 17], allowing them to prove hardness of a wide
range of properties P. In this work, we refine this technique for graph
properties that are non-trivial on edge-transitive graphs with a prime power
number of edges. In particular, we fully classify the case of monotone
bipartite graph properties: It is shown that, given any graph property P that
is closed under the removal of vertices and edges, and that is non-trivial for
bipartite graphs, the problem #IndSub(P) is #W[1]-hard and cannot be solved in
time f(k)*n^{o(k)} for any computable function f, unless the Exponential Time
Hypothesis fails. This holds true even if the input graph is restricted to be
bipartite and counting is done modulo a fixed prime. A similar result is shown
for properties that are closed under the removal of edges only.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.10479"><span class="datestr">at April 25, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15798">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/">Why Check A Proof?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Why check another’s proof?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" alt="" width="300" class="alignright size-medium wp-image-15809" height="203" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=584" alt="" class="aligncenter size-full wp-image-15804" /></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.</p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where “proof” is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on “why check proofs” you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead’s three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p></p><h2> Why Check A Proof? </h2><p></p>
<p></p><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person’s proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p></p><h3> Bob has to </h3><p></p>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p></p><h3> Bob wants to </h3><p></p>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice’s theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should—if possible–explain that her proof of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses something new. Proofs that are “just technical inductions” are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction…</i> This is not a good idea. </p>
<p>
</p><p></p><h3> Bob needs to </h3><p></p>
<p>This happens when Bob has some “skin” in the game. A classic situation is when Bob has an earlier result that is affected by Alice’s new theorem. If <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is stronger than Bob’s previous result, then he is motivated to check her theorem. Or if <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p></p><h2> Skin in the Game </h2><p></p>
<p></p><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m,n)}" class="latex" title="{B(m,n)}" /> be the class of groups that are generated by <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> elements and all elements in the group satisfy, <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{n} = 1}" class="latex" title="{x^{n} = 1}" />. Sergei Adian and Pyotr Novikov proved that <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m, n)}" class="latex" title="{B(m, n)}" /> is infinite for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> odd, <img src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{n \ge 4381}}" class="latex" title="{{n \ge 4381}}" /> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton’s proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton’s 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" alt="" width="199" class="aligncenter size-medium wp-image-15802" height="300" /></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of “skin in the game”. He writes about an enumeration problem of counting how many curves lie on a certain manifold—a century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  317,206,375. " class="latex" title="\displaystyle  317,206,375. " /></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2,682,549,425. " class="latex" title="\displaystyle  2,682,549,425. " /></p>
<p>Somewhat a different count—not even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p></p><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p></p>
<p></p><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger’s page, including its use of “showed”:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, proves a new variant of the Union Theorem of McCreight and Meyer for <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p></p><p>
Woeginger gives a link to Hauptmann’s <a href="http://arxiv.org/abs/1602.04781">paper</a>, “On Alternation and the Union Theorem,” and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann’s idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, “Hence the assumption <img src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P = \Sigma_{2}^{p}}" class="latex" title="{P = \Sigma_{2}^{p}}" /> cannot hold.” We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p></p><h2> A Resolved Claim </h2><p></p>
<p></p><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\neq}" class="latex" title="{\neq}" /> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short—the crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24–48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum’s attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann’s paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/"><span class="datestr">at April 24, 2019 02:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold.html">Euler characteristics of non-manifold polycubes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>From a  block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmanifold-polycube.svg" alt="Removing two non-adjacent and non-opposite cubes from a 2x2 block of cubes" /></p>

<p>Any finite union of cubes of the integer lattice (not even necessarily connected) has as its boundary a set of vertices, edges, and squares, with each edge incident to an even number of squares. We can define the Euler characteristic to be the number of vertices minus edges plus squares, in the usual way. But we can also compute it in a different, more intrinsic and topological, way. For any  in the range , define the “shrunken interior” of the polycube to be the set of points of the interior farther than  from the boundary, and define the “shrunken exterior” in the same way. Then the shrunken interior and shrunken exterior both have (possibly disconnected) 2-manifolds as boundaries. We can define their Euler characteristics in the standard way from any cell decomposition of these boundaries (it doesn’t matter which cell decomposition we choose). Then the Euler characteristic of the polycube is the average of the Euler characteristics of the shrunken interior and shrunken exterior!</p>

<p>In the case of the mutilated  block, the shrunken interior and shrunken exterior are both topological balls (ignoring the puncture at infinity as it doesn’t have a boundary), so the average of their Euler characteristics is the Euler characteristic of a sphere, as we calculated.</p>

<p>There’s probably a simpler and more conceptual way of doing it, but here’s an explanation for why the Euler characteristic of the polycube boundary is the average of the Euler characteristics of the interior and exterior. Form a cell complex on the boundary of the interior and exterior, together, in the following way: expand each square of the polycube boundary to a cuboid with thickness 0.1, expand each edge into a cylinder with diameter 0.2 (big enough to enclose all the intersections of two expanded squares), and expand each vertex into a sphere with diameter 0.3 (big enough to enclose all the intersections of two cylinders but small enough that no two of these spheres touch). Remove the union of these expanded shapes from the space, and consider what’s left. It has the same topology as the union of the shrunken interior and exterior, and its boundary is now naturally divided up into cells: offset squares patches on the sides of each expanded square face, cylindrical patches on each expanded edge, and spherical patches on each expanded vertex, with curves where two patches meet.</p>

<p>Let’s calculate the Euler characteristic of this cell complex. Each square of the polycube leads to two offset square patches, so the  squares contribute   to the Euler characteristic. Each edge  of the polycube might be adjacent to two or four squares; call this number . Then the cylinder around  includes  surface patches between pairs of squares and  curves connecting them to the square patches. The patches count  and the curves count  for a total contribution to the Euler characteristic of .</p>

<p>Finally, each vertex of the polycube becomes a sphere, subdivided by the patches and curves of the complex. These spheres also contain all the vertices of the complex. The Euler characteristic of a subdivided sphere would be , but the vertex spheres have some parts of their subdivision removed. Where each edge cylinder or expanded square comes into the sphere, a patch of surfaces is removed, and the curves between these removed patches are also removed. An edge  with degree  contributes to the removal of  curves and  patches (one for itself and  for each adjacent square). So if there are  vertices in the polycube, the  contribution from the Euler characteristics of the subdivided spheres is modified by subtracting  for each incident edge. The total modification at both endpoints of each edge is . The  that we calculated here is cancelled by the  on the cylinder for , and we are left with a total modification of  where  is the number of polycube edges.</p>

<p>Putting all the pieces of this calculation together, the complex we have constructed on the union of the shrunken interior and exterior has Euler characteristic . Therefore, the Euler characteristic of the polycube boundary itself, , equals the average of the characteristics of the interior and exterior. The same reasoning shows more generally that whenever you have a finite cell complex embedded into , dividing space up into chambers, the Euler characteristic of the complex equals half the sum of Euler characteristics of the manifolds bounding shrunken chambers.</p>

<p>Although Euler characteristics of 2-manifolds embedded without boundary in  are always even, this averaging method can produce non-manifold surfaces with odd Euler characteristic. For instance, consider mutilating the  block in a different way, by removing two opposite cubes. The interior and exterior of the resulting polycube are both connected, but the interior is a solid torus and the exterior is a ball. So the Euler characteristic of the polycube should be the average of the torus and sphere, . And if we actually calculate it we get .</p>

<p>As this example shows, it’s possible for a polycube to have different topologies of surface on the interior and exterior, and it’s also possible to have different numbers of surfaces: for instance, two cubes attached vertex-to-vertex produce two interior surfaces but only one exterior. For cell complexes in , there appears to be no restriction on which combinations of surfaces are possible. But for cell complexes in other spaces (other 3-manifolds than Euclidean space) it may be possible to embed 2-manifolds with odd Euler characteristic. When this happens, the number of odd chambers of a cell complex must always be even. For, the parity of the sum of the Euler characteristics of the chambers must be even, in order to be able to divide by two and get an integer as the Euler characteristic of the cell complex.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/101978143052398446">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold.html"><span class="datestr">at April 23, 2019 04:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17325">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">An Invitation to a Conference: Visions in Mathematics towards 2000</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Let me invite you to a conference. The conference took place in 1999 but only recently <a href="https://www.youtube.com/playlist?list=PLP0YToNcfAwLBd8yibTtjv3aHfcbT4GBA">the 57 videos of the lectures and the discussion sessions are publicly available.</a> (I thank Vitali Milman for telling me about it.) One novel idea of Vitali Milman was to hold discussion sessions and they were quite interesting. (But, I am biased, I like discussions.) I will invite you to one of the heated discussions in the next post. There were very many nice talks and very many nice visions. And it is fun to watch the videos and judge the ideas in the perspective of time.</p>
<p>The proceedings appeared as GAFA special volumes, but alas the articles are not electronically available even to GAFA’s subscribers. Let me encourage both Birkhauser and the contributors to make them more available. My talk was: <a href="https://youtu.be/Wjg1_QwjUos">An invitation to Tverberg’s theorem</a>, and my own contribution to the Proceedings <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with a Geometric Flavor </a>is probably the widest scope survey article I ever wrote.  At the end of each section I added a brief philosophical thought about mathematics and those are collected in the post “<a href="https://gilkalai.wordpress.com/2008/10/12/about-mathematics/">about mathematics</a>“.</p>
<p>Two more things: The conference (and few others organized by Vitali) was  in Tel Aviv with a few days at the dead see and this worked very nicely.  Vitali also organized in the mid 90s another very successful geometry conference unofficially celebrating Gromov’s 50th birthday with, among others, a very nice lecture by Gregory Perelman. If videos will become available I will be delighted to invite you to that conference as well. Update from Vitali: It was also the week of Jeff Cheeger’s 50th birthday which was also celebrated. Grisha Perelman gave an absolutely excellent talk  talk on works of Cheeger.  Lectures were not videotaped.</p>
<p></p>
<p><span style="color: #ff0000;">Avi Wigderson’s lecture</span></p>
<p><span style="color: #ff0000;">Are so called “natural questions” good for mathematics. Specifically is Kepler’s questions about the densest packing of unit balls in 3-space interesting? Watch a discussion of Misha Gromov, Noga Alon, Laci Lovasz and others. (next post)</span></p>
<h2></h2>
<h2><a href="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png?w=640&amp;h=363" alt="" width="640" class="alignnone size-full wp-image-17362" height="363" /></a></h2>
<p><span style="color: #ff0000;">We were all so much younger!  (And in that old millennium,  we were also all men <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png" alt="😦" style="height: 1em;" class="wp-smiley" /> )</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha5.png"><img src="https://gilkalai.files.wordpress.com/2019/04/misha5.png?w=640&amp;h=614" alt="" width="640" class="alignnone size-full wp-image-17366" height="614" /></a></p>
<p><span style="color: #ff0000;">Misha Gromov argues passionately that natural problems are bad problems (see next post)</span></p>
<p>Pictures of most participants ad two slides are below</p>
<p><span id="more-17325"></span><br />
<a href="https://gilkalai.files.wordpress.com/2019/04/vim1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/vim1.png?w=640" alt="VIM1.png" style="font-size: 12px;" class="alignnone size-full wp-image-17372" /></a><br />
<img src="https://gilkalai.files.wordpress.com/2019/04/vim2.png?w=640" alt="VIM2.png" class="alignnone size-full wp-image-17373" /><img src="https://gilkalai.files.wordpress.com/2019/04/vim3.png?w=640" alt="VIM3.png" class="alignnone size-full wp-image-17374" /></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/vis.png"><img src="https://gilkalai.files.wordpress.com/2019/04/vis.png?w=640&amp;h=373" alt="" width="640" class="alignnone size-full wp-image-17387" height="373" /></a><img src="https://gilkalai.files.wordpress.com/2019/04/vim5.png?w=640" alt="VIM5.png" class="alignnone size-full wp-image-17376" /><img src="https://gilkalai.files.wordpress.com/2019/04/vim6.png?w=640" alt="VIM6.png" class="alignnone size-full wp-image-17377" /><img src="https://gilkalai.files.wordpress.com/2019/04/vim7.png?w=640" alt="VIM7.png" class="alignnone size-full wp-image-17378" /><a href="https://gilkalai.files.wordpress.com/2019/04/vim4.png"><img src="https://gilkalai.files.wordpress.com/2019/04/vim4.png?w=640" alt="VIM4.png" class="alignnone size-full wp-image-17375" /></a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/"><span class="datestr">at April 23, 2019 03:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8139226761048070665">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html">Quiz Show Scandals/Admissions Scandal/Stormy Daniels/Beer names:being  a lawyer would drive me nuts!!!!!!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
0) Charles van Doren (see <a href="https://en.wikipedia.org/wiki/Charles_Van_Doren">here</a>) passed away recently. For those who don't know he he was (prob most of you) he was one of the contestants involved in RIGGED quiz shows in the 1950's.  While there was a Grand Jury Hearing about Quiz Shows being rigged, nobody went to jail since TV was new and it was not clear if rigging quiz shows was illegal. Laws were then passed to make them it illegal.<br />
<br />
So why are today's so-called reality shows legal? I ask non-rhetorically.<br />
<br />
(The person he beat in a rigged game show- Herb Stempel (see <a href="https://en.wikipedia.org/wiki/Herb_Stempel">here</a>) is still alive.)<br />
<br />
1) The college admissions scandal. I won't restate the details and how awful it is since you can get that elsewhere and I doubt I can add much to it.  One thing I've heard in the discussions about it is a question that is often posted rhetorically but I want to pose for real:<br />
<br />
There are people whose parents give X dollars to a school and they get admitted even though they are not qualified. Why is that legal?<br />
<br />
I ask that question without an ax to grind and without anger. Why is out-right bribery of this sort legal?<br />
<br />
Possibilities:<br />
<br />
a) Its transparent. So being honest about bribery makes it okay?<br />
<br />
b) My question said `even though they are not qualified' - what if they explicitly or implicitly said `having parents give money to our school is one of our qualifications'<br />
<br />
c) The money they give is used to fund scholarships for students who can't afford to go. This is an argument for why its not immoral, not why its not illegal.<br />
<br />
But here is my question: Really, what is the legal issue here? It still seems like bribery.<br />
<br />
2) Big Oil gives money to congressman Smith, who then votes against a carbon tax. This seems like outright bribery<br />
<br />
Caveat:<br />
<br />
a) If Congressman Smith is normally a anti-regulation then he could say correctly that he was given the money because they agree with his general philosophy, so it's  not bribery.<br />
<br />
b) If Congressman smith is normally pro-environment and has no problem with voting for taxes then perhaps it is bribery.<br />
<br />
3) John Edwards a while back and Donald Trump now are claiming (not quite) that the money used to pay off their mistress to be quiet is NOT a campaign contribution, but was to keep the affair from his wife. (I don't think Donald Trump has admitted the affair so its harder to know what his defense is). But lets take a less controversial example of `what is a campaign contribution'<br />
<br />
I throw a party for my wife's 50th birthday and I invite Beto O'Rourke and many voters and some Dem party big-wigs to the party. The party costs me $50,000.  While I claim it's for my wife's bday it really is for Beto to make connections to voters and others. So is that a campaign contribution?<br />
<br />
4) The creators of HUGE ASS BEER are suing GIANT ASS BEER for trademark infringement. I am not making this up- see <a href="https://thetakeout.com/huge-giant-ass-beer-lawsuit-new-orleans-1832989913">here</a><br />
<br />
---------------------------------------------------------<br />
<br />
All of these cases involve ill defined questions (e.g., `what is a bribe'). And the people arguing either side are not unbiased. The cases also illustrate why I prefer mathematics: nice clean questions that (for the most part) have answers. We may have our biases as to which way they go, but if it went the other way we would not sue in a court of law.</div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html"><span class="datestr">at April 23, 2019 03:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4233">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/">The more things change the more they stay the same</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From a 1981 (!!) <a href="https://www.nytimes.com/1981/06/09/us/changing-san-francisco-is-foreseen-as-a-haven-for-wealthy-and-childless.html">New York Times Article</a> titled “Changing San Francisco is foreseen as a haven for wealthy and childless”:</p>
<blockquote><p>
A major reason for the exodus of the middle class from San Francisco, demographers say, is the high cost of housing, the highest in the mainland United States. Last month, the median cost of a dwelling in the San Francisco Standard Metropolitan Statistical Area was $129,000, according to the Federal Home Loan Bank Board in Washington, D.C. The comparable figure for New York, Newark and Jersey City was $90,400, and for Los Angeles, the second most expensive city, $118,400.</p>
<p>”This city dwarfs anything I’ve ever seen in terms of housing prices,” said Mr. Witte. Among factors contributing to high housing cost, according to Mr. Witte and others, is its relative scarcity, since the number of housing units has not grown significantly in a decade; the influx of Asians, whose first priority is usually to buy a home; the high incidence of adults with good incomes and no children, particularly homosexuals who pool their incomes to buy homes, and the desirability of San Francisco as a place to live.
</p></blockquote>
<p>$129,000 in 1981 dollars is $360,748 in 2019 dollars.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/"><span class="datestr">at April 23, 2019 01:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4230">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/">Online Optimization Post 0: Definitions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 Online convex optimization deals with the following setup: we want to design an algorithm that, at each discrete time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,2,\ldots}" class="latex" title="{t=1,2,\ldots}" />, comes up with a solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t \in K}" class="latex" title="{x_t \in K}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is a certain convex set of feasible solution. After the algorithm has selected its solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />, a convex cost function <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" title="{f_t : K \rightarrow {\mathbb R}}" />, coming from a known restricted set of admissible cost functions <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />, is revealed, and the algorithm pays the loss <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x_t)}" class="latex" title="{f_t (x_t)}" />. </p>
<p>
Again, the algorithm has to come up with a solution <em>without knowing what cost functions it is supposed to be optimizing</em>. Furthermore, we will think of the sequence of cost functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%2Cf_2%2C+%5Cldots%2Cf_t%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1,f_2, \ldots,f_t,\ldots}" class="latex" title="{f_1,f_2, \ldots,f_t,\ldots}" /> not as being fixed in advanced and unknown to the algorithm, but as being dynamically generated by an adversary, after seeing the solutions provided by the algorithm. (This resilience to adaptive adversaries will be important in most of the applications.)</p>
<p>
The <em>offline optimum</em> after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is the total cost that the best possible fixed solution would have incurred when evaluated against the cost functions seen by the algorithm, that is, it is a solution to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" title="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " /></p>
<p>
The <em>regret</em> after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is the difference between the loss suffered by the algorithm and the offline optimum, that is, </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" title="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " /></p>
<p>
The remarkable results that we will review give algorithms that achieve regret</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+O_%7BK%2C+%7B%5Ccal+F%7D%7D+%28%5Csqrt+T%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) " class="latex" title="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) " /></p>
<p> that is, for fixed <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal F}}" class="latex" title="{{\cal F}}" />, the regret-per-time-step goes to zero with the number of steps, as <img src="https://s0.wp.com/latex.php?latex=%7BO%5Cleft%28+%5Cfrac+1+%7B%5Csqrt+T%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O\left( \frac 1 {\sqrt T} \right)}" class="latex" title="{O\left( \frac 1 {\sqrt T} \right)}" />. It is intuitive that our bounds will have to depend on how big is the “diameter” of <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> and how large is the “magnitude” and “smoothness” of the functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f\in {\cal F}}" class="latex" title="{f\in {\cal F}}" />, but depending on how we choose to formalize these quantities we will be led to define different algorithms. </p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/"><span class="datestr">at April 22, 2019 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">P=NP Proofs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Advice to claimers</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/claimers3/" rel="attachment wp-att-15790"><img src="https://rjlipton.files.wordpress.com/2019/04/claimers3.png?w=300&amp;h=191" alt="" width="300" class="alignright size-medium wp-image-15790" height="191" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://the-walking-dead-tvseries.fandom.com/wiki/The_Claimers">The Claimers</a></font></td>
</tr>
</tbody>
</table>
<p>
The Claimers are a gang on the hit AMC television <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(TV_series)">series</a> <em>The Walking Dead</em>. They are the main antagonists in the second half of the zombie-apocalypse show’s Season 4. According to Wikipedia’s <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(season_4)#The_Claimers">description</a>, they “live by the philosophy of ‘claiming’.”</p>
<p>
Today Ken and I discuss issues about ‘claiming’ and give advice on how to present your claims—or not.</p>
<p>
Yes, this post is about our own “claimers” in complexity theory. It is especially about those who claim to have a solution to P=NP. We will not give any names today. You know who you are.</p>
<p>
The TV Claimers meet a grisly end. We will not say any more about it. We want to be nice. But we would like to not keep seeing the same level of zombie claims raised again and again.</p>
<p>
<b>Please: Do not stop reading.</b> Yes we know that it is likely that no claimer really has such a proof. However, our suggestions apply to all of us when we have a non-trivial result. Especially a result that has been open, even if the result is not a major open problem. So please keep reading today.</p>
<p>
</p><p></p><h2> So You Can Prove P=NP </h2><p></p>
<p></p><p>
This is a list of ideas for anyone who claims to have solved P=NP or some similar hard open problem in mathematics. There are already lots of suggestions online about what you should do, so this is just a list of additional thoughts. We hope they are helpful.</p>
<p>
</p><p></p><h3> You are being pretty arrogant </h3><p></p>
<p></p><p>
In order to succeed in mathematics research one has to be a bit arrogant. It is quite difficult to prove new things without some swagger. However, proving or resolving P=NP requires a very non-humble attitude. I think many claimers have not thought how arrogant they are being. The P=NP problem is a huge open problem. Thousands and thousands of researchers have spent years thinking about it. Why do you, the claimer, think you see the light and we remain in the dark?</p>
<p>
It might be useful for the claimers to ponder: <i>Why did I succeed where all others have failed?</i> It might be useful to be a bit humble and at least think what did they see that we all missed? If they can say something like:</p>
<blockquote><p><b> </b> <em> The reason I succeeded in finding an algorithm for P=NP is that I noticed that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> No one else seems to see that this insight is very powerful. It is very useful since it implies <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> </em>
</p></blockquote>
<p>
</p><p></p><h3> Working alone </h3><p></p>
<p></p><p>
I think the vast majority of claimers of P=NP or other big results have almost always worked alone. This is okay, but the average number of authors these days of a theory paper is pretty large. So any paper that is sole authored, perhaps, leads the community think it is unusual—and is wrong. Another point is that being part of a team may help control the arrogance. It can also be invaluable in detecting errors. </p>
<p>
</p><p></p><h3> Show them the money </h3><p></p>
<p></p><p>
There is an advantage in “proving” P=NP over other major problems. There is the Clay prize of a million dollars. I wonder if claimers could use the prize money in some interesting way. How about saying: If you read the my proof and repair it or make it more readable and it is correct, then you get something. A certain dollar amount. Or a percentage of the prize. Or—you get the idea.</p>
<p>
</p><p></p><h3> The role of code </h3><p></p>
<p></p><p>
Many claimers have also supplied working code for their algorithm. That is they also supply a program that claims to solve some NP-complete problem. I have several thoughts about this. In some cases it seems that it could be possible to have code that works for small size problems, but not in the general case. This seems to be possible for the claims by some that they can solve the Traveling Salesman Problem, for example. Their algorithm could be correct for small instances.</p>
<p>
Mathematics is filled with surprises like: This effect works for all values of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> less than some bound. If the claimers give a program, our expectation based on experience is that it may work for small cases but will probably fail in general. </p>
<p>
The last point is that working code could actually be valuable. If the code can be used to solve SAT problems how about using your program to enter a SAT contest and win it. A win, or even a good showing, would help tremendously in convincing people to read the paper. Or use the code to break some known cryptosystem. That would also convince people that they need to read your paper.</p>
<p>
</p><p></p><h2> Writing Your Paper </h2><p></p>
<p></p><p>
Okay we all dream about solving a major open problem. Or even a minor one. Here we give an outline of how to write up such a paper. </p>
<p></p><h3> How to write up the proof </h3><p></p>
<p>
I would suggest that you not have any statements about why P=NP is an important problem. None. No history of the problem. No literature survey is needed. None. You goal is to get an expert to read and believe the proof. They will just skip over the above. Also please no statements of how your algorithm that solves P=NP is going to change the world. Just give us the proof.</p>
<p>
</p><p></p><h3> How to get them to read the proof </h3><p></p>
<p></p><p>
This is really hard. Hard. I have read a number of claimers’ papers. I try to be helpful. However, many of us do not have the time to look at such papers. Years ago, before Fermat’s Last Theorem was solved, a famous mathematician once made up a post-card that looked like this:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/postcard/" rel="attachment wp-att-15784"><img src="https://rjlipton.files.wordpress.com/2019/04/postcard.png?w=300&amp;h=221" alt="" width="300" class="aligncenter size-medium wp-image-15784" height="221" /></a></p>
<p>
I think that we all have a mental version of this card. There are definitely ways to help induce someone to read a paper and its proof. Look at some recent top theory papers. Even the authors of these papers, often well known authors, work hard to motivate potential readers. The authors often do several things:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>They often sketch the proof.</em> By leaving out details they may help get a reader interested. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>They often explain the new trick—or tricks.</em> The goal here is to explain some new insight that is used in the proof. We are very self-oriented: If I see that your new trick could be useful in my research that is a huge motivator for me to understand the proof. People are very excited about a strong result, but they are even more excited about a new trick. Explain what is new in your proof. If there is nothing new, no new trick or method, then hmmmm<img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <em>They often first prove a weaker result.</em> That is, they show that their method can already make progress. If you could prove that the zeta function has all its nontrivial zeros on the critical line, that would be apocalyptic—the famous Riemann Hypothesis, of course. But if you could merely prove that there is no zero in some new region, then that would still be <em>wonderful</em>. And also probably more believable. If you could prove that there is no zero <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> with its real part <img src="https://s0.wp.com/latex.php?latex=%7B0.99999%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.99999}" class="latex" title="{0.99999}" /> that would be huge. If the proof of this is simpler, then use it to get readers excited about your full result. </p>
<p>
An observation related to the last point is: </p>
<blockquote><p><b> </b> <em> <i>Why do all claims of progress on P=NP give a polynomial time bound?</i> </em>
</p></blockquote>
<p>How about just getting a better bound of say <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{n/10}}" class="latex" title="{2^{n/10}}" /> for the Traveling Salesman Problem? Or a better bound for factoring? Or a better bound for your favorite problem?</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>I hope these points help. One last pointer is to double-check dependencies. If your proof relies on a result by someone else, make sure the result really gives what you need. Terms may be defined differently from what you expect, or you may really need a feature of the proof rather than the mere statement. A mis-attributed result can become “undead.”</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/"><span class="datestr">at April 22, 2019 03:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/062">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/062">TR19-062 |  Quantum Lower Bounds for Approximate Counting via Laurent Polynomials | 

	Scott Aaronson, 

	Robin Kothari, 

	William Kretschmer, 

	Justin Thaler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This paper proves new limitations on the power of quantum computers to solve approximate counting---that is, multiplicatively estimating the size of a nonempty set $S\subseteq [N]$.

Given only a membership oracle for $S$, it is well known that approximate counting takes $\Theta(\sqrt{N/|S|})$ quantum queries. But what if a quantum algorithm is also given "QSamples"---i.e., copies of the state $|S\rangle = \sum_{i\in S}|i\rangle$---or even the ability to apply reflections about $|S\rangle$? Our first main result is that, even then, the algorithm needs either $\Theta(\sqrt{N/|S|})$ queries or else $\Theta(\min\{|S|^{1/3},\sqrt{N/|S|}\})$ reflections or samples. We also give matching upper bounds.

We prove the lower bound using a novel generalization of the polynomial method of Beals et al. to Laurent polynomials, which can have negative exponents. We lower-bound Laurent polynomial degree using two methods: a new "explosion argument" and a new formulation of the dual polynomials method.

Our second main result rules out the possibility of a black-box Quantum Merlin-Arthur (or QMA) protocol for proving that a set is large. We show that, even if Arthur can make $T$ quantum queries to the set $S$, and also receives an $m$-qubit quantum witness from Merlin in support of $S$ being large, we have $Tm=\Omega(\min\{|S|,\sqrt{N/|S|}\})$. This resolves the open problem of giving an oracle separation between SBP and QMA.

Note that QMA is "stronger" than the queries+QSamples model in that Merlin's witness can be anything, rather than just the specific state $|S\rangle$, but also "weaker" in that Merlin's witness cannot be trusted. Intriguingly, Laurent polynomials also play a crucial role in our QMA lower bound, but in a completely different manner than in the queries+QSamples lower bound. This suggests that the "Laurent polynomial method" might be broadly useful in complexity theory.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/062"><span class="datestr">at April 21, 2019 01:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/061">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/061">TR19-061 |  A Quantum Query Complexity Trichotomy for Regular Languages | 

	Daniel Grier, 

	Luke Schaeffer, 

	Scott Aaronson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present a trichotomy theorem for the quantum query complexity of regular languages. Every regular language has quantum query complexity $\Theta(1)$, $\tilde{\Theta}(\sqrt n)$, or $\Theta(n)$. The extreme uniformity of regular languages prevents them from taking any other asymptotic complexity. This is in contrast to even the context-free languages, which we show can have query complexity $\Theta(n^c)$ for all computable $c \in [1/2,1]$. Our result implies an equivalent trichotomy for the approximate degree of regular languages, and a dichotomy---either $\Theta(1)$ or $\Theta(n)$---for sensitivity, block sensitivity, certificate complexity, deterministic query complexity, and randomized query complexity.

The heart of the classification theorem is an explicit quantum algorithm which decides membership in any star-free language in $\tilde{O}(\sqrt n)$ time. This well-studied family of the regular languages admits many interesting characterizations, for instance, as those languages expressible as sentences in first-order logic over the natural numbers with the less-than relation. Therefore, not only do the star-free languages capture functions such as OR, they can also express functions such as ``there exist a pair of 2's such that everything between them is a 0."  

Thus, we view the algorithm for star-free languages as a nontrivial generalization of Grover's algorithm which extends the quantum quadratic speedup to a much wider range of string-processing algorithms than was previously known.  We show a variety of applications---new quantum algorithms for dynamic constant-depth Boolean formulas, balanced parentheses nested constantly many levels deep, binary addition, a restricted word break problem, and path-discovery in narrow grids---all obtained as immediate consequences of our classification theorem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/061"><span class="datestr">at April 21, 2019 01:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17317">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/04/21/the-random-matrix-and-more/">The (Random) Matrix and more</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Three pictures, and a few related links.</p>
<h3>Van Vu</h3>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/thematrixvv.jpg"><img src="https://gilkalai.files.wordpress.com/2019/04/thematrixvv.jpg?w=640&amp;h=989" alt="" width="640" class="alignnone size-full wp-image-17318" height="989" /></a></p>
<p><span style="color: #ff0000;">Spoiler: In one of the most intense scenes, the protagonist, with his bare hands and against all odds, took care of the mighty Wigner semi-circle law in two different ways. (From VV’s FB)</span></p>
<p><a href="https://math.virginia.edu/ims/lectures/van-vu/">More information on Van Vu’s series of lectures</a>. <a href="http://campuspress.yale.edu/vanvu/">Van Vu’s home page</a>; Related posts: <a href="https://www.scottaaronson.com/blog/?p=3482">did physicists really just prove that the universe is not a computer simulation—that we can’t be living in the Matrix?</a> (Shtetl-Optimized); A <a href="https://terrytao.wordpress.com/2012/02/02/random-matrices-the-universality-phenomenon-for-wigner-ensembles/">related 2012 post</a> on What’s New;</p>
<p>Two more pictures the first also from FB<span id="more-17317"></span></p>
<h3>Saharon Shelah</h3>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/saharon75.jpg"><img src="https://gilkalai.files.wordpress.com/2019/04/saharon75.jpg?w=640&amp;h=498" alt="" width="640" class="alignnone size-full wp-image-17320" height="498" /></a></p>
<p>Shaharon Shelah ICM 1974 (Vancouver) (Mohammad Golshani over FB)</p>
<p>See my post <a href="https://gilkalai.wordpress.com/2012/01/18/a-theorem-about-infinite-cardinals-everybody-should-know/" rel="bookmark">A theorem about infinite cardinals everybody should know</a>; Gowers’s post <a href="https://gowers.wordpress.com/2017/09/19/two-infinities-that-are-surprisingly-equal/">Two infinities that are surprisingly equal </a>about a recent breakthrough result by <a href="https://en.wikipedia.org/wiki/Maryanthe_Malliaris">Maryanthe Malliaris</a> and <a href="https://en.wikipedia.org/wiki/Saharon_Shelah">Saharon Shelah</a>; and <a href="https://arxiv.org/abs/1806.04917">a recent 4-page solution to a conjecture of Spencer</a> on finitary Hindman numbers by <a href="https://arxiv.org/search/math?searchtype=author&amp;query=Mohsenipour%2C+S">Shahram Mohsenipour</a> and Shelah.  More information on the last paper: (1) It is an Iranian-Israeli collaboration (2) Spencer asked Shelah the question during the workshop: Combinatorics: Challenges and Applications, celebrating Noga Alon’s <a href="https://gilkalai.wordpress.com/2015/08/10/nogafest-nogaformulas-and-amazing-cash-prizes/">60th birthday</a>, Tel Aviv University, January 17-21, 2016. (3) This is paper 1146 in Shelah’s (main) <a href="http://shelah.logic.at/">list of publications</a>. Shelah’s 1974 lecture was called “Why There Are Many Nonisomorphic Models for Unsuperstable Theories.”</p>
<h3>Sándor Szalai,  Catherine Rényi, Alfréd Rényi András Hajnal and Paul Erdős</h3>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/matrahazaszalairenyihajnalerdos1958.jpg"><img src="https://gilkalai.files.wordpress.com/2019/04/matrahazaszalairenyihajnalerdos1958.jpg?w=640&amp;h=434" alt="" width="640" class="alignnone size-full wp-image-17314" height="434" /></a></p>
<p>From left: Sándor Szalai,  Catherine Rényi, Alfréd Rényi, András Hajnal and Paul Erdős (Matrahaza ) The picture is from Janos Pach’s Lancaster lecture, who also discussed how Szalai came up with Ramsey’s theorem. (See also Noga Alon and Michel Krivelevich’s chapter <a href="http://www.cs.tau.ac.il/~nogaa/PDFS/epc7.pdf" target="_blank" rel="noopener" title="opens in a new window">Extremal and Probabilistic Combinatorics, In: Princeton Companion to Mathematics, W. T. Gowers, Ed., Princeton University Press 2008, pp. 562-575.</a>)</p>
<p>In the course of an examination of friendship between children some fifty years ago, the Hungarian sociologist Sandor Szalai observed that among any group of about twenty children he checked he could always find four children any two of whom were friends, or else four children no two of whom were friends. Despite the temptation to try to draw sociological conclusions, Szalai realized that this might well be a mathematical phenomenon rather than sociological one.  He got interested in the problem, discussed it with  Erdős, Rényi , and Turán and in a short time he came up with a number of interesting constructions. In fact, he obtained record lower bound estimates for several Ramsey numbers.</p>
<p>(Janos’ further remarks: “Sandor (Alexander) Szalai was a well known Hungarian sociologist and a famously bright and witty man. I am not sure whether he was the first to notice and study the the laws of clique- and anti-clique formation among groups of schoolchildren, but I suspect that he was not. The sociology of small groups used to be a popular alternative to more “dangerous” Marxist theories of classes in the 50-ies and 60-ies. My guess would be that Szalai discussed these issues and was fascinated by this subject some time around 1960. It is fair to say that he independently<em> conjectured</em> Ramsey’s theorem.”)</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/04/21/the-random-matrix-and-more/"><span class="datestr">at April 21, 2019 06:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4172">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4172">Not yet retired from research</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Last night, two papers appeared on the quantum physics arXiv that my coauthors and I have been working on for more than a year, and that I’m pretty happy about.</p>



<p>The first paper, with Guy Rothblum, is <a href="https://arxiv.org/abs/1904.08747">Gentle Measurement of Quantum States and Differential Privacy</a> (85 pages, to appear in STOC’2019).  This is Guy’s first paper that has anything to do with quantum, and also my first paper that has anything to do with privacy.  (What do I care about privacy?  I just share everything on this blog…)  The paper has its origin when I gave a talk at the Weizmann Institute about “shadow tomography” (a task where you have to measure quantum states very carefully to avoid destroying them), and Guy was in the audience, and he got all excited that the techniques sounded just like what they use to ensure privacy in data-mining, and I figured it was just some wacky coincidence and brushed him off, but he persisted, and it turned out that he was 100% right, and our two fields were often studying the same problems from different angles and we could prove it.  Anyway, here’s the abstract:</p>



<blockquote>In <i>differential privacy (DP)</i>, we want to query a database about n users, in a way that “leaks at most ε about any individual user,” even conditioned on any outcome of the query.  Meanwhile, in <i>gentle measurement</i>, we want to measure n quantum states, in a way that “damages the states by at most α,” even conditioned on any outcome of the measurement.  In both cases, we can achieve the goal by techniques like deliberately adding noise to the outcome before returning it.  This paper proves a new and general connection between the two subjects.  Specifically, we show that on products of n quantum states, any measurement that is α-gentle for small α is also O(α)-DP, and any product measurement that is ε-DP is also O(ε√n)-gentle.
<p>Illustrating the power of this connection, we apply it to the recently studied problem of <i>shadow tomography</i>.  Given an unknown d-dimensional quantum state ρ, as well as known two-outcome measurements E<sub>1</sub>,…,E<sub>m</sub>, shadow tomography asks us to estimate Pr[E<sub>i</sub> accepts ρ], for <i>every</i> i∈[m], by measuring few copies of ρ.  Using our connection theorem, together with a quantum analog of the so-called <i>private multiplicative weights</i> algorithm of Hardt and Rothblum, we give a protocol to solve this problem using O((log m)<sup>2</sup>(log d)<sup>2</sup>) copies of ρ, compared to Aaronson’s previous bound of ~O((log m)<sup>4</sup>(log d)).  Our protocol has the advantages of being <i>online</i> (that is, the E<sub>i</sub>‘s are processed one at a time), gentle, and conceptually simple.
</p><p>Other applications of our connection include new <i>lower</i> bounds for shadow tomography from lower bounds on DP, and a result on the safe use of estimation algorithms as subroutines inside larger quantum algorithms.</p></blockquote>



<p>The second paper, with Robin Kothari, UT Austin PhD student William Kretschmer, and Justin Thaler, is <a href="https://arxiv.org/abs/1904.08914">Quantum Lower Bounds for Approximate Counting via Laurent Polynomials</a>.  Here’s the abstract:</p>



<blockquote>Given only a membership oracle for S, it is well-known that approximate counting takes Θ(√(N/|S|)) quantum queries.  But what if a quantum algorithm is also given “QSamples”—i.e., copies of the state |S〉=Σ<sub>i∈S</sub>|i〉—or even the ability to apply reflections about |S〉?  Our first main result is that, even then, the algorithm needs either Θ(√(N/|S|)) queries or else Θ(min{|S|<sup>1/3</sup>,√(N/|S|)}) reflections or samples.  We also give matching upper bounds.

<p>We prove the lower bound using a novel generalization of the polynomial method of Beals et al. to <i>Laurent polynomials</i>, which can have negative exponents.  We lower-bound Laurent polynomial degree using two methods: a new “explosion argument” that pits the positive- and negative-degree parts of the polynomial against each other, and a new formulation of the dual polynomials method.

</p><p>Our second main result rules out the possibility of a black-box Quantum Merlin-Arthur (or QMA) protocol for proving that a set is large. More precisely, we show that, even if Arthur can make T quantum queries to the set S⊆[N], and also receives an m-qubit quantum
witness from Merlin in support of S being large, we have Tm=Ω(min{|S|,√(N/|S|)}).  This resolves the open problem of giving an oracle separation between SBP, the complexity class that captures
approximate counting, and QMA.

</p><p>Note that QMA is “stronger” than the queries+QSamples model in that Merlin’s witness can be anything, rather than just the specific state |S〉, but also “weaker” in that Merlin’s witness cannot be trusted.  Intriguingly, Laurent polynomials <i>also</i> play a crucial role in our QMA lower bound, but in a completely different
manner than in the queries+QSamples lower bound.  This suggests that the “Laurent polynomial method” might be broadly useful in complexity theory.</p></blockquote>



<p>I need to get ready for our family’s Seder now, but after that, I’m happy to answer any questions about either of these papers in the comments.</p>



<p>Meantime, the biggest breakthrough in quantum complexity theory of the past month isn’t either of the above: it’s the <a href="https://arxiv.org/abs/1904.05870">paper by Anand Natarajan and John Wright</a> showing that MIP*, or multi-prover interactive proof systems with entangled provers, contains NEEXP, or nondeterministic <strong>doubly</strong>-exponential time (!!).  I’ll try to blog about this later, but if you can’t wait, check out <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">this excellent post by Thomas Vidick</a>.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4172"><span class="datestr">at April 19, 2019 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/18/a-reason-why-circuit-lower-bounds-are-hard/">A Reason Why Circuit Lower Bounds Are Hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>And a possible approach to avoid this obstacle</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/04/kabanetssfu.jpg"><img src="https://rjlipton.files.wordpress.com/2019/04/kabanetssfu.jpg?w=129&amp;h=200" alt="" width="129" class="alignright wp-image-15769" height="200" /></a></p>
<p>
Valentine Kabanets is a famous complexity theorist from Simon Fraser University. He has been at the forefront of lower bounds for over two decades. </p>
<p>
Today we draw attention to this work and raise an idea about trying to unravel what makes circuit lower bounds hard.<span id="more-15768"></span></p>
<p>
He is the common author on <a href="https://eccc.weizmann.ac.il/report/2019/022/">two</a> new <a href="https://eccc.weizmann.ac.il/report/2019/018/">papers</a> on the Minimum Circuit Size Problem (MCSP), which belongs to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> but is not known to be complete or in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P}}" class="latex" title="{\mathsf{P}}" />. We <a href="https://rjlipton.wordpress.com/2015/03/05/news-on-intermediate-problems/">posted</a> on MCSP four years ago and mentioned his 1999 <a href="http://eccc.hpi-web.de/report/1999/045">paper</a> with Jin-Yi Cai, which gives evidence for MCSP truly being neither complete nor in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P}}" class="latex" title="{\mathsf{P}}" />. This “intermediate” status and the problem’s simplicity have raised hopes that direct attacks might succeed. The new papers prove direct lower bounds against some restricted circuit/formula models, including constant-depth circuits with mod-<img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> gates for <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> prime. But they stop short of mod-<img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> composite and other barrier cases.</p>
<p>
He has a nifty research <a href="https://www.cs.sfu.ca/~kabanets/research.html">statement</a> on his home page. It shows how derandomization, pseudorandomness, circuit complexity, and crypto combine into his two current projects. In a clickable tab for the third heading, he puts the meta-issue in pithy terms:</p>
<blockquote><p><b> </b> <em> <i>Why is proving circuit lower bounds so difficult?</i> </em>
</p></blockquote>
<p></p><p>
His first answer tab speaks a connection we have also often emphasized here:</p>
<blockquote><p><b> </b> <em> Traditionally, designing efficient algorithms is the subject of the theory of algorithms, while lower bounds are sought in complexity theory. It turns out, however, that there is a deep connection between the two directions: better algorithms (for a certain class of problems) also yield strong lower bounds (for related problems), and vice versa: strong lower bounds translate into more efficient algorithms. </em>
</p></blockquote>
<p></p><p>
Of course we agree, and we love connections shown in the new papers to problems such as distinguishing a very slightly biased coin from a true one. But we will try to supplement the algorithmic view of circuit lower bounds with a direct look at the underlying logic.</p>
<p>
</p><p></p><h2> Logical Structure of Lower Bounds </h2><p></p>
<p></p><p>
Okay we all know that circuit lower bounds are hard. For all Kabanets’ success and beautiful work—he like the rest of the complexity field—are unable to prove what we believe is true. They cannot in the full circuit model prove anything close to what is believed to be true for at least a half a century: There are explicit Boolean functions that cannot be computed by any linear size circuit.</p>
<p>
We feel that the logical structure of lower bounds statements gives insight into their difficulty. Perhaps this is almost a tautology. Of course the logical structure of any mathematical statement helps us understand its inherent difficulty. But we believe more: That this structure can reveal quite a bit about lower bounds. Let’s take a look at lower bounds and see if this belief holds up.</p>
<p>
In particular let’s compare the two main approaches to proving lower bounds: non-uniform and uniform. Our claim is that they have different logical structure, and that this difference explains why there is such a gap between the two. While lower bounds—non-uniform or uniform—are hard, uniform ones are at least possible now. Non-uniform lower bounds are really very difficult.</p>
<p>
Here is one example. To prove an explicit size lower bound for Boolean circuits—we’ll be content with just a linear one—we must give a particular family of Boolean functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bn%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{n}(x)}" class="latex" title="{f_{n}(x)}" /> (each of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> inputs) so that:</p>
<ol>
<li>
Given <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> we can evaluate <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bn%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{n}(x)}" class="latex" title="{f_{n}(x)}" /> in polynomial time; <p></p>
</li><li>
There is no Boolean circuit of size <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha n}" class="latex" title="{\alpha n}" /> that correctly computes <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bn%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{n}(x)}" class="latex" title="{f_{n}(x)}" /> on all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> of length <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.
</li></ol>
<p>
Here <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" /> is a constant and <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is assumed to be large enough. The terrific <a href="http://www.wisdom.weizmann.ac.il/~ranraz/publications/P5nlb.pdf">paper</a> of Kazuo Iwama, Oded Lachish, Hiroki Morizumi, and Ran Raz gives explicit Boolean functions whose size for circuits with the usual <em>not</em> and binary <em>and</em> and <em>or</em> operators exceeds <img src="https://s0.wp.com/latex.php?latex=%7B5n-o%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{5n-o(n)}" class="latex" title="{5n-o(n)}" />. </p>
<p>
</p><p></p><h2> An Approach </h2><p></p>
<p></p><p>
Let’s look at the above example more carefully. Suppose that in place of a single Boolean function on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> inputs we have a list of them: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%7Bn%2C1%7D%28x%29%2C%5Cdots%2Cf_%7Bn%2Cm%7D%28x%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_{n,1}(x),\dots,f_{n,m}(x). " class="latex" title="\displaystyle  f_{n,1}(x),\dots,f_{n,m}(x). " /></p>
<p>Can we prove the following? </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cexists+n_%7B0%7D%5C+%5Cforall+n+%3E+n_%7B0%7D+%5C+%5Cexists+f_%7Bn%2Ck%7D+%5C+%5Cforall+C+%5Cin+%5Cmathsf%7BSIZE%7D%28%5Calpha+n%29+%5C+%5Cneg%5Cmathsf%7Bcompute%7D%28C%2Cf_%7Bn%2Ck%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \exists n_{0}\ \forall n &gt; n_{0} \ \exists f_{n,k} \ \forall C \in \mathsf{SIZE}(\alpha n) \ \neg\mathsf{compute}(C,f_{n,k}). " class="latex" title="\displaystyle  \exists n_{0}\ \forall n &gt; n_{0} \ \exists f_{n,k} \ \forall C \in \mathsf{SIZE}(\alpha n) \ \neg\mathsf{compute}(C,f_{n,k}). " /></p>
<p>The first thing to note is the effect of letting the number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> of functions vary:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m = 1}" class="latex" title="{\bf m = 1}" />, this just becomes our original explicit circuit lower bound problem. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> is a huge value, however, this becomes the exponential lower bound shown by Claude Shannon—a known quantity. </p>
<p>
In our terms, the latter takes <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> equal to <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B2%5E%7Bn%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{2^{n}}}" class="latex" title="{2^{2^{n}}}" />, so that given <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> our function list is just the list of all Boolean functions. If all we care about is an <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha n}" class="latex" title="{\alpha n}" /> lower bound, then the high end of the range can be something like <img src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+2%5E%7B2%5Calpha+n%5Clog%28n%29%7D+%3D+n%5E%7B2%5Calpha+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m = 2^{2\alpha n\log(n)} = n^{2\alpha n}}" class="latex" title="{m = 2^{2\alpha n\log(n)} = n^{2\alpha n}}" />. So at the high end we have a simple counting argument for the proof but have traded away explicitness. The question will be about the tradeoffs for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> in-between the extremes.</p>
<p>
</p><p></p><h2> An Analogy </h2><p></p>
<p></p><p>
The above idea that we can model the lower bound methods by controlling the length of the list of the functions is the key to our approach. Perhaps it may help to note an analogy to other famous hard problems of constructing explicit objects. In particular, let’s look at constructing transcendental numbers. Recall these are real numbers that are not algebraic: they are not roots of polynomials with integer coefficients. They include <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+%3D+3.14159%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi = 3.14159\dots}" class="latex" title="{\pi = 3.14159\dots}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+2.71828%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = 2.71828\dots}" class="latex" title="{e = 2.71828\dots}" /></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The <a href="https://en.wikipedia.org/wiki/Liouville_number">Liouville</a> numbers of Joseph Liouville. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+%5Csum_%7Bk%3D1%7D%5E%5Cinfty+%5Cfrac%7Ba_k%7D%7Bb%5E%7Bk%21%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x = \sum_{k=1}^\infty \frac{a_k}{b^{k!}}. " class="latex" title="\displaystyle  x = \sum_{k=1}^\infty \frac{a_k}{b^{k!}}. " /></p>
<p>These are explicit numbers that were proved by him in 1844 to be transcendental. In terms of our model <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m=1}" class="latex" title="{\bf m=1}" />.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The great <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi}" class="latex" title="{\pi}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> puzzle. This is the observation that of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+%2B+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi + e}" class="latex" title="{\pi + e}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi+-+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pi - e}" class="latex" title="{\pi - e}" />, at least one is a transcendental number. In our terms this gives <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m=2}" class="latex" title="{\bf m=2}" />.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> The famous theorem of Georg Cantor—read as proving the existence of transcendental numbers since algebraic ones are countable.</p>
<p>
Here the high end of the range is as extreme as can be. Cantor’s `list’ of numbers is uncountable—in our model, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> is the cardinality of the real numbers. Note, the fact that his <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> is huge, really huge, may explain why some at the time were unimpressed by this result. They wanted the ‘list’ to be small, actually they wanted <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m=1}" class="latex" title="{\bf m=1}" />. See <a href="https://www.jstor.org/stable/2975129?seq=1#page_scan_tab_contents">this</a> for a discussion of the history of these ideas.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> The theorem by Waim Zudilin, in a 2001 <a href="https://iopscience.iop.org/article/10.1070/RM2001v056n04ABEH000427/meta">paper</a>, that at least one of the numbers <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%285%29%2C+%5Czeta%287%29%2C+%5Czeta%289%29%2C+%5Czeta%2811%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\zeta(5), \zeta(7), \zeta(9), \zeta(11)}" class="latex" title="{\zeta(5), \zeta(7), \zeta(9), \zeta(11)}" /> must be irrational. It is for “irrational” not “transcendental,” but exemplified <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m = 4}" class="latex" title="{\bf m = 4}" /> in a highly nontrivial manner. The technical point that makes this work is interactions among these numbers that cannot be captured just by considering any one of them separately. This has <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m = 4}" class="latex" title="{\bf m = 4}" />.</p>
<p>
</p><p></p><h2> Joining Functions </h2><p></p>
<p></p><p>
The issue is this: Suppose that we have a list of several boolean functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B1%7D%28x%29%2C%5Cdots%2Cf_%7Bm%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{1}(x),\dots,f_{m}(x)}" class="latex" title="{f_{1}(x),\dots,f_{m}(x)}" />. Then we can join them together to form one function <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(x,y)}" class="latex" title="{g(x,y)}" /> so that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%2C1%29+%3D+f_%7B1%7D%28x%29%2C+%5Ccdots%2C+g%28x%2Cm%29+%3D+f_%7Bm%7D%28x%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g(x,1) = f_{1}(x), \cdots, g(x,m) = f_{m}(x). " class="latex" title="\displaystyle  g(x,1) = f_{1}(x), \cdots, g(x,m) = f_{m}(x). " /></p>
<p>Clearly the function <img src="https://s0.wp.com/latex.php?latex=%7Bg%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(x,y)}" class="latex" title="{g(x,y)}" /> is easy implies that all of the <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7By%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{y}(x)}" class="latex" title="{f_{y}(x)}" /> are easy. This join trick shows that we can encode several boolean functions into one function. Note, we can even make <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> have only order <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(n)}" class="latex" title="{\log(n)}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> has <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits.</p>
<p>
Thus we can join any collection of functions to make a “universal” one that is at least as hard as the worst of the single functions. More precisely, 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7Bcomplexity%7D%28g%29+%5Cge+%5Cmathsf%7Bcomplexity%7D%28f_%7By%7D%29+%5Ctext%7B+for+%7D+y%3D1%2C%5Cdots%2Cm.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{complexity}(g) \ge \mathsf{complexity}(f_{y}) \text{ for } y=1,\dots,m. " class="latex" title="\displaystyle  \mathsf{complexity}(g) \ge \mathsf{complexity}(f_{y}) \text{ for } y=1,\dots,m. " /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bcomplexity%7D%28h%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{complexity}(h)}" class="latex" title="{\mathsf{complexity}(h)}" /> is the circuit complexity of the boolean function <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h}" class="latex" title="{h}" />.</p>
<p>
If <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> is bigger than <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{O(n)}}" class="latex" title="{2^{O(n)}}" />, that is if <img src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+2%5E%7B%5Comega%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m = 2^{\omega(n)}}" class="latex" title="{m = 2^{\omega(n)}}" />, then the joined function has more than linearly many variables. Can we possibly establish nontrivial interactions among so many functions, say <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+m%7D+%3D+n%5E%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\bf m} = n^{2n}}" class="latex" title="{{\bf m} = n^{2n}}" />?</p>
<p>
One can also try to get this effect with fewer or no additional variables by taking the XOR of some subset of functions in the list. If this is done randomly for each input length <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> then one can expect hard functions to show up for many <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. If this process can then be <em>de-randomized</em>, then this may yield an explicit hard function. We wonder how this idea might meld with Andy Yao’s famous XOR Lemma and conditions to <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.2818&amp;rep=rep1&amp;type=pdf">de-randomize</a> it.</p>
<p>
</p><p></p><h2> Joining Numbers </h2><p></p>
<p></p><p>
Ken and I thought about the above simple fact about joins, which seems special to functions. Joining by interleaving the decimal expansions is not an arithmetic operation. However, it appears that there may be a similar result possible for transcendental numbers. </p>
<blockquote><p><b>Lemma 1</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\beta}" class="latex" title="{\beta}" /> are real numbers. Then 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Calpha+%2B+i%5Cbeta+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  \alpha + i\beta " class="latex" title="\displaystyle  \alpha + i\beta " /></p>
</em><p><em>is a transcendental complex number if at least one of <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\beta}" class="latex" title="{\beta}" /> are transcendental. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%3D+%5Calpha+%2B+i%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\gamma = \alpha + i\beta}" class="latex" title="{\gamma = \alpha + i\beta}" /> be an algebraic number. Thus there must be a polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bq%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q(x)}" class="latex" title="{q(x)}" /> with integer coefficients so that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++q%28%5Cgamma%29+%3D+q%28%5Calpha+%2B+i%5Cbeta%29+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  q(\gamma) = q(\alpha + i\beta) = 0. " class="latex" title="\displaystyle  q(\gamma) = q(\alpha + i\beta) = 0. " /></p>
<p>Then it follows by complex conjugation that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++q%28%5Calpha+-+i%5Cbeta%29+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  q(\alpha - i\beta) = 0. " class="latex" title="\displaystyle  q(\alpha - i\beta) = 0. " /></p>
<p>	 Therefore <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%2B+i%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha + i\beta}" class="latex" title="{\alpha + i\beta}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+-i%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha -i\beta}" class="latex" title="{\alpha -i\beta}" /> are both algebraic; thus, so is their sum which is <img src="https://s0.wp.com/latex.php?latex=%7B2%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\alpha}" class="latex" title="{2\alpha}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" /> is algebraic. It follows that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\beta}" class="latex" title="{\beta}" /> is also algebraic. This shows that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\gamma}" class="latex" title="{\gamma}" /> is transcendental. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
A question: Can we show that we can do a “join” operation for three or more numbers? That is given numbers <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7B1%7D%2C%5Cdots%2Cx_%7Bm%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{1},\dots,x_{m}}" class="latex" title="{x_{1},\dots,x_{m}}" /> can we construct a number <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> that is transcendental if and only if at least one of <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{i}}" class="latex" title="{x_{i}}" /> is transcendental?</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Is the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m}" class="latex" title="{\bf m}" /> model useful? Is it possible for it to succeed where a direct explicit argument (<img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+m%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\bf m} = 1}" class="latex" title="{{\bf m} = 1}" />) does not? Does it need <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+m%7D+%5Cgg+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\bf m} \gg 2^n}" class="latex" title="{{\bf m} \gg 2^n}" /> to rise above technical dependence on the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf+m+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf m = 1}" class="latex" title="{\bf m = 1}" /> case via the join construction?</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/04/valentine-sandan-test-action-small.jpeg"><img src="https://rjlipton.files.wordpress.com/2019/04/valentine-sandan-test-action-small.jpeg?w=235&amp;h=158" alt="" width="235" class="aligncenter wp-image-15770" height="158" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2">Maybe the barriers just need 3-Dan martial-arts treatment.  <a href="https://www.vancouverwestaikikai.com/programs/intro-beginner.shtml">Source</a>—our congrats.<br />
</font>
</td>
</tr>
</tbody></table></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/04/18/a-reason-why-circuit-lower-bounds-are-hard/"><span class="datestr">at April 18, 2019 10:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-4286404438772500234">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/04/the-complexity-of-identifying.html">The Complexity of Identifying Characteristic Formulae</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
One of the classic results in concurrency theory is the Hennessy-Milner Theorem. This result states that<br /><ol><li>two bisimilar states in a labelled transition system satisfy exactly the same formulae in a multi-modal logic now called Hennessy-Milner logic, and </li><li>two states in a labelled transition system that satisfy a mild finiteness constraint (called image finiteness)  and enjoy the same properties expressible in Hennessy-Milner logic are bisimilar.</li></ol>See, for instance, Section 1.2 in <a href="http://homepages.inf.ed.ac.uk/cps/chapbisim.pdf">these notes by Colin Stirling</a> for an exposition of that result. A consequence of the Hennessy-Milner Theorem is that whenever two states <i>p </i>and <i>q </i>in a labelled transition system are <i>not</i> bisimilar, one can come up with a formula in Hennessy-Milner logic that <i>p </i>satisfies, but<i> q </i>does not<i>. </i>Moreover, for each state <i>p </i>in a finite, loop-free labelled transition systems, it is possible to construct a formula <i>F(p) </i>in Hennessy-Milner logic that completely characterizes <i>p</i> up to bisimilarity. This means that, for each state <i>q</i>, <i>p</i> is bisimilar to <i>q</i> if, and only if, <i>q</i> satisfies <i>F(p)</i>. The formula<i> F(p) </i>is called a characteristic formula for<i> p </i>up to bisimilarity.<i> </i>One can obtain a similar result for states in finite labelled transition systems by extending Hennessy-Milner logic with greatest fixed points. <i><br /></i><br /><br />Characteristic formulae have a long history in concurrency theory. However, to be best of my knowledge, the complexity of determining whether a formula is characteristic had not been studied before <a href="https://sites.google.com/view/antonisachilleos">Antonis Achilleos</a> first addressed the problem in <a href="https://arxiv.org/abs/1605.01004">this conference paper</a>. In that paper, Antonis focused on the complexity of the problem of determining whether a formula <i>F</i> is complete, in the sense that, for each formula <i>G</i>, it can derive either <i>G</i> or its negation.<br /><br />Our recent preprint    <a href="http://icetcs.ru.is/theofomon/CharFormComplexity.pdf"><i>The   Complexity of Identifying Characteristic Formulae</i></a> extends the results originally obtained by Antonis to a variety of modal logics, possibly including least and greatest fixed-point operators. In the paper, we show that completeness, characterization, and validity have the same complexity — with some exceptions for which there are, in general, no complete formulae. So, for most modal logics of interest, the problem is coNP-complete or PSPACE-complete, and becomes EXPTIME-complete for modal logics with fixed points. To prove our upper bounds, we present a nondeterministic procedure with an oracle for validity that combines tableaux and a test for bisimilarity, and determines whether a formula is complete.<br /><br />I think that there is still a lot of work that can be done in studying this problem, with respect to a variety of other notions of equivalence considered in concurrency theory, so stay tuned for further updates.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/04/the-complexity-of-identifying.html"><span class="datestr">at April 18, 2019 05:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/060">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/060">TR19-060 |  Gentle Measurement of Quantum States and Differential Privacy | 

	Scott Aaronson, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In differential privacy (DP), we want to query a database about $n$ users, in a way that "leaks at most $\varepsilon$ about any individual user," even conditioned on any outcome of the query.  Meanwhile, in gentle measurement, we want to measure $n$ quantum states, in a way that "damages the states by at most $\alpha$," even conditioned on any outcome of the measurement.  In both cases, we can achieve the goal by techniques like deliberately adding noise to the outcome before returning it.  This paper proves a new and general connection between the two subjects. Specifically, we show that on products of $n$ quantum states, any measurement that is $\alpha$-gentle for small $\alpha$ is also $O( \alpha)$-DP, and any product measurement that is $\varepsilon$-DP is also $O(\varepsilon\sqrt{n})$-gentle.

Illustrating the power of this connection, we apply it to the recently studied problem of shadow tomography.  Given an unknown $d$-dimensional quantum state $\rho$, as well as known two-outcome measurements $E_{1},\ldots,E_{m}$, shadow tomography asks us to estimate $\Pr\left[  E_{i}\text{ accepts }\rho\right]  $, for every $i\in\left[ m\right]  $, by measuring few copies of $\rho$. Using our connection theorem, together with a quantum analog of the so-called private multiplicative weights algorithm of Hardt and Rothblum, we give a protocol to solve this problem using $O\left( \left(  \log m\right)  ^{2}\left(  \log d\right)  ^{2}\right)$ copies of $\rho$, compared to Aaronson's previous bound of $\widetilde{O} \left(\left(  \log m\right) ^{4}\left( \log d\right)\right) $.  Our protocol has the advantages of being online (that is, the $E_{i}$'s are processed one at a time), gentle, and conceptually simple.

Other applications of our connection include new lower bounds for shadow tomography from lower bounds on DP, and a result on the safe use of estimation algorithms as subroutines inside larger quantum algorithms.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/060"><span class="datestr">at April 18, 2019 12:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
