<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at September 04, 2021 10:26 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1871">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/1st-acm-conference-on-equity-and-access-in-algorithms-mechanisms-and-optimization-eaamo21/">1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO’21)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>From <a href="https://sites.google.com/view/irene-lo/home?authuser=0">Irene Yuan Lo</a>:</p>



<p>We are thrilled to announce that the registration for EAAMO ‘21 is now live! Please register for regular admission on <a href="https://eaamo21.eventbrite.com/" target="_blank" rel="noreferrer noopener">Eventbrite</a> by September 10, 2021.</p>



<p><strong>Conference registration</strong> is $20 for ACM members, $15 for students, and $35 for non-ACM members. We also provide <strong>financial assistance and data grants</strong> in order to waive registration fees and provide data plans to facilitate virtual attendance. Please apply <a href="https://docs.google.com/forms/d/e/1FAIpQLSe-SHJToB7Sm59M9L0ZNaPcGbzoHK5vUrg3DsZC8pcgDOJziQ/viewform" target="_blank" rel="noreferrer noopener">here</a> before September 10, 2021.</p>



<p>A main goal of the conference is to bridge research and practice. Please <a href="https://forms.gle/8kcKvQExYB7rJQaM7" target="_blank" rel="noreferrer noopener">nominate practitioners</a> working with underserved and disadvantaged communities to join us at the conference (you can also nominate yourself if you are a practitioner). Invited practitioners will be included in facilitated discussions with researchers. </p>



<p>For more information, please see below or visit <a href="https://eaamo.org/#home" target="_blank" rel="noreferrer noopener">our website</a> and contact us at <a href="mailto:gc@eaamo.org" target="_blank" rel="noreferrer noopener">gc@eaamo.org</a> with any questions.</p>



<p class="has-text-align-center">***</p>



<p>The inaugural <strong>Conference on </strong><strong>Equity and Access in Algorithms, Mechanisms, and Optimization</strong> (EAAMO ‘21) will take place on October 5-9, 2021, virtually, on Zoom and Gather.town. EAAMO ‘21 will be sponsored by ACM <a href="https://sigai.acm.org/" target="_blank" rel="noreferrer noopener">SIGAI</a> and <a href="https://www.sigecom.org/" target="_blank" rel="noreferrer noopener">SIGecom</a>. </p>



<p>The goal of this event is to highlight work where techniques from algorithms, optimization, and mechanism design, along with insights from the social sciences and humanistic studies, can improve access to opportunity for historically underserved and disadvantaged communities. </p>



<p>The conference aims to foster a multi-disciplinary community, facilitating interactions between academia, industry, and the public and voluntary sectors. The program will feature keynote presentations from researchers and practitioners as well as contributed presentations in the research and policy &amp; practice tracks. </p>



<p>We are excited to host a series of <strong>keynote speakers</strong> from a variety of fields: <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sassefa" target="_blank" rel="noreferrer noopener">Solomon Assefa</a> (IBM Research), <a href="https://campuspress.yale.edu/dirkbergemann/" target="_blank" rel="noreferrer noopener">Dirk Bergemann</a> (Yale University), <a href="https://www.elloraderenoncourt.com/" target="_blank" rel="noreferrer noopener">Ellora Derenoncourt</a> (University of California, Berkeley), <a href="https://web.stanford.edu/~ashishg/" target="_blank" rel="noreferrer noopener">Ashish Goel</a> (Stanford University), <a href="https://marylgray.org/" target="_blank" rel="noreferrer noopener">Mary Gray</a> (Microsoft Research), <a href="https://people.mpi-sws.org/~gummadi/" target="_blank" rel="noreferrer noopener">Krishna Gummadi</a> (Max Planck Institute for Software Systems), <a href="https://u.cs.biu.ac.il/~avinatan/" target="_blank" rel="noreferrer noopener">Avinatan Hassidim</a> (Bar Ilan University), <a href="https://www.some.ox.ac.uk/people/radhika-khosla/" target="_blank" rel="noreferrer noopener">Radhika Khosla</a> (University of Oxford), <a href="https://www.gob.mx/mejoredu/articulos/sylvia-ortega-salazar-fue-elegida-por-unanimidad-como-presidenta-del-consejo-ciudadano-de-mejoredu" target="_blank" rel="noreferrer noopener">Sylvia Ortega Salazar</a> (National College of Vocational and Professional Training), and <a href="https://bdtrust.org/trooper-sanders/" target="_blank" rel="noreferrer noopener">Trooper Sanders</a> (Benefits Data Trust).</p>



<p><em>ACM EAAMO is part of the </em><a href="http://md4sg.com/" target="_blank" rel="noreferrer noopener"><em>Mechanism Design for Social Good</em></a><em> (MD4SG) initiative,</em><em> and builds on the MD4SG technical </em><a href="http://md4sg.com/workshop/EC19/cfp.html" target="_blank" rel="noreferrer noopener"><em>workshop series</em></a><em> and tutorials at conferences including ACM EC, ACM COMPASS, ACM FAccT, and WINE.</em></p></div>







<p class="date">
by Omer Reingold <a href="https://toc4fairness.org/1st-acm-conference-on-equity-and-access-in-algorithms-mechanisms-and-optimization-eaamo21/"><span class="datestr">at August 24, 2021 03:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/08/22/itcs-2022-cfp-is-out/">ITCS 2022 – CFP is out</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>ITCS 2022 <a href="http://itcs-conf.org/itcs22/itcs22-cfp.html">Call for Papers</a> is out. Submission deadline is pretty soon:</p>



<p>All papers should be pre-registered with title, authors, and abstract, by <strong>September 5, 7:59PM EDT (UTC−04:00)</strong>.<br />Final submissions are due by <strong>September 9, 7:59PM EDT (UTC−04:00)</strong>.<br />(Intended author notification: <strong>November 1</strong>.)</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2021/08/22/itcs-2022-cfp-is-out/"><span class="datestr">at August 23, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=328">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/">How to Ask for a Letter of Recommendation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The end of the year is approaching at an alarming rate, which means that many students will require letters of recommendations for graduate school or jobs. These letters are written by more senior academics and researchers at the busiest time of the year, when they’re handling other responsibilities like research, teaching, grant writing, and reviewing, and may be writing a dozen letters on top of it all. The purpose of this post is to guide applicants on how to ask for a letter of recommendation, simultaneously strengthening the applicant’s package, and making the letter writing process as painless as possible for their writers. Feel free to share this with anyone applying for grad school or jobs. If someone is asking you for a letter, you could share this post with them, so they know what you need.</p>



<p>Note that this document is written by a computer scientist and intended for applications to computer science things, and may or may not carry over to other fields.</p>



<p><strong>Whom to ask</strong></p>



<p>Ask people who know you and your work well. Ideally people who you’ve worked on research with. Their words will carry the most influence for whatever position you’re applying to. On the other hand, if you don’t have a letter from someone you did a major research project with (especially when applying for grad school), this is a red flag. Aim to have at least one such letter. While grad school applications typically solicit three letters of recommendation, very few people have all focused on research (I personally had two, from co-advisors on one project).</p>



<p>Some have suggested having someone “at arm’s length” is beneficial for faculty applications (and anything that comes later, including awards and fellowships), in order to show that your work is known more broadly in the community. </p>



<p>Letters from people who did nothing more than teach you a course are typically valued much less than research-based letters. Nonetheless, most grad school applicants will have at least one or two such letters, since it is rare to have the opportunity to do research projects with three different advisors. If you must, try to request letters from people who you left an impression on, or otherwise had meaningful interactions with. Try to avoid letters from people who can do nothing more than repeat information on your CV/transcript: these letters are often very short and uninformative, and as with any such letter, are unlikely to help your case (and may even hurt it).</p>



<p><strong>When to ask</strong></p>



<p>Please ask <strong>well </strong>in advance. As a rough rule of thumb, by late October is reasonable for “standard” deadlines which are in December, which is common for CS grad school, postdoc, and faculty applications. You might have to ask earlier if there are earlier deadlines (which is the case for some postdoc applications), but give at least a 6 week lead time. Try to mention the earliest deadline in your email request.</p>



<p>Asking early also benefits the applicant. You should have a solid picture about these key details of your application well in advance.</p>



<p>I may still agree to write a letter for you even after the end of October, but chances decrease the longer you wait. On the other hand, I am more likely to agree to late requests if I know you very well or we have worked together very closely. However, I would still be annoyed, and you probably want to avoid annoying your allies in this process.</p>



<p><strong>What to provide</strong></p>



<p>You should give your letter writers as much information as possible. They might not use all of it, but it is good to have on hand when they are inevitably writing letters at 4 AM the day <s>before</s> after the deadline. I’m writing this as a catch-all list, so some points might not be relevant for the position you’re applying to. Say, when you’re applying for faculty positions, they don’t need to know your undergraduate grades. The goal is to give your writers the gist of who you’re trying to market yourself as, so they can support your story as best they can.</p>



<p>This doesn’t all need to be provided when you first ask, but please share it as early as possible. Make it as easy to find as you can. For example, a single email with everything as an attachment, and a clear title to the email such as “X’s application materials”. You could also share a link to a Dropbox or Google Drive folder with all the materials. This has the benefit that you can include preliminary versions of your materials, and update them as you refine them.</p>



<p>Mandatory:</p>



<ul><li>A shared spreadsheet with a list of all the places you’re applying to, the deadlines, any special instructions for letter submission, and a space to mark when the letter is submitted.</li><li>Your research statement/statement of purpose</li><li>Your CV</li></ul>



<p>Optional but recommended:</p>



<ul><li>Anything else that the application asks you for, e.g. your cover letter, teaching statement,  diversity statement, transcript.</li><li>Who else is writing letters for you, and what your relationship to them is.</li><li>A brief summary of the work you’ve done or the contributions you’ve made with the recommender, to refresh their memory.</li></ul>



<p>The last one is helpful since I like to know what “role” I’m playing. Am I your lead letter writer who people will look to first? Or am I the third letter, mostly confirming what other people already know? There may also be some unique perspective I can give, based on who your other reviewers are.</p>



<p><strong>Afterwards?</strong></p>



<p>To the best of my knowledge, it is not customary in Computer Science to send gifts to your letter writers. You should just thank them genuinely, and pass on the favour when you are in a similar position of power.</p>



<p><strong>What if…</strong></p>



<p>It has been brought to my attention that, unfortunately, many letter writers ask the applicant for either a draft or full letter, which they simply sign. I could write a whole post on this topic, but in short, I consider this unacceptable behaviour from the perspective of the letter writer, and I urge them to rethink this practice. </p>



<p>But what should the applicant do when put in this position? It helps if you can see what typical academic recommendation letters look like, since they have a certain tone, but access to such letters is generally out of the reach of most applicants. My recommendation is to be positive (now is not the time for modesty), but don’t lie (these things can follow you around for longer than you might think). Try to make the first paragraph a brief summary that conveys the overall sentiment of the letter. In subsequent paragraphs, you should describe the research you worked on with the letter writer (don’t assume the reader is highly familiar with the topic, but be succinct in terms of background), as well as your specific contributions to the project. Include anecdotes, if appropriate. This may be awkward, but you have been put into an awkward position.</p>



<p><strong>Acknowledgments</strong></p>



<p>Thanks to Anand Sarwate, Thomas Steinke, and Jon Ullman for helpful comments.</p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/"><span class="datestr">at August 18, 2021 04:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2762">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/08/13/random-2021-starts-on-monday/">RANDOM 2021 Starts on Monday</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Call for Participation: APPROX/RANDOM 2021</p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" target="_blank" rel="noreferrer noopener">https://approxconference.wordpress.com/approx-2021/</a><br />RANDOM: <a href="https://randomconference.com/random-2021-home/" target="_blank" rel="noreferrer noopener">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" target="_blank" rel="noreferrer noopener">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p>Hope to see you at the conference!</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2021/08/13/random-2021-starts-on-monday/"><span class="datestr">at August 13, 2021 11:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">School on Modern Directions in Discrete Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 13-17, 2021 Online https://www.him.uni-bonn.de/programs/future-programs/future-trimester-programs/discrete-optimization/discrete-optimization-school/ Aims and Scope: The school provides an introduction to some of the main topics of the trimester program on discrete optimization. The lectures will address the interface between tropical geometry and discrete optimization; recent developments in continuous optimization with applications to combinatorial problems; topics in approximation algorithms; and fixed parameter … <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/" class="more-link">Continue reading <span class="screen-reader-text">School on Modern Directions in Discrete Optimization</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/"><span class="datestr">at August 08, 2021 02:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">Workshop on Algorithms for Large Data (Online) 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 23-25, 2021 Online https://waldo2021.github.io/ Registration deadline: August 20, 2021 This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/"><span class="datestr">at August 06, 2021 09:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2754">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">Average-Case Fine-Grained Hardness, Part III</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Continuing our previous discussion, I will show another application of the new recipe described in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> (i.e., constructing a “good” polynomial for a problem of interest), which will establish average-case hardness of a problem related to the orthogonal vector (OV) problem. (Recall the OV problem: Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3D%5Comega%28%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\omega(\log n)" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. The reader can look up the worst-case hardness of the OV problem in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">first post</a> of the series.)</p>



<p>The motivating question is can we show average-case hardness for counting the number of orthogonal pairs in the OV problem by constructing a “good” polynomial? (One motivation is that such average-case hardness result could serve as the source of reductions for proving average-case hardness for many other problems, because OV is a main source of fine-grained hardness.) There is a good reason to believe the answer is no. Specifically, an <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-%5Cdelta%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-\delta})" class="latex" />-time algorithm for counting orthogonal pairs for average-case OV instances (here “average-case” is Erdős–Rényi random input model, and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta" class="latex" /> is a constant that depends on the parameter of the input model) was given in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>, while constructing a “good” polynomial would prove <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%5E%7B2-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(n^{2-\varepsilon})" class="latex" /> average-case hardness for any constant <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon&gt;0" class="latex" /> assuming randomized SETH. This indicates that sometimes constructing a “good” polynomial might be a little too ambitious goal.</p>



<p>Instead, can we first come up with a nice combinatorial problem that encodes OV on a slightly larger binary input space and then construct a “good” polynomial for counting solutions of this combinatorial problem? (The motivation is that since this combinatorial problems encodes OV, it is at least as hard as OV for worst case, and moreover, if we can construct a “good” polynomial for counting solutions of this combinatorial problem, by the new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>, counting solutions of this combinatorial problem for average case is (almost) as hard as that for worst case. Therefore, counting solutions of this combinatorial problem for average case is at least as hard as OV for worst case. More importantly, this combinatorial problem could serve as the source of reductions thanks to its combinatorial structure.) The factored OV problem introduced in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> gives a positive answer to this question. Analogously, they proposed factored variants for many other flagship fine-grained hard problems. By reductions to these factored problems, they managed to prove average-case fine-grained hardness for many natural combinatorial problems such as counting regular expression matchings (the featured image of this post is the web of reductions in their paper).</p>



<p>Next, for the purpose of exposition, I briefly sketch the high-level idea behind the factored OV problem (without even explicitly describing its combinatorial interpretation, since I will not show any reduction from this problem). </p>



<p><strong>Counting solutions for factored OV.</strong>  Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" /> (actually, <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog%5E2+n%2F%5Clog%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log^2 n/\log\log n)" class="latex" /> would also work, and the choice here is for simplicity), we encode <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%3A%3D%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29%5Ccirc+%5Ctextrm%7BLONG%7D%28x%5B%5Csqrt%7Bd%7D%2B1%3A2%5Csqrt%7Bd%7D%5D%29%5Ccirc%5Cdots%5Ccirc%5Ctextrm%7BLONG%7D%28x%5Bd-%5Csqrt%7Bd%7D%2B1%3Ad%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x):=\textrm{LONG}(x[1:\sqrt{d}])\circ \textrm{LONG}(x[\sqrt{d}+1:2\sqrt{d}])\circ\dots\circ\textrm{LONG}(x[d-\sqrt{d}+1:d])" class="latex" />,<br />where <img src="https://s0.wp.com/latex.php?latex=x%5Bi%3Aj%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x[i:j]" class="latex" /> represents the subvector (block) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> from the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th to the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="j" class="latex" />-th coordinate, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> is the long code encoding (specifically, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> maps a <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sqrt{d}" class="latex" />-dimensional binary vector <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> to a <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^{\sqrt{d}}" class="latex" />-dimensional vector binary vector <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x')" class="latex" /> of which all the coordinates are zero except the coordinate indexed by <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" />). Namely, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> is the concatenation of the long code encoding of each block of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, and thus, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" />, and for our choice of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />, we have that <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7Bn%5E%7Bo%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{n^{o(1)}}" class="latex" />. Therefore, by taking such encoding for the vectors in the OV instance, we blow up the size of input (and hence weaken the worst-case hardness of OV) very mildly.</p>



<p>The key advantage of such encoding is that the indicator function <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%5Cperp+y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[1:\sqrt{d}]\perp y[1:\sqrt{d}])" class="latex" /> (which outputs <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> if the two vectors are orthogonal and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0" class="latex" /> otherwise) can be represented as the sum of degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> monomials, of which the variables are the coordinates of <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" />. Indeed, we can first enumerate all the orthogonal pairs of vectors <img src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2%5Cin+%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1,v_2\in \{0,1\}^{\sqrt{d}}" class="latex" />, and we check whether <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_2" class="latex" />, by taking product of these two coordinates, and then, we take the sum of all the products.</p>



<p>Using this approach, for each <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, for each <img src="https://s0.wp.com/latex.php?latex=i%5Cin%5B%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i\in[\sqrt{d}]" class="latex" />, we get a degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> polynomial that computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%5Cperp+y%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[(i-1)\sqrt{d}+1:i\sqrt{d}]\perp y[(i-1)\sqrt{d}+1:i\sqrt{d}])" class="latex" /> on input <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(y)" class="latex" />. The product of these polynomials obviously computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5Cperp+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x\perp y)" class="latex" />, and moreover, this product is a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial (<img src="https://s0.wp.com/latex.php?latex=d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'" class="latex" />-partite polynomial is defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>), where each part corresponds to the coordinates of the long code encoding of a block (subvector) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />. If we sum up these <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomials for all pairs <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, we get a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial that precisely counts orthogonal pairs for the OV instance. We can let the field size of this polynomial be a prime <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^2" class="latex" /> is a trivial upper bound of the number of orthogonal pairs) such that the output of this polynomial is indeed the number of orthogonal pairs.</p>



<p>Notice that (i) Since the encoding only blows up the input size mildly, the worst-case hardness of OV (almost) carries over to evaluating this polynomial. (ii) Since <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" />, the <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial is a “good” polynomial (defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>). It follows from our new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> that evaluating this polynomial on binary input for average case (here “average case” means Erdős–Rényi random input model) is (almost) as hard as worst case. (iii) Last but not least, evaluating this polynomial on any <img src="https://s0.wp.com/latex.php?latex=z%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" /> (not just <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" />) can be interpreted as counting solutions for a combinatorial problem, which is the factored OV problem in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>. As I mentioned earlier, I will not explain the combinatorial interpretation in details. The takeaway is that counting solutions of such factored problem is average-case fine-grained hard, and its combinatorial structure allows possible reductions to other natural combinatorial problems.</p>



<p>Finally, I mention two broad research directions in this area: (i) design cryptographic primitives, e.g., one-way functions, based on these fine-grained average-case hardness results (or show complexity barriers) and (ii) prove fine-grained average-case hardness for decision problems (or design more efficient algorithms).</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk. </p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/"><span class="datestr">at August 06, 2021 03:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">Average-Case Fine-Grained Hardness, Part II</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, we did two examples of proving average-case fine-grained hardness via worst-case to average-case reductions. In this post, I want to continue the discussion of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques (in particular, on <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi graphs</a>) to showcase a new technique, which builds on the general recipe and the example of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques described in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>. In the <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">next post</a>, I will discuss how this new technique can be applied to some other combinatorial problems.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in Erdős–Rényi graph.</strong> A strong follow-up <a href="https://arxiv.org/abs/1903.08247">[BBB19]</a> of the result <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> we discussed in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> shows that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertex graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B%5Clog%5E%7BO%281%29%7D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{\log^{O(1)} n}" class="latex" /> in Erdős–Rényi graph (whereas the sampable distribution of the random graph in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> is somewhat unnatural). The key idea is a decomposition lemma which says for sufficiently large prime <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog%28p%29%5Clog%28p%2F%5Cvarepsilon%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log(p)\log(p/\varepsilon))" class="latex" />, for any constants <img src="https://s0.wp.com/latex.php?latex=0%3Cp%5E%7B%281%29%7D%2C%5Cdots%2Cp%5E%7B%28k%29%7D%3C1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0&lt;p^{(1)},\dots,p^{(k)}&lt;1" class="latex" /> (for our application, these constants will be equal), given independent Bernoulli random variables <img src="https://s0.wp.com/latex.php?latex=y_%7B%5Cell%7D%5Csim%5Ctextrm%7BBern%7D%28p%5E%7B%28%5Cell%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{\ell}\sim\textrm{Bern}(p^{(\ell)})" class="latex" />, the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B%5Cell%7D%5Cmod+p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sum_{\ell=0}^k 2^{\ell}y_{\ell}\mod p" class="latex" /> is close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}" class="latex" />, i.e., the statistical distance is less than <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> (later when we apply this lemma, we want <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" />, and therefore <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog+p%5Ccdot%28%5Clog+p%2B%5Clog+n%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log p\cdot(\log p+\log n))" class="latex" /> ). We skip the proof of this lemma which is a nice application of basic Fourier analysis.</p>



<p>As in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> denotes a constructed polynomial that computes the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques when the input is an adjacency matrix of a graph. The step 3 of the general recipe from the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> reduces counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for the worst-case graph to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> many uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> (recall in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> is the degree of the polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> after the Chinese remaindering trick). Based on the decomposition lemma, using standard sampling scheme (this is essentially <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>, which I will not go into the details, but I just want to mention that we would like the <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> in the decomposition lemma to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" /> such that the sampling scheme succeeds w.h.p. by a few attempts), we can further reduce evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" /> is a random 0-1 valued matrix that is statistically close to the adjacency matrix of an Erdős–Rényi graph. Now, if we can “pull out” the weighted sum in <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, then we are done, because evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on the adjacency matrix of an Erdős–Rényi graph is precisely counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for Erdős–Rényi graph.</p>



<p>When can we “pull out” the weighted sum for a polynomial <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />? One answer is when the polynomial is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite.</p>



<p>An <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m" class="latex" />-variate polynomial <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite if there is a partition of the set of variables <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7B%5Cbigcup%7D_%7Bj%5Cin%5Bd%5D%7D+S_j%3D%5Bm%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\dot{\bigcup}_{j\in[d]} S_j=[m]" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is the sum of monomials in which each monomial contains exactly one variable from each part <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_j" class="latex" /> (more formally, <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29%3D%5Csum_%7B%28i_1%2Ci_2%2C%5Cdots%2Ci_d%29%5Cin+S%7D%5Cprod_%7Bj%5Cin%5Bd%5D%7Dx_%7Bi_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)=\sum_{(i_1,i_2,\dots,i_d)\in S}\prod_{j\in[d]}x_{i_j}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+S_1%5Ctimes+S_2%5Ctimes%5Cdots%5Ctimes+S_d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S\subseteq S_1\times S_2\times\dots\times S_d" class="latex" />).</p>



<p>For <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, it is not hard to show that<br /><img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B1%2C%5Cell%7D%2C%5Cdots%2C%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7Bm%2C%5Cell%7D%29%3D%5Csum_%7B%5Cell_1%3D0%7D%5Ek%5Csum_%7B%5Cell_2%3D0%7D%5Ek%5Cdots%5Csum_%7B%5Cell_d%3D0%7D%5Ek+2%5E%7B%5Cell_1%2B%5Cdots%2B%5Cell_d%7D%5Ccdot+f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell}y_{1,\ell},\dots,\sum_{\ell=0}^k 2^{\ell}y_{m,\ell})=\sum_{\ell_1=0}^k\sum_{\ell_2=0}^k\dots\sum_{\ell_d=0}^k 2^{\ell_1+\dots+\ell_d}\cdot f(y_{1,\ell_1},\dots,y_{m,\ell_m})." class="latex" /><br />(Essentially, because two variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> never appear in the same monomial, we can enumerate variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> in the same order.)</p>



<p>Let us think of each <img src="https://s0.wp.com/latex.php?latex=y_%7Bi%2C%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{i,\ell}" class="latex" /> as a coordinate of Erdős–Rényi adjacency matrix <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(y_{1,\ell_1},\dots,y_{m,\ell_m})" class="latex" /> is evaluating <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> on an ensemble of distinct coordinates of <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />‘s, and such ensemble is obviously Erdős–Rényi as well. Therefore, we have managed to decompose <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> into sum of <img src="https://s0.wp.com/latex.php?latex=k%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^d" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s where each <img src="https://s0.wp.com/latex.php?latex=Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y^{(\ell_1,\dots,\ell_d)}" class="latex" /> denotes an Erdős–Rényi adjacency matrix. In the next paragraph, we will construct a <img src="https://s0.wp.com/latex.php?latex=d%3D%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, and therefore, we have reduced computing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> to computing <img src="https://s0.wp.com/latex.php?latex=k%5E%7B%5Cbinom%7Bt%7D%7B2%7D%7D%3D%5Clog%5E%7BO%281%29%7D+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^{\binom{t}{2}}=\log^{O(1)} n" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D+%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}} (Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s, which is a mild blow-up of the number of the random instances which the reduction needs to solve. (In general, we consider the reduction to be efficient when the number of random instances it needs to solve is <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />, and hence, as long as <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" />, we are good to go.)</p>



<p>Unfortunately, the <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> given in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> does not work. Instead, we first reduce counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <a href="https://en.wikipedia.org/wiki/Multipartite_graph"><img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph</a>, and then we construct a <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph. Reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph is standard. Simply consider the <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor product</a> between the original graph and another <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. The number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in the tensor product graph is exactly <img src="https://s0.wp.com/latex.php?latex=t%21&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t!" class="latex" /> times that in the original graph. Now given the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-paritite graph, let <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> denote the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th part of vertices, and let <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> (for <img src="https://s0.wp.com/latex.php?latex=i%3Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i&lt;j" class="latex" />) denote the adjacency matrix between <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_j" class="latex" />. Consider the new polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%5E%7B%281%2C2%29%7D%2CX%5E%7B%281%2C3%29%7D%2C%5Cdots%2CX%5E%7B%28t-1%2Ct%29%7D%29%3A%3D%5Csum_%7Bv_1%5Cin+V_1%7D%5Csum_%7Bv_2%5Cin+V_2%7D%5Cdots%5Csum_%7Bv_t%5Cin+V_t%7D%5Cprod_%7B%28i%2Cj%29%5Cin%5Cbinom%7B%5Bt%5D%7D%7B2%7D%7D+X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X^{(1,2)},X^{(1,3)},\dots,X^{(t-1,t)}):=\sum_{v_1\in V_1}\sum_{v_2\in V_2}\dots\sum_{v_t\in V_t}\prod_{(i,j)\in\binom{[t]}{2}} X_{v_i,v_j}^{(i,j)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X_{v_i,v_j}^{(i,j)}" class="latex" /> denotes the coordinate that indicates if there is an edge between <img src="https://s0.wp.com/latex.php?latex=v_i%2Cv_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_i,v_j" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by picking one vertex for each part and checking if these <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> vertices form a clique. It is indeed <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite as each <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> corresponds to a part of variables.</p>



<p><strong>New recipe for worst-case to average-case reductions.</strong> Let us take a minute to think about what structural properties of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques we have used in the entire reduction except for constructing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />.</p>



<p>The answer is none! The only part specific to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques was cooking up a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" /> (let us call such polynomial “good”) that encodes this problem. The reduction we showed above works as long as we can construct such “good” polynomial for a problem. Therefore, a new recipe, which was explicitly formulated in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> for proving average-case (here “average-case” means Erdős–Rényi random input model) fine-grained hardness for a problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D%5Ccap+%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}\cap \textrm{poly}(n)" class="latex" /> in P, is </p>



<ol><li>Construct a “good” polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_p%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_p^n" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=p%3D%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p=\textrm{poly}(n)" class="latex" />, such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li></ol>



<p>Short and sweet.</p>



<p>In the <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">final post</a>, I will present another instantiation of this new recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/"><span class="datestr">at July 30, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=891">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">Chess.com: 5|5 &gt; 1000</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today for the first time, I surpassed score 1000 on Chess.com playing 5|5, which means you start with a 5-minute budget, and every move you get 5 more seconds.  For a while I also played 3|2 (2-second increments), but it takes me about 2 seconds to move a piece, which means I lost games in which I knew exactly what to do, but simply couldn’t move the pieces fast enough, which I found frustrating.  Longer games I tried but I don’t seem to have the patience for.</p>



<p>I won’t reveal my id, because I feel bad about how much time I am spending losing at chess (and I think you could see all my games with my id, but I am not sure).  My self-imposed limit is losing no more than one game a day, which means on average playing 2 games per day.  (I had to stop and Google <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+i%2F2%5Ei+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\sum_i i/2^i = 2" class="latex" />; there’s a neat calculation-free proof of it which hopefully will make me remember this fact next time.)</p>



<p>However not being a robot, I sometimes get upset at the way I lose.  Most of my games are classified as <em>giveaway</em>, which means I was winning according to the computer (and myself), but then because of some stupid mistake I end up losing the game.  And so what the heck, I am better than this!, I break the rule and start another match — only to lose again, chess seems not to forgive hot heads.</p>



<p>The main reason why I play seems to be that fast-paced chess has the ability to completely absorb my mind, so it’s a good quick escape.  Of course, there are also the little feel-good voices reminding me that it’s better than watching TV and that by playing I sharpen my mind.</p>



<p>While 1000 can of course be a ridiculously low bar by some standard, I found reaching it more difficult than I expected, and I like to think that the 5|5 format attracts stronger players, so that the competition is tougher, even though it may not be true.   (But it does seem true that a certain score in a certain format does not correspond to the same score in a different format.)  For one thing, I had to familiarize myself with several basic openings.  I bought a little cute book <em>Chess openings for kids</em> which is good for people like me whose knowledge of chess openings was “e4 e5.”  I don’t do anything fancy, but it was fun to read about common openings.  I think I also wouldn’t mind playing random chess, but it seems harder to find opponents.</p>



<p>So why don’t you try and see what is your 5|5 score?  And if you want to play sometimes, drop me a line.</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/"><span class="datestr">at July 28, 2021 06:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5690">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/">A Basic Question About Multiplicative Energy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As part of some research project, I got to a basic question about multiplicative energy. Embarrassingly , I wasn’t able to get any non-trivial bound for it. Here is the problem. Any information about it would be highly appreciated. Problem. Let . Let be a set of real numbers. How large can the multiplicative energy […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/"><span class="datestr">at July 23, 2021 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">Average-Case Fine-Grained Hardness, Part I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I recently finished my qualifying exam for which I surveyed worst-case to average-case reductions for problems in NP. As part of my survey, I reviewed recent results on average-case hardness in P (a.k.a. average-case fine-grained hardness). I would like to give an overview of some of these results in a series of blog posts, and I want to start by giving some motivations.</p>



<p><strong>Why do we care about average-case fine-grained hardness?</strong></p>



<p>(i) We hope to explain why we are struggling to find faster algorithms for problems in P such as DNA sequencing even for some random large datasets.</p>



<p>(ii) Average-case hard problems in P is useful to cryptography. For example, constructing <a href="https://en.wikipedia.org/wiki/One-way_function">one-way functions</a> (i.e., functions that are easy to compute but hard to invert) based on worst-case complexity assumptions such as NP <img src="https://s0.wp.com/latex.php?latex=%5Cnot%5Csubseteq&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\not\subseteq" class="latex" /> BPP has been a long-standing open question. In this context, “easy” means polynomial-time computable. If we consider “easy” to be computable in time like <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B100%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{100})" class="latex" /> instead, then a natural alternative question is whether we can construct such one-way functions based on worst-case fine-grained complexity assumptions. Another example is <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof of work</a> <a href="https://link.springer.com/chapter/10.1007/3-540-48071-4_10">[DN92]</a>. When a miner tries to mine the <a href="https://en.wikipedia.org/wiki/Cryptocurrency">cryptocurrency</a>, the miner is asked to solve a random puzzle, that is average-case hard but still tractable, and then the miner needs to prove they have indeed solved the puzzle through efficient protocols. An interesting and more recent follow-up is proof of useful work <a href="https://eprint.iacr.org/2017/203.pdf">[BRSV17b]</a>, which proposes that instead of wasting computing power on a random puzzle that comes from nowhere, we can first reduce a computational task of practical interest to multiple random puzzles and then ask the miner to solve those puzzles.</p>



<p>The most common approach for proving average-case fine-grained hardness is arguably worst-case to average-case reduction, i.e., reducing an arbitrary instance to a number of random instances of which the distribution is polynomial-time sampable. Before I give concrete examples, I want to describe a general recipe for designing such worst-case to average-case reductions. (Some reader might notice that the step 3 of the recipe is same as the argument for proving <a href="https://en.wikipedia.org/wiki/Random_self-reducibility#Permanent_of_a_matrix">computing permanent is self-reducible</a> <a href="https://www.semanticscholar.org/paper/New-Directions-In-Testing-Lipton/fb2eba4d69bdab34c2d240380d4370be2feeacb9">[L91]</a>, which essentially uses <a href="https://en.wikipedia.org/wiki/Locally_decodable_code#The_Reed%E2%80%93Muller_code">local decoding for Reed-Muller codes</a>.)</p>



<p><strong>General recipe for worst-case to average-case reductions:</strong></p>



<ol><li>Choose our favorite hard problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}" class="latex" /> in P.</li><li>Construct a low-degree (degree-<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />) polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^n" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li><li>To solve <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />: sample a uniformly random <img src="https://s0.wp.com/latex.php?latex=y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y\in\mathbf{F}_{p}^n" class="latex" />, compute <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%2Bt_1+y%29%2C%5Cdots%2Cf_%7BL%7D%28x%2Bt_%7Bd%2B1%7D+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x+t_1 y),\dots,f_{L}(x+t_{d+1} y)" class="latex" /> for distinct nonzero <img src="https://s0.wp.com/latex.php?latex=t_1%2C%5Cdots%2Ct_%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t_1,\dots,t_{d+1}" class="latex" /> using average-case solver (note each <img src="https://s0.wp.com/latex.php?latex=x%2Bt_%7Bi%7D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x+t_{i} y" class="latex" /> is uniformly random), interpolate univariate polynomial <img src="https://s0.wp.com/latex.php?latex=g%28t%29%3A%3Df_%7BL%7D%28x%2Bt+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(t):=f_{L}(x+t y)" class="latex" /> using these points, and output <img src="https://s0.wp.com/latex.php?latex=g%280%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(0)" class="latex" /> which is <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" />. (This step can be replaced by decoding algorithms for <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> to handle larger errors of average-case solver.)</li><li>(The above steps already show that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> uniformly random inputs is as hard as solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case input.) If we want to show <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> itself is average-case fine-grained hard, it suffices to give a reduction from computing <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\mathbf{F}_{p}^n" class="latex" /> back to solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />.</li></ol>



<p>Notice that the above general recipe reduces a worst-case instance to <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> random instances at the step 3, and thus, we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be small (like <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />) such that it does not blow up the total runtime significantly. Typically, the step 4 would also blow up the runtime, and sometimes it depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />. For all the problems (or the techniques) in this series, I will explicitly quantify the runtime blow-up in the step 4 (if there is any) and explain how small we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be (if the blow-up depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />).</p>



<p>Now let us go through a concrete example. Consider one of the flagship problems in fine-grained complexity — orthogonal vector problem (OV): Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%27%5Cin%5Comega%28%5Clog+n%29%5Ccap+n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'\in\omega(\log n)\cap n^{o(1)}" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. It is known OV has no <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-\varepsilon})" class="latex" />-time algorithm for any constant <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon&gt;0" class="latex" /> assuming strong <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential-time hypothesis</a> (SETH) <a href="https://link.springer.com/chapter/10.1007/978-3-540-27836-8_101">[W05]</a>, and there is a generalization of OV called <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV that has no <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7Bk-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{k-\varepsilon})" class="latex" />-time algorithm under the same assumption <a href="https://dl.acm.org/doi/10.1145/3300150.3300158">[W18]</a>.</p>



<p><strong>Polynomial evaluation.</strong> Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=d%27%3Dn%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'=n^{o(1)}" class="latex" />, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=2d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2d'" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D%28X%2CY%29%3A%3D%5Csum_%7Bi%2Cj%5Cin%5Bn%5D%7D%5Cprod_%7Bt%5Cin%5Bd%27%5D%7D%281-x_%7Bi%2Ct%7Dy_%7Bi%2Ct%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}(X,Y):=\sum_{i,j\in[n]}\prod_{t\in[d']}(1-x_{i,t}y_{i,t})" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7B2nd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{2nd'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2Ct%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_{i,t}" class="latex" /> denotes the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-th coordinate of <img src="https://s0.wp.com/latex.php?latex=x_i%5Cin+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i\in X" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> simply enumerates all the pairs of vectors and counts the number of orthogonal pairs for the OV instance, and obviously counting is at least as hard as decision. Using the aforementioned general recipe, it follows immediately that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> requires <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-o(1)})" class="latex" /> time for average case assuming randomized version of SETH. This result was shown in <a href="https://eprint.iacr.org/2017/202.pdf">[BRSV17a]</a>, and analogously, they constructed a polynomial for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV, which implies an average-case time hierarchy assuming randomized SETH.</p>



<p>However, the polynomial evaluation problem is algebraic. Next, let us consider a natural combinatorial problem — counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertices graph (for simplicity, think of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> as a large constant). This problem has worst-case complexity <img src="https://s0.wp.com/latex.php?latex=n%5E%7B%5CTheta%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{\Theta(t)}" class="latex" /> assuming ETH <a href="https://core.ac.uk/download/pdf/82508832.pdf">[CHKX06]</a>.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques.</strong> It was first shown in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{4}" class="latex" /> in some polynomial-time sampable random graph. The proof also follows our general recipe. First, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%3A%3D%5Csum_%7B%5Ctextrm%7Bsize-%7Dt%5C%2C+T%5Csubseteq+%5Bn%5D%7D%5Cprod_%7Bi%3Cj%5Cin+T%7D+X_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X):=\sum_{\textrm{size-}t\, T\subseteq [n]}\prod_{i&lt;j\in T} X_{i,j}" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{n^2}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> is the adjacency matrix. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by enumerating each size-<img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> subset of vertices and checking whether it is a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. It remains to work out the step 4 of the general recipe. This was done by a gadget reduction, that runs in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, from evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" />-vertices graph <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a>.</p>



<p>Although this gadget reduction is nice, I will not explain it here, because later works <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a> show that if the polynomial constructed at the step 2 has certain structure, then there is a general technique to reduce evaluating this polynomial back to the original problem (at the cost of requiring smaller error probability of average-case solver), which I will discuss in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>. Finally, let me point out an issue in the previous paragraph — <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is too large for the gadget reduction to be useful! We need <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" /> (note <img src="https://s0.wp.com/latex.php?latex=n%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^t" class="latex" /> is a trivial upper bound of the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques) such that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> indeed outputs the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, but the gadget reduction takes <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, and moreover, we do not know how to find a prime <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" /> time. This issue was handled in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> using <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a>. Specifically, we choose <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> many distinct primes <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" />‘s upper bounded by <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> whose product is <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> (the existence of such primes follows from <a href="https://en.wikipedia.org/wiki/Prime_number_theorem">asymptotic distribution of the prime numbers</a>). Then, we apply the whole reduction described so far to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%5Cmod+p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)\mod p_i" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" /> and then use Chinese remainder theorem to recover <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)" class="latex" />. (We can use Chinese remaindering with errors <a href="https://dl.acm.org/doi/10.1109/18.850672">[GRS99]</a> to handle larger error probability.)</p>



<p>Hopefully, the above examples give you a flavor of worst-case to average-case reductions for fine-grained hard problems. As promised, in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>, I will continue to discuss the new technique in  <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a>, which automatizes the step 4 of the general recipe by requiring more structural properties for the polynomial constructed in the step 2 of the general recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p>



<p><br /></p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/"><span class="datestr">at July 23, 2021 01:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6127">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/laplace-method/">Approximating integrals with Laplace’s method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Integrals appear everywhere in all scientific fields, and their numerical computation is an active area of research. In the playbook of approximation techniques, my personal favorite is “la méthode de Laplace”, a must-know for students that like to cut integrals into pieces, that comes with lots of applications.</p>



<p class="justify-text">We will be concerned with integrals of the form $$I(t) =  \int_K h(x) e^{ \, – \, t f(x) } dx, $$ where \(K\) is compact (bounded and closed) subset of \(\mathbb{R}^d\), and \(h\) and \(f\) are two real-valued functions defined on \(K\) such that the integral is well defined for large enough \(t \in \mathbb{R}\). The goal is to obtain an asymptotic equivalent when \(t\) tends to infinity.</p>



<p class="justify-text">Within machine learning, as explained below, this is useful where computing and approximating integrals is important. This thus comes up naturally in Bayesian inference where \(t\) will be the number of observations in a statistical model. But let’s start with presenting this neat asymptotic result that Laplace discovered for a particular one-dimensional case in 1774 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">1</a>].</p>



<h2>Laplace approximation</h2>



<p>Let’s first state the main result. Assume that:</p>



<ul class="justify-text"><li>\(h\) is continuous on \(K\) and that \(f\) is twice continuously differentiable on \(K\).</li><li>\(f\) has a strict global minimizer \(x_\ast\) on \(K\), which is in the interior of \(K\), where the gradient \(f^\prime(x_\ast)\) is thus equal to zero, and where the Hessian \(f^{\prime \prime}(x_\ast)\) is a positive definite matrix (it is always positive semi-definite because \(x_\ast\) is a local minimizer of \(f\)); moreover, \(h(x_\ast) \neq 0\).</li></ul>



<p class="justify-text">Then, as \(t\) tends to infinity, we have the following asymptotic equivalent: $$ I(t) \sim \frac{h(x_\ast)}{\sqrt{ \det f^{\prime \prime}(x_\ast) }} \Big( \frac{2\pi}{t}\Big)^{d/2}  e^{ \, – \, t f(x_\ast) }.$$</p>



<p class="justify-text"><strong>Where does it come from?</strong> The idea is quite simple: for \(t&gt;0\), the exponential term \(e^{ \, – \, t f(x) }\) is largest when \(x\) is equal to the minimizer \(x_\ast\). Hence only contributions close to \(x_\ast\) will count in the integral. Then we can do Taylor expansions of the two functions around \(x_\ast\), as \(h(x) \approx h(x_\ast)\) and \(f(x) \approx f(x_\ast) + \frac{1}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\), and approximate \(I(t)\) as $$ I(t) \approx \int_K h(x_\ast) \exp\Big[ – t f(x_\ast) \ – \frac{t}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\Big] dx.$$ We can then make a change of variable \(y = \sqrt{t} f^{\prime \prime}(x_\ast)^{1/2}( x – x_\ast)\) (where \(f^{\prime \prime}(x_\ast)^{1/2}\) is the positive square root of \(f^{\prime \prime}(x_\ast)\)), to get, with the Jacobian of the transformation leading to the term \(  (\det f^{\prime \prime}(x_\ast) )^{1/2} t^{d/2} \): $$I(t) \approx \frac{ h(x_\ast) e^{ \, – \, t f(x_\ast) }}{( \det f^{\prime \prime}(x_\ast) )^{1/2}t^{d/2} } \int_{\sqrt{t}f^{\prime \prime}(x_\ast)^{1/2}( K \ – x_\ast)} \!\!\! \exp\big[ – \frac{1}{2} y^\top  y  \big] dy.$$ Since \(x_\ast\) is in the interior of \(K\), when \(t\) tends to infinity, the set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\) tends to \(\mathbb{R}^d\) (see illustration below), and we obtain the usual Gaussian integral that leads to the normalizing constant of the Gaussian distribution, which is equal to \((2\pi)^{d/2} \).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="544" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/Klaplace-1024x288.png" class="wp-image-6263" height="153" />Left: set \(K\) with \(x_\ast\) in its interior. Right: set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\), which is a translated (by \(x_\ast\)), tilted (by \(f^{\prime\prime}(x_\ast)\)) and scaled (by \(\sqrt{t}\)) version of \(K\).</figure></div>



<p class="justify-text"><strong>Formal proof.</strong> Let’s first make our life simpler: without loss of generality, we may assume that \(f(x_\ast) = 0\) (by subtracting the minimal value), that \(f^{\prime \prime}(x_\ast) = I\) (by a change of variable whose Jacobian is the square root of \(f^{\prime \prime}(x_\ast)\), leading to the determinant term), and that \(x_\ast = 0\). With the dominated convergence theorem (which was unfortunately unknown to me when I first learned about the method in high school, forcing students to cut integrals into multiple pieces), the proof sketch above is almost exact. We simply need a simple argument, based on the existence of a continuous function \(g: K \to \mathbb{R}\) such that $$ g(x) = \frac{f(x) } {\|  x  \|^2}$$ for \(x \neq 0\) and \(g(0) = \frac{1}{2}\) (here \(\| \! \cdot \! \|\) denotes the standard Euclidean norm). The function \(g\) is trivially defined and continuous on \(K \backslash \{0\}\), the value and continuity at zero is a simple consequence of the Taylor expansion $$ f(x) = f(0) + f^\prime(0)^\top x + \frac{1}{2} x^\top f^{\prime\prime}(0)x + o( \| x\|^2) = \frac{1}{2} \| x\|^2 + o( \| x\|^2). $$</p>



<p class="justify-text">We may then use the same change of variable as above to get the <em>equality</em>: $$I(t) =  \frac{ 1 }{ t^{d/2}  }  \int_{\sqrt{t}  K } h\big( \frac{1}{\sqrt{t}}  y\big) \exp\big[ – \frac{1}{2} y^\top y \cdot g\big(  \frac{1}{\sqrt{t}} y\big) \big] dy.$$ We can write the integral part of the expression above as $$J(t) = \int_{\mathbb{R}^d} a(y,t) dy, $$ with $$a(y,t) = 1_{ y \in \sqrt{t} K } h\big(  \frac{1}{\sqrt{t}}  y\big)\exp\big[ – \frac{1}{2} y^\top y \cdot g\big( \frac{1}{\sqrt{t}} y\big) \big].$$ We have for all \(t&gt;0\) and \(y \in \mathbb{R}^d\), \(|a(y,t)| \leqslant \max_{z \in K} | h(z)| \exp\Big[ – \| y\|^2 \cdot \min_{ z \in K } g(z) \Big]\), which is integrable because \(h\) is continuous on the compact set \(K\) and thus bounded, and \(g\) is strictly positive on \(K\) (since \(f\) is strictly positive except at zero as \(0\) is a strict global minimum), and by continuity, its minimal value is strictly positive. Thus by the <a href="https://en.wikipedia.org/wiki/Dominated_convergence_theorem">dominated convergence theorem</a>: $$\lim_{t \to +\infty} J(t) = \int_{\mathbb{R}^d} \big( \lim_{t \to +\infty} a(y,t) \big) dy = \int_{\mathbb{R}^d}\exp\big[ – \frac{1}{2} y^\top y \big] dy = ( 2\pi)^{d/2}.$$ This leads to the desired result since \(I(t) = J(t) / t^{d/2}\).</p>



<h2>Classical examples</h2>



<p>Two cute examples are often mentioned as applications (adapted from [2]).</p>



<p><strong><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s formula</a></strong>. We have, by definition of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a> \(\Gamma\), and the change of variable \(u = tx\):<br />$$\Gamma(1+t) = \int_0^\infty \!\! e^{-u} u^{t} du = \int_0^\infty \!\! e^{-tx}t^t x^t t dx = t^{t+1} \int_0^\infty \!\! e^{-t(x-\log x)} dx.$$ Since \(x \mapsto x\, – \log x\) is minimized at \(x=1\) with second derivative equal to \(1\), we get: $$\Gamma(1+t) \sim t^{t+1} e^{-t} \sqrt{2\pi / t} = \big( \frac{t}{e} \big)^t \sqrt{ 2\pi t}.,$$ which is exactly Stirling’s formula, often used when \(t\) is an integer, and then, \(\Gamma(1+t) = t!\).</p>



<p class="justify-text"><strong>Convergence of \(L_p\)-norms to the \(L_\infty\)-norm.</strong> We consider the \(L_p\)-norm of a positive twice continuously differentiable function on the compact set \(K\), with a unique global maximizer at \(x_\ast\) in the interior of \(K\). Then we can write its \(L_p\)-norm \(\|g\|_p\) through $$\| g\|_p^p = \int_K g(x)^p dx = \int_K e^{ p \log g(x)} dx.$$ The function \(f: x \mapsto\  – \log g(x)\) has gradient \(f^\prime(x) = \ – \frac{1}{g(x)}g^\prime(x)\) and Hessian \(f^{\prime\prime}(x)=\  – \frac{1}{g(x)} g^{\prime\prime}(x) + \frac{1}{g(x)^2} g^\prime(x) g^\prime(x)^\top .\) At \(x_\ast\), we get \(f^{\prime\prime}(x_\ast) = \ – \frac{1}{g(x_\ast)} g^{\prime \prime}(x_\ast)\). Thus, using Laplace’s method, we have: $$ \|g\|_p^p =  \frac{A}{p^{d/2}} g(x_\ast)^p (1 + o(1) ),$$ with \(\displaystyle A = \frac{(2\pi g(x_\ast))^{d/2}}{(\det (-g^{\prime\prime}(x_\ast)))^{1/2}}\).</p>



<p class="justify-text">Taking the power \(1/p\), we get: $$ \|g\|_p = \exp \big( \frac{1}{p} \log \|g\|_p^p\big) =  \exp \Big( \frac{1}{p} \log A \ – \frac{d}{2p} \log p +   \log g(x_\ast) + o(1/p) \Big).$$ This leads to, using \(\exp(u) = 1+u + o(u)\) around zero: $$ \| g\|_p = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + \frac{1}{p} \log A + o(1/p) \Big) = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + O(1/p) \Big).$$ A surprising fact is that the second-order term does not depend on anything but \(g(x_\ast)\) (beyond \(p\) and \(d\)). Note that this applies also to continuous log-sum-exp functions.</p>



<h2>Applications in Bayesian inference</h2>



<p class="justify-text">It turns out that Laplace’s motivation in deriving this general technique for approximating integrals was exactly Bayesian inference, which he in fact essentially re-discovered and extended (see an interesting account <a href="https://ebrary.net/118879/history/laplaces_bayesian_analysis_1774_1781">here</a>). Let me now explain how Laplace’s method applies.</p>



<p class="justify-text">We consider a parameterized family of probability distributions with density \(p(x|\theta)\) with respect to some base measure \(\mu\) on \(x \in \mathcal{X}\), with \(\theta \in \Theta \subset \mathbb{R}^d\) a set of parameters. </p>



<p class="justify-text">As a running example (the one from Laplace in 1774), we consider the classical Bernoulli distribution for \(x \in \mathcal{X} = \{0,1\}\), and densities with respect to the counting measure, that is, the parameter \(\theta \in [0,1]\) is the probability that \(x=1\).</p>



<p class="justify-text"><strong>From frequentist to Bayesian inference. </strong>We are given \(n\) independent and identically distributed observations \(x_1,\dots,x_n \in \mathcal{X}\), from an unknown probability distribution \(q(x)\). One classical goal of statistical inference is to find the parameter \(\theta \in \Theta\) so that \(p(\cdot| \theta)\) is as close to \(q\) as possible.</p>



<p class="justify-text">In the frequentist framework, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> amounts to maximizing $$ \sum_{i=1}^n \log p(x_i | \theta)$$ with respect to \(\theta \in \Theta\). It comes with a series of guarantees, in particular (but not only) when \(q = p(\cdot | \theta)\) for a certain \(\theta \in \Theta\). For our running example, the maximum likelihood estimator \(\hat{\theta}_{\rm ML} = \frac{1}{n} \sum_{i=1}^n x_i\) is the frequency of non-zero outcomes.</p>



<p class="justify-text">In the Bayesian framework, the data are assumed to be generated by a certain \(\theta\), but now with \(\theta\) itself random with some prior probability density \(p(\theta)\) (with respect to the Lebesgue measure). Statistical inference typically does not lead to a point estimator (like the maximum likelihood estimator), but to the full posterior distribution of the parameter \(\theta\) given the observations \(x_1,\dots,x_n\).</p>



<p class="justify-text">The celebrated <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a> states that the posterior density \(p(\theta | x_1,\dots,x_n)\) can be written as: $$p(\theta | x_1,\dots,x_n) = \frac{ p(\theta) \prod_{i=1}^n p(x_i|\theta) }{p(x_1,\dots,x_n)},$$ where \(p(x_1,\dots,x_n)\) is the marginal density of the data (once the parameter has been marginalized out).</p>



<p class="justify-text">If pressured, a Bayesian will end up giving you a point estimate, but a true Bayesian will not give away the maximum a posteriori estimate (see <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Limitations">here</a> for some reasons why), but if you give him or her a loss function (e.g., the square loss), he or she will give away (reluctantly) the estimate that minimizes the posterior risk (e.g., the posterior mean for the square loss).</p>



<p class="justify-text"><strong>Bernoulli and Beta distributions. </strong>In our running Bernoulli example for coin tossing, it is standard to put a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> on \(\theta \in [0,1]\), which is here a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> with parameters \(\alpha\) and \(\beta\), that is, $$p(\theta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1} ( 1- \theta)^{\beta-1}.$$ The posterior distribution is then also a Beta distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\beta + \sum_{i=1}^n ( 1- x_i)\). The posterior mean is $$\mathbb{E} [ \theta | x_1,\dots,x_n ] = \frac{\alpha+ \sum_{i=1}^n x_i}{n + \alpha + \beta},$$ and corresponds to having pre-observed \(\alpha\) ones and \(\beta\) zeroes (this is sometimes referred to as Laplace smoothing). However, it is rarely possible to compute the posterior distribution in closed form, hence the need for approximations.</p>



<p class="justify-text"><strong>Laplace approximation. </strong>Thus, in Bayesian inference, integrals of the form $$ \int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta$$ for some function \(h: \Theta \to \mathbb{R}\), are needed. For example, computing the marginal likelihood corresponds to \(h=1\).</p>



<p class="justify-text">By taking logarithms, we can write $$\int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta = \int_{\Theta} h(\theta) \exp\Big(  \log p(\theta) + \sum_{i=1}^n \log p(x_i|\theta) \Big) d\theta, $$ and with \(f_n(\theta) = \ – \frac{1}{n} \log p(\theta) \ – \frac{1}{n} \sum_{i=1}^n \log p(x_i|\theta),\) we have an integral in Laplace form, that is, $$\int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta,$$ with a function \(f_n\) that now varies with \(n\). This simple variation does not matter as because of the law of large numbers, when \(n\) is large, \(f_n(\theta)\) tends to a fixed function \(\mathbb{E} \big[ \log p(x|\theta) \big]\). </p>



<p class="justify-text">The Laplace approximation thus requires to compute the minimizer of \(f_n(\theta)\), which is exactly the maximum a posteriori estimate \(\hat{\theta}_{\rm MAP}\), and use the approximation: $$ \int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta \approx (2\pi / n)^{d/2} \frac{h( \hat{\theta}_{\rm MAP})}{(\det f_n^{\prime \prime}( \hat{\theta}_{\rm MAP}))^{1/2}} \exp( – n f_n( \hat{\theta}_{\rm MAP})).$$</p>



<p class="justify-text"><strong>Gaussian posterior approximation.</strong> Note that the Laplace approximation exactly corresponds to approximating the log-posterior density by a quadratic form and thus approximating the posterior by a Gaussian distribution with mean \(\hat{\theta}_{\rm MAP}\) and covariance matrix \(\frac{1}{n} f_n^{\prime \prime}( \hat{\theta}_{\rm MAP})^{-1}\). Note that Laplace’s method gives one natural example of such Gaussian approximation and that <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational inference</a> can be used to find better ones.</p>



<p class="justify-text"><strong>Example.</strong> We consider the Laplace approximation of a Beta random variable, that is a Gaussian with mean at the mode of the original density and variance equal to the inverse of the second derivative of the log-density. Below, the mean \(\alpha / (\alpha +\beta)\) is set to a constant, while the variance shrinks due to an increasing \(\alpha+\beta\) (which corresponds to the number of observations in the Bayesian interpretation above).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="598" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/plot_beta-3.gif" class="wp-image-6335" height="248" />Left: densities. Right: negative log-densities translated so that they have matched first two derivatives at their minimum.</figure></div>



<p class="justify-text">We see above, that for large variances (small \(\alpha +\beta\)), the Gaussian approximation is not tight, while it becomes tighter as the mass gets concentrated around the mode.</p>



<h2>Extensions</h2>



<p class="justify-text"><strong>High-order expansion.</strong> The approximation is based on Taylor expansions of the functions \(h\) (order \(0\)) and \(f\) (order \(2\)). In order to obtain extra terms of the form \(t^{d/2+\nu}\), for \(\nu\) a positive integer, we need higher-order derivatives of \(h\) and \(f\). In more than one dimension, that quickly gets complicated (see, e.g., [3, 4]).</p>



<p class="justify-text">One particular case which is interesting in one dimension is \(h(x) \sim A ( x- x_\ast)^\alpha\), and \(f(x)-f(x_\ast) = B|x-x_\ast|^{\beta}\). Note that \(\alpha=0\) and \(\beta=2\) corresponds to the regular case. A short calculation gives the equivalent \(I(t) \sim \frac{2}{2+\beta}  \frac{A \Gamma\big( \frac{\alpha+1}{\beta} \big)}{(tB)^{\frac{\alpha+1}{\beta}}}\).</p>



<p class="justify-text"><strong><a href="https://en.wikipedia.org/wiki/Stationary_phase_approximation">Stationary phase approximation</a>.</strong> We can also consider integrals of the form $$I(t) = \int_K h(x) \exp( – i t f(x) ) dx,$$ where \(i\) is the usual square root of \(-1\). Here, the main contribution also comes from vectors \(x\) where the gradient of \(f\) is zero. This can be further extended to more general <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">complex integrals</a>.</p>



<h2>When Napoléon meets Laplace and Lagrange</h2>



<p class="justify-text">As a conclusion, I cannot resist mentioning a <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#I_had_no_need_of_that_hypothesis">classical (potentially not totally authentic) anecdote</a> about encounters between Laplace and Lagrange, two mathematical heroes of mine, and Napoléon, as described in [<a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">5</a>, page 343]:</p>



<blockquote class="wp-block-quote justify-text"><p>Laplace went in state to beg Napoleon to accept a copy of his work, and the following account of the interview is well authenticated, and so characteristic of all the parties concerned that I quote it in full. Someone had told Napoleon that the book contained no mention of the name of God; Napoleon, who was fond of putting embarrassing questions, received it with the remark, “M. Laplace, they tell me you have written this large book on the system of the universe, and have never even mentioned its Creator.” Laplace, who, though the most supple of politicians, was as stiff as a martyr on every point of his philosophy, drew himself up and answered bluntly, “Je n’avais pas besoin de cette hypothèse-là.” Napoleon, greatly amused, told this reply to Lagrange, who exclaimed, “Ah! c’est une belle hypothèse; ça explique beaucoup de choses.”</p><cite>W. W. Rouse Ball, A Short Account of the History of Mathematics, 1888.</cite></blockquote>



<h2>References</h2>



<p class="justify-text">[1] Pierre-Simon Laplace. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">Mémoire sur la probabilité des causes par les événements</a>, Mémoires de l’Académie royale des sciences de Paris (Savants étrangers), t. VI. p. 621, 27-65, 1774.<br />[2] Norman Bleistein, Richard A. Handelsman. Asymptotic Expansions of Integrals. Dover<br />Publications, 1986.[3] Stephen M. Stigler. <a href="https://www.jstor.org/stable/pdf/2245475.pdf">Laplace’s 1774 memoir on inverse probability</a>. <em>Statistical Science</em>, 1(3):359-378, 1986.<br />[3] Luke Tierney, Robert E. Kass, Joseph B. Kadane. <a href="https://www.jstor.org/stable/pdf/2289652.pdf">Fully exponential Laplace approximations to expectations and variances of nonpositive functions</a>. <em>Journal of the American Statistical Association</em>, 84(407): 710-716, 1989.<br />[4] Zhenming Shun, Peter McCullagh. <a href="https://www.jstor.org/stable/pdf/2345941.pdf">Laplace approximation of high dimensional integrals</a>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 57(4): 749-760, 1995.<br />[5] W. W. Rouse Ball. <a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">A Short Account of the History of Mathematics</a>, 1888.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/laplace-method/"><span class="datestr">at July 23, 2021 06:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blog.simons.berkeley.edu/?p=304">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/simons.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/">Trends in Machine Learning Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series is disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/" target="_blank" rel="noreferrer noopener">Learning Theory Alliance</a> and is overseen by <a href="http://www.gautamkamath.com/" target="_blank" rel="noreferrer noopener">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" target="_blank" rel="noreferrer noopener">Learning Theory Alliance blog</a>.</p>



<p>This is the sixth and final post in the series, on trends in machine learning theory, written by <a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz" target="_blank" rel="noreferrer noopener">Michal Moshkovitz</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/" target="_blank" rel="noreferrer noopener">Cyrus Rashtchian</a>.</p>



<p><strong>Introduction</strong><br />Throughout the last few decades, we have witnessed unprecedented growth of machine learning. Originally a topic formalized by a small group of computer scientists, machine learning now impacts many areas: the physical sciences, medicine, commerce, finance, urban planning, and more. The rapid growth of machine learning can be partially attributed to the availability of large amounts of data and the development of powerful computing devices. Another important factor is that machine learning has foundations in many other fields, such as theoretical computer science, algorithms, applied mathematics, statistics, and optimization. </p>



<p>If machine learning is already mathematically rooted in many existing research areas, why do we need a field solely dedicated to learning theory? According to <a href="https://www.cs.columbia.edu/~djhsu/" target="_blank" rel="noreferrer noopener">Daniel Hsu</a>, “Learning theory serves (at least) two purposes: to help make sense of machine learning, and also to explore the capabilities and limitations of learning algorithms.” Besides finding innovative applications for existing tools, learning theorists also provide answers to long-standing problems and ask new fundamental questions. </p>



<p>Modern learning theory goes beyond classical statistical and computer science paradigms by: </p>



<ul><li>developing insights about specific computational models (e.g., neural networks) </li><li>analyzing popular learning algorithms (e.g., stochastic gradient descent)</li><li>taking into account data distributions (e.g., margin bounds or manifold assumptions)</li><li>adding auxiliary goals (e.g., robustness or privacy), and </li><li>rethinking how algorithms interact with and access data (e.g., online or reinforcement learning).</li></ul>



<p>By digging deep into the basic questions, researchers generate new concepts and models that change the way we solve problems and help us understand emerging phenomena.</p>



<p>This article provides a brief overview of three key areas in machine learning theory: new learning paradigms, trustworthy machine learning, and reinforcement learning. We describe the main thrust of each of these areas, as well as point to a few papers from <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT 2021</a> (the 32nd International Conference on Algorithmic Learning Theory) that touch each of these topics. To share a broader view, we also asked experts in the areas to comment on the field and on their recent papers. Needless to say, this article only scratches the surface. At the end, we point to places to learn more about learning theory.</p>



<span id="more-304"></span>



<p><strong>New Machine Learning Paradigms</strong><br />The traditional learning theory framework, probably approximately correct (PAC) learning, defines what it means to learn a ground-truth classifier from a candidate class of possible classifiers. Alongside PAC learning is Vapnik-Chervonenkis (VC) theory, which characterizes the number of samples needed and sufficient to learn a classifier from a given class. The generalization analysis from VC theory is restricted to guarantees that hold independently of the data distribution — that is, even for worst-case distributions. Additionally, the VC/PAC learning paradigm suggests that whenever learning is possible, it can be accomplished by choosing the classifier that minimizes loss on the training data, called the empirical risk minimizer (ERM). </p>



<p>This classical framework unfortunately fails to explain the empirical success of machine learning (ML). “The distribution-free setting, while it comes with the elegant VC theory, turned out to be unsatisfactory,” says Csaba Szepesvári. “Due to the oversimplified setting, the theory could not contribute meaningfully to understanding all kinds of learning methods such as learning with trees, boosting, neural networks, SVMs, or using any other nonparametric methods.” Researchers posit that stronger guarantees should be possible if we leverage natural assumptions about the data distribution, though identifying the right “natural assumptions” is a challenging task. Similarly, understanding which of many possible ERM solutions a learning algorithm chooses may yield better generalization results than those yielded by VC theory. </p>



<p>Methods that provide <em>distribution-specific </em>guarantees aren’t new to learning theory. A canonical example is known as a margin bound, where the test error of a classifier is analyzed in terms of the margin that separates the different prediction categories. In <a href="http://proceedings.mlr.press/v132/hanneke21a.html" target="_blank" rel="noreferrer noopener">one</a> of the ALT best papers, Steve Hanneke and Aryeh Kontorovich prove generalization guarantees in terms of the size of the margin for two popular classification algorithms: <a href="https://en.wikipedia.org/wiki/Support-vector_machine" target="_blank" rel="noreferrer noopener">support vector machines</a> (SVMs) and the <a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="noreferrer noopener">perceptron</a> algorithm. The authors answer a core open question, showing that SVMs achieve the optimal margin bound!</p>



<p>Further work at ALT uses an assumption that data lies on a low-dimensional manifold to prove guarantees for generative models. Generative models synthesize <em>original</em> samples, such as images or text, that resemble training data, but without copying the data directly. While so-called <em>generative adversarial networks</em> work well in practice, few guarantees exist because it is challenging to statistically formulate the requirement of generating original samples. Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak S. Dalalyan consider a new framework in their <a href="http://proceedings.mlr.press/v132/schreuder21a.html" target="_blank" rel="noreferrer noopener">paper</a>, in which they guarantee originality by outputting a continuous distribution, which ensures that it is very unlikely to output training examples. If the training data is generated from a low-dimensional manifold, they show that it is possible to learn a good generator, which outputs a smooth transformation of a random point.</p>



<p>In another <a href="http://proceedings.mlr.press/v132/tosh21a.html" target="_blank" rel="noreferrer noopener">paper</a>, Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu use distributional assumptions to show when unsupervised methods can use unlabeled data to learn useful <em>representations </em>of data. A linear function trained on these representations and some labeled data can then be used for downstream prediction of the labels. The key idea is to look at when data has <em>multiview redundancy (MVR), </em>which arises, for instance, when data is augmented: Under MVR, each data point can be viewed as a pair (<em>X,</em> <em>Z</em>), and the label <em>Y</em> can be predicted almost as well from <em>X</em> or <em>Z</em> as from the full pair. For instance, each pair might be two halves of an article, or two rotations of an image. The authors show how a theoretical approach called <em>landmark embedding </em>can produce a representation that enables low-error linear classification. Additionally, they analyze when the representations are learned implicitly while training a model to predict whether two views <em>X</em> and <em>Z</em> correspond to the same example, which is close to what is done in practice. </p>



<p>Another new paradigm considers how the specific<em> training algorithm</em> affects which of many candidate ERM solutions are chosen. This is called the <em>implicit bias </em>of a training algorithm: If there are multiple equally good solutions, then why does an algorithm choose one over the other? This is particularly relevant when studying neural networks, which are typically<em> overparameterized</em> and can be trained to find many solutions with zero empirical risk. In one <a href="http://proceedings.mlr.press/v132/ji21a/ji21a.pdf" target="_blank" rel="noreferrer noopener">paper</a>, Ziwei Ji and Matus Telgarsky characterize the implicit bias of using gradient descent to train a linear classifier with a general loss function. They show that the solution relates to the optimizer of a particular smoothed margin function. While this paper does not yield generalization guarantees, this type of implicit bias analysis can sometimes lead to generalization guarantees via margin bounds.</p>



<p>The goal of this area is to go beyond traditional learning theory paradigms by leveraging distributional and algorithmic properties. But a major open question is designing a more general mathematical theory that exploits such properties. <a href="http://www.cs.technion.ac.il/~shaymrn/" target="_blank" rel="noreferrer noopener">Shay Moran</a> offers a standard for new theory: “I hope that in the next 10 years we will develop more realistic models of learning, but I will insist that they still be mathematically clean.”</p>



<p><strong>Trustworthy ML</strong><br />Machine learning has inspired many new areas and technologies: personalized health care, drug discovery, advertising, résumé screening, credit loans, and more. However, these critical and user-centered applications require a higher standard of testing and verification because mistakes may deeply affect many people. Addressing these challenges has inspired a new field of research centered on making machine learning more trustworthy and reliable, which is the motivation for many of the ALT papers this year as well. An expert in the area, <a href="http://cseweb.ucsd.edu/~kamalika/" target="_blank" rel="noreferrer noopener">Kamalika Chaudhuri</a>, says, “For my field, which is trustworthy ML, the theoretical goal and challenge remains modeling and frameworks.” She elaborates: “Coming up with new conceptual frameworks for learning has always been one of the core challenges in learning theory since its early days, and it is doubly important now.” Researchers have been exploring this direction in many areas, including privacy, data deletion, robustness, fairness, interpretability, and causality. </p>



<p>Several ALT 2021 papers cover questions in privacy. The general goal is to understand how to modify existing learning methods to take into account privacy constraints. One of the <a href="http://proceedings.mlr.press/v132/wang21a.html" target="_blank" rel="noreferrer noopener">papers</a>, by Di Wang, Huanyu Zhang, Marco Gaboardi, and Jinhui Xu, considers generalized linear models in a differential privacy model. A central motivation is to understand the role of public, unlabeled data in improving the learnability of these problems in a private setting. Summarizing another direction, <a href="http://www.gautamkamath.com" target="_blank" rel="noreferrer noopener">Gautam Kamath</a> comments on his <a href="http://proceedings.mlr.press/v132/aden-ali21a.html" target="_blank" rel="noreferrer noopener">paper</a> with co-authors Ishaq Aden-Ali and Hassan Ashtiani, “This paper focuses on a very simple but surprisingly challenging question: Can we learn a general multivariate Gaussian under the constraint of differential privacy? Prior works focused on restricted settings — for example, with bounded parameters or known covariance. We gave the first finite sample complexity bound for this problem, which evidence suggests is near optimal. The next question is to design a computationally efficient algorithm for this problem.” Another <a href="http://proceedings.mlr.press/v132/cesar21a.html" target="_blank" rel="noreferrer noopener">paper</a> on privacy, by Mark Cesar and Ryan Rogers, studies the <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf" target="_blank" rel="noreferrer noopener">composition</a> of various privacy mechanisms in the context of real-world data analytics pipelines.</p>



<p>Another aspect of respecting user privacy is allowing people to choose to stop sharing their data. Concretely, this means removing their data from data sets and ensuring that existing and future models do not make use of their data in any way. One name for this process is <em>machine unlearning</em>, and the main challenge is removing the data efficiently without retraining all models from scratch. One <a href="http://proceedings.mlr.press/v132/neel21a.html" target="_blank" rel="noreferrer noopener">paper</a>, by Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi, addresses this challenge. They propose ways to strategically update the model by using modified gradient descent methods. They also analyze this approach, and prove new upper and lower bounds for updating models after data deletion with their new optimization algorithm. </p>



<p>Robust methods for machine learning and statistics aim to provide rigorous guarantees in the presence of outliers or adversarially modified data points. The field of robustness has been steadily growing as researchers uncover more and more models where deviations in the data can lead to unexpected and dramatic changes in model behavior. In ALT 2021, a <a href="http://proceedings.mlr.press/v132/shen21a.html" target="_blank" rel="noreferrer noopener">paper</a> by Jie Shen and Chicheng Zhang covers learning half-spaces nearly optimally even in the presence of malicious noise.</p>



<p>As a final and thought-provoking direction in trustworthy ML, <a href="https://omereingold.wordpress.com" target="_blank" rel="noreferrer noopener">Omer Reingold</a> points out that we need to better understand “the meaning of individual probabilities/risk scores,” which are common ways that ML systems summarize or justify decisions. Ideally, the output of a model should be something that people can interpret directly and use to potentially modify their future actions. He elaborates that it is important to think about “the individual quantities (which imply important decisions) that ML is trying to approximate” and to answer “what does fitting the parameters of a model on the entire population imply for individuals and subcommunities?” This question brings to the forefront the fact that ML systems affect both individuals and groups of people, which is an important consideration when formulating rigorous definitions of fairness (e.g., see <a href="https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207" target="_blank" rel="noreferrer noopener">this book</a> or <a href="https://fairmlbook.org/" target="_blank" rel="noreferrer noopener">this one</a>).</p>



<p><strong>Reinforcement Learning</strong><br />Reinforcement learning (RL) is a framework for interactive learning where an agent interacts with an environment, and the agent’s actions govern the rewards it receives from the environment. Part of the motivation for studying RL is that relevant problems are everywhere. Sometimes the agents are autonomous vehicles. Other times, they are programs playing games like chess or go. People interact more and more with ML models, and hence, living our lives is actually being a part of a multiagent game where we, humans, and the ML models are the agents. “The most exciting direction in learning theory of recent years,” says <a href="https://www.ehazan.com" target="_blank" rel="noreferrer noopener">Elad Hazan</a>, “is adding rigorous theoretical guarantees to reinforcement learning.” </p>



<p>The RL environment is typically modeled as a <em>Markov decision process (MDP)</em>: a set of states, actions, and transition probabilities that determine the next state and reward given the agent’s current state and action. The agent uses a <em>policy</em> to choose its action from each state with the goal of maximizing its cumulative reward over time. A central challenge is balancing<em> exploration</em> (learning about the environment) and <em>exploitation</em> (spending time choosing actions in states where they can collect high rewards).</p>



<p>In the most basic setting, <em>multi-armed bandits</em>, there is a single state, and each action (or “arm”) leads to a stochastic reward. “Here, the theory is quite mature, though interesting problems remain in connection to the limits of how structure can be exploited,” <a href="https://sites.ualberta.ca/~szepesva/" target="_blank" rel="noreferrer noopener">Csaba Szepesvári</a> says. In two works at ALT, by <a href="http://proceedings.mlr.press/v132/jourdan21a.html" target="_blank" rel="noreferrer noopener">Marc Jourdan, Mojmír Mutný, Johannes Kirschner, and Andreas Krause</a> and by <a href="http://proceedings.mlr.press/v132/cuvelier21a.html" target="_blank" rel="noreferrer noopener">Thibaut Cuvelier, Richard Combes, and Eric Gourdin</a>, the authors show that efficient exploration is possible in a <em>combinatorial semi-bandit </em>setting. Here, the agent can choose an allowed <em>set</em> of arms in each step, and it receives a distinct reward for each chosen arm. While the number of action choices for the agent is larger, this more detailed feedback makes the problem tractable.</p>



<p>Beyond the stateless bandit setting, ML theorists are still figuring out how fast agents can learn how to play optimally in MDPs with <em>finite</em> state spaces and action spaces. Recent progress on this front has given lower bounds on the sample complexity required for agents to learn the best policy. In one ALT <a href="http://proceedings.mlr.press/v132/domingues21a.html" target="_blank" rel="noreferrer noopener">paper</a>, the authors give a unified view of lower bounds for three distinct, but related, problems in RL. The hard MDP instances they construct to show lower bounds are based on hard instances for multi-armed bandit problems.</p>



<p>In the more challenging setting where the state space is infinite, a central question is whether the agent can learn from exploring a finite number of states, and generalize<em> </em>to perform well on unknown areas of the environment. For certain MDPs, generalizing is impossible, but some assumptions on the structure of the MDP may enable generalization. “While algorithm independent problem formulations existed and have been studied in the finite case, a quite recent development is to extend these to the case of ‘large’ environments where the use of <em>function approximation </em>techniques becomes crucial for achieving nontrivial results,” explains Csaba Szepesvári. </p>



<p>Function approximation has to do with the optimal <em>action-value function, </em>which captures the long-term reward of playing a certain action from a given state. This function can sometimes be approximated by some simple class of functions. One of the strongest such assumptions is <em>linear realizability, </em>where the optimal action-value function<em> </em>is a linear function of some representation of the action and state. In one of the <a href="http://proceedings.mlr.press/v132/weisz21a.html" target="_blank" rel="noreferrer noopener">papers</a> receiving a best paper award in ALT, Gellert Weisz, Philip Amortila, and Csaba Szepesvári show that even under this strong assumption of linear realizability, the agent needs a number of samples exponential in the length of the episode or the dimension of the representation in order to generalize. Looking forward, the goal is to follow the lead of these papers and better understand the landscape of sample complexity: When can we learn models with a polynomial number of samples, and when is an exponential number necessary?</p>



<p>Nearly every offline learning problem can be studied in an interactive setting, where inputs arrive in an online fashion and need to be processed immediately, which is common in many real-world settings. Models for interactive machine learning provide a framework for studying problems and algorithms in this more challenging setting. Beyond the MDP setting, interactive learning spans online learning (e.g., <a href="http://proceedings.mlr.press/v132/moshkovitz21a.html" target="_blank" rel="noreferrer noopener">no-substitution</a> <a href="http://proceedings.mlr.press/v132/bhattacharjee21a.html" target="_blank" rel="noreferrer noopener">clustering</a>), nonstochastic control theory (e.g., <a href="https://arxiv.org/abs/1911.12178" target="_blank" rel="noreferrer noopener">robust controllers for dynamical systems</a>), <a href="https://arxiv.org/abs/1909.05207" target="_blank" rel="noreferrer noopener">online convex optimization</a>, and many more domains. </p>



<p><strong>Conclusion</strong><br />We hope this provides a fairly broad view on some of the topics that people are researching right now in learning theory. Of course, there are many more areas that we don’t have space to describe: theory of deep neural networks, quantum algorithms for machine learning problems, human-centered considerations, learning with strategic agents and multiplayer games, convex/nonconvex optimization, federated and distributed learning algorithms, and many more. In general, as Gautam Kamath observes, “A lot of important questions in learning theory arise through interplay between the theoretical and applied machine learning communities.” To have a greater impact, it is important to collaborate with people doing empirical research, and to learn from the front lines about the most interesting phenomena to explain, or the challenges that do not seem surmountable by combining existing tools.</p>



<p>To learn more and to get more involved, we have listed a variety of resources (blogs, workshops, videos, etc.) that can help you get started in this area. As a final motivation for writing this article, we remark that people in the area are keenly aware that we need more young talent to help uncover truth and contribute groundbreaking ideas. As Gautam Kamath puts it, “There are far more interesting questions in learning theory than there are researchers to solve them.”</p>



<h2>Places to learn more</h2>



<p><strong>Blogs:</strong> <a href="https://ucsdml.github.io" target="_blank" rel="noreferrer noopener">UCSD ML blog</a>, <a href="http://www.offconvex.org" target="_blank" rel="noreferrer noopener"><em>Off the Convex Path</em></a>, <a href="https://windowsontheory.org" target="_blank" rel="noreferrer noopener"><em>Windows On Theory</em></a>, <a href="https://blogs.princeton.edu/imabandit/" target="_blank" rel="noreferrer noopener"><em>I’m a bandit</em></a>, <a href="https://francisbach.com" target="_blank" rel="noreferrer noopener">Francis Bach’s blog</a>, <a href="https://differentialprivacy.org" target="_blank" rel="noreferrer noopener">Differential Privacy blog</a>, <a href="https://distill.pub" target="_blank" rel="noreferrer noopener"><em>Distill</em></a>, <a href="http://thegradient.pub" target="_blank" rel="noreferrer noopener"><em>The Gradient</em></a></p>



<p><strong>Conferences:</strong> <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT</a>, <a href="http://learningtheory.org/colt2021/" target="_blank" rel="noreferrer noopener">COLT</a>, <a href="https://icml.cc" target="_blank" rel="noreferrer noopener">ICML</a>, <a href="https://nips.cc" target="_blank" rel="noreferrer noopener">NeurIPS</a>, <a href="https://aistats.org/aistats2021/" target="_blank" rel="noreferrer noopener">AISTATS</a>, <a href="https://www.auai.org/uai2021/" target="_blank" rel="noreferrer noopener">UAI</a>, <a href="https://responsiblecomputing.org" target="_blank" rel="noreferrer noopener">FORC</a>, <a href="http://acm-stoc.org/stoc2021/" target="_blank" rel="noreferrer noopener">STOC</a>, <a href="http://ieee-focs.org" target="_blank" rel="noreferrer noopener">FOCS</a>, <a href="http://itcs-conf.org" target="_blank" rel="noreferrer noopener">ITCS</a>, <a href="https://iclr.cc/virtual_2020/index.html" target="_blank" rel="noreferrer noopener">ICLR</a>, <a href="https://dl.acm.org/conference/soda" target="_blank" rel="noreferrer noopener">SODA</a></p>



<p><strong>Podcasts:</strong> <a href="https://twimlai.com" target="_blank" rel="noreferrer noopener"><em>TWIML</em></a>, <a href="https://wandb.ai/site/podcast" target="_blank" rel="noreferrer noopener"><em>Gradient Dissent</em></a>, <a href="https://www.quantamagazine.org/tag/the-joy-of-x/" target="_blank" rel="noreferrer noopener"><em>Joy of x</em></a>, <a href="https://shows.acast.com/the-robot-brains" target="_blank" rel="noreferrer noopener"><em>The Robot Brains</em></a>, <a href="https://www.thetalkingmachines.com/home" target="_blank" rel="noreferrer noopener"><em>Talking Machines</em></a>, <a href="https://www.talkrl.com/episodes/marc-bellemare" target="_blank" rel="noreferrer noopener"><em>TalkRL</em></a>, <a href="https://www.listennotes.com/podcasts/underrated-ml-sara-hooker-sean-hooker-nG4owP2C8s3/" target="_blank" rel="noreferrer noopener"><em>Underrated ML</em></a></p>



<p><strong>Videos:</strong> <a href="https://simons.berkeley.edu" target="_blank" rel="noreferrer noopener">Simons Institute</a>, <a href="https://www.youtube.com/watch?v=IXotICNx2qk&amp;list=PLdDZb3TwJPZ5dqqg_S-rgJqSFeH4DQqFQ" target="_blank" rel="noreferrer noopener">IAS deep learning workshop</a>, <a href="https://www.oneworldml.org/home" target="_blank" rel="noreferrer noopener">One World ML</a>, <a href="https://www.trustworthyml.org" target="_blank" rel="noreferrer noopener">Trustworthy ML</a>, <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener"><em>Foundations of Data Science</em>,</a> <a href="https://sites.google.com/view/rltheoryseminars/home" target="_blank" rel="noreferrer noopener">RL Theory Virtual Seminars</a>, <a href="https://www.imsi.institute/the-multifaceted-complexity-of-machine-learning/" target="_blank" rel="noreferrer noopener">iMSi: The Multifaceted Complexity of Machine Learning</a>, <em><a href="https://sites.google.com/view/control-meets-learning/home" target="_blank" rel="noreferrer noopener">Control Meets Learning</a></em> </p>



<p><strong>Acknowledgments:</strong> We thank Kamalika Chaudhuri, Elad Hazan, Daniel Hsu, Gautam Kamath, Shay Moran, Omer Reingold, Csaba Szepesvári, and Claire Vernade for helpful comments and thoughtful quotes. We thank Kush Bhatia, Lee Cohen, Neha Gupta, Nika Haghtalab, Max Hopkins, Gautam Kamath, Gaurav Mahajan, and Uri Sherman for helpful feedback on initial drafts.</p>



<div class="wp-block-image"><figure class="alignleft is-resized"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/GlasgowMargalit-2-edited-1.jpg" class="wp-image-339" height="150" /></figure></div>



<p class="has-text-align-left"><a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener"><strong>Margalit Glasgow</strong></a> is a PhD student in Stanford’s Computer Science Department, advised by Mary Wootters. Her research focuses on theoretical machine learning and random matrices.</p>



<div style="height: 20px;" class="wp-block-spacer"></div>



<div class="wp-block-image"><figure class="alignright size-large is-resized"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/MoshkovitzMichal.jpg" class="wp-image-327" height="150" /></figure></div>



<p><a href="https://sites.google.com/view/michal-moshkovitz" target="_blank" rel="noreferrer noopener"><strong>Michal Moshkovitz</strong></a> is a postdoc at the Qualcomm Institute at UC San Diego. She received her PhD and MSc in computational neuroscience from the Hebrew University of Jerusalem, and her MSc in computer science from Tel Aviv University. Her research focuses on the foundations of AI, exploring how different constraints affect learning. She has worked on bounded-memory learning, explainable machine learning, and online decision-making in unsupervised learning. </p>



<div class="wp-block-image"><figure class="alignleft size-thumbnail"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/RashtchianCyrus-150x150.jpg" class="wp-image-332" height="150" /></figure></div>



<p><strong><a href="https://sites.google.com/site/cyrusrashtchian/" target="_blank" rel="noreferrer noopener">Cyrus Rashtchian</a></strong> is a postdoc at UC San Diego in computer science and engineering, and he received his PhD from the University of Washington. His research focuses on trustworthy machine learning, algorithms for big data, statistical reconstruction, and DNA data storage.</p></div>







<p class="date">
by 1737780 <a href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/"><span class="datestr">at July 19, 2021 09:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">5th SIAM Symposium on Simplicity in Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 10-11, 2022 Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa22 Submission deadline: August 9, 2021 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding of the … <a href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">5th SIAM Symposium on Simplicity in Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/"><span class="datestr">at July 18, 2021 01:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=879">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/">Windows never changes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some things I didn’t want to remove, often with a lot of effort, because space was so tight that Windows didn’t even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program — a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/"><span class="datestr">at July 06, 2021 04:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">ADFOCS 2021: Convex Optimization and Graph Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 26 – August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this year’s edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to … <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and Graph Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/"><span class="datestr">at July 03, 2021 01:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/06/29/trace-reconstruction/">Trace Reconstruction from Complex Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> is an unknown binary string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. We are asked to recover <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> from its <em>traces</em>, and each <em>trace</em> <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is a random subsequence obtained by deleting the bits of <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> independently with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />.</p>



<p>More formally, let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> denote the distribution of traces obtained from string <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" />. For example, when <img src="https://s0.wp.com/latex.php?latex=s+%3D+110&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = 110" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> assigns a probability mass of <img src="https://s0.wp.com/latex.php?latex=1%2F8&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/8" class="latex" /> to each element in the multiset<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Ctext%7Bempty%7D%2C+1%2C+1%2C+0%2C+11%2C+10%2C+10%2C+110%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{\text{empty}, 1, 1, 0, 11, 10, 10, 110\}." class="latex" /><br /></p>



<p>We then ask: what is the smallest number <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> such that we can recover any <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" /> (say, with probability <img src="https://s0.wp.com/latex.php?latex=%5Cge+0.99&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\ge 0.99" class="latex" />) given <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> independent samples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" />?</p>



<p>This <em>trace reconstruction</em> problem was first formulated by <a href="https://people.cs.umass.edu/~mcgregor/papers/04-soda.pdf" target="_blank" rel="noreferrer noopener">Batu-Kannan-Khanna-McGregor</a> in 2004, and their central motivation is from the <a href="https://en.wikipedia.org/wiki/Multiple_sequence_alignment" target="_blank" rel="noreferrer noopener">multiple sequence alignment</a> problem in computational biology. Trace reconstruction is also a fundamental problem related to the <a href="https://en.wikipedia.org/wiki/Deletion_channel" target="_blank" rel="noreferrer noopener">deletion channel</a> in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />. Then the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> tells us the number of independent copies that need to be sent for the receiver to determine the original message.</p>



<p>Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the computational complexity). In 2017, <a href="https://arxiv.org/pdf/1612.03148.pdf" target="_blank" rel="noreferrer noopener">De-O’Donnell-Servedio</a> and <a href="https://arxiv.org/pdf/1612.03599.pdf" target="_blank" rel="noreferrer noopener">Nazarov-Peres</a> independently proved <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Cle+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \le \exp(O(n^{1/3}))" class="latex" />. A very recent breakthrough due to <a href="https://arxiv.org/pdf/2009.03296.pdf" target="_blank" rel="noreferrer noopener">Chase</a> further improved this bound to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Ctilde+O%28n%5E%7B1%2F5%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(\tilde O(n^{1/5}))" class="latex" />, which is still super-polynomial. On the other hand, the best known sample complexity lower bound is merely <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%5COmega%28n%5E%7B3%2F2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde\Omega(n^{3/2})" class="latex" />, proved by <a href="https://arxiv.org/pdf/1905.03031.pdf" target="_blank" rel="noreferrer noopener">Chase</a> in another recent work.</p>



<p>In this blog post, I will explain why this problem is much more non-trivial than it might appear at first glance. I will also give an overview on the work of [DOS17, NP17], which, interestingly, reduces this seemingly combinatorial problem to complex analysis.</p>



<p><strong>Observation: reconstruction <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> TV-distance.</strong> Let us start with a natural first attempt at the problem. Define <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> as the minimum statistical distance between the trace distribution of two different length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> strings:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n+%3D+%5Cmin_%7Bx+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En%7Dd_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n = \min_{x \ne y \in \{0, 1\}^n}d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)." class="latex" /><br /></p>



<p>It is not hard to show that <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n" class="latex" /> bounds <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> on both sides, up to a polynomial factor:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n+%5Clesssim+M%28n%29+%5Clesssim+n%2F%5Cdelta_n%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n \lesssim M(n) \lesssim n/\delta_n^2." class="latex" /><br /></p>



<p class="has-black-color has-text-color">The lower bound holds because any trace reconstruction algorithm must be able to distinguish <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for every pair of different strings <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" />, and this requires <img src="https://s0.wp.com/latex.php?latex=%5COmega%281+%2F+%5Cdelta_n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(1 / \delta_n)" class="latex" /> samples for the minimizer <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> in the definition of <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />. For the upper bound, we note that every pair <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> can be distinguished with an <img src="https://s0.wp.com/latex.php?latex=o%282%5E%7B-n%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="o(2^{-n})" class="latex" /> error probability using <img src="https://s0.wp.com/latex.php?latex=O%28n%2F%5Cdelta_n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n/\delta_n^2)" class="latex" /> samples. We say that string <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> “beats” <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />, if the distinguisher for <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> decides that “<img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" />“. By a union bound, the correct answer <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> “beats” every other string with high probability. We can then obtain a reconstruction algorithm by running the distinguisher for every string pair, and outputting the unique string that “beats” the other <img src="https://s0.wp.com/latex.php?latex=2%5En-1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^n-1" class="latex" /> strings.</p>



<p>Thus, to determine whether the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> is polynomial in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />, it suffices to determine whether <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> scales as <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\mathrm{poly}(n)" class="latex" /> or is much smaller. Unfortunately, it turns out to be highly non-trivial to bound <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />, and even bounding <img src="https://s0.wp.com/latex.php?latex=d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)" class="latex" /> for “simple” <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> can be hard. Imagine that we try to reason about the distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" />: the probability mass that <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> assigns to string <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is proportional to the number of times that <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> appears as a subsequence in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, but this count is already hard to express or control, unless <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> has a very simple pattern. This is roughly where this natural attempt gets stuck.</p>



<p><strong>Distinguishing using estimators.</strong> Now we turn to a different approach that underlies the recent breakthrough on trace reconstruction algorithms. As discussed earlier, we can focus on the problem of distinguishing the case <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for fixed strings <img src="https://s0.wp.com/latex.php?latex=x+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x \ne y \in \{0, 1\}^n" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f: \{0, 1\}^{\le n} \to \mathbb{R}" class="latex" /> be a function defined over all possible traces from a length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> string. We consider the following algorithm for distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" />:<br /><br /><strong>Step 1.</strong> Given traces <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s_1%2C+%5Ctilde+s_2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s_1, \tilde s_2, \ldots" class="latex" />, compute the average of <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s_1%29%2C+f%28%5Ctilde+s_2%29%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s_1), f(\tilde s_2), \ldots" class="latex" /><br /><strong>Step 2.</strong> Output <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> if the average is closer to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+x+%5Csim+%5Cmathcal%7BD%7D_x%7D%5Bf%28%5Ctilde+x%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde x \sim \mathcal{D}_x}[f(\tilde x)]" class="latex" /> than to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+y+%5Csim+%5Cmathcal%7BD%7D_y%7D%5Bf%28%5Ctilde+y%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde y \sim \mathcal{D}_y}[f(\tilde y)]" class="latex" />, and output <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> otherwise.<br /></p>



<p>For the above to succeed, <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> needs to satisfy the following two conditions:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" /> are well-separated under <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, namely <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+x%29%5D+-+%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+y%29%5D%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\mathbb{E}[f(\tilde x)] - \mathbb{E}[f(\tilde y)]| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon &gt; 0" class="latex" />.<br /><strong>(Boundedness)</strong> Expectation of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can be efficiently estimated. This can be guaranteed if for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Cf%28%5Ctilde+s%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|f(\tilde s)| \le B" class="latex" /> holds for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%5Cin+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s \in \{0, 1\}^{\le n}" class="latex" />.<br /></p>



<p>Assuming the above, a standard concentration argument shows that we can distinguish <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> using <img src="https://s0.wp.com/latex.php?latex=O%28%28B%2F%5Cepsilon%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O((B/\epsilon)^2)" class="latex" /> samples, and thus <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Clesssim+n+%5Ccdot+%28B%2F%5Cepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \lesssim n \cdot (B/\epsilon)^2" class="latex" />.</p>



<p>In principle, a near-optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> would be setting <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s)" class="latex" /> to be the indicator of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x%28%5Ctilde+s%29+%5Cge+%5Cmathcal%7BD%7D_y%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x(\tilde s) \ge \mathcal{D}_y(\tilde s)" class="latex" />, which gives boundedness <img src="https://s0.wp.com/latex.php?latex=B+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = 1" class="latex" /> and separation <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29+%5Cge+%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y) \ge \delta_n" class="latex" />. However, as argued above, this optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can still be hard to analyze. Instead, we focus on choosing a simpler function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, which potentially gives a suboptimal <img src="https://s0.wp.com/latex.php?latex=B%2F%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B/\epsilon" class="latex" /> but admits simple analyses.</p>



<p><strong>Sketch of the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> Upper Bound.</strong> The main results of [DOS17] and [NP17] follow from choosing <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> to be a linear function. Given a trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%3D+%5Ctilde+s_0+%5Ctilde+s_1+%5Ctilde+s_2+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s = \tilde s_0 \tilde s_1 \tilde s_2 \cdots" class="latex" />, we consider the following polynomial with coefficients being the bits of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29+%3D+%5Csum_%7Bk%7D%5Ctilde+s_k+z%5Ek+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z) = \sum_{k}\tilde s_k z^k = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots." class="latex" /><br /></p>



<p>For some number <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> to be determined later, we consider the estimator <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29+%3D+%5Ctilde+S%28z%29+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s) = \tilde S(z) = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots" class="latex" />, which is indeed a linear function in the trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />.</p>



<p>At first glance, it might be unclear why we choose the coefficients to be powers of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" />. The following fact justifies this choice by showing that polynomials interact with the deletion channel very nicely: The expectation of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z)" class="latex" /> is exactly the evaluation of the polynomial <img src="https://s0.wp.com/latex.php?latex=S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S(\cdot)" class="latex" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=s_0%2C+s_1%2C+%5Cldots%2C+s_%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s_0, s_1, \ldots, s_{n-1}" class="latex" />, but at a slightly different point.<br /></p>



<p><strong>Fact:</strong> For any string <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+s+%5Csim+%5Cmathcal%7BD%7D_s%7D%5Cleft%5B%5Ctilde+S%28z%29%5Cright%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%5Cleft%28%5Cfrac%7Bz%2B1%7D%7B2%7D%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde s \sim \mathcal{D}_s}\left[\tilde S(z)\right] = \frac{1}{2}S\left(\frac{z+1}{2}\right)." class="latex" /><br /></p>



<p>Equivalently, we have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+S%282z-1%29%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}[\tilde S(2z-1)] = \frac{1}{2}S(z)" class="latex" />, which allows us to rephrase our requirements on the choice of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> as follows:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%7CX%28z%29+-+Y%28z%29%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|X(z) - Y(z)| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> that is not too small.<br /><strong>(Boundedness)</strong> <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)| \le B" class="latex" /> for all possible trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />, for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" /> that is not too large.<br /></p>



<p>After some thought, it is beneficial to choose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> such that both <img src="https://s0.wp.com/latex.php?latex=%7Cz%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|z|" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1|" class="latex" /> are close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />, since this ensures that different bits in the string are assigned weights of similar magnitudes in the polynomials above. These two conditions hold if and only if <img src="https://s0.wp.com/latex.php?latex=z+%5Capprox+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \approx 1" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="512" alt="" src="https://theorydish.files.wordpress.com/2021/06/plan.png?w=1024" class="wp-image-2442" height="230" />The overall plan for distinguishing strings <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />.</figure></div>



<p>The crucial idea in both papers [DOS17, NP17] is to consider the polynomials in the complex plane <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{C}" class="latex" /> instead of on the real line. Fortunately, all the previous discussion still holds for complex numbers, with <img src="https://s0.wp.com/latex.php?latex=%7C%5Ccdot%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\cdot|" class="latex" /> interpreted as modulus instead of absolute value. In [NP17], the authors chose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> from a small arc of the unit circle:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=A_L+%3D+%5C%7Be%5E%7Bi%5Ctheta%7D%3A+%5Ctheta%5Cin%5B-%5Cpi%2FL%2C+%5Cpi%2FL%5D%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L = \{e^{i\theta}: \theta\in[-\pi/L, \pi/L]\}" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="311" alt="" src="https://theorydish.files.wordpress.com/2021/06/a_l.png?w=622" class="wp-image-2445" height="262" />Arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" /> marked by the red box and dotted lines.</figure></div>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" />, it is easy to upper bound <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)|" class="latex" />: it follows from simple calculus that <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C+%5Cle+1+%2B+O%28L%5E%7B-2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1| \le 1 + O(L^{-2})" class="latex" />, and thus for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctilde+S%282z-1%29%5Cright%7C+%5Cle+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D%5Cleft%7C%282z-1%29%5Ek%5Cright%7C+%5Cle+n+%5Ccdot+%5Cleft%5B1+%2B+O%28L%5E%7B-2%7D%29%5Cright%5D%5En+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\left|\tilde S(2z-1)\right| \le \sum_{k=0}^{n-1}\left|(2z-1)^k\right| \le n \cdot \left[1 + O(L^{-2})\right]^n = e^{O(n/L^2)}" class="latex" />.</p>



<p>To lower bound <img src="https://s0.wp.com/latex.php?latex=X%28z%29+-+Y%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X(z) - Y(z)" class="latex" />, note that since both <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> are binary, all the coefficients of <img src="https://s0.wp.com/latex.php?latex=X-Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X-Y" class="latex" /> are in <img src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+0%2C+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{-1, 0, 1\}" class="latex" />. Such polynomials are known as <em>Littlewood polynomials</em>. The separation condition is then reduced to the following claim in complex analysis:</p>



<p class="has-text-align-center"><em><strong>Littlewood polynomials cannot to be too “flat” over a short arc around 1.</strong></em></p>



<p>This is indeed the case:</p>



<p id="lemma-1"><strong>Lemma 1.</strong> (<a href="https://www.jstor.org/stable/pdf/24899666.pdf" target="_blank" rel="noreferrer noopener">[Borwein and Erdélyi, 1997]</a>) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge e^{-O(L)}" class="latex" />.<br /></p>



<p>By <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a>, there exists <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" /> such that the separation condition holds for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = e^{-O(L)}" class="latex" />. Recall that the boundedness holds for <img src="https://s0.wp.com/latex.php?latex=B+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = e^{O(n/L^2)}" class="latex" />. This shows that the sample complexity is upper bounded by <img src="https://s0.wp.com/latex.php?latex=%28B%2F%5Cepsilon%29%5E2+%3D+%5Cexp%28O%28n%2FL%5E2+%2B+L%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(B/\epsilon)^2 = \exp(O(n/L^2 + L))" class="latex" />, which is minimized at <img src="https://s0.wp.com/latex.php?latex=L+%3D+n%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L = n^{1/3}" class="latex" />. This proves the upper bound <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%3D+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) = \exp(O(n^{1/3}))" class="latex" />.</p>



<p><strong>Proof of a weaker lemma.</strong> While the original proof of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> is a bit technical, [NP17] presented a beautiful and much simpler proof of the following weaker result, in which the <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="e^{-O(L)}" class="latex" /> lower bound is replaced by <img src="https://s0.wp.com/latex.php?latex=n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{-O(L)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is the degree of the Littlewood polynomial.</p>



<p><strong>Lemma 2.</strong> ([Lemma 3.1, NP17]) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> of degree <img src="https://s0.wp.com/latex.php?latex=%3C+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt; n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge n^{-O(L)}" class="latex" />.<br /></p>



<p><strong>Proof.</strong> Without loss of generality, we assume that the constant term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />. Suppose otherwise, that the lowest order term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z^m" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=m+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m \ge 1" class="latex" />. We may consider the polynomial <img src="https://s0.wp.com/latex.php?latex=p%28z%29+%2F+z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z) / z^m" class="latex" /> instead.</p>



<p>Let <img src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M = \max_{z \in A_L}|p(z)|" class="latex" /> be the maximum modulus of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> over arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+e%5E%7B2%5Cpi+i%2FL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega = e^{2\pi i/L}" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />-th root of unity. Consider the following polynomial:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=q%28z%29+%3D+%5Cprod_%7Bj%3D0%7D%5E%7BL-1%7Dp%28%5Comega%5Ej+z%29+%3D+p%28z%29+%5Ccdot+p%28%5Comega+z%29+%5Ccdot+%5Ccdots+%5Ccdot+p%28%5Comega%5E%7BL-1%7D+z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z) = \prod_{j=0}^{L-1}p(\omega^j z) = p(z) \cdot p(\omega z) \cdot \cdots \cdot p(\omega^{L-1} z)" class="latex" />.</p>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, at least one point <img src="https://s0.wp.com/latex.php?latex=%5Comega%5Ej+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^j z" class="latex" /> falls into the arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7Cp%28%5Comega%5Ej+z%29%7C+%5Cle+M&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|p(\omega^j z)| \le M" class="latex" />. (See <a href="https://theorydish.blog/feed/#windmill">figure below</a> for a proof by picture.) The moduli of the remaining <img src="https://s0.wp.com/latex.php?latex=L+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L - 1" class="latex" /> factors are trivially bounded by <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. Thus, <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cle+M%5Ccdot+n%5E%7BL-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \le M\cdot n^{L-1}" class="latex" />.</p>



<p>On the other hand, we note <img src="https://s0.wp.com/latex.php?latex=q%280%29+%3D+%5Bp%280%29%5D%5EL+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(0) = [p(0)]^L = 1" class="latex" />. The <a href="https://en.wikipedia.org/wiki/Maximum_modulus_principle" target="_blank" rel="noreferrer noopener">maximum modulus principle</a> implies that for some <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, we have <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \ge 1" class="latex" />. Therefore, we must have <img src="https://s0.wp.com/latex.php?latex=M+%5Ccdot+n%5E%7BL+-+1%7D+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \cdot n^{L - 1} \ge 1" class="latex" />, which implies <img src="https://s0.wp.com/latex.php?latex=M+%5Cge+n%5E%7B-%28L+-+1%29%7D+%3D+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \ge n^{-(L - 1)} = n^{-O(L)}" class="latex" /> and completes the proof. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<div class="wp-block-image" id="windmill"><figure class="aligncenter size-large is-resized"><img width="462" alt="" src="https://theorydish.files.wordpress.com/2021/06/example.png?w=924" class="wp-image-2449" height="261" />Points involved in the definition of <img src="https://s0.wp.com/latex.php?latex=q%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=L%3D4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L=4" class="latex" />.<br />In this case, <img src="https://s0.wp.com/latex.php?latex=%5Comega%5E3+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^3 z" class="latex" /> is inside arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, which is marked by the red lines.</figure></div>



<p><strong>Beyond linear estimators?</strong> The work of [DOS17, NP17] not only proved the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> upper bound, but also showed that this is the best sample complexity we can get from linear estimator <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />. The recent work of [Cha20] goes beyond these linear estimators by taking the higher moments of the trace into account. The idea turns out to be a natural one: consider the “<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-grams” (i.e., length-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> substrings) of the string for some <img src="https://s0.wp.com/latex.php?latex=k+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k &gt; 1" class="latex" />. In more detail, we fix some string <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5C%7B0%2C+1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w \in \{0, 1\}^k" class="latex" />, and consider the binary polynomial with coefficients corresponding to the positions at which <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> appears as a (contiguous) substring. The key observation is that there exists a choice of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> such that the resulting polynomial is sparse (in the sense that the degrees of the nonzero monomials are far away). The technical part of [Cha20] is then devoted to proving an analogue of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> that is specialized to these “sparse” Littlewood polynomials.</p>



<p><strong>Acknowledgments.</strong> I would like to thank Moses Charikar, Li-Yang Tan and Gregory Valiant for being on my quals committee and for helpful discussions about this problem.</p></div>







<p class="date">
by Mingda Qiao <a href="https://theorydish.blog/2021/06/29/trace-reconstruction/"><span class="datestr">at June 29, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 4, 2021 – July 7, 2021 Wrocław, Poland and online https://cpm2021.ii.uni.wroc.pl/ The Annual Symposium on Combinatorial Pattern Matching (CPM) has by now over 30 years of tradition and is considered to be the leading conference for the community working on Stringology. The objective of the annual CPM meetings is to provide an international forum … <a href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/" class="more-link">Continue reading <span class="screen-reader-text">Annual Symposium on Combinatorial Pattern Matching (summer school + conference))</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/06/22/annual-symposium-on-combinatorial-pattern-matching-summer-school-conference/"><span class="datestr">at June 22, 2021 08:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/">Social at STOC 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I’ve been asked to pass along a message by <a href="https://ccanonne.github.io/">Clément Canonne</a>, social chair for STOC 2021. This STOC might have the best social program of any I’ve ever seen, either virtual or in-person, so be sure to check it out!</p>



<hr class="wp-block-separator" />



<p>STOC’21 is around the corner, starting tomorrow; [don’t forget to register](<a href="http://acm-stoc.org/stoc2021/">http://acm-stoc.org/stoc2021/</a>), if you haven’t yet! This year, the (virtual) conference will include several social activities (games, TCS trivia, mystery hunt…); among which, two “junior/senior lunches,” on Monday and Friday.<br /><br />Those both will be held in the Gather space for STOC (<a href="http://acm-stoc.org/stoc2021/venue.html">http://acm-stoc.org/stoc2021/venue.html</a>), and — as in previous years — are the occasion for senior researchers in the field, broadly construed, to have an informal chat with students, postdocs, and junior faculty, answer their questions, discuss their research, and generally have a nice conversation.<br /><br />If you are interested, don’t forget to sign up! This is done through the “feedback box” placed on the Information Desk in the Gather space’s Lobby, which gives access to a spreadsheet.<br /><br />Hoping to see you at STOC!</p>



<figure class="wp-block-image size-large"><img src="https://kamathematics.files.wordpress.com/2021/06/image.png?w=778" alt="" class="wp-image-325" /></figure></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2021/06/21/social-at-stoc-2021/"><span class="datestr">at June 21, 2021 02:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5861">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/quest-for-adaptivity/">The quest for adaptivity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Most machine learning classes and textbooks mention that there is no universal supervised learning algorithm that can do reasonably well on all learning problems. Indeed, a series of “no free lunch theorems” state that even in a simple input space, for any learning algorithm, there always exists a bad conditional distribution of outputs given inputs where this algorithm performs arbitrarily bad. </p>



<p class="justify-text">For example, from the classic book of Luc Devroye, László Györfi, and Gábor Lugosi [<a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">1</a>, Theorem 7.2 and its extensions], for inputs uniformly distributed in \([0,1]\), for any decreasing sequence \((\varepsilon_n)_{n \geqslant 0}\) which is less than 1/16, for any learning algorithm (which takes pairs of observations and outputs a prediction function), there exists a conditional distribution on \(\{-1,1\}\) for which the expected risk of the classifier learned from \(n\) independent and identically distributed observations of (input,output) pairs is greater than \(\varepsilon_n\) for all \(n \geqslant 1\), while the best possible expected error rate is zero.</p>



<p class="justify-text">Such theorems do not imply that all learning methods are equally bad, but rather that all learning methods will suffer from some weaknessess. Throughout this blog we will try to better understand the weaknessess and strengths of popular methods through learning theory.</p>



<p class="justify-text">The key is to control the potential weaknesses of a learning method by making sure that in “favorable” scenarios, it leads to strong guarantees. When taking the simplest example of vectorial inputs in \(\mathbb{R}^d\), we can construct model classes of increasing complexity for which we can start to draw useful comparisons between learning methods.</p>



<p class="justify-text">Several aspects of the joint distribution of (input,output) \((X,Y)\) make the problem easy or hard. For concreteness and simplicity, I will focus on regression problems where the output space is \(\mathcal{Y} = \mathbb{R}\) and with the square loss, so that the optimal function, the “target” function, is \(f^\ast(x) = \mathbb{E}(Y|X=x)\). But much of the discussion extends to classification or even more complex outputs (see, e.g., [1]).</p>



<p class="justify-text"><strong>Curse of dimensionality.</strong> In the context of vectorial inputs, the slowness of universal learning algorithms can be characterized more precisely, and leads to the classical <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>. Only assuming that the optimal target function is Lipschitz-continuous, that is, for all \(x,x’\), \(| f^\ast(x)-f^\ast(x’)| \leqslant  L \| x – x’\|\) (for any arbitrary norm on \(\mathbb{R}^d\)), the optimal excess risk of a prediction function \(\hat{f}\) obtained from \(n\) observations, that is, the expected squared difference between \(f^\ast(x)\) and \(\hat{f}(x)\) cannot be less than a constant times \(n^{-2/(d+2)}\), that is, in order for this rate to be smaller than some \(\varepsilon &lt;1 \), we need \(n\) to be larger than \(\displaystyle ( {1}/{\varepsilon} )^{d/2+1}\), with thus an exponential dependence in \(d\) (see [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>]).</p>



<p class="justify-text">In other words, exponentially many observations are needed for a reasonable performance on all problems with a minimal set of assumptions (here Lipschitz-continuity), and this bad behavior is unavoidable unless extra assumptions are added, which we now describe.</p>



<h2>Support, smoothness and latent variables</h2>



<p class="justify-text"><strong>Low-dimensional support. </strong>If the data occupy only a \(r\)-dimensional subspace of \(\mathbb{R}^d\), with \(r \leqslant d\) (and typically much smaller), then one should expect a better convergence rate. This also extends to data supported on a (smooth) <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>, as illustrated below. Note that this assumption does not concern the outputs, and can reasonably be checked given the data by performing principal component analysis or some form of <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">manifold learning</a>. Essentially, in terms of convergence rates, \(d\) is replaced by \(r\). This is obvious if the learning algorithm has access to the \(r\)-dimensional representation, and it requires more work if not.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="508" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/support.png" class="wp-image-5959" height="225" />Left: samples from the uniform distribution in the unit disk in \(\mathbb{R}^2\). Right: samples from the uniform distribution in the unit circle in \(\mathbb{R}^2\), which is a one-dimensional smooth manifold.</figure></div>



<p class="justify-text"><strong>Smoothness of the target function. </strong>This is done by assuming some bounded derivatives for the target function \(f^\ast\). With bounded \(s\)-th order derivatives, we could expect that the problem is easier (note that Lipschitz-continuity corresponds to \(s=1\)). This is illustrated below with one-dimensional inputs. Essentially, in terms of rates, \(d\) is replaced by \(d/s\) [<a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">2</a>, Theorem 3.2]. Therefore, when the smoothness order \(s\) is of order \(d\), the dependence in the dimension disappears.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="517" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/smooth-1024x445.png" class="wp-image-5961" height="225" />Regression problem with one-dimensional inputs, with target function in red, and observations in blue. Left: non-smooth (even not Lipschitz-continuous) function, right: smooth function.</figure></div>



<p class="justify-text"><strong>Latent variables.</strong> If we assume that the target function depends only on a \(r\)-dimensional linear projection of the input, then we should expect a better complexity. The most classical example is the dependence on a subset of the \(d\) original variables. Essentially, in terms of rates, \(d\) is replaced by \(r\). This is obvious when the latent variables are known (as we can replace input data by the \(r\) latent variables), totally not otherwise, as this requires some form of <em>adaptivity</em> (see below).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="553" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/latent-1024x467.png" class="wp-image-5964" height="252" />Regression problem with two-dimensional inputs, with target function (with no noise). Left:  function that cannot be written with a latent variable, right: function that has a (linear) latent variable representation (there exists a direction with no variation).</figure></div>



<p class="justify-text"><strong>Need for adaptivity.</strong> We typically don’t know in advance if these properties are satisfied or not, as some are easily testable (support of distribution) without knowing the target function \(f^\ast\), while others are not.</p>



<p class="justify-text">The goal is to have a single method that can adapt to all of these situations (which are non-exclusive). That is, if the problem has any of these reasons to be easy, will the learning method benefit from it? Typically, most learning methods have at least one hyperparameter controlling overfitting (e.g., a regularization parameter), and the precise value of this hyperparameter will depend on the difficulty of the problem, and we will assume that we have a reasonable way to estimate this hyperparameter (e.g., cross-validation). A method is then said adaptive if with a well chosen value of the hyperparameter, we get the optimal (or close to optimal) rate of estimation that benefits from the extra assumption.</p>



<p class="justify-text"><strong>Quest for adaptivity: who wins?</strong> Among classical learning techniques, which ones are adaptive to which properties? In short, <em>barring computational and optimization issues</em>: $$\mbox{ local averaging } &lt; \mbox{ positive definite kernels } &lt; \mbox{ neural networks }.$$ Every time, the next method in the list gains adaptivity to support, smoothness and then latent variables. <span class="has-inline-color has-vivid-red-color">Note however that optimization for neural networks is more delicate (see below).</span></p>



<p class="justify-text">Let’s now briefly look at these methods one by one. I will assume basic knowledge of these, for more details see [<a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">4</a>, <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">5</a>] or my <a href="https://francisbach.com/i-am-writing-a-book/">new book in preparation</a>.</p>



<h2>Local averaging</h2>



<p class="justify-text">The earliest and simplest learning methods that could adapt to any target functions were <em>local averaging</em> methods aiming at approximating directly the conditional expectation \(f^\ast(x) = \mathbb{E}(Y|X=x)\), with the most classical examples being <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><em>k</em>-nearest neighbors</a> and <a href="https://en.wikipedia.org/wiki/Kernel_regression">Nadaraya-Watson</a> estimators.</p>



<p class="justify-text">These methods are naturally adaptive to having a reduced support for inputs. Indeed, in the simplest case of a distribution supported in a low-dimensional subspace, the global metric is equivalent to a local metric on the support, without any need to explicitly know the subspace, a situation which extends to smooth manifolds. In order to be adaptive, the hyperparameter has to depend on the dimension \(r\) of the manifold, e.g., for \(k\)-nearest-neighbors, \(k\) has to be taken proportional to \(n^{2/(2+r)}\) (see [<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf">3</a>]).</p>



<p class="justify-text">However, there is no adaptivity to the smoothness of the target function or to potential latent variables unless dedicated algorithms are used such as <a href="https://en.wikipedia.org/wiki/Local_regression">local regression</a>. Kernel methods and neural networks lead to such adaptivity.</p>



<h2>From kernels to neural networks</h2>



<p class="justify-text">We consider prediction functions of the form $$f(x) = \sum_{j=1}^m a_j ( b_j^\top x )_+,$$ which is the traditional single hidden layer fully connected neural network with ReLU activation functions (a constant term can be added to the linear term within the ReLU by simply appending \(1\) to \(x\), please do not call these terms “biases” as this has <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">another meaning in statistics</a>).</p>



<p class="justify-text">The vector \(a \in \mathbb{R}^m\) represents <em>output weights</em>, while the matrix \(b \in \mathbb{R}^{m \times d}\) represents <em>input weights</em>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="564" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/nn_blog-1024x546.png" class="wp-image-5975" height="300" /></figure></div>



<p class="justify-text"><strong>Empirical risk minimization (ERM). </strong>We assume given \(n\) i.i.d. observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathbb{R}^d \times \mathbb{R}\), and we will fit models by minimizing the \(\ell_2\)-regularized empirical risk (you can call it “weight decay” but it  already has a better name in machine learning and statistics). That is, we minimize $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ where \(\lambda &gt; 0\) is a regularization parameter.</p>



<p class="justify-text"><strong>Overparametrization. </strong>We will consider the limit of large number \(m\) of hidden neurons. Depending whether we optimize over both input weights \(b\) and output weights \(a\), or simply output weights (with then a proper initialization of the input weights), we get different behaviors.</p>



<h2>Kernel regime</h2>



<p class="justify-text">We assume that the input weights \(b_j \in \mathbb{R}^d\) are sampled uniformly from the Euclidean sphere of radius \(1 / \sqrt{m}\), and that we only optimize over the output weights \(a_j, j = 1,\dots,m\). This is exactly a <a href="https://en.wikipedia.org/wiki/Ridge_regression">ridge regression</a> problem (square loss and squared Euclidean penalty), for which the theory of positive definite kernels applies and will lead to an interesting behavior for infinite \(m\) [7, <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>]. That is, the solution can be obtained by using the kernel function \(\hat{k}\) defined as $$\hat{k}(x,x’) = \sum_{j=1}^m ( b_j^\top x )_+( b_j^\top x’ )_+,$$ and looking for prediction functions of the form $$f(x) = \sum_{i=1}^n \alpha_i \hat{k}(x,x_i).$$ See, e.g., [6] for details. In particular, as long as \(\hat{k}(x,x’)\) can be computed efficiently, the complexity of solving the ERM problem is independent of the number of neurons \(m\). We will now discuss in further detail the over-parameterized case where \(m=\infty\).</p>



<p class="justify-text">When \(m\) tends to infinity, If the input weights are fixed and initialized randomly from the \(\ell_2\)-sphere of radius \(1/\sqrt{m}\), then by the law of large numbers, \(\hat{k}(x,x’)\) tends to $$k(x,x’) = \mathbb{E}_{b \sim {\rm uniform} (\mathbb{S}^{d-1}) } (b^\top x )_+ (b^\top x’)_+,$$ where \(\mathbb{S}^{d-1}\) is the unit \(\ell_2\)-sphere in \(d\) dimensions. This kernel has a closed form and happens to be equal to (see [<a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">9</a>]) $$ \frac{1}{2\pi d}  \|x\|_2   \| x’\|_2 \big[ ( \pi \, – \varphi) \cos \varphi + \sin \varphi \big],$$ where \(\cos \varphi = \frac{ x^\top x’  }{{ \|x\|_2  } { \| x’\|_2   } }\). It can also be seen (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] for details) as using predictors of the form $$f(x) = \int_{\mathbb{S}^{d-1}}  ( b^\top x)_+ d \mu(b),$$ for some measure \(\mu\) on \(\mathbb{S}^{d-1}\), with the penalty $$\frac{\lambda}{2} \int_{\mathbb{S}^{d-1}} \big| \frac{d\mu}{d\tau}(b) \big|^2 d\tau(b),$$ for the uniform probability measure \(d\tau\) on the hypersphere \(\mathbb{S}^{d-1}\). This representation will be useful for the comparison with neural networks.</p>



<p class="justify-text">Note that here we use random features in a different way from their common use [<a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">8</a>], where the kernel \(k\) comes first and is approximated by \(\hat{k}\) to obtain fast algorithms, typically with \(m &lt; n\). Here we start from \(\hat{k}\) and find the limiting \(k\) to understand the behavior of overparameterization. The resulting function space is a (<a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev</a>) space of functions on the sphere with all \(s\)-th order derivatives which are square-integrable, with \(s = d/2+3/2\) [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>].</p>



<p>Finally, the number of neurons \(m\) needed to reach the kernel regime is well understood, at least in simple situations [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>, <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">16</a>].</p>



<p class="justify-text">We can now look at the various forms of adaptivity of kernel methods based on Sobolev spaces.</p>



<p class="justify-text"><strong>Adaptivity to reduced support.</strong> Like model averaging techniques, adaptivity to input data supported on a subspace is rather straightforward, and it extends to smooth manifolds (see, e.g., [<a href="https://arxiv.org/pdf/2003.06202.pdf">12</a>] for a proof for the Gaussian kernel).</p>



<p class="justify-text"><strong>Adaptive to smoothness.</strong> A key attractive feature of kernel methods is that they can circumvent the curse of dimensionality for <em>smooth</em> target functions, and, by simply ajusting the regularization parameter \(\lambda\), ridge regression will typically adapt to the smoothness of the target function, and thus benefit from easy problems.</p>



<p class="justify-text">The simplest instance is when the target function is within the space of functions defined above, where we immediately get estimation rates which are independent of dimension (at least in the exponent). This however requires at least \(s&gt;d/2\) derivatives (because Sobolev spaces are <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert spaces</a> (RKHS) only in this situation), but the adaptivity extends to functions outside of the RKHS (see, e.g., [11]).</p>



<p class="justify-text"><strong>Adaptivity to linear latent variables.</strong> Unfortunately, kernel methods are not adaptive even to basic linear structures. That is, if \(f^\ast\) depends only on the first component of \(x\), then ridge regression, if not modified, will not take advantage of it. In the context of neural networks, one striking example is that a single neuron \(x \mapsto (b^\top x)_+\) <em>does not belong to the RKHS</em>, and leads to bad estimation rates (see, e.g., [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]).</p>



<p class="justify-text">Before looking at some experiments, let’s look at some optimization considerations, that will be important later for neural networks.</p>



<p class="justify-text"><strong>Avoiding overparameterized neuron representations.</strong> In the kernel regime, optimizing directly the cost function with \(m\) neurons by gradient descent is problematic for two reasons when \(m\) is large:</p>



<ul class="justify-text"><li>The optimization problem, although convex, is severely ill-conditioned as can be seen from the spectrum of the covariance matrix of the feature vector, with the \(i\)-th eigenvalue equivalent to \(i^{-1-3/d}\) for the uniform input distribution (see [14, Section 2.3]). Therefore, (stochastic) gradient descent will take a lot of time to converge.</li><li>We can use kernels directly (for \(m = +\infty\)) to avoid using \(m\) very large (which would be useless in practice). The running-time complexity is then of the order \(O(n^2)\) if done naively, with lots of ways to go below \(O(n^2)\), such as column sampling (see a nice analysis in [<a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">15</a>]).</li></ul>



<p class="justify-text"><strong>Experiments.</strong> We first consider a very simple one-dimensional example, where we look at how the estimation with the kernel function \(k\) varies as a function of the hyperparameter \(\lambda\), from underfitting to overfitting. We can observe that all learned functions are smooth, as expected.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="574" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/rff_biplot.gif" class="wp-image-6045" height="252" />Regression problems in one dimension with kernels (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, the kernel method adapts to it.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="925" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_rff_subplots.png" class="wp-image-6042" height="241" />Left: estimation with the optimal regularization parameter for smooth target function and kernel methods. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure></div>



<h2>Neural networks</h2>



<p class="justify-text">We can now optimize over all weights, output \(a\) <em>and</em> input \(b\). Four natural questions come to mind:</p>



<ol class="justify-text"><li>Where does it converge to when the number \(m\) of hidden neurons tends to infinity?</li><li>What are the adaptivity properties of the resulting predictor?</li><li>How big \(m\) needs to be to achieve the infinite width behavior?</li><li>Can we actually solve the non-convex optimization problem?</li></ol>



<p class="justify-text"><strong>Overparameterization limit (feature learning regime).</strong> When optimizing over both sets of weights, we can first note that the prediction function is invariant by the change of variable $$ a_j \leftarrow \mu_j a_j \ , \ \ b_j \leftarrow \mu_j^{-1} b_j.$$ Then by optimizing over \(\mu_j\) which is only involved in the penalty term \(\frac{\lambda}{2} \big( a_j^2 + \|b_j\|_2^2 \big)\), we get \(\mu_j^\ast = a_j^{-2} \| b_j \|_2^2\), and the penalty \(\lambda |a_j| \| b_j\|_2\). We thus get the equivalent optimization problem of minimizing $$\tilde{R}(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + {\lambda} \sum_{j=1}^m |a_j| \| b_j\|_2 .$$ By restricting the \(b_j\) on the unit sphere (which is OK because by optimizing over \(\mu_j\) we become scale-invariant), we can write $$\sum_{j=1}^m a_j ( b_j^\top x_i )_+ = \int_{\mathbb{S}^{d-1}} (b^\top x)_+ d\mu(b), $$ for $$ \mu = \sum_{j=1}^m a_j \delta_{b_j}$$ a weighted sum of Diracs, and \(\sum_{j=1}^m |a_j| \| b_j\|_2 = \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|\) the <a href="https://en.wikipedia.org/wiki/Total_variation">total variation</a> of \(\mu\). Thus, letting \(m\) go to infinity, the infinite sums become integrals of general measures, and we end up considering the set of functions that can be written as (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>] and the many references therein for details): $$f(x) = \int_{\mathbb{S}^{d-1}}   ( b^\top x)_+ d \mu(b),$$ with the penalty $$\lambda \int_{\mathbb{S}^{d-1}} \! | d\mu(b)|.$$ It has an \(\ell_1\)-norm flavor (as opposed to the \(\ell_2\)-norm for kernels), and leads to further adaptivity, if optimization problems are resolved (which is itself not easy in polynomial time, see below).</p>



<p class="justify-text">The minimal number of neurons to achieve the limiting behavior in terms of predictive performance (assuming optimization problems are resolved) can also be characterized, and grows exponentially in dimension without strong assumptions like the ones made in this blog post (see [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). We will now study the adaptivity properties of using the function space above.</p>



<p class="justify-text"><strong>Adaptive to low-dimensional support and smoothness.</strong> It turns out that the adaptivity properties of kernel methods are preserved, both with respect to the support and the smoothness of the target function (see, e.g., [17, <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>]). However, neural networks can do better!</p>



<p class="justify-text"><strong>Adaptive to linear substructures for one hidden layer.</strong> Given that the training of neural networks involves finding the best possible input weights, which are involved in the first linear layer, it is no surprise that we obtain adaptivity to linear latent variables. That is, if we can write \(f^\ast(x) = g(c_1^\top x, \dots, c_r^\top x)\) for some function \(g: \mathbb{R}^r \to \mathbb{R}\), then the vectors \(c_j\)’s will be essentially estimated among the \(b_j\)’s by the optimization algorithm. Therefore, when using the \(\ell_1\)-based function space, the convergence rate in the excess risk will depend on \(r\) and not \(d\) in the exponent [<a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">10</a>, <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>]. In particular, neural networks can perform <em>non-linear</em> variable selection (while the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> only performs <em>linear</em> variable selection). See a nice experiment in [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>] for binary classification.</p>



<p class="justify-text">One could imagine that with more hidden layers, this extends to non-linear smooth projections of the data, that is, to cases where we assume that \(f^\ast(x) = g(h(x))\) where \(h: \mathbb{R}^d \to \mathbb{R}^r\) is a smooth function. </p>



<p class="justify-text">Thus, we obtain a stronger adaptivity for infinite \(m\) and the good choice of regularization parameter \(\lambda\). We could then try to reproduce for neural networks the figures obtained for kernel methods with varying \(\lambda\). Unfortunately, this is where non-convex optimization will make everything harder.</p>



<h2>Overfitting with a single hidden layer is hard!</h2>



<p class="justify-text"><strong>Global convergence of gradient flow for infinite width. </strong>The traditional algorithm to minimize the empirical risk is gradient descent and its stochastic extensions. In this blog post, we consider gradient descent with small step-sizes, which can be approximated by a gradient flow (as explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>). All parameters are randomly initialized, and we consider \(b_j\) uniformly distributed on the sphere of radius \(1/\sqrt{m}\), and \(a_j\) uniformly distributed in \(\{-1/\sqrt{m},1/\sqrt{m}\}\) (this is essentially equivalent to the traditional “Glorot” initialization [<a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">18</a>]).</p>



<p class="justify-text">This corresponds to the “mean-field” scaling of initialization where neurons from both layers move (see more details in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last </a><a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/">July</a><a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/"> blog post</a> for other scalings). As shown in a joint work with Lénaïc Chizat [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>] and explained in <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">last June blog post</a>, when \(m\) tends to infinity, then the gradient flow can only converge to the global minimum of the objective function, which is a non-trivial result because the cost function is not convex in \((a,b)\). </p>



<p class="justify-text"><strong>Apparent good properties for small \(m\).</strong> A key property which is not yet well understood is that the global convergence behavior can be observed for \(m\) relatively small. For example, considering the one-dimensional regression example below, where the target function is the combination of 5 hidden neurons, with \(m=32\) hidden neurons, it is already possible to learn a good function with high probability (this would not be the case with \(m=5\)).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="319" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_triangular_noisy_32.png" class="wp-image-6048" height="292" />One-dimensional regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">One extra benefit here is that no regularization (e.g., no penalty) seems needed to obtain this good behavior. Therefore it seems that we get the best of both worlds: reasonably small \(m\) (so not too costly algorithmically), and no need to regularize.</p>



<p class="justify-text">However, like in any situation where overfitting is not observed, there is the possibility of <em>underfitting</em>. To study this, let us consider a noiseless problem where \(y\) is a deterministic function of \(x\), and with a sligthly more complicated target function, which now requires 9 hidden neurons.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="327" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/regular_flow_hard_noiseless_8192.png" class="wp-image-6077" height="277" />One-dimensional noiseless regression from \(n = 64\) observations, by minimizing with gradient descent (and small step-size) the unregularized empirical risk.</figure></div>



<p class="justify-text">Even with \(m = 8192\) hidden neurons, the resulting function is not able to fit the data, which is one extreme form of simplicity bias [<a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">21</a>]. There is thus strong underfitting with a single hidden layer (the situation seems to be different with deeper networks). How can this be alleviated? </p>



<h2>Kernel learning to the rescue</h2>



<p class="justify-text">A simple way to avoid the underfitting phenomenon above (but potentially get overfitting, see below) is to minimize the cost \(R(a,b)\) by leveraging a convex sub-problem, as done in the glorious days of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a>.</p>



<p class="justify-text">In the cost function $$R(a,b) = \frac{1}{2n} \sum_{i=1}^n \Big( y_i \, – \sum_{j=1}^m a_j ( b_j^\top x_i )_+ \Big) ^2 + \frac{\lambda}{2} \sum_{j=1}^m \Big\{ a_j^2 + \| b_j\|_2^2 \Big\},$$ the problem is convex with respect to all \((a_j)\)’s; moreover, it is a least-squares problems which can be solved in closed form by matrix inversion (with algorithms that are much more robust to ill-conditioning than gradient descent). </p>



<p class="justify-text">Then, if we denote by \(\Phi(b) \in \mathbb{R}^{n \times m}\) the matrix with elements \((b_j^\top x_i)_+\), the cost function can be written as a function of \(a \in \mathbb{R}^m\) as $$R(a,b) = \frac{1}{2n} \| y \, – \Phi(b) a \|_2^2 +\frac{\lambda}{2}\| a\|_2^2 +  \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ It can be minimized in closed form as $$ a = ( \Phi(b)^\top \Phi(b) + n \lambda I)^{-1} \Phi(b)^\top y = \Phi(b)^\top ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y,$$ where the matrix inversion lemma has been used. This leads to: $$S(b) = \inf_{a \in \mathbb{R}^m} R(a,b) = \frac{\lambda}{2} y^\top  ( \Phi(b)\Phi(b)^\top + n \lambda I)^{-1} y + \frac{\lambda}{2} \sum_{j=1}^m \| b_j\|_2^2.$$ The matrix \(\Phi(b)\Phi(b)^\top \in \mathbb{R}^{n \times n}\) is the traditional kernel matrix of the associated non-linear features. The resulting optimization problem is exactly one used in kernel learning [<a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">20</a>].</p>



<p class="justify-text">Computing \(S(b)\) and its gradient with respect to \(b\) can be done in time \(O(n^2 m)\) when \(m\) is large. Since given the hidden neurons, we get the global optimum, we avoid the underfitting problem as well as the ill-conditioning issues.</p>



<p class="justify-text">When \(m\) tends to infinity, then the optimization problem fits our analysis in [<a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">19</a>], that is, we can see the limiting flow as a Wasserstein gradient flow that can only converge to the global optimum of the associated cost function although the overall problem is not convex (as opposed to what happens in multiple kernel learning described in this <a href="https://francisbach.com/the-eta-trick-reloaded-multiple-kernel-learning/">older post</a>). While there is currently no further quantitative theoretical evidence that a good optimization behavior can be reached for a smaller \(m\), it empirically solves the underfitting issue above, as shown below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="607" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/two_flows_hard_noiseless_128.png" class="wp-image-6050" height="254" />Comparison of the the two flows. Right: regular gradient flow on both sets of weights (normal training), left: gradient flow on the cost function where the output weights are maximized out.</figure></div>



<p>We can now perform experiments with varying \(\lambda\)’s.</p>



<p class="justify-text"><strong>Experiments. </strong>The two plots below depict the learnt functions for a smooth and non-smooth objective as \(\lambda\) is varied, highlighting the importance of regularization.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="592" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/nn_biplot.gif" class="wp-image-6046" height="261" />Regression problems in one dimension with neural networks (left: smooth target, right: non-smooth target), with \(n=32\) observations, with varying regularization parameters, with the optimal one shown below.</figure></div>



<p class="justify-text">We can now compare the convergence rates for the excess risk (expected squared distance beween \(f\) and \(f^\ast\)). We can see that the rates are better for smooth functions, and with the proper choice of regularization parameter, neural networks adapt to it. For adaptivity to linear latent variables, see [<a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">13</a>].</p>



<figure class="wp-block-image size-full justify-text"><img width="1833" alt="" src="https://francisbach.com/wp-content/uploads/2021/06/simulations_adaptivity_nn_subplots.png" class="wp-image-6043" height="480" />Left: estimation with the optimal regularization parameter for smooth target function and neural networks. Middle: estimation with a non-smooth target function. Right: convergence rates when the number \(n\) of observations increases, averaged over 32 replications.</figure>



<h2>Conclusion</h2>



<p class="justify-text">As I tried to show in this blog post, adaptivity is a key driver of good predictive performance of learning methods. It turns out that for the three classes of methods I considered, the more complex algorithms led to more adaptivity: no optimization for local averaging, finite-dimensional convex optimization for kernel methods and then non-convex optimization for neural networks (or equivalently infinite-dimensional convex optimization).</p>



<p class="justify-text">While neural networks led to more adaptivity, they are still not totally well understood, in particular in terms of the various biases that their training implies, both for shallow and deep networks. This makes a lot of research directions to follow!</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Lénaïc Chizat and Lawrence Stewart for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Luc Devroye, László Györfi, and Gábor Lugosi. <em><a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">A Probabilistic Theory of Pattern Recognition</a></em>,<br />volume 31. Springer Science &amp; Business Media, 1996.<br />[2] László Györfi, Michael Kohler, Adam Krzyżak, Harro Walk<em>.</em> <em><a href="https://web.stanford.edu/class/ee378a/books/book1.pdf">A Distribution-free Theory of Nonparametric Regression</a></em>. New York : Springer, 2002.<br />[3] Samory Kpotufe.<a href="http://www.columbia.edu/~skk2175/Papers/kNNRegressionLocRatesFullVersion.pdf"> k-NN regression adapts to local intrinsic dimension</a>. In Advances in Neural Information Processing Systems, 2011.<br />[4] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. <em><a href="https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1">Foundations of Machine Learning</a></em>. MIT Press, 2018.<br />[5] Shai Shalev-Shwartz and Shai Ben-David. <em><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a></em>. Cambridge University Press, 2014.<br />[6] Bernhard Schölkopf and Alexander J. Smola. <em>Learning with Kernels</em>. MIT Press, 2001.<br />[7] Radford M. Neal. <a href="https://www.cs.toronto.edu/~radford/ftp/thesis.ps">Bayesian Learning for Neural Networks</a>. PhD thesis, University of Toronto, 1995.<br />[8] Ali Rahimi and Benjamin Recht. <a href="https://papers.nips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf">Random features for large-scale kernel machines</a>. In Advances in Neural Information Processing Systems, 2008.<br />[9] Youngmin Cho and Lawrence K. Saul. <a href="https://papers.nips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">Kernel methods for deep learning</a>. In Advances in Neural Information Processing Systems, 2009.<br />[10] Francis Bach. <a href="https://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks</a>. The Journal of Machine Learning Research, 18(1):629–681, 2017.<br />[11] Andreas Christmann, Ingo Steinwart. Support Vector Machines. Springer, 2008.<br />[12] Thomas Hamm, Ingo Steinwart. <a href="https://arxiv.org/pdf/2003.06202.pdf">Adaptive Learning Rates for Support Vector Machines Working on Data with Low Intrinsic Dimension</a>. Technical report, Arxiv 2003.06202, 2021.<br />[13] Lénaïc Chizat, Francis Bach. <a href="http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a>. <em>Proceedings of the Conference on Learning Theory (COLT)</em>, 2020.<br />[14] Francis Bach. <a href="http://jmlr.org/papers/volume18/15-178/15-178.pdf">On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions</a>. <em>Journal of Machine Learning Research</em>, 18(19):1-38, 2017. <br />[15] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. <a href="https://papers.nips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf">Less is more: Nyström computational regularization</a>. In Advances in Neural Information Processing Systems, pages 1657–1665, 2015.<br />[16] Alessandro Rudi, Lorenzo Rosasco. <a href="https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf">Generalization properties of learning with random features</a>. In Advances in Neural Information Processing Systems, 2017.<br />[17] Ryumei Nakada, Masaaki Imaizumi. <a href="https://jmlr.org/papers/volume21/20-002/20-002.pdf">Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality</a>. <em>Journal of Machine Learning Research</em>, 21(174):1−38, 2020.<br />[18] Xavier Glorot, Yoshua Bengio. <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, 2010.<br />[19] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a>. <em>Advances in Neural Information Processing Systems</em>, 2018<br />[20] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, Sayan Mukherjee. <a href="https://link.springer.com/content/pdf/10.1023/a:1012450327387.pdf">Choosing multiple parameters for support vector machines</a>. <em>Machine Learning</em>, 46(1):131-159, 2002.<br />[21] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli. <a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">The Pitfalls of Simplicity Bias in Neural Networks</a>. In Advances in Neural Information Processing Systems, 2020.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/quest-for-adaptivity/"><span class="datestr">at June 17, 2021 04:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=857">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/">Data-structure lower bounds without encoding arguments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>                        </p>
<p style="text-align: justify;">I have recently posted the paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#Xviola-rank-samp">Vio21</a>]</span> (<a href="https://eccc.weizmann.ac.il/report/2021/073/">download</a>) which does something that I have been trying to do for a long time, more than ten years, on and off. Consider the basic data-structure problem of storing <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m" class="latex" /> bits of data <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bm%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="x\in \{0,1\}^{m}" class="latex" /> into <img src="https://s0.wp.com/latex.php?latex=m%2Br&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m+r" class="latex" /> bits so that the <em>prefix-sum queries</em></p>
<div style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+%7B%5Ctext+%7B%5Ctextsc+%7BRank%7D%7D%7D%28i%29%3A%3D%5Csum+_%7Bj%5Cle+i%7Dx_%7Bj%7D+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb {\text {\textsc {Rank}}}(i):=\sum _{j\le i}x_{j} \end{aligned}" class="latex" /></div>
<p style="text-align: justify;">   can be computed by probing <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> <em>cells</em> (or <em>words</em>) of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w" class="latex" /> bits each. (You can think <img src="https://s0.wp.com/latex.php?latex=w%3D%5Clog+m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w=\log m" class="latex" /> throughout this post.) The paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> with Pǎtraşcu shows that <img src="https://s0.wp.com/latex.php?latex=r%5Cge+m%2Fw%5E%7BO%28q%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r\ge m/w^{O(q)}" class="latex" />, and this was recently shown to be tight by Yu <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/conf/stoc/Yu19">Yu19</a>]</span> (building on the breakthrough data structure <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascu08Succincter">Pǎt08</a>]</span> which motivated the lower bound and is not far from it).</p>
<p style="text-align: justify;">   As is common in data-structure lower bounds, the proof in <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> is an <em>encoding argument</em>. In the recently posted paper, an alternative proof is presented which avoids the encoding argument and is perhaps more in line with other proofs in complexity lower bounds. Of course, <em>everything</em> is an encoding argument, and <em>nothing </em>is an encoding argument, and this post won’t draw a line.</p>
<p style="text-align: justify;">   The new proof establishes an <em>intrinsic property</em> of efficient data structures, whereas typical proofs including <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> are somewhat tailored to the problem at hand. The property is called the <em>separator</em> and is a main technical contribution of the work. At the high level the separator shows that in any efficient data structure you can restrict the input space a little so that many queries are nearly <em>pairwise independent</em>.</p>
<p style="text-align: justify;">   Also, the new proof rules out a stronger object: a <em>sampler</em> (<a href="https://emanueleviola.wordpress.com/2014/11/09/is-nature-a-low-complexity-sampler/">see previous post here</a> on sampling lower bounds). Specifically, the distribution Rank<img src="https://s0.wp.com/latex.php?latex=%28U%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="(U)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="U" class="latex" /> is the uniform distribution cannot be sampled, not even slightly close, by an efficient cell-probe algorithm. This implies the data-structure result, and it can be informally interpreted as saying that the “reason” why the lower bound holds is not that the data is compressed, but rather that one can’t generate the type of dependencies occurring in Rank via an efficient cell-probe algorithm, regardless of what the input is.</p>
<p style="text-align: justify;">   Building on this machinery, one can prove several results about sampling, like showing that cell-probe samplers are strictly weaker than AC0 samplers. While doing this, it occurred to me that one gets a corollary for data structures which I had not seen in the literature. The corollary is a <em>probe hierarchy</em>, showing that some problem can be solved with zero redundancy (<img src="https://s0.wp.com/latex.php?latex=r%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r=0" class="latex" />) with <img src="https://s0.wp.com/latex.php?latex=O%28q%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="O(q)" class="latex" /> probes, while it requires almost linear <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> probes. For example I don’t know of a result yielding this for small <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> such as <img src="https://s0.wp.com/latex.php?latex=q%3DO%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q=O(1)" class="latex" />; I would appreciate a reference. (As mentioned in the                                                                                                                                                        paper, the sampling viewpoint is not essential and just like for Rank one can prove the data-structure corollaries directly. Personally, and obviously, I find the sampling viewpoint useful.)</p>
<p style="text-align: justify;">   One of my favorite open problems in the area still is: can a uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[m]" class="latex" /> be approximately sampled by an efficient cell-probe algorithm? I can’t even rule out samplers making <em>two </em>probes!</p>
<h3 class="likesectionHead"><a id="x1-1000"></a>References</h3>
<p style="text-align: justify;">
</p><div class="thebibliography">
<p class="bibitem"><span class="biblabel">  [Pǎt08]<span class="bibsp">   </span></span><a id="XPatrascu08Succincter"></a>Mihai  Pǎtraşcu.      Succincter.      In  49th  IEEE  Symp. on         Foundations of Computer Science (FOCS). IEEE, 2008.</p>
<p class="bibitem"><span class="biblabel">  [PV10]<span class="bibsp">   </span></span><a id="XPatrascuV10"></a>Mihai Pǎtraşcu and Emanuele Viola.  Cell-probe lower bounds         for succinct partial sums.  In 21th ACM-SIAM Symp. on Discrete         Algorithms (SODA), pages 117–122, 2010.</p>
<p class="bibitem"><span class="biblabel">  [Vio21]<span class="bibsp">   </span></span><a id="Xviola-rank-samp"></a>Emanuele       Viola.                    Lower       bounds       for         samplers and data structures via the cell-probe separator. Available         at <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>, 2021.</p>
<p class="bibitem"><span class="biblabel">  [Yu19] <span class="bibsp">   </span></span><a id="XDBLP:conf/stoc/Yu19"></a>Huacheng  Yu.     Optimal  succinct  rank  data  structure  via         approximate nonnegative tensor decomposition.  In Moses Charikar         and Edith Cohen, editors, ACM Symp. on the Theory of Computing         (STOC), pages 955–966. ACM, 2019.</p>
</div></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/"><span class="datestr">at June 07, 2021 03:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">Workshop on Machine Learning for Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 13-14, 2021 Foundations of Data Science Institute (FODSI) https://fodsi.us/ml4a.html In recent years there has been increasing interest in using machine learning to improve the performance of classical algorithms in computer science, by fine-tuning their behavior to adapt to the properties of the input distribution. This “data-driven” or “learning-based” approach to algorithm design has the … <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">Workshop on Machine Learning for Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/"><span class="datestr">at June 07, 2021 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/">TCS+ talk: Wednesday, June 9 — Pravesh Kothari (CMU) and Ankur Moitra (MIT)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, June 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC).<strong> <a href="https://www.cs.cmu.edu/~praveshk/">Pravesh Kothari</a>  and <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a> </strong>from CMU and MIT will (jointly) speak about “<em>Robustly Learning Mixtures of Gaussians</em>” (abstract below).</p>
<p>Note that the seminar will be a bit longer than the usual: it’s a double feature!</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: For a while now the problem of robustly learning a high-dimensional mixture of Gaussians has had a target on its back. The first works in algorithmic robust statistics gave provably robust algorithms for learning a single Gaussian. Since then there has been steady progress, including algorithms for robustly learning mixtures of spherical Gaussians, mixtures of Gaussians under separation conditions, and arbitrary mixtures of two Gaussians. In this talk we will discuss two recent works that essentially resolve the general problem. There are important differences in their techniques, setup, and overall quantitative guarantees, which we will discuss.</p>
<p>The talk will cover the following independent works:</p>
<ul>
<li>Liu, Moitra, “Settling the Robust Learnability of Mixtures of Gaussians”</li>
<li>Bakshi, Diakonikolas, Jia, Kane, Kothari, Vempala, “Robustly Learning Mixtures of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" /> Arbitrary Gaussians”</li>
</ul>
</blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/"><span class="datestr">at June 03, 2021 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/forc-2021-is-coming-up/">FORC 2021 is coming up!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The Second annual Symposium on the Foundations of Responsible Computing (FORC) will be held virtually (on Gather.town) June 9-11, 2021.<a href="https://docs.google.com/forms/d/e/1FAIpQLSfELj0MvgcI83dmYThUPiynqWj8-G9Xve2dVf84EqKzgXXsHQ/viewform" target="_blank" rel="noreferrer noopener"> Registration is free, but required, by June 7</a>. Instructions for joining the event will be emailed to registered participants and posted online on June 8.</p>



<p>The program will feature 24 papers, six exciting panel discussions, three social hours featuring interactive board games, keynotes by Julie Owono (of the Facebook Oversight Board) and Kate Crawford (on her new book, the Atlas of AI), and a mentoring meetup.</p>



<p>The full program is here: <a href="https://responsiblecomputing.org/forc-2021-program/" target="_blank" rel="noreferrer noopener">https://responsiblecomputing.org/forc-2021-program/</a></p>



<p>The Symposium on Foundations of Responsible Computing (FORC) is a forum for mathematical research in computation and society writ large. The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>



<figure class="wp-block-image size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/white-logo-no-background.png?resize=800%2C858&amp;ssl=1" class="wp-image-1770" height="858" /></figure></div>







<p class="date">
by galoosh33 <a href="https://toc4fairness.org/forc-2021-is-coming-up/"><span class="datestr">at June 01, 2021 06:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2235">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/05/26/few-lessons-from-the-history-of-multiparty-computation/">A few lessons from the history of multiparty computation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>As part of my Ph.D. qualifying exam, I gave a survey talk on some of the recent progress in practical multi-party computation (MPC). After the talk, Omer Reingold asked me why so much progress on MPC happened in the last decade or so, given that the main theoretical groundwork for multiparty computation was laid out in the 1980s.  Omer’s question got me interested, so I looked at some MPC papers from over the years and talked to a few people who have been working on MPC for some time now. In this post, I want to share some of what I’ve learned about the history of the progress of MPC, a few answers to Omer’s question, and potentially a few takeaways on progress in science more broadly.</p>



<p>To give a bit of background, MPC is one of the most fundamental problems in cryptography. In particular, MPC protocols allow a set of parties, each of which holds its private input, to compute a joint function over their inputs, in a way that protects the confidentiality and integrity of the computation from an adversary that controls a subset of the parties and the communication channels. Defining this precisely requires a lot of care, yet for now, you can think of an MPC protocol as being secure, if the adversary can only cause as much damage as it can in an “ideal world” in which the computation is performed with the aid of a trusted incorruptible party. Intuitively, in such an ideal world, the adversary cannot do much more than choose its inputs, modify its output, or choose not to participate in the computation at all. </p>



<p>In the 1980s, a <a href="https://ieeexplore.ieee.org/abstract/document/4568207">sequence</a> <a href="https://dl.acm.org/doi/10.1145/28395.28420">of</a> <a href="https://dl.acm.org/doi/10.1145/62212.62213">works</a> <a href="https://dl.acm.org/doi/10.1145/62212.62214">initiated</a> the study of multiparty computation and established very powerful and general results, showing the feasibility of constructing protocols for securely computing any multi-party functionality in a variety of settings. Those results were mostly theoretical. In contrast, the last 15 years saw a growing interest in MPC from a more practical perspective. To put this rising interest in context, consider the following two graphs (data for the graphs is <a href="https://docs.google.com/spreadsheets/d/1IzfKDRqv6C9hSeD9TyosvCl9fknN2V3Ja4tQjy54y0A/edit?usp=sharing">here</a>).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="620" alt="" src="https://theorydish.files.wordpress.com/2021/05/comptime.png?w=620" class="wp-image-2245" height="372" /></figure></div>



<p>This graph shows the time of a two-party secure computation of the AES block cipher over the years. That is the time it takes to evaluate AES(k,x), where one party holds a 128-bit key k to the block cipher, the other party holds a 256-bit data block x, and the output is 256-bit long. The measurements are from the original papers. There are different variants of this “benchmark”, in terms of the number of parties (2PC/MPC), the number of corrupted parties (honest/dishonest majority), precise security model (semi-honest/fully malicious), hardware, network (LAN/WAN), and cost model (single/batch execution), but, for concreteness, the graph only includes results in the batch-evaluation setting in the fully malicious two-party setting. </p>



<p class="has-text-align-center"><img width="624" src="https://lh4.googleusercontent.com/lfa7-L5D3d5U8ZCHbAmUoqRH-RVaUmpFfSiFhBl7yF38ieoUmdo-LL0xH-gzXEaPauRXkCUmOYWDovfJvE792a0ikVxojM6g4P7EFD-mg4A2XbvzJWAslBug8UE6W4CnRhtIt4el" height="375" /></p>



<p>The second graph shows the number of papers on two-party computation, over the years. This is not a very well-defined characterization, but for simplicity, I looked at all the papers referenced from the relevant chapters in the <a href="https://securecomputation.org/">book on MPC</a> by Evans, Kolesnikov, and Rosulek. Admittedly, these are not all the papers in the area, there might be valuable indirect contributions that are missing, and, more importantly, the number of papers is a bit of a superficial metric. Yet, even with these caveats, I think this is helpful to get a general picture. On top of the flurry of academic research, MPC has also been thriving in the industry, with companies like <a href="https://www.curv.co/">Curv</a>, <a href="https://partisia.com/">Partisia</a>, <a href="https://sharemind.cyber.ee/">Sharemind</a>, <a href="https://www.unboundsecurity.com/">Unbound</a>, and others building MPC-based products and services.</p>



<p>The first graph shows quite impressive progress. In less than a decade, the time to securely compute AES in the two-party setting decreased by more than 60,000x, from more than 10 minutes to less than 10 milliseconds. (For reference, in the same period, CPU speeds increased <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-975+%40+3.33GHz&amp;id=841">roughly</a> <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-7740X+%40+4.30GHz&amp;id=3041">twofold</a>, and typical local-area network speeds increased from 1GB/S to 10GB/S.) The second graph illustrates the increase in the volume of research, which has enabled this progress. This includes advances such as <a href="https://dl.acm.org/doi/10.1145/100216.100287">various</a> <a href="https://dl.acm.org/doi/10.1145/336992.337028">increasingly</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-70583-3_40">efficient</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-46803-6_8">garbling</a> <a href="https://dl.acm.org/doi/10.1145/3133956.3134053">techniques</a>, <a href="https://link.springer.com/chapter/10.1007%2F11745853_30">more</a> <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.2627&amp;rep=rep1&amp;type=pdf">efficient</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-72540-4_4">cut</a>–<a href="https://link.springer.com/chapter/10.1007/978-3-642-20465-4_22">and</a>–<a href="https://link.springer.com/chapter/10.1007/978-3-642-19571-6_20">choose</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-40084-1_3">techniques</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-00457-5_22">for</a> <a href="https://dl.acm.org/doi/10.1145/2508859.2516698">malicious security</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-642-20465-4_11">protocols</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-32009-5_38">based on</a> <a href="https://link.springer.com/chapter/10.1007/978-3-642-32009-5_40">information-theoretic</a> <a href="https://dl.acm.org/doi/10.1145/2976749.2978357">message-authentication</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-78372-7_6">codes</a>, <a href="https://crypto.stanford.edu/craig/craig-thesis.pdf">homomorphic</a> <a href="https://dl.acm.org/doi/10.1145/2090236.2090262">encryption</a>, and many others. (See the EKR book for an overview of these techniques and more references.) The second graph also clearly shows the inflection point that happened about 15 years ago. This inflection point illustrates the premise of the question that has triggered this blog post.</p>



<p>I use the term inflection point, since progress in MPC never really stopped between the late 80s and the mid-2000s, and there was a lot of research in the interim period. For example, <a href="https://eprint.iacr.org/2005/187.pdf">oblivious</a> <a href="https://dl.acm.org/doi/10.1145/1008908.1008920">transfer</a>–one of the fundamental primitives used <a href="https://dl.acm.org/doi/10.1145/3812.3818">to</a> <a href="https://link.springer.com/content/pdf/10.1007%2F3-540-47721-7_17.pdf">build</a> <a href="https://dl.acm.org/doi/10.1145/62212.62215">MPC</a>–<a href="https://link.springer.com/chapter/10.1007/3-540-44750-4_9">received</a> <a href="https://dl.acm.org/doi/abs/10.1145/237814.237996">a</a> <a href="https://link.springer.com/article/10.1007/BF00208002">lot</a> <a href="https://link.springer.com/chapter/10.1007/BFb0054139">of</a> <a href="https://dl.acm.org/doi/10.1145/301250.301312">attention</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44987-6_8">and</a> <a href="https://dl.acm.org/doi/abs/10.5555/365411.365502">saw</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-45146-4_9">significant</a> <a href="https://link.springer.com/chapter/10.1007/978-3-540-40061-5_27">progress</a>. There <a href="https://link.springer.com/article/10.1007/BF02252866">have</a> <a href="https://link.springer.com/chapter/10.1007/3-540-38424-3_6">been</a> <a href="https://link.springer.com/article/10.1007/BF00196771">many</a> <a href="https://link.springer.com/chapter/10.1007/3-540-46766-1_32">works</a> <a href="https://dl.acm.org/doi/10.1145/167088.167109">developing</a> <a href="https://dl.acm.org/doi/abs/10.1145/195058.195408">the</a> <a href="https://dl.acm.org/doi/10.1145/259380.259412">theoretical</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44598-6_5">framework</a> <a href="https://dl.acm.org/doi/10.1145/352600.352639">of</a> <a href="https://link.springer.com/chapter/10.1007/3-540-44448-3_13">secure</a> <a href="https://dl.acm.org/doi/abs/10.1145/380752.380855">computation</a>, studying <a href="https://eprint.iacr.org/2000/067">secure composition</a>, and <a href="https://dl.acm.org/doi/10.1145/72981.72995">reducing</a> <a href="https://dl.acm.org/doi/10.1145/100216.100287">the</a> <a href="https://link.springer.com/chapter/10.1007/3-540-38424-3_5">round</a> <a href="https://ieeexplore.ieee.org/document/595170">complexity</a> <a href="https://ieeexplore.ieee.org/document/814630">of</a> <a href="https://link.springer.com/chapter/10.1007/3-540-45022-X_43">secure</a> <a href="https://link.springer.com/chapter/10.1007/3-540-45539-6_23">multiparty</a> <a href="https://ieeexplore.ieee.org/abstract/document/892118">computation</a> <a href="https://dl.acm.org/doi/10.5555/646766.704286">protocols</a>. Other works developed special-purpose secure computation protocols for concrete applications such as <a href="https://ieeexplore.ieee.org/document/502223">secure auctions</a> and  <a href="https://dl.acm.org/doi/10.1145/293347.293350">private information retrieval</a>. Many of the techniques and ideas in these works were instrumental for the more recent progress.</p>



<p>Having said that, there still seems to have been a dip in the attention devoted to MPC for a certain period. One explanation that I’ve found interesting is that the powerful feasibility results from the early days made the problem seem “solved” from a theoretical point of view. As a result, interest in MPC within the theory community decreased. It took time before the interest ramped up again, this time more within the more-practically oriented crypto community. (Interestingly, since then, MPC found renewed interest in the theoretical cryptography community as well.)</p>



<p>One work that, to me, is a key point in the transition of MPC from theory to practice is the 2004 <a href="https://www.usenix.org/conference/13th-usenix-security-symposium/fairplay%E2%80%94-secure-two-party-computation-system">Fairplay paper</a>. This was the first full-fledged system that implemented generic secure function evaluation. I believe that, together with other early implementations of MPC-based systems (such as <a href="https://link.springer.com/chapter/10.1007/978-3-642-03549-4_20">the system</a> developed for the Danish sugar-beet auction and the <a href="https://link.springer.com/chapter/10.1007/978-3-540-88313-5_13">Sharemind system</a>), it played a pivotal role in accelerating practical research. I view it as a lesson on the value of “first systems”: they help to identify bottlenecks and discover new concerns, which are hard to predict without building an actual system. Moreover, such systems put “theory problems” on the radar of practitioners and demonstrate which ideas are closer to being practical.  In some sense, a system is an evidence that “the constants in the big-O have been worked out…and they are not astronomical.” Furthermore, when code is made public, follow-up works can build upon an initial system and improve on it more rapidly. A first system can mark the transition of an idea from pure theory to practice. It’s still fair to ask what determines the timing of the appearance of such a first system. One answer is that it often takes the right type of collaboration of researchers from different research communities, as seems to have been the case with “Fairplay”. </p>



<p>I think that external factors also played a role in the timing of events. Specifically, the growth of the Internet and the demand for security on the Internet stimulated a lot of interest and progress in applied cryptography. The late 1990s saw large-scale adoption of public-key cryptography with the successful development and deployment of SSL. More complex Internet applications began to appear later, including those that are inherently security-sensitive, like e-commerce. These applications stimulated interest in more advanced cryptographic tools, MPC being one of them. Indeed, MPC papers from that period discuss new Internet-driven applications as part of their motivation. </p>



<p>A final factor that I want to mention ties back to the first graph above. The fact that many works demonstrate their performance by measuring AES evaluation is, to me, an example of the value of standardized benchmarks. Such benchmarks help to evaluate new techniques and ideas within a certain area and can help progress on a particular dimension, partly because of the competitive element they add. As with any measurement, there is a risk of overfitting to the benchmark, rather than focusing on the true goal, but I think they are very helpful overall.</p>



<p>I am sure there are other valuable lessons to be learned from this case study. Moreover, I am quite certain that folks with more experience in the area have a much better perspective on the history of these developments than I do. I will be very happy to hear other perspectives!</p>



<p><strong>Acknowledgements:</strong> I would like to thank my quals committee, Dan Boneh, Omer Reingold, and Mary Wootters, for an interesting discussion that has led to this blog post. I am grateful to Yuval Ishai and Benny Pinkas for interesting conversations on the progress of MPC and to Henry Corrigan-Gibbs and Saba Eskandarian for providing me with helpful comments.</p></div>







<p class="date">
by Dima Kogan <a href="https://theorydish.blog/2021/05/26/few-lessons-from-the-history-of-multiparty-computation/"><span class="datestr">at May 27, 2021 01:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/05/21/international-school-conference-in-algorithms-combinatorics-and-complexity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/05/21/international-school-conference-in-algorithms-combinatorics-and-complexity/">International School-conference in Algorithms, Combinatorics, and Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 24-28, 2021 Online https://indico.eimi.ru/event/199/ An advanced school for young researchers featuring three minicourses in vibrant areas of mathematics and computer science. The target audience includes graduate, master, and senior bachelor students of any mathematical speciality.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/05/21/international-school-conference-in-algorithms-combinatorics-and-complexity/"><span class="datestr">at May 21, 2021 09:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=562">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/05/20/tcs-talk-wednesday-may-26-kira-goldner-columbia-university/">TCS+ talk: Wednesday, May 26 — Kira Goldner, Columbia University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://www.kiragoldner.com/"><strong>Kira Goldner</strong></a> from Columbia University will speak about “<em>An Overview of Using Mechanism Design for Social Good</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In order to accurately predict an algorithm’s outcome and quality when it interacts with participants who have a stake in the outcome, we must design it to be robust to strategic manipulation. This is the subject of algorithmic mechanism design, which borrows ideas from game theory and economics to design robust algorithms. In this talk, I will show how results from the theoretical foundations of algorithmic mechanism design can be used to solve problems of societal concern.</p>
<p>I will overview recent work in this area in many different applications — housing, labor markets, carbon license allocations, health insurance markets, and more — as well as discuss open problems and directions ripe for tools from both mechanism design and general TCS.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/05/20/tcs-talk-wednesday-may-26-kira-goldner-columbia-university/"><span class="datestr">at May 20, 2021 09:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1883">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/">Entropy Estimation via Two Chains: Streamlining the Proof of the Sunflower Lemma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://en.wikipedia.org/wiki/Sunflower_(mathematics)">sunflower lemma</a> describes an interesting combinatorial property of set families: any large family of small sets must contain a large <em>sunflower</em>—a sub-family consisting of sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_r" class="latex" /> with a shared <em>core</em> <img src="https://s0.wp.com/latex.php?latex=S%3A%3DA_1%5Ccap%5Ccdots%5Ccap+A_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S:=A_1\cap\cdots\cap A_r" class="latex" /> and disjoint <em>petals</em> <img src="https://s0.wp.com/latex.php?latex=A_1%5Cbackslash+S%2C%5Cldots%2CA_r%5Cbackslash+S.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1\backslash S,\ldots,A_r\backslash S." class="latex" /> (See the <a href="https://theorydish.blog/feed/#fig-sunflower">figure</a> below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential <a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">paper</a> by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see <a href="https://dl.acm.org/doi/abs/10.1145/3357713.3384234">STOC version</a> of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science).</p>



<div class="wp-block-image" id="fig-sunflower"><figure class="aligncenter size-full is-resized"><img width="474" alt="" src="https://theorydish.files.wordpress.com/2021/04/sunflower_example-1.png" class="wp-image-2195" height="207" />Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower.</figure></div>



<p>The lemma was first proved by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=111692">Erdős-Rado</a> in 1960, who gave the quantitative bound that among <img src="https://s0.wp.com/latex.php?latex=%28r-1%29%5Ekk%21%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(r-1)^kk!+1" class="latex" /> distinct sets each of size at most <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> one can find <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r" class="latex" /> sets forming a sunflower. Erdos-Rado conjectured that the <img src="https://s0.wp.com/latex.php?latex=k%21%3Dk%5E%7Bk%281+-+o%281%29%29%7D+%3D+k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k!=k^{k(1 - o(1))} = k^{\Omega(k)}" class="latex" /> in the bound could be significantly improved to <img src="https://s0.wp.com/latex.php?latex=O%281%29%5Ek.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(1)^k." class="latex" /> For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the <img src="https://s0.wp.com/latex.php?latex=k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^{\Omega(k)}" class="latex" /> dependence on <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> even for <img src="https://s0.wp.com/latex.php?latex=r+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r = 3" class="latex" /> despite much research effort. In 2019, a breakthrough work by <a href="https://arxiv.org/abs/1908.08483">Alweiss-Lovett-Wu-Zhang</a> improved the bound significantly to <img src="https://s0.wp.com/latex.php?latex=%28Cr%5E3%5Clog+k%5Clog%5Clog_2+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr^3\log k\log\log_2 k)^k" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> being at least <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="3" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="C" class="latex" /> being an absolute constant. The breakthrough led to many new results including improved monotone circuit lower bounds by <a href="https://arxiv.org/abs/2012.03883">Cavalar-Kumar-Rossman</a>.</p>



<p>Since the breakthrough of Alweiss-Lovett-Wu-Zhang, researchers have been refining the bound and simplifying the proof. The current best bound is <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log k)^k" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=k+%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k \ge 2" class="latex" /> proved by Bell-Chueluecha-Warnke via a minor but powerful twist in the proof of an earlier <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+%28rk%29%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log (rk))^k" class="latex" /> <a href="https://arxiv.org/abs/1909.04774">bound</a> by Anup Rao:</p>



<p id="thm-1"><strong>Theorem 1</strong> (Sunflower Lemma <a href="https://arxiv.org/abs/2009.09327">[BCW’20]</a>).<em> There exists a constant <img src="https://s0.wp.com/latex.php?latex=C%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="C&gt;0" class="latex" /> such that the following holds for all positive integers <img src="https://s0.wp.com/latex.php?latex=k%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k\ge 2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=r.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r." class="latex" /> Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be a family of at least <img src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(Cr\log k)^k" class="latex" /> distinct sets each of size at most <img src="https://s0.wp.com/latex.php?latex=k.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k." class="latex" /> Then <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> contains <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="r" class="latex" /> sets that form a sunflower.</em></p>



<p>The aim of this blog post is to present a streamlined proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a>. The proof is largely based on a <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a> by Terence Tao where he presented an elegant proof of Rao’s result using <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. However, Tao’s proof included a trick of <em>passing to a conditional copy twice</em>, which Tao described as <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/\#comment-579138">“somewhat magical”</a>. <strong>We show here that the trick is not necessary for the proof, and avoiding the trick gives a simpler proof with a slightly better constant in the bound.</strong></p>



<p>We start by defining <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread families, a notion key to all recent proofs of the sunflower lemma. We use <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> as a shorthand for <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2CN%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{1,\ldots,N\}." class="latex" /> We use boldface symbols to denote random variables, and non-boldface ones to denote deterministic quantities.</p>



<p><strong>Definition 1</strong> (Spread family). <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be a family of finite sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N" class="latex" /> that are not necessarily distinct. For <img src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R &gt; 1" class="latex" />, we say <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread if for all <img src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+%5Ccup_%7Bn%3D1%7D%5ENA_n%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S\subseteq \cup_{n=1}^NA_n," class="latex" /></em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CPr%5BS%5Csubseteq+A_%7B%5Cmathbf+n%7D%5D+%5Cle+R%5E%7B-%7CS%7C%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \Pr[S\subseteq A_{\mathbf n}] \le R^{-|S|},\end{aligned}" class="latex" /></em></p>



<p><em>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> is chosen uniformly at random from <img src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]." class="latex" /></em></p>



<p>The main technical part of recent improvements in the sunflower lemma happens in the proof of the following refinement lemma. We use the base-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> logarithm throughout.</p>



<p id="lm-2"><strong>Lemma 2</strong> (Refinement). <em>Let <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> be a finite set. <em>For <img src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R &gt; 1" class="latex" /></em>, let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal F" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread family of sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N%5Csubseteq+X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N\subseteq X." class="latex" /></em> <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> be a size-<img src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta|X|" class="latex" /> subset of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> chosen uniformly at random for <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin%281%2FR%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta\in(1/R,1]" class="latex" /> assuming <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta|X|" class="latex" /> is an integer. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> be a uniform random number in <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> independent of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W." class="latex" /> There is a random number <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]" class="latex" /> (being not independent from <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+n%2C%5Cmathbf+W%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(\mathbf n,\mathbf W)" class="latex" /> in general) such that <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccup%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_{\mathbf n'}\subseteq A_{\mathbf n}\cup\mathbf W" class="latex" /> almost surely and </em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C%5Cle%5Cfrac%7B4%7D%7B%5Clog%28R%5Cdelta%29%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%7D%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}\mathbb E|A_{\mathbf n'}\backslash \mathbf W|\le\frac{4}{\log(R\delta)}\mathbb E|A_{\mathbf n}|.\end{aligned}" class="latex" /></em></p>



<p>The proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> using Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> can be found in Rao’s <a href="https://arxiv.org/abs/1909.04774">paper</a> and Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, so we omit it here and focus on proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>. Rao and Tao both proved a slightly weaker version of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> but this weakness can be overcome using a minor twist observed by <a href="https://arxiv.org/abs/2009.09327">Bell-Chueluecha-Warnke</a>. They also used slightly different forms of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> but the differences are non-essential.</p>



<p>It is easy to see that in Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> one can convert <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> to a deterministic function of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> without violating any property of the original <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> guaranteed by the lemma, because after conditioning on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> one can fix any additional randomness in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|A_{\mathbf n'}\backslash \mathbf W|" class="latex" /> is minimized. This more deterministic version of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> can be somewhat more convenient for proving Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> and it was used in Rao’s proof, but allowing additional randomness in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> enabled Tao to obtain a simpler proof of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using a construction of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'" class="latex" /> that is easier to analyze.</p>



<p>We follow Tao’s idea of proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using Shannon entropy, but we present the proof in a more streamlined fashion with a slightly sharper constant in the bound (Tao proved a version of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> where the constant 4 was replaced by 5). Specifically, we present the proof in a way resembling a basic technique in combinatorics called <em><a href="https://en.wikipedia.org/wiki/Double_counting_(proof_technique)">counting in two ways</a></em>: one can show that two quantities are equal by showing that they both count the number of elements in the same set. Here, we estimate the entropy of the same collection of random variables in two different ways, and prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> by comparing the two estimates. The way we obtain the two estimates relies crucially on the chain rule of conditional entropy:</p>



<p class="has-text-align-center" id="eq-1"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29+%3D+%5Csum_%7Bi%3D1%7D%5Em%5Cmathbb+H%28%5Cmathbf+a_i%7C%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_%7Bi-1%7D%29.+%26%26++%281%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb H(\mathbf a_1,\ldots,\mathbf a_m) = \sum_{i=1}^m\mathbb H(\mathbf a_i|\mathbf a_1,\ldots,\mathbf a_{i-1}). &amp;&amp;  (1) \end{aligned}" class="latex" /></p>



<p>Equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> holds for arbitrary random variables <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf a_1,\ldots,\mathbf a_m" class="latex" /> taking values in a discret set. We say that equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> computes the entropy of <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(\mathbf a_1,\ldots,\mathbf a_m)" class="latex" /> via the chain </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbf+a_1%5Crightarrow+%5Ccdots+%5Crightarrow+%5Cmathbf+a_m.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}\mathbf a_1\rightarrow \cdots \rightarrow \mathbf a_m.\end{aligned}" class="latex" /></p>



<p>To prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>, we obtain two entropy estimates for the same collection of random variables by applying <a href="https://theorydish.blog/feed/#eq-1">(1)</a> to two different chains.</p>



<p>We need the following useful lemmas about Shannon entropy. We omit their proofs here as they can be found in Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, where many basic properties of Shannon entropy are also discussed. (We also highly recommend Tao’s other blog posts about <a href="https://terrytao.wordpress.com/2017/03/01/special-cases-of-shannon-entropy/">Shannon entropy</a> and the <a href="https://terrytao.wordpress.com/2009/08/05/mosers-entropy-compression-argument/">entropy compression argument</a>.)</p>



<p id="lm-3"><strong>Lemma 3</strong> (Subsets of small sets have small conditional entropy). <em>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%2C%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A,\mathbf B" class="latex" /> be finite random sets. Assume <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A\subseteq \mathbf B" class="latex" /> almost surely. Then <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%7C%5Cmathbf+B%29%5Cle+%5Cmathbb+E%7C%5Cmathbf+B%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf A|\mathbf B)\le \mathbb E|\mathbf B|." class="latex" /></em></p>



<p id="lm-4"><strong>Lemma 4</strong> (Information-theoretic interpretation of spread). <em>Let <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_1,\ldots,A_N" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="R" class="latex" />-spread family of finite sets. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n" class="latex" /> be chosen uniformly at random from <img src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="[N]." class="latex" /> Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf S" class="latex" /> be a random set satisfying <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S%5Csubseteq+A_%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf S\subseteq A_{\mathbf n}" class="latex" /> almost surely. Then</em></p>



<p class="has-text-align-center"><em><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+n%7C%5Cmathbf+S%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+n%29+-+%28%5Clog+R%29+%5Cmathbb+E%7C%5Cmathbf+S%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb H(\mathbf n|\mathbf S) \le \mathbb H(\mathbf n) - (\log R) \mathbb E|\mathbf S|.\end{aligned}" class="latex" /></em></p>



<p id="lm-5"><strong>Lemma 5</strong> (Information-theoretic properties of uniformly random subsets of fixed size). <em>Let <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> be a finite set. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf W" class="latex" /> be a size-<img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta |X|" class="latex" /> subset of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> chosen uniformly at random for <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin+%280%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta\in (0,1]" class="latex" /> assuming <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta |X|" class="latex" /> is an integer. Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A" class="latex" /> be a random subset of <img src="https://s0.wp.com/latex.php?latex=X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X." class="latex" /> </em>The following inequalities hold:</p>



<ol><li><em>(absorption) <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%5Ccup+%5Cmathbf+W%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+%2B+1+%2B+%281+%2B+%5Clog%281%2F%5Cdelta%29%29%5Cmathbb+E%7C%5Cmathbf+A%7C%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf A\cup \mathbf W) \le \mathbb H(\mathbf W) + 1 + (1 + \log(1/\delta))\mathbb E|\mathbf A|;" class="latex" /></em></li><li><em>(spread) if <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf A\subseteq \mathbf W" class="latex" /> almost surely, then <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+W%7C%5Cmathbf+A%29%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+-+%5Clog%281%2F%5Cdelta%29%5Cmathbb+E%7C%5Cmathbf+A%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb H(\mathbf W|\mathbf A)\le \mathbb H(\mathbf W) - \log(1/\delta)\mathbb E|\mathbf A|." class="latex" /></em></li></ol>



<p><em>Proof of Lemma </em><a href="https://theorydish.blog/feed/#lm-2">2</a>. If there exists <img src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n\in[N]" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=A_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_n" class="latex" /> is empty, we can simply choose <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27%3Dn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf n'=n" class="latex" /> deterministically. We assume henceforth that <img src="https://s0.wp.com/latex.php?latex=A_n%5Cneq+%5Cemptyset&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_n\neq \emptyset" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n\in[N]." class="latex" /></p>



<p>Following Tao’s proof, we construct <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}'" class="latex" /> by creating a conditionally independent copy <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> of <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%27+%3D+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\cup {\mathbf W}' = {A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" /> In other words, <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> have the same conditional distribution given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}," class="latex" /> and they are also conditionally independent given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" /> This construction guarantees that <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" /> almost surely, which implies that <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_{\mathbf n'}\backslash \mathbf W\subseteq A_{\mathbf n}\cap A_{\mathbf n'}" class="latex" /> almost surely.</p>



<p>It remains to prove that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}'" class="latex" /> constructed as above satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|." class="latex" /> We achieve this by estimating <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" /> using the chain rule <a href="https://theorydish.blog/feed/#eq-1">(1)</a> via two different chains, one for a lower bound and the other for an upper bound. The lower bound is obtained via the following chain:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbf n},{\mathbf W}\rightarrow {\mathbf n}',{\mathbf W}'. \end{aligned}" class="latex" /></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following way:</p>



<p class="has-text-align-center" id="eq-2"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29.+%26%26+%282%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') = {\mathbb H}({\mathbf n},{\mathbf W}) + {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}). &amp;&amp; (2)\end{aligned}" class="latex" /></p>



<p>By the independence of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf n}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf W}" class="latex" />,</p>



<p class="has-text-align-center" id="eq-3"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29.+%26%26+%283%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W}) = {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}). &amp;&amp; (3)\end{aligned}" class="latex" /></p>



<p>By the conditional independence of <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n}',{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="({\mathbf n},{\mathbf W})" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" /> and their identical conditional distribution,</p>



<p class="has-text-align-center" id="eq-4"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Cmathbb+H%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%5Cge+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%26+%284%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \mathbb H({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}) \\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W},{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ \ge {} &amp; {\mathbb H}({\mathbf n}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|,&amp; (4) \end{aligned}" class="latex" /></p>



<p>where the last inequality is by Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 1. Plugging <a href="https://theorydish.blog/feed/#eq-3">(3)</a>, <a href="https://theorydish.blog/feed/#eq-4">(4)</a> into <a href="https://theorydish.blog/feed/#eq-2">(2)</a>, we get the following lower bound:</p>



<p class="has-text-align-center" id="eq-5"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cge+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%285%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \ge {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|. &amp; (5)\end{aligned}" class="latex" /></p>



<p>Now we establish an upper bound for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" /> via a different chain:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%5Crightarrow+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%5Crightarrow+%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+W%7D%27.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbf n}\rightarrow {A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\rightarrow {\mathbf n}'\rightarrow {\mathbf W}\rightarrow {\mathbf W}'.\end{aligned}" class="latex" /></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following manner:</p>



<p class="has-text-align-center" id="eq-6"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+W%7D%27%29%5C%5C%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29.%26%286%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')\\ = {} &amp; {\mathbb H}({\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W},{\mathbf W}')\\= {} &amp; {\mathbb H}({\mathbf n})\\&amp; + {\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\\&amp; + {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\&amp; + {\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}')\\&amp; + {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}).&amp;(6)\end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}," class="latex" /></p>



<p class="has-text-align-center" id="eq-7"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5Cle%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26%26+%287%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}{\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\le{\mathbb E}|{A_{{\mathbf n}}}|. &amp;&amp; (7) \end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-4">4</a> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}'}}," class="latex" /></p>



<p class="has-text-align-center" id="eq-8"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+-+%28%5Clog+R%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%288%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}'|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}') - (\log R){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (8) \end{aligned}" class="latex" /></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 2 and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Csubseteq+%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}\subseteq {\mathbf W}," class="latex" /></p>



<p class="has-text-align-center" id="eq-9"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29+%5C%5C++%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%28%5Clog+%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%289%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}') \\  \le {} &amp; {\mathbb H}({\mathbf W}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}) \\ \le {} &amp; {\mathbb H}({\mathbf W}) - (\log (1/\delta)){\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}|. &amp; (9)\end{aligned}" class="latex" /></p>



<p>Since <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D%27+%3D+%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5Cbackslash%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf W}' = ({A_{{\mathbf n}}}\cup{\mathbf W})\backslash({A_{{\mathbf n}'}}\backslash{\mathbf W}')" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{{\mathbf n}'}}\backslash{\mathbf W}'\subseteq {A_{{\mathbf n}'}}," class="latex" /> by Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a>,</p>



<p class="has-text-align-center" id="eq-10"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26%2810%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp; {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}) \\ \le {} &amp; {\mathbb H}({A_{{\mathbf n}'}}\backslash{\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W})\\ \le {} &amp; {\mathbb E}|{A_{{\mathbf n}'}}|. &amp;(10)\end{aligned}" class="latex" /></p>



<p>Plugging <a href="https://theorydish.blog/feed/#eq-7">(7)</a>, <a href="https://theorydish.blog/feed/#eq-8">(8)</a>, <a href="https://theorydish.blog/feed/#eq-9">(9)</a>, <a href="https://theorydish.blog/feed/#eq-10">(10)</a> into <a href="https://theorydish.blog/feed/#eq-6">(6)</a> and simplifying using</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%2C%5Cquad+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%5C%5C%26%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+-+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n}') = {\mathbb H}({\mathbf n}),\quad {\mathbb E}|{A_{{\mathbf n}'}}| = {\mathbb E}|{A_{{\mathbf n}}}|,\\&amp;{\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}| = {\mathbb E}|{A_{{\mathbf n}'}}| - {\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|,\end{aligned}" class="latex" /></p>



<p>we get the following upper bound:</p>



<p class="has-text-align-center" id="eq-11"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cle+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+%2B+%282+-+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%5C%5C+%26+-+%28%5Clog+%28R%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%2811%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \le {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) + (2 - \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}| \\ &amp; - (\log (R\delta)){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (11)\end{aligned}" class="latex" /></p>



<p>Comparing <a href="https://theorydish.blog/feed/#eq-5">(5)</a> and <a href="https://theorydish.blog/feed/#eq-11">(11)</a> proves the desired inequality <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|.\square" class="latex" /></p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Moses Charikar, Omer Reingold, and Li-Yang Tan for valuable feedback and inspiring discussions.</p></div>







<p class="date">
by Lunjia Hu <a href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/"><span class="datestr">at May 19, 2021 11:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/">Submit your papers to the 62nd FOCS!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The <a href="https://easychair.org/conferences/?conf=focs2021">submission server</a> for the 62nd FOCS is open! </p>



<p>Please read the <a href="https://focs2021.cs.colorado.edu/cfp/">Call for Papers</a> carefully before you submit. </p>



<p>Below are some important points:</p>



<ul><li>The title/abstract registration deadline is <strong>5 pm EDT</strong> on <strong>May 31st, 2021</strong>. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after this deadline.</li><li>The full paper submission deadline is <strong>5 pm EDT</strong> on <strong>June 3, 2021</strong>.</li><li>The conference is expected to take place <strong>physically</strong> in Boulder, Colorado <strong>Feb 7-10, 2022</strong>.</li><li>Papers that broaden the reach of the theory of computing, make foundational connections to other areas, or raise important problems and demonstrate that they can benefit from theoretical investigation and analysis are especially encouraged.</li><li>Reviewers will be asked to evaluate submissions <strong>both on conceptual and technical merits</strong>. So, authors are encouraged to emphasize both conceptual and technical novelty in the first few pages of the paper. </li><li>FOCS is an IEEE conference and as such, we will review papers in alignment with the IEEE ethics <a href="https://www.ieee.org/about/corporate/governance/p7-8.html">guidelines</a>. Authors are encouraged to reflect on these guidelines in shaping their work, dissemination, and submission.</li></ul>



<p></p>



<p>The PC is eagerly looking forward to your submissions!</p>



<p> </p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/"><span class="datestr">at May 19, 2021 02:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1659">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/">Fair Clustering with Probabilistic Group Membership</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br /><br />Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h \in {\cal H}" class="latex" /> to designate its group membership and we were to consider the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="l_h" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="u_h" class="latex" /> are the lower and upper pre-set proportionality bounds for color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C_i" class="latex" /> denotes the points in cluster <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="i" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C^h_i" class="latex" /> denotes the subset of those points with color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br /></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" class="wp-image-1735" height="151" />Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br /></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" class="wp-image-1737" height="210" />Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-center, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-median, and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p></p></div>







<p class="date">
by seyed2357 <a href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/"><span class="datestr">at May 07, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=51">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/">ALT Highlights – A Report on the First ALT Mentoring Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fifth post in the series, coverage of the first <a href="https://www.let-all.com/alt.html">ALT Mentoring Workshop</a> organized by the Learning Theory Alliance, written by <a href="https://www.let-all.com/blog/feed/knaggita@ttic.edu">Keziah Naggita</a> and <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>.</p>



<hr class="wp-block-separator" />



<h2><strong>1 Introduction</strong></h2>



<p class="has-text-align-left has-text-align-justify">The Learning Theory Alliance (Let-All) is an online initiative aimed at developing a supportive learning theory community, founded by (1) <a href="https://www.cs.utexas.edu/~surbhi/" target="_blank" rel="noopener noreferrer">Surbhi Goel</a>, a postdoctoral researcher at Microsoft Research New York, (2) <a href="https://people.eecs.berkeley.edu/~nika/" target="_blank" rel="noopener noreferrer">Nika Haghtalab</a>, an assistant professor at UC Berkeley EECS, (3) and <a href="https://vitercik.github.io" target="_blank" rel="noopener noreferrer">Ellen Vitercik</a>, a Ph.D. Student at CMU; and advised by <a href="https://www.stat.berkeley.edu/~bartlett/" target="_blank" rel="noopener noreferrer">Peter Bartlett</a>, <a href="https://home.ttic.edu/~avrim/" target="_blank" rel="noopener noreferrer">Avrim Blum</a>, <a href="https://people.csail.mit.edu/stefje/" target="_blank" rel="noopener noreferrer">Stefanie Jegelka</a>, <a href="https://www.dpmms.cam.ac.uk/%7Epll28/" target="_blank" rel="noopener noreferrer">Po-Ling Loh</a>, and <a href="http://www.jennwv.com" target="_blank" rel="noopener noreferrer">Jenn Wortman Vaughan</a>. The goal of the alliance is to ensure healthy community growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers. Let-All’s efforts towards realizing these goals include a series of ongoing and future activities, such as the first ALT mentoring workshop, coordinating the ALT Highlights blog series, and other upcoming community initiatives. This article reports on  Let-All’s <a href="https://let-all.com/alt.html" target="_blank" rel="noopener noreferrer">first Mentoring Workshop</a>, which was affiliated with the 32<sup>nd</sup> International Conference on Algorithmic Learning Theory.</p>



<p class="has-text-align-left has-text-align-justify">The workshop had two main sessions to cater to the time zone differences of the participants.  These sessions had three main components: an academic program, which included how-to-talks, Ask Me Anythings (AMAs), and presentation dissections; a technical program, which included research talks; and a social program, which included discussion tables and other activities.</p>



<p class="has-text-align-left has-text-align-justify">The workshop participants included students, researchers, and industry professionals, all at different levels of familiarity with learning theory. Because of the ongoing COVID-19 pandemic, the workshop was virtual. It was held on the online platforms Zoom and Gather town, a virtual interactive environment that mimics an in-person workshop setting. For accessibility, the workshop organizers opened up the workshop free of cost to all registered participants. </p>



<h2><strong>2 Program Highlights</strong></h2>



<h3><strong>2.1 Academic Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">To kick off the workshop, one of the organizers began with a welcome lecture: Surbhi in session one and Nika in session two.  They read out the code of conduct and who to contact in case of issues, outlined the workshop’s purpose, and gave attendees demographic information. They explained how participants could navigate the workshop-themed Gather town workspace and then ended the introduction with encouragement for participants to mingle. </p>



<p class="has-text-align-left has-text-align-justify">The <strong><em>How-to-Talks</em></strong> sessions covered writing papers, giving talks, and networking. In Session 1, <a href="https://www.cs.cmu.edu/~praveshk/" target="_blank" rel="noopener noreferrer">Pravesh Kothari</a> talked in great detail about the dos and don’ts of what to add in the abstract, overview, introduction, and appendix when advising participants on how to best structure research papers. He told attendees to always put effort into understanding their intended reader or talk audience.  Pravesh encouraged attendees to consider the expertise and interests of the reader or listener to capture their attention since these highly determine the attention span and interest in the information presented to them.  He strongly recommended attendees watch the <em><a href="https://www.youtube.com/watch?v=vtIzMaLkCaM" target="_blank" rel="noopener noreferrer">Leadership Lab: The Craft of Writing Effectively</a></em> by Larry McEnerney , Director of the University of Chicago Writing Program.<em> </em>In session 2 of the workshop, <a href="https://home.cs.colorado.edu/~raf/" target="_blank" rel="noopener noreferrer">Rafael Frongillo</a>, similar to Pravesh, discussed how to capture the intended audience when one writes a paper, reviews, and talks.</p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>networking session</em></strong>, <a href="https://www.cc.gatech.edu/~jabernethy9/" target="_blank" rel="noopener noreferrer">Jacob Abernethy</a> encouraged participants to seek out horizontal and vertical networking, for example, through collaborations, talks, and reach outs. He said that currently, in academia, Ph.D. admissions, faculty hiring, and tenure appointments are heavily risk-averse. Therefore, people seek out candidates based on their network. For this reason, it is crucial for students to network from early on in their careers. He gave great examples of how junior researchers can reach out and forge relationships with other researchers. For example, when you meet academics, faculty/postdocs at events, ask to give a talk at their lab. Jacob also candidly talked about his earlier failures at MIT and how they shaped his journey. He talked about luck and how <a href="https://www.microsoft.com/en-us/research/people/jcl/" target="_blank" rel="noopener noreferrer">John Langford</a>, who was at Toyota Technological Institute at Chicago at the time, took a chance on him that forever changed his life. Jacob, therefore, advised academics to take chances on people as this would change the course of the field. </p>



<p class="has-text-align-left has-text-align-justify"><a href="https://jamiemorgenstern.com" target="_blank" rel="noopener noreferrer">Jamie Morgenstern</a> discussed different networking methods in the <strong><em>second How-to-talks session</em></strong>. She emphasized that for junior researchers, it’s important to attend conferences and to network with others, to advertise their research through talks, and to reach out to faculty for collaboration. To introduce oneself and capture the listener’s attention, Jamie said, for conferences, prepare to do so in two minutes, for social four minutes, bar 12 minutes, and faculty interview 25 minutes. Senior grad students may help introduce the juniors during lunch/poster sessions. Finally, when emailing faculty about research, she said one should avoid discussion about other people’s work and instead should stick to the recipient’s work – “showing deep understanding and possibly open questions which might lead to collaboration.” </p>



<p class="has-text-align-left has-text-align-justify">In both workshop sessions, there were<strong><em> two parallel talk dissections, </em></strong>in which senior faculty members gave both positive and constructive feedback on talks junior researchers presented. In the first session, <a href="https://www.cs.cornell.edu/~rdk/" target="_blank" rel="noopener noreferrer">Bobby Kleinberg</a> discussed <a href="https://www.emilyruthdiana.com" target="_blank" rel="noopener noreferrer">Emily Diana</a>‘s talk titled “Minimax and Lexicographically Fair Learning: Algorithms, Experiments, and Generalization”. He highlighted parts that were impressive, those that needed improvement, and gave general advice on structuring an audience-based presentation. When Bobby suggested including more diagrams than text, a few people made suggestions of free tools including tikz, matcha.io, PowerPoint, and draw.io. In parallel, <a href="http://cseweb.ucsd.edu/~kamalika/" target="_blank" rel="noopener noreferrer">Kamalika Chaudhuri</a> dissected a talk on “Efficient, Noise-Tolerant, and Private Learning via Boosting” by <a href="https://marco.ntime.org" target="_blank" rel="noopener noreferrer">Marco Carmosino</a>. Two main takeaways of this talk dissection were the balance of technical and nontechnical content (e.g., explaining ideas with fun pictures, etc.) and having one main and clear idea as the talk’s takeaway. </p>



<p class="has-text-align-left has-text-align-justify">In the second session, <a href="https://sites.google.com/site/acmonsterqiao/" target="_blank" rel="noopener noreferrer">Mingda Qiao</a> gave a talk titled: “Stronger Calibration Lower Bounds via Sidestepping” which <a href="https://praneethnetrapalli.org" target="_blank" rel="noopener noreferrer">Praneeth Netrapalli</a> dissected. Praneeth remarked that theory folks often jump into the problem straight away without covering much background. In conferences, this might be fine due to time pressure and specific interests. However, in broader settings such as departmental seminars, he advised the speaker to allocate more time to introduce the problem lucidly and concisely. In parallel, <a href="https://sites.google.com/site/marywootters/" target="_blank" rel="noopener noreferrer">Mary Wootters</a> dissected a talk titled “List-Decodable Subspace Recovery: Dimension Independent Error in Polynomial Time” that <a href="http://aineshbakshi.com" target="_blank" rel="noopener noreferrer">Ainesh Bakshi</a> presented. </p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>AMA session</em></strong> moderated by <a href="https://www.stat.cmu.edu/~aramdas/" target="_blank" rel="noopener noreferrer">Aaditya Ramdas</a>, <a href="https://web.stanford.edu/~lmackey/" target="_blank" rel="noopener noreferrer">Lester Mackey</a> refreshingly answered several of the attendees’ well-curated questions about what makes strong collaborations, how to get into grad school, and whether or not he ever felt like quitting his Ph.D., among others.  He encouraged students to take classes with professors they are interested in as it makes it easy to ask for a mentorship opportunity. Lester talked about collaborations and imposter syndrome and encouraged attendees to look on the brighter side of things, to remember that we all are working towards one big goal, creating positive changes in the world. Therefore if someone discovers a result before us, we should applaud them, collaborate if possible, and move onto new problems. He said he did not necessarily plan to do a Ph.D. but got into it towards the end of his undergraduate degree due to an internship that made him fall in love with doing research. </p>



<p class="has-text-align-left has-text-align-justify">In the evening, there was an AMA session with <a href="http://people.csail.mit.edu/shafi/" target="_blank" rel="noopener noreferrer">Shafi Goldwasser</a> moderated by Nika. Shafi gave thoughtful and candid answers to attendees’ captivating questions about research, life in academia, collaborations, among others. Shafi told attendees that healthy competition, trust, and overlap of research interest, is crucial for successful research in the early stage of the career. She also asserted that fundamental science is always impactful. She mentioned that the high points of her career were working on problems she was curious about: cryptography, pseudo-randomness, and zero-knowledge proofs. Finally, when asked about what advice she wished she had during the early stage of her career, interestingly Shafi replied: “having good colleagues, good friends at work, very important, most important – having a listening, promoting and supportive cohort of friends rather than an individualistic path as a scholar is priceless.” </p>



<h3><strong>2.2 Technical Program</strong></h3>



<p class="has-text-align-left has-text-align-justify"><strong><em>Two research talks</em></strong> happened in the first session. First, Po-Ling Loh gave a talk titled “Mean estimation for entangled single-sample distributions.” Then <a href="https://web.stanford.edu/~vsharan/" target="_blank" rel="noopener noreferrer">Vatsal Sharan</a> talked about “Sample Amplification: Increasing Dataset Size even when Learning is Impossible”.<br />Similarly in the second session, <a href="https://www.cohennadav.com" target="_blank" rel="noopener noreferrer">Nadav Cohen</a> gave the first talk about tensor and matrix completion problems and the importance of understanding the theory behind deep learning from theoretical and practical perspectives. After, <a href="https://i.cs.hku.hk/~zhiyi/" target="_blank" rel="noopener noreferrer">Zhiyi Huang</a> gave a talk titled “Setting the Sample Complexity of Single-parameter Revenue Maximization.” </p>



<h3><strong>2.3 Social Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">In both sessions, during the social hours, <a href="https://sites.google.com/view/sumegha-garg/home" target="_blank" rel="noopener noreferrer">Sumegha Garg</a>, <a href="http://sgunasekar.github.io" target="_blank" rel="noopener noreferrer">Suriya Gunasekar</a>, and <a href="https://teddlyk.github.io" target="_blank" rel="noopener noreferrer">Thodoris Lykouris</a> organized the <strong><em>table topics</em></strong> to help attendees meet and interact with senior researchers and professors on different topics. The table topics included the following; starting on ML research, research agendas, ML+X: multidisciplinary research, advisor-advisee relationships, collaborators, communicating research and networking, beyond your institution: internships and research visits, planning after grad school: academia versus industry, Grad school applications, Work ethics, and Open research discussion. </p>



<p class="has-text-align-left has-text-align-justify">The table topics were chaired by; Jacob Abernethy, <a href="https://www.shivani-agarwal.net" target="_blank" rel="noopener noreferrer">Shivani Agarwal</a>, <a href="https://ericbalkanski.com" target="_blank" rel="noopener noreferrer">Eric Balkanski</a>, Peter Bartlett, Avrim Blum, <a href="http://sbubeck.com/" target="_blank" rel="noreferrer noopener">Sébastien Bubeck</a>, Kamalika Chaudhuri, Nadav Cohen, Sumegha Garg, Surbhi Goel, Suriya Gunasekar, Nika Haghtalab,  <a href="https://www.cs.columbia.edu/~djhsu/" target="_blank" rel="noopener noreferrer">Daniel Hsu</a>, <a href="https://www.prateekjain.org" target="_blank" rel="noopener noreferrer">Prateek Jain</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html" target="_blank" rel="noopener noreferrer">Mike Jordan</a>, <a href="https://homes.cs.washington.edu/~sham/" target="_blank" rel="noopener noreferrer">Sham Kakade</a>, <a href="https://www.microsoft.com/en-us/research/people/adum/" target="_blank" rel="noopener noreferrer">Adam Kalai</a>, Pravesh Kothari, <a href="https://people.cs.umass.edu/~akshay/" target="_blank" rel="noopener noreferrer">Akshay Krishnamurthy</a>, <a href="https://jerryzli.github.io" target="_blank" rel="noopener noreferrer">Jerry Li</a>, Po-Ling Loh, Thodoris Lykouris, <a href="https://www.tau.ac.il/~mansour/" target="_blank" rel="noopener noreferrer">Yishay Mansour</a>, <a href="https://pasin30055.github.io" target="_blank" rel="noopener noreferrer">Pasin Manurangsi</a>, <a href="https://vmuthukumar.ece.gatech.edu" target="_blank" rel="noopener noreferrer">Vidya Muthukumar</a>, Praneeth Netrapalli, <a href="https://wensun.github.io" target="_blank" rel="noopener noreferrer">Wen Sun</a>, <a href="https://www.bowaggoner.com" target="_blank" rel="noopener noreferrer">Bo Waggoner</a>, <a href="https://mzampet.com" target="_blank" rel="noopener noreferrer">Manolis Zampetakis</a>, and <a href="https://cyrilzhang.com/">Cyril Zhang</a>. </p>



<p class="has-text-align-left has-text-align-justify">Lastly, at the end of the two general research talks in sessions one and two of the workshop, attendees assembled on Gather town to close the workshop. The social event included 1:1 social interactions with other attendees, an attempt at forging relationships, and activities like dancing.</p>



<h2><strong>3 Attendance Statistics, Testimonials and Feedback</strong></h2>



<h3><strong>3.1 Participants Statistics</strong></h3>



<p class="has-text-align-left has-text-align-justify">ALT mentoring workshop welcomed talented academics, researchers, and professionals from a wide array of backgrounds. Of the 438 registered to attend the workshop, 197 were new to the learning theory community, 37 attended at least one ALT/COLT conference in the past, and 146 hadn’t attended ALT/COLT but had attended machine learning conferences (STOC, NeurIPS, etc.) as shown in Figure 1. </p>



<p class="has-text-align-left has-text-align-justify">The workshop participants came from different parts of the world, were of different genders, races, and seniority levels. We use Figures 2, 3b, 4a, and 4b to highlight this demographic information about the participants. Some participants chose session one, and others chose session two, a choice driven by their schedules and time zones. The attendance composition is as shown in figure 3a.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="515" alt="" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.28-AM.png?resize=515%2C310&amp;ssl=1" class="wp-image-58" height="310" />Figure 1: Registrants familiarity with the community</figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.11-AM-2.png?resize=678%2C401&amp;ssl=1" class="wp-image-61" height="401" />Figure 2: Career stages of participants</figure></div>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.33.54-AM-1.png?resize=678%2C414&amp;ssl=1" class="wp-image-70" height="414" /></figure></li><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.25-AM-1.png?resize=678%2C406&amp;ssl=1" class="wp-image-71" height="406" /></figure></li></ul>Figure 3: Session preferences (a) and locations of participants (b)</figure>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.57-AM.png?resize=678%2C404&amp;ssl=1" class="wp-image-75" height="404" /></figure></li><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.42-AM-1.png?resize=678%2C404&amp;ssl=1" class="wp-image-77" height="404" /></figure></li></ul>Figure 4: Race (a) and Gender (b) distribution of participants</figure>



<p></p>



<h3><strong>3.2 Testimonials and Feedback</strong></h3>



<p class="has-text-align-left has-text-align-justify">In this section, we give a recount of testimonials from participants who we interviewed after the workshop. We also highlight some of the common themes in feedback from participants.</p>



<p class="has-text-align-left has-text-align-justify">In general, participants loved the content delivered in the sessions. They said it was informative, intuitive, and rare to find. Several participants loved interacting with peers and senior members and wished they had more time and activities to do it. The How-to-talks session (focused on networking skills, structuring papers, talks, and reviews) was the most popular session among attendees. 76.7% of the survey respondents said the session helped them gain new technical skills or hone existing skills, see opportunities in academia and how to use them, and see barriers in academia and ways to overcome them. Figure 5 highlights attendees’ ratings of the skills acquired from the workshop. </p>



<figure class="wp-block-image size-large is-resized is-style-default"><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.11-AM.png?resize=678%2C404&amp;ssl=1" class="wp-image-80" height="404" />Figure 5: Usefulness ratings of skills gained from the workshop</figure>



<p><strong>Below is a recount of the workshop experiences of the interviewed attendees.</strong></p>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlight was the How-to-Talks since they provided a lot of more personal information/inputs that you cannot easily find online and which was very valuable. The event helped me to remember and reflect upon which qualities are crucial to becoming a good researcher. I even made a list in a place that I see every day to keep them in mind.” – <em><a href="https://www.michaelaerni.com" target="_blank" rel="noopener noreferrer">Michael Aerni</a>, MSc student at ETH Zurich.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“The workshop highlights for me were the How-to-talks, the AMA session with Lester, and the social tables. The How-to-talks were extremely valuable as they discussed topics such as structuring papers and networking in the community. These are subtle aspects that are not often explicitly talked about in the community. I, therefore, learned a lot from them. The AMA session was refreshingly honest and open. Finally, the social tables were also great as I got to meet and talk to some well-established senior community members like Sebastien Bubeck, Shivani Agarwal, and Akshay Krishnamurthy.” – <em><a href="https://people.eecs.berkeley.edu/~tgautam23/" target="_blank" rel="noopener noreferrer">Tanmay Gautam</a>, a second-year Ph.D. student at UC Berkeley. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“It was awesome to have Lester Mackey answer my questions, indubitably. I learned about staying true to the research questions I genuinely believe in regardless of external opinions and rewards. As an NYU AI School organizer, I can appreciate how much effort went into organizing the workshop. The organizers did a stellar job! I think a version of the same event again would be perfect.” – <em><a href="https://swapneelm.github.io" target="_blank" rel="noopener noreferrer">Swapneel Mehta</a>, a Data Science Ph.D. student at New York University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlights were getting to talk with senior members of the community. These opportunities rarely come by for someone who lives in a foreign country. For a timid person like myself, I am also thankful for the senior members for helping me (and other participants) breaking the ice and easing us into the conversations. Thanks to this event, I am now more confident in engaging with other researchers.”- <em><a href="http://www.donlapark.cmustat.com" target="_blank" rel="noopener noreferrer">Donlapark Ponnoprat</a>, a Statistics lecturer at Chiang Mai University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main highlights were Dr. Goldwasser’s session with Dr. Haghtalab and the socials. In the socials, I was able to ask professors and senior researchers for advice on varied topics based on the tables. Insights from Dr. Cohen’s lecture on tensor rank and implicit regularisation gave me several pointers to ideas in the literature that I was not aware of as a junior researcher from a slightly different AI specialty. These ideas might be beneficial for my research in the long term. </p><p class="has-text-align-left has-text-align-justify"> I send a sincere thank you to all the organizers. The mentorship workshop was a great event, and it models concrete actions, what it means to foster a welcoming community. It is clear how kind and dedicated folks are here as some researchers even stayed beyond midnight in their time zones to answer questions that attendees had. If an event like this happens again, I am most definitely signing up to come.” – <em><a href="https://github.com/esraa-saleh" target="_blank" rel="noopener noreferrer">Esra’a Saleh</a>, a Masters in Computer Science student at the University of Alberta, affiliated with AMII and RLAI.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main takeaway from the event was an inside look at academia. As an undergrad, my only experience in academia has been the little experience I have with my advisory professors. While this is an invaluable experience, this event was nice as it was one of the very few that cater to students, including undergrads, with the intent of bringing them into the academia fold. Getting to know new people and talking to them was extremely interesting, especially during lockdown when connecting with others is a much more valuable commodity.” – <em><a href="https://pages.cs.wisc.edu/~shrey/" target="_blank" rel="noopener noreferrer">Shrey Shah</a>, a penultimate year undergraduate student at the University of Wisconsin-Madison. 
</em></p></blockquote>



<p class="has-text-align-left has-text-align-justify">Several participants enjoyed the workshop sessions and hoped that Let-All holds more similar themed workshops in conferences. Attendees suggested ways for attendees to interact more with each other. Some of the suggestions included the following: ‘beginner-friendly open problems sessions where attendees can collaborate’ – Esra’a Saleh, ‘an icebreaker session at the beginning that encourages attendees to mingle’ – Michael Aerni, and ‘a poster session for participants to present their work’ – Shrey Shah. </p>



<h2><strong>4 Conclusion</strong></h2>



<p class="has-text-align-left has-text-align-justify">The ALT mentorship workshop organized by the Learning Theory Alliance brought together many academics and researchers. It was, and we hope it continues to be, an opportunity for the budding researchers to learn about research and meta-research, forge collaborations, and be inspired. Kudos to the organizers and the Alliance in general for dreaming such a positive vision and then striving to make it a great success! </p>



<p class="has-text-align-left has-text-align-justify"><em>Thanks to <a href="http://www.gautamkamath.com" target="_blank" rel="noreferrer noopener">Gautam Kamath</a>, <a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener">Margalit Glasgow</a>, Surbhi Goel, Nika Haghtalab</em> <em>and Ellen Vitercik for helpful conversations and comments.</em></p></div>







<p class="date">
by Keziah <a href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/"><span class="datestr">at May 06, 2021 03:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/">Self-fulfilling and self-negating predictions: a short tale of performativity in machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<blockquote class="wp-block-quote has-text-align-center"><p><em>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo.</em></p></blockquote>



<p>In 1998, Michel Callon wrote what would be the first in an ongoing series of controversial publications in economic sociology [1]. He was the first to propose the idea that “the economy is not embedded in society but in economics”. With this, he challenged the conventional view that economic theories and models passively observe markets and infer their behavior, just like laws of physics passively describe the principles governing natural phenomena. Instead, Callon argued that economic theories are <em>performative:</em> they induce the economy, creating the phenomena they aim to describe.</p>



<p>One example that is often cited in support of Callon’s claims is the impact of the celebrated Black-Scholes-Merton options pricing model [2, 3]. MacKenzie and Millo [4] investigated the role of this model in the economy and found that it “made itself true”. In their words,</p>



<p class="has-text-align-center"><em>“Black, Scholes, and Merton’s model did not describe an already existing world: when first formulated, its assumptions were quite unrealistic, and empirical prices differed systematically from the model. Gradually, though, the financial markets changed in a way that fitted the model”.</em></p>



<p>Indeed, participants in the market started making decisions assuming the market obeys the mathematical laws implied by the Black-Scholes-Merton model. As MacKenzie and Millo put it, “pricing models came to shape the very way participants thought and talked about options”.</p>



<p>This phenomenon — whereby models and predictions inform decision-making and thus alter the target of prediction itself — is by no means special to economic forecasts.</p>



<p>Predictive policing, for example, develops algorithms that use historical data to estimate the likelihood of crime at a given location. Those locations where criminal behavior is deemed likely by the system typically get more police patrols and surveillance in general. In a kind of self-fulfilling prophecy [5], these actions resulting from prediction might further increase the <em>perceived</em> crime rate at the patrolled locations, thus biasing the data used for future decisions.</p>



<p>A similar feedback loop arises in traffic predictions, when drivers decide which route to take based on the estimated time of arrival (ETA) calculated by a traffic prediction system. If the predictive system estimates low ETA for a given route, many drivers take the route, potentially leading to an overflow of traffic and making the ETA prediction inaccurate as a result. Contrary to the previous example, traffic predictions arguably exhibit a self-negating prophecy: low ETA might imply a longer travel time, and vice versa.</p>



<p>While the previous examples deal with qualitatively different feedback mechanisms, the interplay of predictions and decision-making is similar. First, one uses historical data to build a predictive model. Then, the predictions of the model feed into and inform consequential decisions. Finally, these decisions trigger changes in the environment, making future observations differ from those in the initial dataset.</p>



<p class="has-text-align-center"><img width="624" src="https://lh3.googleusercontent.com/C3qD-SK6AD4gDpkJznllugJS8OZaJwGvXSNi96qYKPu2ALr5vSxPMhteCScLq-CCdKOvh8bZ8KB-gvTRVkbZALVEuML0WhSV0pX8YM1MHupTLPjZ8PKEydiNu_h3iUdlVAYUcFDT" height="81" /></p>



<p>We refer to prediction problems that exhibit this feedback-loop behavior as <em>performative prediction</em> problems.</p>



<p>In the language of machine learning, such a change in patterns would often be called <em>distribution shift</em>. Notably, however, performative distribution shift is not due to external factors independent of the model, such as, say, when traffic patterns change due to seasonal effects. Rather, the distribution shift is triggered directly by the choice of predictive model. (Of course, distribution shifts can also be caused by a combination of external factors and model choice.)</p>



<p>To formalize performative prediction mathematically, it is instructive to contrast performative prediction problems with supervised learning problems. In supervised learning, the decision-maker observes pairs of features and outcomes <img width="72" src="https://lh5.googleusercontent.com/2U92MhGBO6DJqe607HL2T4uWicPXKFgrU2PoSaeymNLxbOteL6r_dhkuuFo91W2AXoAzrhxt8Ndg1jfhf8KVqZMbID1koi01cpGcXz-CTACfH_b54DohHMqpO2hJ5bEtfdE6v6Ag" height="15" /> drawn from a <em>fixed</em> distribution <img width="14" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" height="13" />. The key difference in performative prediction is that there is no longer an unknown static distribution generating observations; rather, data is drawn from a <em>model-dependent distribution</em> <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="17" />, where <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="12" /> is a parameter vector specifying the deployed model. For example, <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" /> could be the weights of a neural network, or a vector of linear regression coefficients. For a given choice of parameters <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />, <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="17" /> should be thought of as the distribution over features and outcomes that results from making decisions according to the model specified by <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />. In the context of the traffic prediction example, <img width="32" src="https://lh4.googleusercontent.com/HDm0VWvPNAUIdPfMSLc0ptEDS7Tph1p00WbC6TlSbQ8oYjK2nJSuSMiLuKN8IAA89L2H8rDLRPf9GWwc37Zb_0_qLxN-UuVVsz_hkBB43hQcTOtV4HUg30BtsHel_hpCFHfYJa6E" height="17" /> could be a distribution over traffic conditions and travel times, given that drivers make routing decisions in response to ETA forecasts by model <img width="8" src="https://lh3.googleusercontent.com/TkPLvHJ9t0soCQkx43reFppYsx4b0sJrNUBWd_Yx_rarIU4nS92GpBFBn-D_jhvDNAU04Aee4NMsVzHEozOLxXN11DMl4h1O2TvHfNih9IqHi0R3wM1otXYEp1MbkZlbI0faSV1i" height="13" />.</p>



<p>In supervised learning, the quality of a model <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" /> is typically measured by its <em>risk</em>, namely, the expected loss of the model on instances from distribution <img width="14" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" height="13" /> as measured via a loss function <img width="9" src="https://lh5.googleusercontent.com/iLwCF0E2qkddKEmiLhziUoOB171Na3z6aLjqWSYTPykfGKIx5PBtNT9qsZXIMPLc3hVXkkupbpqILbC4Fx9p_h5G8a1GmlYVL3rcx787SVO-24HKrL39LxAwPGaQ2i8wuPURBoyS" height="16" />:</p>



<p class="has-text-align-center"><img width="153" src="https://lh5.googleusercontent.com/vP7Ll-u4VopJmW24L2lMvqwf8N9bOrKaoLFDDlmcZgmML4CM8bM53_lMEG--5Om2mTYOQ5uNqUBkzlp2Hgd9i3EO8Y8bzC8kvkFgQKSfqOgHoWiMpK2MVasGJmdFTdERwIUg0RzK" height="26" /></p>



<p>Since performative prediction does not admit one true data-generating distribution, but rather a family of distributions <img width="58" src="https://lh6.googleusercontent.com/un2s0qA36MFKtNGHoDb7sQdPfToUoZjYy45nbwmSy2jQhSDJbiouNjATQWsSwDlpUF-PfOk1ElFPDb8Rsr_TAAB0GUemVV5Y_bG1EISEUD7M9N5RGxug1gGdvpXa-D70ATQSafzT" height="18" />, evaluating a model <img width="8" src="https://lh4.googleusercontent.com/zJsapVnRoEAmwRciJFyPDFM3pc3YOuB_6gM71GJX1Aa_9Tm09RLD4mj_DOEkH0CzKHFPV93LesWVf1AUH9QlwQoU0x07xusBHiT8-EeAApf2qMsufNc8Vuzc77LWu4bxEuYwgxXp" height="13" /> calls for a new risk concept. Arguably the most natural counterpart of the risk in supervised learning is the expected loss on the distribution that arises once the model is deployed and feeds into consequential decisions. This leads to the notion of <em>performative risk</em>, defined as:</p>



<p class="has-text-align-center"><img width="181" src="https://lh5.googleusercontent.com/zWCmnf2uG6j8LZG7w68p8y4I2wHcKc82ZULfh_MUmGc3YI6UByQP8dB3nBDugFmg8t0VybMz6RV4tzviUsIyHmVY1uUH1Xjv2XKTSEQGpFW3-o0e50-rxKeouEula7UNdqC6HYHr" height="30" /></p>



<p>Adopting the performative risk as the single overarching measure of quality, a model would be optimal if it minimizes the performative risk. While an appealing solution concept, performative optimality is difficult to achieve, seeing the double dependence of the risk function on <img width="8" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" height="13" />. One of the main computational difficulties is the fact that, even if the loss <img width="9" src="https://lh5.googleusercontent.com/WZWkgjmj0CvaBREmhCbwBw1i5JfL1Tviq96JbXG1SePOG-BXJ8uv_fUUE2KKJCw2qvd5zpA_GUhHZ9azV6AUENm0qVsKnTPMg6WzLvT5QGGLNwcLyhRU0iOhYtRy6Fylx66uMGRS" height="16" /> is convex in <img width="8" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" height="13" />, <img width="43" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" height="17" /> need not be convex. Prior work on strategic classification [6] implies a set sufficient conditions for <img width="43" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" height="17" /> to be convex in a binary classification context, and in recent work [7] we identified a complementary set of conditions when the family of distributions <img width="58" src="https://lh4.googleusercontent.com/Ti5C4ibszTKUqTM5evbcuaHojWHAN_Rq20XcnDlunJNG1M4lBKsjB7nJOKEN4lTSdWNmgKkcxqAwPPRahQcZ4uL8kCZHn_AuaB_Z9T35eztMk2CZzMcK50GMGnH69Jdjd0euwDtI" height="18" /> forms an appropriate location-scale family. That said, in many practical settings convexity might be an unrealistic and unnecessarily strong guarantee to aim for. As we know from present-day machine learning, even non-convex problems can sometimes be amenable to simple optimization algorithms. Understanding the optimization landscape of the performative risk beyond convex settings is a fruitful direction going forward.</p>



<p>A seemingly less ambitious target is to find a model that is <em>locally</em> optimal in some appropriate sense. For example, one could optimize for models that are optimal on the distribution that they induce:</p>



<p class="has-text-align-center"><img width="233" src="https://lh6.googleusercontent.com/ENuLNHEI7mNEDONR7rUA2FzwbkvqbINkwHJfO7jiiODrZ3_Ko9NZmJAt4Qv0cPNF4-_C68yI3CE4UMcFjzP8v5UHdu_LgBkdrCnUj4A-E6uccXlclWWZef2onVzsd5X2sHq5tTlh" height="22" /></p>



<p>We call a model that satisfies the fixed-point equation above <em>performatively stable</em>. Performative stability arises naturally when the decision-maker applies the heuristic of myopically updating the model based on the distribution resulting from the previous deployment:</p>



<p class="has-text-align-center"><img width="248" src="https://lh4.googleusercontent.com/RlZ0ZmF3E0VHyj_ktWb3QtAxsXTY_WjoGS29Jss3v_5cXnDVsWbeEvaTAA4l4tUYa5ccYvJPWcPXy5oLa3BnBT0Yqws89mNind-2UT8IA2ip3_pt3PbFhiNVcgIhgMS1omjfgzI1" height="20" /></p>



<p>If this retraining strategy converges, then it necessarily converges to a performatively stable solution. This is an appealing property, since it says that stability eliminates the need for retraining. Several existing works [8, 9, 10] have identified necessary and sufficient conditions for the above retraining heuristic, and some of its efficient approximations, to converge to a stable point. Roughly speaking, retraining converges to a stable solution if the loss is well-behaved and the performative feedback effects are not too strong. If either of those two conditions is violated, there is no guarantee of convergence.</p>



<p>In the language of game theory, one can think of performative prediction as a two-player game between a decision-maker, who decides which predictive model to deploy, and the model’s environment, which generates observations according to <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="16" />. If <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="16" /> is thought of as the “best response” (according to some underlying utility) of the model’s environment to the deployment of model <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />, then a performatively stable solution corresponds to a <em>Nash</em> equilibrium, while a performatively optimal solution corresponds to a <em>Stackelberg</em> equilibrium with the decision-maker acting as the leader.</p>



<p>Only in special cases, such as in well-behaved zero-sum games, it is known that Nash equilibria coincide with Stackelberg equilibria. Therefore, whenever performative prediction is a well-behaved zero-sum game, all stable solutions are also performatively optimal. However, <em>performative prediction is typically not a zero-sum game</em>. For example, if the decision-maker’s loss simply measures predictive accuracy, it seems odd that the environment’s primary objective is to hurt the model’s accuracy. Indeed, a typical performative prediction problem is a general-sum game without much structure. This implies that stable solutions and performative optima can be <em>very different</em>. And, since naive retraining strategies only converge to stability, this means that such myopic updates can be an inadequate method of overcoming performative distribution shifts and achieving low performative risk. This observation further motivates understanding the optimization landscape of the performative risk, as well as developing efficient algorithms for optimizing it. Recent work has explored several algorithmic solutions [11, 7], appropriate in convex settings.</p>



<p>Performative prediction relates to many other areas beyond game theory, including bandits, reinforcement learning, control theory. These frameworks are flexible enough to capture performative prediction as a special case, however performativity arises via distinctive feedback mechanisms and as such deserves its own specialized analysis. There is a long way to go in understanding the properties of performative distribution shifts, how they connect to feedback mechanisms in other disciplines, and how to tackle these shifts in practice. Furthermore, it is unclear whether a single distribution <img width="32" src="https://lh6.googleusercontent.com/a-1tKaIQT9pJwi6WQ4eV4ZO3xcFy1e_joEh0tz1QdG-cj7x-60nDwfRyyztRSzjEaL-0xbImihmV5LDB8njPUhSlsIeyJABtowIBOktz79sRW7hAY8B75CePuDxp47xEFOZ68gSA" height="16" /> is expressive enough to describe the observations after model deployment; in practice there are different kinds of memory effects [12] and self-reinforcing loops that make the data distribution evolve with time, even when the model is kept fixed. Finally, to make the existing theoretical insights actionable, going forward we need to think about what is the right solution concept — both statistically and ethically — to optimize for in performative settings.</p>



<p><br />[1] M. Callon. Introduction: the embeddedness of economic markets in economics. <em>The Sociological Review</em>, 1998<br />[2] F. Black, M. Scholes. The pricing of options and corporate liabilities. <em>The Journal of Political Economy</em>, 1973<br />[3] R. C. Merton. Theory of rational option pricing. <em>The Bell Journal of Economics and Management Science</em>, 1973<br />[4] D. MacKenzie, Y. Millo. Constructing a market, performing theory: The historical sociology of a financial derivatives exchange. <em>American Journal of Sociology</em>, 2003<br />[5] D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, S. Venkatasubramanian. Runaway feedback loops in predictive policing. <em>ACM Conference on Fairness, Accountability and Transparency</em>, 2018<br />[6] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, Z. S. Wu. Strategic classification from revealed preferences. <em>ACM Conference on Economics and Computation</em>, 2018<br />[7] J. Miller, J. C. Perdomo, T. Zrnic. Outside the echo chamber: Optimizing the performative risk. <em>arXiv preprint</em>, 2021<br />[8] J. C. Perdomo, T. Zrnic, C. Mendler-Dünner, M. Hardt. Performative prediction. <em>International Conference on Machine Learning</em>, 2020<br />[9] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, M. Hardt. Stochastic optimization for performative prediction. <em>Conference on Neural Information Processing Systems</em>, 2020<br />[10] D. Drusvyatskiy, L. Xiao. Stochastic optimization with decision-dependent distributions. <em>arXiv preprint</em>, 2020<br />[11] Z. Izzo, L. Ying, J. Zou. How to learn when data reacts to your model: Performative gradient descent. <em>arXiv preprint</em>, 2021<br />[12] G. Brown, S. Hod, I. Kalemaj. Performative prediction in a stateful world. <em>arXiv preprint</em>, 2020</p></div>







<p class="date">
by tijanazrnic <a href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/"><span class="datestr">at May 06, 2021 12:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=55">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/">ALT Highlights – An Interview with the PC Chairs of ALT 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fourth post in the series, an interview with ALT 2021 PC Chairs <a href="http://vtaly.net/">Vitaly Feldman</a> and <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>, written by <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a> and <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>.</p>



<hr class="wp-block-separator" />



<p>We had the great opportunity to attend <em>The 32nd International Conference on Algorithmic Learning Theory</em>, held online between March 16-19, 2021, and co-chaired by Vitaly Feldman and Katrina Ligett. Vitaly is a research scientist at Apple AI Research and has done foundational works in machine learning and privacy-preserving data analysis. Katrina is an Associate Professor of Computer Science at the Hebrew University of Jerusalem and has done pivotal works in data privacy, algorithmic fairness, algorithmic game theory, and online algorithms. We asked for an interview with them about their experiences and perspectives as co-chairs, to which they kindly agreed. We are happy to share with the readers the excerpts of this interview.</p>



<p class="has-text-align-center"><img src="https://lh4.googleusercontent.com/7RUOfC-v06amphwIhfCYsQNH4jUg82AVsePZcbWO0D40DhSRk-Cf_wnXx9y5RI-qVYz6D-IRAxRzsQ9lWBpraofuggrTYC_sG40GehOCvSrQyZfj1khlMTVqK23NKOZueGcbK_xa" style="width: 225px;" />           <img width="194" src="https://lh4.googleusercontent.com/lUMmpb6Bcfq3bPljS7jYaF2TFaXRks64--IeslcdR3WzqnCtUETu-ymoOSxm7ys_6eyRFb2mGmd1mCe1boNHdxii0d1_UCthAEJy_eTsWsGQsFKpsV5snZe3Nm9g-QDy0JTnzPKx" height="252" /></p>



<p><strong>How it started</strong></p>



<p>How are chairs and program committees chosen? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: The chairs are selected by the Association for Algorithmic Learning Theory (AALT) Steering Committee (<a href="http://algorithmiclearningtheory.org/alt-steering-committee/">http://algorithmiclearningtheory.org/alt-steering-committee/</a>), and the chairs then select the program committee members. Vitaly and I brainstormed potential PC member names, solicited additional suggestions, and also considered the lists of people who have served on recent ALT and COLT PCs. In building the PC, we had many considerations in mind, including coverage of research areas, and various metrics of diversity.</p>



<p><strong>The chairs’ role</strong></p>



<p>What are the different tasks a chair has? What is the most difficult task?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: At a high level, the roles of the PC chairs are to build the PC, oversee the reviewing process and create the conference program. Practically, though, there are a lot of decisions that need to be discussed, emails to be sent, and a lot of organizational aspects to tend to—configuring the reviewing platform, sending reminders, chasing down late reviews, and so on. Vitaly and I have a very, very long joint “to do” list—and luckily, it’s now almost all crossed off! We also had additional responsibilities this year because of the move to the virtual conference format, including selecting the technologies, overseeing the pre-recording process, and much more.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I think the hardest and probably the most time-consuming is making the accept/reject decisions on papers.  For a large fraction of the papers arriving at the decision requires getting a sense of the results; understanding the main points in reviews, author responses and discussion (while calibrating them to the PC members and reviewers); ensuring that each paper is properly discussed by chasing reviewers, asking questions and often soliciting additional opinions. We also needed to come up with a set of criteria for deciding on borderline cases and make sure that these criteria are applied as consistently as possible. At the end it is a rather long and iterative process that luckily for us has converged to a program we are happy with.</p>



<p>How much time do you spend doing chair tasks? How do you balance chairing a conference (a massive amount of work) with all your other commitments? Do you turn down other service items you would generally accept, etc.?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: It’s difficult to estimate the number of hours, but I think we have been meeting regularly since early June 2020, and we’re only wrapping up our work now, in late March 2021. It’s a much longer-timeframe commitment than serving as a PC member. I actually am chairing a second conference this year, FORC, and together it makes for a pretty serious load. As a result, I have been declining all other conference-related service. I also have a couple of other pretty substantial service commitments, as well, so I just don’t have bandwidth this year for additional PC and Area Chair-type roles.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s hard to tell how much time we spent in total. My rough estimate is that it’s about a month of full-time work. I also had to decline most other service commitments during that period some of which I’d normally accept. Naturally, it also slows down other work so I definitely had to lean more on my collaborators in some of the ongoing projects <img src="https://s.w.org/images/core/emoji/13.1.0/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /></p>



<p>Can chairs bring their own personality into the conference? How? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: One area where the chairs enjoy freedom is in selecting the keynote and tutorial speakers. I’m biased, of course, but I think we chose very well, and all of these speakers (Joelle Pineau, Shay Moran, and Costis Daskalakis) gave excellent talks (they were recorded—check them out if you missed them)! We also were fortunate to be able to work with amazing partners who organized the mentoring workshop (Surbhi Goel, Nika Hagtalab, and Ellen Vitercik) and the Women in ML Theory event (Tosca Lechner and Ruth Urner). These aspects of the conference beyond the papers are a way for the chairs to express their priorities.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: The chairs have a lot of freedom in choosing how to run the review process, design the conference program and who else will be involved. So, inevitably, the chairs’ personalities and tastes end up being reflected in the final results. </p>



<p>Does the online conference impact the chair job? How? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: Typically, the PC chairs build the program, and then many details of organizing and running the conference get handed off to local organization chairs. But this year, since ALT was held virtually, there were many unusual tasks that fell to the PC chairs—not just the obvious ones like choosing the technologies and format and negotiating those contracts, but smaller things like chasing down authors who failed to upload their recordings, and developing instructions for people in various roles to interact with the conference platform.</p>



<p style="color: #0000ff;" class="has-text-color">In addition, COVID times placed strains on many people, which made it more challenging to recruit PC members, and resulted in a higher than usual rate of late reviews and PC drop-outs, which of course left us scrambling.</p>



<p>What motivates you to spend time on a conference service?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: We all rely on the conference system for our personal professional advancement, and for the advancement of our field as a whole. So we all owe that system our service, of course, according to our abilities, availability, and seniority. Also, it’s fun to get this different perspective on the conference review process and on the field. And it’s an honor to be entrusted with shepherding a conference for a year and hopefully nurturing its growth.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s a mix of (1) contribution to the community I’m a member of and, perhaps, an opportunity to improve some of its processes (2) a learning experience that gives one a higher-level view of the research that is happening and people who do it (3) honor and recognition that come with the job.</p>



<p><strong>Awards </strong></p>



<p>How do you decide which papers were chosen as awards? </p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: A necessary condition for a paper to receive an award is that at least one of the PC members/reviewers assigned to the paper is excited about the results.  So we start by looking at papers with the highest scores (typically all papers that received at least one “strong accept”) and reading their reviews. This allowed us to narrow down the list to a set of 5-6 candidates. From those we selected the winners by learning more about the results and selecting those, we found the most significant and interesting for the community.</p>



<p><strong>The review process</strong></p>



<p>What are your thoughts about the current peer review process in ALT? What are the downsides and advantages? Do you have suggestions for improvement?</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: It is common that confidence in correctness of the results in a conference submission is based on higher-level sanity checks and general intuition of the reviewers. Naturally, the more interesting and important result the more likely it is to be scrutinized. At this ALT we did not run into a situation where the authors’ reputation affected our confidence in the correctness of the results. In case of concerns about correctness of an interesting result we would ask either an expert on the PC or an external expert to try to verify the result.</p>



<p class="has-luminous-vivid-orange-color has-text-color">ALT currently relies on a traditional theory conference model of reviewing and for a typical submission has several PC members who are experts in the subarea. The reviewing load is also relatively light (8 papers per PC member). So I think that the overall reviewing quality is pretty much as good as it gets in ML (and is similar to COLT). Naturally, the model is not perfect and there is still variation in the quality of individual reviews. This year many more reviewers and PC members were under unusual time pressure due to the pandemic so perhaps the variation was higher than usual.</p>



<p><strong>The future</strong></p>



<p>What are your suggestions for the next chair? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: Make sure you have a good co-chair. <img src="https://s.w.org/images/core/emoji/13.1.0/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> Vitaly has been a great partner for this process—fun to work with, reliable, always willing to pitch in even on the less-fun tasks, and I have great respect for his technical perspective.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that diversity of perspectives and expertise is useful in several ways. Most notably, it gives the chairs a wider network of people to select the PC from. But I completely agree with Katrina, that the most important thing is the ability of co-chairs to work well together: after all, it’s a lot of work and complicated decisions that need to be made jointly. Here, I couldn’t have asked for more: Katrina is amazing both professionally and personally. Working with her was definitely the highlight of being the ALT co-chair and learned a lot from her in the process as well.</p></div>







<p class="date">
by Gautam Kamath <a href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/"><span class="datestr">at May 04, 2021 04:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/">TCS+ talk: Wednesday, May 12 — Santhoshini Velusamy, Harvard University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://scholar.harvard.edu/santhoshiniv/home"><strong>Santhoshini Velusamy</strong></a> from Harvard University will speak about “<em>Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: A maximum constraint satisfaction problem, Max-CSP(F), is specified by a finite family of constraints F, where each constraint is of arity k. An instance of the problem on n variables is given by m applications of constraints from F to length-k subsequences of the n variables, and the goal is to find an assignment to the n variables that satisfies the maximum number of constraints. The class of Max-CSP(F) includes optimization problems such as Max-CUT, Max-DICUT, Max-3SAT, Max-q-Coloring, Unique Games, etc.</p>
<p>In this talk, I will present our recent dichotomy theorem on the approximability of Max-CSP(F) for every finite family F, in the single-pass dynamic streaming setting. In this setting, at each time step, a constraint is either added to or deleted from the stream. In the end, the streaming algorithm must estimate the maximum number of constraints that can be satisfied using space that is only polylogarithmic in n. No background in streaming algorithms or constraint satisfaction problems will be needed to enjoy this talk!</p>
<p>The talk will be based on <a href="https://eccc.weizmann.ac.il/report/2021/011/">this paper</a>, and <a href="https://eccc.weizmann.ac.il/report/2021/063/">this paper</a> with Chi-Ning Chou, Alexander Golovnev, and Madhu Sudan.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/"><span class="datestr">at May 04, 2021 06:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=94">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/04/29/thursday-may-6th-hamed-hassani-from-university-of-pennsylvania/">Thursday May 6th — Hamed Hassani  from University of Pennsylvania</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, May 6</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 19:00 Central European Time, 18:00 UTC).  <strong><a href="https://www.seas.upenn.edu/~hassani/index.html" target="_blank" rel="noreferrer noopener">Hamed Hassani</a></strong> from<strong> Univeristy of Pennsylvania</strong> will speak about “Learning Robust Models: How does the Geometry of Perturbations Play a Role?”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: In this talk, we will focus on the emerging field of (adversarially) robust machine learning. The talk will be self-contained and no particular background on robust learning will be needed. Recent progress in this field has been accelerated by the observation that despite unprecedented performance on clean data, modern learning models remain fragile to seemingly innocuous changes such as small, norm-bounded additive perturbations.  Moreover, recent work in this field has looked beyond norm-bounded perturbations and has revealed that various other types of distributional shifts in the data can significantly degrade performance.  However, in general our understanding of such shifts is in its infancy and several key questions remain unaddressed.</p>



<p class="has-text-align-justify">The goal of this talk is to explain why robust learning paradigms have to be designed — and sometimes rethought — based on the geometry of the input perturbations.  We will cover a wide range of perturbation geometries from simple norm-bounded perturbations, to sparse, natural, and more general distribution shifts.  As we will show, the geometry of the perturbations necessitates fundamental modifications to the learning procedure as well as the architecture in order to ensure robustness. In the first part of the talk, we will discuss our recent theoretical results on robust learning with respect to various geometries, along with fundamental tradeoffs between robustness and accuracy, phase transitions, etc.  The remaining portion of the talk will be about developing practical robust training algorithms and evaluating the resulting (robust) deep networks against state-of-the-art methods on naturally-varying, real-world datasets.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/04/29/thursday-may-6th-hamed-hassani-from-university-of-pennsylvania/"><span class="datestr">at April 29, 2021 09:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/04/21/tcs-talk-wednesday-april-28-ronen-eldan-weizmann-institute/">TCS+ talk: Wednesday, April 28 — Ronen Eldan, Weizmann Institute</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://www.wisdom.weizmann.ac.il/~ronene/"><strong>Ronen Eldan</strong></a> from the Weizmann Institute will speak about “<em>Localization, stochastic localization, and Chen’s recent breakthrough on the Kannan-Lovasz-Simonivits conjecture</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: The Kannan-Lovasz and Simonovits (KLS) conjecture considers the following isoperimetric problem on high-dimensional convex bodies: Given a convex body <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="K" class="latex" />, consider the optimal way to partition it into two pieces of equal volume so as to minimize their interface. Is it true that up to a universal constant, the minimal partition is attained via a hyperplane cut? Roughly speaking, this question can be thought of as asking “to what extent is a convex set a good expander”?</p>
<p>In analogy to expander graphs, such lower bounds on the capacity would imply bounds on mixing times of Markov chains associated with the convex set, and so this question has direct implications on the complexity of many computational problems on convex sets. Moreover, it was shown that a positive answer would imply Bourgain’s slicing conjecture.</p>
<p>Very recently, Yuansi Chen obtained a striking breakthrough, nearly solving this conjecture. In this talk, we will overview some of the central ideas used in the proof. We will start with the classical concept of “localization” (a very useful tool to prove concentration inequalities) and its extension, stochastic localization – the main technique used in the proof.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/04/21/tcs-talk-wednesday-april-28-ronen-eldan-weizmann-institute/"><span class="datestr">at April 21, 2021 04:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=39">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Introducing ALT Highlights 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The 32nd International Conference on Algorithmic Learning Theory (<a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>) just wrapped up, featuring a wide selection of exciting results at the frontiers of learning theory. The <a href="http://proceedings.mlr.press/v132/">proceedings</a> and all <a href="https://www.youtube.com/channel/UC7wMo5OivSnsQJNfZm8zmJQ/videos">talk recordings</a> are available online for perusal. </p>



<p>Did you miss out on the conference? Don’t have time to go through all the proceedings? Fear not, the <a href="https://let-all.com/">Learning Theory Alliance</a> is pleased to bring you ALT Highlights, a series of blog posts spotlighting various happenings at ALT, including plenary talks, tutorials, trends in learning theory, and more!</p>



<p>In order to reach a broad audience in learning theory, we’ll be releasing these posts across a number of different blogs. All content will be linked from this post, so be sure to bookmark this post so you don’t miss anything!</p>



<p>ALT Highlights will be brought to you by an amazing team of junior researchers, written by <a href="https://people.eecs.berkeley.edu/~kush/">Kush Bhatia</a>, <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>, <a href="https://web.stanford.edu/~mglasgow/">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>, <a href="https://www.ttic.edu/students/">Keziah Naggita</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/">Cyrus Rashtchian</a>, and overseen and edited by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. </p>



<p>Links to articles:<br />1. <a href="https://hunch.net/?p=13762948">An Interview with Joelle Pineau</a><br />2. <a href="https://differentialprivacy.org/alt-highlights/">An Equivalence between Private Learning and Online Learning</a><br />3. <a href="https://windowsontheory.org/2021/04/30/alt-highlights-equilibrium-computation-and-the-foundations-of-deep-learning/">Equilibrium Computation and the Foundations of Deep Learning</a><br />4. <a href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/">An Interview with the PC Chairs of ALT 2021</a><br />5. <a href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/">A Report on the First ALT Mentoring Workshop</a><br />6. <a href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/">Trends in Machine Learning Theory</a></p></div>







<p class="date">
by admin <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/"><span class="datestr">at April 20, 2021 04:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/">TOC: a Personal Perspective (2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Editorial note</strong>: this post has been written in celebration of 25 years for “<a href="http://www.wisdom.weizmann.ac.il/~oded/toc-sp.html">TOC: a Scientific Perspective (1996),</a>” by <a href="http://www.wisdom.weizmann.ac.il/~oded/">Oded Goldreich</a> and <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>. In the process, I have been made aware of a Facebook discussion from a few weeks ago (which I don’t know how to link to), and to <a href="https://simons.berkeley.edu/talks/tbd-271">Avi Wenderson’s recent talk</a> that addresses this discussion (and more). I will not attribute statements from the Facebook discussion to individuals (nor will I specify its initiator), as they may not identify with the statements, when taken out of their original context or in the editorialized version here. In any case, this specific discussion is not the point. Feel free to claim ownership in comments .</p>



<p>———————————–</p>



<p>I’m sure that, like me, you read on social media: “TOC is in crisis, scratch that, it is lost! We do not have agreed-upon challenges (that are not way out of reach) and do not know how to evaluate papers. Therefore our conferences are favoring progress in techniques and favor complicated papers on obscure problems over progress on important problems (in fact, there is shortage of interesting work on relevant problems). Our flagship conferences are broken and newer conferences, that aimed to do better, have become just more of the same. This is not TCS as we remember it!”</p>



<p>There are many points with which I agree. Like others, I have been critical at times about the way we do some things, mourning papers and subfields that our conferences have missed. On several occasions, I have been pushing for change. At times I was successful but in other instances <a href="https://windowsontheory.org/2015/06/08/can-we-get-serious/">my suggestions </a>were deemed radical by the powers that be (and more modest/timid suggestions were adopted). <strong><em>But did TOC really lose its way?</em> </strong> </p>



<p>One of the commenter was saying “I have been hearing these complaints about focs and Stoc for more than ten years. They have come from powerful people that sit on committees. … So why nothing changes?“ This comment strikes a chord with me, but let me revise it and say that I have been hearing such complaints for the last 25 years (since attending my first conference), and it is almost always stated by the powerful people, those that have the responsibility to shape the field.</p>



<p>Following the continuous self-criticism, we are likely to assume that TOC is a dysfunctional field and has been so for many years. But if we look at the research achievements of TOC in the last quarter century, we must conclude that this was a glorious period. And the contributions of TCS were on different fronts. Contributions to applied CS and industry, growing contributions outside of CS as well as progress on fundamental questions within TOC. Said progress was obtained by simple papers and by complex and long papers. By papers developing new techniques, by papers making progress on known problems and by papers that introduced new problems, models or even papers that initiated new subfields. They have been made by breakthrough papers and by long sequences of modest papers. By papers in FOCS/STOC and papers in other conferences. So <strong>if TOC is in a continuous crisis, it is the most wonderful crisis possible</strong>.</p>



<p>During my studies (ages ago), I was intensely attracted to TOC. But at the same time, I felt that the field is under constant external attack. It was claimed that we are not as deep as Math and not as useful as CS. Many fewer universities than today have been hiring theoreticians. The field was grossly underfunded (still underfunded but less grossly) but still calls have been made to reduce funding to any area in which TOC is not directly serving other, more applied areas. The dissonance between my intuitive attraction and external criticism could have deterred me from TOC, but there were incredible leaders of TOC that effectively defended the field and shouted – look, something amazing is happening here. “TOC: a Scientific Perspective (1996),” by Oded Goldreich and Avi Wigderson gave me courage to continue. 25 years later, the case they once made in defense of TOC is so much easier to support (and they kept on making this case throughout the years in essays and <a href="https://www.math.ias.edu/avi/book">books</a>). <a href="https://simons.berkeley.edu/talks/tbd-271">As Avi argued</a>, TOC’s success have brought growth and diversification, influx of young talent, scientific respect, industrial respect, and societal respect. <strong>We should do better on self-respect.</strong></p>



<p>I am of course not advocating resting on our laurels’. Like in Alice’s adventures, in our fast field, it takes all the running we can do, to keep in the same place. If we want to get somewhere else, we must run at least twice as fast as that! Constructive criticism is a good thing but our tendency for alarmist/defeatist cries is not serving us well. As someone who grew (scientifically) in an atmosphere of struggle, I am grateful for the progress made in establishing our field by the generations that preceded me. We shouldn’t take for granted how easy we have it. But more importantly, confidence in our field and optimism towards our future are important for our impact on the world (I discussed one aspect of this <a href="https://theorydish.blog/2019/06/24/on-the-importance-of-disciplinary-pride-for-multidisciplinary-collaboration/">here</a>). Finally, thinking of students interested in TOC today and hearing the most powerful people in the field announcing that it is lost. I ask myself, who are the Avi and Oded that will give them the needed courage and optimism? The answer is still that their Avi and Oded are the very same Avi and Oded from my student years. But isn’t it about time that we lend a hand?</p>



<p></p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/"><span class="datestr">at April 15, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1613">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/">Ensuring equity in high-stakes online advertising</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility down to advertisers rather than addressing it at the platform leaves manipulating a complex system to those least aware of the system’s inner workings. She then proposes a simpler, more unified solution to this problem: advertising slots should be either targetable or untargetable, and high-stakes ads should be in the untargeted segment. Finally, the post concludes with a discussion of how this segmentation need not cost these systems substantial revenue if reserve prices are used appropriately.</p>



<p></p>



<p><a href="https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420">https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420</a></p></div>







<p class="date">
by jamiemorgenstern <a href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/"><span class="datestr">at April 07, 2021 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=547">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/">TCS+ talk: Wednesday, April 14 — Andrea Lincoln, UC Berkeley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: left;">The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://sites.google.com/site/andrealiresume/home"><strong>Andrea Lincoln</strong></a> from UC Berkeley will speak about “<em>New Techniques for Proving Fine-Grained Average-Case Hardness</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In this talk I will cover a new technique for worst-case to average-case reductions. There are two primary concepts introduced in this talk: “factored” problems and a framework for worst-case to average-case fine-grained (WCtoACFG) self reductions.</p>
<p>We will define new versions of OV, kSUM and zero-k-clique that are both worst-case and average-case fine-grained hard assuming the core hypotheses of fine-grained complexity. We then use these as a basis for fine-grained hardness and average-case hardness of other problems. Our hard factored problems are also simple enough that we can reduce them to many other problems, e.g. to edit distance, k-LCS and versions of Max-Flow. We further consider counting variants of the factored problems and give WCtoACFG reductions for them for a natural distribution.</p>
<p>To show hardness for these factored problems we formalize the framework of [Boix-Adsera et al. 2019] that was used to give a WCtoACFG reduction for counting k-cliques. We define an explicit property of problems such that if a problem has that property one can use the framework on the problem to get a WCtoACFG self reduction. In total these factored problems and the framework together give tight fine-grained average-case hardness for various problems including the counting variant of regular expression matching.</p>
<p>Based on joint work with Mina Dalirrooyfard and Virginia Vassilevska Williams.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/"><span class="datestr">at April 06, 2021 08:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
