<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 29, 2020 10:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/29/postdoctoral-researchers-at-millennium-institute-for-foundational-research-on-data-imfd-chile-apply-by-january-7-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/29/postdoctoral-researchers-at-millennium-institute-for-foundational-research-on-data-imfd-chile-apply-by-january-7-2020/">Postdoctoral Researchers at Millennium Institute for Foundational Research on Data, IMFD Chile (apply by January 7, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Millennium Institute for Foundational Research on Data (IMFD Chile, <a href="http://www.imfd.cl" rel="nofollow">http://www.imfd.cl</a>) offers a postdoc position to advance the understanding of theoretical aspects of neural network architectures, in particular, its expressive and computational power. The coordinators of this project are Professors Pablo Barceló (<a href="http://pbarcelo.ing.uc.cl/">http://pbarcelo.ing.uc.cl/</a>) and Jorge Pérez (<a href="https://users.dcc.uchile.cl/~jperez/">https://users.dcc.uchile.cl/~jperez/</a>).</p>
<p>Website: <a href="https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit">https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit</a><br />
Email: pbarcelo@ing.puc.cl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/29/postdoctoral-researchers-at-millennium-institute-for-foundational-research-on-data-imfd-chile-apply-by-january-7-2020/"><span class="datestr">at March 29, 2020 04:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=410">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/28/tcs-talk-wednesday-april-1-venkat-guruswami-cmu/">TCS+ talk: Wednesday, April 1 — Venkat Guruswami, CMU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 1th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Venkat Guruswami</strong> from CMU will speak about “<em>Arıkan meets Shannon: Polar codes with near-optimal convergence to channel capacity</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We establish a constructive version of Shannon’s noisy coding theorem for binary codes, with information rate converging near-optimally fast to channel capacity as a function of code length. Specifically, let <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=fff&amp;fg=444444&amp;s=0" alt="W" class="latex" title="W" /> be an arbitrary binary-input memoryless symmetric channel with Shannon capacity <img src="https://s0.wp.com/latex.php?latex=I%28W%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="I(W)" class="latex" title="I(W)" />, and fix any <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3E0&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\delta &gt;0" class="latex" title="\delta &gt;0" />. We construct, for any sufficiently small <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon+%3E0&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\varepsilon &gt;0" class="latex" title="\varepsilon &gt;0" />, binary linear codes of block length <img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Cvarepsilon%5E%7B2%2B%5Cdelta%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="O(1/\varepsilon^{2+\delta})" class="latex" title="O(1/\varepsilon^{2+\delta})" /> and rate <img src="https://s0.wp.com/latex.php?latex=I%28W%29-%5Cvarepsilon&amp;bg=fff&amp;fg=444444&amp;s=0" alt="I(W)-\varepsilon" class="latex" title="I(W)-\varepsilon" /> that enable reliable communication on <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=fff&amp;fg=444444&amp;s=0" alt="W" class="latex" title="W" /> with quasi-linear time encoding and decoding. Shannon’s theorem implies the existence of such codes (without efficient constructions or decoding) with block length <img src="https://s0.wp.com/latex.php?latex=O%281%2F%5Cvarepsilon%5E2%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="O(1/\varepsilon^2)" class="latex" title="O(1/\varepsilon^2)" />. This quadratic dependence on the gap epsilon to capacity is known to be best possible. Previously such a construction was only known for the case of the erasure channel.</p>
<p>Our codes are a variant of Arıkan’s polar codes based on multiple carefully constructed local kernels, one for each intermediate channel that arises in the decoding. A key technical ingredient in the analysis is a strong converse to the noisy coding theorem that shows extreme unpredictability of even a single message bit when communicating via random linear codes at rates slightly above capacity.</p>
<p>Joint work with Andrii Riazanov and Min Ye.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/28/tcs-talk-wednesday-april-1-venkat-guruswami-cmu/"><span class="datestr">at March 28, 2020 04:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7660">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/26/technology-for-theory-covid-19-edition/">Technology for theory: COVID-19 edition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The new coronavirus upended much of society, including our little corner of it. I believe at this point almost all theorists are teaching and doing research at home, and I thought it would be good to share some of the tools we use for doing so. Below I will describe my setup, but I hope other people share theirs too.</p>



<h2>Teaching and virtual whiteboards</h2>



<p>I am teaching using Zoom and using an iPad pro with a pencil to simulate a whiteboard. I use a laptop to connect to Zoom and for the camera, laptop, and chat window, and then join the iPad.There are several ways to connect an iPad to a Zoom session:</p>



<ol><li>Join the session from the iPad separately using the iPad Zoom app. (To do so you need to logout of your other account.)</li><li>Within desktop Zoom you can choose “share screen” and then one of the option is to join an iPad connected to the same wireless network as the laptop/desktop and use “screen mirroring”.</li><li>You can use a wired connection, which is either by just connecting through USB (in a Mac) or a complex combination of combining an adapter to take an HDMI signal out of an iPad with an HDMI capture card to stream this signal to the computer.</li></ol>



<p>I use option 2. The main reason I prefer it is because the application I use for a whiteboard –  <a href="https://www.goodnotes.com/">GoodNotes</a> – has a <a href="https://support.goodnotes.com/hc/en-us/articles/360001100576-How-to-use-Presentation-Mode">presentation mode</a> that behaves differently when you are connected to an external display or use AirPlay (which is what option 2 does). In this presentation mode the students don’t see your interface, and so you can Zoom, go to the page selector and more without it disturbing what they see. GoodNotes also has a great “laser pointer”. I set the pages at a landscape orientation,  and pre-write a good chunk of what I plan to present before the lecture. I also use the ability to “duplicate pages” to achieve the PowerPoint like effect of gradual reveal.</p>



<p>It is not perfect – I’ve discovered that the screen share sometimes stops refreshing and I need to leave GoodNotes and return to it for it to go back.</p>



<p>Monitoring the chat window and raised hands in Zoom is non-trivial. It helps a lot that I have a teaching assistant that participates in lecture and notices if I missed something. </p>



<p>Some people say that a “matte screen protector” such as <a href="https://paperlike.com/">PaperLike</a> makes the iPad more comfortable to write on – haven’t yet tried it.</p>



<p>I have a good Webcam (<a href="https://www.amazon.com/Logitech-BRIO-Conferencing-Recording-Streaming/dp/B01N5UOYC4">Logitech Brio</a>) but at the moment I’m not using it since it seems too taxing on my laptop and so I went back to the native webcam. I have a very nice wireless headset/mic combo (<a href="https://www.amazon.com/Jabra-Bluetooth-Headphones-Including-Packaging/dp/B072JWYJMC">Jabra Evolve 75</a>) that I am constantly using and have been very happy with. I particularly like the feature of being able to unmute and mute yourself by raising and lowering the mike. </p>



<p></p>



<h2>Research</h2>



<p>For research Slack continues to extremely useful. For working jointly on a paper <a href="https://www.overleaf.com/">Overleaf </a> is of course great, but for maintaining a shared document it sometimes useful to use simpler platform that are not full fledged LaTeX. Some options include:</p>



<ul><li><a href="https://hackmd.io/">Hackmd.io</a> for markdown (supports LaTeX math)</li><li>Google docs of course. I heard about the <a href="https://gsuite.google.com/marketplace/app/autolatex_equations/850293439076">Auto-LaTeX plugin</a> but haven’t used it yet.</li><li><a href="https://www.dropbox.com/paper">Dropbox Paper</a> supports LaTeX natively.</li></ul>



<p><a href="https://jamboard.google.com/">Google JamBoard</a> is an interactive whiteboard, also with an <a href="https://apps.apple.com/us/app/jamboard/id1143591418">iPad app</a>. I haven’t tried it yet but it seems promising.</p>



<p></p>



<h2>Keeping children busy</h2>



<p>For many people I imagine childcare is one of the most challenging aspects. At the moment at least the Cambridge Public Schools are not keeping my kids too busy. While probably most of their time is spent in non educational pursuits, we try to also encourage (i.e., bribe/extort/threaten/beg) them to do some learning. If your kids are interested in math, I highly recommend the courses offered by the <a href="https://artofproblemsolving.com/">Art of Problem Solving</a> (they also have a theory connection: one of their books was co-authored by theorist Ravi Boppana). For younger kids you can also try their <a href="https://beastacademy.com/">Beast Academy</a>.</p>



<p>The AOPS program is not free but over the years my kids (especially my 13 year old daughter Alma) have also spent a lot of time on the free and amazing <a href="https://www.khanacademy.org/">Khan Academy</a>. In fact, last year she was inspired enough by Khan’s JavaScript course to write the following poem which (for obvious reasons)  moved me very much:</p>



<p>Tending / Alma Barak</p>



<p>My mind is a desert<br />of boredom<br />of blankness<br />of leaning-back-in-your-chairness<br />and of simply-staring-at-the-ceilingness<br />I kick at a crumpled<br />deserted baby-blonde post-it<br />with my spotted socked feet<br />waiting for Aba to come.</p>



<p>We dive when he comes<br />into the seas of Khan<br />free-styling our way<br />into the lakes of Java<br />we pass codes we already<br />knew<br />while loops<br />for loops<br />arrays<br />reminding me of what I’d learned</p>



<p>We frog-kick to an<br />uncharted<br />unfamiliar<br />lagoon of code I don’t know<br />sometimes I get swept away by currents<br />of confusion<br />but my aba, my dad<br />grabs my hand<br />and shows me the way through<br />teaching me<br />tending to me<br />washing away the sands of boredom with the sea of Khan.</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/26/technology-for-theory-covid-19-edition/"><span class="datestr">at March 26, 2020 12:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7657">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/25/new-cs-theory-talk-aggragator/">New CS theory talk aggragator</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Shcachar Lovett has put together a new website aggregating information about virtual talks in CS theory: <a href="https://cstheorytalks.wordpress.com/">https://cstheorytalks.wordpress.com/</a></p>



<p> It has a Google calendar that people can add to their own, and a form to submit a new talk that automatically gets added to the Google calendar. </p>



<p>This can be a fantastic resource these days that almost no one can travel – please publicize this and also submit to it talks that you are organizing.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/25/new-cs-theory-talk-aggragator/"><span class="datestr">at March 25, 2020 05:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/24/full-professor-w3-at-tu-dortmund-university-apply-by-april-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/24/full-professor-w3-at-tu-dortmund-university-apply-by-april-15-2020/">Full Professor (W3)  at TU Dortmund University (apply by April 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at TU Dortmund University (<a href="https://tu-dortmund.de/en">https://tu-dortmund.de/en</a>) is seeking to fill the position of a Full Professor Position (W3) in “Efficient Algorithms and Complexity Theory” commencing as soon as possible.</p>
<p>Website: <a href="https://service.tu-dortmund.de/documents/18/2120803/Professor+W3+in+Efficient+Algorithms+and+Complexity+Theory.pdf/64785b24-b188-fbaa-e337-a2405661868f">https://service.tu-dortmund.de/documents/18/2120803/Professor+W3+in+Efficient+Algorithms+and+Complexity+Theory.pdf/64785b24-b188-fbaa-e337-a2405661868f</a><br />
Email: thomas.schwentick@tu-dortmund.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/24/full-professor-w3-at-tu-dortmund-university-apply-by-april-15-2020/"><span class="datestr">at March 24, 2020 10:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7654">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/19/focs-deadline-pushed-back-6-days/">FOCS deadline pushed back 6 days</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From Sandy Irani, FOCS 2020 PC chair:</p>



<p>In light of the very unusual developments in the world due to the spread of Covid-19 and the disruption it is causing to many people in our field, especially those with young children at home, the FOCS PC has decided to push back the final deadline for papers by six days. We would have liked to do more, but we still are trying to stick to our timeline for notification since that date is coordinated with other deadlines in the theory community. In addition, some members of the committee are also affected by this crisis and there is concern that we may not be able to do our job as a committee in a shorter time frame. Pre-registration (including titles and abstracts) is still required by the original deadline. Here are the new dates:</p>



<p><strong>Pre-registration (including titles and abstracts):</strong> Thursday <strong>April 9</strong>, 11:59 PM (EDT)</p>



<p><strong>Final Paper Submission:</strong> Wednesday <strong>April 15</strong>, 11:59 PM (EDT)</p>



<p>Conference url: <a href="https://focs2020.cs.duke.edu/" target="_blank" rel="noreferrer noopener">https://focs2020.cs.duke.edu/</a></p>



<p>We hope you all stay healthy!</p>



<p>–Sandy Irani, for the FOCS 2020 Committee</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/19/focs-deadline-pushed-back-6-days/"><span class="datestr">at March 19, 2020 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=39">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/03/19/friday-march-27-sujay-sanghavi-from-ut-austin/">Friday, March 27 — Sujay Sanghavi from UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The second Foundations of Data Science virtual talk will take place next Friday, March 27th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Sujay Sanghavi</strong> from University of Texas at Austin will speak about “<em>Towards Model Agnostic Robustness</em>”.</p>



<p><strong>Abstract</strong>: It is now common practice to try and solve machine learning problems by starting with a complex existing model or architecture, and fine-tuning/adapting it to the task at hand. However, outliers, errors or even just sloppiness in training data often lead to drastic drops in performance. </p>



<p>We investigate a simple generic approach to correct for this, motivated by a classic statistical idea: trimmed loss. This advocates jointly (a) selecting which training samples to ignore, and (b) fitting a model on the remaining samples. As such this is computationally infeasible even for linear regression. We propose and study the natural iterative variant that alternates between these two steps (a) and (b) – each of which individually can be easily accomplished in pretty much any statistical setting. We also study the batch-SGD variant of this idea. We demonstrate both theoretically (for generalized linear models) and empirically (for vision and NLP neural network models) that this effectively recovers accuracy in the presence of bad training data.</p>



<p>This work is joint with Yanyao Shen and Vatsal Shah and appears in NeurIPS 2019, ICML 2019 and AISTATS 2020.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/03/19/friday-march-27-sujay-sanghavi-from-ut-austin/"><span class="datestr">at March 19, 2020 02:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/03/18/private-and-secure-distributed-matrix-multiplication/">Private and Secure Distributed Matrix Multiplication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Machine learning on big data sets takes a significant amount of computational power, so it  is often necessary to offload some of the work to external distributed systems, such as an Amazon EC2 cluster. It is useful to be able to utilize external resources for computation tasks while keeping the actual data <em>private</em> and<em> secure</em>. In particular, matrix multiplication is an essential step in many machine learning processes, but the owner of the matrices may have reasons to keep the actual values protected.</p>



<p>In this post, we’ll discuss four works about secure distributed computation. First, we’ll talk about a method of using MDS (maximum distance separable) error correcting codes to add security and privacy to general data storage (“<a href="https://arxiv.org/pdf/1808.07457.pdf">Cross Subspace Alignment and the Asymptotic Capacity of X-Secure T-Private Information Retrieval”</a> by Jia, Sun, Jafar). </p>



<p>Then we’ll discuss method of adapting a coding strategy for straggler mitigation (<a href="http://papers.nips.cc/paper/7027-polynomial-codes-an-optimal-design-for-high-dimensional-coded-matrix-multiplication.pdf">“Polynomial codes: an optimal design for high-dimensional coded matrix multiplication”</a> by Yu, Qian, Maddah-Ali, Avestimehr) in matrix multiplication to instead add security or privacy (<a href="https://uweb.engr.arizona.edu/~wchang/Globecom-SecureMM-2018.pdf">“On the capacity of secure distributed matrix multiplication”</a> by Chang, Tandon and <a href="https://ieeexplore-ieee-org.stanford.idm.oclc.org/abstract/document/8832193">“Private Coded Matrix Multiplication”</a> by Kim, Yang, Lee)</p>



<p>Throughout this post we will use variations on the following communication model:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig1.png?w=1024" alt="" width="404" class="wp-image-1572" height="192" /></figure>



<p>The data in the grey box is only given to the master, so workers only have access to what they receive (via green arrows). Later on we will also suppose the workers have a shared library not available to the master. The workers do not communicate with each other as part of the computation, but we want to prevent them from figuring out anything about the data if they do talk to each other.</p>



<p> This model is related to <em>private computation</em> but not exactly the same. We assume the servers are “honest but curious”, meaning they won’t introduce malicious computations. We also only require the master to receive the final result, and don’t need to protect any data from the master. This is close to the BGW scheme ([Ben-Or, Goldwasser, Wigderson ’88]), but we do not allow workers to communicate with each other as part of the computation of the result.</p>



<p> We consider <em>unconditional</em> or <em>information-theoretic</em> security, meaning the data is protected even if the workers have unbounded computational power. Furthermore, we will consider having <em>perfect  secrecy</em>, in which the mutual information between the information revealed to the workers and the actual messages is zero.</p>



<h2>X-Secure T-Private Information Retrieval</h2>



<p> Before we get into matrix-matrix multiplication, consider the problem of storing information on the workers to be retrieved by the master, such that it is “protected.” What do we mean by that? [Jia, Sun, and Jafar ’19] define X-secure T-private information retrieval as follows: </p>



<blockquote class="wp-block-quote"><p>Let <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_{K}" class="latex" title="W_1,...,W_{K}" /> be a data set of messages, such that each <img src="https://s0.wp.com/latex.php?latex=W_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_i" class="latex" title="W_i" /> consists of <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> random bits. A storage scheme of <img src="https://s0.wp.com/latex.php?latex=W_%7B1%7D%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_{1},...,W_{K}" class="latex" title="W_{1},...,W_{K}" /> on <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> nodes is </p><p>1. <em>X-secure</em> if any set of up to <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> servers cannot determine anything about any <img src="https://s0.wp.com/latex.php?latex=W_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_i" class="latex" title="W_i" /> and</p><p>2. <em>T-private </em> if given a query from the user to retrieve some data element <img src="https://s0.wp.com/latex.php?latex=W_%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_{\theta}" class="latex" title="W_{\theta}" />, any set of up to <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="T" class="latex" title="T" /> users cannot determine the value of <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\theta" class="latex" title="\theta" />.</p><cite>[Jia, Sun, and Jafar ’19]</cite></blockquote>



<p>Letting <img src="https://s0.wp.com/latex.php?latex=Q_%7B1%7D%5E%7B%5B%5Ctheta%5D%7D%2C...%2CQ_%7BN%7D%5E%7B%5B%5Ctheta%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Q_{1}^{[\theta]},...,Q_{N}^{[\theta]}" class="latex" title="Q_{1}^{[\theta]},...,Q_{N}^{[\theta]}" /> be the set of queries sent to each node and <img src="https://s0.wp.com/latex.php?latex=S_1%2C...%2CS_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1,...,S_N" class="latex" title="S_1,...,S_N" /> be the information stored on each node  (all vectors of length L), we depict this as:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig2.png?w=1024" alt="" width="452" class="wp-image-1581" height="223" /></figure>



<p>The information theoretic requirements of this system to be correct can be summarized as follows (using notation <img src="https://s0.wp.com/latex.php?latex=S_%7B%5B1%3AN%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_{[1:N]}" class="latex" title="S_{[1:N]}" /> for set <img src="https://s0.wp.com/latex.php?latex=S_1%2C...%2CS_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1,...,S_N" class="latex" title="S_1,...,S_N" />):</p>



<figure class="wp-block-table"><table><tbody><tr><td><strong>Property</strong></td><td><strong>Information Theoretic Requirement</strong></td></tr><tr><td>Data messages are size <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> bits</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%29%3DH%28W_2%29%3D....%3DH%28W_K%29+%3D+L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1)=H(W_2)=....=H(W_K) = L" class="latex" title="H(W_1)=H(W_2)=....=H(W_K) = L" /></td></tr><tr><td>Data messages are independent</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%2C...%2CW_K%29+%3D+KL&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1,...,W_K) = KL" class="latex" title="H(W_1,...,W_K) = KL" /></td></tr><tr><td>Data can be determined from the stored information</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_1%2C....%2CW_K%29%7CS_%7B%5B1%3AN%5D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_1,....,W_K)|S_{[1:N]}) = 0" class="latex" title="H(W_1,....,W_K)|S_{[1:N]}) = 0" /></td></tr><tr><td>User has no prior knowledge of server data</td><td><img src="https://s0.wp.com/latex.php?latex=I%28S_%7B%5B1%3AN%5D%7D%3BQ%5E%7B%5B%5Ctheta%5D%7D_%7B%5B1%3AN%5D%7D%2C%5Ctheta%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(S_{[1:N]};Q^{[\theta]}_{[1:N]},\theta) = 0" class="latex" title="I(S_{[1:N]};Q^{[\theta]}_{[1:N]},\theta) = 0" /></td></tr><tr><td>X-Security</td><td><img src="https://s0.wp.com/latex.php?latex=I%28S_%7B%5Cmathcal%7BX%7D%7D%3BW_1%2C...%2CW_K%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(S_{\mathcal{X}};W_1,...,W_K) = 0" class="latex" title="I(S_{\mathcal{X}};W_1,...,W_K) = 0" />, <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%5Cmathcal%7BX%7D%5Csubset+%5B1%3AN%5D%2C%7C%5Cmathcal%7BX%7D%7C%3DX&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\forall \mathcal{X}\subset [1:N],|\mathcal{X}|=X" class="latex" title="\forall \mathcal{X}\subset [1:N],|\mathcal{X}|=X" /></td></tr><tr><td> T-Privacy</td><td><img src="https://s0.wp.com/latex.php?latex=I%28Q_%7B%5Cmathcal%7BT%7D%7D%5E%7B%5B%5Ctheta%5D%7D%2CS_%7B%5Cmathcal%7BT%7D%7D%3B+%5Ctheta%29+%3D+0%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="I(Q_{\mathcal{T}}^{[\theta]},S_{\mathcal{T}}; \theta) = 0," class="latex" title="I(Q_{\mathcal{T}}^{[\theta]},S_{\mathcal{T}}; \theta) = 0," />  <img src="https://s0.wp.com/latex.php?latex=%5Cforall+%5Cmathcal%7BT%7D+%5Csubset+%5B1%3AN%5D%2C+%7C%5Cmathcal%7BT%7D%7C%3DT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\forall \mathcal{T} \subset [1:N], |\mathcal{T}|=T" class="latex" title="\forall \mathcal{T} \subset [1:N], |\mathcal{T}|=T" /></td></tr><tr><td>Nodes answer only based on their data and received query </td><td><img src="https://s0.wp.com/latex.php?latex=H%28A_n%5E%7B%5B%5Ctheta%5D%7D%7C+Q_n%5E%7B%5B%5Ctheta%5D%7D%2CS_n%29+%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(A_n^{[\theta]}| Q_n^{[\theta]},S_n) =0" class="latex" title="H(A_n^{[\theta]}| Q_n^{[\theta]},S_n) =0" /></td></tr><tr><td>User can decode desired message from answers</td><td><img src="https://s0.wp.com/latex.php?latex=H%28W_%7B%5Ctheta%7D+%7C+A_%7B%5B1%3AN%5D%7D%5E%7B%5B%5Ctheta%5D%7D%2CQ_%7B%5B1%3AN%5D%7D%5E%7B%5B%5Ctheta%5D%7D+%2C%5Ctheta%29+%3D+0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="H(W_{\theta} | A_{[1:N]}^{[\theta]},Q_{[1:N]}^{[\theta]} ,\theta) = 0" class="latex" title="H(W_{\theta} | A_{[1:N]}^{[\theta]},Q_{[1:N]}^{[\theta]} ,\theta) = 0" /></td></tr></tbody></table></figure>



<p>Given these constraints, Jia et al. give bounds on the capacity of the system. Capacity is the maximum rate achievable, where rate is defined as bits requested by the worker (<img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" />, the length of a single message) divided by the number of bits downloaded by the worker. The bounds are in terms of the capacity of T-Private Information Retrieval, (which is the same as the above definition, with only requirement 2).</p>



<blockquote class="wp-block-quote"><p>If <img src="https://s0.wp.com/latex.php?latex=N+%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N \leq X+T" class="latex" title="N \leq X+T" /> then for arbitrary <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" />, <img src="https://s0.wp.com/latex.php?latex=C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BN%7DC_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BNK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="C_{XSTPIR}(N,K,X,T) = \frac{N-X}{N}C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" class="latex" title="C_{XSTPIR}(N,K,X,T) = \frac{N-X}{N}C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" />.</p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%5Cleq+%5Cleft%28%5Cfrac%7BN-X%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle C_{XSTPIR}(N,K,X,T) \leq \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T)" class="latex" title="\displaystyle C_{XSTPIR}(N,K,X,T) \leq \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T)" /></p><p> When <img src="https://s0.wp.com/latex.php?latex=N%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N\leq X+T" class="latex" title="N\leq X+T" />: <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Cleft%28%5Cfrac%7BN-X%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D+%5Cfrac%7BN-X%7D%7BNK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle C_{XSTPIR}(N,K,X,T) = \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" class="latex" title="\displaystyle C_{XSTPIR}(N,K,X,T) = \left(\frac{N-X}{N}\right) C_{TPIR}(N-X,K,T) = \frac{N-X}{NK}" /> </p><p> <img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5Clim_%7BK%5Cto+%5Cinfty%7D+C_%7BXSTPIR%7D%28N%2CK%2CX%2CT%29+%3D+%5Clim_%7BK%5Cto+%5Cinfty%7D+%5Cleft%28%5Cfrac%7BN-K%7D%7BN%7D%5Cright%29+C_%7BTPIR%7D%28N-X%2CK%2CT%29+%3D%5Cbegin%7Bcases%7D%C2%A0+%C2%A01-%28%5Cfrac%7BX%2BT%7D%7BN%7D%29+%26+N%3EX%2BT++%5C%5C%C2%A0++0+%26+N%5Cleq+X%2BT+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle\lim_{K\to \infty} C_{XSTPIR}(N,K,X,T) = \lim_{K\to \infty} \left(\frac{N-K}{N}\right) C_{TPIR}(N-X,K,T) =\begin{cases}   1-(\frac{X+T}{N}) &amp; N&gt;X+T  \\   0 &amp; N\leq X+T \end{cases}" class="latex" title="\displaystyle\lim_{K\to \infty} C_{XSTPIR}(N,K,X,T) = \lim_{K\to \infty} \left(\frac{N-K}{N}\right) C_{TPIR}(N-X,K,T) =\begin{cases}   1-(\frac{X+T}{N}) &amp; N&gt;X+T  \\   0 &amp; N\leq X+T \end{cases}" /></p><cite>[Jia, Sun, and Jafar ’19]</cite></blockquote>



<p>Jia et al. give schemes that achieve these bounds while preserving the privacy and security constraints by introducing random noise vectors into how data is stored and queries are constructed. The general scheme for <img src="https://s0.wp.com/latex.php?latex=N%3EX%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N&gt;X+T" class="latex" title="N&gt;X+T" /> uses <em>cross subspace alignment</em>, which essentially chooses how to construct the stored information and the queries such that the added noise mostly “cancels out” when the master combines all the response from the servers. The scheme for <img src="https://s0.wp.com/latex.php?latex=N%5Cleq+X%2BT&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N\leq X+T" class="latex" title="N\leq X+T" /> is straightforward to explain, and demonstrates the idea of using error correcting codes that treat the random values as the message and the actual data as the “noise.”</p>



<p>For this scheme, the message length <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L" class="latex" title="L" /> is set to <img src="https://s0.wp.com/latex.php?latex=N-X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N-X" class="latex" title="N-X" /> (the number of nodes <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />, minus the maximum number of colluding servers <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" />). First, we generate <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" /> random bit vectors of length <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" />:</p>



<figure class="wp-block-image size-large is-resized"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig3.png?w=856" alt="" width="370" class="wp-image-1590" height="157" /></figure>



<p>Next, apply an <img src="https://s0.wp.com/latex.php?latex=%28N%2CX%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(N,X)" class="latex" title="(N,X)" /> MDS code to <img src="https://s0.wp.com/latex.php?latex=Z_1%2C...%2CZ_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_1,...,Z_K" class="latex" title="Z_1,...,Z_K" /> to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_1%2C..%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_1,..,\bar{Z}_K" class="latex" title="\bar{Z}_1,..,\bar{Z}_K" />, which are <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K" class="latex" title="K" /> encoded vectors of length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig4.png?w=1024" alt="" class="wp-image-1591" /></figure>



<p>For our data <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_K" class="latex" title="W_1,...,W_K" />, we pad each vector with zeros to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BW%7D_1%2C..%2C%5Cbar%7BW%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{W}_1,..,\bar{W}_K" class="latex" title="\bar{W}_1,..,\bar{W}_K" /> of length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig5.png?w=1024" alt="" class="wp-image-1592" /></figure>



<p>Now that the dimensions line up, we can add the two together and store each column <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n" class="latex" title="n" /> at the <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bth%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n^{th}" class="latex" title="n^{th}" /> node:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig6.png?w=1024" alt="" class="wp-image-1593" /></figure>



<p>To access the data, the user downloads all <img src="https://s0.wp.com/latex.php?latex=NK&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="NK" class="latex" title="NK" /> bits. The length <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> string downloaded from  row <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> can be used to decode <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_i" class="latex" title="\bar{Z}_i" />: <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BW%7D_%7BL%2B1%7D%2C%5Cbar%7BW%7D_%7BL%2B2%7D%2C...%2C%5Cbar%7BW%7D_%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{W}_{L+1},\bar{W}_{L+2},...,\bar{W}_{N}" class="latex" title="\bar{W}_{L+1},\bar{W}_{L+2},...,\bar{W}_{N}" /> are all zero, so columns <img src="https://s0.wp.com/latex.php?latex=L%2B1+%3D+N-X%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="L+1 = N-X+1" class="latex" title="L+1 = N-X+1" /> through <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> have the values of <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_%7Bi%2CN-X%2B1%7D%2C...%2C%5Cbar%7BZ%7D_%7Bi%2CN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_{i,N-X+1},...,\bar{Z}_{i,N}" class="latex" title="\bar{Z}_{i,N-X+1},...,\bar{Z}_{i,N}" />. This gives the user <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> values from the MDS code used on each row, so they can decode and get <img src="https://s0.wp.com/latex.php?latex=Z_1%2C...%2CZ_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_1,...,Z_K" class="latex" title="Z_1,...,Z_K" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_%7B1%7D%2C...%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_{1},...,\bar{Z}_K" class="latex" title="\bar{Z}_{1},...,\bar{Z}_K" />. Then a subtraction from the downloaded data gives <img src="https://s0.wp.com/latex.php?latex=W_1%2C...%2CW_%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="W_1,...,W_{K}" class="latex" title="W_1,...,W_{K}" />. Because of the MDS property of the code used to get <img src="https://s0.wp.com/latex.php?latex=%5Cbar%7BZ%7D_1%2C...%2C%5Cbar%7BZ%7D_K&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bar{Z}_1,...,\bar{Z}_K" class="latex" title="\bar{Z}_1,...,\bar{Z}_K" />, this scheme is X-secure and because the user downloads all bits, it is T-private.</p>



<h2>Matrix Multiplication with Polynomial Codes</h2>



<p>We now move on to the task of matrix-matrix multiplication. The methods for secure and private distributed matrix multiplication we will discuss shortly are based on <em>polynomial codes</em>, used by [Yu, Maddah-Ali, Avestimehr ’17] for doing distributed matrix multiplications robust to stragglers. Suppose the master has matrices <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BA%7D%7D+%5Cin+%5Cmathbb%7BF%7D_q%5E%7Bm%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{A}} \in \mathbb{F}_q^{m\times n}" class="latex" title="{\bf{A}} \in \mathbb{F}_q^{m\times n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BB%7D%7D+%5Cin+%5Cmathbb%7BF%7D_q%5E%7Bn+%5Ctimes+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{B}} \in \mathbb{F}_q^{n \times p}" class="latex" title="{\bf{B}} \in \mathbb{F}_q^{n \times p}" /> for some finite field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D_q&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mathbb{F}_q" class="latex" title="\mathbb{F}_q" />, and <img src="https://s0.wp.com/latex.php?latex=m%2Cn%2Cp+%5Cin+%5Cmathbb%7BZ%7D%5E%2B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="m,n,p \in \mathbb{Z}^+" class="latex" title="m,n,p \in \mathbb{Z}^+" />. Assume <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="m" class="latex" title="m" /> and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" /> are divisible by <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />, so we can represent the matrices divided into <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> submatrices: </p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BA%7D%7D%3D+%5Cbegin%7Bbmatrix%7DA_1%5C%5C+A_2+%5C%5C+%5Cvdots+%5C%5C+A_m%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{A}}= \begin{bmatrix}A_1\\ A_2 \\ \vdots \\ A_m\end{bmatrix}" class="latex" title="{\bf{A}}= \begin{bmatrix}A_1\\ A_2 \\ \vdots \\ A_m\end{bmatrix}" />               and        <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BB%7D%7D%3D+%5Cbegin%7Bbmatrix%7DB_1%26+B_2+%26+%5Cdots+%26B_n%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{B}}= \begin{bmatrix}B_1&amp; B_2 &amp; \dots &amp;B_n\end{bmatrix}" class="latex" title="{\bf{B}}= \begin{bmatrix}B_1&amp; B_2 &amp; \dots &amp;B_n\end{bmatrix}" /> </p>



<p>So to recover <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{AB}" class="latex" title="\bf{AB}" />, the master needs each entry of: </p>



<p><img src="https://s0.wp.com/latex.php?latex=%7B%5Cbf%7BAB%7D%7D+%3D+%5Cbegin%7Bbmatrix%7DA_1B_1+%26+A_1B_2+%26+%5Cdots+%26+A_1B_n%5C%5CA_2B_1+%26+A_2B_2+%26+%5Cdots+%26+A_2+B_n%5C%5C%5Cvdots+%26+%5Cvdots+%26%5Cvdots+%26%5Cvdots%5C%5CA_mB_1+%26A_mB_2+%26+%5Cdots%C2%A0+%26+A_mB_n+%5Cend%7Bbmatrix%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bf{AB}} = \begin{bmatrix}A_1B_1 &amp; A_1B_2 &amp; \dots &amp; A_1B_n\\A_2B_1 &amp; A_2B_2 &amp; \dots &amp; A_2 B_n\\\vdots &amp; \vdots &amp;\vdots &amp;\vdots\\A_mB_1 &amp;A_mB_2 &amp; \dots  &amp; A_mB_n \end{bmatrix}." class="latex" title="{\bf{AB}} = \begin{bmatrix}A_1B_1 &amp; A_1B_2 &amp; \dots &amp; A_1B_n\\A_2B_1 &amp; A_2B_2 &amp; \dots &amp; A_2 B_n\\\vdots &amp; \vdots &amp;\vdots &amp;\vdots\\A_mB_1 &amp;A_mB_2 &amp; \dots  &amp; A_mB_n \end{bmatrix}." /></p>



<p>The key idea of polynomial codes is to encode <img src="https://s0.wp.com/latex.php?latex=A_1%2C...%2CA_m&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A_1,...,A_m" class="latex" title="A_1,...,A_m" /> and <img src="https://s0.wp.com/latex.php?latex=B_1%2C...%2CB_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="B_1,...,B_n" class="latex" title="B_1,...,B_n" /> in polynomials <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" /> to be sent to the <img src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i^{th}" class="latex" title="i^{th}" /> worker,  where they are multiplied and the result is returned. The goal of Yu et al. was to create robustness to stragglers, and so they add redundancy in this process so that not all workers need to return a result for the master to be able to determine <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{AB}" class="latex" title="\bf{AB}" />. In particular, only <img src="https://s0.wp.com/latex.php?latex=mn&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="mn" class="latex" title="mn" /> returned values are needed, so <img src="https://s0.wp.com/latex.php?latex=N-mn&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N-mn" class="latex" title="N-mn" /> servers can be slow or fail completely without hurting the computation. This method can be thought of as setting up the encodings of <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{A}" class="latex" title="\bf{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\bf{B}" class="latex" title="\bf{B}" /> so that the resulting multiplications <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BC%7D_1%3D%5Ctilde%7BA%7D_1%5Ctilde%7BB%7D_1%2C...%2C%5Ctilde%7BC%7D_N%3D%5Ctilde%7BA%7D_N%5Ctilde%7BB%7D_N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{C}_1=\tilde{A}_1\tilde{B}_1,...,\tilde{C}_N=\tilde{A}_N\tilde{B}_N" class="latex" title="\tilde{C}_1=\tilde{A}_1\tilde{B}_1,...,\tilde{C}_N=\tilde{A}_N\tilde{B}_N" /> are evaluations of a polynomial with coefficients  <img src="https://s0.wp.com/latex.php?latex=A_1B_1%2CA_1B_2%2C...%2CA_2B_1....%2CA_mB_n&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A_1B_1,A_1B_2,...,A_2B_1....,A_mB_n" class="latex" title="A_1B_1,A_1B_2,...,A_2B_1....,A_mB_n" /> at <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> different points — equivalent to a Reed-Solomon code. </p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig7.png?w=1024" alt="" class="wp-image-1603" /></figure>



<p>This idea is adapted by [Chang, Tandon ’18] to protect the data from colluding servers: noise is incorporated into the encodings such that the number of encoded matrices required to determine anything about the data is greater than the security threshold <img src="https://s0.wp.com/latex.php?latex=X+%3C+N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X &lt; N" class="latex" title="X &lt; N" />. Since the master receives all <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> responses it is able to decode the result of <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{AB}" class="latex" title="\textbf{AB}" />, but no set of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X" class="latex" title="X" /> nodes can decode <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" />, <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" />, or <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{AB}" class="latex" title="\textbf{AB}" />. Similarly, [Kim, Yang, Li ’19] adapts this idea to impose privacy on a matrix-matrix multiplication: workers are assumed to have a shared library <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D_i%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}_i\}_{i=1}^M" class="latex" title="\{{\textbf{B}}_i\}_{i=1}^M" />, and the user would like to multiply <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BA%7D%7D%7B%5Ctextbf%7BB%7D%7D_%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{A}}{\textbf{B}}_{D}" class="latex" title="{\textbf{A}}{\textbf{B}}_{D}" /> for some <img src="https://s0.wp.com/latex.php?latex=D+%5Cin%5B1%3AM%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="D \in[1:M]" class="latex" title="D \in[1:M]" /> without revealing the value of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="D" class="latex" title="D" /> to the workers. The workers encode the entire library such that when the  encoding is multiplied by an encoded input <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}" class="latex" title="\tilde{A}" /> from the master, the result is useful to the master in decoding <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BA%7D%7D%7B%5Ctextbf%7BB%7D%7D_D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{A}}{\textbf{B}}_D" class="latex" title="{\textbf{A}}{\textbf{B}}_D" />.</p>



<p>Chang and Tandon consider the following two privacy models, where up to <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\ell" class="latex" title="\ell" /> servers may collude. The master also has <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K^{(A)}" class="latex" title="K^{(A)}" /> (and in the second model, <img src="https://s0.wp.com/latex.php?latex=K%5E%7B%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="K^{(B)}" class="latex" title="K^{(B)}" />), which are matrices of random values with the same dimensions as <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> (and <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" />). These are used in creating the encodings <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> (and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" />).</p>



<p> <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{B}" class="latex" title="\textbf{B}" /> is public, <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> is private:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig8.png?w=1024" alt="" class="wp-image-1604" /></figure>



<p>Both private:</p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig9.png?w=1024" alt="" class="wp-image-1605" /></figure>



<p>Kim, Yang, and Lee take a similar approach of applying the method of polynomial code to <em>private</em> matrix multiplication. As before, there are <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> workers, but now the master wants to multiply <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> with some <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextbf%7BB%7D%7D_D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textbf{B}}_D" class="latex" title="{\textbf{B}}_D" /> in shared library <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D_i%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}_i\}_{i=1}^M" class="latex" title="\{{\textbf{B}}_i\}_{i=1}^M" /> (all the workers have the shared library). </p>



<p>Since the master isn’t itself encoding <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}\}_{i=1}^M" class="latex" title="\{{\textbf{B}}\}_{i=1}^M" /> it has to tell the workers how to encode the library so that it can reconstruct the desired product. This is done by having the master tell the workers what values of <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\vec{y}" class="latex" title="\vec{y}" /> they should use to evaluate the polynomial that corresponds to encoding each library matrix. We denote the encoding of the library done by each worker <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> as the multivariate polynomial <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h" class="latex" title="h" /> which is evaluated at <img src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%5Ctextbf%7BB%7D%7D%5C%7D_%7Bi%3D1%7D%5EM&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{{\textbf{B}}\}_{i=1}^M" class="latex" title="\{{\textbf{B}}\}_{i=1}^M" /> and the node-specific vector <img src="https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D_%7B%5B1%3AM%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y^{(i)}_{[1:M]}" class="latex" title="y^{(i)}_{[1:M]}" /> to get the node’s encoding, <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BB%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{B}_i" class="latex" title="\tilde{B}_i" />. The worker multiplies this with the encoding of <img src="https://s0.wp.com/latex.php?latex=%5Ctextbf%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\textbf{A}" class="latex" title="\textbf{A}" /> it receives, <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BA%7D_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\tilde{A}_i" class="latex" title="\tilde{A}_i" /> and returns the resulting value <img src="https://s0.wp.com/latex.php?latex=Z_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="Z_i" class="latex" title="Z_i" />. All together, we get the following communication model: </p>



<figure class="wp-block-image size-large"><img src="https://theorydish.files.wordpress.com/2020/03/blog_fig10.png?w=1024" alt="" class="wp-image-1611" /></figure>



<h2>Conclusion</h2>



<p>As we’ve seen, coding techniques originally designed to add redundancy and protect against data loss can also be used to intentionally incorporate noise for data protection. In particular, this can be done when out-sourcing matrix multiplications, making it a useful technique in many data processing and machine learning applications.</p>



<p>References:</p>



<ul><li><a href="https://arxiv.org/pdf/1808.07457.pdf">Jia, Zhuqing, Hua Sun, and Syed Ali Jafar. “Cross Subspace Alignment and the Asymptotic Capacity of  X-Secure T-Private Information Retrieval.” <em>IEEE Transactions on Information Theory</em> 65.9 (2019): 5783-5798.</a></li><li><a href="http://papers.nips.cc/paper/7027-polynomial-codes-an-optimal-design-for-high-dimensional-coded-matrix-multiplication.pdf">Yu, Qian, Mohammad Maddah-Ali, and Salman Avestimehr. “Polynomial codes: an optimal design for high-dimensional coded matrix multiplication.” <em>Advances in Neural Information Processing Systems</em>. 2017.</a></li><li><a href="https://uweb.engr.arizona.edu/~wchang/Globecom-SecureMM-2018.pdf">Chang, Wei-Ting, and Ravi Tandon. “On the capacity of secure distributed matrix multiplication.” <em>2018 IEEE Global Communications Conference (GLOBECOM)</em>. IEEE, 2018.</a></li><li><a href="https://ieeexplore-ieee-org.stanford.idm.oclc.org/abstract/document/8832193">Kim, Minchul, Heecheol Yang, and Jungwoo Lee. “Private Coded Matrix Multiplication.” <em>IEEE Transactions on Information Forensics and Security</em> (2019).</a></li></ul>



<p></p></div>







<p class="date">
by Alex Porter <a href="https://theorydish.blog/2020/03/18/private-and-secure-distributed-matrix-multiplication/"><span class="datestr">at March 18, 2020 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=405">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/18/tcs-talk-wednesday-march-25-dana-moshkovitz-ut-austin/">TCS+ talk: Wednesday, March 25 — Dana Moshkovitz, UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 25th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Dana Moshkovitz</strong> from UT Austin will speak about “<em>Nearly Optimal Pseudorandomness From Hardness</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Existing proofs that deduce BPP=P from circuit lower bounds convert randomized algorithms into deterministic algorithms with a large polynomial slowdown. We convert randomized algorithms into deterministic ones with little slowdown. Specifically, assuming exponential lower bounds against randomized single-valued nondeterministic (SVN) circuits, we convert any randomized algorithm over inputs of length n running in time <img src="https://s0.wp.com/latex.php?latex=t%5Cgeq+n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="t\geq n" class="latex" title="t\geq n" /> to a deterministic one running in time <img src="https://s0.wp.com/latex.php?latex=t%5E%7B2%2B%5Calpha%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="t^{2+\alpha}" class="latex" title="t^{2+\alpha}" /> for an arbitrarily small constant <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+0&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\alpha &gt; 0" class="latex" title="\alpha &gt; 0" />. Such a slowdown is nearly optimal, as, under complexity-theoretic assumptions, there are problems with an inherent quadratic derandomization slowdown. We also convert any randomized algorithm that errs rarely into a deterministic algorithm having a similar running time (with pre-processing). The latter derandomization result holds under weaker assumptions, of exponential lower bounds against deterministic SVN circuits. Our results follow from a new, nearly optimal, explicit pseudorandom generator. The construction uses, among other ideas, a new connection between pseudoentropy generators and locally list recoverable codes.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/18/tcs-talk-wednesday-march-25-dana-moshkovitz-ut-austin/"><span class="datestr">at March 18, 2020 11:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=398">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/17/updated-spring-schedule-increased-talk-frequency-and-joining-as-individuals/">Updated Spring schedule: increased talk frequency, and joining as individuals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We hope you are all as safe and sound as possible these days, and will be for the weeks to come.</p>
<p>For those of you confined at home, it may be hard to remain connected with the TCS community or stay up-to-date with the current research, as in-person seminars and conferences are cancelled. In case this may help, we have decided to <strong>increase for now the frequency of our online seminars</strong>, to try and mitigate this aspect. This won’t restore normalcy, but every little thing counts.</p>
<p>Here is our current, now <strong>weekly</strong> schedule: we may add more talks to it in the days to come, so keep an eye on <a href="https://sites.google.com/site/plustcs/">our calendar</a> and don’t hesitate to <a href="https://sites.google.com/site/plustcs/suggest">suggest talks and results</a> you are curious about.</p>
<ul>
<li>03/25: <a href="https://www.cs.utexas.edu/~danama/">Dana Moshkovitz</a> (UT Austin) on <em>“Nearly Optimal Pseudorandomness From Hardness”</em></li>
<li>04/01: <a href="http://www.cs.cmu.edu/~venkatg/">Venkat Guruswami</a> (CMU) on <em>“Arıkan meets Shannon: Polar codes with near-optimal convergence to channel capacity”</em></li>
<li>04/08: Rahul Ilango (MIT) on <em>“NP-Hardness of Circuit Minimization for Multi-Output Functions”</em></li>
<li>04/15: <span class="JtukPc"><span id="rAECCd"><a href="https://web.math.princeton.edu/~rvan/">Ramon van Handel</a> (Princeton) </span></span>on <em>“Rademacher type and Enflo type coincide”</em></li>
<li>04/22: <a href="https://www.cs.princeton.edu/~hy2/">Huacheng Yu</a> (Princeton) on <em>“Nearly Optimal Static Las Vegas Succinct Dictionary”</em></li>
<li>04/29: <a href="https://www.mit.edu/~mahabadi/">Sepideh Mahabadi</a> (TTIC) on<em> “Non-Adaptive Adaptive Sampling on Turnstile Streams”</em></li>
</ul>
<p>We emphasize that <strong>you can</strong> (and <em>should</em>) <strong>join as individuals</strong>, from home: if you are interested in a talk, ask for a spot for yourself, no need to go to your institution. We have the live audience capacity for this to work, so don’t hesitate!</p>
<p>We will post individual announcements several days before each talk, including the abstracts and how to ask for a spot, as usual; and the talks will of course be available on <a href="https://www.youtube.com/user/TCSplusSeminars/">YouTube</a> and <a href="https://sites.google.com/site/plustcs/past-talks">our website</a> afterwards if you couldn’t make it to the live, interactive one. Crucially, <strong>we would like your feedback</strong>: not only talk suggestions as pointed out before, but also any idea or suggestion you may have of things we could do or implement, or of content you would like to see. You can send us feedback by <a href="https://sites.google.com/site/plustcs/credits">emailing any of our organizers</a>, or leaving a comment below.</p>
<p>Stay safe,</p>
<p>The TCS+ team</p>
<p><span class="css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0"><strong>Note:</strong> <em>you don’t need to sign up in advance</em> (the link will be made public on <a href="https://sites.google.com/site/plustcs/livetalk">our website</a></span><span class="css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0"> the day of the talk, and you can just join then). We only encourage you to do so in order for us to get a sense of the audience size, but it’s optional: don’t feel you have to plan ahead!</span></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/17/updated-spring-schedule-increased-talk-frequency-and-joining-as-individuals/"><span class="datestr">at March 18, 2020 12:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/13/postdoc-at-university-of-edinburgh-apply-by-april-9-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/13/postdoc-at-university-of-edinburgh-apply-by-april-9-2020/">postdoc at University of Edinburgh (apply by April 9, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a position of Research Associate on Algorithms and Machine Learning, which is provided by Dr He Sun’s 5-year £1.51M EPSRC Fellowship “Efficient Spectral Algorithms for Massive and Dynamic Graphs”. The post associated with the Fellowship is to work on graph clustering, and the post will receive many support for attending conferences and academic visits.</p>
<p>Website: <a href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=051734">https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=051734</a><br />
Email: h.sun@ed.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/13/postdoc-at-university-of-edinburgh-apply-by-april-9-2020/"><span class="datestr">at March 13, 2020 10:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=741">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/03/12/1348-1665-2020/">1348, 1665, 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Besides unimaginable suffering and horror, the Black Death of the 1340’s also brought increased wages and better living standards.  It came back, among other times, in 1665.  Then like now, universities closed and students went home.  Among them was Newton, who spent his time alone in the countryside thus:</p>



<p><strong>In the beginning of the year 1665 I found the method of approximating series and the rule for reducing any dignity [power] of any binomial into such a series. The same year in May I found the method of tangents of Gregory and Slusius, and in November had the direct method of fluxions and the next year [1666] in January had the theory of colours and in May following I had entrance into the inverse method of fluxions. And the same year I began to think of gravity extending to the orb of the moon … All this was in the two plague years of 1665 and 1666, for in those days I was in the prime of my age for invention and minded Mathematics and Philosophy more than at any time since.</strong></p>



<p>Today’s Coronavirus pandemic is probably the first in history that’s been fought with telecommunication.  People are advised to work remotely, and many universities are switching to online courses.  Besides the suffering and horror, it is also an opportunity <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">to realize that many things can be done remotely just as well if not better, change our lifestyle, and stop polluting the environment.</a></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/03/12/1348-1665-2020/"><span class="datestr">at March 13, 2020 01:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7645">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/03/12/life-and-cs-theory-in-the-age-of-coronavirus/">Life and CS theory in the age of Coronavirus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Harvard University, as well most other places that I know of will be moving to remote lectures. I just gave the last in-person lecture in my <a href="https://cs127.boazbarak.org/schedule/">cryptography course</a>. I would appreciate technical suggestions on the best format for teaching remotely. At the moment I plan to use Zoom and log-in from both my laptop (for the video) and from my iPad pro (for the interactive whiteboard).  </p>



<p>The one silver lining is that more lectures will be available online. In particular, if you’re teaching algorithms, you might find <a href="https://gt-algorithms.com/">Eric Vigoda’s videos helpful</a>. (If you know of more sources, please let us know.)</p>



<p>I was hoping that reducing meetings and activities will be good for research, but currently find it hard to concentrate on anything except this train-wreck of a situation. The <a href="https://www.ft.com/content/ff3affea-63c7-11ea-b3f3-fe4680ea68b5">financial times</a> has a chart that summarizes the progress of the  disease in several countries:</p>



<figure class="wp-block-image size-large"><img src="https://windowsontheory.files.wordpress.com/2020/03/http___com.ft_.imagepublish.upp-prod-us.s3.amazonaws.png?w=700" alt="" class="wp-image-7646" /></figure>



<p>The number of confirmed cases grows by about 33% each day. This growth in confirmed cases is partially due to increased testing  as cases increase – there is <a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30260-9/fulltext">some evidence</a>  that the doubling time of the disease (time between <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> infected people to <img src="https://s0.wp.com/latex.php?latex=2n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2n" class="latex" title="2n" /> infected) is about 6 days (rather than the 2.4 days that this figure suggest). However, a doubling time of 6 days still means that the number of cases grows 10-fold in a month, and so if there are 10K actual cases in the U.S., today, there would be 100K by mid April and 1M by mid May.</p>



<p></p>



<p>Strong quarantine regimes, contact tracing, and drastically reducing activity and increasing “social distance” can very significantly reduce the base of this exponent. Reducing the base of the exponent is more than simply “delaying the inevitable”. The mortality statistics mask the fact that this can be a very serious illness even for the people who don’t die of it – about 5% of the cases need intensive care (see this <a href="https://www.economist.com/united-states/2020/03/12/covid-19-is-rapidly-spreading-in-america-the-country-does-not-look-ready">Economist article</a>). Spreading the infections over time will enable the healthcare system to handle the increased caseload, which will completely overwhelm it otherwise.</p>



<p>Such steps are clearly much easier to achieve before the number of cases is too large to be manageable, but despite having “advance warning” from other countries, this lesson does not seem at the moment to have sunk in, at least here in the U.S. At the moment no such initiatives are taken at the federal level, the states are doing more but still not enough, and it’s up to private companies and institutions to come up with their own policies. As faculty and citizens there is not much we can do about it except support such decisions even when they are <a href="https://www.thecrimson.com/article/2020/3/12/parents-petition-coronavirus-measure/">unpopular</a>, and just try to make the remote experience as good as possible for us, our colleagues, and our students.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/03/12/life-and-cs-theory-in-the-age-of-coronavirus/"><span class="datestr">at March 12, 2020 08:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/11/canada-research-chair-tier-2-at-university-of-victoria-apply-by-april-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/11/canada-research-chair-tier-2-at-university-of-victoria-apply-by-april-30-2020/">Canada Research Chair, Tier 2 at University of Victoria  (apply by April 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Electrical and Computer Engineering and Department of Computer Science, University of Victoria, invite applications for a Canada Research Chair (CRC) Tier 2 in Quantum Computing and Engineering.</p>
<p>For full details, please visit our website at <a href="https://www.uvic.ca/opportunities/faculty-librarian/current/engn_220_102.php.">https://www.uvic.ca/opportunities/faculty-librarian/current/engn_220_102.php.</a></p>
<p>Website: <a href="https://www.uvic.ca/opportunities/faculty-librarian/current/engn_220_102.php">https://www.uvic.ca/opportunities/faculty-librarian/current/engn_220_102.php</a><br />
Email: engradr@uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/11/canada-research-chair-tier-2-at-university-of-victoria-apply-by-april-30-2020/"><span class="datestr">at March 11, 2020 09:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/">Workshop in Quantum Information, Complexity &amp; Cryptography</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 11-12, 2020 University of York, UK https://sites.google.com/york.ac.uk/quicc/quicc We are organising a two-day event at the University of York in collaboration with York Interdisciplinary Centre for Cyber Security, which brings researchers in Quantum Information, Complexity and Cryptography together. The goal is to cover recent topics in these areas and facilitate interactions between them. The event … <a href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/" class="more-link">Continue reading <span class="screen-reader-text">Workshop in Quantum Information, Complexity &amp; Cryptography</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/03/10/workshop-in-quantum-information-complexity-cryptography/"><span class="datestr">at March 10, 2020 05:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/03/10/cacm-through-the-lens-of-a-passionate-theoretician/">CACM – Through the Lens of a Passionate Theoretician</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Spending time in insulation? Nothing more to watch on Netflix? Looking for something to read instead? <a href="https://cacm.acm.org/magazines/2020/3/243025-through-the-lens-of-a-passionate-theoretician/fulltext">My CACM article</a>, about <a href="https://www.math.ias.edu/avi/">Avi Wigderson</a>‘s <a href="https://www.math.ias.edu/avi/book">excellent book</a> and about the reach of computation appeared recently. Enjoy!</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/03/10/cacm-through-the-lens-of-a-passionate-theoretician/"><span class="datestr">at March 10, 2020 04:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/06/postdoc-at-national-university-of-singapore-apply-by-april-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/06/postdoc-at-national-university-of-singapore-apply-by-april-15-2020/">Postdoc at National University of Singapore (apply by April 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two post-doctoral positions are available for work on developing statistical and computational guarantees for causal inference algorithms. I am looking for candidates with strong publication records either in theoretical computer science and statistics or in causal inference. Please email me regarding your interest, along with your CV and names of two references. .</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~arnab/causal-postdoc.html">https://www.comp.nus.edu.sg/~arnab/causal-postdoc.html</a><br />
Email: arnabb@nus.edu.sg</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/06/postdoc-at-national-university-of-singapore-apply-by-april-15-2020/"><span class="datestr">at March 06, 2020 09:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=395">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/03/04/tcs-talk-wednesday-march-11-thomas-steinke-ibm-research-almaden/">TCS+ talk: Wednesday, March 11 — Thomas Steinke, IBM Research Almaden</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Thomas Steinke</strong> from IBM Research Almaden will speak about “<em>Reasoning About Generalization via Conditional Mutual Information</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. In view of the recent travel restrictions and coronavirus precautions, in particular, do not hesitate to reserve a seat even for a group <i class="moz-txt-slash">of size one</i>: there should be enough room for everyone, so don’t be shy!</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.</p>
<p>Based on joint work with Lydia Zakynthinou.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/03/04/tcs-talk-wednesday-march-11-thomas-steinke-ibm-research-almaden/"><span class="datestr">at March 05, 2020 01:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/03/02/conferences-in-an-era-of-expensive-carbon/">Conferences in an Era of Expensive Carbon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>At least there’s that: I live in a world where some people care about it and publish their <a href="https://cacm.acm.org/magazines/2020/3/243024-conferences-in-an-era-of-expensive-carbon/abstract">viewpoint in the latest CACM</a>. Read it on your next flight.  Some interesting things that won’t shock anyone:</p>



<p>There’s a nice picture with different environmental costs based on the location of the conference.  It also shows that people like to go to nearby conferences, one of the reasons why “The impulse to ignore the issue is entirely understandable.”  For more perspective see some of our earlier posts for example <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">here</a> and <a href="https://emanueleviola.wordpress.com/2020/01/01/publish-and-perish/">here</a>.</p>



<p>The viewpoint also reports on a recent switch from in-person to online program committees for flagship conferences (POPL and ICFP), following a recent trend.  For starters we continue to suggest that <a href="https://emanueleviola.wordpress.com/2017/07/28/stocfocs-pc-meetings-does-nature-of-decisions-justify-cost/">STOC and FOCS do the same, because the nature of decisions does not justify the cost.</a> The latter post also includes hard numbers on the added value of a physical meeting (with respect to accept/reject decisions — of course one can value at infinity meeting in person luminaries in your field, but that can be done in other ways and should not be tied to PC meetings).</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/03/02/conferences-in-an-era-of-expensive-carbon/"><span class="datestr">at March 02, 2020 03:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/">Prague Summer School on Discrete Mathematics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 24-28, 2020 Prague, Czech Republic http://pssdm.math.cas.cz/ Registration deadline: March 22, 2020 The third edition of Prague Summer School on Discrete Mathematics will feature two lecture series: Subhash Khot (New York University): Hardness of Approximation: From the PCP Theorem to the 2-to-2 Games Theorem, and Shayan Oveis Gharan (University of Washington): Polynomial Paradigm in Algorithm … <a href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/" class="more-link">Continue reading <span class="screen-reader-text">Prague Summer School on Discrete Mathematics</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/03/02/prague-summer-school-on-discrete-mathematics/"><span class="datestr">at March 02, 2020 02:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2386">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/richardson-extrapolation/">On the unreasonable effectiveness of Richardson extrapolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month, I will follow up on <a href="https://francisbach.com/acceleration-without-pain/">last month’s blog post</a>, and describe classical techniques from numerical analysis that aim at accelerating the convergence of a vector sequence to its limit, by only combining elements of the sequence, and without the detailed knowledge of the iterative process that has led to this sequence. </p>



<p class="justify-text">Last month, I focused on sequences that converge to their limit exponentially fast (which is referred to as <em>linear</em> convergence), and I described <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) method</a>, the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, Anderson acceleration and its <a href="https://arxiv.org/pdf/1606.04133">regularized version</a>. These methods are called “non-linear” acceleration techniques, because, although they combine linearly iterates as \(c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}\), the scalar weights in the linear combination depend non-linearly on \(x_k,\dots,x_{k+m}\).</p>



<p class="justify-text">In this post, I will focus on sequences that converge sublinearly, that is, with a difference to their limit that goes to zero as an inverse power of \(k\), typically in \(O(1/k)\). </p>



<h2>Richardson extrapolation</h2>



<p class="justify-text">We consider a sequence \((x_k)_{k \geq 0} \in \mathbb{R}^d\), with an asymptotic expansion of the form $$ x_k = x_\ast + \frac{1}{k}\Delta + O\Big(\frac{1}{k^2}\Big), $$ where \(x_\ast \in \mathbb{R}^d\) is the limit of \((x_k)_k\) and \(\Delta\) a vector in \(\mathbb{R}^d\).</p>



<p class="justify-text">The idea behind <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> [<a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">1</a>] is to combine linearly two iterates taken at two different values of \(k\) so that the zero-th order term \(x_\ast\) is left unchanged, but the first order term in \(1/k\) cancels out. For \(k\) even, we can consider $$  2 x_k – x_{k/2} =  2 \Big( x_\ast + \frac{1}{k} \Delta  +O\Big(\frac{1}{k^2}\Big)  \Big) \, – \Big( x_\ast +  \frac{2}{k} \Delta  + O\Big(\frac{1}{k^2}\Big) \Big)  =  x_\ast +O\Big(\frac{1}{k^2}\Big).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><a href="https://arxiv.org/pdf/1707.06386"><img src="https://francisbach.com/wp-content/uploads/2020/02/reg_k-1024x454.png" alt="" width="404" class="wp-image-2421" height="179" /></a>Illustration of Richardson extrapolation. Iterates (in black) with their first-order expansions (in red). The deviations (represented by circles) are of order \(O(1/k^2)\). Adapted from [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="https://arxiv.org/pdf/2002.02835">2</a>].  </figure></div>



<p class="justify-text">The key benefit of Richardson extrapolation is that we only need to know that the leading term in the asymptotic expansion is proportional to \(1/k\), <em>without the need to know the vector \(\Delta\)</em>. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/richardson_2d.gif" alt="" width="332" class="wp-image-2481" height="280" />Richardson extrapolation in two dimensions. The sequence is of the form \(x_k = \frac{1}{k} \Delta_1 + \frac{(-1)^k}{k^2} \Delta_2\). The extrapolated sequence \(2 x_k – x_{k/2}\) is only plotted for \(k\) even.</figure></div>



<p class="justify-text">In this post, following [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], I will explore situations where Richardson extrapolation can be useful within machine learning. I identified three situations where Richardson extrapolation can be useful (there are probably more):</p>



<ol class="justify-text"><li>Iterates of an optimization algorithms \((x_k)_{k \geq 0}\), and the extrapolation is \( 2x_k – x_{k/2}.\)</li><li>Extrapolation on the step-size of stochastic gradient descent, where we will combine iterates obtained from two different values of the step-size.</li><li>Extrapolation on a regularization parameter.</li></ol>



<p class="justify-text">As we will show, extrapolation techniques come with no significant loss in performance, but in several situations strong gains. It is thus “<a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences">unreasonably effective</a>“.</p>



<h2>Application to optimization algorithms</h2>



<p class="justify-text">We consider an iterate \(x_k\) of an iterative optimization algorithm which is minimizing a function \(f\), thus converging to a global minimizer \(x_\ast\) of \(f\). Then so is \(x_{k/2}\), and thus also $$  x_k^{(1)} = 2x_k – x_{k/2}.$$ Therefore, performance is never significantly deteriorated (the risk is essentially to lose half of the iterations). The potential gains depend on the way \(x_k\) converges to \(x_\ast\). The existence of a convergence rate of the form \(f(x_k) -f(x_\ast) = O(1/k)\) or \(O(1/k^2)\) is not enough, as Richardson extrapolation requires a specific direction of asymptotic convergence. As illustrated below, some algorithms are oscillating around their solutions, while some converge with a specific direction. Only the latter ones can be accelerated with Richardson extrapolation, while the former ones are good candidates for <a href="https://francisbach.com/acceleration-without-pain/">Anderson acceleration</a>.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/nonoscillating_oscillating-1024x350.png" alt="" width="476" class="wp-image-2395" height="162" /> Left: Oscillating convergence, where Richardson extrapolation does not lead to any gain. Right: non-oscillating  convergence, with a main direction \(\Delta\) (in red dotted), where Richardson extrapolation can be beneficial if the oscillations orthogonal to the direction \(\Delta\) are negligible compared to convergence along the direction \(\Delta\). </figure></div>



<p class="justify-text"><strong>Averaged gradient descent.</strong> We consider the usual gradient descent algorithm $$x_k = x_{k-1} – \gamma f'(x_{k-1}),$$ where \(\gamma &gt; 0 \) is a step-size, with Polyak-Ruppert averaging [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>]: $$ y_k = \frac{1}{k} \sum_{i=0}^{k-1} x_i.$$ Averaging is key to robustness to potential noise in the gradients [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>]. However it comes with the unintended consequence of losing the exponential forgetting of initial conditions for strongly-convex problems [<a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">6</a>].</p>



<p class="justify-text">A common way to restore exponential convergence (up to the noise level in the stochastic case) is to consider “tail-averaging”, that is, to replace \(y_k\) by the average of only the latest \(k/2\) iterates [<a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">7</a>]. As shown below for \(k\) even, this corresponds exactly to Richardson extrapolation on the sequence \((y_k)_k\): $$ \frac{2}{k} \sum_{i=k/2}^{k-1} x_i = \frac{2}{k} \sum_{i=0}^{k-1} x_i – \frac{2}{k} \sum_{i=0}^{k/2-1} x_i = 2 y_k – y_{k/2}. $$</p>



<p class="justify-text">With basic  assumptions on \(f\), it is shown in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>] that for locally strongly-convex problems: $$y_k = x_\ast + \frac{1}{k} \Delta + O(\rho^k), $$ where  \(\displaystyle \Delta = \sum_{i=0}^\infty (x_i – x_\ast)\) and \(\rho \in (0,1)\) depends on the condition number of \(f”(x_\ast)\). This is illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/averaged_gradient.png" alt="" width="342" class="wp-image-2507" height="250" />Averaged gradient descent on a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem in dimension \(d=400\), and with \(n=4000\) observations. For the regular averaged recursion, the line in the log-log plot has slope \(-2\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].</figure></div>



<p class="justify-text">We can make the following observations:</p>



<ul class="justify-text"><li>Before Richardson extrapolation, the asymptotic convergence rate after averaging is of order \(O(1/k^2)\), which is better than the usual \(O(1/k)\) upper-bound for the rate of gradient descent, but with a stronger assumption that in fact leads to exponential convergence before averaging.</li><li>While \(\Delta\) has a simple expression, it cannot be computed in practice (but Richardson extrapolation does not need to know it).</li><li>Richardson extrapolation leads to an exponentially convergent algorithm from an algorithm converging asymptotically in \(O(1/k^2)\).</li></ul>



<p class="justify-text"><strong>Accelerated gradient descent.</strong> Above, we considered averaged gradient descent, which is asymptotically converging as \(O(1/k^2)\), and on which Richardson extrapolation could be used with strong gains. Is it possible also for the accelerated gradient descent method [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">8</a>], which has a (non-asymptotic) convergence rate of \(O(1/k^2)\) for convex functions?</p>



<p class="justify-text">It turns out that the behavior of the iterates of accelerated gradient descent is exactly of the form depicted in the left plot of the figure above: that is, the iterates \(x_k\) oscillate around the optimum [<a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">9</a>, <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">10</a>], and Richardson extrapolation is of no help, but is not degrading performance too much. See below for an illustration. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/accelerated_gradient.png" alt="" width="332" class="wp-image-2509" height="243" />Accelerated gradient descent on a quadratic optimization problem in dimension \(d=1000\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].  </figure></div>



<p class="justify-text"><strong>Other algorithms.</strong> It is tempting to test it on other optimization algorithms. For example, as explained in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], Richardson extrapolation can be used to the <a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe</a> algorithm, where sometimes it helps, sometimes it doesn’t. Others could be tried.</p>



<h2>Extrapolation on the step-size of stochastic gradient descent</h2>



<p class="justify-text">While above we have focused on Richardson extrapolation applied to the number of iterations of an iterative algorithm, it is most often used in integration methods (for computing integrals or solving ordinary differential equations), and then often referred to as <a href="https://en.wikipedia.org/wiki/Romberg%27s_method">Romberg-Richardson extrapolation</a>. Within machine learning, in a similar spirit, this can be applied to the step-size of stochastic gradient descent [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">11</a>], which I now describe.</p>



<p class="justify-text">We consider the minimization of a function \(F(x)\) defined on \(\mathbb{R}^d\), which can be written as an expectation as $$F(x) = \mathbb{E}_{z} f(x,z).$$ We assume that we have access to \(n\) independent and identically distributed observations (i.i.d.) \(z_1,\dots,z_n\). This is a typical scenario in machine learning, where \(f(x,z)\) represents the loss for the predictor parameterized by \(x\) on the observation \(z\). </p>



<p class="justify-text">The stochastic gradient method is particularly well adapted, and we consider here a single pass, as $$x_i= x_{i-1} – \gamma f'(x_{i-1},z_i),$$ where the gradient is taken with respect to the first variable, for \(i = 1,\dots,n\). It is known that with a constant step-size, when \(n\) tends to infinity, \(x_n\) will <em>not</em> converge to the minimizer \(x_\ast\) of \(F\), as the algorithm always moves [<a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">16</a>], as illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/logistic_2d-1.gif" alt="" width="403" class="wp-image-2488" height="279" />Stochastic gradient descent on a logistic regression problem: (blue) without averaging, (red) with averaging.</figure></div>



<p class="justify-text">One way to damp the oscillations is to consider averaging, that is, $$ y_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i$$ (we consider uniform averaging for simplicity). For least-squares regression, this leads to a converging algorithm [<a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">12</a>] with attractive properties for ill-conditioned problems (see also <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/">January’s blog post</a>). However, for general loss functions, it is shown in [<a href="https://arxiv.org/pdf/1707.06386">3</a>] that \(y_n\) converges to some \(y^{(\gamma)} \neq x_\ast\). There is a bias due to a step-size \(\gamma\) that does not go to zero. In order to apply Richardson extrapolation, together with Aymeric Dieuleveut and Alain Durmus [<a href="https://arxiv.org/pdf/1707.06386">3</a>], we showed that $$ y^{(\gamma)} = x_\ast + \gamma \Delta + O(\gamma^2),$$ for some \(\Delta \in \mathbb{R}^d\) with some complex expression. Thus, we have $$2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2),$$ thus gaining one order. If we consider the iterate \(y_n^{(\gamma)}\) and \(y_n^{(2 \gamma)}\) associated to the two step-sizes \(\gamma\) and \(2 \gamma\), the linear combination $$2 y_n^{(\gamma)} – y_n^{(2\gamma)} $$ has an improved behavior as it tends to \(2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2)\): it remains not convergent, but get to way smaller values. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/SGD_logistic-1.png" alt="" width="359" class="wp-image-2525" height="274" />Averaged stochastic gradient descent on a logistic regression problem in dimension 20.</figure></div>



<p class="justify-text"><strong>Higher-order extrapolation.</strong> If we can accelerate a sequence by extrapolation, why not extrapolate the extrapolated sequence? This is possible if we have an higher-order expansion of the form $$ y^{(\gamma)} = \theta_\ast + \gamma \Delta_1 + \gamma^2 \Delta_2 + O(\gamma^3),$$ for some (typically unknown) vectors \(\Delta_1\) and \(\Delta_2\). Then, the sharp reader can check that $$3 y_n^{(\gamma)} – 3 y_n^{(2\gamma)} +  y_n^{(3\gamma)}, $$ will lead to cancellation of the first two orders \(\gamma\) and \(\gamma^2\). This is illustrated above for SGD.</p>



<p class="justify-text">Then, why not extrapolate the extrapolation of the extrapolated sequence? One can check that $$4 y_n^{(\gamma)} – 6 y_n^{(2\gamma)} + 4  y_n^{(3\gamma)}  -y_n^{(4\gamma)}, $$ will lead to cancellation of the first three orders of an expansion of \(y^{(\gamma)}\). The <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> aficionados have already noticed the pattern there, and checked that $$ \sum_{i=1}^{m+1} (-1)^{i-1} { m+1 \choose i} y_n^{(i\gamma)}$$ will lead to cancellations of the first \(m\) orders.</p>



<p class="justify-text">Then, why not go on forever? First because \(m+1\) recursions have to be run in parallel, and second, because the constant in front of the term in \(\gamma^{m+1}\) typically explodes, a phenomenon common to many expansion methods.</p>



<h2>Extrapolation on a regularization parameter</h2>



<p class="justify-text">We now explore the application of Richardson extrapolation to regularization methods. In a nutshell, regularization allows to make an estimation problem more stable (less subject to variations for statistical problems) or the algorithm faster (for optimization problems). However, regularization adds a bias that needs to be removed. In this section, we apply Richardson extrapolation to the regularization parameter to reduce this bias. I will only present an application to smoothing for non-smooth optimization (see an application to  ridge regression in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>]).</p>



<p class="justify-text"><strong>Non-smooth optimization problems</strong>. We consider the minimization of a convex function of the form \(f = h + g\), where \(h\) is smooth and \(g\) is non-smooth. These optimization problems are ubiquitous in machine learning and signal processing, where the lack of smoothness can come from (a) non-smooth losses such as max-margin losses used in support vector machines and more generally structured output classification [<a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">13</a>], and (b) sparsity-inducing regularizers (see, e.g., [<a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">14</a>] and references therein). While many algorithms can be used to deal with this non-smoothness, we consider a classical smoothing technique below.</p>



<p class="justify-text"><strong>Nesterov smoothing</strong>. In this section, we consider the smoothing approach of Nesterov [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>] where the non-smooth term is “smoothed” into \(g_\lambda\), where \(\lambda\) is a regularization parameter, and accelerated gradient descent is used to minimize \(h+g_\lambda\). </p>



<p class="justify-text">A typical way of smoothing the function \(g\) is to add \(\lambda\) times a strongly convex regularizer (such as the squared Euclidean norm) to the Fenchel conjugate of \(g\); this leads to a function \(g_\lambda\) which has a smoothness constant (defined as the maximum of the largest eigenvalues of all Hessians) proportional to \(1/\lambda\), with a uniform error of \(O(\lambda)\) between \(g\) and \(g_\lambda\). Given that accelerated gradient descent leads to an iterate with excess function values proportional to \(1/(\lambda k^2)\) after \(k\) iterations, with the choice of \(\lambda \propto 1/k\), this leads to an excess in function values proportional to \(1/k\), which improves on the subgradient method which converges in \(O(1/\sqrt{k})\). Note that the amount of regularization depends on the number of iterations, so that this smoothing method is not “anytime”.</p>



<p class="justify-text"><strong>Richardson extrapolation.</strong> If we denote by \(x_\lambda\) the minimizer of \(h+g_\lambda\) and \( x_\ast\) the global minimizer of \( f=h+g\), if we can show that \( x_\lambda = x_\ast + \lambda \Delta + O(\lambda^2)\), then \( x^{(1)}_\lambda = 2 x_\lambda – x_{2\lambda} = O(\lambda^2)\) and we can expand \( f(x_\lambda^{(1)})  = f(x_\ast)  + O(\lambda^2)\), which is better than the \(O(\lambda)\) approximation without extrapolation. </p>



<p class="justify-text">Then, given a number of iterations \(k\), with \( \lambda \propto k^{-2/3}\), to balance the two terms \( 1/(\lambda k^2)\) and \( \lambda^2\),  we get an overall convergence rate for the non-smooth problem of \( k^{-4/3}\). </p>



<p class="justify-text"><strong>\(m\)-step Richardson extrapolation</strong>. Like above for the step-size, we can also consider \(m\)-step Richardson extrapolation \(x_{\lambda}^{(m)}\), which leads to a bias proportional to \(\lambda^{m+1}\). Thus, if we consider \(\lambda \propto 1/k^{2/(m+2)}\), to balance the terms \(1/(\lambda k^2)\) and \(\lambda^{m+1}\), we get an error for the non-smooth problem of \(1/k^{2(m+1)/(m+2)}\), which can get arbitrarily close to \(1/k^2\) when \(m\) gets large. The downsides (like for the extrapolation on the step-size above) are that (a) the constants in front of the asymptotic equivalent may blow up (a classical problem in high-order expansions), and (b) \(m\)-step extrapolation requires running the algorithm \(m\) times (this can be down in parallel). In the experiment below, 3-step extrapolation already brings in most of the benefits.</p>



<p class="justify-text">In order to experimentally study the benefits of extrapolation, for the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> optimization problem, and for a series of regularization parameters equal to \(2^{i}\) for \(i\) between \(-18\) and \(1\) (sampled every \(1/5\)), we run accelerated gradient descent on \(h+g_\lambda\) and we plot the value of \(f(x)-f(x_\ast)\) for the various estimates, where for each number of iterations, we minimize over the regularization parameter. This is an oracle version of varying \(\lambda\) as a function of the number of iterations. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/smoothing.png" alt="" width="351" class="wp-image-2530" height="255" />Excess function values as a function of the number of iterations, <em>taking into account that \(m\)-step Richardson extrapolation requires \(m\)-times more iterations</em>. There is indeed a strong improvement approaching the rate \(1/k^2\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">These last two blog posts were dedicated to acceleration techniques coming from numerical analysis. They are cheap to implement, typically do not interfere with the underlying algorithm, and when used in the appropriate situation, can bring in significant speed-ups.</p>



<p class="justify-text">Next month, I will most probably host an invited post by my colleague <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a>, who will explain how machines can <s>replace</s> help researchers that prove bounds on optimization algorithms.</p>



<h2>References</h2>



<p class="justify-text">[1] Lewis Fry Richardson. <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam</a>. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, 210(459-470):307–357, 1911.<br />[2] Francis Bach. <a href="https://arxiv.org/pdf/2002.02835">On the Effectiveness of Richardson Extrapolation in Machine Learning</a>. Technical report, arXiv:2002.02835, 2020.<br />[3] Aymeric Dieuleveut, Alain Durmus, Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains</a>. To appear in <em>The Annals of Statistics</em>, 2019.<br />[4] Boris T. Polyak,  Anatoli B. Juditsky. <a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">Acceleration of stochastic approximation by averaging</a>. <em>SIAM journal on control and optimization</em> 30(4):838-855, 1992.<br />[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>. </em><a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br />[6] Francis Bach, Eric Moulines. <a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. <em>Advances in Neural Information Processing Systems</em>, 2011.<br />[7] Prateek Jain, Praneeth Netrapalli, Sham Kakade, Rahul Kidambi, Aaron Sidford. <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</a>. <em>The Journal of Machine Learning Research</em>, 18(1), 8258-8299, 2017.<br />[8] Yurii E. Nesterov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">A method of solving a convex programming problem with convergence rate \(O(1/k^2)\)</a>, <em>Doklady Akademii Nauk SSSR</em>, 269(3):543–547, 1983.<br />[9] Weijie Su, Stephen Boyd, and Emmanuel J. Candes. <a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1):5312-5354, 2016.<br />[10] Nicolas Flammarion, and Francis Bach. <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">From Averaging to Acceleration, There is Only a Step-size</a>. <em>Proceedings of the International Conference on Learning Theory (COLT)</em>, 2015. <br />[11] Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, and Gaël Richard. <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">Stochastic gradient Richardson-Romberg Markov chain Monte Carlo</a>. In <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2016.<br />[12] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)</a>. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2013.<br />[13] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. <a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">Learning structured prediction models: A large margin approach</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2005.<br />[14] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. <a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. Foundations and Trends in Machine Learning, 4(1):1–106, 2012<br />[15] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming , 103(1):127–152, 2005.<br />[16] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">Stochastic minimization with constant step-size: asymptotic laws</a>. <em>SIAM Journal on Control and Optimization</em>, (24)4:655-666, 1986.</p>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/richardson-extrapolation/"><span class="datestr">at March 01, 2020 12:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/">postdoc at UC San Diego (apply by March 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UCSD has several Postdoctoral Fellowships, aimed at preparing outstanding researchers for academic and leadership careers in Data Science. Areas of interest include both theoretical and practical aspects of machine learning, statistics, algorithms, and their applications. Applications will be reviewed until positions are filled.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/"><span class="datestr">at March 01, 2020 01:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=26">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/">Friday, February 28 — Jon Kleinberg from Cornell University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The first Foundations of Data Science virtual talk will take place this coming Friday, February 28th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC). <strong>Jon Kleinberg</strong> from Cornell University will speak about “<em>Fairness and Bias in Algorithmic Decision-Making</em>”.</p>



<p><strong>Abstract</strong>: As data science has broadened its scope in recent years, a number of domains have applied computational methods for classification and prediction to evaluate individuals in high-stakes settings. These developments have led to an active line of recent discussion in the public sphere about the consequences of algorithmic prediction for notions of fairness and equity. In part, this discussion has involved a basic tension between competing notions of what it means for such classifications to be fair to different groups. We consider several of the key fairness conditions that lie at the heart of these debates, and in particular how these properties operate when the goal is to rank-order a set of applicants by some criterion of interest, and then to select the top-ranking applicants. The talk will be based on joint work with Sendhil Mullainathan and Manish Raghavan.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/"><span class="datestr">at February 27, 2020 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/">Postdoc at University of Chicago (apply by March 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Econometrics and Statistics group at the Booth School of Business of the University of Chicago invites applications for a postdoctoral researcher working with Prof. Bryon Aragam. Potential candidates should have a background in statistics and machine learning, for example nonconvex optimization, nonparametric statistics, and/or learning theory.</p>
<p>Website: <a href="https://uchicago.wd5.myworkdayjobs.com/External/job/Hyde-Park-Campus/Principal-Researcher_JR07406">https://uchicago.wd5.myworkdayjobs.com/External/job/Hyde-Park-Campus/Principal-Researcher_JR07406</a><br />
Email: bryon@chicagobooth.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/"><span class="datestr">at February 23, 2020 06:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/02/21/tenure-track-faculty-all-levels-at-boston-university-apply-by-february-21-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/02/21/tenure-track-faculty-all-levels-at-boston-university-apply-by-february-21-2020/">Tenure-track faculty, all levels at Boston University (apply by February 21, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Faculty of Computing &amp; Data Sciences invites applications for multiple tenured and tenure-track faculty positions at all ranks. Founded in 2019, the Faculty of Computing &amp; Data Sciences is a new university-wide, degree-granting academic unit. Consideration and review of applications will be on a rolling basis. See the full ad for more information.</p>
<p>Website: <a href="https://www.bu.edu/cds-faculty/join-us/">https://www.bu.edu/cds-faculty/join-us/</a><br />
Email: ads22@bu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/02/21/tenure-track-faculty-all-levels-at-boston-university-apply-by-february-21-2020/"><span class="datestr">at February 21, 2020 07:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/02/21/lecturer-to-reader-tenured-assistant-professor-to-tenured-associate-professor-at-royal-holloway-university-of-london-apply-by-february-27-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/02/21/lecturer-to-reader-tenured-assistant-professor-to-tenured-associate-professor-at-royal-holloway-university-of-london-apply-by-february-27-2020/">Lecturer to Reader (tenured Assistant Professor to tenured Associate Professor) at Royal Holloway, University of London (apply by February 27, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CS Department at Royal Holloway, University of London is looking to appoint up to six new academics to support our expansion in research and teaching. We carry out outstanding research and deliver excellent teaching at both undergraduate and postgraduate level.<br />
(The ad is not specific for theory of CS, but strong candidates in theory are encouraged. Vacancy open to all nationalities.)</p>
<p>Website: <a href="https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0120-036">https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0120-036</a><br />
Email: carlos.matos@rhul.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/02/21/lecturer-to-reader-tenured-assistant-professor-to-tenured-associate-professor-at-royal-holloway-university-of-london-apply-by-february-27-2020/"><span class="datestr">at February 21, 2020 06:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/20/tcs-talk-wednesday-february-26-henry-yuen-university-of-toronto/">TCS+ talk: Wednesday, February 26 — Henry Yuen, University of Toronto</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, February 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Henry Yuen</strong> from University of Toronto will speak about “<em>MIP* = RE</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: MIP* denotes the class of problems that admit interactive proofs with quantum entangled provers. It has been an outstanding question to characterize the complexity of MIP*. Most notably, there was no known computable upper bound on this class.<br />
We show that MIP* is equal to the class RE, the set of recursively enumerable languages. In particular, this shows that MIP* contains uncomputable problems. Through a series of known connections, this also yields a negative answer to Connes’ Embedding Problem from the theory of operator algebras. In this talk, I will explain the connection between Connes’ Embedding Problem, quantum information theory, and complexity theory. I will then give an overview of our approach, which involves reducing the Halting Problem to the problem of approximating the entangled value of nonlocal games.<br />
Joint work with Zhengfeng Ji, Anand Natarajan, Thomas Vidick, and John Wright.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/20/tcs-talk-wednesday-february-26-henry-yuen-university-of-toronto/"><span class="datestr">at February 20, 2020 08:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/02/19/associate-professor-professor-of-computer-science-at-university-of-oxford-apply-by-april-24-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/02/19/associate-professor-professor-of-computer-science-at-university-of-oxford-apply-by-april-24-2020/">Associate Professor/Professor of Computer Science at University of Oxford (apply by April 24, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science and St Catherine’s College are recruiting an Associate Professor of Computer Science in the Department of Computer Science, to start before 31 July 2020 if possible and by no later than 1 October 2020. The successful candidate will also be appointed as Fellow and Tutor in Computer Science at St Catherine’s College. (see link for details)</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/1782-full.html">http://www.cs.ox.ac.uk/news/1782-full.html</a><br />
Email: agalanis@cs.ox.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/02/19/associate-professor-professor-of-computer-science-at-university-of-oxford-apply-by-april-24-2020/"><span class="datestr">at February 19, 2020 01:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=717">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/">Working remotely will be the most significant transformation since agriculture</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Its impact on civilization will be exactly opposite.  Rather than concentrating population, it will disperse it.  Commuting and the traffic crisis will disappear.  So will the housing crisis.  You will have a large lot of land with a robot-ready house built new with safe, eco-friendly material and free of hazardous substance.  You will live away from volcanoes, fault lines, tornadoes, wild fires and other hazards. You’ll be able to move to a location with ideal climate, which for historical reasons are now under-populated.  This will dramatically reduce housing costs, especially heating, and solve  or greatly mitigate the pollution problem. Huge amounts of space will be cleared up and given back to nature, or used for housing.</p>



<p>Doctors will visit patients remotely.  This will enable patients to be followed up more regularly and consistently throughout their lives regardless of where they are.  Doctors will have more time to give meaningful advice rather than having the patient wait 1 year for the appointment and then spend 1 hour to get to the doctor for a 10-minute visit of which 8 are spent looking at the screen and filling reports. Robo-tools will take measurements and send them to the doctor.  If a complicated procedure is required, the expert will connect with the patient and the doctor remotely first, and then the patient will schedule a trip for the procedure.</p>



<p>You’ll take gym classes remotely via a remote gym. The instructor will give you personalized advice and follow your progress anywhere, anytime. Demanding facilities like swimming pools will be next to your house.</p>



<p>Courts of law, and the entire judicial system will be taken off-line.</p>



<p>People will vote from home, elections will be more frequent and granular.  Constituents choosing not to vote will (maybe) have to specifically abstain.  This will finally realize the democratic ideal where the government represents the will of the people.</p>



<p>Constituents will be able to participate to discussions, instead of having to travel 1 hour for a 5-minute in-person discussion.  The level of engagement will be measured by the level of engagement as opposed to travel distance.</p>



<p>Wireless won’t be used on a large scale, since its noxious effects will be undeniable. Instead we will have network cables densely spread out over the earth — one of the few duties of the government will be to maintain these cables for the free, democratic, public use.</p>



<p>Banking will be done remotely, and physical money will disappear.</p>



<p>We will have immersive work-stations with wall-to-wall, solar-powered e-ink screens, holographic images, and audio indistinguishable from reality.  You will be able to attend meetings while exercising, like walking or biking on a machine or outside.  This will boost your health, lowering health care costs for all.</p>



<p>People with special needs will have the same opportunities and duties as everyone else and will be fully integrated.</p>



<p>All learning will be done remotely.  The instructor will be able to provide better, more personalized teaching, and connect with each student face-to-face.  Testing will be done remotely, each student monitored via cameras.  Critical examinations will be administered in special-purpose facilities which are next to your house (similar in spirit to say the way GRE is administered, but much more large scale and flexible, including for example synchronized examination).</p>



<p>You will have farms next to your house, growing organic food that you can eat fresh. Epidemics will be much rarer and more easily controlled, as population will be less concentrated and will travel less.</p>



<p>Fantasy? Actually, many of these things are already happening!</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/02/18/working-remotely-will-be-the-most-significant-transformation-since-agriculture/"><span class="datestr">at February 18, 2020 08:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1555">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/02/11/approx-random-2020/">APPROX-RANDOM 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CFPs for <a href="https://approxconference.wordpress.com/approx-2020/">APPROX 2020</a> and <a href="https://randomconference.com/random-2020-home/">RANDOM 2020</a> are out. The conference will be held August 17-19, 2020 at the University of Washington in Seattle. <strong>Submissions:</strong> April 24, 2020.</p>
<p> </p>
<p> </p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/02/11/approx-random-2020/"><span class="datestr">at February 12, 2020 06:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=40">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2020/02/08/icalp-and-lics-2020-relocation-and-extended-deadline/">ICALP (and LICS) 2020 – Relocation and Extended Deadline</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Due to the Wuhan coronavirus outbreak, the organizers of ICALP and LICS have made the difficult decision to relocate both (co-located) conferences from Beijing, China, to Saarbrücken, Germany. Speaking specifically about ICALP now (I do not have further information about LICS): As a result of previous uncertainty regarding the situation, the deadline has been extended by about six days, until Tuesday February 18, 2020, at 6 AM GMT. The dates of the conference remain (roughly) the same, July 8 – 11, 2020. <br />The following is a more official message from ICALP Track A Chair, Artur Czumaj.</p>



<hr class="wp-block-separator" />



<p>The ICALP and the LICS steering committee have agreed together with the conference chairs in Beijing to relocate the two conferences.<br />ICALP and LICS 2020 will take place in <strong>Saarbrücken</strong>, Germany, July 8 – 11 2020 (with satellite workshops on July 6 – 7 2020).<br />The deadline is extended, see below.</p>



<p><strong>Call for Papers – ICALP 2020</strong><br /><strong>July 8 – 11 2020, Saarbrücken, Germany</strong></p>



<p><strong>NEW Paper submission deadline: Tuesday February 18, 2020, 6am GMT</strong><br /><a href="https://easychair.org/conferences/?conf=icalp2020">https://easychair.org/conferences/?conf=icalp2020</a></p>



<p>ICALP (International Colloquium on Automata, Languages and Programming) is the main European conference in Theoretical Computer Science and annual meeting of the European Association for Theoretical Computer Science (EATCS). ICALP 2020 will be hosted on the Saarland Informatics Campus in Saarbrücken, in co-location with LICS 2020 (ACM/IEEE Symposium on Logic in Computer Science).</p>



<p><strong>Invited speakers:</strong><br />Track A: Virginia Vassilevska (MIT), Robert Krauthgamer (Weizmann)<br />Track B: Stefan Kiefer (Oxford)<br />Joint ICALP-LICS: Andrew Yao (Tsinghua), Jérôme Leroux (Bordeaux)</p>



<p><strong>Submission Guidelines:</strong> see <a href="https://easychair.org/conferences/?conf=icalp2020">https://easychair.org/conferences/?conf=icalp2020</a></p>



<p><strong>NEW Paper submission deadline: February 18</strong>, 2020, 6am GMT<br />notifications: April 15, 2020<br />camera ready: April 28, 2020</p>



<p>Topics: ICALP 2020 will have the two traditional tracks<br />A (Algorithms, Complexity and Games – including Algorithmic Game Theory, Distributed Algorithms and Parallel, Distributed and External Memory Computing) and<br />B (Automata, Logic, Semantics and Theory of Programming).<br /><strong><em>    (Notice that the old tracks A and C have been merged into a single track A.)</em></strong><br />Papers presenting original, unpublished research on all aspects of theoretical computer science are sought.</p>



<p>Typical, but not exclusive topics are:</p>



<p>Track A — Algorithmic Aspects of Networks and Networking, Algorithms for Computational Biology, Algorithmic Game Theory, Combinatorial Optimization, Combinatorics in Computer Science, Computational Complexity, Computational Geometry, Computational Learning Theory, Cryptography, Data Structures, Design and Analysis of Algorithms, Foundations of Machine Learning, Foundations of Privacy, Trust and Reputation in Network, Network Models for Distributed Computing, Network Economics and Incentive-Based Computing Related to Networks, Network Mining and Analysis, Parallel, Distributed and External Memory Computing, Quantum Computing, Randomness in Computation, Theory of Security in Networks</p>



<p>Track B — Algebraic and Categorical Models, Automata, Games, and Formal Languages, Emerging and Non-standard Models of Computation, Databases, Semi-Structured Data and Finite Model Theory, Formal and Logical Aspects of Learning, Logic in Computer Science, Theorem Proving and Model Checking, Models of Concurrent, Distributed, and Mobile Systems, Models of Reactive, Hybrid and Stochastic Systems, Principles and Semantics of Programming Languages, Program Analysis and Transformation, Specification, Verification and Synthesis, Type Systems and Theory, Typed Calculi</p>



<p><strong>PC Track A chair: Artur Czumaj</strong> (University  of Warwick)<br /><strong>PC Track B chair: Anuj Dawar</strong> (University of Cambridge)</p>



<p>Contact<br />All questions about submissions should be emailed to the PC Track chairs:<br />Artur Czumaj <a href="mailto:A.Czumaj@warwick.ac.uk">A.Czumaj@warwick.ac.uk&lt;mailto:A.Czumaj@warwick.ac.uk&gt;</a><br />Anuj Dawar <a href="mailto:Anuj.Dawar@cl.cam.ac.uk">Anuj.Dawar@cl.cam.ac.uk&lt;mailto:Anuj.Dawar@cl.cam.ac.uk&gt;</a></p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2020/02/08/icalp-and-lics-2020-relocation-and-extended-deadline/"><span class="datestr">at February 08, 2020 03:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=387">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/06/tcs-talk-wednesday-february-12-albert-atserias-universitat-politecnica-de-catalunya/">TCS+ talk: Wednesday, February 12 — Albert Atserias, Universitat Politecnica de Catalunya</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, February 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Albert Atserias</strong> from Universitat Politecnica de Catalunya will speak about “<em>Automating Resolution is NP-Hard</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We show that it is NP-hard to distinguish CNF formulas that have Resolution refutations of almost linear length from CNF formulas that do not even have weakly exponentially long ones. It follows from this that Resolution is not automatable in polynomial time unless P = NP, or in weakly exponential time unless ETH fails. The proof of this is simple enough that all its ideas can be explained in a talk. Along the way, I will try to explain the process of discovery that led us to the result. This is joint work with Moritz Müller.</p></blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/06/tcs-talk-wednesday-february-12-albert-atserias-universitat-politecnica-de-catalunya/"><span class="datestr">at February 06, 2020 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5488">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/02/04/an-algorithms-course-with-minimal-prerequisites/">An Algorithms Course with Minimal Prerequisites</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
There are amazing materials for teaching theoretical algorithms courses: excellent books, lecture notes, and online courses. But none of the resources I am familiar with fits the algorithms course I was supposed to prepare. I wanted to teach a course for students who hardly have any prerequisites. My students are non-CS majors (mostly math majors), […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/02/04/an-algorithms-course-with-minimal-prerequisites/"><span class="datestr">at February 04, 2020 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2109">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/acceleration-without-pain/">Acceleration without pain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">I don’t know of any user of iterative algorithms who has not complained one day about their convergence speed. Whether the data are too big, the processors not fast or numerous enough, waiting for an algorithm to converge unfortunately remains a core practical component of computer science and applied mathematics. This was already a concern long before computers were invented (and most of the techniques I will describe date back to the early 19th century): imagine you are doing all the operations (multiplications, additions, divisions) by hand, wouldn’t you want some cheap way to accelerate your algorithm (and here literally reduce your pain)?</p>



<p class="justify-text">Acceleration is a key concept in numerical analysis and can be carried through in two main ways. The first way is to modify some steps of the algorithm (such as Nesterov acceleration for gradient descent, or <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> / <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> acceleration for linear recursions). This requires a good knowledge of the inner structure of the underlying algorithm. A second way is to totally ignore the specifics of the algorithm, and see the acceleration problem as trying to find good “combinations” of the observed iterates that converge faster.</p>



<p class="justify-text">In this blog post, I thus consider a sequence of iterates \((x_k)_{k \geq 0}\) in \(\mathbb{R}^d\) obtained from an iterative algorithm \(x_{k+1} = T(x_k)\), which will typically be an optimization algorithm. The main question I will address is: Can we do better than outputting the last iterate?</p>



<p class="justify-text">This has a long history in numerical analysis, where many techniques have been developed for uni-dimensional sequences. Acceleration techniques vary according to the <a href="https://en.wikipedia.org/wiki/Rate_of_convergence">type of convergence</a> of the original sequence (quadratic, linear, sublinear), the amount of knowledge about the asymptotic behavior, and the possibility of extensions to high-dimensional and noisy problems.</p>



<p class="justify-text">Acceleration techniques are often based on an explicit or implicit modelling of the sequence \(x_k\), either through a model of the function \(T: \mathbb{R}^d \to \mathbb{R}^d\) (the iteration of the algorithm) or through an asymptotic expansion of \(x_k\). In this post, I will focus on linearly convergent sequences, that is, sequences \(x_k\) converging to some \(x_\ast\) at an exponential rate. As we will see, this will done through modelling \(x_k\) as an autoregressive process.</p>



<p class="justify-text">I will first start from the simplest scheme, the <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) process</a> from 1926 [1], then look at higher order generalizations still in one dimension, and finally to the general vector case. We will then apply all that to gradient descent.</p>



<h2>Aitken’s \(\Delta^2\) process</h2>



<p class="justify-text">This is the simplest of all techniques and the source of all others in this post. We try to model \(x_k\in \mathbb{R}\) as first-order auto-regressive sequence, that is, \(x_{k+1} = ax_{k}+b\), for \(a,b \in \mathbb{R}\). The method works by (a) estimating \(a\) and \(b\) from a sequence of few consecutive (here three) iterates, and (b) extrapolating by computing the limit \(x_{\rm acc}\) of the estimated model. Given that we fit the model to consecutive iterates \((x_k,x_{k+1},x_{k+2})\), the model \((a,b)\) will also depend on \(k\), as well as its limit \(x_{\rm acc}\). In order to avoid having too many \(k\)’s in my notations, I will drop the dependence in \(k\) of the model parameters.</p>



<p class="justify-text">In this situation, the model recursion has a limit when \(a \neq 1\), and the limit is \(x_{\rm acc} =  \frac{b}{1-a}\). In order to fit the two parameters, we need two equations, which can be obtained by considering two consecutive evaluations of the recursions (which require three iterates). That is, we consider the linear system in \((a,b)\): $$ \Big\{ \begin{array}{ll} ax_{k}+b  &amp; = x_{k+1} \\    ax_{k+1}+b &amp; = x_{k+2} \end{array}$$ which can be solved in a variety of ways. All of them are equivalent, but naturally lead to different extensions.</p>



<p class="justify-text"><strong>Solving by elimination.</strong> We can eliminate \(b\) by subtracting the two equations, leading to $$ x_{k+2}  – x_{k+1} = a ( x_{k+1} – x_{k}),$$  and thus $$a = \frac{ x_{k+2}  – x_{k+1}}{ x_{k+1} – x_{k}}.$$ We then get $$b = x_{k+1} – a x_{k} = x_{k+1} –  \frac{ x_{k+2}  – x_{k+1}}{ x_{k+1} – x_{k}} x_{k} =  \frac{  x_{k+1}^2 – x_{k} x_{k+2}}{ x_{k+1} – x_{k}}, $$ and the extrapolating sequence $$x_{\rm acc} = \frac{b}{1-a} = \frac{x_{k} x_{k+2} – x_{k+1}^2 }{-2 x_{k+1} + x_{k+2} + x_{k}},$$ which we denote \(x^{(1)}_k\), to highlight its dependence on \(k\). Note that to compute \(x^{(1)}_k\), we need access to the three iterates \((x_k,x_{k+1},x_{k+2})\), and thus, when comparing the original sequence to the extrapolated one, we will compare \(x_k\) and \(x^{(1)}_{k-2}\).</p>



<p class="justify-text"><strong>Asymptotic auto-regressive model</strong>. A key feature of the acceleration techniques that I describe in this post is that although they implicitly or explicitly model sequence with auto-regressive processes, the models do not need to be correct, that is, they also work if the autoregressive recursion is true only asymptotically, for example \(\displaystyle \frac{x_{k+1}-x_\ast}{x_k – x_\ast}\) converging to a constant \(a \in [-1,1)\). Then we also get some acceleration, which can be quantified (see the end of the post for details), and for which we present a classical example below.</p>



<p class="justify-text"><strong>Approximating \(\pi\).</strong> We consider the <a href="https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80">Leibniz formula</a>, which is one of many ways of <a href="https://en.wikipedia.org/wiki/Approximations_of_%CF%80">approximating \(\pi\)</a>: $$ \pi = \lim_{k \to +\infty} x_k  \mbox{ with } x_k = 4 \sum_{k=0}^{+\infty} \frac{(-1)^k}{2k+1}.$$ This formula can be proved by expanding the derivative \(x \mapsto \frac{1}{1+x^2}\) of \(x \mapsto \arctan x\) as a power series and then integrating it. We can check that $$\frac{x_{k+1}-x_\ast}{x_k – x_\ast} =  \ – 1 + \frac{1}{k} + o( \frac{1}{k}), $$ and as detailed at the end of the post, we should expect the error to go from \(1/k\) to \(1/k^3\).  Below, we show the first 10 iterates of the two sequences, with the correct significant digits in bold. $$ \begin{array}{|l|l|l|} \hline k &amp; x_k &amp;  x_k^{(1)} \\ \hline  1  &amp;   4.0000   &amp;  \times \\      2  &amp;  2.6667     &amp;    \times \\  3  &amp;  \mathbf{3}.4667   &amp; \mathbf{3.1}667 \\   4 &amp;   2.8952  &amp;  \mathbf{3.1}333 \\     5  &amp;  \mathbf{3}.3397  &amp;  \mathbf{3.14}52 \\  6 &amp;   2.9760 &amp;   \mathbf{3.1}397 \\   7  &amp;  \mathbf{3}.2837  &amp;  \mathbf{3.14}27 \\   8  &amp;  \mathbf{3}.0171  &amp;  \mathbf{3.14}09 \\  9  &amp;  \mathbf{3}.2524  &amp;  \mathbf{3.14}21 \\   10  &amp;  \mathbf{3}.0418 &amp;  \mathbf{3.141}3 \\ \hline \end{array}$$ We see that the extrapolated sequence converges much faster. This is confirmed in the convergence plot below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/01/deltasquared.png" alt="" width="335" class="wp-image-2148" height="283" />\(\Delta^2\) method on the Leibniz series.  Notice the improvement from \(O(1/k)\) to \(O(1/k^3)\).</figure></div>



<h2>Higher-order one-dimensional extensions</h2>



<p class="justify-text">The Aitken’s \(\Delta^2\) process relies on fitting a first-order auto-regressive model, or on assuming that \(x_{k+1} – x_\ast – a (x_k – x_\ast) \to 0\) asymptotically. This can be extended to \(m\)-th order constant recursions. This corresponds to modelling \(x_k\) as the sum of \(m\) exponentials.</p>



<p class="justify-text">We thus try to fit the model $$x_{k+m} = a_0 x_k + a_1 x_{k+1} + \cdots + a_{m-1} x_{k+m-1} + b = \sum_{i=0}^{m-1} a_i x_{k+i} + b, $$ which has \(m+1\) parameters. We thus need \(m + 1\) equations, that is we consider the recursion for \(k, k+1,\dots, k+m\), which requires the knowledge of the \(2m+1\) iterates \(x_k, x_{k+1},\dots,x_{k+2m}\). This leads to the \(m+1\) equations: $$x_{k+m+j} = a_0 x_{k+j} + a_1 x_{k+j+1} + \cdots + a_{m-1} x_{k+j+m-1} + b = \sum_{i=0}^{m-1} a_i x_{k+j+i} + b,$$ for \(j \in \{0,\dots,m\}\). This is a system with \(m+1\) unknowns and \(m+1\) equations, from which we could get all \(a_j\)’s and \(b\), and then the model limit as the extrapolated sequence \(x^{(m)}_k = \frac{b}{1 – a_0 – a_1 – \cdots – a_{m-1}}\). </p>



<p class="justify-text">This linear system can be solved in a variety of ways. At the end of the blog post, I show how it can be solved using determinants of Hankel-like matrices, often referred to as the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, which then leads to an iterative algorithm dating back from Wynn [2], which is called the <a href="https://fr.wikipedia.org/wiki/Epsilon_algorithme">\(\varepsilon\)-algorithm</a>. In order to smooth our way to the vector case extension, I will present it in a slightly non-standard way. See [3] for a detailed survey on acceleration and extrapolation.</p>



<p class="justify-text">Instead of learning the model parameters to estimate \(x_{k+m}\) from the past iterates, we focus directly on the prediction of the limit \(x_{\rm acc}\) by looking for real numbers \(c_0,\dots,c_m\) such that for all \(k\), $$\sum_{i=0}^m c_i ( x_{k+i} – x_{\rm acc} ) = 0,$$ with the arbitrary normalization \(\sum_{i=0}^m c_i = 1\). The \(c_i\)’s can be obtained from the \(a_i\)’s and \(b\) as \((c_0,c_1,\dots,c_{m-1},c_m)\propto (a_0,a_1,\dots,a_{m-1},-1)\). We then have $$x_{\rm acc} = \sum_{i=0}^m c_{i} x_{k+i}.$$ Again, the parameters \(c_i\)’s depend on \(k\), but we omit this dependence.</p>



<p class="justify-text">In order to estimate the \(m+1\) parameters \(c_0,\dots,c_m\), we subtract two versions of the equality for \(k\) and \(k+1\), leading to $$\sum_{i=0}^m c_i ( x_{k+1+i} – x_{k+i} ) = 0.$$ Defining the matrix \(U \in \mathbb{R}^{m \times (m+1)}\) by $$U_{ji} = x_{k+1+i+j} – x_{k+i+j},$$ for \(i \in \{0,\dots,m\}\) and \(j \in \{0,\dots,m-1\}\), we have $$ U c = 0. $$ Together with the constraint \(1_{m+1}^\top c = 1\), this leads to the correct number of equations to estimate \(c\), from \(2m+1\) iterates \(x_k,\dots,x_{k+2m}\). The extrapolated iterate \(x^{(m)}_k\) is then $$x^{(m)}_k = x_{\rm acc} =   \sum_{i=0}^m c_{i} x_{k+i}.$$ Note that the extrapolation is exact when the sequence is exactly following a \(m\)-th order recursion. See an example of application on the Leibniz formula below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/shanks_Uc.png" alt="" width="424" class="wp-image-2320" height="330" />Higher-order acceleration through the Shanks transformation for the Leibniz formula. Acceleration is possible only up to machine precision.</figure></div>



<h2>Extension to vectors</h2>



<p class="justify-text">We now consider accelerating vector sequences \(x_k \in \mathbb{R}^d\). There are multiple approaches to extend acceleration from real numbers to vectors, as presented in [4, 5]. The simplest way is to apply high-order extrapolation to all coordinates separately (which is often called the vector \(\varepsilon\)-algorithm [10]); this depends however a lot on the chosen basis, requires too many linear systems to solve, and performs worse (see examples below for gradient descent). We now present a vector extension which exists under many names: the Eddy-Mesina method [6,7], reduced rank extrapolation [4, 11, 12], or Anderson acceleration [8]. </p>



<p class="justify-text">We want to model the sequence \(x_k \in \mathbb{R}^d\) as $$x_{k+1} = A x_{k} + b,$$ where \(A \in \mathbb{R}^{d \times d}\) and \(b \in \mathbb{R}^d\). By a simple variable / equation counting arguments, there are \(d^2+d\) parameters, and we thus need \(d+1\) equations in \(\mathbb{R}^d\), and thus \(d+2\) consecutive iterates, to estimate \(A\) and \(b\). </p>



<p class="justify-text">In order to use only \(m+2\) iterates, with \(m \) much less than \(d\), we will focus directly on the extrapolation equation $$x_{\rm acc} = c_0 x_k + c_1 x_{k+1}+  \cdots +c_m x_{k+m}, $$ with the constraint that \(c_0+c_1+\cdots+c_m =1\). Therefore, we will not try to explicitly fit the model parameters \(A\) and \(b\).</p>



<p class="justify-text">A sufficient condition for good extrapolation weights is that the extrapolated version is close for two consecutive \(k\)’s, that is $$c_0 x_k + c_1 x_{k+1}+  \cdots +c_m x_{k+m} \approx c_0 x_{k+1} + c_1 x_{k+2}+  \cdots +c_m x_{k+m+1},$$ which can be rewritten as $$c_0 (x_{k}-x_{k+1}) + c_1 ( x_{k+1} -x_{k+2}) + \cdots + c_m (x_{k+m} – x_{k+m+1}) \approx 0.$$ A natural criterion is thus to minimize the \(\ell_2\)-norm $$ \big\| c_0 \Delta x_{k} + c_1  \Delta x_{k+1} + \cdots + c_m \Delta x_{k+m} \big\|_2 \mbox{ such that } c_0+c_1+\cdots+c_m = 1,$$ where \(\Delta x_{i} = x_{i} -x_{i+1}\). Denoting \(U \in \mathbb{R}^{d \times (m+1)}\) the matrix with columns \(\Delta x_{k}, \dots,  \Delta x_{k+m}\), we need to minimize \(\| U c \|_2\) such that \(c^\top 1_{m+1} = 1\), whose solution is $$ c \propto ( U^\top U)^{-1} 1_{m+1}, \mbox{ that is, } c = \frac{1}{1_{m+1}^\top (U^\top U)^{-1} 1_{m+1}}  ( U^\top U)^{-1} 1_{m+1}.$$ Note that while the weights \(c_0,\dots,c_m\) sum to one, they may be negative, that is, the extrapolated sequence is not always a convex combination (hence the name extrapolation). Moreover, note that unless \(m\) is large enough, the optimal \(U c\) is in general not equal to zero (it is when modelling real sequences, see below).</p>



<p class="justify-text">For \(m=1\), the solution is particularly simple, as we need to minimize $$ \|\Delta x_{k+1}  – c_0 ( \Delta x_{k+1} – \Delta x_k ) \|^2,$$ leading to $$c_0 = \frac{\Delta x_{k+1}^\top ( \Delta x_{k+1} – \Delta x_k )}{ \|  \Delta x_{k+1} – \Delta x_k \|^2} \mbox{ and } c_1 = \frac{\Delta x_{k}^\top ( \Delta x_{k} – \Delta x_{k+1} )}{ \|  \Delta x_{k+1} – \Delta x_k \|^2}.$$ The acute reader can check that when \(d=1\), we recover Aitken’s formula. See an example in two dimensions below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/anderson_2d.gif" alt="" width="380" class="wp-image-2276" height="319" />Anderson acceleration in two dimensions. The sequence is following an auto-regressive process with a symmetric \(A\) with eigenvalues in \((-1,1)\). Anderson acceleration cancels the oscillation due to the eigenvalue with largest magnitude.</figure></div>



<p class="justify-text"><strong>Recovering one-dimensional sequence acceleration.</strong> Given a real sequence \(y_k \in \mathbb{R}\), we can define the vector \(x_k\) in \(\mathbb{R}^m\) as $$x_k = \left( \begin{array}{c} y_k \\  y_{k+1} \\ \vdots \\ y_{k+m-1} \end{array} \right).$$ One can then check that the matrix \(U\) defined for this vector sequence is exactly the same as the matrix \(U\) defined earlier for the real valued sequence. The optimal \(\| Uc \|\) is then equal to zero (which is not the case in general).</p>



<p class="justify-text"><strong>When is it exact?</strong> The derivation I followed is only intuitive, and as for the other acceleration mechanisms, a natural question is: when is it exact? We will consider linear recursions.</p>



<p class="justify-text"><strong>Analysis for linear recursions.</strong> Assuming that \(x_{k+1} = A x_{k} + b\) is exact for all \(k \geq 0\), then \(x_k – x_\ast = A^{k} ( x_0 – x_\ast)\), and thus, following [13], $$\sum_{i=0}^m c_i (x_{k+i}-x_{k+i+1}) = \sum_{i=0}^m c_i A^{i} A^{k}(I – A )(x_0 -x_\ast) = P_m(A)(I – A) A^{k}(x_0 -x_\ast) ,$$ for \(P_m(\sigma)  =  \sum_{i=0}^m c_i \sigma^{i}\) a \(m\)-th order polynomial such that \(P_m(1) = 1\). We can write \(Uc\)  as $$ Uc = P_m(A) ( x_k – x_{k+1}) = ( I – A)  P_m(A)  (x_k – x_\ast) .$$ The error between the true limit and the extrapolation is equal to: $$  x_\ast – \sum_{i=0}^m c_i x_{k+i} = \sum_{i=0}^m c_i ( x_\ast – x_{k+i}  ) = P_m(A) (   x_\ast -x_{k}) = (I-A)^{-1} Uc.$$ Thus, we have $$ \Big\| x_\ast – \sum_{i=0}^m c_i x_{k+i} \Big\|_2 \leq \| U c \|_2 \times  \|(I – A)^{-1}\|_{\rm op} \leq \|(I – A)^{-1}\|_{\rm op}  \|I – A\|_{\rm op} \|P_m(A) ( x_k – x_\ast)\|.  $$</p>



<p class="justify-text">The method will be exact when one can find a degree \(m\) polynomial so that \(P_m(A) (x_{k} – x_\ast) = 0 \), and a sufficient condition is that \(P_m(A)=0\), which is only possible if \(A\) had only \(m\) distinct eigenvalues. This is exactly minimal polynomial extrapolation [9]. Another situation is when \(m = d\) (like for the special case of real sequences above). </p>



<p class="justify-text">Otherwise, the method will be inexact, but the method can find a good polynomial \(P_m\), and the error is less than the infimum of \( \|P_m(A) ( x_k – x_\ast)\|\) over all polynomial of degree \(m\) such that \(P_m(1)=1\). Assuming that the matrix \(A\) is symmetric and with all eigenvalues between \(-\rho\) and \(\rho\) (which will be the case for the gradient method below), then the error is less than the infimum of \(\sup_{\sigma \in [-\rho,\rho]} |P_m(\sigma)|\),  which is attained for the Chebyshev polynomial (see a <a href="https://francisbach.com/chebyshev-polynomials/">previous post</a>). The improvement in terms of convergence is similar to Chebyshev acceleration, but (a) without the need to know \(\rho\) in advance (the method is totally adaptive), and (b) with a provable robustness when the iterates deviate from following an autoregressive process (see [13] for details).</p>



<p class="justify-text"><strong>Going beyond linear recursions. </strong>As presented, Anderson acceleration does not lead to stable acceleration (see the experiment below for gradient descent). The main reason is that when iterates deviate from an autoregressive process, or when the recursion is naturally noisy, the estimation of the parameters \(c\) is unstable, in particular because the matrix \(U^\top U\) which has to be inverted is severely ill-conditioned [14]. In a joint work with Damien Scieur and Alexandre d’Aspremont, we considered regularizing  the estimation of \(c\) by penalizing its \(\ell_2\)-norm. We thus minimize  \( \| U c \|_2^2 + \lambda \| c\|_2^2\) such that \(c^\top 1_{m+1} = 1\), whose solution is $$ c \propto ( U^\top U + \lambda I)^{-1} 1_{m+1}, \mbox{ that is, } c = \frac{1}{1_{m+1}^\top (U^\top U + \lambda I)^{-1} 1_{m+1}}  ( U^\top U +  \lambda I)^{-1} 1_{m+1}.$$ This simple modification leads to theoretical guarantees for non-linear recursions, and I will refer to it as regularized non-linear acceleration (RNA, see [13] for details; the “non-linearity” comes from the non-linear dependence of \(c\) on the iterates).</p>



<h2>Application to gradient descent</h2>



<p class="justify-text">We can apply RNA to the recursion, $$x_{k+1}  = x_k – \gamma \nabla f(x_k),$$ where \(f: \mathbb{R}^d \to \mathbb{R}^d\) is a differentiable function, and \(\gamma\) a step-size. In the plot below, we consider accelerating gradient descent with \(m\)-th order RNA, with \(m=8\). We compare this acceleration with applying RNA to each variable separately (“RNA-univ.”), and to the unregularized version (“Anderson”). We can see the benefits of our simple extrapolation steps, and in particular the instability of unregularized acceleration.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/anderson_grad_nonest.png" alt="" width="419" class="wp-image-2331" height="326" />Gradient descent on a regularized <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem, with 1000 observations in dimension 100. We compare regular gradient descent, to plain Anderson acceleration (with no regularization), RNA [13] applied to each variable separately, and RNA. All accelerations are with order \(m =8\).</figure></div>



<p class="justify-text">In order to obtain stronger benefits from non-linear acceleration, several extensions are considered in [13]; in particular line search to find the good regularization parameter \(\lambda\) is quite useful. Another interesting extension is the <em>online</em> version of the algorithm [16, section 2.5], where the extrapolated sequence is used directly within the acceleration procedure, and not as a separate sequence with no interaction with the original gradient method: this corresponds to using RNA to accelerate iterates coming from RNA!</p>



<p class="justify-text">Moreover, while the simplest theoretical guarantees come for deterministic convex optimization problems and gradient descent, RNA can be extended to stochastic algorithms [15] and to non-convex optimization problems such as the ones encountered in deep learning [16].</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I described acceleration techniques that combine iterates of an existing algorithm, without the need to understand finely the inner structure of the original algorithm. They come at little extra-cost and can provide strong benefits.</p>



<p class="justify-text">This month’s post was dedicated to algorithms which converge linearly, that is, the iterates are asymptotically equivalent to sums of exponentials. Next month, I will consider situations where the convergence is sublinear, where <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> excels.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. This post is based on joint work with Damien Scieur and Alexandre d’Aspremont, and in particular on their presentation slides. I would also like to thank them for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Alexander Aitken, On Bernoulli’s numerical solution of algebraic equations, <em>Proceedings of the Royal Society of Edinburgh</em>, 46:289–305, 1926.<br />[2] Peter Wynn. On a device for computing the \(e_m(S_n)\) transformation. <em>Mathematical Tables and Other Aids to Computation</em>, 91-96, 1956.<br />[3] Claude Brezinski. <em>Accélération de la convergence en analyse numérique</em>. Lecture notes in mathematics, Springer (584), 1977.<br />[4] David A. Smith, William F. Ford, Avram Sidi. Extrapolation methods for vector sequences. <em>SIAM review</em>, 29(2):199-233, 1987<br />[5] Allan J. Macleod. Acceleration of vector sequences by multi‐dimensional \(\Delta^2\) methods. <em>Communications in Applied Numerical Methods</em>, 2(4):385-392, 1986.<br />[6] Marián Mešina. Convergence acceleration for the iterative solution of the equations X= AX+ f. <em>Computer Methods in Applied Mechanics and Engineering</em>, <em>10</em>(2), 1977.<br />[7] Robert P. Eddy. Extrapolating to the limit of a vector sequence. <em>Information linkage between applied mathematics and industry</em>, 387-396, 1979.<br />[8] Homer F. Walker, Peng Ni. Anderson acceleration for fixed-point iterations. <em>SIAM Journal on Numerical Analysis</em>, 49(4):1715-1735, 2011.<br />[9] Sidi, Avram, William F. Ford, and David A. Smith. Acceleration of convergence of vector sequences. <em>SIAM Journal on Numerical Analysis</em> 23(1):178-196, 1986.<br />[10] Peter Wynn. Acceleration techniques for iterated vector and matrix problems. <em>Mathematics of Computation</em>, 16(79), 301-322,  1962.<br />[11] Stan Cabay,  L. W. Jackson. A polynomial extrapolation method for finding limits and antilimits of vector sequences. <em>SIAM Journal on Numerical Analysis</em>, 13(5), 734-752, 1976.<br />[12] Stig Skelboe. Computation of the periodic steady-state response of nonlinear networks by extrapolation methods. <em>IEEE Transactions on Circuits and Systems</em>, 27(3), 161-175, 1980.<br />[13] Damien Scieur, Alexandre d’Aspremont, Francis Bach. Regularized Nonlinear Acceleration. <em>Mathematical Programming</em>, 2018.<br />[14] Evgenij E. Tyrtyshnikov. How bad are Hankel matrices? <em>Numerische Mathematik</em>, 67(2):261-269, 1994.<br />[15] Damien Scieur, Alexandre d’Aspremont, Francis Bach. Nonlinear Acceleration of Stochastic Algorithms. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2017.<br />[16] Damien Scieur, Edouard Oyallon, Alexandre d’Aspremont, Francis Bach. Nonlinear Acceleration of Deep Neural Networks. Technical report, arXiv-1805.09639, 2018.</p>



<h2>Asymptotic analysis for Aitken’s \(\Delta^2\) process</h2>



<p class="justify-text">In one dimension, we do not need the auto-regressive model to be exact, and an asymptotic analysis is possible. The asymptotic condition corresponds to \(\displaystyle \frac{x_{k+1}-x_\ast}{x_k – x_\ast}\) converging to a constant \(a \in [-1,1)\). More precisely if $$\frac{x_{k+1}-x_\ast}{x_k – x_\ast} = a + \varepsilon_{k+1},$$ with \(\varepsilon_k\) tending to zero, then we can estimate \(a\) through the <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitkens \(\Delta^2\) method</a> [1] as, $$ a_{k+1} = \frac{x_{k+1}-x_k}{x_{k}-x_{k-1}} = \frac{(x_{k+1}-x_\ast) – (x_k-x_\ast) }{(x_{k} – x_\ast) -( x_{k-1}-x_\ast) } = \frac{ a + \varepsilon_{k+1} – 1}{1 – 1/(a+\varepsilon_{k})} = a + \varepsilon_{k+1} + o( \varepsilon_{k+1}).$$ A closer Taylor expansion leads to $$ a + \varepsilon_{k} + \frac{1}{a-1}( \varepsilon_{k+1}- \varepsilon_{k}) + O(\varepsilon_{k}^2).$$ We can then provide a better estimate of \(x_\ast\) as $$ \frac{x_{k+1} – a_{k+1}x_k}{1- a_{k+1}} = x_\ast + \frac{x_{k+1} -x_\ast – a_{k+1}( x_k-x_\ast)}{1- a_{k+1}} = x_\ast + \frac{(a+\varepsilon_{k+1}-a_{k+1}) ( x_k – x_\ast)}{1-a_{k+1}},$$ whose difference with \(x_\ast\) is equivalent to $$ \frac{(a+\varepsilon_k-a_{k+1}) ( x_k – x_\ast)}{1-a }  \sim \frac{\varepsilon_{k+1}- \varepsilon_{k}}{(1-a)^2} ( x_k – x_\ast).$$ We have thus provided an acceleration of order \(\displaystyle \frac{\varepsilon_{k+1}- \varepsilon_{k}}{(1-a)^2} \).</p>



<h2>High-order Shanks transformation</h2>



<p>In order to relate our formulas to classical expressions, we first rewrite the recursion for \(m=1\) and then \(m=2\), and then to general \(m\).</p>



<p class="justify-text"><strong>First-order recursion (Aitken’s \(\Delta^2\)).</strong> We write the autorecursive recursion as $$ (x_{k+1} – x_\ast) = a ( x_{k} – x_\ast) \Leftrightarrow c_0(x_k – x_\ast) + c_1 (x_{k+1}-x_\ast) = 0 ,$$ with the constraint \(c_0 + c_1 = 1\), that is, \(c_0 = \frac{-a}{1-a}\) and \(c_1 = \frac{1}{1-a}\). We can then write \(x_\ast = c_0 x_k + c_1 x_{k+1}\), and we have the linear system in \((c_0,c_1,x_\ast)\): $$  \left( \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ x_k &amp; x_{k+1} &amp; – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using Cramer’s formula $$x_{\rm acc} = x_\ast = \frac{\left| \begin{array}{cc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2}  \\ x_{k} &amp; x_{k+1} \end{array}\right|}{\left| \begin{array}{cc}  x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2}  \\ 1 &amp; 1 \end{array}\right|},$$  which leads to the same formula for \(x^{(1)}_k\).</p>



<p class="justify-text"><strong>Second-order recursion.</strong> Here, we only consider the case \(m=2\) for simplicity. We consider the model, $$c_0 (x_k – x_\ast) + c_1 (x_{k+1} – x_\ast)+ c_{2} (x_{k+2} – x_\ast) = 0, $$ with the normalization \(c_0 + c_1 + c_2 = 1\). We can then extract \(x_\ast\) as $$ x_\ast = c_0 x_k + c_1 x_{k+1} +  c_2 x_{k+2}.$$ In order to provide the extra \(2\) equations that are necessary to estimate the three parameters, we take first order differences and get $$c_0 (x_k -x_{k+1}) + c_1 (x_{k+1}-x_{k+2}) +  c_2 ( x_{k+2} – x_{k+3} ) =0,$$ for \(k\) and \(k+1\). This leads to the linear system in \((c_0,c_1, c_2, x_\ast)\): $$  \left( \begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3} &amp; 0 \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4} &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; 0  \\ x_{k} &amp; x_{k+1} &amp; x_{k+2} &amp;  – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ c_2 \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c}  0  \\ 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using <a href="https://en.wikipedia.org/wiki/Cramer%27s_rule">Cramer’s rule</a> (and classical manipulations of determinants) as $$x_k^{(2)} = \frac{\left| \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3}   \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4}   \\ x_{k} &amp; x_{k+1} &amp; x_{k+2}    \end{array} \right|}{\left| \begin{array}{ccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; x_{k+2} – x_{k+3}     \\ x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp;  x_{k+3} – x_{k+4}   \\ 1 &amp; 1 &amp; 1 &amp;  \end{array} \right|}.$$  </p>



<p class="justify-text">The formula extends to order \(m\) (see below) and is often called the Shanks transformation; it is cumbersome and not easy to use. However, the coefficients can be computed recursively (which is to be expected for a Hankel matrix, but rather tricky to derive), through Wynn’s \(\varepsilon\)-algorithm.  See [3] for a survey on acceleration and extrapolation.</p>



<p class="justify-text"><strong>Higher-order recursion.</strong> We consider the model, for \(m \geq 1\), $$c_0 (x_k – x_\ast) + c_1 (x_{k+1} – x_\ast) + \cdots + c_{m} (x_{k+m} – x_\ast) = 0, $$ with the normalization \(c_0 + c_1 + \cdots + c_m = 1\). We can then extract \(x_\ast\) as $$ x_\ast = c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}.$$ In order to provide the extra \(m\) equations, we take first order differences and get $$c_0 (x_k -x_{k+1}) + c_1 (x_{k+1}-x_{k+2}) + \cdots + c_m ( x_{k+m} – x_{k+m+1} ) =0,$$ for \(k, k+1,\dots, k+m-1\). This leads to the linear system in \((c_0,c_1,\cdots, c_m, x_\ast)\): $$  \left( \begin{array}{ccccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} &amp; 0 \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2} &amp; 0 \\ \vdots &amp; \vdots &amp;  &amp; \vdots  &amp; \vdots \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} &amp; 0 \\ 1 &amp; 1 &amp; \dots &amp; 1 &amp; 0  \\ x_{k+m+1} &amp; x_{k+m+2} &amp; \cdots &amp; x_{k+2m+1} &amp;  – 1  \end{array}\right)  \left( \begin{array}{c} c_0 \\ c_1 \\ \vdots \\ c_m \\ x_\ast  \end{array}\right) =  \left( \begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \\ 0 \end{array}\right), $$ which can be solved using Cramer’s formula $$x_\ast = \frac{\left|\begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2}   \\ \vdots &amp; \vdots &amp;  &amp; \vdots  \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} \\ x_{k+m+1} &amp; x_{k+m+2} &amp; \cdots &amp; x_{k+2m+1}  \end{array} \right|}{\left|\begin{array}{cccc} x_{k}-x_{k+1} &amp; x_{k+1}-x_{k+2} &amp; \cdots &amp; x_{k+m} – x_{k+m+1} \\  x_{k+1}-x_{k+2} &amp; x_{k+2}-x_{k+3} &amp; \cdots &amp; x_{k+m+1} – x_{k+m+2}   \\ \vdots &amp; \vdots &amp;  &amp; \vdots  \\ x_{k+m-1}-x_{k+m} &amp; x_{k+m}-x_{k+m+1} &amp; \cdots &amp; x_{k+2m-1} – x_{k+2m} \\ 1 &amp; 1 &amp; \cdots &amp; 1 \end{array}  \right|}.$$  </p>



<p class="justify-text">Like for \(m=1\), Wynn’s \(\varepsilon\)-algorithm can be used to compute the iterates recursively.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/acceleration-without-pain/"><span class="datestr">at February 04, 2020 07:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1302">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/02/02/upcoming-sigact-awards-deadlines/">Upcoming SIGACT Awards deadlines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From the SIGACT executive committee:</p>
<p>The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below.</p>
<ul>
<li><a href="https://sigact.org/prizes/g%C3%B6del/g%C3%B6del_call20.pdf">Gödel Prize</a>: deadline <strong>February 15</strong>, 2020.</li>
<li><a href="https://sigact.org/prizes/knuth.html">Knuth prize</a>: deadline <strong>April 12</strong>, 2020. Note that this deadline is a bit later than usual, because the award will be presented at FOCS this year. Next year the deadline will be moved back to February.</li>
<li><a href="https://sigact.org/prizes/service.html">SIGACT Distinguished Service Award</a>: deadline <strong>March 1</strong>, 2020.</li>
</ul></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/02/02/upcoming-sigact-awards-deadlines/"><span class="datestr">at February 02, 2020 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=703">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2020/02/02/the-will-of-the-framers/">The will of the framers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>What a great title for a legal thriller.  And for a history buff like me — something I never thought I would become and that must be a side effect of having learnt absolutely zero history in school — how arousing it is to hear what John Adams said in 1776, and details of the Great Debate, and the rhetoric!  It is  apt that I am following the discussion on an analog radio, my habit of the last 10 years or so, another thing I never thought I would do but that I actually find quite relaxing now.  It takes my mind off my own worry, and it soothes my eyes.  I recommend it at small doses to avoid sudden onset of nausea.  It is also apt that I follow it from <a href="https://www.redfin.com/MA/Chestnut-Hill/150-Woodland-Rd-02467/home/11457721">my house</a>, built two centuries ago though not as long ago as reported online, as I recently discovered sifting historical records. It comes to my mind that I now know what it means to renovate an old house. This is not something that I can recommend, but it is an experience that has had a profound and lasting impact on me.  I am aware of mortise locks, three-tab shingles, the terminological jungle of drywall et similia, caulking, baseboards, the difference between granite and quartz, between 4-inch and no backsplash, pvc, fixtures, the evolution of toilets and countless other things that I can’t list but that suddenly spring up in my mind when entering any house, including most recent additions such as the electrical system.</p>



<p>Once, while waiting for yet another late sub-contractor I wrote:</p>



<p>The revenge of the housekeepers</p>



<p>For centuries they slept in niches inside their masters’ houses, cooked meals in crammed kitchens, hand-washed laundry bent in basements. Now they are gone, but the houses still stand. Their niches are our offices where we can’t fit a table. We spend most of our family time in the crammed kitchen, the other rooms unused since nobody has the energy to shuttle the food, or clean. And faltering to hoist the laundry load from the basement we bump the head.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2020/02/02/the-will-of-the-framers/"><span class="datestr">at February 02, 2020 02:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=385">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/02/01/tcs-spring-approaches-talks-resume/">TCS+: Spring approaches, talks resume!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The winter hiatus is nearly over, and the new season of TCS+ is about to start! Our first two talks will take place on Feb 12th and 26th, respectively. We’re pretty excited about them…</p>
<ul>
<li>On February 12th, 1pm EST, <a href="https://www.cs.upc.edu/~atserias/">Albert Atserias</a> (Universitat Politecnica de Catalunya) will tell us all about how <em>Automating Resolution is NP-Hard</em>.</li>
<li>Then, on February 26th, <a href="http://www.henryyuen.net/">Henry Yuen</a> (University of Toronto) will speak about the recent proof that <em>MIP*=RE</em>.</li>
</ul>
<p>Stay tuned for the official talk announcements. And this is only the beginning of the semester…</p>
<p><em>In the meantime, if you have suggestions, <a href="https://sites.google.com/site/plustcs/suggest">here is the link</a>.</em></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/02/01/tcs-spring-approaches-talks-resume/"><span class="datestr">at February 01, 2020 08:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/">Krajíček’s Fest – Celebrating Jan Krajíček’s 60th Anniversary and his Contributions to Logic and Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 1, 2020 Tábor, Czech Republic https://www.dcs.warwick.ac.uk/~igorcarb/events/krajicek-fest/index.html We would like to invite you to participate in a workshop on the Logical Foundations of Complexity Theory to celebrate Prof. Jan Krajicek’s 60th anniversary. Preliminary list of speakers: Pavel Pudlák (Czech Adacemy of Sciences) Samuel Buss (University of California, San Diego) Neil Thapen (Czech Adacemy of Sciences) … <a href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/" class="more-link">Continue reading <span class="screen-reader-text">Krajíček’s Fest – Celebrating Jan Krajíček’s 60th Anniversary and his Contributions to Logic and Complexity</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/01/28/krajiceks-fest-celebrating-jan-krajiceks-60th-anniversary-and-his-contributions-to-logic-and-complexity/"><span class="datestr">at January 28, 2020 03:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7640">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/01/21/summer-school-on-statistical-physics-and-machine-learning/">Summer School on Statistical Physics and Machine Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Gerard Ben Arous, Surya Ganguli, Florent Krzakala and Lenka Zdeborova are organizing a <a href="http://leshouches2020.krzakala.org/">summer school on statistical physics of machine learning </a>on August 2-28, 2020 in Les Houches, France. If you don’t know Les Houches, it apparently  looks like this:</p>



<figure class="wp-block-image size-large"><img src="https://windowsontheory.files.wordpress.com/2020/01/chalet.jpg?w=414" alt="" class="wp-image-7641" /></figure>



<p>They are looking for applications from students, postdocs, and young researchers in physics &amp; math, as well computer scientists. While I am biased (I will be lecturing there too) I think the combination of lecturers, speakers, and audience members will yield a very unique opportunity for interaction across communities, and strongly encourage theoretical computer scientists to apply (which you can from the <a href="http://leshouches2020.krzakala.org/">website</a>). Let me also use this opportunity to remind people again of <a href="https://windowsontheory.org/2019/03/30/physics-computation-blog-post-round-up/">Tselil Schramm’s blog post</a> where she collected some of the lecture notes from the seminar we ran on physics &amp; computation.</p>



<p><strong>More information about the summer school:</strong></p>



<p>The “Les Houches school of physics”, situated close to Chamonix and the Mont Blanc in the French Alps, has a long history of forming generations of young researchers on the frontiers of their fields. Our school is aimed primarily at the growing audience of theoretical physicists and applied mathematicians interested in machine learning and high-dimensional data analysis, as well as to <strong>colleagues from other fields</strong> interested in this interface. <em>[my emphasis –Boaz]</em> We will cover basics and frontiers of high-dimensional statistics, machine learning, the theory of computing and learning, and probability theory. We will focus in particular on methods of statistical physics and their results in the context of current questions and theories related to machine learning and neural networks. The school will also cover examples of applications of machine learning methods in physics research, as well as other emerging applications of wide interest. Open questions and directions will be presented as well.</p>



<p>Students, postdocs and young researchers interested to participate in the event are invited to apply on the website <a href="http://leshouches2020.krzakala.org/" rel="nofollow">http://leshouches2020.krzakala.org/</a> before March 15, 2020. The capacity of the school is limited, and due to this constraint participants will be selected from the applicants and participants will be required to attend the whole event.</p>



<p><strong>Lecturers:</strong></p>



<ul><li>Boaz Barak (Harvard): Computational hardness perspectives</li><li>Giulio Biroli (ENS, Paris): High-dimensional dynamics</li><li>Michael Jordan (UC Berkeley): Optimization, diffusion &amp; economics</li><li>Marc Mézard (ENS, Paris): Message-Passing algorithms</li><li>Yann LeCun (Facebook AI, NYU). Challenges and directions in machine learning</li><li>Remi Monasson (ENS, Paris): Statistical physics or learning in neural networks</li><li>Andrea Montanari (Stanford): High-dimensional statistics &amp; neural networks</li><li>Maria Schuld (Univ. KwaZulu Natal &amp; Xanadu): Quantum machine learning</li><li>Haim Sompolinsky (Harvard &amp; Hebrew Univ.): Statistical mechanics of deep neural networks</li><li>Nathan Srebro (TTI-Chicago): Optimization and implicit regularisation</li><li>Miles Stoudenmire (Flatiron, NYC): Tensor network methods</li><li>Pierre Vandergheynst (EPFL, Lausanne): Graph signal processing &amp; neural networks</li></ul>



<p><strong>Invited Speakers</strong> (to be completed):</p>



<ul><li>Christian Borgs (UC Berkeley)</li><li>Jennifer Chayes (UC Berkeley)</li><li>Shirley Ho (Flatiron NYC)</li><li>Levent Sagun (Facebook AI)</li></ul></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/01/21/summer-school-on-statistical-physics-and-machine-learning/"><span class="datestr">at January 21, 2020 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1295">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2020/01/19/prize-nominations-relevant-to-tcs-due-on-april-1/">Prize nominations relevant to TCS: due on April 1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Breakthrough Prize committee is now accepting nominations for the <strong>Breakthrough Prize</strong> as well as <strong>New Horizons Prizes</strong> in Mathematics. The New Horizons Prizes are awarded to early-career researchers (Ph.D. within the last 10 years) who have already produced important work. In addition, for the first time, nominations will be taken for the <strong>Maryam Mirzakhani New Frontiers Prize</strong> – an annual $50,000 award that will be presented to early-career women mathematicians who have completed their PhDs within the previous two years.</p>
<p>Further information is available at <a href="https://breakthroughprize.org/Rules/3" rel="nofollow">https://breakthroughprize.org/Rules/3</a>. Nominations for 2021 are due on April 1, 2020.</p>
<p>Please consider nominating deserving candidates. The task of nominating someone for a high-profile award can be daunting, but the CATCS is available to help. Please contact us if you need help with preparing a nomination.</p>
<p> </p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2020/01/19/prize-nominations-relevant-to-tcs-due-on-april-1/"><span class="datestr">at January 20, 2020 03:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
