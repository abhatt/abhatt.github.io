<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 08, 2021 02:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02544">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02544">Polygon Placement Revisited: (Degree of Freedom + 1)-SUM Hardness and an Improvement via Offline Dynamic Rectangle Union</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=nnemann:Marvin.html">Marvin Künnemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nusser:Andr=eacute=.html">André Nusser</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02544">PDF</a><br /><b>Abstract: </b>We revisit the classical problem of determining the largest copy of a simple
polygon $P$ that can be placed into a simple polygon $Q$. Despite significant
effort, known algorithms require high polynomial running times. (Barequet and
Har-Peled, 2001) give a lower bound of $n^{2-o(1)}$ under the 3SUM conjecture
when $P$ and $Q$ are (convex) polygons with $\Theta(n)$ vertices each. This
leaves open whether we can establish (1) hardness beyond quadratic time and (2)
any superlinear bound for constant-sized $P$ or $Q$.
</p>
<p>In this paper, we affirmatively answer these questions under the $k$SUM
conjecture, proving natural hardness results that increase with each degree of
freedom (scaling, $x$-translation, $y$-translation, rotation): (1) Finding the
largest copy of $P$ that can be $x$-translated into $Q$ requires time
$n^{2-o(1)}$ under the 3SUM conjecture. (2) Finding the largest copy of $P$
that can be arbitrarily translated into $Q$ requires time $n^{2-o(1)}$ under
the 4SUM conjecture. (3) The above lower bounds are almost tight when one of
the polygons is of constant size: we obtain an $\tilde O((pq)^{2.5})$-time
algorithm for orthogonal polygons $P,Q$ with $p$ and $q$ vertices,
respectively. (4) Finding the largest copy of $P$ that can be arbitrarily
rotated and translated into $Q$ requires time $n^{3-o(1)}$ under the 5SUM
conjecture.
</p>
<p>We are not aware of any other such natural $($degree of freedom $+ 1)$-SUM
hardness for a geometric optimization problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02544"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6799651120968614396">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html">Reflections on Trusting ``Trustlessness'' in the era of ``Crypto'' Blockchains (Guest Post)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> </p><p><i>I trust Evangelos Georgiadis to do a guest post on Trust and Blockchain. </i></p><div>Today we have a guest post by Evangelos Georgiadis on Trust. It was written before Lance's post on trust <a href="https://blog.computationalcomplexity.org/2021/08/trusting-scientists.html">here</a> but it can be viewed as a followup to it. </div><div><br /></div><div>And now, here's E.G:</div><div><div><br /></div><div>==========================================================</div><div><br /></div><div>Trust is a funny concept, particularly in the realm of blockchains and "crypto".</div><div><br /></div><div>Do you trust the consensus mechanism of a public blockchain?</div><div><br /></div><div>Do you trust the architects that engineered the consensus mechanism?</div><div><br /></div><div>Do you trust the software engineers that implemented the code for the consensus mechanism?</div><div><br /></div><div>Do you trust the language that the software engineers used?</div><div><br /></div><div>Do you trust the underlying hardware that that the software is running?</div><div><br /></div><div>Theoretical Computer Science provides tools for some of this. But then the question becomes</div><div>Do you trust the program verifier?</div><div>Do you trust the proof of security?</div><div><br /></div><div>I touch on these issues in: </div><div><br /></div><div>                   <i>Reflections on Trusting ‘Trustlessness’ in the era of ”Crypto”/Blockchains</i></div><div><br /></div><div> which is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/cbit-4-2.pdf">here</a>. Its only 3 pages so enjoy!</div></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html"><span class="datestr">at November 07, 2021 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/150">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/150">TR21-150 |  Extractors: Low Entropy Requirements Colliding With Non-Malleability | 

	Eldon Chung, 

	Maciej Obremski, 

	Divesh Aggarwal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The known constructions of negligible error (non-malleable) two-source extractors can be broadly classified in three categories:

(1) Constructions where one source has min-entropy rate about $1/2$, the other source can have small min-entropy rate, but the extractor doesn't guarantee non-malleability.
(2) Constructions where one source is uniform, and the other can have small min-entropy rate, and the extractor guarantees non-malleability when the uniform source is tampered.
(3) Constructions where both sources have entropy rate very close to $1$ and the extractor guarantees non-malleability against the tampering of both sources. 

We introduce a new notion of collision resistant extractors and in using it we obtain a strong two source non-malleable extractor where we require the first source to have $0.8$ entropy rate and the other source can have min-entropy polylogarithmic in the length of the source.  

We show how the above extractor can be applied to obtain a non-malleable extractor with output rate $\frac 1 2$, which is optimal. We also show how, by using our extractor and extending the known protocol, one can  obtain a privacy amplification secure against memory tampering where the size of the secret output is almost optimal.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/150"><span class="datestr">at November 07, 2021 08:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/149">TR21-149 |  On polynomially many queries to NP or QMA oracles | 

	Dorian Rudolph, 

	Sevag Gharibian</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the complexity of problems solvable in deterministic polynomial time with access to an NP or Quantum Merlin-Arthur (QMA)-oracle, such as $P^{NP}$ and $P^{QMA}$, respectively.
The former allows one to classify problems more finely than the Polynomial-Time Hierarchy (PH), whereas the latter characterizes physically motivated problems such as Approximate Simulation (APX-SIM) [Ambainis, CCC 2014].
In this area, a central role has been played by the classes $P^{NP[\log]}$ and $P^{QMA[\log]}$, defined identically to $P^{NP}$ and $P^{QMA}$, except that only logarithmically many oracle queries are allowed. Here, [Gottlob, FOCS 1993] showed that if the adaptive queries made by a $P^{NP}$ machine have a "query graph" which is a tree, then this computation can be simulated in $P^{NP[\log]}$.

 In this work, we first show that for any verification class $C\in\{NP,MA,QCMA,QMA,QMA(2),NEXP,QMA_{\exp}\}$, any $P^C$ machine with a query graph of "separator number" $s$ can be simulated using deterministic time $\exp(s\log n)$ and $s\log n$ queries to a $C$-oracle.
When $s\in O(1)$ (which includes the case of $O(1)$-treewidth, and thus also of trees), this gives an upper bound of $P^{C[\log]}$, and when $s\in O(\log^k(n))$, this yields bound $QP^{C[\log^{k+1}]}$ (QP meaning quasi-polynomial time).
We next show how to combine Gottlob's "admissible-weighting function" framework with the "flag-qubit" framework of [Watson, Bausch, Gharibian, 2020], obtaining a unified approach for embedding $P^C$ computations directly into APX-SIM instances in a black-box fashion.
Finally, we formalize a simple no-go statement about polynomials (c.f. [Krentel, STOC 1986]): Given a multi-linear polynomial $p$ specified via an arithmetic circuit, if one can "weakly compress" $p$ so that its optimal value requires $m$ bits to represent, then $P^{NP}$ can be decided with only $m$ queries to an NP-oracle.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/149"><span class="datestr">at November 07, 2021 12:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">IDEAL mini-workshop on “New Directions on Robustness in ML”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
November 16, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-new-directions-on-robustness-in-ml/ As machine learning systems are being deployed in almost every aspect of decision-making, it is vital for them to be reliable and secure to adversarial corruptions and perturbations of various kinds. This workshop will explore newer notions of robustness and the different challenges that arise in designing reliable ML algorithms. … <a href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/" class="more-link">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “New Directions on Robustness in ML”</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/"><span class="datestr">at November 07, 2021 04:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1584">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/">News for October 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled su-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br />(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/"><span class="datestr">at November 07, 2021 02:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03046">Introduction to Coresets: Approximated Mean</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maalouf:Alaa.html">Alaa Maalouf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jubran:Ibrahim.html">Ibrahim Jubran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Dan.html">Dan Feldman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03046">PDF</a><br /><b>Abstract: </b>A \emph{strong coreset} for the mean queries of a set $P$ in ${\mathbb{R}}^d$
is a small weighted subset $C\subseteq P$, which provably approximates its sum
of squared distances to any center (point) $x\in {\mathbb{R}}^d$. A \emph{weak
coreset} is (also) a small weighted subset $C$ of $P$, whose mean approximates
the mean of $P$. While computing the mean of $P$ can be easily computed in
linear time, its coreset can be used to solve harder constrained version, and
is in the heart of generalizations such as coresets for $k$-means clustering.
In this paper, we survey most of the mean coreset construction techniques, and
suggest a unified analysis methodology for providing and explaining classical
and modern results including step-by-step proofs. In particular, we collected
folklore and scattered related results, some of which are not formally stated
elsewhere. Throughout this survey, we present, explain, and prove a set of
techniques, reductions, and algorithms very widespread and crucial in this
field. However, when put to use in the (relatively simple) mean problem, such
techniques are much simpler to grasp. The survey may help guide new researchers
unfamiliar with the field, and introduce them to the very basic foundations of
coresets, through a simple, yet fundamental, problem. Experts in this area
might appreciate the unified analysis flow, and the comparison table for
existing results. Finally, to encourage and help practitioners and software
engineers, we provide full open source code for all presented algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03046"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03034">Optimal Mixing Time for the Ising Model in the Uniqueness Regime</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xiaoyu.html">Xiaoyu Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Weiming.html">Weiming Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yitong.html">Yitong Yin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Xinyuan.html">Xinyuan Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03034">PDF</a><br /><b>Abstract: </b>We prove an optimal $O(n \log n)$ mixing time of the Glauber dynamics for the
Ising models with edge activity $\beta \in \left(\frac{\Delta-2}{\Delta},
\frac{\Delta}{\Delta-2}\right)$. This mixing time bound holds even if the
maximum degree $\Delta$ is unbounded.
</p>
<p>We refine the boosting technique developed in [CFYZ21], and prove a new
boosting theorem by utilizing the entropic independence defined in [AJK+21].
The theorem relates the modified log-Sobolev (MLS) constant of the Glauber
dynamics for a near-critical Ising model to that for an Ising model in a
sub-critical regime.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03034"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03033">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03033">Computational thresholds for the fixed-magnetization Ising model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlson:Charlie.html">Charlie Carlson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Ewan.html">Ewan Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolla:Alexandra.html">Alexandra Kolla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perkins:Will.html">Will Perkins</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03033">PDF</a><br /><b>Abstract: </b>The ferromagnetic Ising model is a model of a magnetic material and a central
topic in statistical physics. It also plays a starring role in the algorithmic
study of approximate counting: approximating the partition function of the
ferromagnetic Ising model with uniform external field is tractable at all
temperatures and on all graphs, due to the randomized algorithm of Jerrum and
Sinclair. Here we show that hidden inside the model are hard computational
problems. For the class of bounded-degree graphs we find computational
thresholds for the approximate counting and sampling problems for the
ferromagnetic Ising model at fixed magnetization (that is, fixing the number of
$+1$ and $-1$ spins). In particular, letting $\beta_c(\Delta)$ denote the
critical inverse temperature of the zero-field Ising model on the infinite
$\Delta$-regular tree, and $\eta_{\Delta,\beta,1}^+$ denote the mean
magnetization of the zero-field $+$ measure on the infinite $\Delta$-regular
tree at inverse temperature $\beta$, we prove, for the class of graphs of
maximum degree $\Delta$:
</p>
<p>1. For $\beta &lt; \beta_c(\Delta)$ there is an FPRAS and efficient sampling
scheme for the fixed-magnetization Ising model for all magnetizations $\eta$.
</p>
<p>2. For $\beta &gt; \beta_c(\Delta)$, there is an FPRAS and efficient sampling
scheme for the fixed-magnetization Ising model for magnetizations $\eta$ such
that $|\eta| &gt;\eta_{\Delta,\beta,1}^+ $.
</p>
<p>3. For $\beta &gt; \beta_c(\Delta)$, there is no FPRAS for the
fixed-magnetization Ising model for magnetizations $\eta$ such that $|\eta|
&lt;\eta_{\Delta,\beta,1}^+ $ unless NP=RP\@.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03033"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03005">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03005">Parallel Global Edge Switching for the Uniform Sampling of Simple Graphs with Prescribed Degrees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allendorf:Daniel.html">Daniel Allendorf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyer:Ulrich.html">Ulrich Meyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penschuck:Manuel.html">Manuel Penschuck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tran:Hung.html">Hung Tran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03005">PDF</a><br /><b>Abstract: </b>The uniform sampling of simple graphs matching a prescribed degree sequence
is an important tool in network science, e.g., to construct graph generators or
null-models. Here, the Edge Switching Markov Chain (ES-MC) is a common choice.
Given an arbitrary simple graph with the required degree sequence, ES-MC
carries out a large number of small changes involving at most four edges to
eventually obtain a uniform sample. In practice, reasonably short runs
efficiently yield approximate uniform samples.
</p>
<p>We first engineer a simple sequential ES-MC implementation representing the
graph in a hash-set. Despite its simplicity and to the best of our knowledge,
our implementation significantly outperforms all openly available solutions.
</p>
<p>Secondly, we propose the Global Edge Switching Markov Chain (G-ES-MC) and
show that it, too, converges to a uniform distribution. We provide empirical
evidence that G-ES-MC requires not more switches than ES-MC (and often fewer).
</p>
<p>Thirdly, we engineer shared-memory parallel algorithms for ES-MC and G-ES-MC;
we find that they benefit from the easier dependency structure of the G-ES-MC.
In an empirical evaluation, we demonstrate the scalability of our
implementations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03005"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02999">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02999">Quantum search-to-decision reductions and the state synthesis problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Irani:Sandy.html">Sandy Irani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natarajan:Anand.html">Anand Natarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nirkhe:Chinmay.html">Chinmay Nirkhe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Sujit.html">Sujit Rao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuen:Henry.html">Henry Yuen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02999">PDF</a><br /><b>Abstract: </b>It is a useful fact in classical computer science that many search problems
are reducible to decision problems; this has led to decision problems being
regarded as the $\textit{de facto}$ computational task to study in complexity
theory. In this work, we explore search-to-decision reductions for quantum
search problems, wherein a quantum algorithm makes queries to a classical
decision oracle to output a desired quantum state. In particular, we focus on
search-to-decision reductions for $\mathsf{QMA}$, and show that there exists a
quantum polynomial-time algorithm that can generate a witness for a
$\mathsf{QMA}$ problem up to inverse polynomial precision by making one query
to a $\mathsf{PP}$ decision oracle. We complement this result by showing that
$\mathsf{QMA}$-search does $\textit{not}$ reduce to $\mathsf{QMA}$-decision in
polynomial-time, relative to a quantum oracle.
</p>
<p>We also explore the more general $\textit{state synthesis problem}$, in which
the goal is to efficiently synthesize a target state by making queries to a
classical oracle encoding the state. We prove that there exists a classical
oracle with which any quantum state can be synthesized to inverse polynomial
precision using only one oracle query and to inverse exponential precision
using two oracle queries. This answers an open question of Aaronson from 2016,
who presented a state synthesis algorithm that makes $O(n)$ queries to a
classical oracle to prepare an $n$-qubit state, and asked if the query
complexity could be made sublinear.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02999"><span class="datestr">at November 07, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02992">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02992">The Shortest Even Cycle Problem is Tractable</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bj=ouml=rklund:Andreas.html">Andreas Björklund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Husfeldt:Thore.html">Thore Husfeldt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaski:Petteri.html">Petteri Kaski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02992">PDF</a><br /><b>Abstract: </b>Given a directed graph, we show how to efficiently find a shortest (directed,
simple) cycle on an even number of vertices. As far as we know, no
polynomial-time algorithm was previously known for this problem. In fact,
finding any even cycle in a directed graph in polynomial time was open for more
than two decades until Robertson, Seymour, and Thomas (Ann. of Math. (2) 1999)
and, independently, McCuaig (Electron. J. Combin. 2004; announced jointly at
STOC 1997) gave an efficiently testable structural characterisation of
even-cycle-free directed graphs.
</p>
<p>Methodologically, our algorithm relies on algebraic fingerprinting and
randomized polynomial identity testing over a finite field, and uses a
generating polynomial implicit in Vazirani and Yannakakis ( Discrete Appl.
Math. 1989) that enumerates weighted cycle covers as a difference of a
permanent and a determinant polynomial. The need to work with the permanent is
where our main technical contribution occurs. We design a family of finite
commutative rings of characteristic 4 that simultaneously (i) give a
nondegenerate representation for the generating polynomial identity via the
permanent and the determinant, (ii) support efficient permanent computations,
and (iii) enable emulation of finite-field arithmetic in characteristic 2. Here
our work is foreshadowed by that of Bj\"orklund and Husfeldt (SIAM J. Comput.
2019), who used a considerably less efficient ring design to obtain a
polynomial-time algorithm for the shortest two disjoint paths problem.
</p>
<p>Building on work of Gilbert and Tarjan (Numer. Math. 1978) as well as Alon
and Yuster (J. ACM 2013), we also show how ideas from the nested dissection
technique for solving linear equation systems leads to faster algorithm designs
when we have control on the separator structure of the input graph; for
example, this happens when the input has bounded genus.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02992"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02967">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02967">An Empirical Comparison of the Quadratic Sieve Factoring Algorithm and the Pollard Rho Factoring Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zongxia.html">Zongxia Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gasarch:William.html">William Gasarch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02967">PDF</a><br /><b>Abstract: </b>One of the most significant challenges on cryptography today is the problem
of factoring large integers since there are no algorithms that can factor in
polynomial time, and factoring large numbers more than some limits(200 digits)
remain difficult. The security of the current cryptosystems depends on the
hardness of factoring large public keys. In this work, we want to implement two
existing factoring algorithms - pollard-rho and quadratic sieve - and compare
their performance. In addition, we want to analyze how close is the theoretical
time complexity of both algorithms compared to their actual time complexity and
how bit length of numbers can affect quadratic sieve's performance. Finally, we
verify whether the quadratic sieve would do better than pollard-rho for
factoring numbers smaller than 80 bits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02967"><span class="datestr">at November 07, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02773">Danzer's Problem, Effective Constructions of Dense Forests and Digital Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsokanos:Ioannis.html">Ioannis Tsokanos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02773">PDF</a><br /><b>Abstract: </b>A 1965 problem due to Danzer asks whether there exists a set in Euclidean
space with finite density intersecting any convex body of volume one. A recent
approach to this problem is concerned with the construction of dense forests
and is obtained by a suitable weakening of the volume constraint. A dense
forest is a discrete point set of finite density getting uniformly close to
long enough line segments. The distribution of points in a dense forest is then
quantified in terms of a visibility function. Another way to weaken the
assumptions in Danzer's problem is by relaxing the density constraint. In this
respect, a new concept is introduced in this paper, namely that of an optical
forest. An optical forest in $\mathbb{R}^{d}$ is a point set with optimal
visibility but not necessarily with finite density. In the literature, the best
constructions of Danzer sets and dense forests lack effectivity. The goal of
this paper is to provide constructions of dense and optical forests which yield
the best known results in any dimension $d \ge 2$ both in terms of visibility
and density bounds and effectiveness. Namely, there are three main results in
this work: (1) the construction of a dense forest with the best known
visibility bound which, furthermore, enjoys the property of being
deterministic; (2) the deterministic construction of an optical forest with a
density failing to be finite only up to a logarithm and (3) the construction of
a planar Peres-type forest (that is, a dense forest obtained from a
construction due to Peres) with the best known visibility bound. This is
achieved by constructing a deterministic digital sequence satisfying strong
dispersion properties.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02773"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02755">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02755">A Compound Logic for Modification Problems: Big Kingdoms Fall from Within</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stamoulis:Giannos.html">Giannos Stamoulis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thilikos:Dimitrios_M=.html">Dimitrios M. Thilikos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02755">PDF</a><br /><b>Abstract: </b>We introduce a novel model-theoretic framework inspired from graph
modification and based on the interplay between model theory and algorithmic
graph minors. We propose a new compound logic operating with two types of
sentences, expressing graph modification: the modulator sentence, defining some
property of the modified part of the graph, and the target sentence, defining
some property of the resulting graph. In our framework, modulator sentences are
in monadic second-order logic and have models of bounded treewidth, while
target sentences express first-order logic properties along with
minor-exclusion. Our logic captures problems that are not definable in first
order logic and, moreover, may have instances of unbounded treewidth. Also, it
permits the modelling of wide families of problems involving vertex/edge
removals, alternative modulator measures (such as elimination distance or
G-treewidth), multistage modifications, and various cut problems. Our main
result is that, for this compound logic, model checking can be done in
quadratic time. This algorithmic meta-theorem encompasses, unifies, and extends
all known meta-algorithmic results on minor-closed graph classes. Moreover, all
derived algorithms are constructive and this, as a byproduct, extends the
constructibility horizon of the algorithmic applications of the Graph Minors
theorem of Robertson and Seymour. The proposed logic can be seen as a general
framework to capitalize on the potential of the irrelevant vertex technique.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02755"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02688">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02688">The No Endmarker Theorem for One-Way Probabilistic Pushdown Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamakami:Tomoyuki.html">Tomoyuki Yamakami</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02688">PDF</a><br /><b>Abstract: </b>In various models of one-way pushdown automata, the explicit use of two
designated endmarkers on a read-once input tape has proven to be extremely
useful for making a conscious, final decision on the acceptance/rejection of
each input word right after reading the right endmarker. With no endmarkers, by
contrast, a machine must constantly stay in either accepting or rejecting
states at any moment since it never notices the end of the input instance. This
situation, however, helps us analyze the behavior of the machine whose tape
head makes the consecutive moves on all prefixes of a given extremely long
input word. Since those two machine formulations have their own advantages, it
is natural to ask whether the endmarkers are truly necessary to correctly
recognize languages. In the deterministic and nondeterministic models, it is
well-known that the endmarkers are removable without changing the acceptance
criteria of each input instance. This paper proves that, for a more general
model of one-way probabilistic pushdown automata, the endmarkers are also
removable. This is proven by employing probabilistic transformations from an
"endmarker" machine to an equivalent "no-endmarker" machine at the cost of
double exponential state complexity without compromising its error probability.
By setting this error probability appropriately, our proof also provides an
alternative proof to both the deterministic and the nondeterministic models as
well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02688"><span class="datestr">at November 07, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02657">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02657">Average Sensitivity of Dynamic Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumabe:Soh.html">Soh Kumabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02657">PDF</a><br /><b>Abstract: </b>When processing data with uncertainty, it is desirable that the output of the
algorithm is stable against small perturbations in the input. Varma and Yoshida
[SODA'21] recently formalized this idea and proposed the notion of average
sensitivity of algorithms, which is roughly speaking, the average Hamming
distance between solutions for the original input and that obtained by deleting
one element from the input, where the average is taken over the deleted
element.
</p>
<p>In this work, we consider average sensitivity of algorithms for problems that
can be solved by dynamic programming. We first present a
$(1-\delta)$-approximation algorithm for finding a maximum weight chain (MWC)
in a transitive directed acyclic graph with average sensitivity
$O(\delta^{-1}\log^3 n)$, where $n$ is the number of vertices in the graph. We
then show algorithms with small average sensitivity for various dynamic
programming problems by reducing them to the MWC problem while preserving
average sensitivity, including the longest increasing subsequence problem, the
interval scheduling problem, the longest common subsequence problem, the
longest palindromic subsequence problem, the knapsack problem with integral
weight, and the RNA folding problem. For the RNA folding problem, our reduction
is highly nontrivial because a naive reduction generates an exponentially large
graph, which only provides a trivial average sensitivity bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02657"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02614">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02614">Finding All Leftmost Separators of Size $\leq k$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belbasi:Mahdi.html">Mahdi Belbasi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/F=uuml=rer:Martin.html">Martin Fürer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02614">PDF</a><br /><b>Abstract: </b>We define a notion called leftmost separator of size at most $k$. A leftmost
separator of size $k$ is a minimal separator $S$ that separates two given sets
of vertices $X$ and $Y$ such that we "cannot move $S$ more towards $X$" such
that $|S|$ remains smaller than the threshold. One of the incentives is that by
using leftmost separators we can improve the time complexity of treewidth
approximation. Treewidth approximation is a problem which is known to have a
linear time FPT algorithm in terms of input size, and only single exponential
in terms of the parameter, treewidth. It is not known whether this result can
be improved theoretically. However, the coefficient of the parameter $k$ (the
treewidth) in the exponent is large. Hence, our goal is to decrease the
coefficient of $k$ in the exponent, in order to achieve a more practical
algorithm. Hereby, we trade a linear-time algorithm for an $\mathcal{O}(n \log
n)$-time algorithm. The previous known $\mathcal{O}(f(k) n \log n)$-time
algorithms have dependences of $2^{24k}k!$, $2^{8.766k}k^2$ (a better analysis
shows that it is $2^{7.671k}k^2$), and higher. In this paper, we present an
algorithm for treewidth approximation which runs in time
$\mathcal{O}(2^{6.755k}\ n \log n)$,
</p>
<p>Furthermore, we count the number of leftmost separators and give a tight
upper bound for them. We show that the number of leftmost separators of size
$\leq k$ is at most $C_{k-1}$ (Catalan number). Then, we present an algorithm
which outputs all leftmost separators in time
$\mathcal{O}(\frac{4^k}{\sqrt{k}}n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02614"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02598">Universal Private Estimators</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Wei.html">Wei Dong</a>, Ke Yi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02598">PDF</a><br /><b>Abstract: </b>We present $\textit{universal}$ estimators for the statistical mean,
variance, and scale (in particular, the interquartile range) under pure
differential privacy. These estimators are universal in the sense that they
work on an arbitrary, unknown distribution $\mathcal{P}$ over $\mathbb{R}$,
while yielding strong utility guarantees except for ill-behaved $\mathcal{P}$.
For certain distribution families like Gaussians or heavy-tailed distributions,
we show that our universal estimators match or improve existing estimators,
which are often specifically designed for the given family and under
$\textit{priori}$ boundedness assumptions on the mean and variance of
$\mathcal{P}$. The removal of these boundedness assumptions is surprising, as
existing work believes that they are necessary under pure differential privacy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02598"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02591">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02591">Minimum-Complexity Graph Simplification under Fr\'echet-Like Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirzanezhad:Majid.html">Majid Mirzanezhad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wenk:Carola.html">Carola Wenk</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02591">PDF</a><br /><b>Abstract: </b>Simplifying graphs is a very applicable problem in numerous domains,
especially in computational geometry. Given a geometric graph and a threshold,
the minimum-complexity graph simplification asks for computing an alternative
graph of minimum complexity so that the distance between the two graphs remains
at most the threshold. In this paper, we propose several NP-hardness and
algorithmic results depending on the type of input and simplified graphs, the
vertex placement of the simplified graph, and the distance measures between
them (graph and traversal distances [1,2]). In general, we show that for
arbitrary input and output graphs, the problem is NP-hard under some specific
vertex-placement of the simplified graph. When the input and output are trees,
and the graph distance is applied from the simplified tree to the input tree,
we give an $O(kn^5)$ time algorithm, where $k$ is the number of the leaves of
the two trees that are identical and $n$ is the number of vertices of the
input.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02591"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02579">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02579">Reallocation Problems with Minimum Completion Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ishii:Toshimasa.html">Toshimasa Ishii</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawahara:Jun.html">Jun Kawahara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makino:Kazuhisa.html">Kazuhisa Makino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ono:Hirotaka.html">Hirotaka Ono</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02579">PDF</a><br /><b>Abstract: </b>Reallocation scheduling is one of the most fundamental problems in various
areas such as supply chain management, logistics, and transportation science.
In this paper, we introduce the reallocation problem that models the scheduling
in which products are with fixed cost, non-fungible, and reallocated in
parallel, and comprehensively study the complexity of the problem under various
settings of the transition time, product size, and capacities. We show that the
problem can be solved in polynomial time for a fundamental setting where the
product size and transition time are both uniform. We also show that the
feasibility of the problem is NP-complete even for little more general
settings, which implies that no polynomial-time algorithm constructs a feasible
schedule of the problem unless P$=$NP. We then consider the relaxation of the
problem, which we call the capacity augmentation, and derive a reallocation
schedule feasible with the augmentation such that the completion time is at
most the optimal of the original problem. When the warehouse capacity is
sufficiently large, we design constant-factor approximation algorithms under
all the settings. We also show the relationship between the reallocation
problem and the bin packing problem when the warehouse and carry-in capacities
are sufficiently large.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02579"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02572">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02572">A Constant-Factor Approximation for Quasi-bipartite Directed Steiner Tree on Minor-Free Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friggstad:Zachary.html">Zachary Friggstad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mousavi:Ramin.html">Ramin Mousavi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02572">PDF</a><br /><b>Abstract: </b>We give the first constant-factor approximation algorithm for quasi-bipartite
instances of Directed Steiner Tree on graphs that exclude fixed minors. In
particular, for $K_r$-minor-free graphs our approximation guarantee is
$O(r\cdot\sqrt{\log r})$ and, further, for planar graphs our approximation
guarantee is 20.
</p>
<p>Our algorithm uses the primal-dual scheme. We employ a more involved method
of determining when to buy an edge while raising dual variables since, as we
show, the natural primal-dual scheme fails to raise enough dual value to pay
for the purchased solution. As a consequence, we also demonstrate integrality
gap upper bounds on the standard cut-based linear programming relaxation for
the Directed Steiner Tree instances we consider.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02572"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02480">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02480">Linear-time Minimization of Wheeler DFAs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alanko:Jarno.html">Jarno Alanko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cotumaccio:Nicola.html">Nicola Cotumaccio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02480">PDF</a><br /><b>Abstract: </b>Wheeler DFAs (WDFAs) are a sub-class of finite-state automata which is
playing an important role in the emerging field of compressed data structures:
as opposed to general automata, WDFAs can be stored in just $\log\sigma + O(1)$
bits per edge, $\sigma$ being the alphabet's size, and support optimal-time
pattern matching queries on the substring closure of the language they
recognize. An important step to achieve further compression is minimization.
When the input $\mathcal A$ is a general deterministic finite-state automaton
(DFA), the state-of-the-art is represented by the classic Hopcroft's algorithm,
which runs in $O(|\mathcal A|\log |\mathcal A|)$ time. This algorithm stands at
the core of the only existing minimization algorithm for Wheeler DFAs, which
inherits its complexity. In this work, we show that the minimum WDFA equivalent
to a given input WDFA can be computed in linear $O(|\mathcal A|)$ time. When
run on de Bruijn WDFAs built from real DNA datasets, an implementation of our
algorithm reduces the number of nodes from 14% to 51% at a speed of more than 1
million nodes per second.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02480"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02478">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02478">HOLZ: High-Order Entropy Encoding of Lempel-Ziv Factor Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02478">PDF</a><br /><b>Abstract: </b>We propose a new representation of the offsets of the Lempel-Ziv (LZ)
factorization based on the co-lexicographic order of the processed prefixes.
The selected offsets tend to approach the k-th order empirical entropy. Our
evaluations show that this choice of offsets is superior to the rightmost LZ
parsing and the bit-optimal LZ parsing on datasets with small high-order
entropy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02478"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02318">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02318">Nearly Tight Lower Bounds for Succinct Range Minimum Query</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Mingmou.html">Mingmou Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02318">PDF</a><br /><b>Abstract: </b>Given an array of distinct integers $A[1\ldots n]$, the Range Minimum Query
(RMQ) problem requires us to construct a data structure from $A$, supporting
the RMQ query: given an interval $[a,b]\subseteq[1,n]$, return the index of the
minimum element in subarray $A[a\ldots b]$, i.e. return
$\text{argmin}_{i\in[a,b]}A[i]$. The fundamental problem has a long history.
The textbook solution which uses $O(n)$ words of space and $O(1)$ time by
Gabow, Bentley, Tarjan (STOC 1984) and Harel, Tarjan (SICOMP 1984) dates back
to 1980s. The state-of-the-art solution is presented by Fischer, Heun (SICOMP
2011) and Navarro, Sadakane (TALG 2014). The solution uses
$2n+n/\left(\frac{\log n}{t}\right)^t+\tilde{O}(n^{3/4})$ bits of space and
$O(t)$ query time, assuming the word-size is $\Theta(\log n)$ bits. On the
other hand, the only known lower bound is proven by Liu and Yu (STOC 2020).
They show that any data structure which solves RMQ in $t$ query time must use
$2n+n/(\log n)^{O(t^2\log^2t)}$ bits of space, assuming the word-size is
$\Theta(\log n)$ bits.
</p>
<p>In this paper, we prove nearly tight lower bound for this problem. We show
that, for any data structure which solves RMQ in $t$ query time, $2n+n/(\log
n)^{O(t\log^2t)}$ bits of space is necessary in the cell-probe model with
word-size $\Theta(\log n)$. We emphasize that, for any $r$, we present a lower
bound of $t=\Omega(t_{opt}/\log^3 t_{opt})$, where $t_{opt}$ is the optimal
time cost when the data structure is allowed to consume $2n+r$ bits of space.
Hence our lower bound is nearly tight.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02318"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.05738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.05738">Lower Bound for Succinct Range Minimum Query</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Mingmou.html">Mingmou Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Huacheng.html">Huacheng Yu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.05738">PDF</a><br /><b>Abstract: </b>Given an integer array $A[1..n]$, the Range Minimum Query problem (RMQ) asks
to preprocess $A$ into a data structure, supporting RMQ queries: given $a,b\in
[1,n]$, return the index $i\in[a,b]$ that minimizes $A[i]$, i.e.,
$\mathrm{argmin}_{i\in[a,b]} A[i]$. This problem has a classic solution using
$O(n)$ space and $O(1)$ query time by Gabow, Bentley, Tarjan (STOC, 1984) and
Harel, Tarjan (SICOMP, 1984). The best known data structure by Fischer, Heun
(SICOMP, 2011) and Navarro, Sadakane (TALG, 2014) uses $2n+n/(\frac{\log
n}{t})^t+\tilde{O}(n^{3/4})$ bits and answers queries in $O(t)$ time, assuming
the word-size is $w=\Theta(\log n)$. In particular, it uses
$2n+n/\mathrm{poly}\log n$ bits of space as long as the query time is a
constant.
</p>
<p>In this paper, we prove the first lower bound for this problem, showing that
$2n+n/\mathrm{poly}\log n$ space is necessary for constant query time. In
general, we show that if the data structure has query time $O(t)$, then it must
use at least $2n+n/(\log n)^{\tilde{O}(t^2)}$ space, in the cell-probe model
with word-size $w=\Theta(\log n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.05738"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br />
Email: See the url</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/"><span class="datestr">at November 05, 2021 10:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br />
Email: bmkapron@uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/"><span class="datestr">at November 05, 2021 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">Assistant Professor at Charles University (apply by January 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered.</p>
<p>Website: <a href="https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022">https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/"><span class="datestr">at November 05, 2021 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">Summer Research Intern at Adobe Research  (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. Interested candidates should send their CV to Zhao.</p>
<p>Website: <a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en</a><br />
Email: zsong@adobe.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/"><span class="datestr">at November 05, 2021 04:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/">Faculty at University of Haifa at Oranim College (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022.</p>
<p>Website: <a href="https://mathphys.haifa.ac.il/en/announcements/">https://mathphys.haifa.ac.il/en/announcements/</a><br />
Email: ackerman@sci.haifa.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/"><span class="datestr">at November 04, 2021 01:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-3367673395710171015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html">HotNets Presentation : Zero-CPU Collection with Direct Telemetry Access</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>HotNets has asked that we let people know that the 2021 presentations <a href="https://www.youtube.com/channel/UCZZ5nf4RNDIIe4nifI8grwQ/videos">are available here</a>.  I'm using that an excuse to highlight our paper on Zero-CPU Collection with Direct Telemetry Access (<a href="https://arxiv.org/abs/2110.05438">arxiv version here</a>), but really I want to highlight the talk by graduate student Jonatan Langlet (Queen Mary University of London) who, as is the nature of graduate students, did all of the real work, and who really did a great job on <a href="https://www.youtube.com/watch?v=_M8AbF_f8Kk&amp;t=2s">the talk (direct link)</a>.  If you guessed from my involvement this involves hashing in some way, your maximum likelihood estimate turns out to be correct.</p><p>I think our work fits the HotNets call, which asks for new approaches and preliminary work.  Specifically, the call for the HotNets workshop says this:</p><p></p><blockquote><p>We invite researchers and practitioners to submit short position papers. We encourage papers that identify fundamental open questions, advocate a new approach, offer a constructive critique of the state of networking research, re-frame or debunk existing work, report unexpected early results from a deployment, report on promising but unproven ideas, or propose new evaluation methods. Novel ideas need not be supported by full evaluations; well-reasoned arguments or preliminary evaluations can support the possibility of the paper’s claims.</p><p>We seek early-stage work, where the authors can benefit from community feedback. An ideal submission has the potential to open a line of inquiry for the community that results in multiple conference papers in related venues (SIGCOMM, NSDI, CoNEXT, SOSP, OSDI, MobiCom, MobiSys, etc.), rather than a single follow-on conference paper. The program committee will explicitly favor early work and papers likely to stimulate reflection and discussion over “conference papers in miniature”.</p></blockquote><p>There are similar other "Hot" workshops in other areas, and it was about <a href="http://mybiasedcoin.blogspot.com/2007/08/hottheory-workshop.html">14 years ago that I asked whether CS theory should have a HotTheory workshop</a>.  There's been a proliferation of new conferences and workshops in theory since then, but none of them really seem to have this flavor.  So maybe it's worth asking again whether a HotTheory workshop would make sense?  Or do existing theory events meet the theory community needs?</p><p></p><p><br /></p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html"><span class="datestr">at November 04, 2021 01:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/">Lecturer in Theoretical Computer Science, Sheffield (UK) at University of Sheffield (apply by November 16, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science">https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science</a><br />
Email: g.j.brown@sheffield.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/"><span class="datestr">at November 04, 2021 12:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">Professor, Associate Professor, Assistant Professor at George Washington University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications to multiple faculty positions at all ranks. Our search is focused on machine learning; artificial intelligence; computer and distributed systems; security and privacy; and candidates that can support our multidisciplinary initiatives in Smart and Trustworthy Systems and Meaningful Computing, broadly defined.</p>
<p>Website: <a href="https://www.gwu.jobs/postings/87400">https://www.gwu.jobs/postings/87400</a><br />
Email: cssearch@gwu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/"><span class="datestr">at November 04, 2021 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/11/04/perceptron/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/11/04/perceptron/">The Perceptron as a prototype for machine learning theory.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Just as many of the algorithms and community practices of machine learning were invented <a href="http://www.argmin.net/2021/10/20/highleyman/">in the late 1950s and early 1960s</a>, the foundations of machine learning theory were also established during this time. Many of the analyses of this period were strikingly simple, had surprisingly precise constants, and provided prescient guidelines for contemporary machine learning practice. Here, I’ll summarize the study of the Perceptron, highlighting both its algorithmic and statistical analyses, and using it as a prototype to illustrate further how prediction deviates from the umbrella of classical statistics.</p>

<p>Let’s begin with a classification problem where each individual from some population has a feature vector $x$ and an associated binary label $y$ that we take as valued $\pm 1$ for notational convenience. The goal of the Perceptron is to find a linear separator such that $\langle w, x \rangle&gt;0$ for when $y=1$ and $\langle w, x \rangle&lt;0$ when $y=-1$. We can write this compactly as saying that we want to find a $w$ for which $y \langle w, x \rangle &gt;0$ for as many individuals in the population as possible.</p>

<p>Rosenblatt’s Perceptron provides a simple algorithm for finding such a $w$. The Perceptron inputs an example, checks if it makes the correct classification. If yes, it does nothing and proceeds to the next example. If no, the decision boundary is nudged in the direction of classifying the example correctly next time.</p>

<p><strong>Perceptron</strong></p>

<ul>
  <li>Start from the initial solution $w_0=0$</li>
  <li>At each step $t=0,1,2,…$:
    <ul>
      <li>Select an individual from the population and look up their attributes: (x_t,y_t).</li>
      <li>Case 1: If $y_t\langle w_t, x_t\rangle \leq 0$, put
\(w_{t+1} = w_t + y_t x_t\)</li>
      <li>Case 2: Otherwise put $w_{t+1} = w_t$.</li>
    </ul>
  </li>
</ul>

<p>If the examples were selected at random, machine learners would recognize this algorithm as an instance of stochastic gradient descent, still the most ubiquitous way to train classifiers whether they be deep or shallow. Stochastic gradient descent minimizes sums of functions</p>

\[f(w) = \frac{1}{N} \sum_{i=1}^N \mathit{loss}( f(x_i; w) , y_i)\]

<p>with the update</p>

\[w_{t+1} = w_t - \alpha_t \nabla_w \mathit{loss}( f(x_t; w_t) , y_t)\,.\]

<p>When the examples are sampled randomly, the Perceptron is stochastic gradient descent with $\alpha_t=1$, $f(x;w) = \langle w,x \rangle$, and loss function $\mathit{loss}(\hat{y},y) = \max(-\hat{y} y, 0)$.</p>

<p>Stochastic gradient methods were invented a few years before the Perceptron. And the relations between these methods were noted by the mid-60s. Vapnik discusses some of this history in Chapter 1.11 of <a href="https://link.springer.com/book/10.1007/978-1-4757-3264-1"><em>The Nature of Statistical Learning Theory</em></a>.</p>

<p>While we might be tempted to use a standard stochastic gradient analysis to understand the optimization properties of the Perceptron, it turns out that a more rarified proof technique applies that uses no randomization whatsoever. Moreover, the argument will not only bound errors in optimization but also in generalization. Optimization is concerned with errors on a training data set. Generalization is concerned with errors on data we haven’t seen. The analysis from the 1960s links these two by first understanding the dynamics of the algorithm.</p>

<p><a href="https://cs.uwaterloo.ca/~y328yu/classics/novikoff.pdf">A celebrated result by Al Novikoff in 1962</a> showed that under reasonable conditions the algorithm makes a bounded number of updates no matter how large the sample size. Novikoff’s result is typically referred to as a <em>mistake bound</em> as it bounds the number of total misclassifications made when running the Perceptron on some data set. The key assumption in Novikoff’s argument is that the positive and negative examples are cleanly separated by a linear function. People often dismiss the Perceptron because of this <em>separability</em> assumption. But for any finite data set, can always add features and end up with a linearly separable problem. And if we add enough features, we’ll usually be separable no matter how many points we have.</p>

<p>This has been the trend in modern machine learning: don’t fear big models and don’t fear getting zero errors on your training set. This is no different than what was being proposed in the Perceptron. In fact, <a href="https://cs.uwaterloo.ca/~y328yu/classics/kernel.pdf">Aizerman, Braverman, and Roeznoer</a> recognized the power of such overparameterization, and extended Novikoff’s argument to “potential functions” that we now recognize as functions belonging to an infinite dimensional Reproducing Kernel Hilbert Space.</p>

<p>To state Novikoff’s result, we make the following assumptions: First, we assume as input a set of examples $S$. We assume every data point has norm at most $R(S)$ and that there exists a hyperplane that correctly classifies all of the data points and is of distance at least $\gamma(S)$ from every data point. This second assumption is called a <em>margin condition</em> that quantifies how separated the given data is. With these assumptions, Novikoff proved the Perceptron algorithm makes at most</p>

\[{\small
\frac{R(S)^2}{\gamma(S)^{2}}
}\]

<p>mistakes when run on $S$. No matter what the ordering of the data points in $S$, the algorithm makes a bounded number of errors.</p>

<p>The algorithmic analysis of Novikoff has many implications. First, if the data is separable, we can conclude that the Perceptron will terminate if it is run over the data set several times. This is because we can think of $k$ epochs of the Perceptron as running on the union of $k$ distinct copies of $S$, and the Perceptron eventually stops updating when run on this enlarged data set. Hence, the mistake bound tells us something particular about optimization: the Perceptron converges to a solution with zero training errors and hence a global minimizer of the empirical risk.</p>

<p>Second, we can think of the Perceptron algorithm as an <em>online learning algorithm</em>. We need not assume anything distributional about the sequence $S$. We can instead think about how long it takes for the Perceptron to converge to a solution that would have been as good as the optimal classifier. We can quantify this convergence by measuring the <em>regret</em>, equal to</p>

\[\mathcal{R}_T = \sum_{t=1}^T \mathrm{error}(w_t, (x_t,y_t)) - \sum_{t=1}^T \mathrm{error}(w_\star, (x_t,y_t))\,,\]

<p>where $w_\star$ denotes the optimal hyperplane. That is, the regret counts how frequently the classifier at step $t$ misclassifies the next example in the sequence. Novikoff’s argument shows that, if a sequence is perfectly classifiable, then the accrued regret is a constant that does not scale with T.</p>

<p>A third, less well known application of Novikoff’s bound is as a building block for a  <em>generalization bound</em>. A generalization bound estimates the probability of making an error on a new example given that the new example is sampled from the same population as the data thus far sceen. To state the generalization bound for the Perceptron, I <em>now</em> need to return to statistics. Generalization theory concerns statistical validity, and hence we need to define some notion of sampling from the population. I will use the same sampling model I have been using in this blog series. Rather than assuming a statistical model of the population, I will assume we have some population of data from which we can uniformly sample. Our training data will consist of $n$ points sampled uniformly from this population: $S={(x_1,y_1)\ldots, (x_n,y_n) }$.</p>

<p>We know that the Perceptron will find a good linear predictor for the training data if it exists. What we now show is that this predictor also works on new data sampled uniformly from the same population.</p>

<p>To analyze what happens on new data, I will employ an elegant argument I learned from Sasha Rakhlin. This argument appears in a book on Learning Theory by Vapnik and Chervonenkis from 1974, which, to my knowledge, is only available in Russian. Sasha also believes this argument is considerably older as <a href="http://www.mit.edu/~rakhlin/papers/chervonenkis_chapter.pdf">Aizermann and company were making similar “online to batch” constructions in the 1960s</a>. The proof here leverages the assumption that the data are sampled in such a way that they are identically distributed, so we can swap the roles of training and test examples in the analysis. It foreshadows later studies of stability and generalization that would be revisited decades later.</p>

<p><strong>Theorem</strong> <em>Let $w(S)$ be the output of the Perceptron on a dataset $S$ after running until the hyperplane makes no more mistakes on $S$. Let $S_n$ denote a training set of $n$ samples uniformly at random from some population. And let $(x,y)$ be an additional independent uniform sample from the same population. Then, the probability of making a mistake on $(x,y)$ is bounded as</em></p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \leq \frac{1}{n+1} {\mathbb{E}}_{S_{n+1}}\left[ \frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2} \right]\,.\]

<p>To prove the theorem, define the “leave-one-out set” to be the set where we drop $(x_k,y_k)$:</p>

\[{\scriptsize
S^{-k}=\{(x_1,y_1),\dots,(x_{k-1},y_{k-1}),(x_{k+1},y_{k+1}),...,(x_{n+1},y_{n+1})\}\,.
}\]

<p>With this notation, since all of the data are sampled identically and independently, we can rewrite the probability of a mistake on the final data point as the expectation of the leave-one-out error</p>

\[{\small
\Pr[y \langle w(S_n), x \rangle   \leq 0]
= \frac1{n+1}\sum_{k=1}^{n+1} \mathbb{E}[\mathbb{1}\{y_k \langle w(S^{-k}), x_k \rangle \leq 0\}]\,.
}\]

<p>Novikoff’s mistake bound asserts the Perceptron makes at most</p>

\[{\small
m=\frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2}
}\]

<p>mistakes when run on the entire sequence $S_{n+1}$. Let $I={i_1,\dots,i_m}$ denote the indices on which the algorithm makes a mistake in any of its cycles over the data. If $k$ is not in $I$, the output of the algorithm remains the same after we remove the $k$-th sample from the sequence. It follows that such $k \in S_{n+1}\setminus I$ satisfy  $y_k w(S^{-k})x_k \geq 0$ and therefore do not contribute to the right hand side of the summation. The other terms can at most contribute $1$ to the summation.
Hence,</p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \le \frac{\mathbb{E}[m]}{n+1}\,,\]

<p>which is what we wanted to prove.</p>

<p>What’s most stunning to me about this argument is that there are no numerical constants or logarithms. The generalization error is perfectly quantified by a simple formula of $R$, $\gamma$, and $n$. There are a variety of other arguments that get the $\tilde{O}(R/(n\gamma))$ scaling with far more complex arguments and large constants and logarithmic terms. For example, one can show that the set of hyperplanes in Euclidean space with norm bounded by $\gamma^{-1}$ has <a href="https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034">VC dimension $R/\gamma$</a>. Similarly, a <a href="https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">Rademacher complexity argument will achieve a similar scaling</a>. These arguments apply to far more algorithms than the Perceptron, but it’s frustrating how this simple algorithm from 1956 gets such a tight bound with such a short argument whereas analyzing more “powerful” algorithms often takes pages of derivations.</p>

<p>It’s remarkable that these bounds on optimization, regret, and generalization worked out in the 1960s all turned out to be optimal for classification theory. This strikes me as particularly odd because when I was in graduate school I was taught that the Perceptron was a failed enterprise. But as fads in AI have come and gone, the role of the Perceptron has remained central for 65 years. We’ve made more progress in machine learning theory since then, but it’s not always at the front of our minds just how long ago we had established our modern learning theory framework.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/11/04/perceptron/"><span class="datestr">at November 04, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8862914568867887724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html">A Complexity View of Machine Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Complexity is at its best when it models new technologies so we can study it in a principled way. Quantum computing comes to mind as a good relatively recent example. With machine learning playing an every growing role in computing, how can complexity play a role?</p><p>The theory community questions about machine learning typically look at finding mathematical reasons to explain why the models well with little overfitting or trying to get good definitions of privacy, fairness, explainability to mitigate the social challenges of ML. But what about from a computational complexity point of view? I don't have a great answer yet but here are some thoughts.</p><p>In much of structural complexity, we use relativization to understand the relative power of complexity classes. We define an oracle as a set A where a machine can ask questions about membership to A and magically get an answer. Relativization can be used to help us define classes like Σ<sub>2</sub><sup>P</sup> = NP<sup>NP</sup> or allow us to succinctly state <a href="https://doi.org/10.1137/0220053">Toda's theorem</a> as PH in P<sup>#P</sup>.</p><p>As I <a href="https://twitter.com/fortnow/status/1453827400383488002">tweeted</a> last week, machine learning feels like an oracle, after all machine learning models and algorithms are typically accessed through APIs and Python modules. What kind of oracle? Definitely not an NP-complete problem like SAT since machine learning fails miserably if you try to use it to break cryptography. </p><p>The real information in machine learning comes from the data. For a length parameter n, consider a string x which might be exponential in n. Think of x as a list of labeled or unlabeled examples of some larger set S. Machine learning creates a model M from x that tries to predict whether x is in S. Think of M as the oracle, as some compressed version of S.</p><p>Is there a computational view of M? We can appeal to Ockham's razor and consider the simplest model consistent with the data for which x as a set are random in the S that M generates. One can formalize this Minimum Description Length approach using <a href="https://doi.org/10.1109/18.825807">Kolmogorov Complexity</a>. This model is too ideal, for one it can also break cryptography, and typical deep learning models are not simple at all with sometimes millions of parameters.</p><p>This is just a start. One could try time bounds on the Kolmogorov definitions or try something different completely. Adversarial and foundational learning models might yield different kinds of oracles. </p><p>If we can figure out even a rough complexity way to understand learning, we can start to get a hold of learning's computational power and limitations, which is the purpose of studying complexity complexity in the first place. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html"><span class="datestr">at November 03, 2021 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/148">TR21-148 |  Explicit Exponential Lower Bounds for Exact Hyperplane Covers | 

	Benjamin Diamond, 

	Amir Yehudayoff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We describe an explicit and simple subset of the discrete hypercube which cannot be exactly covered by fewer than exponentially many hyperplanes. The proof exploits a connection to communication complexity, and relies heavily on Razborov's lower bound for disjointness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/148"><span class="datestr">at November 03, 2021 11:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=581">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/">TCS+ talk: Wednesday, November 10 — Kuikui Liu, University of Washington</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 10th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="https://homes.cs.washington.edu/~liukui17/"><strong>Kuikui Liu</strong></a> from the University of Washington will speak about “<em>Spectral Independence: A New Tool to Analyze Markov Chains</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Markov chain Monte Carlo is a widely used class of algorithms for sampling from high-dimensional probability distributions, both in theory and in practice. While simple to implement, analyzing the rate of convergence to stationarity, i.e. the “mixing time”, remains a challenging problem in many settings. We introduce a new technique to bound mixing times called “spectral independence”, which says that certain pairwise correlation matrices all have bounded spectral norm. This surprisingly powerful technique originates in the emerging study of high-dimensional expanders, and has allowed us to “unify” nearly all existing approaches to approximate counting and sampling by building new connections with other areas, including statistical physics, geometry of polynomials, functional analysis, and more. Through these connections, several long-standing open problems have recently been answered, including counting bases of matroids and optimal mixing of the Glauber dynamics/Gibbs sampler up to the algorithmic phase transition threshold.</p>
<p>Based on several joint works with Dorna Abdolazimi, Nima Anari, Zongchen Chen, Shayan Oveis Gharan, Eric Vigoda, Cynthia Vinzant, and June Vuong.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/"><span class="datestr">at November 03, 2021 09:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5760">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/">CS Tenure Track Positions at CUNY</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Interested in a computer science position in Manhattan? Apply to our positions here! We are starting to form a computer science program at CUNY’s Baruch College. Joining us at the beginning of this process will give you a chance to influence how computer science will look like at our college: the research and teaching directions that […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/"><span class="datestr">at November 03, 2021 01:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">Postdoc at UC Santa Barbara (apply by January 10, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position is available to work with Eric Vigoda (and other theory faculty) at UC Santa Barbara. The position is for 2 years (no teaching required). Start date is flexible.<br />
Interested candidates should send their CV to Eric Vigoda.</p>
<p>Website: <a href="https://sites.cs.ucsb.edu/~vigoda/">https://sites.cs.ucsb.edu/~vigoda/</a><br />
Email: ericvigoda@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/"><span class="datestr">at November 02, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
