<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at December 24, 2020 08:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsmath.wordpress.com/?p=2300">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/jrl.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsmath.wordpress.com/2020/12/23/itcs-2021-registration/">ITCS 2021 Registration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Head to <a href="http://itcs-conf.org/">http://itcs-conf.org/</a> to register for ITCS 2021, being held <strong>virtually</strong> at the Simons Institute from January 6-8, 2021.  One can find the list of accepted papers at the same site, and the schedule of talks will be posted shortly.  Live talks will occur from 8:30AM-3:30PM PST, and longer versions of the talks will be available to stream before the conference.  Note that <em>registration is free!</em></p>



<p>If you would like to have the ability to be seen/heard during the talk sessions, you will need to check the corresponding box on the registration form.  Join us for the first virtual TCS conference of 2021!</p>



<p>James R. Lee, ITCS 2021 PC Chair</p></div>







<p class="date">
by James <a href="https://tcsmath.wordpress.com/2020/12/23/itcs-2021-registration/"><span class="datestr">at December 23, 2020 10:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12216">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12216">Quantitative Correlation Inequalities via Semigroup Interpolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a>, Shivam Nadimpalli, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Servedio:Rocco_A=.html">Rocco A. Servedio</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12216">PDF</a><br /><b>Abstract: </b>Most correlation inequalities for high-dimensional functions in the
literature, such as the Fortuin-Kasteleyn-Ginibre (FKG) inequality and the
celebrated Gaussian Correlation Inequality of Royen, are qualitative statements
which establish that any two functions of a certain type have non-negative
correlation. In this work we give a general approach that can be used to
bootstrap many qualitative correlation inequalities for functions over product
spaces into quantitative statements. The approach combines a new extremal
result about power series, proved using complex analysis, with harmonic
analysis of functions over product spaces. We instantiate this general approach
in several different concrete settings to obtain a range of new and
near-optimal quantitative correlation inequalities, including:
</p>
<p>$\bullet$ A quantitative version of Royen's celebrated Gaussian Correlation
Inequality. Royen (2014) confirmed a conjecture, open for 40 years, stating
that any two symmetric, convex sets must be non-negatively correlated under any
centered Gaussian distribution. We give a lower bound on the correlation in
terms of the vector of degree-2 Hermite coefficients of the two convex sets,
analogous to the correlation bound for monotone Boolean functions over
$\{0,1\}^n$ obtained by Talagrand (1996).
</p>
<p>$\bullet$ A quantitative version of the well-known FKG inequality for
monotone functions over any finite product probability space, generalizing the
quantitative correlation bound for monotone Boolean functions over $\{0,1\}^n$
obtained by Talagrand (1996). The only prior generalization of which we are
aware is due to Keller (2008, 2009, 2012), which extended Talagrand's result to
product distributions over $\{0,1\}^n$. We also give two different quantitative
versions of the FKG inequality for monotone functions over the continuous
domain $[0,1]^n$, answering a question of Keller (2009).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12216"><span class="datestr">at December 23, 2020 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12153">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12153">An Improved Algorithm for Coarse-Graining Cellular Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Yerim Song, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grochow:Joshua_A=.html">Joshua A. Grochow</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12153">PDF</a><br /><b>Abstract: </b>In studying the predictability of emergent phenomena in complex systems,
Israeli &amp; Goldenfeld (Phys. Rev. Lett., 2004; Phys. Rev. E, 2006) showed how to
coarse-grain (elementary) cellular automata (CA). Their algorithm for finding
coarse-grainings of supercell size $N$ took doubly-exponential $2^{2^N}$-time,
and thus only allowed them to explore supercell sizes $N \leq 4$. Here we
introduce a new, more efficient algorithm for finding coarse-grainings between
any two given CA that allows us to systematically explore all elementary CA
with supercell sizes up to $N=7$, and to explore individual examples of even
larger supercell size. Our algorithm is based on a backtracking search, similar
to the DPLL algorithm with unit propagation for the NP-complete problem of
Boolean Satisfiability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12153"><span class="datestr">at December 23, 2020 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12138">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12138">Projection-Free Bandit Optimization with Privacy Guarantees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ene:Alina.html">Alina Ene</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Huy_L=.html">Huy L. Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vladu:Adrian.html">Adrian Vladu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12138">PDF</a><br /><b>Abstract: </b>We design differentially private algorithms for the bandit convex
optimization problem in the projection-free setting. This setting is important
whenever the decision set has a complex geometry, and access to it is done
efficiently only through a linear optimization oracle, hence Euclidean
projections are unavailable (e.g. matroid polytope, submodular base polytope).
This is the first differentially-private algorithm for projection-free bandit
optimization, and in fact our bound of $\widetilde{O}(T^{3/4})$ matches the
best known non-private projection-free algorithm (Garber-Kretzu, AISTATS `20)
and the best known private algorithm, even for the weaker setting when
projections are available (Smith-Thakurta, NeurIPS `13).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12138"><span class="datestr">at December 23, 2020 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12070">Towards a short proof of the Fulek--Kyn\v{c}l criterion for modulo 2 embeddability of graphs to surfaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Arthur Bikeev <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12070">PDF</a><br /><b>Abstract: </b>A connected graph K has a modulo 2 embedding to the sphere with g handles if
and only if there is a general position PL map f of K in the plane and a
symmetric square matrix A of size |E(K)| with modulo 2 entries and zeros on the
diagonal such that $rk A \leq 2g$ and $A_{\sigma, \tau} = |f\sigma \cap f\tau|
mod 2$ for any non-adjacent edges $\sigma, \tau$. This is essentially proved by
R. Fulek and J. Kyn\v{c}l. The main of results of this note is an alternative
proof of the (=&gt;) part.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12070"><span class="datestr">at December 23, 2020 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12059">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12059">Constructing minimally 3-connected graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Costalonga:Jo=atilde=o_Paulo.html">João Paulo Costalonga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kingan:Robert_J=.html">Robert J. Kingan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kingan:Sandra_R=.html">Sandra R. Kingan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12059">PDF</a><br /><b>Abstract: </b>A $3$-connected graph is minimally 3-connected if removal of any edge
destroys 3-connectivity. We present an algorithm for constructing minimally
3-connected graphs based on the results in (Dawes, JCTB 40, 159-168, 1986)
using two operations: adding an edge between non-adjacent vertices and
splitting a vertex. In order to test sets of vertices and edges for
3-compatibility, which depends on the cycles of the graph, we develop a method
for obtaining the cycles of $G'$ from the cycles of $G$, where $G'$ is obtained
from $G$ by one of the two operations above. We eliminate isomorphs using
certificates generated by McKay's isomorphism checker nauty. The algorithm
consecutively constructs the non-isomorphic minimally 3-connected graphs with
$n$ vertices and $m$ edges from the non-isomorphic minimally 3-connected graphs
with $n-1$ vertices and $m-2$ edges, $n-1$ vertices and $m-3$ edges, and $n-2$
vertices and $m-3$ edges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12059"><span class="datestr">at December 23, 2020 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11965">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11965">Tractable Orders for Direct Access to Ranked Answersof Conjunctive Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carmeli:Nofar.html">Nofar Carmeli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tziavelis:Nikolaos.html">Nikolaos Tziavelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gatterbauer:Wolfgang.html">Wolfgang Gatterbauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kimelfeld:Benny.html">Benny Kimelfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riedewald:Mirek.html">Mirek Riedewald</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11965">PDF</a><br /><b>Abstract: </b>We study the question of when we can answer a Conjunctive Query (CQ) with an
ordering over the answers by constructing a structure for direct (random)
access to the sorted list of answers, without actually materializing this list,
so that the construction time is linear (or quasilinear) in the size of the
database. In the absence of answer ordering, such a construction has been
devised for the task enumerating query answers of free-connex acyclic CQs, so
that the access time is logarithmic. Moreover, it follows from past results
that within the class of CQs without self-joins, being free-connex acyclic is
necessary for the existence of such a construction (under conventional
assumptions in fine-grained complexity).
</p>
<p>In this work, we embark on the challenge of identifying the answer orderings
that allow for ranked direct access with the above complexity guarantees. We
begin with the class of lexicographic orderings and give a decidable
characterization of the class of feasible such orderings for every CQ without
self-joins. We then continue to the more general case of orderings by the sum
of attribute scores. As it turns out, in this case ranked direct access is
feasible only in trivial cases. Hence, to better understand the computational
challenge at hand, we consider the more modest task of providing access to only
one single answer (i.e., finding the answer at a given position). We indeed
achieve a quasilinear-time algorithm for a subset of the class of full CQs
without self-joins, by adopting a solution of Frederickson and Johnson to the
classic problem of selection over sorted matrices. We further prove that none
of the other queries in this class admit such an algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11965"><span class="datestr">at December 23, 2020 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11913">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11913">On rich points and incidences with restricted sets of lines in 3-space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharir:Micha.html">Micha Sharir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Noam.html">Noam Solomon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11913">PDF</a><br /><b>Abstract: </b>Let $L$ be a set of $n$ lines in $R^3$ that is contained, when represented as
points in the four-dimensional Pl\"ucker space of lines in $R^3$, in an
irreducible variety $T$ of constant degree which is \emph{non-degenerate} with
respect to $L$ (see below). We show:
</p>
<p>\medskip \noindent{\bf (1)} If $T$ is two-dimensional, the number of $r$-rich
points (points incident to at least $r$ lines of $L$) is
$O(n^{4/3+\epsilon}/r^2)$, for $r \ge 3$ and for any $\epsilon&gt;0$, and, if at
most $n^{1/3}$ lines of $L$ lie on any common regulus, there are at most
$O(n^{4/3+\epsilon})$ $2$-rich points. For $r$ larger than some sufficiently
large constant, the number of $r$-rich points is also $O(n/r)$.
</p>
<p>As an application, we deduce (with an $\epsilon$-loss in the exponent) the
bound obtained by Pach and de Zeeuw (2107) on the number of distinct distances
determined by $n$ points on an irreducible algebraic curve of constant degree
in the plane that is not a line nor a circle.
</p>
<p>\medskip \noindent{\bf (2)} If $T$ is two-dimensional, the number of
incidences between $L$ and a set of $m$ points in $R^3$ is $O(m+n)$.
</p>
<p>\medskip \noindent{\bf (3)} If $T$ is three-dimensional and nonlinear, the
number of incidences between $L$ and a set of $m$ points in $R^3$ is
$O\left(m^{3/5}n^{3/5} + (m^{11/15}n^{2/5} + m^{1/3}n^{2/3})s^{1/3} + m + n
\right)$, provided that no plane contains more than $s$ of the points. When $s
= O(\min\{n^{3/5}/m^{2/5}, m^{1/2}\})$, the bound becomes
$O(m^{3/5}n^{3/5}+m+n)$.
</p>
<p>As an application, we prove that the number of incidences between $m$ points
and $n$ lines in $R^4$ contained in a quadratic hypersurface (which does not
contain a hyperplane) is $O(m^{3/5}n^{3/5} + m + n)$.
</p>
<p>The proofs use, in addition to various tools from algebraic geometry, recent
bounds on the number of incidences between points and algebraic curves in the
plane.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11913"><span class="datestr">at December 23, 2020 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11891">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11891">Fast and Accurate $k$-means++ via Rejection Sampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen=Addad:Vincent.html">Vincent Cohen-Addad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lattanzi:Silvio.html">Silvio Lattanzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Norouzi=Fard:Ashkan.html">Ashkan Norouzi-Fard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sohler:Christian.html">Christian Sohler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11891">PDF</a><br /><b>Abstract: </b>$k$-means++ \cite{arthur2007k} is a widely used clustering algorithm that is
easy to implement, has nice theoretical guarantees and strong empirical
performance. Despite its wide adoption, $k$-means++ sometimes suffers from
being slow on large data-sets so a natural question has been to obtain more
efficient algorithms with similar guarantees. In this paper, we present a near
linear time algorithm for $k$-means++ seeding. Interestingly our algorithm
obtains the same theoretical guarantees as $k$-means++ and significantly
improves earlier results on fast $k$-means++ seeding. Moreover, we show
empirically that our algorithm is significantly faster than $k$-means++ and
obtains solutions of equivalent quality.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11891"><span class="datestr">at December 23, 2020 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11772">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11772">Power-SLIC: Diagram-based superpixel generation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Maximilian Fiedler, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alpers:Andreas.html">Andreas Alpers</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11772">PDF</a><br /><b>Abstract: </b>Superpixel algorithms, which group pixels similar in color and other
low-level properties, are increasingly used for pre-processing in image
segmentation. Commonly important criteria for the computation of superpixels
are boundary adherence, speed, and regularity.
</p>
<p>Boundary adherence and regularity are typically contradictory goals. Most
recent algorithms have focused on improving boundary adherence. Motivated by
improving superpixel regularity, we propose a diagram-based superpixel
generation method called Power-SLIC.
</p>
<p>On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art
algorithms in terms of compactness and boundary precision, and its boundary
adherence is the most robust against varying levels of Gaussian noise. In terms
of speed, Power-SLIC is competitive with SLIC.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11772"><span class="datestr">at December 23, 2020 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11748">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11748">Mesh Denoising and Inpainting using the Total Variation of the Normal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Lukas Baumgärtner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergmann:Ronny.html">Ronny Bergmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Herrmann:Marc.html">Marc Herrmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Herzog:Roland.html">Roland Herzog</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt:Stephan.html">Stephan Schmidt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal=N=uacute==ntilde=ez:Jos=eacute=.html">José Vidal-Núñez</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11748">PDF</a><br /><b>Abstract: </b>In this paper we present a novel approach to solve surface mesh denoising and
inpainting problems. The purpose is not only to remove noise while preserving
important features such as sharp edges, but also to fill in missing parts of
the geometry. A discrete variant of the total variation of the unit normal
vector field serves as a regularizing functional to achieve this goal. In order
to solve the resulting problem, we present a novel variant of the split Bregman
(ADMM) iteration. Numerical examples are included demonstrating the performance
of the method with some complex 3D geometries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11748"><span class="datestr">at December 23, 2020 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11742">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11742">Efficient sequential and parallel algorithms for multistage stochastic integer programming using proximity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cslovjecsek:Jana.html">Jana Cslovjecsek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eisenbrand:Friedrich.html">Friedrich Eisenbrand</a>, Michał Pilipczuk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venzin:Moritz.html">Moritz Venzin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weismantel:Robert.html">Robert Weismantel</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11742">PDF</a><br /><b>Abstract: </b>We consider the problem of solving integer programs of the form $\min
\{\,c^\intercal x\ \colon\ Ax=b, x\geq 0\}$, where $A$ is a multistage
stochastic matrix in the following sense: the primal treedepth of $A$ is
bounded by a parameter $d$, which means that the columns of $A$ can be
organized into a rooted forest of depth at most $d$ so that columns not bound
by the ancestor/descendant relation in the forest do not have non-zero entries
in the same row. We give an algorithm that solves this problem in
fixed-parameter time $f(d,\|A\|_{\infty})\cdot n\log^{O(2^d)} n$, where $f$ is
a computable function and $n$ is the number of rows of $A$. The algorithm works
in the strong model, where the running time only measures unit arithmetic
operations on the input numbers and does not depend on their bitlength. This is
the first fpt algorithm for multistage stochastic integer programming to
achieve almost linear running time in the strong sense. For the case of
two-stage stochastic integer programs, our algorithm works in time
$2^{(2\|A\|_\infty)^{O(r(r+s))}}\cdot n\log^{O(rs)} n$. The algorithm can be
also parallelized: we give an implementation in the PRAM model that achieves
running time $f(d,\|A\|_{\infty})\cdot \log^{O(2^d)} n$ using $n$ processors.
</p>
<p>The main conceptual ingredient in our algorithms is a new proximity result
for multistage stochastic integer programs. We prove that if we consider an
integer program $P$, say with a constraint matrix $A$, then for every optimum
solution to the linear relaxation of $P$ there exists an optimum (integral)
solution to $P$ that lies, in the $\ell_{\infty}$-norm, within distance bounded
by a function of $\|A\|_{\infty}$ and the primal treedepth of $A$. On the way
to achieve this result, we prove a generalization and considerable improvement
of a structural result of Klein for multistage stochastic integer programs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11742"><span class="datestr">at December 23, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.11702">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.11702">Scheduling Coflows with Dependency Graph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shafiee:Mehrnoosh.html">Mehrnoosh Shafiee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaderi:Javad.html">Javad Ghaderi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.11702">PDF</a><br /><b>Abstract: </b>Applications in data-parallel computing typically consist of multiple stages.
In each stage, a set of intermediate parallel data flows (Coflow) is produced
and transferred between servers to enable starting of next stage. While there
has been much research on scheduling isolated coflows, the dependency between
coflows in multi-stage jobs has been largely ignored. In this paper, we
consider scheduling coflows of multi-stage jobs represented by general DAGs
(Directed Acyclic Graphs) in a shared data center network, so as to minimize
the total weighted completion time of jobs. This problem is significantly more
challenging than the traditional coflow scheduling, as scheduling even a single
multi-stage job to minimize its completion time is shown to be NP-hard.
</p>
<p>In this paper, we propose a polynomial-time algorithm with approximation
ratio of $O(\mu\log(m)/\log(\log(m)))$, where $\mu$ is the maximum number of
coflows in a job and $m$ is the number of servers. For the special case that
the jobs' underlying dependency graphs are rooted trees, we modify the
algorithm and improve its approximation ratio. To verify the performance of our
algorithms, we present simulation results using real traffic traces that show
up to $53 \%$ improvement over the prior approach. We conclude the paper by
providing a result concerning an optimality gap for scheduling coflows with
general DAGs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.11702"><span class="datestr">at December 23, 2020 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/22/postdoc-at-university-of-warwick-uk-apply-by-january-13-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/22/postdoc-at-university-of-warwick-uk-apply-by-january-13-2021/">Postdoc at University of Warwick, UK (apply by January 13, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I (Tom Gur) invite applications for a postdoc position, with generous travel support, flexible conditions, and full academic freedom. Relevant research interests include, but not limited to: complexity, sublinear-time algorithms, coding theory, PCP/IP, PAC learning, and quantum computing.</p>
<p>To apply, please send me an email introducing yourself, and include your CV and 2-3 representative papers.</p>
<p>Website: <a href="https://www.dcs.warwick.ac.uk/~tomgur/">https://www.dcs.warwick.ac.uk/~tomgur/</a><br />
Email: tom.gur@warwick.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/22/postdoc-at-university-of-warwick-uk-apply-by-january-13-2021/"><span class="datestr">at December 22, 2020 04:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5209">The case for moving to a red state</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (Dec. 23):</span></strong> This post quickly attracted many of the most … colorful comments in this blog’s 15-year history.  My moderation queue is overflowing right now with “gas the kikes,” “[f-word] [n-words],” “race war now,” “kikes deserve to burn in hell,” “a world without [n-words],” “the day of the rope approaches,” and countless similar contributions.  One commenter focused on how hilarious he found my romantic difficulties earlier in life.</p>



<p>The puzzle, for me, is that I’d spent <em>years</em> denouncing Trump’s gleeful destruction of the country that I grew up believing in, using the strongest language I could muster.  So why am I only <em>now</em> getting all the hate-spam?</p>



<p>Then a possible explanation hit me: namely, the sort of person who’d leave such comments is <em>utterly impervious to moral condemnation</em>.  The only thing such a person cares about—indeed, as it turns out, feels a volcanic need to shout down—is someone articulating an actual plausible path to removing his resentment-fueled minority from power.  If this is right, then I’m proud to have hit a nerve. –SA</p>



<p></p><hr />



<ol><li>The US is now a failed democracy, with a president who’s considering <a href="https://www.cnn.com/2020/12/19/politics/trump-oval-office-meeting-special-counsel-martial-law/index.html">declaring martial law</a> to avoid conceding a lost election, and with the majority of his party eager to follow him arbitrarily far into the abyss.  Even assuming, as I do, that the immediate <em>putsch</em> will fail, the Republic will not magically return to normal.<br /></li><li>The survival of Enlightenment values on Earth now depends, in large part, on the total electoral humiliation and defeat of the forces that enabled Trump—something that the last election failed to deliver.<br /></li><li>Alas, ever since it absorbed the Southern racists in the 1960s, the Republican Party has maintained a grip on power wholly out of proportion to its numbers through anti-democratic means.  The most durable of these means are built into the Constitution itself: the Electoral College, the overrepresentation of sparsely-populated rural states in the Senate, and the gerrymandering of Congressional districts.  Every effort to fix these anachronisms, whether by legislation or Constitutional amendment, has been blocked for generations.  It’s fantasy to imagine the beneficiaries of these unjust advantages ever voluntarily giving them up.<br /></li><li>Accordingly, the survival of the nation might come down to whether enough Americans, in deep-blue areas like California and New York and Massachusetts, are willing to <em>pick up and move</em> to where their votes actually count.<br /></li><li>The pandemic has awoken tens of millions of people to the actual practical feasibility of working from home or in a different time zone from their employer.  The culture has finally caught up to the abridgment of distance that the Internet, smartphones, and videoconferencing achieved well over a decade ago.<br /></li><li>Still, one doesn’t expect Brooklynites to settle by the thousands on remote mountaintops.  And even if they did, there are <em>many</em> remote mountaintops, so the transplants’ power could be diluted to near nothing.  Better for the transplants to concentrate themselves in a few <a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)">Schelling points</a>: ideally, cities where they could both swing the national electoral calculus <em>and</em> actually want to live.<br /></li><li>There’s been a <a href="https://austonia.com/city/bay-area-austin-move">spate</a> of <a href="https://www.ktvu.com/news/silicon-valley-exodus-bay-area-tech-companies-leaving-for-texas?fbclid=IwAR0b3y6D8m0c1crdNDptyZaDNPeW6f01au_drfSmL7yEDgO7H-AlGUdJ1Uc">recent</a> <a href="https://www.techrepublic.com/article/silicon-valley-exodus-the-majority-of-professionals-said-theyd-follow-tech-leaders-to-emerging-tech-hubs/">articles</a> about the possible exodus of tech companies and professionals from the Bay Area, because of whatever combination of sky-high rents, NIMBYism, taxes, mismanagement, wildfires, blackouts, and the pandemic having removed the once-overwhelming reasons to be in the Bay.  Oft-mentioned alternatives include Miami, Denver, and of course my own adopted hometown of Austin, TX, where <a href="https://www.builtinaustin.com/2020/12/09/elon-musk-texas-move-tesla-austin-hiring">Elon Musk</a> and <a href="https://www.cnbc.com/2020/12/11/oracle-is-moving-its-headquarters-from-silicon-valley-to-austin-texas.html">Oracle</a> just announced they’re moving.<br /></li><li>If you were trying to optimize your environment for urban Blue-Tribeyness—indie music, craft beer, ironic tattoos, Bernie Sanders yard signs, etc. etc.—<em>but subject to living in an important red or purple state, where your vote could plausibly contribute to a historic political realignment of the US</em>—then you couldn’t do much better than Austin.  Where else is in the running?  Atlanta, Houston, San Antonio, Pittsburgh?<br /></li><li>It’s true that Texas is the state of <a href="https://www.expressnews.com/opinion/columnists/josh_brodesky/article/Brodesky-Stain-of-Paxton-s-lawsuit-will-be-15814916.php">Ken Paxton</a>, the corrupt and unhinged Attorney General who unsuccessfully petitioned the US Supreme Court to overturn Trump’s election loss.  But it’s also the state of MD Anderson, often considered the best oncology center on earth, and of Steven Weinberg, possibly the greatest living physicist.  It’s where the spike proteins of both the Pfizer and Moderna covid vaccines were developed.  It’s where <a href="https://en.wikipedia.org/wiki/Young_Sheldon">Sheldon Cooper grew up</a>—alright, he’s fictional, but I’ve worked with undergrads at UT Austin who almost <em>could’ve</em> been Sheldon.  Like the US as a whole, the state has potential.<br /></li><li>Accelerating the mass migration of blue Americans to cities like Austin isn’t <em>only</em> good for the country and the world.  The New Yorkers and San Franciscans left behind will thank the migrants for lower rents!<br /></li><li>But won’t climate change make Texas a living hell?  Alas, as recent wildfires and hurricanes remind us, there aren’t many places on earth that climate change <em>won’t</em> soon make various shades of hell.  At least Austin, like many red locales, is far inland.  For the summers, there are lots of swimming pools and lakes.<br /></li><li>If Austin gets overrun by Silicon Valley refugees, won’t they recreate whatever dysfunctional conditions caused them to flee Silicon Valley in the first place?  Maybe, eventually, but it would take quite a while.  One problem at a time!  And the “problems of Silicon Valley” are problems most places should desperately want.<br /></li><li>Is Texas winnable—or is a blue Texas like controlled nuclear fusion, forever a decade or two in the future?  Well, Trump’s 6-point margin in Texas this November, 3 points less than his margin in 2016, amounted to 630,000 votes out of 11.3 million cast.  Meanwhile, net migration to Texas over the past decade included 356,000 to Austin (growing its population by 20%), 687,000 to Dallas, 603,000 to Houston, 260,000 to San Antonio.  Let’s say we want two million more transplants.  The question is not whether they’re going to arrive but at what rate.<br /></li><li>Can the cities of Texas accommodate two million more people?  Well, traffic will get worse, rents will get higher … but the answer is an unequivocal yes.  Land, Texas has.<br /></li><li>Do the tech workers who I’d like to relocate even vote blue?  Given the unremitting scorn that the woke press now heaps on “racist, sexist, greedy Silicon Valley techbros,” it can be easy to forget this, but the answer to the question is: <em>yes, overwhelmingly, they do</em>.  Mountain View, CA, for example, <a href="https://mv-voice.com/news/2020/11/09/election-recap-mountain-view-swings-left-but-rejects-statewide-changes-to-rent-control?fbclid=IwAR17en2fsShjdcJREPbkSszEwU4Yuf1j2yU041-44Jn6ga0-_9B5vE4ylWM">went</a> 83% Biden and only 15% Trump in November.<br /></li><li>Even if everything I’ve said is obvious, in order for the Great Red-State Tech-Worker Migration happen at the rate I want, it needs to become <a href="https://www.scottaaronson.com/blog/?p=2410">common knowledge</a> that it’s happening—not merely known but known to be known, and so forth.  Closely related, it needs to become a serious <strong>status symbol</strong> for any blue-triber to relocate to a contested state.  (“You’re moving to Georgia to help save the Republic?  <em>And</em> you’ll be able to afford a four-bedroom house?  I’m <em>so</em> jealous!”)<br /></li><li>This has been the real purpose of this post: to make it clear that, if you help settle the wild frontier like my family did, then a tiny bit of the unattainable coolness of a stuttering quantum complexity theory blogger/professor could rub off on <em>you</em>.<br /></li><li>Think about it this way.  Many of our grandparents gave their lives to save the world from fascism.  Would you have done the same in their place?  OK now, what if you didn’t have to lose your life: you only had to live in Austin or Miami?<br /></li><li>If this post plays a role in any like-minded reader’s decision to move to Austin, then once covid is over, they should tell me to redeem a personal welcome celebration from me and Dana.  We’ll throw some extra brisket on the barbie.</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5209"><span class="datestr">at December 22, 2020 08:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/unadversarial/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/unadversarial/">Unadversarial Examples: Designing Objects for Robust Vision</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2012.12235" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://git.io/unadversarial" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>


<div class="footnote">
    A video demo of an <em>unadversarial patch</em> for garbage trucks.
    Placing the patch on misclassified garbage trucks significantly
    improves performance and robustness.
</div>

<p>Modern computer vision systems are sensitive to transformations and corruptions
of their inputs, making reliability and safety real concerns at deployment time.
For example, fog or snow can severely impact self-driving cars’ perception
systems; in an <a href="https://gradientscience.org/background">earlier post</a>, we found that even changes in image
background are often enough to cause model misbehavior. Techniques like data
augmentation, domain randomization, and robust training can all improve
robustness, but tend to perform poorly in the face of the unfamiliar
distribution shifts that computer vision systems encounter in the wild.</p>

<h2 id="how-can-we-make-classifiers-more-robust">How can we make classifiers more robust?</h2>

<p>So far, most approaches to robust machine learning have focussed on better model
training methods. However, there is more than one way to peel a banana. Instead
of tackling the general problem of designing universally robust vision
algorithms, we leverage an additional degree of freedom present in many
real-world scenarios. Specifically, we observe that in many situations, the
system designer not only trains the model used to make predictions, but also has
the ability to modify objects of interest. Critically, such a system designer could
modify objects to improve the model’s performance! For example, a drone
operator training a landing pad detection model can modify the landing pad
surface for easier detection, and a roboticist manipulating custom objects could
alter objects’ texture or geometry to assist with perception. Indeed, a similar
insight motivates QR codes, which are patterns explicitly designed to encode
easily recoverable bits in photographs. In our work, we ask: can we reliably
create objects that <em>augment</em>Note that QR codes don't
really fit the bill here, since they require their own decoder. (For
completeness, we still tested QR codes in our paper and
found them not robust to distribution shift.) the
performance of existing machine learning models on vision tasks like
classification or regression?</p>

<h2 id="designing-objects-for-robust-vision">Designing objects for robust vision</h2>
<p>Research on <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a> has demonstrated that modern vision
models can be extraordinarily sensitive to small changes in input: carefully
chosen yet imperceptible perturbations of natural images can cause models to
make wildly inaccurate predictions. In particular, previous work has shown that
we can perturb objects’ textures so that they induce misclassification when
photographed <a href="https://arxiv.org/abs/1607.02533">[Kurakin et al.]</a>. Indeed, 
<a href="https://arxiv.org/abs/1707.07397">Athalye et al.</a> 3D print a textured 
turtle that looks like a turtle, but classifies as a rifle when shown to a
classifier!</p>

<p>So, given that we can make objects <em>fool</em> ML models, can we also make them <em>help</em>?</p>

<p>The answer is yes! In our work, we turn the same oversensitivity behind
adversarial examples into a tool for robustly solving vision tasks.
Specifically, instead of optimizing inputs to mislead models (as in adversarial
attacks), we optimize inputs to reinforce correct behavior, yielding what we
refer to as <em>unadversarial examples</em> or <em>robust objects</em>.</p>

<p>We demonstrate that even a simple gradient-based algorithm can successfully
construct unadversarial examples for a variety of vision settings. In fact, by
optimizing objects for vision systems (rather than only vice-versa) we can
greatly improve performance on both in-distribution data and previously unseen
out-of-distribution data. For example, as illustrated below, optimizing the
texture of a 3D-modelled jet enables the image classifier to classify that jet
more robustly under various weather conditions, despite never being exposed to
such conditions in training:</p>

<p><img src="https://gradientscience.org/assets/unadversarial/headline_fig.png" alt="Example of unadversarial example (jet)" /></p>
<div class="footnote">
    In our work, we demonstrate that optimizing objects (e.g., the
    pictured jet) for pre-trained neural networks can significantly boost
    performance and robustness on computer vision tasks.
</div>

<p>The model correctly classifies both the original jet and its unadversarial
counterpart in standard conditions, but recognizes only the unadversarial jet in
foggy and dusty conditions.</p>

<h2 id="constructing-unadversarial-examples">Constructing unadversarial examples</h2>
<p>Our paper proposes two ways of designing robust objects: unadversarial
stickers (patches), and unadversarial textures:</p>

<p><img src="https://gradientscience.org/assets/unadversarial/patch_vs_texture.png" alt="Example of unadversarial patch and unadversarial texture" /></p>
<div class="footnote">
    Examples of the two considered methods for constructing unadversarial objects.
</div>

<p>As with adversarial examples, we construct both unadversarial patches and
textures via gradient descent. In both cases, we need a pre-trained vision model
to optimize objects for:</p>

<ul>
  <li><strong>Unadversarial patches</strong>: To construct/train unadversarial patches, we repeatedly
sample natural images from a given dataset, and place a patch onto the sampled
image (with random orientation and position) corresponding to the label of the
image. We then feed the augmented image into the pre-trained vision model, and
use gradient descent to minimize the standard classification loss of the model
with respect to the patch pixels. The procedure is almost identical to that of
<a href="https://arxiv.org/abs/1712.09665">Brown et al.</a> for constructing <em>adversarial</em>
patches; the main difference is that here we minimize, rather than maximize,
the loss of the pre-trained model.</li>
  <li><strong>Unadversarial textures</strong>: To construct adversarial textures, we start with a 3D
mesh of the object we would like to optimize, as well as a dataset of
background images. At each iteration, we render the current unadversarial
texture onto the 3D mesh, and overlay the rendering onto a random background
image. The result is fed to the pre-trained classifier, and we (again) use
gradient descent to minimize the classification loss with respect to the
texture. In previous work, <a href="https://arxiv.org/abs/1707.07397">Athalye et al.</a>
use an extremely similar approach to optimize <em>adversarial</em> textures (the most
famous example of which being the 
<a href="https://www.labsix.org/physical-objects-that-fool-neural-nets/">rifle turtle</a>).</li>
</ul>

<h2 id="evaluating-unadversarial-examples">Evaluating unadversarial examples</h2>
<p>After designing our unadversarial examples, we evaluate them in two ways. First,
we want to ensure that classifiers still maintain (or improve on) high accuracy
levels on standard benchmarks augmented with unadversarial examples. Second, we
want to understand the extent to which patches confer robustness to
<em>out-of-distribution</em> samples—how well do models perform on patches under
distribution shifts, despite such shifts never being considered in training?</p>

<p>To answer these questions, we first test our methods on standard datasets
(ImageNet and CIFAR) and robustness benchmarks (CIFAR10-C, ImageNet-C). It turns
out that adding unadversarial patches to ImageNet dataset boosts the ImageNet
accuracy as well as the robustness of a pretrained ResNet-18 model under various
corruptions!</p>

<p><img src="https://gradientscience.org/assets/unadversarial/benchmark_performance.png" alt="Performance on ImageNet and ImageNet-C" /></p>
<div class="footnote">
Accuracy on (a) clean ImageNet images and (b) synthetically corrupted
ImageNet-C images as a function of patch size (given as a percentage of image
area). In (b), each bar denotes the average accuracy over the five severities
in ImageNet-C, and the horizontal dashed lines report the accuracy on the
original (non-patched) datasets.
</div>

<p>In our <a href="https://arxiv.org/abs/2012.12235">paper</a>,
we also compare to some non-optimization based baselines,
like QR code-based augmentation and predetermined patches; unadversarial patches
comfortably outperform all of the baselines we tested.</p>

<h2 id="how-do-robust-objects-fare-in-more-realistic-settings">How do robust objects fare in more realistic settings?</h2>
<p>The promising results we observe on standard classification and synthetic
corruption benchmarks motivate us to consider more realistic tasks. To this end,
we extend our evaluation to consider unadversarial examples in three additional
settings: (a) classifying objects in a high-fidelity renderer, (b) localizing
objects in a drone-landing simulator, and (c) recognizing objects in the real
world. In the first two (simulated) settings, we optimize over objects’ textures
directly and simulate weather conditions in the renderings themselves. In the
final setting, we print out patches designed for ImageNet and study how well
they aid classification when photographed on real world objects.</p>

<h3 id="a-recognizing-objects-in-a-high-fidelity-simulator">A) Recognizing Objects in a High-Fidelity Simulator</h3>
<p>We first test how well unadversarial examples aid recognition of
three-dimensional objects in a high-fidelity simulator. Generating unadversarial
textures for objects corresponding to ImageNet classes, such as “warplane” and
“automobile,” we then use Microsoft AirSim to evaluate how well the
unadversarial textures help classification in both standard and severe weather
conditions—like fog and snow (note that in AirSim these weather conditions are
explicitly simulated in the 3D scene, and not applied through image
post-processing). We observe that our unadversarial models are much more
recognizable than their original counterparts by the pre-trained ImageNet model,
especially under distribution shift. For example, our unadversarial jet beats
the baselines “Bright Jet” and “Dark Jet,” textures manually designed with the
goal of visibility in severe weather) in a variety of weather conditions and
severities:</p>

<p><img src="https://gradientscience.org/assets/unadversarial/jetplots.png" alt="Classification of a standard and unadversarial jet in simulation" /></p>
<div class="footnote">
The jet unadversarial example task. We show example conditions under which we
evaluate the objects, along with aggregate statistics for how well an
ImageNet classifier classifies the objects in different conditions.
</div>

<h3 id="b-localization-for-simulated-drone-landing">B) Localization for (Simulated) Drone Landing</h3>
<p>We then take the realism of our simulations a step further by training
unadversarial patches for a simulated drone landing task. The drone is equipped
with a pre-trained regression model that localizes the landing pad. To improve
localization, we place an unadversarial patch on the pad and optimize it by
minimizing a squared error loss (instead of a classification loss as in the
previous experiments) corresponding to the drone’s error in predicting landing
pad location. We then evaluate the effectiveness of the patch by examining  the
landing success rate in clear, moderate, and severe (simulated) weather
conditions. As shown below, we find that the unadversarial landing pad improves
landing success rate across all conditions:</p>

<p><img src="https://gradientscience.org/assets/unadversarial/landing.png" alt="Simulated drone landing with unadversarial landing pad" /></p>
<div class="footnote">
Drone landing task. On the left we show the unadversarial versus standard landing pads. On the right we show the results for the task when both the standard and unadversarial landing pads are used.
</div>

<h3 id="c-physical-world-unadversarial-examples">C) Physical World Unadversarial Examples</h3>
<p>Finally, we study unadversarial examples’ generalization to the physical world.
Printing out unadversarial patches, we place them on top of real-world objects
and classify these object-patch pairs along a diverse set of viewpoints and
object orientations. Our results are detailed below; we find that the
unadversarial patches consistently improve performance despite printing
artifacts, lighting conditions, partial visibility, and other naturally-arising
distribution shifts.</p>

<p><img src="https://gradientscience.org/assets/unadversarial/physical_world_results.png" alt="Physical-world unadversarial examples" /></p>
<div class="footnote">
Physical-world experiments. We take pictures of objects at diverse orientations while varying the presence of a patch on the object. Note that we don't do any additional data augmentation on the patches, which are the same used in our previous ImageNet benchmark experiment.
</div>

<h2 id="conclusions">Conclusions</h2>
<p>We have shown that it is possible to design objects that improve the performance
of computer vision models, even under strong and unforeseen corruptions and
distribution shifts. By minimizing the standard loss of a pre-trained model,
without training for robustness to anything specific, we significantly boosted
both model performance and model robustness across a variety of tasks and
settings. Our results suggest that designing unadversarial inputs could be a
promising route towards increasing reliability and out-of-distribution
robustness of computer vision models.</p></div>







<p class="date">
<a href="https://gradientscience.org/unadversarial/"><span class="datestr">at December 22, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=205">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2020/12/21/soda-2021-funds-for-student-registration-fees/">SODA 2021: Funds for Student Registration Fees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Please see below for a message from Shang-Hua Teng, regarding the possibility of waivers for SODA 2021 registration for students.</p>



<hr class="wp-block-separator" />



<p>Dear TCS students:<br /><br />By now, it is hard to overestimate the impact of the COVID19 pandemic to society. However, like every challenge, it has created some opportunities. For example, essentially all major conferences in TCS this year have been transformed into virtual ones, making them more accessible to scholars/students across the world (of course at the expense of traditional interactions). <br /><br />ACM-SIAM Symposium on Discrete Algorithms (SODA21) will be held virtually this year, on Jan. 10 – 13, 2021. As you may know, this is the premier conference on algorithms .<br /><br />See <a href="https://www.siam.org/conferences/cm/conference/soda21" target="_blank" rel="noreferrer noopener">https://www.siam.org/conferences/cm/conference/soda21</a><br /><br />Thanks to our industry partners and ACM SIGACT group, SODA has some funds for covering student registrations. I am writing to informing you this opportunity and encourage you to apply:<br /> See: <br />1. <a href="https://awards.siam.org/" target="_blank" rel="noreferrer noopener">https://awards.siam.org/</a> <br />2. <a href="https://www.siam.org/conferences/cm/lodging-and-support/travel-support/soda21-travel-support" target="_blank" rel="noreferrer noopener">https://www.siam.org/conferences/cm/lodging-and-support/travel-support/soda21-travel-support</a><br />That deadline is Dec. 27, 2020. Like before, having papers in SODA is not prerequisite.<br /><br />Shang-Hua Teng<br />On Behalf of SODA Steering Committee<br /></p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2020/12/21/soda-2021-funds-for-student-registration-fees/"><span class="datestr">at December 21, 2020 07:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20677">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/12/21/to-cheer-you-up-in-difficult-times-15-yuansi-chen-achieved-a-major-breakthrough-on-bourgains-slicing-problem-and-the-kannan-lovasz-and-simonovits-conjecture/">To Cheer You Up in Difficult Times 15: Yuansi Chen Achieved a Major Breakthrough on Bourgain’s Slicing Problem and the Kannan, Lovász and Simonovits Conjecture</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="width: 310px;" class="wp-caption alignnone" id="attachment_20687"><a href="https://gilkalai.files.wordpress.com/2020/12/yuansi.jpg"><img width="300" alt="" src="https://gilkalai.files.wordpress.com/2020/12/yuansi.jpg?w=300&amp;h=225" class="size-medium wp-image-20687" height="225" /></a><p class="wp-caption-text" id="caption-attachment-20687"><span style="color: #ff0000;"><strong>Yuansi Chen</strong></span></p></div>
<h3>This post gives some background to  a recent breakthrough  paper: <a href="https://arxiv.org/abs/2011.13661">An Almost Constant Lower Bound of the Isoperimetric Coefficient in the KLS Conjecture</a> by <a href="https://people.math.ethz.ch/~chenyua/">Yuansi Chen</a>. <span style="color: #0000ff;">Congratulations Yuansi! </span></h3>
<h2>The news</h2>
<p>Yuansi Chen gave an almost constant bounds for Bourgain’s 1984 slicing problem and for the Kannan-Lovasz-Simonovits 1995 conjecture. In this post I will describe these conjectures.</p>
<p>Unrelated cheerful news: Here is <a href="https://www.quantamagazine.org/a-mathematicians-adventure-through-the-physical-world-20201216/">a very nice Quanta article</a> by Kevin Hartnett on Lauren Williams, the positive Grassmannian and connections to physics. (See <a href="https://gilkalai.wordpress.com/2015/02/16/the-simplex-the-cyclic-polytope-the-positroidron-the-amplituhedron-and-beyond/">this related 2015 post</a>.)</p>
<h2>Bourgain’s slicing problem</h2>
<p><strong>Bourgain’s slicing problem (1984):</strong>  Is there <em>c &gt; 0</em> such that for any dimension n and any centrally symmetric convex body <em>K ⊆</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathbb R^n" class="latex" title="\mathbb R^n" /> of volume one, there exists a hyperplane <em>H</em> such that the<em> (n − 1)-</em>dimensional volume of <em>K ∩ H</em> is at least <em>c</em>?</p>
<p>Vitali Milman <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiEobqGkNrtAhUJ3IUKHeerBUMQFjAAegQIAhAC&amp;url=https%3A%2F%2Fwww.weizmann.ac.il%2Fmath%2Fklartag%2Fsites%2Fmath.klartag%2Ffiles%2Fuploads%2Fbourgain_slicing_problem.pdf&amp;usg=AOvVaw3Rq7Rw9I4wbSUIyTUCAn08">wrote</a>: <span style="color: #0000ff;">“I was told once by Jean that he had spent more time on this problem and had dedicated more efforts to it than to any other problem he had ever worked on.”  </span></p>
<p>A positive answer to the problem is sometimes referred to as the hyperplane conjecture. Bourgain himself proved in the late 1980s that  the answer is yes if <img src="https://s0.wp.com/latex.php?latex=c%3D1%2Fn%5E%7B1%2F4%7D%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="c=1/n^{1/4}\log n" class="latex" title="c=1/n^{1/4}\log n" /> and twenty years later Boaz Klartag shaved away the log<em>n </em>factor. For more on the problem see <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiEobqGkNrtAhUJ3IUKHeerBUMQFjAAegQIAhAC&amp;url=https%3A%2F%2Fwww.weizmann.ac.il%2Fmath%2Fklartag%2Fsites%2Fmath.klartag%2Ffiles%2Fuploads%2Fbourgain_slicing_problem.pdf&amp;usg=AOvVaw3Rq7Rw9I4wbSUIyTUCAn08">this article by Boaz Klartag and Vitali Milman</a> and for the history see the moving foreword  by Vitali Milman.</p>
<p>Yuansi Chen’s result (combined with an earlier result of Klartag and Ronen) asserts that c can be taken as <img src="https://s0.wp.com/latex.php?latex=n%5E%7B-o%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n^{-o(1)}" class="latex" title="n^{-o(1)}" />.</p>
<h2>Computing the volume of convex sets and the 1995 Kannan-Lovasz-Simonovits conjecture</h2>
<p>Kannan, Lovász and Simonovits (KLS) conjectured  in 1995 that for any distribution that is<br />
log-concave, the Cheeger isoperimetric coefficient equals to that achieved by half-spaces up to a universal constant factor.  The crucial point here is that the constant does not depend on the dimension.</p>
<p>Let <em>K</em> be a convex body of volume one in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathbb R^n" class="latex" title="\mathbb R^n" /> and consider the uniform probability distribution on <em>K</em>. Given a subset <em>A</em> of <em>K</em> of volume <em>t≤1/2</em> we can ask what is the surface <em>S(A)</em> area of <img src="https://s0.wp.com/latex.php?latex=A+%5Ccap+int+K&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A \cap int K" class="latex" title="A \cap int K" />. (We don’t want to consider points in the surface of <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> itself.)   Let <img src="https://s0.wp.com/latex.php?latex=f_K%28t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="f_K(t)" class="latex" title="f_K(t)" /> be the minimum value of <em>S(A)</em> for subsets of volume <em>t</em>. The Cheeger constant or the expansion constant is the minimum value of <em>f(t)/t. </em>The KAS conjecture asserts that up to a universal constant independent from the dimension the Cheeger constant is realized by separating <em>K </em>with a hyperplane! (The conjecture is also called the Kannan, Lovász and Simonovits hyperplane conjecture.)</p>
<p>The motivation for the KMS conjecture came from a major 1990 breakthrough in the theory of algorithms. Dyer, Frieze, and Kannan found a polynomial-time algorithm to compute the volume of a convex set K in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathbb R^d" class="latex" title="\mathbb R^d" />.  Kannan, Lovász and Simonovich (later joined by Santosh Vampala) wrote a series of ingenious papers where they improved the exponents of the polynomial and in the process introduced new methods for proving rapid mixing of Markov chains and new Euclidean isoperimetric results and conjectures.</p>
<p>As it turned out the KLS conjecture implies Bourgain’s hyperplane conjecture and also the “thin-shell conjecture” that  I will not describe here.</p>
<p>Yuansi Chen’s proved  an almost constant lower bound <img src="https://s0.wp.com/latex.php?latex=d%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="d^{o(1)}" class="latex" title="d^{o(1)}" /> of the isoperimetric coefficient in the KLS conjecture!</p>
<p>This also gives faster mixing time bounds of Markov chain Monte Carlo (MCMC) sampling algorithms on log-concave measures.</p>
<h2><a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a>‘s  stochastic localization scheme and the strategy <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id" dir="auto">proposed by Yin Tat Lee and SantoshVempala</span></h2>
<p>Yuansi Chen’s proof relies on <a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a>‘s  stochastic localization scheme that was introduced in this 2013 paper</p>
<p class="title mathjax">Ronen Eldan, <a href="https://arxiv.org/abs/1203.0893">Thin shell implies spectral gap up to polylog via a stochastic localization scheme</a></p>
<p>It follows a strategy laid by Yin Tat Lee and Santosh Vempala 2017 FOCS paper</p>
<p>Y. T. Lee and S. S. Vempala,  <a href="https://arxiv.org/abs/1612.01507">Eldan’s stochastic localization and the KLS hyperplane conjecture: an improved lower bound for expansion.</a></p>
<p>There is not much more I can tell you now about Ronen’s theory, about <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id" dir="auto">Yin Tat and Santosh’s </span>strategy, about Yuansi’s proof, and about further intermediate results. (<a href="https://gilkalai.wordpress.com/2020/02/28/remarkable-new-stochastic-methods-in-abf-ronen-eldan-and-renan-gross-found-a-new-proof-for-kkl-and-settled-a-conjecture-by-talagrand/">This post here might be somewhat related to Ronen’s method</a>.) But let me very briefly mention a few related matters.</p>
<ol>
<li>There are interesting negative results regarding volume computations, see the 1987 <a href="https://link.springer.com/article/10.1007/BF02187886">Barany Furedi’s paper</a> and <a href="https://arxiv.org/abs/0903.2634">Ronen Eldan’s 2009 paper</a></li>
<li>It is a very interesting question (that I heard from Moshe Vardi) to understand the connection between practice and theory regarding Markov chain Monte Carlo<br />
(MCMC) sampling algorithms e.g. for approximating permanents and volumes.</li>
<li>Related topics are: <a href="https://en.wikipedia.org/wiki/Dvoretzky%27s_theorem">Dvoretzky’s theorem</a>, Milman’s theorem, isotropic position, the<a href="https://en.wikipedia.org/wiki/Concentration_of_measure"> concentration of measure phenomenon,</a> asymptotic convex geometry.</li>
</ol>
<h3>The Busseman-Petty problem</h3>
<p><a href="https://en.wikipedia.org/wiki/Busemann%E2%80%93Petty_problem">The Busseman-Petty problem</a> from 1956 asks whether it is true that a symmetric <a href="https://en.wikipedia.org/wiki/Convex_body" title="Convex body">convex body</a> with larger central hyperplane sections has a larger volume. So the question was  if <em>K</em> and <em>L</em> are centrally symmetric problems and all hyperplane sections of <em>K</em> (through the origin)  have a larger volume than those of L, is it true that the volume of <em>K</em> is larger than that of L.  Busemann and Petty showed that the answer is positive if <i>K</i> is a ball. (A positive answer when L is a ball would have given a very strong version of the slicing conjecture.) But the answer is negative for dimensions larger than or equal to 5.</p>
<p>Here is a brief history:</p>
<p>Larman and Rogers constructed a counterexample in dimension 12 in their example L was a ball and K was a perturbation of a ball. Ball showed that taking L to be a ball and K the standard cube gives a counterexample in dimension 10;  <span class="text">Giannopoulos and</span> Bourgain gave counterexamples in dimension 7   and Papadimitrakis finally found a counterexample in dimension 5. Gardner gave a  positive for answer for dimensions 3  Zhang gave a positive solution for dimension 4. Erwin Lutwak’s theory of intersection bodies plays an important role in the solution.  Gardner, Koldobskiy, and Schlumprecht presented a unified solution in all dimensions.</p>
<p>This is a really beautiful story and was saw a slice of the story in our <a href="https://gilkalai.wordpress.com/2008/12/09/test-your-intuition2/">second “test your intuition”</a>.  In the late 90s I had a “what is new” corner on my homepage and I remember devoting it to some of these developments.</p>
<p>Sources: Gardner’s book “<a href="https://www.cambridge.org/core/books/geometric-tomography/FA1A47CCEF63CEBA228520A57294CF7A">Geometric tomography</a>“, and Koldobskiy’s book <a href="https://books.google.co.il/books/about/Fourier_Analysis_in_Convex_Geometry.html?id=tlXzBwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">“Fourier analysis in convex geometry”.</a></p>
<p>Here are<a href="https://gilkalai.files.wordpress.com/2020/12/talk-busemannpetty.pdf"> very nice slides from a lecture of Alexandr Koldobskiy</a> on the problem that also contain some information on the slicing problem.</p>
<p><a href="https://simons.berkeley.edu/workshops/counting2016-1">2016 meeting: Approximate Counting, Markov Chains and Phase Transitions</a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/12/21/to-cheer-you-up-in-difficult-times-15-yuansi-chen-achieved-a-major-breakthrough-on-bourgains-slicing-problem-and-the-kannan-lovasz-and-simonovits-conjecture/"><span class="datestr">at December 21, 2020 04:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/188">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/188">TR20-188 |  Hard QBFs for Merge Resolution | 

	Olaf Beyersdorff, 

	Joshua Blinkhorn, 

	Meena Mahajan, 

	Tomáš Peitl, 

	Gaurav Sood</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove the first proof size lower bounds for the proof system Merge Resolution (MRes [Olaf Beyersdorff et al., 2020]), a refutational proof system for prenex quantified Boolean formulas (QBF) with a CNF matrix. Unlike most QBF resolution systems in the literature, proofs in MRes consist of resolution steps together with information on countermodels, which are syntactically stored in the proofs as merge maps. As demonstrated in [Olaf Beyersdorff et al., 2020], this makes MRes quite powerful: it has strategy extraction by design and allows short proofs for formulas which are hard for classical QBF resolution systems.

Here we show the first exponential lower bounds for MRes, thereby uncovering limitations of MRes. Technically, the results are either transferred from bounds from circuit complexity (for restricted versions of MRes) or directly obtained by combinatorial arguments (for full MRes). Our results imply that the MRes approach is largely orthogonal to other QBF resolution models such as the QCDCL resolution systems QRes and QURes and the expansion systems ?Exp+Res and IR.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/188"><span class="datestr">at December 20, 2020 11:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-985389002707236859">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/12/dr-jill-biden.html">Dr Jill Biden</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(I was helped on this post by Gorjan Alagic, Andrew Childs,  Tom Goldstein, Daniel Gottsman, Clyde Kruskal,  Jon Katz. I emailed them for their thoughts on the issue and some of those thoughts are embedded in the post. ) </p><p>The First First Lady to have a college degree was Lucy Hayes (Rutherford B Hayes was Prez 1876-1880). Nickname: Lemonade Lucy since she did not serve alcohol. </p><p>Trivia: who was the last first lady to not have a college degree? I'll answer that one at the end of this post. </p><p>The First First Lady to keep her day job was Abigail Fillmore who was a teacher. (Millard Fillmore was Prez in 1850-1852. He became prez  after Zachary Taylor died in office) . </p><p>In recent times it is  uncommon for a first lady to have a day job. So much so that it was notable when Elizabeth Dole said that if her husband (Bob Dole) won in 1996 she would keep her job at the Red Cross. </p><p>For other first lady firsts  see <a href="https://en.wikipedia.org/wiki/List_of_United_States_First_Lady_firsts#:~:text=Grace%20Coolidge,-Main%20article%3A%20Grace&amp;text=First%20first%20lady%20to%20earn%20a%20four%2Dyear%20undergraduate%20degree.">here</a>.</p><p>Jill Biden is the First First Lady to have a PhD. (ADDED LATER- one of the comments pointed out that she has an Ed. D, Doctor of Education.)   She says she will keep her day job as a professor.  Four other First ladies had advanced degrees: Pat Nixon (MS in Education), Laura Bush (MS in Library Science), Hillary Clinton (Law degree), Michelle Obama (Law degree). If I missed any, let me know. </p><p>The WSJ had an op-ed  that said Jill Biden should not call herself `Dr'.  Inspired by that, here are thoughts on the use of the word `Dr'</p><p>1) At the 1986 Structures Conference (now Complexity Conference ) Lane Hemachandra (now Lane Hemaspaandra) gave a talk. He had just gotten his PhD a few weeks ago, so he was introduced as `DOCTOR Lane Hemchandra' Gee, most of the talks were by people with PhD's but were not introduced as such.</p><p>2) Most people I know within academia do not call themselves Dr since it sounds pretentious. However, speaking in public about an issue one might want to use `Dr' to signal that you...know stuff. However, it would be odd for a PhD in (say) linguistics to claim he knows a lot about (say) politics. It has been said: an Intellectual is someone who is an expert in one field and pontificates in another field. </p><p>3) Does the general public think of DOCTOR as Medical Doctor? Probably yes. There are some exceptions: Dr. Martin Luther King and Dr. Henry Kissinger. Also, I think it is  more common in Psychology, pharmacy, education, and counseling to call yourself `Doctor'  </p><p>4) The article also criticized her PhD (in education, about community colleges) as `useless' . If that's the reason to not call her doctor than I shudder to think what the WSJ would think of degrees in, say, set theory with an emphasis on Large Cardinals. GEE, you can't call yourself  DOCTOR since your degree is on Ramsey Cardinals. OH, now they found an application, so now you CAN call yourself DOCTOR. OH, the application is to extending the Canonical Ramsey Theory from Polish spaces  to meta- compact  cardinals, so we can't call you DOCTOR after all. Do we really want to go down this path? </p><p>5) I ask all of the following non-rhetorically:  Did the author read her PhD thesis? Is he qualified to judge it? Did he (as one should do) look at her entire body of work to judge her? What point is he trying to make anyway? </p><p>6) Should Dr. Who call themselves a doctor? Are they  a medical doctor? PhD? If so, in what? Is `Who' part of their name? For other TV and movie tropes about the use of the word doctor, see <a href="https://tvtropes.org/pmwiki/pmwiki.php/Quotes/NotThatKindOfDoctor">here</a>.</p><p>7) I avoid saying I am a doctor since people will then ask me about the medical condition.</p><p>I avoid saying I am a computer scientist since people will then ask me how to help them with their Facebook privacy settings. </p><p>I avoid saying I am a mathematician since people will ask me to help their daughter with her trigonometry. </p><p>8) The answer to my trivia question: The last first lady to not have a college degree: Melania Trump. She went to college for a year and then left. The one before her was Barbara Bush who also went for a year and then left. </p><p>ADDED LATER: Many supervillians who don't have a PhD or an MD call themselves `Doctor', see <a href="https://www.buzzfeed.com/donnad/11-super-villains-masquerading-as-doctors">here</a>. Why no outrage about this? Because (1) they are fictional, and (2) imagine the scenario: Not only does Dr. Doom want to take over the world, he also doesn't even have a PhD or an MD!</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/12/dr-jill-biden.html"><span class="datestr">at December 20, 2020 06:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5129">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/optimization-is-as-hard-as-approximation/">Optimization is as hard as approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Optimization is a key tool in machine learning, where the goal is to achieve the best possible objective function value in a minimum amount of time. Obtaining any form of global guarantees can usually be done with <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex</a> objective functions, or with special cases such as risk minimization with one-hidden over-parameterized layer neural networks (see the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">June post</a>). In this post, I will consider low-dimensional problems (imagine 10 or 20), with no constraint on running time (thus get ready for some running-times that are exponential in dimension!).</p>



<p class="justify-text">We consider minimizing a function \(f\) on a bounded subset  \(\mathcal{X}\) of \(\mathbb{R}^d\), based only on function evaluations, a problem often referred to as zero-th order optimization or <a href="https://en.wikipedia.org/wiki/Derivative-free_optimization">derivative-free optimization</a>. No convexity is assumed, so we should not expect fast rates, and, again, no efficient algorithms that can provably find a global minimizer. Good references on what I am going to cover in this post are [1, 2, 5].</p>



<p class="justify-text">One may wonder why this is interesting at all. Clearly, such algorithms are not made to be used to find millions of parameters for logistic regression or neural networks, but they are often used for hyperparameter tuning (regularization parameters, size of neural network layer, etc.). See, e.g., [<a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">6</a>] for applications.</p>



<p class="justify-text">We are going to assume some regularity for the functions we want to minimize, typically bounded derivatives. We will thus assume that \(f \in \mathcal{F}\), for a space \(\mathcal{F}\) of functions from \(\mathcal{X}\) to \(\mathbb{R}\). We are going to take a worst-case approach, where we characterize convergence over all members of \(\mathcal{F}\).  That is, we want our guarantees to hold for <em>all</em> functions in \(\mathcal{F}\). Note that this worst-case analysis may not predict well what’s happening for a particular function; in particular, it is (by design) pessimistic.</p>



<p class="justify-text">An algorithm \(\mathcal{O}\) will be characterized by (a) the choice of points \(x_1,\dots,x_n \in \mathcal{X}\) to query the function, and (b) the algorithm to output a candidate \(\hat{x} \in \mathcal{X}\) such that \(f(\hat{x}) \ – \inf_{x \in \mathcal{X}} f(x)  \) is small. The estimate \(\hat{x}\) can only depend on \((x_i,f(x_i))\), for \(i \in \{1,\dots,n\}\). In most of this post, the choice of points \(x_1,\dots,x_n\) is made once (without seeing any function values). We show later in this blog post that going <em>adaptive</em>, where the point \(x_{i+1}\) is selected after seeing \((x_j,f(x_j))\) for all \(j \leqslant i\), does not bring much (at least in the worst-case sense).</p>



<p class="justify-text">Given a selection of points and the algorithm \(\mathcal{O}\), the rate of convergence is the supremum over all functions \(f \in \mathcal{F}\) of the error \(f(\hat{x}) \ – \inf_{x \in \mathcal{X}} f(x)\). This is a function \(\varepsilon_n(\mathcal{O})\) of the number \(n\) of sampled points (and of the the class of functions \(\mathcal{F}\)). The optimal algorithm (minimizing \(\varepsilon_n(\mathcal{O})\)) will lead to a rate we denote \(\varepsilon_n^{\rm opt}\), and which we aim to characterize.</p>



<h2>Direct lower/upper bounds for Lipschitz-continuous functions</h2>



<p class="justify-text">The argument is particularly simple for a bounded metric space \(\mathcal{X}\) with distance \(d\), and \(\mathcal{F}\) the class of \(L\)-Lipschitz-continuous functions, that is, such that for all \(x,x’ \in \mathcal{X}\), \(|f(x) -f(x’)| \leqslant L d(x,x’)\). This is a very large set of functions, so expect weak convergence rates.</p>



<p class="justify-text"><strong>Set covers. </strong>We will need to cover the set \(\mathcal{X}\) with balls of a given radius. The minimal radius \(r\) of a cover of \(\mathcal{X}\) by \(n\) balls of radius \(r\) is denoted \(r_n(\mathcal{X},d)\). This corresponds to \(n\) ball centers \(x_1,\dots,x_n\). See example below for the unit cube \(\mathcal{X} = [0,1]^2\) and the metric obtained from the \(\ell_\infty\)-norm, with \(n = 16\), and \(r_n([0,1]^2,\ell_\infty) = 1/8\). See more details on covering numbers <a href="https://en.wikipedia.org/wiki/Covering_number">here</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="269" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/cover-1024x1024.png" class="wp-image-5187" height="269" /></figure></div>



<p class="justify-text">More generally, for the unit cube \(\mathcal{X} = [0,1]^d\), we have \(r_n([0,1]^d,\ell_\infty) \approx \frac{1}{2} n^{-1/d}\) (which is not an approximation where \(n\) is the \(d\)-th power of an integer). For other normed metrics, (since all norms are equivalent) the scaling as \(r_n \sim {\rm diam} (\mathcal{X}) n^{-1/d}\) is the same on any bounded set in \(\mathbb{R}^d\) (with an extra constant that depends on \(d\)).</p>



<p class="justify-text"><strong>Algorithm. </strong>Given the ball centers \(x_1,\dots,x_n\), outputting the minimum of function values \(f(x_i)\) for \(i=1,\dots,n\), leads to an error which is less than \(L r_n(\mathcal{X},d)\), as the optimal \(x_\ast \in \mathcal{X}\) is at most at distance \(r_n(\mathcal{X},d)\) from one of the cluster centers, let’s say \(x_k\), and thus \(f(x_k)\  – f(x_\ast) \leqslant L d(x_k,x_\ast) \leqslant L r_n(\mathcal{X},d)\). This provides an upper-bound on \(\varepsilon_n^{\rm opt}\). The algorithm we just described seems naive, but it turns out to be optimal for this class of problems.</p>



<p class="justify-text"><strong>Lower-bound.</strong> Consider any optimization algorithm, with its first \(n\) point queries and its estimate \(\hat{x}\). By considering the functions which are zero in these \(n+1\) points, the algorithm outputs zero. We now simply need to construct a function \(f \in \mathcal{F}\) such that \(f\) is zero at these points, but maximally smaller than zero at a different point. </p>



<p class="justify-text">Consider a cover of \(\mathcal{X}\) with \(n+2\) balls of minimal radius  (equal to \(r_{n+2}(\mathcal{X},d)\)), there has to exist at least one of the \(n+2\) corresponding ball centers such that the corresponding ball contains no points from the algorithm (denote by \(y\) its center). We can then construct the function $$ f(x)  = \ – L \big( r_{n+2}(\mathcal{X},d) \ – d(x,y) \big)_+ = \ – L \max \big\{  r_{n+2}(\mathcal{X},d) \ – d(x,y) ,0 \big\}, $$ which is zero on all points of the algorithm and the output point \(\hat{x}\), and with minimum value \(– L r_{n+2}(\mathcal{X},d)\) attained at \(y\). Thus, we must have \(\varepsilon_n^{\rm opt} \geqslant 0 \ – (\ – L r_{n+2}(\mathcal{X},d) ) = L r_{n+2}(\mathcal{X},d)\). This difficult function is plotted below in one dimension.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="423" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/lowerbound-1-1024x178.png" class="wp-image-5260" height="73" /></figure></div>



<p class="justify-text">Thus, the performance of any algorithm from \(n\) function values has to be larger than \(L r_{n+2}(\mathcal{X},d)\).  Thus, so far, we have shown that $$L r_{n+2}(\mathcal{X},d) \leqslant \varepsilon_n^{\rm opt} \leqslant L r_{n}(\mathcal{X},d).$$  For \(\mathcal{X} \subset \mathbb{R}^d\), \(r_n(\mathcal{X},d)\) is typically of order \({\rm diam}(\mathcal{X}) n^{-1/d}\), and thus the difference between \(n\) and \(n+2\) above is negligible. Note that the rate in \(n^{-1/d}\) is <strong>very</strong> slow, and symptomatic of the classical curse of dimensionality. The appearance of a covering number is not totally random here, as we will see below.</p>



<p class="justify-text"><strong>Random search.</strong> We can have a similar bound up to logarithmic terms for random search, that is, after selecting independently \(n\) points \(x_1,\dots,x_n\), uniformly at random in \(\mathcal{X}\), and selecting the points with smallest function value \(f(x_i)\). The performance can be shown to be proportional to \(L {\rm diam}(\mathcal{X}) ( \log n )^{1/d} n^{-1/d}\) in high probability, leading to an extra logarithmic term (the proof can be obtained with a simple covering argument, as shown at the end of the post). Therefore, random search is optimal up to logarithmic terms for this very large class of functions to optimize.</p>



<p class="justify-text">We would like to go beyond Lipschitz-continuous functions, and study if we can leverage smoothness, and hopefully avoid the dependence in \(n^{-1/d}\). This can be done by a somewhat surprising equivalence between worst case guarantees from optimization and worst case guarantees for uniform approximation.</p>



<h2>Optimization is as hard as uniform function approximation</h2>



<p class="justify-text">We now also consider the problem of outputting a whole function \(\hat{f} \in \mathcal{F}\), such that \(\|f – \hat{f}\|_\infty = \max_{x \in \mathcal{X}} | \hat{f}(x)\ – f(x)|\) is as small as possible. For any approximation algorithm \(\mathcal{A}\) that builds an estimate \(\hat{f} \in \mathcal{F}\) from \(n\) function values, we can define its convergence rate in the same way as for optimization algorithm \(\varepsilon_n(\mathcal{A})\), as a function of \(n\). The optimal approximation algorithm has a convergence rate denoted by \(\varepsilon_n^{\rm app}\).</p>



<p class="justify-text"><strong>From approximation to optimization. </strong>Clearly, an approximation algorithm \(\mathcal{A}\) leads to an optimization algorithm \(\mathcal{O}\) with at most twice the same rate, that is, $$  \varepsilon_n(\mathcal{O}) \leqslant 2\varepsilon_n(\mathcal{A}),$$ by simply approximating \(f\) by \(\hat{f}\) and outputting any \(\hat{x} \in \arg \min_{x \in \mathcal{X}} \hat{f}(x)\), for which $$f(\hat{x})  \leqslant \hat{f}(\hat{x}) + \| \hat{f}\ – f\|_\infty =  \min_{x \in \mathcal{X}} \hat{f}(x) + \| \hat{f}\  – f\|_\infty \leqslant \min_{x \in \mathcal{X}} {f}(x) +2  \| \hat{f} \ – f\|_\infty. $$ See an illustration below (with a function estimated from the values at green points), with the candidate minimizer in orange.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="470" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/optim_approx-1024x553.png" class="wp-image-5268" height="254" /></figure></div>



<p class="justify-text">Thus in terms of worst-case performance, we get $$ \varepsilon_n^{\rm opt} \leqslant 2\varepsilon_n^{\rm app}.$$ Intuitively, it seems that this upper-bound should be very loose, as approximating uniformly a function, in particular far from its minimum, seems useless for minimization. For worst-case performance, this intuition is incorrect… Indeed, the optimal rate for optimization with \(n\) function evaluations happens to be greater than the optimal rate for approximation with \(n+1\) function evaluations, which we will now show.</p>



<p class="justify-text"><strong>Lower-bound on optimization performance. </strong>We will need an extra assumption, namely that the function space \(\mathcal{F}\) is convex and symmetric (and bounded in uniform norm).</p>



<p class="justify-text">We consider an optimization algorithm over the class of function \(\mathcal{F}\), that is considering \(n\) observation points \(x_1,\dots,x_n\), and an estimate \(\hat{x}\). The worst-case performance over all functions in \(\mathcal{F}\) is greater than over the (smaller) class of functions for which \(f(x_1) = \cdots = f(x_n) = f(\hat{x}) = 0\). Given that the performance measure is \(f(\hat{x})\  – \inf_{x \in \mathcal{X}} f(x)\), the performance is greater than the supremum of  \(– \inf_{x \in \mathcal{X}} f(x)\) over functions in \(\mathcal{F}\) such that \(f(x_1) = \cdots = f(x_n) = f(\hat{x}) = 0\). For Lipschitz-continuous functions above, we built explicitly a hard function. In the general case, an explicit construction is not that easy, but we will relate the construction of such a function to the general approximation problem.</p>



<p class="justify-text">As we just saw, the optimal rate of the optimization algorithm is greater than $$\inf_{x_1,\dots,\, x_{n+1} \in \, \mathcal{X}} \sup_{f \in \mathcal{F}, \ f(x_1)\ =\ \cdots \ = \ f(x_{n+1}) \ =\ 0} – \inf_{x \in \mathcal{X}} f(x) . $$ The quantity above characterizes how small a function can be when equal to zero on \(n+1\) points. When the set of function \(\mathcal{F}\) is centrally symmetric, that is \(f \in \mathcal{F} \Rightarrow \ – f \in \mathcal{F}\), we can replace \(– \inf_{x \in \mathcal{X}} f(x)\) in the expression above by \(\| f\|_\infty\).</p>



<p class="justify-text">It turns out that for convex and centrally symmetric spaces \(\mathcal{F}\) of functions, this happens to be the optimal rate of approximation with \(n+1\) function evaluations. This is sometimes referred to as Smolyak’s lemma [3], which we state here (see a very nice and short proof in [4]).</p>



<p class="justify-text"><strong>Smolyak’s lemma. </strong>We consider a space of functions \(\mathcal{F}\) which is convex and centrally symmetric. Then:</p>



<ul class="justify-text"><li>For any \(x_1,\dots,x_n \in \mathcal{X}\), the optimal approximation method, that is the optimal map \(\mathcal{S}: \mathbb{R}^n \to \mathcal{F}\), i.e., an algorithm that computes \(\hat{f} = \mathcal{S}(f(x_1),\dots,f(x_n)) \in \mathcal{F}\), is <em>linear</em>, that is, there exist functions \(g_1,\dots,g_n: \mathcal{X} \to \mathbb{R}\) such that $$\mathcal{S}(f(x_1),\dots,f(x_n)) = \sum_{i=1}^n f(x_i) g_i.$$</li><li>The optimal rate of uniform approximation in \(\mathcal{F}\) is equal to $$ \displaystyle \varepsilon_n^{\rm app} = \inf_{x_1,\dots,\, x_{n} \in \, \mathcal{X}} \sup_{f \in \mathcal{F}, \ f(x_1)\ =\ \cdots \ = \ f(x_{n}) \ =\ 0} \| f \|_\infty.$$</li></ul>



<p class="justify-text">Thus, we have “shown” that: $$ \varepsilon_{n+1}^{\rm app} \leqslant \varepsilon_n^{\rm opt} \leqslant 2\varepsilon_n^{\rm app}, $$ that is, optimization is at most twice as hard as uniform approximation (still in the worst-case sense). We can now consider examples of uniform approximation algorithms to get optimization algorithms.</p>



<h2>Examples and optimal algorithms</h2>



<p class="justify-text">As seen above, we simply need algorithms that approximate the function in uniform norm, and then we minimize the approximation. This is computationally optimal in terms of access to function evaluations (but clearly not in terms of computational complexity).</p>



<p class="justify-text"><strong>Lipschitz-continuous functions.</strong> One can check that with \(x_1,\dots,x_n\) the centers of the cover above, and a piecewise constant function on each of the ball (with arbitrary values at intersections), then we have $$ \| \hat{f} \, – f \|_\infty \leqslant L   r_n(\mathcal{X},d),$$ which is a standard approximation result using covering numbers. We recover the result above directly from the cover argument (and here the sharper result that the rates of uniform approximation and optimization are asymptotically the same for this class of functions).</p>



<p class="justify-text"><strong>Smooth functions in one dimension.</strong> For simplicity, I will focus on the one-dimensional problem, where all concepts are simpler, some of them directly extend to higher dimensions, some of them don’t.</p>



<p class="justify-text">We consider \(\mathcal{X} = [0,1]\). The simplest interpolation techniques are piecewise constant and piecewise affine interpolations. That is, if we observe \(0 = x_1 \leqslant \cdots \leqslant x_n = 1\), we consider \(\hat{f}\) defined on \([x_i,x_{i+1}]\) as $$\hat{f}(x) = \frac{1}{2}f(x_i) + \frac{1}{2}f(x_{i+1}) $$ </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="339" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation_constant-2-1024x451.png" class="wp-image-5286" height="152" /></figure></div>



<p class="justify-text">or $$ \hat{f}(x) = f(x_i) + \frac{ x\  – x_i}{x_{i+1}-x_i}\big[ f(x_{i+1})\ – f(x_i) \big].$$</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="324" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation_affine-1024x449.png" class="wp-image-5287" height="144" /></figure></div>



<p class="justify-text">If we assume that \(f\) is twice differentiable with second-derivative bounded by \(M_2\), then, we can show that on \([x_i,x_{i+1}]\), we have for piecewise affine interpolation: $$ | f(x) \ – \hat{f}(x) | \leqslant \frac{M_2}{8} | x_{i+1} – x_i |^2.$$</p>



<p class="justify-text">If we assume that \(f\) is differentiable with first-derivative bounded by \(M_1\), then, we can show that on \([x_i,x_{i+1}]\), we have for piecewise affine or constant interpolation: $$ | f(x) \ – \hat{f}(x) | \leqslant \frac{M_1}{2} | x_{i+1} – x_i |.$$</p>



<p class="justify-text">This leads to, when \(x_i = (i-1)/(n-1)\), to uniform error in \(O(1/n)\) for functions with a bound on a single derivative and or \(O(1/n^2)\) for two derivatives. Thus, we see two effects, that are common in approximations problems: (a) the more regular the function to approximate, the better the approximation rate, (b) If we consider a method tailored to smoother functions, it often works well for less smooth functions.</p>



<p class="justify-text">Explicit piecewise affine interpolation is harder to perform in higher dimensions, where other techniques can be used as presented below, such as <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a> (again!).</p>



<p class="justify-text"><strong>Going high-dimensional with kernel methods.</strong> We assume that we have a positive definite kernel \(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) on \(\mathcal{X}\), and, given the \(n\) elements \(x_1,\dots,x_n\) of \(\mathcal{X}\) and function values \(f(x_1),\dots,f(x_n)\), we look for the interpolating function \(f\) in the corresponding reproducing kernel Hilbert space with minimum norm. By the <a href="https://en.wikipedia.org/wiki/Representer_theorem">representer theorem</a>, it has to be of the form \(\sum_{i=1}^n  \alpha_i k(x,x_i)\). Since it has to interpolate, we must have \(K\alpha = y\) and thus \(\alpha = K^{-1} y\), where \(y_i = f(x_i)\) for \(i \in \{1,\dots,n\}\), and \(K\) the \(n \times n\) kernel matrix. Thus, interpolation can then be easily done in higher dimensions.</p>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a> are commonly used in the interpolation context, and for \(s\) square-integrable derivatives, for \(s &gt; d/2\), they are reproducing kernel Hilbert spaces. See below for examples of interpolations with various Sobolev spaces and increasing number of interpolating points, where the approximation errors (in uniform norm) are computed. With \(s=1\), we recover piecewise affine interpolation, but smoother functions are obtained for \(s=2\).</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="468" alt="" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation-2.gif" class="wp-image-5238" height="250" /></figure></div>



<p class="justify-text"><strong>Optimal rates of approximation.</strong> With the method defined above, we can obtain the optimal rates of approximation (and hence of optimization) of \(\varepsilon_n^{\rm app} \approx \displaystyle \frac{1}{n^{m/d}}\), for the set of functions with \(m\) bounded derivatives. Thus, for very smooth functions, we can escape the curse of dimensionality (that is, obtain a power of \(n\) that does not decay too slowly). See [1] for more details.</p>



<h2>The powerlessness of adaptivity</h2>



<p class="justify-text">The critical reader may argue that the algorithm set-up described in this post is too simple: the points at which we take function values are decided once and for all, independently of the observed function values. Clearly, some form of adaptivity should beat random search. The sad truth is that the bound for worst-case performance is the same… up to a factor of 2 at most (still in the worst-case sense).</p>



<p class="justify-text">The argument is essentially the same as for non-adaptive algorithms: for the Lipschitz-continuous example, our hard function can also be built for adaptive algorithms, while for the general case, it simply turns out that the rate for adaptive approximation is exactly the same than for adaptive approximation [4].</p>



<p class="justify-text">Thus, adaptive and non-adaptive approximations are just a factor of two of each other. Note that given the practical success of <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> (which is one instance of adaptive optimization) on some problems, there could be at least two explanations (choose the one you prefer, or find a new one): (1) the worst-case analysis can be too pessimistic, or (2) what is crucial in Bayesian optimization is not the adaptive choice of points to evaluate the function, but the adaptivity to smoothness of the function to optimize (that is, if the function has \(m\) derivatives, then the rate is \(n^{-m/d}\), which can be much better than \(n^{-1/d}\)).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I highlighted the strong links between approximation problems and optimization problems. It turns out that the parallel between worst-case performances goes beyond: computing integrals from function evaluations, a problem typically referred to as <a href="https://en.wikipedia.org/wiki/Numerical_integration">quadrature</a>, is also as hard. More on this in a future post.</p>



<p class="justify-text">As warned early in the post, the algorithms presented here all have running time that are exponential in dimension, as they either perform random sampling or need to minimize a model of the function to optimize. </p>



<p class="justify-text">The only good news in this post: optimization is hard as approximation, but the scenario is more varied than expected. Indeed approximation can be fast and avoid the curse of dimensionality, at least in the exponent of the rate, when the function is very smooth (indeed, for \(m\)-times differentiable functions, the constant in front of \(n^{-m/d}\) still depends exponentially in \(d\)). It would be nice to catch this property also in an optimization algorithm, with a running time complexity depending polynomially on \(n\) only, and not exponentially in \(d\). Next month, I will present recent work with Alessandro Rudi and Ulysse Marteau-Ferey [<a href="https://arxiv.org/pdf/2012.11978">7</a>] that does exactly this, by combining the tasks of interpolation and optimization in a single convex optimization problem.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Alessandro Rudi and Ulysse Marteau-Ferey for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Erich Novak. <em>Deterministic and stochastic error bounds in numerical analysis</em>. Springer, 2006.<br />[2] Erich Novak, Henryk Woźniakowski. Tractability of Multivariate Problems: Standard information for functionals, European Mathematical Society, 2008.<br />[3] S. A. Smolyak. On optimal restoration of functions and functionals of them, Candidate Dissertation, Moscow State University, 1965.<br />[4] Nikolai Sergeevich Bakhvalov. <a href="https://www.sciencedirect.com/science/article/abs/pii/0041555371900176">On the optimality of linear methods for operator approximation in convex classes of functions</a>. USSR Computational Mathematics and Mathematical Physics. 11(4): 244-249, 1971.<br />[5] Yurii Nesterov. <em>Lectures on Convex Optimization</em>. Springer, 2018.<br />[<a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">6</a>] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. <a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">Practical Bayesian optimization of machine learning algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 2015.<br />[7] Alessandro Rudi, Ulysse Marteau-Ferey, Francis Bach. <a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">Finding Global Minima via Kernel </a><a href="https://arxiv.org/pdf/2012.11978">Approximations</a>. Technical report arXiv 2012.11978, 2020.</p>



<h2>Performance of random search</h2>



<p class="justify-text">We consider sampling independently and uniformly in \(\mathcal{X}\) \(n\) points \(x_1,\dots,x_n\). For a given \(L\)-Lipschitz-continuous function \(f\), with global minimizer \(x\) on \(\mathcal{X}\), we have $$\min_{i \in \{1,\dots,n\}} f(x_i)\  – f(x) \leqslant L \min_{i \in \{1,\dots,n\}} d(x,x_i).$$ Thus, to obtain an upper-bound on performance over all functions \(f\) (and potentially all locations of the global minimizer \(x\)), we need to bound $$\max_{x \in \mathcal{X}} \min_{i \in \{1,\dots,n\}} d(x,x_i).$$ If we have a cover with \(m\) points \(y_1,\dots,y_m\), and radius \(r = r_m(\mathcal{X},d)\), we have $$\max_{x \in \mathcal{X}} \min_{i \in \{1,\dots,n\}} d(x,x_i) \leqslant r + \max_{j  \in \{1,\dots,m\}} \min_{i \in \{1,\dots,n\}} d(y_j,x_i) .$$ We can then bound the probability that $$ \max_{j \in \{1,\dots,m\}} \min_{i \in \{1,\dots,n\}} d(y_j,x_i) \geqslant r$$ by the union bound (and using the independence of the \(x_i\)’s) as \(m\) times the \(n\)-th power of the probability that one of the \(x_i\) is not in a given ball of radius \(r\). By a simple volume argument (and assuming that all balls of a given radius have the same volume), this probability is less than \(( 1 – 1/m)\). Thus with probability greater than \(1 – m(1 – 1/m)^n \geqslant 1 – m e^{ – n /m}\) the worst-case performance is less than \(2Lr\).</p>



<p class="justify-text">Since for normed metrics in \(\mathbb{R}^d\), \(r  \sim m^{-1/d} {\rm diam}(\mathcal{X})\), by selecting \(m = 2n \log n\), we get an overall performance proportional to \( L \big( \frac{ \log n}{n} \big)^{1/d}\) with probability greater than \(1 – \frac{ \log n}{n}\) (which tends to one when \(n\) grows).</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/optimization-is-as-hard-as-approximation/"><span class="datestr">at December 18, 2020 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17896">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/">Database and Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>A dean of engineering</em><br />
</span></p>
<p><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/dean-2/" rel="attachment wp-att-17922"><img width="180" alt="" src="https://rjlipton.files.wordpress.com/2020/12/dean-1.jpg?w=180&amp;h=173" class="alignright wp-image-17922" height="173" /></a></p>
<p>Jennifer Widom was appointed as the <a href="https://news.stanford.edu/2017/02/27/computer-scientist-jennifer-widom-named-dean-stanford-school-engineering/">Dean</a> of Engineering at Stanford not long ago. No doubt her research in databases and her teaching played a major role in her selection. For example, she is known for her contributions to online education including teaching one of the first massive open courses—<a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOCS</a>—which had over 100,000 students.</p>
<p>Today I thought we might highlight her area, databases, and one of its connections to theory.</p>
<p>But first, I was impressed to hear her say in an ACM <a href="https://learning.acm.org/bytecast">podcast</a> that she used punch cards during the start of her career:</p>
<blockquote><p><b> </b> <em> So my passion in high school actually became music, and when it came time to select a college, I chose to go to music school. I’m pretty sure at this point, I’m the only dean of engineering anywhere who has a bachelor’s degree in trumpet performance, but that’s actually what my undergraduate degree is in. Late in my music education, I just sort of randomly took a class called computer applications and music research. And it was a class in the music school about using programming to analyze music, and it was my first exposure to computer programming. I have to say, it’ll reveal my age, but I used punch cards in that class. It was sort of the end of the punch card era. </em></p></blockquote>
<p>I admit to having used these too at the start of my days as an undergrad at <a href="https://www.fourmilab.ch/documents/univac/cards.html">Case</a> Institute. Do you know what a punch card is?</p>
<p><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/case/" rel="attachment wp-att-17901"><img src="https://rjlipton.files.wordpress.com/2020/12/case.png?w=600" alt="" class="aligncenter size-full wp-image-17901" /></a></p>
<p>Ken once had his own experience with punch cards and music. While taking an intro programming course during his sophomore year—taught then by Jeffrey Ullman—Ken and his roommate joined an excursion to NYC for dinner and the New York Philharmonic. Still dressed in suits, on return to Princeton, they descended into the computer center to do their assignment—via punch cards on a batch server. Ken’s roommate got out before 2am, but Ken says he was still fixing punch card typos that ruined runs as late as 4am.</p>
<h2>Data Management</h2>
<p>Widom’s <a href="https://cs.stanford.edu/people/widom/">research</a> has always been in the area of non-traditional databases: semi-structured data, data streams, uncertain data and data provenance.</p>
<p>We theorists view the world as made of questions like:</p>
<ul>
<li>Upper Bounds Can we find a faster algorithm for problem X?</li>
<li>Lower Bounds Can we prove there are no faster algorithms for X? Sometimes we replace “faster” by other complexity measures, but these are the dominant questions we ask.
<p>Widom proposed a notion that is called <a href="https://www.sciencedirect.com/topics/computer-science/data-provenance">lineage</a>. Theorists worry about the performance of searching, but this issue is about where did the data come from? See also <a href="http://db.cis.upenn.edu/DL/fsttcs.pdf">provenance</a>.</p>
<p>The insight boils down to garbage in and garbage out, or <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out">GIGO</a>:</p>
<blockquote><p><b> </b> <em> On two occasions I have been asked, “Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?”</em></p>
<p><em> Charles Babbage (1864). </em></p></blockquote>
<p>The issue of lineage and provenance is exactly this made modern. How can one track where the data comes from? Of course, finding fast algorithms to do the tracking is still an important question. Oh well.</p>
<h2>Databases Meet Theory</h2>
<p>Of course databases store and allow us to retrieve information—they are everywhere these days. One reason for their dominance is they support complex queries. For example, we can use databases to obtain all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R(x,y)}" class="latex" title="{R(x,y)}" /> is true for some <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" />:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7B+x+%5Cmid+%5Cexists+y+R%28x%2Cy%29+%5C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \{ x \mid \exists y R(x,y) \}. " class="latex" title="\displaystyle \{ x \mid \exists y R(x,y) \}. " /></p>
<p>From a theory point of view any first-order question is allowed.</p>
<p>But searches often use approximate matches. Thus one might wish all <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%27%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R(x',y)}" class="latex" title="{R(x',y)}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x'}" class="latex" title="{x'}" /> is near <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" />. We might want all <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%27%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R(x',y)}" class="latex" title="{R(x',y)}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x'}" class="latex" title="{x'}" /> is not “Michael” but also includes “Micheal”. This type of error is not too hard to handle, but a worse type of error is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Michael+%5Ciff+Michal+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle Michael \iff Michal " class="latex" title="\displaystyle Michael \iff Michal " /></p>
<p>Formally changing a letter is not as difficult as deleting or adding letters. This is the <a href="https://web.stanford.edu/class/cs124/lec/med.pdf">edit distance</a> between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. An important application is to biology.</p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/edit/" rel="attachment wp-att-17903"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2020/12/edit.png?w=600&amp;h=289" class="aligncenter size-full wp-image-17903" height="289" /></a></td>
</tr>
</tbody>
</table>
<p>This leads to into the more-general notion of <em>entity resolution</em> (ER). Widom co-leads the <em>Stanford Entity Resolution Framework</em> (<a href="http://infolab.stanford.edu/serf/">SERF</a>) with Hector Garcia-Molina; they were two of the authors on its earliest <a href="http://ilpubs.stanford.edu:8090/779/">paper</a> in 2006. The page highlights the lexical nature of ER:</p>
<blockquote><p><b> </b> <em> For instance, two records on the same person may provide different name spellings, and addresses may differ. The goal of ER is to “resolve” entities, by identifying the records that represent the same entity and reconciling them to obtain one record per entity. </em></p></blockquote>
<p>Their approach “involves functions that `match’ records (i.e., decide whether they represent the same entity) and `merge’ them.” Their framework has an outer layer that treats the functions as generic. Widom’s emphasis has been on axiomatizing properties of the functions to enable efficient and extensible manipulation by the outer layer.</p>
<p>Necessarily under the hood is a measure of edit distance. Ken recently monitored a large tournament where two players had the same surname and first three prenames, differing only in the last four letters of their last prename. The widget used to download the game file originally given to Ken truncated the names so that there seemed to be one person playing twice as many games. They could also have been distinguished by other database information. More common is when the same player is referenced with different spellings, such as (grandmaster Artur) Jussupow or Yusupov. The similarity metric needs to be extended beyond the name. This is when computation time becomes more a factor and theory enters in.</p>
<h2>Edit Distance</h2>
<p>Computing the <a href="https://en.wikipedia.org/wiki/Edit_distance">edit distance</a> between two strings is a long-studied problem. The best algorithm known is order quadratic time, as we covered <a href="https://rjlipton.wordpress.com/2009/03/22/bellman-dynamic-programming-and-edit-distance/">here</a>. There are better running times if we allow approximate algorithms or quantum algorithms. See <a href="https://arxiv.org/pdf/1804.04178.pdf">this</a> for the latter and this <a href="https://rjlipton.wordpress.com/2015/07/22/alberto-apostolico-1948-2015/">post</a> for work related to the former.</p>
<p>Theorists have tried to improve the time needed for classical exact edit distance. It remains at quadratic time, however, and there is <a href="https://www.quantamagazine.org/edit-distance-reveals-hard-computational-problems-20150929/">evidence</a> of inability to do better that we have remarked as <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">puzzling</a>. One of the strange traits of theorists is that when something is hard we try to <a href="https://www.theidioms.com/when-life-gives-you-lemons/">exploit</a> the hardness.</p>
<blockquote><p><b> </b> <em> When life gives you lemons, make lemonade. </em></p></blockquote>
<p>For instance, factoring integers is hard—so let us make crypto systems. The lemonade in the case of edit distance is a connection to the hardness of really-hard problems. To quote the seminal <a href="https://arxiv.org/pdf/1412.0348.pdf">paper</a> by Arturs Backurs and Piotr Indyk:</p>
<blockquote><p><b> </b> <em> In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be tight. Specifically, we show that, if the edit distance can be computed in time <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2-%5Cepsilon%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(n^{2-\epsilon})}" class="latex" title="{O(n^{2-\epsilon})}" /> for some constant <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" />, then the satisfiability of conjunctive normal form formulas with <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" title="{N}" /> variables and <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" title="{M}" /> clauses can be solved in time subexp. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist. </em></p></blockquote>
<h2>Edit Distance—Searching</h2>
<p>An open problem that is closer to databases is not computing edit distance. Instead, the problem how to search for strings that are <em>near</em> a given string. This can be a generic use of an edit-distance function. A larger question is whether this can be approached more directly.</p>
<p>I believe that while edit distance may indeed be quadratic time, it still is open how fast searches can be done for approximate matches. The problem is given a large collection of strings <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> how fast can we find the closest string in the collection to some word <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w}" class="latex" title="{w}" />? That the best algorithms for edit distance are probably quadratic does not—I believe—entail that this search question is difficult.</p>
<h2>Open Problems</h2>
<p>By the way, see <a href="https://databasetheory.org/node/76">this</a> for other open problems in databases.</p>
<p>[deleted stray line]</p></li>
</ul></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/"><span class="datestr">at December 17, 2020 04:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-411297283565155244">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/12/optiland.html">Optiland</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Many of you have heard of Russell Impagliazzo's <a href="https://blog.computationalcomplexity.org/2004/06/impagliazzos-five-worlds.html">five worlds</a> from his 1995 classic <a href="https://doi.org/10.1109/SCT.1995.514853">A personal view of average-case complexity</a> In short </p><ul><li><i>Algorithmica</i>: P = NP or something "morally equivalent" like fast probabilistic algorithms for NP. </li><li><i>Heuristica</i>: NP problems are hard in the worst case but easy on average.</li><li><i>Pessiland</i>: NP problems hard on average but no one-way functions exist. We can easily create hard NP problems, but not hard NP problems where we know the solution. </li><li><i>Minicrypt</i>: One-way functions exist but we do not have public-key cryptography.</li><li><i>Cryptomania</i>: Public-key cryptography is possible, i.e. two parties can exchange secret messages over open channels.</li></ul><div>Impagliazzo's world has an explicit "you can't have your cake and eat it too", either you can solve NP-hard problems on average, or have cryptography but not both (neither is possible). That's the mathematical world of P v NP. </div><div><br /></div><div>The reality is looking more and more like <i>Optiland</i>, where we can solve difficult NP problems and still have cryptography thanks to vast improvements in machine learning and optimization on faster computers with specialized hardware.</div><div><br /></div><div>Back in 2004 I <a href="https://blog.computationalcomplexity.org/2004/05/what-if-p-np.html">gave my guess</a> of the world of P = NP</div><div><blockquote>Learning becomes easy by using the principle of Occam's razor--we simply find the smallest program consistent with the data. Near perfect vision recognition, language comprehension and translation and all other learning tasks become trivial. We will also have much better predictions of weather and earthquakes and other natural phenomenon.</blockquote><p>Today you can take your smartphone, unlock it by having the phone scan your face, and ask it a question by talking and often get a reasonable answer, or have your question translated into a different language. You get alerts on your phone for weather and earthquakes, with far better predictions than we would have though possible a dozen years ago.</p><p>We have computed the <a href="http://www.math.uwaterloo.ca/tsp/uk/index.html">shortest traveling-salesman tour</a> through nearly 50K UK pubs. <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold</a> can simulate protein folding with an accuracy nearly as good as what we get with real-world experiments. You can view GPT-3 as generating a form of a universal prior. Even beyond P = NP, we have self-trained computers easily besting humans in Chess, Go and Poker.</p><p>Meanwhile these techniques have done little to break cryptographic functions. Plenty of cybersecurity attacks but rarely by breaking the cryptography. </p><p>Not all is rosy--there is still much more we could do positively if P = NP  and we are already seeing some of the negative effects of learning such as loss of privacy. Nevertheless we are heading to a de facto best of both worlds when complexity theory tells us those worlds are incompatible. </p></div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/12/optiland.html"><span class="datestr">at December 16, 2020 02:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5159">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5159">Chinese BosonSampling experiment: the gloves are off</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Two weeks ago, I <a href="https://www.scottaaronson.com/blog/?p=5122">blogged about</a> the striking claim, by the group headed by Chaoyang Lu and Jianwei Pan at USTC in China, to have achieved quantum supremacy via BosonSampling with 50-70 detected photons.  I also did a <a href="https://asiatimes.com/2020/12/chinas-quantum-computer-a-step-not-a-leap/">four-part interview</a> on the subject with Jonathan Tennenbaum at Asia Times, and other interviews elsewhere.  None of that stopped some people, who I guess didn’t google, from writing to tell me how disappointed they were by my silence!</p>



<p>The reality, though, is that a lot has happened since the original announcement, so it’s way past time for an update.</p>



<p><strong>I. The Quest to Spoof</strong></p>



<p>Most importantly, other groups almost immediately went to work trying to refute the quantum supremacy claim, by finding some efficient classical algorithm to spoof the reported results.  It’s important to understand that this is exactly how the process is <em>supposed</em> to work: as I’ve often stressed, a quantum supremacy claim is credible only if it’s open to the community to refute and if no one can.  It’s also important to understand that, for reasons we’ll go into, there’s a decent chance that people <em>will</em> succeed in simulating the new experiment classically, although they haven’t yet.  All parties to the discussion agree that the new experiment is, far and away, the closest any BosonSampling experiment has ever gotten to the quantum supremacy regime; the hard part is to figure out if it’s already there.</p>



<p>Part of me feels guilty that, as one of reviewers on the <em>Science</em> paper—albeit, one stressed and harried by kids and covid—it’s now clear that I didn’t exercise the amount of diligence that I could have, in searching for ways to kill the new supremacy claim.  But another part of me feels that, with quantum supremacy claims, much like with proposals for new cryptographic codes, vetting <em>can’t be the responsibility of one or two reviewers</em>.  Instead, provided the claim is serious—as this one obviously is—the only thing to do is to get the paper out, so that the <em>entire community</em> can then work to knock it down.  Communication between authors and skeptics is also a hell of a lot faster when it doesn’t need to go through a journal’s editorial system.</p>



<p>Not surprisingly, one skeptic of the new quantum supremacy claim is Gil Kalai, who (despite Google’s result last year, which Gil still believes must be in error) rejects the entire possibility of quantum supremacy on quasi-metaphysical grounds.  But other skeptics are current and former members of the Google team, including Sergio Boixo and John Martinis!  And—pause to enjoy the irony—<em>Gil has effectively teamed up with the Google folks</em> on questioning the new claim.  Another central figure in the vetting effort—one from whom I’ve learned much of what I know about the relevant issues over the last week—is Dutch quantum optics professor and frequent <em>Shtetl-Optimized</em> commenter <a href="https://people.utwente.nl/j.j.renema">Jelmer Renema</a>.</p>



<p>Without further ado, why might the new experiment, impressive though it was, be efficiently simulable classically?  A central reason for concern is photon loss: as Chaoyang Lu has now explicitly confirmed (it was implicit in the paper), up to ~70% of the photons get lost on their way through the beamsplitter network, leaving only ~30% to be detected.  At least with “Fock state” BosonSampling—i.e., the original kind, the kind with single-photon inputs that Alex Arkhipov and I proposed in 2011—it seems likely to me that such a loss rate would be fatal for quantum supremacy; see for example <a href="https://arxiv.org/abs/1809.01953">this 2019 paper</a> by Renema, Shchesnovich, and Garcia-Patron.</p>



<p>Incidentally, if anything’s become clear over the last two weeks, it’s that I, the co-inventor of BosonSampling, am no longer any sort of expert on the subject’s literature!</p>



<p>Anyway, one source of uncertainty regarding the photon loss issue is that, as I said in my last post, the USTC experiment implemented a 2016 variant of BosonSampling called <a href="https://arxiv.org/abs/1612.01199">Gaussian BosonSampling (GBS)</a>—and Jelmer tells me that the computational complexity of GBS in the presence of losses hasn’t yet been analyzed in the relevant regime, though there’s been <a href="https://arxiv.org/abs/1905.12075">work aiming in that direction</a>.  A second source of uncertainty is simply that the classical simulations work in a certain limit—namely, fixing the rate of noise and then letting the numbers of photons and modes go to infinity—but any real experiment has a fixed number of photons and modes (in USTC’s case, they’re ~50 and ~100 respectively).  It wouldn’t do to reject USTC’s claim via a theoretical asymptotic argument that would equally well apply to <em>any</em> non-error-corrected quantum supremacy demonstration!</p>



<p>OK, but if an efficient classical simulation of lossy GBS experiments exists, then <em>what is it?</em>  How does it work?  It turns out that we have a plausible candidate for the answer to that, originating with a <a href="https://arxiv.org/abs/1409.3093">2014 paper</a> by Gil Kalai and Guy Kindler.  Given a beamsplitter network, Kalai and Kindler considered an infinite hierarchy of better and better approximations to the BosonSampling distribution for that network.  Roughly speaking, at the first level (k=1), one pretends that the photons are just classical distinguishable particles.  At the second level (k=2), one correctly models quantum interference involving <em>pairs</em> of photons, but none of the higher-order interference.  At the third level (k=3), one correctly models three-photon interference, and so on until k=n (where n is the total number of photons), when one has reproduced the original BosonSampling distribution.  At least when k is small, the time needed to spoof outputs at the k<sup>th</sup> level of the hierarchy should grow like n<sup>k</sup>.  As theoretical computer scientists, Kalai and Kindler didn’t care whether their hierarchy produced any physically realistic kind of noise, but later work, by Shchesnovich, Renema, and others, showed that (as it happens) it does.</p>



<p>In its original paper, the USTC team ruled out the possibility that the first, k=1 level of this hierarchy could explain its experimental results.  More recently, in response to inquiries by Sergio, Gil, Jelmer, and others, Chaoyang tells me they’ve ruled out the possibility that the k=2 level can explain their results either.  We’re now eagerly awaiting the answer for larger values of k.</p>



<p>Let me add that I owe Gil Kalai the following public <em>mea culpa</em>.  While his objections to QC have often struck me as unmotivated and weird, in the case at hand, Gil’s 2014 work with Kindler is clearly helping drive the scientific discussion forward.  In other words, at least with BosonSampling, it turns out that Gil put his finger precisely on a key issue.  He did exactly what every QC skeptic should do, and what I’ve always implored the skeptics to do.</p>



<p><strong>II. BosonSampling vs. Random Circuit Sampling: A Tale of HOG and CHOG and LXEB</strong></p>



<p>There’s a broader question: why should skeptics of a BosonSampling experiment even have to <em>think</em> about messy details like the rate of photon losses?  Why shouldn’t that be solely the <em>experimenters’</em> job?</p>



<p>To understand what I mean, consider the situation with Random Circuit Sampling, the task Google demonstrated last year with 53 qubits.  There, the Google team simply collected the output samples and fed them into a benchmark that they called “Linear Cross-Entropy” (LXEB), closely related to what Lijie Chen and I called “Heavy Output Generation” (HOG) in a <a href="https://arxiv.org/abs/1612.05903">2017 paper</a>.  With suitable normalization, an ideal quantum computer would achieve an LXEB score of 2, while classical random guessing would achieve an LXEB score of 1.  Crucially, according to a <a href="https://arxiv.org/abs/1910.12085">2019 result</a> by me and Sam Gunn, under a plausible (albeit strong) complexity assumption, <em>no</em> subexponential-time classical spoofing algorithm should be able to achieve an LXEB score that’s even slightly higher than 1.  In its experiment, Google reported an LXEB score of about 1.002, with a confidence interval <em>much</em> smaller than 0.002.  Hence: quantum supremacy (subject to our computational assumption), with no further need to know anything about the sources of noise in Google’s chip!  (More explicitly, Boixo, Smelyansky, and Neven <a href="https://arxiv.org/abs/1708.01875">did a calculation</a> in 2017 to show that the Kalai-Kindler type of spoofing strategy <em>definitely</em> isn’t going to work against RCS and Linear XEB, with no computational assumption needed.)</p>



<p>So then why couldn’t the USTC team do something analogous with BosonSampling?  Well, they tried to.   They defined a measure that they called “HOG,” although it’s different from my and Lijie Chen’s HOG, more similar to a cross-entropy.  Following Jelmer, let me call their measure CHOG, where the C could stand for Chinese, Chaoyang’s, or Changed.  They calculated the CHOG for their experimental samples, and showed that it exceeds the CHOG that you’d get from the k=1 and k=2 levels of the Kalai-Kindler hierarchy, as well as from various other spoofing strategies, thereby ruling those out as classical explanations for their results.</p>



<p>The trouble is this: <em>unlike </em>with Random Circuit Sampling and LXEB, with BosonSampling and CHOG, we <em>know</em> that there are fast classical algorithms that achieve better scores than the trivial algorithm, the algorithm that just picks samples at random.  That follows from Kalai and Kindler’s work, and it even more simply follows from a <a href="https://arxiv.org/abs/1309.7460">2013 paper</a> by me and Arkhipov, entitled “BosonSampling Is Far From Uniform.”  Worse yet, with BosonSampling, we currently have no analogue of my 2019 result with Sam Gunn: that is, a result that would tell us (under suitable complexity assumptions) the highest possible CHOG score that we expect any efficient classical algorithm to be able to get.  And since we don’t know exactly where that ceiling is, we can’t tell the experimentalists exactly what target they need to surpass in order to claim quantum supremacy.  Absent such definitive guidance from us, the experimentalists are left playing whac-a-mole against <em>this</em> possible classical spoofing strategy, and <em>that</em> one, and <em>that</em> one.</p>



<p>This is an issue that I and others were aware of for years, although the new experiment has certainly underscored it.  Had I understood <em>just how serious</em> the USTC group was about scaling up BosonSampling, and fast, I might’ve given the issue some more attention!</p>



<p><strong>III. Fock vs. Gaussian BosonSampling</strong></p>



<p>Above, I mentioned another complication in understanding the USTC experiment: namely, their reliance on Gaussian BosonSampling (GBS) rather than Fock BosonSampling (FBS), sometimes also called Aaronson-Arkhipov BosonSampling (AABS).  Since I gave this issue short shrift in my previous post, let me make up for it now.</p>



<p>In FBS, the initial state consists of either 0 or 1 photons in each input mode, like so: |1,…,1,0,…,0⟩.  We then pass the photons through our beamsplitter network, and measure the number of photons in each output mode.  The result is that the amplitude of each possible output configuration can be expressed as the <a href="https://en.wikipedia.org/wiki/Permanent_(mathematics)">permanent</a> of some n×n matrix, where n is the total number of photons.  It was interest in the permanent, which plays a central role in classical computational complexity, that led me and Arkhipov to study BosonSampling in the first place.</p>



<p>The trouble is, preparing initial states like |1,…,1,0,…,0⟩ turns out to be really hard.  No one has yet build a source that reliably outputs <em>one and only one photon</em> at exactly a specified time.  This led two experimental groups to propose an idea that, in a <a href="https://www.scottaaronson.com/blog/?p=1579">2013 post on this blog</a>, I named Scattershot BosonSampling (SBS).  In SBS, you get to use the more readily available “Spontaneous Parametric Down-Conversion” (SPDC) photon sources, which output superpositions over different numbers of photons, of the form $$\sum_{n=0}^{\infty} \alpha_n |n \rangle |n \rangle, $$ where α<sub>n</sub> decreases exponentially with n.  You then measure the left half of each entangled pair, <em>hope</em> to see exactly one photon, and are guaranteed that if you do, then there’s also exactly one photon in the right half.  Crucially, one can show that, if Fock BosonSampling is hard to simulate approximately using a classical computer, then the Scattershot kind must be as well.</p>



<p>OK, so what’s <em>Gaussian</em> BosonSampling?  It’s simply the generalization of SBS where, instead of SPDC states, our input can be an arbitrary “Gaussian state”: for those in the know, a state that’s exponential in some quadratic polynomial in the creation operators.  If there are m modes, then such a state requires ~m<sup>2</sup> independent parameters to specify.  The quantum optics people have a much easier time creating these Gaussian states than they do creating single-photon Fock states.</p>



<p>While the amplitudes in FBS are given by permanents of matrices (and thus, the probabilities by the absolute squares of permanents), the probabilities in GBS are given by a more complicated matrix function called the <a href="https://en.wikipedia.org/wiki/Hafnian">Hafnian</a>.  Roughly speaking, while the permanent counts the number of perfect matchings in a bipartite graph, the Hafnian counts the number of perfect matchings in an <em>arbitrary</em> graph.  The permanent and the Hafnian are both #P-complete.  In the USTC paper, they talk about yet another matrix function called the “Torontonian,” which was <a href="https://arxiv.org/pdf/1807.01639.pdf">invented two years ago</a>.  I gather that the Torontonian is just the modification of the Hafnian for the situation where you only have “threshold detectors” (which decide whether one or more photons are present in a given mode), rather than “number-resolving detectors” (which <em>count</em> how many photons are present).</p>



<p>If Gaussian BosonSampling includes Scattershot BosonSampling as a special case, and if Scattershot BosonSampling is at least as hard to simulate classically as the original BosonSampling, then you might hope that GBS would <em>also</em> be at least as hard to simulate classically as the original BosonSampling.  Alas, this doesn’t follow.  Why not?  Because for all we know, a <em>random</em> GBS instance might be a lot easier than a <em>random</em> SBS instance.  Just because permanents can be expressed using Hafnians, doesn’t mean that a random Hafnian is as hard as a random permanent.</p>



<p>Nevertheless, I think it’s very likely that the sort of analysis Arkhipov and I did back in 2011 could be mirrored in the Gaussian case.  I.e., instead of starting with reasonable assumptions about the distribution and hardness of random permanents, and then concluding the classical hardness of approximate BosonSampling, one would start with reasonable assumptions about the distribution and hardness of random Hafnians (or “Torontonians”), and conclude the classical hardness of approximate GBS.  But this is theoretical work that remains to be done!</p>



<p><strong>IV. Application to Molecular Vibronic Spectra?</strong></p>



<p>In 2014, Alan Aspuru-Guzik and collaborators put out a <a href="https://arxiv.org/abs/1412.8427">paper</a> that made an amazing claim: namely that, contrary to what I and others had said, BosonSampling was <em>not</em> an intrinsically useless model of computation, good only for refuting QC skeptics like Gil Kalai!  Instead, they said, a BosonSampling device (specifically, what would later be called a GBS device) could be directly applied to solve a practical problem in quantum chemistry.  This is the computation of “molecular vibronic spectra,” also known as “Franck-Condon profiles,” whatever those are.</p>



<p>I never understood nearly enough about chemistry to evaluate this striking proposal, but I was always a bit skeptical of it, for the following reason.  Nothing in the proposal seemed to take seriously that BosonSampling is a <em>sampling</em> task!  A chemist would typically have some <em>specific numbers</em> that she wants to estimate, of which these “vibronic spectra” seemed to be an example.  But while it’s often convenient to estimate physical quantities via Monte Carlo sampling over simulated observations of the physical system you care about, that’s not the <em>only</em> way to estimate physical quantities!  And worryingly, in all the other examples we’d seen where BosonSampling could be used to estimate a number, the same number could <em>also</em> be estimated using one of several polynomial-time classical algorithms invented by Leonid Gurvits.  So why should vibronic spectra be an exception?</p>



<p>After an email exchange with Alex Arkhipov, Juan Miguel Arrazola, Leonardo Novo, and Raul Garcia-Patron, I believe we finally got to the bottom of it, and the answer is: vibronic spectra are <em>not</em> an exception.</p>



<p>In terms of BosonSampling, the vibronic spectra task is simply to estimate the probability histogram of some weighted sum like $$ w_1 s_1 + \cdots + w_ m s_m, $$ where w<sub>1</sub>,…,w<sub>m</sub> are fixed real numbers, and (s<sub>1</sub>,…,s<sub>m</sub>) is a possible outcome of the BosonSampling experiment, s<sub>i</sub> representing the number of photons observed in mode i.  Alas, while it takes some work, it turns out that Gurvits’s classical algorithms can be adapted to estimate these histograms.  Granted, running the actual BosonSampling experiment would provide <em>slightly</em> more detailed information—namely, some exact sampled values of $$ w_1 s_1 + \cdots + w_ m s_m, $$ rather than merely additive approximations to the values—but since we’d still need to sort those sampled values into coarse “bins” in order to compute a histogram, it’s not clear why that additional precision would ever be of chemical interest.</p>



<p>This is a pity, since if the vibronic spectra application <em>had</em> beaten what was doable classically, then it would’ve provided not merely a first practical use for BosonSampling, but also a lovely way to <em>verify</em> that a BosonSampling device was working as intended.</p>



<p><strong>V. Application to Finding Dense Subgraphs?</strong></p>



<p>A different potential application of Gaussian BosonSampling, first suggested by the Toronto-based startup <a href="https://www.xanadu.ai/">Xanadu</a>, is <a href="https://arxiv.org/abs/1803.10730">finding dense subgraphs in a graph</a>.  (Or at least, providing an initial seed to classical optimization methods that search for dense subgraphs.)</p>



<p>This is an NP-hard problem, so to say that I was skeptical of the proposal would be a gross understatement.  Nevertheless, it turns out that there <em>is</em> a striking observation by the Xanadu team at the core of their proposal: namely that, given a graph G and a positive even integer k, a GBS device can be used to sample a random subgraph of G of size k, with probability <em>proportional to the square of the number of perfect matchings in that subgraph</em>.  Cool, right?  And potentially even useful, especially if the number of perfect matchings could serve as a rough indicator of the subgraph’s density!  Alas, Xanadu’s Juan Miguel Arrazola himself recently told me that there’s a cubic-time classical algorithm for the same sampling task, so that the possible quantum speedup that one could get from GBS in this way is at most polynomial.  The search for a useful application of BosonSampling continues!</p>



<p></p><hr /><p></p>



<p>And that’s all for now!  I’m grateful to all the colleagues I talked to over the last couple weeks, including Alex Arkhipov, Juan Miguel Arrazola, Sergio Boixo, Raul Garcia-Patron, Leonid Gurvits, Gil Kalai, Chaoyang Lu, John Martinis, and Jelmer Renema, while obviously taking sole responsibility for any errors in the above.  I look forward to a spirited discussion in the comments, and of course I’ll post updates as I learn more!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5159"><span class="datestr">at December 16, 2020 08:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/12/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/12/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.ams.org/journals/notices/202011/rnoti-p1692.pdf">3d-printed models of the chaotic attractors from dynamical systems</a> (<a href="https://mathstodon.xyz/@11011110/105309562849621245">\(\mathbb{M}\)</a>). Stephen K. Lucas, Evelyn Sander, and Laura Taalman in the cover article of the latest <em>Notices</em>.</p>
  </li>
  <li>
    <p><a href="https://threadreaderapp.com/thread/1333670741590503425.html">Complete classification of tetrahedra whose angles are all rational multiples of \(\pi\)</a> (<a href="https://mathstodon.xyz/@11011110/105311921075649463">\(\mathbb{M}\)</a>, <a href="https://aperiodical.com/2020/12/aperiodical-news-roundup-november-2020/">via</a>). The original paper is “<a href="https://arxiv.org/abs/2011.14232">Space vectors forming rational angles</a>”, by Kiran S. Kedlaya, Alexander Kolpakov, Bjorn Poonen, and Michael Rubinstein.</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/faculty-and-departments/classroom-capsules-and-notes/geometry-strikes-again">Geometry strikes again</a> (<a href="https://mathstodon.xyz/@11011110/105320692550081128">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/189571/slaps-roof-this-bad-boy-can-fit-so-many-fucking-polyhedra-in-it">via</a>), Branko Grünbaum, <em>Math. Mag.</em> 1985. Somehow I don’t think I’d encountered this short paper before but it’s filled with many examples of horribly-drawn mathematics, one in the logo of the MAA. Worth reading as a warning for what not to do. Also for clear instructions on how to draw regular icosahedra correctly.</p>
  </li>
  <li>
    <p><a href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru">Ethical issues in large-corpus natural language processing, or what’s behind the research that got Timnit Gebru kicked out of Google</a> (<a href="https://mathstodon.xyz/@11011110/105326128589157740">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=25311402">via</a>).</p>
  </li>
  <li>
    <p><a href="https://jack.wrenn.fyi/blog/brown-location-surveillance/">How one university (Brown) tracks the physical locations of its students to ensure compliance with its pandemic safety policies</a> (<a href="https://mathstodon.xyz/@11011110/105330487336321470">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=25319392">via</a>). Most of it is pretty obvious: if you use a campus keycard or connect to a campus wireless network, they know you’re on campus.</p>
  </li>
  <li>
    <p><a href="https://www.forbes.com/sites/madhukarpai/2020/11/30/how-prestige-journals-remain-elite-exclusive-and-exclusionary/?sh=1c14baef4d48">How prestige journals remain elite, exclusive and exclusionary</a> (<a href="https://mathstodon.xyz/@11011110/105345990074471874">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/12/05/weekend-reads-google-ai-researcher-fired-after-being-asked-to-retract-paper-journal-accused-of-stonewalling-on-paper-used-to-justify-human-rights-violations-reflecting-on-a-covid-19-retraction/">via</a>). Nature is charging up to €9,500 per paper in open-access fees, as much as some scientists in third-world countries earn in a year, making open-access publication inaccessible to people from low- and middle-income countries.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> (<a href="https://mathstodon.xyz/@11011110/105348855359182047">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematician-solves-centuries-old-grazing-goat-problem-exactly-20201209/">Ingo Ullisch and the goats</a> (<a href="https://mathstodon.xyz/@btcprox/105354303333088090">\(\mathbb{M}\)</a>). A new solution to the problem of how to bisect the area of a circle by another circular arc centered on the first circle. But, given that it involves integrals and trig, is it really fair to call it “more exact” than the previous solution? I don’t think we even know whether the solution radius is transcendental (or transcendental over \(\pi\)).</p>
  </li>
  <li>
    <p><a href="http://www.anilaagha.com/sculpturelooksee">Anila Quayyum Agha’s openwork sculptures cast intricate tessellated shadows on the surrounding surfaces</a> (<a href="https://mathstodon.xyz/@11011110/105360602658987905">\(\mathbb{M}\)</a>). See also <a href="https://en.wikipedia.org/wiki/Anila_Quayyum_Agha">her Wikipedia article</a> and two stories on her work, “<a href="http://canjournal.org/2019/11/between-light-and-shadow-at-the-toledo-museum-of-art/">Between light and shadow at the Toledo Museum of Art</a>” and “<a href="https://news.artnet.com/art-world/anila-quayyum-agha-interview-741371">Anila Quayyum Agha on drawing inspiration from darkness</a>”.</p>
  </li>
  <li>
    <p>Pat Morin notes that it’s “good to see that the pandemic hasn’t affected every aspect of our lives”: <a href="https://mathstodon.xyz/@patmorin/105362925062596140">the registration fees for the online SODA conference are still way too high</a>.</p>
  </li>
  <li>
    <p><a href="https://statmodeling.stat.columbia.edu/2020/12/10/ieees-refusal-to-issue-corrections/">IEEE has no mechanism to publish corrections or errata to conference proceedings papers</a> (<a href="https://mathstodon.xyz/@11011110/105369247233466112">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/12/12/weekend-reads-p-hacking-the-us-election-an-apparently-fake-author-sinks-a-stock-sued-for-using-a-research-tool/">via</a>), violating IEEE’s own code of ethics requiring authors “to acknowledge and correct errors”:  Probably many other conference proceedings have similar issues.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/12/13/shallow-trees-with-heavy-leaves/">Shallow trees with heavy leaves</a> (<a href="https://mathstodon.xyz/@11011110/105375238890363766">\(\mathbb{M}\)</a>). On “the general strategy of searching much fewer positions and expending more effort on each position”, and its application in using SAT solvers to find new spaceships in cellular automata.</p>
  </li>
  <li>
    <p><a href="https://www.flyingcoloursmaths.co.uk/dictionary-of-mathematical-eponymy-the-xuong-tree">Dictionary of mathematical eponymy: The Xuong tree</a> (<a href="https://mathstodon.xyz/@11011110/105382771623486905">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Xuong_tree">see also</a>), a special kind of spanning tree in graphs, used to embed them into surfaces with as high a genus as possible.</p>
  </li>
  <li>
    <p><a href="https://journals.carleton.ca/jocg/index.php/jocg/article/view/461">An explicit PL-embedding of the square flat torus into \(\mathbb{E}^3\)</a> (<a href="https://mathstodon.xyz/@11011110/105385674493868301">\(\mathbb{M}\)</a>). The square torus is like the old Asteroids arcade game: a Euclidean square with boundary conditions that wrap around so if you move off one edge you re-enter at the corresponding point of the opposite edge. In 4d, it has a nice representation as the set \(\{(a,b,c,d)\mid a^2+b^2=c^2+d^2=1\}\), the Cartesian product of two circles. The <a href="https://en.wikipedia.org/wiki/Nash_embedding_theorem">Nash embedding theorem</a> gives it fractal embeddings in 3d, but Tanessi Quintanar finds it as a bona fide polyhedron.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/12/15/linkage.html"><span class="datestr">at December 15, 2020 09:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/15/postdoc-at-microsoft-research-apply-by-january-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/15/postdoc-at-microsoft-research-apply-by-january-15-2021/">Postdoc at Microsoft Research (apply by January 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms group at Microsoft Research Redmond seeks exceptional researchers who are passionate about advancing the state of the art in theoretical computer science and having impact on the industry. Applicants must have a demonstrated ability for independent research and a strong academic publication record in theoretical computer science.</p>
<p>Website: <a href="https://www.microsoft.com/en-us/research/group/algorithms-redmond/">https://www.microsoft.com/en-us/research/group/algorithms-redmond/</a><br />
Email: yekhanin@microsoft.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/15/postdoc-at-microsoft-research-apply-by-january-15-2021/"><span class="datestr">at December 15, 2020 08:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/15/researcher-at-microsoft-research-apply-by-january-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/15/researcher-at-microsoft-research-apply-by-january-31-2021/">Researcher at Microsoft Research (apply by January 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms group at Microsoft Research Redmond seeks exceptional researchers who are passionate about advancing the state of the art in theoretical computer science and having impact on the industry. Applicants must have a demonstrated ability for independent research and a strong academic publication record in theoretical computer science.</p>
<p>Website: <a href="https://www.microsoft.com/en-us/research/group/algorithms-redmond/">https://www.microsoft.com/en-us/research/group/algorithms-redmond/</a><br />
Email: yekhanin@microsoft.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/15/researcher-at-microsoft-research-apply-by-january-31-2021/"><span class="datestr">at December 15, 2020 08:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/15/postdoc-at-university-of-michigan-apply-by-january-10-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/15/postdoc-at-university-of-michigan-apply-by-january-10-2021/">Postdoc at University of Michigan (apply by January 10, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Theory Group at the University of Michigan, Ann Arbor invites applications for a postdoctoral position beginning September, 2021. The position will have an initial appointment for one year, but may be extended depending on circumstances.</p>
<p>Applicants should be recent PhDs with interests that align well with our ongoing research.</p>
<p>Please email a CV, research statement, and names of references.</p>
<p>Website: <a href="https://theory.engin.umich.edu">https://theory.engin.umich.edu</a><br />
Email: pettie@umich.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/15/postdoc-at-university-of-michigan-apply-by-january-10-2021/"><span class="datestr">at December 15, 2020 06:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4446">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/12/15/kim-ki-duk/">Kim Ki-duk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>(Last call for the <a href="https://lucatrevisan.wordpress.com/2020/11/22/post-doc-opportunities-in-milan/">postdoc positions I advertised earlier</a>. Application deadline is tomorrow morning Italian time, tonight American time.)<br /><br />Last weekend I was saddened to hear of the death of Korean filmmaker Kim Ki-duk. Kim was in Latvia, preparing to shoot a movie there. He disappeared about a week ago, and it then transpired that he had been hospitalized with Covid symptoms, and died last Friday from Covid complications.</p>



<p>In the early 2000s, living in the San Francisco Bay Area offered me several opportunities to discover cinema that was new to me. There were the several film festivals held each year in San Francisco, the wonderful retrospectives at the Castro Theater. There was also Netflix, that at the time operated by renting DVDs by mail (I am feeling like grandpa Simpson telling stories here) and whose catalog was basically every movie ever released in the US. I discovered the work of Hirokazu Kore-eda and Wong Kar-Wai, whose movies are now among my favorites, and I watched some challenging but rewarding movies by the likes of Tsai Ming-Lian and Hou Hsiao-Hsien. So, when it came out, I watched “3-iron” by Kim Ki-duk, an amazing movie on the theme of the poor being invisible to the rich, but taken to extreme and fantastical places.</p>



<p>I was reminded of that time during our first lockdown last Spring. After having watched a lot of TV series, I felt like I had to do something a bit more soul-nourishing before the lockdown ended. So I resolved to watch Tarkovsky’s “Solaris” and “Stalker”, which I had never seen. Compared with movies I had watched 15+ years ago like those of Tsai Ming-Lian and Hou Hsiao-Hsien, or like “3-iron”, “Solaris” was like a Michael Bay movie full of explosions and car chases, but I still had a really hard time getting into it. I ended up watching it over five or six sittings, across a couple of weeks (the lockdown actually ended before I was done watching it). I still haven’t seen “Stalker”. I suppose that twelve years of smartphone usage accounts for the difference in attention span.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/12/15/kim-ki-duk/"><span class="datestr">at December 15, 2020 10:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/15/tenure-track-faculty-at-bar-ilan-university-apply-by-february-2-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/15/tenure-track-faculty-at-bar-ilan-university-apply-by-february-2-2021/">Tenure Track Faculty at Bar Ilan University (apply by February 2, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We are recruiting excellent tenure-track faculty for our Industrial and Information Systems Engineering track (and the respective IISE degree—the only one in Israel that formally meets the requirements for both IE and IS disciplines with a strong DS flavor). Requirements: A PhD in Information Systems, Computer Science, and related fields, post-doctoral training.<br />
Want to hear more? Let’s talk.</p>
<p>Website: <a href="https://engineering.biu.ac.il/en/node/8797">https://engineering.biu.ac.il/en/node/8797</a><br />
Email: izack.cohen@biu.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/15/tenure-track-faculty-at-bar-ilan-university-apply-by-february-2-2021/"><span class="datestr">at December 15, 2020 09:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/12/15/postdoc-senior-researcher-at-hamburg-university-of-technology-apply-by-january-11-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/12/15/postdoc-senior-researcher-at-hamburg-university-of-technology-apply-by-january-11-2021/">Postdoc/Senior Researcher at Hamburg University of Technology (apply by January 11, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms and combinatorial optimization with applications in the natural sciences. Data-driven algorithms and algorithm engineering are supported through world-class data facilities at DESY/XFEL.</p>
<p>Website: <a href="https://www.tuhh.de/algo/jobs.html">https://www.tuhh.de/algo/jobs.html</a><br />
Email: algo@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/12/15/postdoc-senior-researcher-at-hamburg-university-of-technology-apply-by-january-11-2021/"><span class="datestr">at December 15, 2020 08:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5171">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5171">Beth Harmon and the Inner World of Squares</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The other day Dana and I finished watching <em>The Queen’s Gambit</em>, Netflix’s fictional saga of an orphaned girl in the 1960’s, Beth Harmon, who breaks into competitive chess and destroys one opponent after the next in her quest to become the world champion, while confronting her inner demons and addictions.</p>



<p>The show is every bit as astoundingly good as everyone says it is, and I might be able to articulate why.  It’s because, perhaps surprisingly given the description, this is a story where <em>chess actually matters</em>—and indeed, the fact that chess matters so deeply to Beth and most of the other characters is central to the narrative.  (As in two pivotal scenes where Beth has sex with a male player, and then either she or he goes right back to working on chess.)</p>



<p>I’ve watched a lot of TV shows and movies, supposedly about scientists, where the science was just an interchangeable backdrop to what the writers clearly regarded as a more important story.  (As one random example, the drama <a href="https://en.wikipedia.org/wiki/Numbers_(TV_series)">NUMB3RS</a>, supposedly about an FBI mathematician, where “math” could’ve been swapped out for “mystical crime-fighting intuition” with barely any change.)</p>



<p>It’s true that a fictional work about scientists shouldn’t try to be a science documentary, just like <em>Queen’s Gambit</em> doesn’t try to be a chess documentary.  But if you’re telling a story about characters who are obsessed with topic X, then you need to <em>make their obsession plausible</em>, make the entire story hinge on it, and even make the audience vicariously feel the same obsession.</p>



<p>This is precisely what <em>Queen’s Gambit</em> does for chess.  It’s a chess drama where the characters are <em>constantly</em> talking about chess, thinking about chess, and playing chess—and that actually succeeds in making that riveting.  (Even if most of the audience can’t follow what’s happening on the board, it turns out that it doesn’t matter, since you can simply convey the drama through the characters’ faces and the reactions of those around them.)</p>



<p>Granted, a few aspects of competitive chess in the series stood out as jarringly unrealistic even to a novice like me: for example, the almost complete lack of draws.  But as for the board positions—well, apparently Kasparov was a consultant, and he helped meticulously design each one to reflect the characters’ skill levels and what was happening in the plot.</p>



<p>While the premise sounds like a feminist wish-fulfillment fantasy—orphan girl faces hundreds of intimidating white men in the sexist 1960s, orphan girl beats them all at their own game with style and aplomb—this is not at all a MeToo story, or a story about male crudity or predation.  It’s after bigger fish than that.  The series, you might say, conforms to all the external requirements of modern woke ideology, yet the actual plot subverts the tenets of that ideology, or maybe just ignores them, in its pursuit of more timeless themes.</p>



<p>At least once Beth Harmon enters the professional chess world, the central challenges she needs to overcome are internal and mental—just like they’re supposed to be in chess.  It’s not the Man or the Patriarchy or any other external power (besides, of course, skilled opponents) holding her down.  Again and again, the top male players are portrayed not as sexist brutes but as gracious, deferential, and even awestruck by Beth’s genius after she’s humiliated them on the chessboard.  And much of the story is about how those vanquished opponents then turn around and try to help Beth, and about how she needs to learn to accept their help in order to evolve as a player and a human being.</p>



<p>There’s also that, after defeating male player after male player, Beth sleeps with them, or at least wants to.  I confess that, as a teenager, I would’ve found that unlikely and astonishing.  I would’ve said: obviously, the only guys who’d even have a <em>chance</em> to prove themselves worthy of the affection of such a brilliant and unique woman would be those who could beat her at chess.  Anyone else would just be dirt between her toes.  In the series, though, each male player propositions Beth only <em>after</em> she’s soundly annihilated him.  And she’s never once shown refusing.</p>



<p>Obviously, I’m no Beth Harmon; I’ll never be close in my field to what she is in hers.  Equally obviously, I grew up in a loving family, not an orphanage.  Still, I was what some people would call a “child prodigy,” what with the finishing my PhD at 22 and whatnot, so naturally that colored my reaction to the show.</p>



<p>There’s a pattern that goes like this: you’re obsessively interested, from your first childhood exposure, in something that most people aren’t.  Once you learn what the something is, it’s evident to you that your life’s vocation couldn’t possibly be anything else, unless some external force prevents you.  Alas, in order to pursue the something, you first need to get past bullies and bureaucrats, who dismiss you as a nobody, put barriers in your way, despise whatever you represent to them.  After a few years, though, the bullies can no longer stop you: you’re finally among peers or superiors in your chosen field, regularly chatting with them on college campuses or at conferences in swanky hotels, and the main limiting factor is just the one between your ears. </p>



<p>You feel intense rivalries with your new colleagues, of course, you desperately want to excel them, but the fact that they’re all on the same obsessive quest as you means you can never actually hate them, as you did the bureaucrats or the bullies.  There’s too much of you in your competitors, and of them in you.</p>



<p>As you pursue your calling, you feel yourself torn in the following way.  On the one hand, you feel close to a moral obligation to humanity not to throw away whatever “gift” you were “given” (what loaded terms), to take the calling as far as it will go.  On the other hand, you also want the same things other people want, like friendship, validation, and of course sex.</p>



<p>In such a case, two paths naturally beckon.  The first is that of asceticism: making a virtue of eschewing all temporal attachments, romance or even friendship, in order to devote yourself entirely to the calling.  The second is that of renouncing the calling, pretending it never existed, in order to fit in and have a normal life.  Your fundamental challenge is to figure out a third path, to plug yourself into a community where the relentless pursuit of the unusual vocation and the friendship and the sex can all complement each other rather than being at odds.</p>



<p>It would be an understatement to say that I have some familiarity with this narrative arc.</p>



<p>I’m aware, of course, of the irony, that I can identify with so many contours of Beth Harmon’s journey—I, Scott Aaronson, who half the Internet denounced six years ago as a misogynist monster who denies the personhood and interiority of women.  In that life-alteringly cruel slur, there was a microscopic grain of truth, and it’s this: I’m not talented at imagining myself into the situations of people different from me.  It’s never been my strong suit.  I might <em>like</em> and <em>admire</em> people different from me, I might sympathize with their struggles and wish them every happiness, but I still don’t know what they’re thinking until they tell me.  And even then, I don’t fully understand it.</p>



<p>As one small but illustrative example, I have no intuitive understanding—zero—of what it’s like to be romantically attracted to men, or what any man could do or say or look like that could possibly be attractive to women.  If you have such an understanding, then imagine yourself sighted and me blind.  Intellectually, I might know that confidence or height or deep brown eyes or brooding artistry are supposed to be attractive in human males, but only because I’m told.  As far as my intuition is concerned, pretty much all men are equally hairy, smelly, and gross, a large fraction of women are alluring and beautiful and angelic, and both of those are just objective features of reality that no one could possibly see otherwise.</p>



<p>Thus, whenever I read or watch fiction starring a female protagonist who dates men, it’s very easy for me to imagine that protagonist judging me, enumerating my faults, and rejecting me, and very hard for me to do what I’m supposed to do, which is to <em>put myself into her shoes</em>.  I could watch a thousand female protagonists kiss a thousand guys onscreen, or wake up in bed next to them, and the thousandth-and-first time I’d still be equally mystified about what she saw in such a sweaty oaf and why she didn’t run from him screaming, and I’d snap out of vicariously identifying with her.  (Understanding gay men of course presents similar difficulties; understanding lesbians is comparatively easy.)</p>



<p>It’s possible to overcome this, but it takes an extraordinary female protagonist, brought to life by an extraordinary writer.  Off the top of my head, I can think of only a few.  There were Renee Feuer and Eva Mueller, the cerebral protagonists of Rebecca Newberger Goldstein’s <a href="https://www.amazon.com/Mind-Body-Problem-Contemporary-American-Fiction/dp/0140172459"><em>The Mind-Body Problem</em></a> and <a href="https://www.amazon.com/Summer-Passion-Woman-Vintage-Contemporaries/dp/0679728236"><em>The Late Summer Passion of a Woman of Mind</em></a>.  Maybe Ellie Arroway from Carl Sagan’s <em>Contact</em>.  And then there’s Beth Harmon.  With characters like these, I can briefly enter a space where their crushes on men seem no weirder or more inexplicable to me than my own teenage crushes … just, you know, inverted.  Sex is in any case secondary to the character’s primary urge to discover timeless truths, an urge that I fully understand because I’ve shared it.</p>



<p>Granted, the timeless truths of chess, an arbitrary and invented game, are less profound than those of quantum gravity or the P vs. NP problem, but the psychology is much the same, and <em>The Queen’s Gambit</em> does a good job of showing that.  To understand the characters of this series is to understand why they could be happier to lose an interesting game than to win a boring one.  And I could appreciate that, even if I was by no means the strongest player at my elementary school’s chess club, and the handicap with which I can beat my 7-year-old daughter is steadily decreasing.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5171"><span class="datestr">at December 14, 2020 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/187">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/187">TR20-187 |  If VNP is hard, then so are equations for it | 

	Mrinal Kumar, 

	C Ramya, 

	Ramprasad Saptharishi, 

	Anamay Tengse</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Assuming that the Permanent polynomial requires algebraic circuits of exponential size, we show that the class VNP *does not* have efficiently computable equations. In other words, any nonzero polynomial that vanishes on the coefficient vectors of all polynomials in the class VNP requires algebraic circuits of super-polynomial size.

In a recent work of Chatterjee and the authors (FOCS 2020), it was shown that the subclasses of VP and VNP consisting of polynomials with bounded integer coefficients *do*  have equations with small algebraic circuits. Their work left open the possibility that these results could perhaps be extended to all of VP or VNP. The results in this paper show that assuming the hardness of Permanent, at least for VNP, allowing polynomials with large coefficients does indeed incur a significant blow up in the circuit complexity of equations.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/187"><span class="datestr">at December 13, 2020 02:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/186">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/186">TR20-186 |  Shrinkage of Decision Lists and DNF Formulas | 

	Benjamin Rossman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We establish nearly tight bounds on the expected shrinkage of decision lists and DNF formulas under the $p$-random restriction $\mathbf R_p$ for all values of $p \in [0,1]$. For a function $f$ with domain $\{0,1\}^n$, let $\mathrm{DL}(f)$ denote the minimum size of a decision list that computes $f$. We show that
\[
  \mathbb E[\ \mathrm{DL}(f|\mathbf R_p)\ ] \le 
  \mathrm{DL}(f)^{\log_{2/(1-p)}(\frac{1+p}{1-p})}.
\]
For example, this bound is $\sqrt{\mathrm{DL}(f)}$ when $p = \sqrt{5}-2 \approx 0.24$. For Boolean functions $f$, we obtain the same shrinkage bound with respect to DNF formula size plus $1$ (i.e.,\ replacing $\mathrm{DL}(\cdot)$ with $\mathrm{DNF}(\cdot)+1$ on both sides of the inequality).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/186"><span class="datestr">at December 13, 2020 07:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/185">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/185">TR20-185 |  Quantum learning algorithms imply circuit lower bounds | 

	Tom Gur, 

	Alex Grilo, 

	Igor Oliveira, 

	Srinivasan Arunachalam, 

	Aarthi Sundaram</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We establish the first general connection between the design of quantum algorithms and  circuit lower bounds. Specifically, let $\mathrm{C}$ be a class of polynomial-size concepts, and suppose that $\mathrm{C}$ can be PAC-learned with membership queries under the uniform distribution with error $1/2 - \gamma$ by a time $T$ quantum algorithm. We prove that if $\gamma^2 \cdot T \ll 2^n/n$, then $\mathrm{BQE} \not\subseteq \mathrm{C}$, where $\mathrm{BQE} = \mathrm{BQTIME}[2^{O(n)}]$ is an exponential-time analogue of $\mathrm{BQP}$. This result is optimal in both $\gamma$ and $T$, since it is not hard to learn any class $\mathcal{C}$ of functions in (classical) time $T = 2^n$  (with no error), or in quantum time $T = \mathrm{poly}(n)$ with error at most $1/2 - \Omega(2^{-n/2})$ via Fourier sampling. In other words, even a marginal improvement on these generic learning algorithms would lead to major consequences in complexity theory. 

Our proof builds on several works in learning theory, pseudorandomness, and computational complexity, and crucially, on a connection between non-trivial classical learning algorithms and circuit lower bounds established by Oliveira and Santhanam (CCC 2017). Extending their approach to quantum learning algorithms turns out to create significant challenges. To achieve that, we show among other results how pseudorandom generators imply learning-to-lower-bound connections in a generic fashion, construct the first conditional pseudorandom generator secure against uniform quantum computations, and extend the local list-decoding algorithm of Impagliazzo, Jaiswal, Kabanets and Wigderson (SICOMP 2010) to quantum circuits via a delicate analysis. We believe that these contributions are of independent interest and might find other applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/185"><span class="datestr">at December 13, 2020 07:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4105007709227774779">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/12/quarterly-theory-workshop-any-lessons.html">Quarterly Th. Wksp `at' Northwestern, and thoughts inspired by it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> On the <a href="https://theory.cs.northwestern.edu/events/">Northwestern CS Theory Group</a> there is a set of Quarterly Theory Workshops. There is one coming up on Dec 17-18, 2020, called the <a href="https://theory.cs.northwestern.edu/events/2020-junior-theorists-workshop/">Junior Theorists Workshop</a>. Take a look and possibly go to it! Because it is virtual you do not need to plan that much ahead- though they do want you to register. </p><p>1) I notice broadly two kinds of meetings:</p><p><br />Based on WHO will be there, e.g., JUNIOR theorists</p><p><br />Based on TOPIC: e.g., there was a meeting on ALGORITHMIC FAIRNESS.</p><p><br /></p><p>2) These types of meetings (NY Theory day is another) are, I believe, intended to be for people that are local (more on that later). But because the meeting will be on zoom, geography is no longer an impediment for either the attendees or the speakers. </p><p><br /></p><p>3) Before covid there was some talk of  `Gee, flying off to STOC, FOCS, other conferences is bad for the environment, what to do about that?'. With that in mind, here is a history which might not be true but makes a point:</p><p><i>In an earlier era FOCS/STOC were attended by mostly Americans and ICALP was attended by mostly Europeans.  I do not think there was any policy of discrimination on admissions, but it was more like Americans just did not submit to ICALP as much, nor Europeans  to FOCS/STOC. But over time when these conferences got to be considered prestigious people would routinely submit to either one depending on timing. If your paper was done in time for Conf X deadline, that's where you submit. If it does not get in then  you edit it some, perhaps add some new results, and submit to Conf Y. </i></p><p>So one solution to the air-travel-global-Warming   problem of conferences is go back to a time (which may not have ever existed) where it was just understood that you go to LOCAL conferences. Math does this, but it helps that their regional conferences are not  prestigious. But even they don't quite get it right: the joint AMS-MAA meeting alternates coasts. One year when it was in California they invited me to be a guest speaker (on the Muffin Problem). The following year it was in Baltimore. Note that I live in Maryland, so perhaps they should have waited a year. </p><p>How to encourage people to submit locally. I DO NOT want to have a rule or a diff standard for those who don't. As such... I have no idea. </p><p><br /></p><p>4)  Are virtual conferences a good idea? This is a hot topic now so I won't dwell on it, just to say that there is still something about being there IN PERSON, meeting people, serendipity that makes live confs better.</p><p>However, to have it at the same time be virtual and recorded will be VERY HELPFL to those who can't afford to go for whatever reason. </p><p>And of course there is the whole issue of if we should have prestigious conferences, which I won't get into now. Or later. That's more Lance's issue (he thinks no). </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/12/quarterly-theory-workshop-any-lessons.html"><span class="datestr">at December 12, 2020 08:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17885">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/12/12/is-the-end-near/">Is The End Near?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Queen’s Gambit and more</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p>
Kenneth Regan is my partner here at GLL and a dear friend. </p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/12/12/is-the-end-near/ken-2/" rel="attachment wp-att-17887"><img width="140" alt="" src="https://rjlipton.files.wordpress.com/2020/12/ken.jpg?w=140&amp;h=180" class="alignright  wp-image-17887" height="180" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>He is a longtime faculty member in computer science at the University of Buffalo, a longtime fan of the Buffalo NFL football team, and a longtime international master of chess. He is also one of the world experts on detecting cheating at chess. Players cheat by making their moves based on computer programs instead of relying on their brains. The key issue is: computer programs play chess better than any human. </p>
<p>
Today I thought we might send Ken a tip-of-the-hat for being in the Wall Street Journal. </p>
<p>
That is he is featured in a WSJ article called <a href="https://www.wsj.com/articles/the-real-queens-gambit-catching-chess-cheaters-11607439491">Queen’s Gambit</a> about chess cheating. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/12/12/is-the-end-near/tip3/" rel="attachment wp-att-17888"><img width="200" alt="" class="aligncenter wp-image-17888" src="https://rjlipton.files.wordpress.com/2020/12/tip3.png?w=200" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
This is not his first mention in the WSJ—see <a href="https://www.wsj.com/articles/high-tech-chess-cheaters-charge-ahead-1444404660">here</a>. See <a href="http://www.buffalo.edu/news/releases/2020/12/010.html">this</a> for additional comments by Cory Nealon, who says: </p>
<blockquote><p><b> </b> <em> The game of chess is no stranger to cheating. But since the pandemic hit and tournaments have moved online, the services of chess detectives have never been more in demand. </em>
</p></blockquote>
<p>Ken is perhaps the world’s most feared chess detective. </p>
<p>
</p><p></p><h2> Beyond Chess </h2><p></p>
<p></p><p>
Chess is important, but it is a game. Yet chess cheating may be an early warning of things to come. Might robotic methods in other areas soon start to dominate humans? Might chess be one of first areas to be lost to robots?</p>
<p>
A 1970 movie that comes to mind is <a href="https://en.wikipedia.org/wiki/Colossus:_The_Forbin_Project">The Forbin Project</a>. The plot is what happens when a computer system, Colossus, takes over the world. Its creator is Dr. Forbin, has the following exchange with it at the end of the movie: </p>
<blockquote><p><b> </b> <em> Colossus later tells Forbin that the world, now freed from war, will create a new human millennium that will raise mankind to new heights, but only under its absolute rule. Colossus informs Forbin that “freedom is an illusion” and that “in time you will come to regard me not only with respect and awe, but with love”. Forbin responds, “Never!” </em>
</p></blockquote>
<p></p><p>
I think there is a reason to worry about computers like Colossus wiping us out. Perhaps not taking absolute control, but nevertheless making us second class citizens. I wonder if the following areas could be potential ones where we lose out to computers:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Protein folding?</i> They already are doing quite <a href="https://rjlipton.wordpress.com/2020/12/02/too-long-didnt-read/">well</a>.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Play calling in the NFL?</i> I made this one up, but I do wonder if using all the statistics available programs could do a better job than players and coaches. They probably could already do better than the Jets defensive coach, who was recently <a href="https://sports.yahoo.com/report-jets-fire-defensive-coordinator-gregg-williams-after-legendarily-awful-loss-to-raiders-161424493.html">fired</a>. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Solving Open Math Problems?</i> We recently talked about <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/#comment-114892">Lean</a> the proof checking system. But what if Lean could start to conjecture, to suggest directions, and give a proof outline? This would be, I believe, much more disruptive than the ability to check proofs.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are some areas that you think could be next? That could be taken over by robots?<br />
Moreover, as more areas are dominated by robots, we will need more Ken’s to stop cheaters. </p>
<p>
Thanks again Ken. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/12/12/is-the-end-near/"><span class="datestr">at December 12, 2020 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20470">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/12/12/open-problem-session-of-huji-combsem-problem-3-ehud-friedgut-independent-sets-and-lionel-levins-infamous-hat-problem/">Open problem session of HUJI-COMBSEM: Problem #3, Ehud Friedgut – Independent sets and Lionel Levine’s infamous hat problem.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Here are the two problems presented by Ehud Friedgut. The first arose by Friedgut, Kindler, and me in the context of studying  Lionel Levine’s infamous hat problem. The second<em> is</em> Lionel Levine’s infamous hat problem.</p>
<h2><a href="https://gilkalai.files.wordpress.com/2020/12/ef3h.jpeg"><img width="169" alt="" src="https://gilkalai.files.wordpress.com/2020/12/ef3h.jpeg?w=169&amp;h=300" class="size-medium wp-image-20618 aligncenter" height="300" /></a></h2>
<p style="text-align: center;"><span style="color: #ff0000;">Ehud Friedgut with a few hats</span></p>
<p>Earlier problems in this series: #1 <a href="https://gilkalai.wordpress.com/2020/11/25/open-problem-session-of-huji-combsem-problem-1-nati-linial-turan-type-theorems-for-simplicial-complexes/">Nati Linial</a>; #2<a href="https://gilkalai.wordpress.com/2020/12/10/open-problem-session-of-huji-combsem-problem-2-chaya-keller-the-krasnoselskii-number/"> Chaya Keller</a>; <a href="https://huji.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1ac330a2-d6b7-4b86-9bac-ac7500d14205">videotaped session</a>.</p>
<h2>Problem 1: Independence numbers for random induced subgraphs</h2>
<p>For a graph <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> let <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha (G)" class="latex" title="\alpha (G)" /> be its independence number, namely the maximum  size of an independent set of vertices in <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" />. Now consider a random subset <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W" class="latex" title="W" /> of the set <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="V" class="latex" title="V" /> of vertices of <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" />. A vertex <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+V&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="v \in V" class="latex" title="v \in V" /> belongs to <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W" class="latex" title="W" /> with probability 1/2, independently.  Now let <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> be the induced graph from <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> on <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W" class="latex" title="W" />. Denote by <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E-%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha^-(G)" class="latex" title="\alpha^-(G)" /> the expected value of <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha (H)" class="latex" title="\alpha (H)" /> over all such random induced subgraphs <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" />.</p>
<p><strong>Conjecture:</strong> For every real number <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\tau" class="latex" title="\tau" />, <img src="https://s0.wp.com/latex.php?latex=0%3C%5Ctau%3C1&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="0&lt;\tau&lt;1" class="latex" title="0&lt;\tau&lt;1" /> there is <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%28%5Ctau+%29+%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\epsilon(\tau ) &gt;0" class="latex" title="\epsilon(\tau ) &gt;0" /> such that if <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%28G%29+%3D%5Ctau+n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha (G) =\tau n" class="latex" title="\alpha (G) =\tau n" /> then <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E-%28G%29+%5Cle+%28%5Ctau-%5Cepsilon%28%5Ctau+%29%29n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha^-(G) \le (\tau-\epsilon(\tau ))n" class="latex" title="\alpha^-(G) \le (\tau-\epsilon(\tau ))n" />.</p>
<p>If we look at the example <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> of the complete graph on 1/τ vertices we get that <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E-%28G%29+%3D%C2%A0+%281-2%5E%7B-1%2F%5Ctau%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha^-(G) =  (1-2^{-1/\tau})" class="latex" title="\alpha^-(G) =  (1-2^{-1/\tau})" />. The reason is that the independence number of <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> is always 1 except in one case occurring with probability <img src="https://s0.wp.com/latex.php?latex=2%5E%7B-1%2F%5Ctau%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="2^{-1/\tau}" class="latex" title="2^{-1/\tau}" /> where <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W" class="latex" title="W" /> is empty.</p>
<p>An example coming from a random graph <img src="https://s0.wp.com/latex.php?latex=G%28n%2Cp%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G(n,p)" class="latex" title="G(n,p)" />  gives that <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%28%5Ctau%29%C2%A0+%3D+%5Ctau+%281-2%5E%7Bc%28-1%2F%5Ctau%29%5Ccdot+%28%5Clog+%281%2F%5Ctau%29%7D%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\epsilon (\tau)  = \tau (1-2^{c(-1/\tau)\cdot (\log (1/\tau)}) " class="latex" title="\epsilon (\tau)  = \tau (1-2^{c(-1/\tau)\cdot (\log (1/\tau)}) " /> and the strong conjecture is that <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%28%5Ctau+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\epsilon(\tau )" class="latex" title="\epsilon(\tau )" /> can be taken to be <img src="https://s0.wp.com/latex.php?latex=2%5E%7BC%28-1%2F%5Ctau%29%5Ccdot%C2%A0+%28%5Clog+%281%2F%5Ctau%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="2^{C(-1/\tau)\cdot  (\log (1/\tau)}" class="latex" title="2^{C(-1/\tau)\cdot  (\log (1/\tau)}" />. (Here, <img src="https://s0.wp.com/latex.php?latex=c%2CC&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="c,C" class="latex" title="c,C" /> are constants.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/12/fkk.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/12/fkk.png?w=640&amp;h=149" class="alignnone size-full wp-image-20603" height="149" /></a></p>
<div style="width: 650px;" class="wp-caption alignnone" id="attachment_20597"><a href="https://gilkalai.files.wordpress.com/2020/12/ll.jpg"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/12/ll.jpg?w=640&amp;h=212" class="size-full wp-image-20597" height="212" /></a><p class="wp-caption-text" id="caption-attachment-20597"><span style="color: #ff0000;">LIONEL LEVINE</span></p></div>
<h2>Problem 2: Lionel Levine’s infamous hat problem.</h2>
<h3>Warm up problem:</h3>
<p><img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> people are assigned at random white/black hats, Each person sees the hats of the other and need to guess the color of her own hat. They can decide in advance on the guessing strategy and their aim is maximize the probability that they all guessed correctly. How well they can do?</p>
<p>It turns out that there is a strategy with 50% chance of winning. They all need to assume that there are overall an even number of black hats.</p>
<h3>The real infamous hat problem by Lionel Levine</h3>
<p>Now every player has an infinite tower of hats on her head and, again, the hats are taken to be white or black at random. Again each players see all the hats of other players and again the players can plan in advance a strategy for success, This time each player has to point to one of the hats on her head and  the players WIN if all these hats are black.</p>
<p>Again, based on the hats of all others the k-th player says something like “I think the 7th hat on my head is black” or “I think the 3rd hat on my head is black” and the players win if all these statement are correct.</p>
<p><strong>Conjecture:</strong> As the number of players grows the probability for success tends to zero.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=p_k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="p_k" class="latex" title="p_k" /> be the success probability for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> players. (Remark: instead of having infinite number of hats we can assume that <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> is fixed, each player has <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> hats, and then let <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> goes to infinity.</p>
<p>Ehud explained why <img src="https://s0.wp.com/latex.php?latex=1%2F3+%5Cle+p_2+%5Cle+3%2F8&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="1/3 \le p_2 \le 3/8" class="latex" title="1/3 \le p_2 \le 3/8" />. He challenged the audience to prove that <img src="https://s0.wp.com/latex.php?latex=p_3+%3C+p_2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="p_3 &lt; p_2" class="latex" title="p_3 &lt; p_2" /> which is unknown.</p>
<p>The problem was presented in <a href="https://blog.tanyakhovanova.com/2011/04/how-many-hats-can-fit-on-your-head/">this 2011 post</a>  on <a href="https://blog.tanyakhovanova.com/" title="Tanya Khovanova's Math Blog: Mathematics, applications of mathematics to life in general, and my life as a mathematician.">Tanya Khovanova’s Math Blog.</a> Tanya also offered a new variant of her own.  It was also <a href="https://www.brand.site.co.il/riddles/201607q.html">July 2016 riddle</a> further discussed <a href="https://www.brand.site.co.il/riddles/201607a.html">here</a> on the math riddles site “<a href="https://www.brand.site.co.il/riddles/">Using your head is permitted</a>”</p>
<p>Below:<span style="color: #ff0000;"> Ehud and Guy</span><a href="https://gilkalai.files.wordpress.com/2020/12/namaste.jpg"><img width="225" alt="" src="https://gilkalai.files.wordpress.com/2020/12/namaste.jpg?w=225&amp;h=300" class="size-medium wp-image-20607 aligncenter" height="300" /></a><a href="https://gilkalai.files.wordpress.com/2020/12/gk-zoom-yoga.png"><img width="300" alt="" src="https://gilkalai.files.wordpress.com/2020/12/gk-zoom-yoga.png?w=300&amp;h=173" class="alignnone size-medium wp-image-20621" height="173" /></a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/12/12/open-problem-session-of-huji-combsem-problem-3-ehud-friedgut-independent-sets-and-lionel-levins-infamous-hat-problem/"><span class="datestr">at December 12, 2020 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/">Raft does not Guarantee Liveness in the face of Network Faults</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Last month, Cloudflare published a postmortem of a recent 6-hour outage caused by a partial switch failure which left etcd unavailable as it was unable to establish a stable leader. This outage has understandably led to discussion online about exactly what liveness guarantees are provided by the Raft consensus algorithm...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/"><span class="datestr">at December 12, 2020 02:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5162">Happy Chanukah / Vaccine Approval Day!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>Inspired by my <a href="https://www.scottaaronson.com/papers/bb.pdf">survey article</a>, John Pavlus has now published an <a href="https://www.quantamagazine.org/the-busy-beaver-game-illuminates-the-fundamental-limits-of-math-20201210/">article on Busy Beaver</a> for <em>Quanta</em> magazine.<br /></li><li>This week, I flitted back and forth between <em>two</em> virtual conferences: the Institute for Advanced Study’s <a href="https://www.ias.edu/sns/quantum-information-workshop-2020">Online Workshop on Qubits and Black Holes</a> (which I co-organized with Juan Maldacena and Mark Van Raamsdonk), and <a href="https://q2b.qcware.com/">Q2B (Quantum 2 Business) 2020</a>, organized by QC Ware, for which I did my now-annual Ask-Me-Anything session.  It was an interesting experience, switching between Euclidean path integrals and replica wormholes that I barely understood, and corporate pitches for near-term quantum computing that I … well, <em>did</em> understand!  Anyway, happy to discuss either conference in the comments.<br /></li><li>For anyone interested in the new Chinese quantum supremacy claim based on Gaussian BosonSampling—the story has developing rapidly all week, with multiple groups trying to understand the classical difficulty of simulating the experiment.  I’ll plan to write a followup post soon!<br /></li><li>The Complexity Zoo has now officially moved from the University of Waterloo to <a href="https://complexityzoo.net/Complexity_Zoo">complexityzoo.net</a>, hosted by the LessWrong folks!  Thanks so much to Oliver Habryka for setting this up.  <strong><span class="has-inline-color has-vivid-red-color">Update (Dec. 12):</span></strong> Alas, complexityzoo.com no longer works if you use https.  I don’t know how to fix it—the Bluehost control panel provides no options—and I’m not at a point in life where I can deal again with Bluehost SSL certificate hell.  (How does everyone else deal with this shit?  That’s the one part I don’t understand.)  So, for now, you’ll need to update your bookmarks to complexityzoo.net.<br /></li><li>In return for his help with Zoo, Oliver asked me to help publicize a handsome $29 five-book set, <a href="https://www.lesswrong.com/books">“A Map that Reflects the Territory,”</a> containing a selection of the best essays from LessWrong, including multiple essays by the much-missed Scott Alexander, and an essay on common knowledge inspired by my own <a href="https://www.scottaaronson.com/blog/?p=2410">Common Knowledge and Aumann’s Agreement Theorem</a>.  (See also the <a href="https://www.lesswrong.com/posts/TTPux7QFBpKxZtMKE/the-lesswrong-book-is-available-for-pre-order">FAQ</a>.)  If you know any LW fans, I can think of few better gifts to go under their Christmas tree or secular rationalist equivalent.</li></ol></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5162"><span class="datestr">at December 11, 2020 05:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
