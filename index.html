<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 10, 2020 05:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/167">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/167">TR20-167 |  Approximate Hypergraph Vertex Cover and generalized Tuza&amp;#39;s conjecture | 

	Venkatesan Guruswami, 

	Sai Sandeep</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A famous conjecture of Tuza states that the minimum number of edges needed to cover all the triangles in a graph is at most twice the maximum number of edge-disjoint triangles.  This conjecture was couched in a broader setting by Aharoni and Zerbib who proposed a hypergraph version of this conjecture, and also studied its implied fractional versions. We establish the fractional version of the Aharoni-Zerbib conjecture up to lower order terms. Specifically, we give a factor $t/2+ O(\sqrt{t \log t})$ approximation based on LP rounding for an algorithmic version of the hypergraph Tur\'{a}n problem (AHTP). The objective in AHTP is to pick the smallest collection of $(t-1)$-sized subsets of vertices of an input $t$-uniform hypergraph such that every hyperedge contains one of these subsets.

Aharoni and Zerbib also posed whether Tuza's conjecture and its hypergraph versions could follow from non-trivial duality gaps between vertex covers and matchings on hypergraphs that exclude certain sub-hypergraphs, for instance, a ``tent" structure that cannot occur in the incidence of triangles and edges. We give a strong negative answer to this question, by exhibiting tent-free hypergraphs, and indeed $\mathcal{F}$-free hypergraphs for any finite family $\mathcal{F}$ of excluded sub-hypergraphs, whose vertex covers must include almost all the vertices.


The algorithmic questions arising in the above study can be phrased as instances of vertex cover on \emph{simple} hypergraphs, whose hyperedges can pairwise share at most one vertex. We prove that the trivial factor $t$ approximation for vertex cover is hard to improve for simple $t$-uniform hypergraphs. However, for set cover on simple $n$-vertex hypergraphs, the greedy algorithm achieves a factor $(\ln n)/2$, better than the optimal $\ln n$ factor for general hypergraphs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/167"><span class="datestr">at November 09, 2020 06:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5091">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5091">On defeating a sociopath</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>There are people who really, genuinely, believe, as far as you can dig down, that winning is everything—that however many lies they told, allies they betrayed, innocent lives they harmed, etc. etc., it was all justified by the fact that <strong>they won and their enemies lost</strong>.  Faced with such sociopaths, people like me typically feel an irresistible compulsion to <em>counterargue</em>: to make the sociopath realize that winning is <em>not</em> everything, that truth and honor are terminal values as well; to subject the sociopath to the standards by which the rest of us are judged; to find the conscience that the sociopath buried even from himself and drag it out into the light.  Let me know if you can think of any case in human history where such efforts succeeded, because I’m having difficulty doing so.</p>



<p>Clearly, in the vast majority of cases if not in all, the only counterargument that a sociopath will ever understand is <em>losing</em>.  And yet not just any kind of losing suffices.  For victims, there’s an <em>enormous</em> temptation to turn the sociopath’s underhanded tools against him, to win with the same deceit and naked power that the sociopath so gleefully inflicted on others.  And yet, if that’s what it takes to beat him, then you have to imagine the sociopath deriving a certain perverse satisfaction from it.</p>



<p>Think of the movie villain who, as the panting hero stands over him with his lightsaber, taunts “Yes … yes … destroy me!  Do it now!  Feel the hate and the rage flow through you!”  What happens next, of course, is that the hero angrily decides to give the villain one more chance, the ungrateful villain lunges to stab the hero in the back or something, and only then does the villain die—either by a self-inflicted accident, or else killed by the hero in immediate self-defense.  Either way, the hero walks away with victory <em>and</em> honor.</p>



<p>In practice, it’s a tall order to arrange all of that.  This explains why sociopaths are so hard to defeat, and why I feel so bleak and depressed whenever I see one flaunting his power.  But, you know, the great upside of pessimism is that it doesn’t take much to beat your expectations!  Whenever a single sociopath <em>is</em> cleanly and honorably defeated, or even just rendered irrelevant—no matter that the sociopath’s friends and allies are still in power, no matter that they’ll be back to fight another day, etc. etc.—it’s a genuine occasion for rejoicing.</p>



<p>Anyway, that pretty much sums up my thoughts regarding Arthur Chu.  In other news, hooray about the election!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5091"><span class="datestr">at November 09, 2020 05:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/166">TR20-166 |  Lower Bounds for Monotone Arithmetic Circuits Via Communication Complexity | 

	Arkadev Chattopadhyay, 

	Rajit Datta, 

	Partha Mukhopadhyay</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Valiant (1980) showed that general arithmetic circuits with negation can be exponentially more powerful than monotone ones. We give the first qualitative improvement to this classical result: we construct a  family of polynomials $P_n$ in $n$ variables, each of its monomials has positive coefficient, such that $P_n$ can be computed by a polynomial-size \emph{depth-three formula} but every monotone circuit computing it has size $2^{\Omega(n^{1/4}/\log(n))}$. 

The polynomial $P_n$ embeds the $\text{SINK}\circ \text{XOR}$ function devised recently by Chattopadhyay, Mande and Sherif (2020) to refute the Log-Approximate-Rank Conjecture in communication complexity. To prove our lower bound for $P_n$, we develop a general connection between corruption of combinatorial rectangles by  any function $f \circ \text{XOR}$ and corruption of product polynomials by a certain polynomial $P^f$ that is an arithmetic embedding of $f$. This connection should be of independent interest.

Using further ideas from communication complexity, we construct another family of set-multilinear polynomials $f_{n,m}$ such that both $F_{n,m} - \epsilon\cdot f_{n,m}$ and $F_{n,m} + \epsilon\cdot f_{n,m}$ have monotone circuit complexity $2^{\Omega(n/\log(n))}$ if $\epsilon \geq 2^{- \Omega( m )}$ and $F_{n,m} = \prod_{i=1}^n \big(x_{i,1} +\cdots+x_{i,m}\big)$, with $m = O( n/\log n )$. The polynomials $f_{n,m}$ have 0/1 coefficients and are in VNP. Proving such lower bounds for monotone circuits has been advocated recently by Hrubeš (2020) as a first step towards proving lower bounds against general circuits via his new approach.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/166"><span class="datestr">at November 09, 2020 01:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/165">TR20-165 |  Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes | 

	Sarah Bordage, 

	Jade Nardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, we initiate the study of proximity testing to Algebraic Geometry (AG) codes. An AG code $C = C(\mathcal C, \mathcal P, D)$ is a vector space associated to evaluations on $\mathcal P$ of functions in the Riemann-Roch space $L_\mathcal C(D)$. The problem of testing proximity to an error-correcting code $C$ consists in distinguishing between the case where an input word, given as an oracle, belongs to $C$ and the one where it is far from every codeword of $C$. AG codes are good candidates to construct \emph{short} proof systems, but there exists no efficient proximity tests for them. We aim to fill this gap.

We construct an Interactive Oracle Proof of Proximity (IOPP) for some families of AG codes by generalizing an IOPP for Reed-Solomon codes, known as the \textsf{FRI} protocol and introduced by Ben-Sasson, Bentov, Horesh and Riabzev in 2018. We identify suitable requirements for designing efficient IOPP systems for AG codes. In addition to proposing the first proximity test targeting AG codes, our IOPP admits quasilinear prover arithmetic complexity and sublinear verifier arithmetic complexity with constant soundness for meaningful classes of AG codes. We take advantage of the algebraic geometry framework that makes any group action on the curve that fixes the divisor $D$ translate into a decomposition of the code $C$. Concretely, our approach relies on Kani's result that splits the Riemann-Roch space of any invariant divisor under this action into several explicit Riemann-Roch spaces on the quotient curve. Under some hypotheses, these spaces behave well enough to define an AG code $C'$ on the quotient curve so that a proximity test to $C$ can be reduced to one to $C'$. Iterating this process thoroughly, we end up with a membership test to a code with significantly smaller length.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/165"><span class="datestr">at November 09, 2020 12:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/164">TR20-164 |  Direct Sum and Partitionability Testing over General Groups | 

	Gautam Prakriya, 

	Andrej Bogdanov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A function $f(x_1, \dots, x_n)$ from a product domain $\mathcal{D}_1 \times \cdots \times \mathcal{D}_n$ to an abelian group $\mathcal{G}$ is a direct sum if it is of the form $f_1(x_1) + \cdots + f_n(x_n)$.  We present a new 4-query direct sum test with optimal (up to constant factors) soundness error.  This generalizes a result of Dinur and Golubev (RANDOM 2019) which is tailored to the target group $\mathcal{G} = \mathbb{Z}_2$.  As a special case, we obtain an optimal affinity test for $\mathcal{G}$-valued functions on domain $\{0, 1\}^n$ under product measure.  Our analysis relies on the hypercontractivity of the binary erasure channel.

We also study the testability of function partitionability over product domains into disjoint components. A $\mathcal{G}$-valued $f(x_1, \dots, x_n)$ is $k$-direct sum partitionable if it can be written as a sum of functions over $k$ nonempty disjoint sets of inputs.  A function $f(x_1, \dots, x_n)$ with unstructured product range $\mathcal{R}^k$ is  direct product partitionable if its outputs depend on disjoint sets of inputs.  

We show that direct sum partitionability and direct product partitionability are one-sided error testable with $O((n - k)(\log n + 1/\epsilon) + 1/\epsilon)$ adaptive queries and $O((n/\epsilon) \log^2(n/\epsilon))$ nonadaptive queries, respectively.  Both bounds are tight up to the logarithmic factors for constant $\epsilon$ even with respect to adaptive, two-sided error testers.  We also give a non-adaptive one-sided error tester for direct sum partitionability with query complexity $O(kn^2 (\log n)^2 / \epsilon)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/164"><span class="datestr">at November 09, 2020 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03495">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03495">Semi-Streaming Bipartite Matching in Fewer Passes and Less Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Yujia.html">Yujia Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03495">PDF</a><br /><b>Abstract: </b>We provide algorithms with improved pass and space complexities for
approximately solving linear programs, optimal transport, bipartite matching,
and more in the semi-streaming model. For instance, we provide a (randomized)
algorithm computing a maximum cardinality matching in an unweighted bipartite
graph in $O(\log^2 n \cdot \epsilon^{-1})$ passes, using $O(n \log^2 n \cdot
\epsilon^{-1})$ auxiliary memory. This marks the first improvements to the
$O(\log\log\epsilon^{-1} \cdot \epsilon^{-2})$ pass, $O(n
\log\log\epsilon^{-1}\cdot \epsilon^{-2})$-space algorithms of [AG13] when
$\epsilon$ is moderately small.
</p>
<p>To obtain our results, we give an $O(n)$ space deterministic semi-streaming
algorithm for approximating the value of linear programs (in the form of
box-simplex games), based on low-space implementations of [She17, JST19]. We
further give a general sampling procedure for explicitly forming a fractional
solution in low space, yielding improved semi-streaming guarantees for optimal
transport and, in some regimes, maximum weighted matching. Finally, we improve
the space complexity of our maximum cardinality matching method using an
implicit implementation of the random walk rounding of [GKK10] via custom
turnstile samplers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03495"><span class="datestr">at November 09, 2020 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03454">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03454">Fixed Parameter Approximation Scheme for Min-max $k$-cut</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chandrasekaran:Karthekeyan.html">Karthekeyan Chandrasekaran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Weihang.html">Weihang Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03454">PDF</a><br /><b>Abstract: </b>We consider the graph $k$-partitioning problem under the min-max objective,
termed as Minmax $k$-cut. The input here is a graph $G=(V,E)$ with non-negative
edge weights $w:E\rightarrow \mathbb{R}_+$ and an integer $k\geq 2$ and the
goal is to partition the vertices into $k$ non-empty parts $V_1, \ldots, V_k$
so as to minimize $\max_{i=1}^k w(\delta(V_i))$. Although minimizing the sum
objective $\sum_{i=1}^k w(\delta(V_i))$, termed as Minsum $k$-cut, has been
studied extensively in the literature, very little is known about minimizing
the max objective. We initiate the study of Minmax $k$-cut by showing that it
is NP-hard and W[1]-hard when parameterized by $k$, and design a parameterized
approximation scheme when parameterized by $k$. The main ingredient of our
parameterized approximation scheme is an exact algorithm for Minmax $k$-cut
that runs in time $(\lambda k)^{O(k^2)}n^{O(1)}$, where $\lambda$ is value of
the optimum and $n$ is the number of vertices. Our algorithmic technique builds
on the technique of Lokshtanov, Saurabh, and Surianarayanan (FOCS, 2020) who
showed a similar result for Minsum $k$-cut. Our algorithmic techniques are more
general and can be used to obtain parameterized approximation schemes for
minimizing $\ell_p$-norm measures of $k$-partitioning for every $p\geq 1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03454"><span class="datestr">at November 09, 2020 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03434">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03434">Maximum Matchings and Popularity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kavitha:Telikepalli.html">Telikepalli Kavitha</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03434">PDF</a><br /><b>Abstract: </b>Let $G$ be a bipartite graph where every node has a strict ranking of its
neighbors. For every node, its preferences over neighbors extend naturally to
preferences over matchings. Matching $N$ is more popular than matching $M$ if
the number of nodes that prefer $N$ to $M$ is more than the number that prefer
$M$ to $N$. A maximum matching $M$ in $G$ is a "popular max-matching" if there
is no maximum matching in $G$ that is more popular than $M$. Such matchings are
relevant in applications where the set of admissible solutions is the set of
maximum matchings and we wish to find a best maximum matching as per node
preferences. It is known that a popular max-matching always exists in $G$. Here
we show a compact extended formulation for the popular max-matching polytope.
So when there are edge costs, a min-cost popular max-matching in $G$ can be
computed in polynomial time. This is in contrast to the min-cost popular
matching problem which is known to be NP-hard. We also consider
Pareto-optimality, which is a relaxation of popularity, and show that computing
a min-cost Pareto-optimal matching/max-matching is NP-hard.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03434"><span class="datestr">at November 09, 2020 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03433">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03433">Detecting and Counting Small Subgraphs, and Evaluating a Parameterized Tutte Polynomial: Lower Bounds via Toroidal Grids and Cayley Graph Expanders</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Marc.html">Marc Roth</a>, Johannes Schmitt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellnitz:Philip.html">Philip Wellnitz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03433">PDF</a><br /><b>Abstract: </b>Given a graph property $\Phi$, we consider the problem
$\mathtt{EdgeSub}(\Phi)$, where the input is a pair of a graph $G$ and a
positive integer $k$, and the task is to decide whether $G$ contains a $k$-edge
subgraph that satisfies $\Phi$. Specifically, we study the parameterized
complexity of $\mathtt{EdgeSub}(\Phi)$ and of its counting problem
$\#\mathtt{EdgeSub}(\Phi)$ with respect to both approximate and exact counting.
We obtain a complete picture for minor-closed properties $\Phi$: the decision
problem $\mathtt{EdgeSub}(\Phi)$ always admits an FPT algorithm and the
counting problem $\#\mathtt{EdgeSub}(\Phi)$ always admits an FPTRAS. For exact
counting, we present an exhaustive and explicit criterion on the property
$\Phi$ which, if satisfied, yields fixed-parameter tractability and otherwise
$\#\mathsf{W[1]}$-hardness. Additionally, most of our hardness results come
with an almost tight conditional lower bound under the so-called Exponential
Time Hypothesis, ruling out algorithms for $\#\mathtt{EdgeSub}(\Phi)$ that run
in time $f(k)\cdot|G|^{o(k/\log k)}$ for any computable function $f$.
</p>
<p>As a main technical result, we gain a complete understanding of the
coefficients of toroidal grids and selected Cayley graph expanders in the
homomorphism basis of $\#\mathtt{EdgeSub}(\Phi)$. This allows us to establish
hardness of exact counting using the Complexity Monotonicity framework due to
Curticapean, Dell and Marx (STOC'17). Our methods can also be applied to a
parameterized variant of the Tutte Polynomial $T^k_G$ of a graph $G$, to which
many known combinatorial interpretations of values of the (classical) Tutte
Polynomial can be extended. As an example, $T^k_G(2,1)$ corresponds to the
number of $k$-forests in the graph $G$. Our techniques allow us to completely
understand the parametrized complexity of computing the evaluation of $T^k_G$
at every pair of rational coordinates $(x,y)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03433"><span class="datestr">at November 09, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03354">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03354">Vertex Fault-Tolerant Geometric Spanners for Weighted Points</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharjee:Sukanya.html">Sukanya Bhattacharjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inkulu:R=.html">R. Inkulu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03354">PDF</a><br /><b>Abstract: </b>Given a set S of n points, a weight function w to associate a non-negative
weight to each point in S, a positive integer k \ge 1, and a real number
\epsilon &gt; 0, we present algorithms for computing a spanner network G(S, E) for
the metric space (S, d_w) induced by the weighted points in S. The weighted
distance function d_w on the set S of points is defined as follows: for any p,
q \in S, d_w(p, q) is equal to w(p) + d_\pi(p, q) + w(q) if p \ne q, otherwise,
d_w(p, q) is 0. Here, d_\pi(p, q) is the Euclidean distance between p and q if
points in S are in \mathbb{R}^d, otherwise, it is the geodesic (Euclidean)
distance between p and q. The following are our results: (1) When the weighted
points in S are located in \mathbb{R}^d, we compute a k-vertex fault-tolerant
(4+\epsilon)-spanner network of size O(k n). (2) When the weighted points in S
are located in the relative interior of the free space of a polygonal domain
\cal P, we detail an algorithm to compute a k-vertex fault-tolerant
(4+\epsilon)-spanner network with O(\frac{kn\sqrt{h+1}}{\epsilon^2} \lg{n})
edges. Here, h is the number of simple polygonal holes in \cal P. (3) When the
weighted points in S are located on a polyhedral terrain \cal T, we propose an
algorithm to compute a k-vertex fault-tolerant (4+\epsilon)-spanner network,
and the number of edges in this network is O(\frac{kn}{\epsilon^2} \lg{n}).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03354"><span class="datestr">at November 09, 2020 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03291">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03291">Fault-Tolerant All-Pairs Mincuts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baswana:Surender.html">Surender Baswana</a>, Abhyuday Pandey <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03291">PDF</a><br /><b>Abstract: </b>Let $G=(V,E)$ be an undirected unweighted graph on $n$ vertices and $m$
edges. We address the problem of fault-tolerant data structure for all-pairs
mincuts in $G$ defined as follows.
</p>
<p>Build a compact data structure that, on receiving a pair of vertices $s,t\in
V$ and any edge $(x,y)$ as query, can efficiently report the value of the
mincut between $s$ and $t$ upon failure of the edge $(x,y)$.
</p>
<p>To the best of our knowledge, there exists no data structure for this problem
which takes $o(mn)$ space and a non-trivial query time. We present two compact
data structures for this problem.
</p>
<p>- Our first data structure guarantees ${\cal O}(1)$ query time. The space
occupied by this data structure is ${\cal O}(n^2)$ which matches the worst-case
size of a graph on $n$ vertices.
</p>
<p>- Our second data structure takes ${\cal O}(m)$ space which matches the size
of the graph. The query time is ${\cal O}(\min(m,n c_{s,t}))$ where $c_{s,t}$
is the value of the mincut between $s$ and $t$ in $G$. The query time
guaranteed by our data structure is faster by a factor of $\Omega(\sqrt{n})$
compared to the best known algorithm to compute a $(s,t)$-mincut.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03291"><span class="datestr">at November 09, 2020 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03212">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03212">Optimal Online Algorithms for File-Bundle Caching and Generalization to Distributed Caching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qin:Tiancheng.html">Tiancheng Qin</a>, S. Rasoul Etesami <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03212">PDF</a><br /><b>Abstract: </b>We consider a generalization of the standard cache problem called file-bundle
caching, where different queries (tasks), each containing $l\ge 1$ files,
sequentially arrive. An online algorithm that does not know the sequence of
queries ahead of time must adaptively decide on what files to keep in the cache
to incur the minimum number of cache misses. Here a cache miss refers to the
case where at least one file in a query is missing among the cache files. In
the special case where $l=1$, this problem reduces to the standard cache
problem. We first analyze the performance of the classic least recently used
(LRU) algorithm in this setting and show that LRU is a near-optimal online
deterministic algorithm for file-bundle caching with regard to competitive
ratio. We then extend our results to a generalized $(h,k)$-paging problem in
this file-bundle setting, where the performance of the online algorithm with a
cache size $k$ is compared to an optimal offline benchmark of a smaller cache
size $h&lt;k$. In this latter case, we provide a randomized $O(l \ln
\frac{k}{k-h})$-competitive algorithm for our generalized $(h,k)$-paging
problem, which can be viewed as an extension of the classic marking algorithm.
We complete this result by providing a matching lower bound for the competitive
ratio, indicating that the performance of this modified marking algorithm is
within a factor of two of any randomized online algorithm. Finally, we look at
the distributed version of the file-bundle caching problem where there are
$m\ge 1$ identical caches in the system. In this case we show that for $m=l+1$
caches, there is a deterministic distributed caching algorithm which is
$(l^2+l)$-competitive and a randomized distributed caching algorithm which is
$O(l\ln(2l+1))$-competitive when $l\ge 2$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03212"><span class="datestr">at November 09, 2020 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03209">Mapper Interactive: A Scalable, Extendable, and Interactive Toolbox for the Visual Exploration of High-Dimensional Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Youjia.html">Youjia Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalapathi:Nithin.html">Nithin Chalapathi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rathore:Archit.html">Archit Rathore</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Yaodong.html">Yaodong Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Bei.html">Bei Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03209">PDF</a><br /><b>Abstract: </b>Mapper algorithm is a popular tool from topological data analysis for
extracting topological summaries of high-dimensional datasets. It has enjoyed
great success in data science, from shape classification to cancer research and
sports analytics. In this paper, we present Mapper Interactive, a web-based
framework for the interactive analysis and visualization of high-dimensional
point cloud data built upon the mapper algorithm. Different from existing
implementations, Mapper Interactive implements the mapper algorithm in an
interactive, scalable, and easily extendable way, thus supporting practical
data analysis. In particular, its command-line API can compute mapper graphs
for 1 million points of 512 dimensions in about 3 minutes (6 times faster than
the vanilla implementation). Its visual interface allows on-the-fly computation
and manipulation of the mapper graph based on user-specified parameters and
supports the addition of new analysis modules with a few lines of code. Mapper
Interactive makes the mapper algorithm accessible to nonspecialists and
accelerates topological analysis workflows.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03209"><span class="datestr">at November 09, 2020 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03194">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03194">Fast Approximation Algorithms for Bounded Degree and Crossing Spanning Tree Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chekuri:Chandra.html">Chandra Chekuri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quanrud:Kent.html">Kent Quanrud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Torres:Manuel_R=.html">Manuel R. Torres</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03194">PDF</a><br /><b>Abstract: </b>We develop fast near-linear time approximation algorithms for the
minimum-cost version of the Bounded-Degree MST problem and its generalization
the Crossing Spanning Tree problem. We solve the underlying LP to within a
$(1+\epsilon)$ approximation factor in near-linear time via multiplicative
weight update (MWU) techniques. To round the fractional solution, in our main
technical contribution, we describe a fast near-linear time implementation of
swap-rounding in the spanning tree polytope of a graph. The fractional solution
can also be used to sparsify the input graph that can in turn be used to speed
up existing combinatorial algorithms. Together these ideas lead to
significantly faster approximation algorithms than known before.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03194"><span class="datestr">at November 09, 2020 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03141">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03141">Quantum randomized encoding, verification of quantum computing, no-cloning, and blind quantum computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morimae:Tomoyuki.html">Tomoyuki Morimae</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03141">PDF</a><br /><b>Abstract: </b>Randomized encoding is a powerful cryptographic primitive with various
applications such as secure multiparty computation, verifiable computation,
parallel cryptography, and complexity lower-bounds. Intuitively, randomized
encoding $\hat{f}$ of a function $f$ is another function such that $f(x)$ can
be recovered from $\hat{f}(x)$, and nothing except for $f(x)$ is leaked from
$\hat{f}(x)$. Its quantum version, quantum randomized encoding, has been
introduced recently [Brakerski and Yuen, <a href="http://export.arxiv.org/abs/2006.01085">arXiv:2006.01085</a>]. Intuitively,
quantum randomized encoding $\hat{F}$ of a quantum operation $F$ is another
quantum operation such that, for any quantum state $\rho$, $F(\rho)$ can be
recovered from $\hat{F}(\rho)$, and nothing except for $F(\rho)$ is leaked from
$\hat{F}(\rho)$. In this paper, we show that if quantum randomized encoding of
BB84 state generations is possible with an encoding operation $E$, then a
two-round verification of quantum computing is possible with a classical
verifier who can additionally do the operation $E$. One of the most important
goals in the field of the verification of quantum computing is to construct a
verification protocol with a verifier as classical as possible. This result
therefore demonstrates a potential application of quantum randomized encoding
to the verification of quantum computing: if we can find a good quantum
randomized encoding (in terms of the encoding complexity), then we can
construct a good verification protocol of quantum computing. We, however, also
show that too good quantum randomized encoding is impossible: if quantum
randomized encoding with a classical encoding operation is possible, then the
no-cloning is violated. We finally consider a natural modification of blind
quantum computing protocols in such a way that the server gets the output like
quantum randomized encoding. We show that the modified protocol is not secure.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03141"><span class="datestr">at November 09, 2020 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03133">Group isomorphism is nearly-linear time for most orders</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dietrich:Heiko.html">Heiko Dietrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:James_B=.html">James B. Wilson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03133">PDF</a><br /><b>Abstract: </b>We show that there is a dense set $\Upsilon\subseteq \mathbb{N}$ of group
orders and a constant $c$ such that for every $n\in \Upsilon$ we can decide in
time $O(n^2(\log n)^c)$ whether two $n\times n$ multiplication tables describe
isomorphic groups of order $n$. This improves significantly over the general
$n^{O(\log n)}$-time complexity and shows that group isomorphism can be tested
efficiently for almost all group orders $n$. We also show that in time $O(n^2
(\log n)^2)$ it can be decided whether an $n\times n$ multiplication table
describes a group; this improves over the known $O(n^3)$ complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03133"><span class="datestr">at November 09, 2020 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03107">Visibility Extension via Reflection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaezi:Arash.html">Arash Vaezi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Bodhayan.html">Bodhayan Roy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghodsi:Mohammad.html">Mohammad Ghodsi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03107">PDF</a><br /><b>Abstract: </b>This paper studies a variant of the Art-gallery problem in which "walls" can
be replaced by \emph{reflecting-edges}, which allows the guard to see further
and thereby see a larger portion of the gallery. The art-gallery is a simple
closed polygon $P$, a guard is a point $p$ in $P$, and a guard sees another
point $q$ in $P$ if the segment $\overline{pq}$ is contained in the interior of
$P$. The visibility region of $p$ is the set of points $q$ in $P$ that are
visible from $p$. If we let an edge of the polygon allow reflections, then the
visibility region should be changed accordingly. We study visibility with
specular and diffuse reflections. Moreover, the number of times a ray can be
reflected can be taken as a parameter. For \emph{vertex} guarding polygons with
$k$ diffuse reflections, we establish an upper bound on the optimum solution.
For this problem, we generalize the $O(logn)$-approximation ratio algorithm of
the Art Gallery Problem. For a bounded $k$, the generalization gives a
polynomial-time algorithm with $O(log n)$-approximation ratio for both cases
diffuse and specular reflections. Furthermore, We show that several cases of
the generalized problem are NP-hard. We also illustrate that if $P$ is a funnel
or a weak visibility polygon, then the problem becomes more straightforward and
can be solved in polynomial time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03107"><span class="datestr">at November 09, 2020 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7627871032207660441">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/11/random-thoughts-on-election-2020.html">Random Thoughts on the Election (2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>1) Biden will be the oldest president (measuring by when they take the oath of office), at 78. The next two are Trump 70 and Reagan 69. Biden will be older entering office then Reagan was leaving office. </p><p>After Biden, Trump, Reagan:</p><p>William Henry Harrison 68. Why do some people have middle names that are commonly spoken and some do not? Others with middle names spoken: Lee Harvey Oswald, John Wilkes Booth. </p><p>James Buchanan 65</p><p>George H. W. Bush 64. Why do some people have their initials commonly spoken and others do not? In this case it may be to distinguish from W. Why are some people known by their middle initial? Well, actually one that I know of, W.</p><p>Youngest was Theodore Roosevelt 42 who took the office after McKinley was assassinated . Kennedy was youngest to take the oath after being ELECTED at 43. Theodore Roosevelt was known as TR. John F Kennedy is often called JFK. Franklin D Roosevelt was called FDR. Why are some people known by their initials? In these cases maybe to distinguish them from other Roosevelts and Kennedys.</p><p>2) Right now it looks like GA will go for Biden. This surprises me. I had heard `GA is on the verge of turning blue and always will be.'</p><p>3) Dem-Blue, Rep-Red always puzzled me since I thought Red was associated with communism.</p><p>4) A quote from the Trump/Schwartz  book THE ART OF THE DEAL about why Carter was a one-termer  is rather predictive:</p><p>See <a href="https://www.thewrap.com/drudge-trump-election-con/">here</a></p><p>(I've heard Schwartz referred to as a ghost writer. That is not true-- Tony Schwartz's name is ON THE COVER, so he is not a ghostwriter.)</p><p>5) During the Trump administration UAE, Bahrain, and Sudan all recognized Israel (gee, when I see it on a map I recognize it, why did it take them so long :-) ). See <a href="https://apnews.com/article/donald-trump-bahrain-israel-united-arab-emirates-sudan-07fe32a9ac1bd3afe5c503d4ff6bc859">here</a>. All three deals were brokered by the US so Trump could  take credit here. So why didn't he? One answer is that the left-wing lame stream media didn't cover it much. But FOX didn't cover it much. Trump didn't mention it much. Trump didn't even mention it as a way to complain about media coverage. So- independent of if you think Trump deserves credit here or not, I am interested in why he didn't brag about this one. (When I have asked this question people point out that Trump's base would not care about this. But Trump could complain that Obama got a Nobel Peace prize for nothing, and he got these deals done and hasn't, gotten a Nobel Prize because of the fake-Nobel-Committee and channel this into anti-Obama sentiment.)</p><p>6) Truth avoids imitating  art: Watch Season five of the HBO show VEEP for how messy a close election can be. </p><p>7) I think Biden will end up being president and the transition will be peaceful. Why? Fox News and other conservative organizations are urging Trump to concede. Republican state legislators are NOT trying to find ways to overturn the results in their states. Judges have found NO evidence for the kinds of fraud that Trump is complaining about. Many Republicans are silent (John Oliver says that means they SUPPORT  Trumps allegations, but I think it means they are NOT supporting Trump's allegations.) </p><p> Why is the Republican establishment NOT backing Trumps claims of fraud? Here are some thoughts.</p><p>a) Because the allegations of fraud are not just false but obviously false.  </p><p>b) Because they think that it is better for the country to have a clean transition.</p><p>c) Because they are tired of Trump also and realize he is not good for the party brand (a bit late now).</p><p>d) Corrupting the electoral process is a bridge to far. (Where did that phrase come from?)</p><p>e) I wonder if Trump himself would have preferred to lose in 2016 and go around having rallies, perhaps have his own TV network. RALLIES are fun, RUNNING THE COUNTRY is not. So those around him may want him to go back to his original plan. </p><p>8) Did Nate (the only pollster with a one-word-name) do well this time around  He thinks so, see <a href="https://www.thedailybeast.com/nate-silver-tells-fivethirtyeight-critics-fuck-you-we-did-a-good-job">here</a>. Its not even clear he did badly in 2016- he gave Trump a 20% chance in 2016 and a 10% chance this time. </p><p>9) I think that if  we had not had a pandemic then  Trump would have won. Two reasons: the country thinks he handled it badly, and it may have literally killed some of his voters.  As a final note on that: Mark Meadows (WH Chief of staff) has COVID. I am surprised Pence didn't get it-- thought maybe he did or will. </p><p>10) Why did people in the Trump WH who one assumes know that Covid is serious and that masks and social distancing were  way to prevent it, not do these simple things?  Perhaps they thought (correctly) that the more people thing about covid, the more likely Trump loses, so they took a risk. Alas, those that trade their health for electability get neither. </p><p>11) Neither Pence nor Harris is particularly young or old as VP's go. </p><p>Youngest: John Breckenridge, 36. Buchanan's VP</p><p>Second Youngest: Richard Nixon, 40, Eisenhower's VP</p><p>Oldest: Alben Barkley, 71, Truman's VP</p><p>Second oldest: Charles Curtis, 69, Hoover's VP</p><p>Pence was 57 when took the oath, Harris will be 56. </p><p>12) If Biden wins then on Jan 20 when he takes the oath there will be 5 living Ex-presidents:Carter, Clinton, W, Obama, Trump (assuming they all stay alive until then). This ties the record for most living ex-presidents. See <a href="https://blog.computationalcomplexity.org/2018/12/george-hw-bush-passed-away-some-non.html">here</a> for my blog post on this. Getting to 6 will be difficult since Carter is 96 years old. </p><p>13) Neither Lance nor I have blogged much about the election, or even about politics. One reason is that whatever I want to say Scott says better (Scott and Lance are the only theory bloggers known by just their first names). I was going to point to Scott's  political blogs but that was hard since he often has blog posts about multiple topics (Like his post about  Mike Pence thinking that the Ind of CH is a sort of relativism that also allows for adultery to be considered okay (see <a href="https://archive.thinkprogress.org/mike-pence-argued-for-criminalization-of-adultery-before-joining-trump-ticket-e27e8a423caa/">here</a> for Pence's pre-Trump views on adultery)  Actually Scott never blogged about Pence and CH  but after reading his posts they kind of blur in my mind.) I will point to one blog entry of his  that I suspect will NOT be relevant but is still very interesting: <a href="https://www.scottaaronson.com/blog/?p=4845">Will he go?</a></p><p>14) Trump claimed the polls showing he was behind were false and part of a conspiracy. I am not sure how this conspiracy would work. If people think their candidate is ahead or behind, then does that affect how or if they vote? Do people say `Gee X is winning, I'll vote for them' ? I doubt it. There are two ways such a conspiracy could work (1)  claim was that some candidate was WAY ahead (it would not matter which one) so you should not bother to vote (2)   in a primary where you are voting on who you think will win the general election.  He also claimed that the early returns saying Biden was winning was a conspiracy. Same problem there- how would that work. This isn't just Trump, other politicians in diff  years claim that early-returns saying X is winning might make it harder for Y to win. I can't see how. </p><p>15) Kamala Harris will be the first women, the first African-American, and the first Asian Veep.  Trivia: There has been a Native American Veep- who was it? More trivia- who coined the term <i>Veep? </i>I won't answer these here, but they might be on my Prez Quiz that I will post after the new President is sworn in.  </p><p>Can she be BOTH the first African-American and the first Asian? Yes.</p><p>16) In my lifetime the election for President was  SETTLED when the losing candidate conceded. This was good for the country's mindset that YES the president is known and even the other candidate agrees. What if Trump does not concede? I doubt this has any practical affect, except that  on Jan 20 he might be trying to arrange a moving van at the last minute. But if the losing candidate does not concede then when is the matter settled?  When the major news venues say it is? Which ones are major? What if there was a really close election and diff news networks declared diff candidates to have won? This does not seem to be a problem for this election cycle, but it is a question: When is the matter SETTLED in that the country ACCEPTS the result, if the losing candidate does not concede?</p><p>17) Carter beat incumbent Ford, but they became friends. Clinton beat incumbent Bush Sr, but they became friends. This is understandable in that so few people are president so they have a shared experience. I doubt that Trump and Biden will become friends.</p><p>18)   Bill Clinton's staff removed W's from the typewriters and did some other damage before W moved into the WH see <a href="https://www.nytimes.com/2002/06/12/us/white-house-vandalized-in-transition-gao-finds.html">here</a>.  This is NOT a tradition, nor is it acceptable in any way, shape. or form.  I do not know of any other similar cases in America (if you do, let me know in the comments).  I wonder if Trump will do damage  to the WH before he leaves. Do presidents put a deposit down on the WH so that any damage they do, they pay for? I  doubt it, but it would be a good idea. </p><p>19)  Obama and Trump had a cordial 90 minute meeting, see <a href="https://www.nytimes.com/2016/11/11/us/politics/white-house-transition-obama-trump.html">here</a>, after Trump won but before he moved in. This makes perfect sense--outgoing presidents know stuff and have experiences worth sharing with the next president.   Obama said North Korea would be a problem and it is (Trump later tried to spin this--`Obama said it would be hard, but it was easy') I wonder if Trump and Biden will have any kind of meeting, cordial or not. </p><p>20) Every state that went for H Clinton in 2016 went for Biden in 2020. The following states went for Trump in 2016 but went for Biden in 2020: Wisc, Mich, PA, AZ, and maybe Georgia and maybe NC (frankly I doubt NC). There was a plausible  scenario (I forget what it was) where Biden would have won 270-268. </p><p>21) Did Third parties matter? In PA the Libertarian Candidate Jo Jorgenson got 1.1% of the vote which was larger than the diff between Biden (49.7) and Trump (49.1) (The Green party either wasn't on the ballot or got so few votes it was not counted). If most of the Libertarians voted for Trump then he would have won PA and possibly the election. However, Trump is not really a Libertarian, so I doubt that would have happened As for the entire country: (1) . The Libertarians got 1.14% of the total vote in 2020, as opposed to 3.25% in 2016, (2)  The Green party got 1.06% in 2016 and 0.02% in 2020. </p><p>21) I was not particular impressed with the satires of the debates and other political satire on SNL this year. Not sure why- maybe Trump is too wild  to satirize and Joltin Jo is too boring. But the following I DID like and is now more relevant. Watch the whole thing since the first half looks like a real ad.</p><p><a href="https://deadline.com/2020/10/snl-cast-members-2020-trump-1234603033/">here</a><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/11/random-thoughts-on-election-2020.html"><span class="datestr">at November 08, 2020 03:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/163">TR20-163 |  Expander Random Walks: A Fourier-Analytic Approach | 

	Gil Cohen, 

	Amnon Ta-Shma, 

	Noam Peri</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work we ask the following basic question:  assume the vertices of an expander graph are labelled by $0,1$. What "test" functions $f : \{ 0,1\}^t \to \{0,1\}$ cannot  distinguish  $t$ independent samples from those obtained by a random walk? The expander hitting property due to Ajtai, Komlos and Szemeredi (STOC 1987) is captured by the $\mathrm{AND}$ test function, whereas the fundamental expander Chernoff bound due to Gillman (SICOMP 1998), Heally (Computational Complexity 2008) is about test functions indicating whether the weight is close to the mean. In fact, it is known that all threshold functions are fooled by a random walk (Kipnis and Varadhan, Communications in Mathematical Physics 1986).  Recently, it was shown that even the highly sensitive $\mathrm{PARITY}$ function is fooled by a random walk (Ta-Shma; STOC 2017).  

We focus on balanced labels. Our first main result is proving that all symmetric functions are fooled by a random walk. Put differently, we prove a central limit theorem (CLT) for expander random walks with respect to the total variation distance, significantly strengthening the classic CLT for Markov Chains that is established with respect to the Kolmogorov distance due to Kipnis and Varadhan. Our approach significantly deviates from prior works. We first study how well a Fourier character $\chi_S$ is fooled by a random walk as a function of $S$. Then, given a test function $f$, we expand $f$ in the Fourier basis and combine the above with known results on the Fourier spectrum of $f$. 

We also proceed further and consider general test functions - not necessarily symmetric. As our approach is Fourier analytic, it is general enough to analyze  such versatile test functions. For our second result, we prove that random walks on sufficiently good expander graphs fool tests functions computed by $\mathbf{AC}^0$ circuits, read-once branching programs, and functions with bounded query complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/163"><span class="datestr">at November 08, 2020 11:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/162">TR20-162 |  High-Probability List-Recovery, and Applications to Heavy Hitters | 

	Dean Doron, 

	Mary Wootters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An error correcting code $\mathcal{C} \colon \Sigma^k \to \Sigma^n$ is list-recoverable from input list size $\ell$ if for any sets $\mathcal{L}_1, \ldots, \mathcal{L}_n \subseteq \Sigma$ of size at most $\ell$, one can efficiently recover the list $\mathcal{L} = \{ x \in \Sigma^k : \forall j \in [n],  \mathcal{C}(x)_j \in \mathcal{L}_j \}$.  While list-recovery has been well-studied in error correcting codes, all known constructions with "efficient" algorithms are not efficient in the parameter $\ell$.  In this work, motivated by applications in algorithm design and pseudorandomness, we study list-recovery with the goal of obtaining a good dependence on $\ell$.  We make a step towards this goal by obtaining it in the weaker case where we allow a randomized encoding map and a small failure probability, and where the input lists are derived from unions of codewords.  As an application of our construction, we give a data structure for the heavy hitters problem in the strict turnstile model that, for some parameter regimes, obtains stronger guarantees than known constructions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/162"><span class="datestr">at November 08, 2020 11:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1436">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1436">News for October 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Sorry for the delay in writing this monthly digest: we hope you didn’t spend the week frantically refreshing your browsers! We found four papers this month: let’s dive in.</p>



<p><strong>A Structural Theorem for Local Algorithms with Applications to Coding, Testing, and Privacy</strong>, by Marcel Dall’Agnol, Tom Gur, and Oded Lachish (<a href="https://eccc.weizmann.ac.il/report/2020/154/">ECCC</a>). This paper introduces the notion of “robust (local) algorithm,” an abstraction which encompasses may types of algorithms: e.g., property testing algorithms, locally decodable codes, etc. The main result of this work is that any (possibly adaptive) \(q\)-query robust local algorithm can be transformed into a non-adaptive, <em>sample-based</em> one making \(n^{1-1/(q^2\log q)}\) queries (where \(n\) is the input size). Here, “sample-based” means that the algorithm doesn’t get to make arbitrary queries, but just gets to observe randomly chosen coordinates of the input. As application of this result, the authors derive new upper and lower bounds for several of the types of local algorithms mentioned above, resolving open questions from previous works.</p>



<p><strong>Testing Tail Weight of a Distribution Via Hazard Rate</strong>, by Maryam Aliakbarpour, Amartya Shankha Biswas, Kavya Ravichandran, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2010.02888">arXiv</a>). The authors consider, from the point of view of distribution testing, the question of deciding whether data is <em>heavy-tailed</em>: as would, for instance, data following a power law. The paper first sets out to formalize the question, and discusses various possible definitional choices before setting on one; after which it provides a test, and analyzes its sample complexity as a function of various parameters (such as smoothness of the unknown distribution). The authors finally back these results with empirical evaluation of their algorithm.</p>



<p><strong>On Testing of Samplers</strong>, by Kuldeep S. Meel, Yash Pote, Sourav Chakraborty (<a href="https://arxiv.org/abs/2010.12918">arXiv</a>). Suppose you are given a (known) distribution \(p\) over some domain \(\Omega\), and want to sample from it conditioned on some predicate \(\varphi\). Now someone comes to you with an algorithm which does exactly that, efficiently, and cheaply: great! But can you easily check that you’re not getting fooled, and that this sampler actually does what it claims? This paper provides this: an algorithm which accepts if the sampled distribution is \(\varepsilon\)-close to what it should (roughly, in a multiplicative, KL divergence sense), and rejects if it’s \(\varepsilon’\)-far (in total variation distance). The number of samples required is polynomial in \(\varepsilon’-\varepsilon\), and depends on some characteristic of \(p\) and \(\varphi\), the “tilt” (ratio between max and min probability of the conditional distribution).</p>



<p>Finally, an omission from late September:<br /><strong>Sample optimal Quantum identity testing via Pauli Measurements</strong>, by Nengkun Yu (<a href="https://arxiv.org/abs/2009.11518">arXiv</a>). The abstract is concise and clear enough to speak for itself: “In this paper, we show that \(\Theta(\textrm{poly}(n)\cdot\frac{4^n}{\varepsilon^2})\) is the sample complexity of testing whether two \(n\)-qubit quantum states \(\rho\) and \(\sigma\) are identical or \(\varepsilon\)-far in trace distance using two-outcome Pauli measurements.”</p>



<p>Please let us know if you spotted a paper we missed!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1436"><span class="datestr">at November 08, 2020 04:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5088">Five Thoughts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>(1) A friend commented that Biden’s victory becomes more impressive after you contemplate the enthusiasm gap: Trump’s base believed that Trump was sent by God, whereas Biden’s base believed that Biden probably wasn’t a terrible human being.  I replied that what we call the “Enlightenment” was precisely this, the switch from cowering before leaders who were sent by God to demanding leaders who probably aren’t terrible human beings.</p>



<p>(2) I would love for Twitter to deactivate Trump’s account—<em>not</em> for any ideological reason, simply for Trump’s hundreds of past violations of Twitter’s Terms of Service, and for there no longer being a compelling public interest in what Trump has to say that would override all his Terms of Service violations.</p>



<p>(3) When Biden appeared last night, and then again tonight, it wasn’t merely that he came across like a President-Elect of the US, but rather that he came across like a President-Elect of the US <em>who’s filling a vacant position</em>.  Until Biden starts, there won’t <em>be</em> a president of the US; there will only continue to be the president of those who voted for him.</p>



<p>(4) Now that Trump has gone this far in shattering all the norms of succession, part of me <em>wants</em> to see him go the rest of the way … to being physically dragged out of the Oval Office by Secret Service agents on January 20, in pathetic and humiliating footage that would define how future generations remembered him.</p>



<p>(5) I had an idea for something that could make a permanent contribution to protecting liberal democracy in the US, and that anti-Trump forces could implement unilaterally for a few tens of millions of dollars—no need to win another election.  The idea is to build a Donald J. Trump Historical Museum in Washington, DC.  But, you see, this museum would effectively be the opposite of a presidential library.  It would be designed by professional historians; they might solicit cooperation from former members of Trump’s inner circle, but would never depend on it.  It would, in fact, be a museum that teenage students might tend to be taken to on the same DC field trips that also brought them to the Vietnam Memorial and the United States Holocaust Memorial Museum (USHMM).  Obviously, the new museum would be <em>different</em> from those bleak places; it would (thankfully) have a little less tragedy and more farce … and that’s precisely the role that the new museum would fill.  To show the kids on the field trips that it’s not <em>always</em> unmitigated horribleness, that here was a case where we Americans took a gigantic stumble backwards, seeming to want to recreate the first few rooms in the USHMM exhibition, the one where the macho-talking clown thrills Germany by being serious rather than literal.  <em>But then, here in the US, we successfully stopped it before it got to the later rooms</em>.  Sure, the victory wasn’t as decisive as we would’ve liked, it came at a great cost, but it was victory nonetheless.  A 244-year-old experiment in self-governance is back in operation.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5088"><span class="datestr">at November 08, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=64">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/cauchy-residue-formula/">The Cauchy residue trick: spectral analysis made “easy”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In many areas of machine learning, statistics and signal processing, eigenvalue decompositions are commonly used, e.g., in principal component analysis, spectral clustering, convergence analysis of Markov chains, convergence analysis of optimization algorithms, low-rank inducing regularizers, community detection, seriation, etc.</p>



<p class="justify-text">Understanding how the spectral decomposition of a matrix changes as a function of a matrix is thus of primary importance, both algorithmically and theoretically. We thus need a perturbation analysis or more generally some differentiability properties for eigenvalues or eigenvectors [1], or any spectral function [2]. These properties can be obtained from many angles, but a generic tool can be used for all of these: it is a surprising and elegant application of Cauchy’s residue formula, which is due to Kato [3].</p>



<p class="justify-text">Before diving into spectral analysis, I will first present the Cauchy residue theorem and some nice applications in computing integrals that are needed in machine learning and kernel methods.</p>



<h2>Cauchy residue formula </h2>



<p class="justify-text">A function \(f : \mathbb{C} \to \mathbb{C}\) is said <em>holomorphic</em> in \(\lambda \in \mathbb{C}\) with derivative \(f'(\lambda) \in \mathbb{C}\), if is differentiable in \(\lambda\), that is if \(\displaystyle \frac{f(z)-f(\lambda)}{z-\lambda}\) tends to \(f'(\lambda)\) when \(z\) tends to \(\lambda\). Many classical functions are holomorphic on \(\mathbb{C}\) or portions thereof, such as the exponential, sines, cosines and their hyperbolic counterparts, rational functions, portions of the logarithm.</p>



<p class="justify-text">We consider a function which is holomorphic in a region of \(\mathbb{C}\) except in \(m\) values \(\lambda_1,\dots,\lambda_m \in \mathbb{C}\), which are usually referred to as <em>poles</em>. We also consider a simple closed directed contour \(\gamma\) in \(\mathbb{C}\) that goes strictly around the \(m\) values above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="549" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic-1-1024x440.png" class="wp-image-5002" height="236" /></figure></div>



<p class="justify-text">The Cauchy residue formula gives an explicit formula for the contour integral along \(\gamma\):<br /> $$ \oint_\gamma f(z) dz = 2 i \pi \sum_{j=1}^m {\rm Res}(f,\lambda_j), \tag{1}$$<br /> where \({\rm Res}(f,\lambda)\) is called the <em>residue</em> of \(f\) at \(\lambda\) . If around \(\lambda\),  \(f(z)\) has a series expansions in powers of \((z − \lambda)\), that is, \(\displaystyle f(z) =  \sum_{k=-\infty}^{+\infty}a_k (z −\lambda)^k\), then \({\rm Res}(f,\lambda)=a_{-1}\).</p>



<p class="justify-text">For example, if \(\displaystyle f(z) =   \frac{g(z)}{z-\lambda}\) with \(g\) holomorphic around \(\lambda\), then \({\rm Res}(f,\lambda) = g(\lambda)\), and more generally, if \(\displaystyle f(z) = \frac{g(z)}{(z-\lambda)^k}\) for \(k \geqslant  1\), then \(\displaystyle {\rm Res}(f,\lambda) = \frac{g^{(k-1)}(\lambda) }{(k-1)!}\). For more details on complex analysis, see [4].</p>



<p class="justify-text">The result above can be naturally extended to vector-valued functions (and thus to any matrix-valued function), by applying the identity to all components of the vector.</p>



<p class="justify-text">This result is due to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> [<a href="https://archive.org/details/mmoiresurlesin00cauc">10</a>] in 1825. The <a href="https://archive.org/details/mmoiresurlesin00cauc">original paper</a> where this is presented is a nice read in French where you can find some pepits like “la fonction s’évanouit pour \(x = \infty\)”.</p>



<p>If you are already familiar with complex residues, you can skip the next section.</p>



<h2>Where does it come from?</h2>



<p class="justify-text">At first, the formula in Eq. (1) seems unsettling. Why doesn’t the result depend more explicitly on the contour \(\gamma\)? Where does the multiplicative term \( {2i\pi}\) come from? Here is a very partial and non rigorous account (go to the <a href="https://terrytao.wordpress.com/tag/residue-theorem/">experts</a> for more rigor!).</p>



<p class="justify-text">Complex-valued functions on \(\mathbb{C}\) can be seen as functions from \(\mathbb{R}^2\) to itself, by writing $$ f(x+iy) = u(x,y) + i v(x,y),$$ where \(u\) and \(v\) are real-valued functions. We have thus a function \((x,y) \mapsto (u(x,y),v(x,y))\) from \(\mathbb{R}^2\) to \(\mathbb{R}^2\). Expanding \(f(z+dz) = f(z) + f'(z) dz\), which is the definition of complex differentiability, into real and imaginary parts, we get (using \(i^2 = -1\)): $$\left\{ \begin{array}{l} u(x+dx,y+dy) = u(x,y) + {\rm Re}(f'(z)) dx\ – {\rm Im}(f'(z)) dy \\ v(x+dx,y+dy) = v(x,y) + {\rm Re}(f'(z)) dy + {\rm Im}(f'(z)) dx. \end{array}\right.$$ This leads to $$\left\{ \begin{array}{l} \displaystyle \frac{\partial u}{\partial x}(x,y) = {\rm Re}(f'(z)) \\ \displaystyle \frac{\partial u}{\partial y}(x,y) = \ – {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial x}(x,y) = {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial y}(x,y) = {\rm Re}(f'(z)). \end{array}\right.$$</p>



<p class="justify-text">This in turn leads to the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a> \(\displaystyle \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}\) and \(\displaystyle \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}\), which are essentially necessary and sufficient conditions to be holomorphic. Thus holomorphic functions correspond to differentiable functions on \(\mathbb{R}^2\) with some equal partial derivatives. These equations are key to obtaining the Cauchy residue formula.</p>



<p class="justify-text"><strong>Contour integral with no poles. </strong>We first consider a contour integral over a contour \(\gamma\) enclosing a region \(\mathcal{D}\) where the function \(f\) is holomorphic everywhere. The contour \(\gamma\) is defined as a differentiable function \(\gamma: [0,1] \to \mathbb{C}\), and the integral is equal to $$\oint_\gamma f(z) dz = \int_0^1 \!\!f(\gamma(t)) \gamma'(t) dt = \int_0^1 \!\![u(x(t),y(t)) +i v(x(t),y(t))] [ x'(t) + i y'(t)] dt,$$ where \(x(t) = {\rm Re}(\gamma(t))\) and \(y(t) = {\rm Im}(\gamma(t))\). By expanding the product of complex numbers, it is thus equal to $$\int_0^1 [ u(x(t),y(t)) x'(t) \ – v(x(t),y(t))y'(t)] dt +i  \int_0^1 [ v(x(t),y(t)) x'(t) +u (x(t),y(t))y'(t)] dt,$$ which we can rewrite in compact form as (with \(dx = x'(t) dt\) and \(dy = y'(t)dt\)): $$\oint_\gamma ( u \, dx\  – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ).$$ We can then use <a href="https://en.wikipedia.org/wiki/Green%27s_theorem">Green’s theorem</a> because our functions are differentiable on the entire region \(\mathcal{D}\) (the set “inside” the contour), to get $$\oint_\gamma ( u \, dx\ – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ) =\  – \int\!\!\!\!\int_\mathcal{D} \! \Big( \frac{\partial v}{\partial x} + \frac{\partial u}{\partial y} \Big) dx dy \ – i \!\! \int\!\!\!\!\int_\mathcal{D} \!\Big( \frac{\partial u}{\partial x} – \frac{\partial v}{\partial y} \Big) dx dy.$$ Thus, because of the Cauchy-Riemann equations, the contour integral is always zero within the domain of differentiability of \(f\). Note that this extends to piecewise smooth contours \(\gamma\).</p>



<p class="justify-text"><strong>Circle and rational functions.</strong> For a circle contour of center \(\lambda \in \mathbb{C}\) and radius \(r\), we have, with \(\gamma(t) = \lambda + re^{ 2i \pi t}\): $$\oint_{\gamma} \frac{dz}{(z-\lambda)^k} =\int_0^{1} \frac{ 2r i \pi e^{2i\pi t}}{ r^k e^{2i\pi kt}}dt= \int_0^{1} r^{1-k} i e^{2i\pi (1-k)t} dt,$$ which is equal to zero if \(k \neq 1\), and to \(\int_0^{1} 2i\pi dt = 2 i \pi\) for \(k =1\). Thus, for a function with a series expansion, the Cauchy residue formula is true for the circle around a single pole, because only the term in \(\frac{1}{z-\lambda}\) contributes.</p>



<p class="justify-text"><strong>No dependence on the contour.</strong> Now that the Cauchy formula is true for the circle around a single pole, we can “deform” the contour below to a circle.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="398" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle-1024x532.png" class="wp-image-5021" height="207" /></figure></div>



<p>This can be done considering two contours \(\gamma_1\) and \(\gamma_2\) below with no poles inside, and thus with zero contour integrals, and for which the integrals along the added lines cancel.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="391" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle_cut-1-1024x537.png" class="wp-image-5022" height="205" /></figure></div>



<p class="justify-text">This “shows” that the integral does not depend on the contour, and so in applications we can be quite liberal in the choice of contour. Note that similar constructions can be used to take into account several poles.</p>



<p class="justify-text">Before going to the spectral analysis of matrices, let us explore some cool choices of contours and integrands, and (again!) some positive definite kernels.</p>



<h2>Classical examples</h2>



<p class="justify-text">The Cauchy residue theorem can be used to compute integrals, by choosing the appropriate contour, looking for poles and computing the associated residues. Here are classical examples, before I show applications to kernel methods. See more examples in <a href="http://residuetheorem.com/">http://residuetheorem.com/</a>, and many in [11].</p>



<p class="justify-text"><strong>Fourier transforms. </strong> For \(\omega&gt;0\), we can compute \( \displaystyle \int_{-\infty}^\infty \!\! f(x) e^{ i \omega x} dx\) for holomorphic functions \(f\) by integrating on the real line and a big upper circle as shown below, with \(R\) tending to infinity (so that the contribution of the half-circle goes to zero because of the exponential term). This leads to \(2i \pi\) times the sum of all residues of the function \(z \mapsto f(z) e^{ i \omega z}\) in the upper half plane. See an example below related to kernel methods.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="385" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_upper-circle-2-1024x570.png" class="wp-image-5026" height="214" /></figure></div>



<p class="justify-text"><strong>Trigonometric integrals</strong>. For holomorphic functions \(Q\), we can compute the integral \(\displaystyle \int_0^{2\pi} \!\!\! Q(\cos \theta, \sin \theta) d\theta\). Indeed, letting \(f(z) = \frac{1}{iz} Q\big( \frac{z+z^{-1}}{2}, \frac{z-z^{-1}}{2i} \big)\), it is exactly equal to the integral on the unit circle. The desired integral is then equal to \(2i\pi\) times the sum of all residues of \(f\) within the unit disk.</p>



<p class="justify-text">For example, when \(Q(\cos \theta, \sin \theta) = \frac{1}{2 + \sin \theta}\), we have \(f(z) = \frac{2}{z^2+4iz-1}\), with a single pole inside the unit circle, namely \(\lambda = i ( \sqrt{3}-2)\), and residue equal to \(-i / \sqrt{3}\), leading to \(\int_0^{2\pi} \frac{d\theta}{2+\sin \theta} = \frac{2\pi}{\sqrt{3}}\).</p>



<p class="justify-text"><strong>Series.</strong> If the function \(f\) is holomorphic and has no poles at integer real values, and satisfies some basic boundedness conditions, then $$\sum_{n \in \mathbb{Z}} f(n) = \ – \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{\cos \pi z}{\sin \pi z} ,\lambda\big).$$ This is a simple consequence of the fact that the function \(z \mapsto \pi \frac{\cos \pi z}{\sin \pi z}\) has all integers \(n \in \mathbb{Z}\) as poles, with corresponding residue equal to \(1\). This is obtained from the contour below with \(m\) tending to infinity.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="284" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_square-1-1024x903.png" class="wp-image-5028" height="250" /></figure></div>



<p class="justify-text">The same trick can be applied to  \(\displaystyle \sum_{n \in \mathbb{Z}} (-1)^n f(n) =\  –  \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{1}{\sin \pi z} ,\lambda\big).\) See [7, Section 11.2] for more details. Experts will see an interesting link with the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula">Euler-MacLaurin formula</a> and <a href="https://en.wikipedia.org/wiki/Bernoulli_polynomials">Bernoulli polynomials</a>.</p>



<p class="justify-text"><strong>Applications to kernel methods.</strong> In non-parametric estimation, regularization penalties are used to constrain real-values functions to be smooth. One such examples are combinations of squared \(L_2\) norms of derivatives. For functions \(f\) defined on an interval \(I\) of the real line, penalties are typically of the form \(\int_I \sum_{k=0}^s \alpha_k | f^{(k)}(x)|^2 dx\), for non-negative weights \(\alpha_0,\dots,\alpha_k\). For these Sobolev space norms, a positive definite kernel \(K\) can be used for estimation (see, e.g., <a href="https://francisbach.com/hermite-polynomials/">last month blog post</a>). </p>



<p class="justify-text">A classical question is: given the norm defined above, how to compute \(K\)? For \(I = \mathbb{R}\), then this can be done using Fourier transforms as: $$K(x,y) =  \frac{1}{2\pi} \int_\mathbb{R} \frac{e^{i\omega(x-y)}}{\sum_{k=0}^s \alpha_k \omega^{2k}} d\omega.$$ This is exactly an integral of the form above, for which we can use the contour integration technique. For example, for \(\alpha_0=1\) and \(\alpha_1=a^2\), we get for \(x-y&gt;0\), one pole \(i/a\) in the upper half plane for the function \(\frac{1}{1+a^2 z^2} = \frac{1}{(1+iaz)(1-iaz)}\), with residue \(-\frac{i}{2a} e^{-(x-y)/a}\), leading to the familiar exponential kernel \(K(x,y) = \frac{1}{2a} e^{-|x-y|/a}\). More complex kernels can be considered (see, e.g., [8, page 277], for \(\sum_{k=0}^s \alpha_k \omega^{2k} = 1 + \omega^{2s}\)).</p>



<p class="justify-text">We can also consider the same penalty on the unit interval \([0,1]\) with periodic functions, leading to the kernel (see [9] for more details): $$ K(x,y) = \sum_{n \in \mathbb{Z}} \frac{ e^{2in\pi(x-y)}}{\sum_{k=0}^s \alpha_k( 2n\pi)^s}.$$ For the same example as above, that is, \(\alpha_0=1\) and \(\alpha_1=a^2\), this leads to an infinite series on which we can apply the Cauchy residue formula as explained above. This leads to, for \(x-y \in [0,1]\), \(K(x,y) =  \frac{1}{2a}  \frac{ \cosh (\frac{1-2(x-y)}{2a})}{\sinh (\frac{1}{2a})}\). We can then extend by \(1\)-periodicity to all \(x-y\). See the detailed computation at the end of the post.</p>



<p>Now that you are all experts in residue calculus, we can move on to spectral analysis.</p>



<h2>Spectral analysis of symmetric matrices</h2>



<p class="justify-text">We consider a symmetric matrix \(A \in \mathbb{R}^{n \times n}\), with its \(n\) ordered real eigenvalues \(\lambda_1 \geqslant  \cdots \geqslant \lambda_n\), counted with their orders of multiplicity, and an orthonormal basis of their eigenvectors \(u_j \in \mathbb{R}^n\), \(j=1,\dots,n\). We have \(A = \sum_{j=1}^n \lambda_j u_j u_j^\top\).  When we consider eigenvalues as functions of \(A\), we use the notation \(\lambda_j(A)\), \(j=1,\dots,n\). These functions are always well-defined even when eigenvalues are multiple (this is not the case for eigenvectors because of the invariance by orthogonal transformations).</p>



<p class="justify-text">The key property that we will use below is that we can express the so-called resolvent matrix \((z I – A)^{-1} \in \mathbb{C}^{n \times n}\), for \(z \in \mathbb{C}\), as: $$  (z I- A)^{-1}  = \sum_{j=1}^n \frac{1}{z-\lambda_j} u_j u_j^\top. $$ The dependence on \(z\) of the form \( \displaystyle \frac{1}{z- \lambda_j}\)  leads to a nice application of Cauchy residue formula.</p>



<p class="justify-text">Assuming the \(k\)-th eigenvalue \(\lambda_k\) is simple, we consider the contour \(\gamma\) going strictly around \(\lambda_k\) like below (for \(k=5\)).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="431" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_single_eigenvalue-1024x433.png" class="wp-image-5039" height="182" /></figure></div>



<p class="justify-text">We consider integrating the matrix above, which leads to: $$ \oint_\gamma<br />  (z I- A)^{-1} dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{1}{z – \lambda_j} dz \Big) u_j u_j^\top<br />  = 2 i \pi \  u_k u_k^\top $$  using the identity \(\displaystyle \oint_\gamma \frac{1}{z – \lambda_j} dz = 1\) if \(j=k\) and \(0\) otherwise (because the pole is outside of \(\gamma\)). We thus obtain an expression for projectors on the one-dimensional eigen-subspace associated with the eigenvalue \(\lambda_k\).</p>



<p class="justify-text">With simple manipulations, we can also access the eigenvalues. Indeed, we have: $$ \oint_\gamma<br />   (z I- A)^{-1} z dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{z}{z – \lambda_j} dz \Big) u_j u_j^\top<br />  = 2 i \pi \lambda_k  u_k u_k^\top, $$ and by taking the trace, we obtain $$ \oint_\gamma<br />  {\rm tr} \big[ z (z I- A)^{-1} \big]  dz    = \lambda_k. $$ The key benefit of these representations is that when the matrix \(A\) is slightly perturbed, then the same contour \(\gamma\) can be used to enclose the corresponding eigenvalues of the perturbed matrix, and perturbation results are simply obtained by taking gradients within the contour integral. Note that several eigenvalues may be summed up by selecting a contour englobing more than one eigenvalues.</p>



<h2>Gradients of eigenvalues</h2>



<p class="justify-text">The expression with contour integrals allows to derive simple formulas for gradients of eigenvalues. These can be obtained by other means [5], but using contour integrals shows that this is simply done by looking at the differential of \((z I – A)^{-1}\) and integrating it. The central component is the following expansion, which is a classical result in matrix differentiable calculus, with \(\|\Delta\|_2\) the operator norm of \(\Delta\) (i.e., its largest singular value):  $$<br /> (z I- A – \Delta)^{-1} = (z I – A)^{-1} + (z I- A)^{-1} \Delta (z I- A)^{-1} + o(\| \Delta\|_2).  $$ Note here that the asymptotic remainder \(o(\| \Delta\|_2)\) can be made explicit. </p>



<p class="justify-text">By expanding the expression on the basis of eigenvectors of \(A\), we get  $$<br /> z (z I- A – \Delta)^{-1}  – z (z I- A)^{-1}  =  \sum_{j=1}^n \sum_{\ell=1}^n u_j u_\ell^\top  \frac{ z \cdot u_j^\top \Delta u_\ell}{(z-\lambda_j)(z-\lambda_\ell)} + o(\| \Delta \|_2).<br /> $$ Taking the trace, the cross-product terms \({\rm tr}(u_j u_\ell^\top) =  u_\ell^\top u_j\) disappear for \(j \neq \ell\), and we get: $$<br /> {\rm tr} \big[ z (z I – A – \Delta)^{-1}  \big] – {\rm tr} \big[ z (z I – A)^{-1}  \big]=  \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} + o(\| \Delta \|_2).<br /> $$ This leads to, by contour integration:<br />$$<br /> \lambda_{k}(A+\Delta) -\lambda_k(A)<br /> =<br /> \frac{1}{2i \pi} \oint_\gamma \Big[ <br />   \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} \Big] dz +  o(\| \Delta \|_2).<br /> $$ By keeping only the pole \(\lambda_k\) which is inside the contour \(\gamma\), we get  $$ \lambda_{k}(A+\Delta) -\lambda_k(A)<br />  =<br /> \frac{1}{2i \pi} \oint_\gamma \Big[ <br />    \frac{ z \cdot u_k^\top \Delta u_k}{(z-\lambda_k)^2} \Big] dz +  o(| \Delta |_2) \<br />    =  u_k^\top \Delta u_k + o(\| \Delta \|_2),<br /> $$ using the identity \(\displaystyle <br />  \oint_\gamma \frac{z dz}{(z – \lambda_k)^2} dz = <br />   \oint_\gamma \Big( \frac{\lambda_k}{(z – \lambda_k)^2}  + \frac{1}{z – \lambda_k} \Big) dz = 1\).  </p>



<p class="justify-text">Thus the gradient of \(\lambda_k\) at a matrix \(A\) where the \(k\)-th eigenvalue is simple is simply \( u_k u_k^\top\), where \(u_k\) is a corresponding eigenvector. Note that this result can be simply obtained by the simple (rough) calculation: if \(x\) is a unit eigenvector of \(A\), then \(Ax =\lambda x\), and \(x^\top x = 1\), leading to \(x^\top dx = 0\) and \(dA\ x + A dx = d\lambda \ x + \lambda dx\), and by taking the dot product with \(x\), \(d\lambda = x^\top dA\ x + x^\top A dx = x^\top dA \ x + \lambda x^\top  dx = x^\top dA \ x\), which is the same result. However, this reasoning is more cumbersome, and does not lead to neat approximation guarantees, in particular in the extensions below.</p>



<h2>Other perturbation results</h2>



<p class="justify-text">Given the gradient, other more classical perturbation results could de derived, such as Hessians of eigenvalues, or gradient of the projectors \(u_k u_k^\top\).  Here I derive a perturbation result for the projector \(\Pi_k(A)=u_k u_k^\top\), when \(\lambda_k\) is a simple eigenvalue. Using the same technique as above, we get: $$ \Pi_k(A+\Delta )\  – \Pi_k(A) = \frac{1}{2i \pi} \oint_\gamma (z I- A)^{-1} \Delta (z I – A)^{-1}dz  + o(\| \Delta\|_2),$$ which we can expand to the basis of eigenvectors as $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j=1}^n \sum_{\ell=1}^n u_j u_j^\top \Delta u_\ell u_\ell^\top \frac{  dz}{(z-\lambda_\ell) (z-\lambda_j)  } + o(\| \Delta\|_2).$$ We can then split in two, with the two terms (all others are equal to zero by lack of poles within \(\gamma\)): $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top)   \frac{  dz}{(z-\lambda_k) (z-\lambda_j)  }= \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top) \frac{1}{\lambda_k – \lambda_j}  $$ and $$\frac{1}{2i \pi} \oint_\gamma   u_k^\top \Delta u_k   u_k u_k^\top  \frac{  dz}{(z-\lambda_k)^2  } = 0 ,$$ finally leading to $$\Pi_k(A+\Delta ) \ – \Pi_k(A) =  \sum_{j \neq k}  \frac{u_j^\top \Delta u_k}{\lambda_k – \lambda_j}    ( u_j u_k^\top + u_k u_j^\top)    + o(\| \Delta\|_2),$$ from which we can compute the Jacobian of \(\Pi_k\).</p>



<h2>Spectral functions</h2>



<p class="justify-text">Spectral functions are functions on symmetric matrices defined as \(F(A) = \sum_{k=1}^n f(\lambda_k(A))\), for any real-valued function \(f\). For \(f(x) = x\), we get back the trace, for \(f(x) = \log x\) we get back the log determinant, and so on. The function \(F\) can be represented as $$F(A) = \sum_{k=1}^n f(\lambda_k(A)) = \frac{1}{2i \pi} \oint_\gamma f(z) {\rm tr} \big[ (z I  – A)^{-1} \big] dz,$$ where the contour \(\gamma\) encloses all eigenvalues (as shown below).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="438" alt="" src="https://francisbach.com/wp-content/uploads/2020/11/contour_all_eigenvalues-1024x433.png" class="wp-image-5087" height="185" /></figure></div>



<p class="justify-text">This representation can be used to compute derivatives of \(F\), by simple derivations, to obtain the same result as [<a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">12</a>].</p>



<h2>Singular values of rectangular matrices</h2>



<p class="justify-text">Singular value decompositions are also often used, for a rectangular matrix \(W \in \mathbb{R}^{n \times d}\). It consists in finding \(r\) pairs \((u_j,v_j) \in \mathbb{R}^{n} \times \mathbb{R}^d\), \(j=1,\dots,r\), of singular vectors and \(r\) positive singular values \(\sigma_1 \geqslant \cdots \geqslant \sigma_r &gt; 0\) such that \(W = \sum_{j=1}^r \sigma_j u_j v_j^\top\) and  \((u_1,\dots,u_r)\) and \((v_1,\dots,v_r)\) are orthonormal families.</p>



<p class="justify-text">There are two natural ways to relate the singular value decomposition to the classical eigenvalue decomposition of a symmetric matrix, �first through \(WW^\top\) (or similarly \(W^\top W\)). Here it is more direct to consider the so-called Jordan-Wielandt matrix, defined by blocks as $$<br /> \bar{W} = \left( \begin{array}{cc}<br />0 &amp; W \\<br />W^\top &amp; 0 \end{array} \right). $$ The matrix \(\bar{W}\) is symmetric, and its non zero eigenvalues are \(+\sigma_i\) and \(-\sigma_i\), \(i=1,\dots,r\), associated with the eigenvectors \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br />u_i \\ v_i \end{array} \right)\) and \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br />u_i \\ -v_i \end{array} \right)\).</p>



<p class="justify-text">All necessary results (derivatives of singular values \(\sigma_j\), or projectors \(u_j v_j^\top\) can be obtained from there); see more details, in, e.g., the appendix of [6].</p>



<h2>Going beyond</h2>



<p class="justify-text">In this post, I have shown various applications of the Cauchy residue formula, for computing integrals and for the spectral analysis of matrices. I have just scratched the surface of spectral analysis, and what I presented extends to many interesting situations, for example, to more general linear operators in infinite-dimensional spaces [3], or to the analysis fo the eigenvalue distribution of random matrices (see a nice and reasonably simple derivation of the semi-circular law from <a href="https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/">Terry Tao’s blog</a>).</p>



<h2>References</h2>



<p class="justify-text">[1] Gilbert W. Stewart and Sun Ji-Huang. <em>Matrix Perturbation Theory</em>. Academic Press, 1990.<br />[2] Adrian Stephen Lewis. Derivatives of spectral functions. <em>Mathematics of Operations Research</em>, 21(3):576–588, 1996. <br />[3] Tosio Kato. <em>Perturbation Theory for Linear Operators</em>, volume 132. Springer, 2013. <br />[4] Serge Lang. <em>Complex Analysis</em>, volume 103. Springer, 2013. <br />[5] Jan R. Magnus. On differentiating eigenvalues and eigenvectors. <em>Econometric Theory</em>, 1(2):179–191, 1985. <br />[6] Francis Bach. Consistency of trace norm minimization. <em>Journal of Machine Learning Research</em>, 9:1019-1048, 2008.<br />[7] Joseph Bak, Donald J. Newman. <em>Complex analysis</em>. New York: Springer, 2010.<br />[8] Alain Berlinet, and Christine Thomas-Agnan. <em>Reproducing kernel Hilbert spaces in probability and statistics</em>. Springer Science &amp; Business Media, 2011.<br />[9] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br />[10] Augustin Louis Cauchy, <a href="https://archive.org/details/mmoiresurlesin00cauc">Mémoire</a><a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf"> sur les intégrales définies, prises entre des limites imaginaires</a>, 1825, <a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf">re-published</a> in Bulletin des Sciences Mathématiques et Astronomiques, Tome 7, 265-304, 1874.<br />[11] Dragoslav S. Mitrinovic, and Jovan D. Keckic. <em>The Cauchy method of residues: theory and applications</em>. Vol. 9. Springer Science &amp; Business Media, 1984.<br />[12] Adrian S. Lewis, and Hristo S. Sendov. <a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">Twice differentiable spectral functions</a>. <em>SIAM Journal on Matrix Analysis and Applications</em> 23.2: 368-386, 2001.</p>



<h2>Computing the Sobolev kernel</h2>



<p class="justify-text">The goal is to compute the infinite sum $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2}$$ for \(q \in (0,1)\). We consider the function $$f(z) = \frac{e^{i\pi (2q-1) z}}{1+(2a \pi z)^2} \frac{\pi}{\sin (\pi z)}.$$ It is holomorphic on \(\mathbb{C}\) except at all integers \(n \in \mathbb{Z}\), where it has a simple pole with residue \(\displaystyle \frac{e^{i\pi (2q-1) n}}{1+(2a \pi n)^2} (-1)^n = \frac{e^{i\pi 2q n}}{1+(2a \pi n)^2}\), at \(z = i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{ – (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} = \ – \frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\), and at \(z = -i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{  (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} =\ – \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\). With all residues summing to zero (note that this fact requires a precise analysis of limits when \(m\) tends to infinity for the contour defined in the main text), we get: $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2} =\frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}+ \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))} = \frac{1}{2a} \frac{ \cosh (\frac{2q-1}{2a})}{\sinh (\frac{1}{2a})}.$$</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/cauchy-residue-formula/"><span class="datestr">at November 07, 2020 12:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/">faculty at University of Rochester (apply by January 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Department at the University of Rochester seeks applicants for tenure-track faculty positions. We are particularly eager to hire in theory, security/privacy/cryptography, quantum computing, data management, natural language processing, and machine learning. Candidates at any level of seniority are encouraged to apply.</p>
<p>Website: <a href="https://www.rochester.edu/faculty-recruiting/positions/show/10942">https://www.rochester.edu/faculty-recruiting/positions/show/10942</a><br />
Email: stefanko@cs.rochester.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/"><span class="datestr">at November 07, 2020 12:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=509">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/11/06/tcs-talk-wednesday-november-11-shuai-shao-uw-madison/">TCS+ talk: Wednesday, November 11 — Shuai Shao, UW-Madison</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Shuai Shao</strong> from UW-Madison will speak about “<em>A Dichotomy for Real Boolean Holant Problems</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: In this talk, we present a complexity dichotomy for Holant problems on the boolean domain with arbitrary sets of real-valued constraint functions. These constraint functions need not be symmetric nor do we assume any auxiliary functions as in previous results. It is proved that for every set <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="F" class="latex" title="F" /> of real-valued constraint functions, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BHolant%7D%28F%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\text{Holant}(F)" class="latex" title="\text{Holant}(F)" /> is either <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" />-time computable or <img src="https://s0.wp.com/latex.php?latex=%5C%23P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\#P" class="latex" title="\#P" />-hard. The classification has an explicit criterion. This is a culmination of much research on a decade-long classification program for Holant problems, and it uses previous results and techniques from many researchers.  However, as it turned out, the journey to the present theorem has been arduous. Some particularly intriguing concrete functions f6, f8 and their associated families with extraordinary closure properties related to Bell states in quantum information theory play an important role in this proof.  <br /><br />Based on joint work with Jin-Yi Cai.</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/11/06/tcs-talk-wednesday-november-11-shuai-shao-uw-madison/"><span class="datestr">at November 07, 2020 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5071">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5071">On the removal of a hideous growth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The title of this post is not an allegory.</p>



<p>At 10am this morning, I had a previously-scheduled appointment with an oral surgeon to remove a large, hideous, occasionally painful growth on the inside of my lower lip.  (I’d delayed getting it looked at for several months because of covid, but I no longer could.)</p>



<p>So right now I’m laying in bed at home, with gauze on my lips, dazed, hopped up on painkillers.  I regret that things ever got to the point where this was needed.  I believe, intellectually, that the surgeon executed about as competently as anyone could ask.  But I still wish, if we’re being honest, that there hadn’t been <em>quite</em> this much pain in the surgery or in the recovery from it.</p>



<p>Again intellectually, I know that there’s still lots more pain in the days ahead.  I’m not sure that whatever it was won’t just quickly grow back.  And yet, I couldn’t be feeling more joy through my whole body with every one of these words that I write.  At last I can honestly tell myself: the growth is gone.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5071"><span class="datestr">at November 06, 2020 07:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/161">TR20-161 |  Seed Protecting Extractors | 

	Gil Cohen, 

	Dean Doron, 

	Shahar Samocha</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce a new type of seeded extractors we dub seed protecting extractors. Informally, a seeded extractor is seed protecting against a class of functions $C$, mappings seeds to seeds, if the seed $Y$ remains close to uniform even after observing the output $\mathrm{Ext}(X,A(Y))$ for every choice of $A \in C$ (or, more generally, observing the outputs corresponding to several adversaries from $C$).

The results of this paper are structural. We establish what we believe to be surprising relations, in fact, equivalences between seed protecting extractors and each of the well-studied strengthenings of seeded extractors: strong extractors, non-malleable extractors (against permutations), and two-source extractors, where each case is classified by a suitable class $C$. Our work put forth a novel approach for constructing nonmalleable extractors against permutations. Indeed, the existing machinery developed for constructing non-malleable extractors focuses on the output and so it is aimed towards breaking correlations. Instead, our work suggests developing techniques for protecting the seed.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/161"><span class="datestr">at November 05, 2020 08:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/">Postdoc in Algorithms at Northwestern University and TTIC (apply by January 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Northwestern University (NU) &amp; Toyota Technology Institute at Chicago (TTIC) invite applications for postdoc fellowships to conduct research in approx algorithms, beyond-worst-case analysis, high-dimensional data analysis. Postdocs will work with Profs. S. Khuller, K. Makarychev, Y. Makarychev, and A. Vijayaraghavan. Positions are based at NU but postdocs are encouraged to spend some time at TTIC.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/17450">https://academicjobsonline.org/ajo/jobs/17450</a><br />
Email: nu.cstheory@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/"><span class="datestr">at November 04, 2020 07:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=69">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/">Monday, Nov 09 — Tal Rabin from University of Pennsylvania</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next Foundations of Data Science virtual talk will take place on Monday, Nov 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Tal Rabin </strong>from UPenn will speak about “<strong>You Only Speak Once — Secure MPC with Stateless Ephemeral Roles</strong>”.</p>



<p><strong>Abstract</strong>: The inherent difficulty of maintaining stateful environments over long periods of time gave rise to the paradigm of serverless computing, where mostly-stateless components are deployed on demand to handle computation tasks, and are teared down once their task is complete. Serverless architecture could offer the added benefit of improved resistance to targeted denial-of-service attacks. Realizing such protection,<br />requires that the protocol only uses stateless parties. Perhaps the most famous example of this style of protocols is the Nakamoto consensus protocol used in Bitcoin. We refer to this stateless property as the You-Only-Speak-Once (YOSO) property, and initiate the formal study of it within a new YOSO model. Our model is centered around the notion of roles, which are stateless parties that can only send a single message. Furthermore, we describe several techniques for achieving YOSO MPC; both computational and information theoretic.</p>



<p>The talk will be self contained.</p>



<p>Based on joint works with: Fabrice Benhamouda, Craig Gentry, Sergey Gorbunov, Shai Halevi, Hugo Krawczyk, Chengyu Lin, Bernardo Magri, Jesper Nielsen, Leo Reyzin, Sophia Yakoubov.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/"><span class="datestr">at November 04, 2020 03:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/04/postdoc-at-ut-san-antonio-apply-by-december-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/04/postdoc-at-ut-san-antonio-apply-by-december-1-2020/">Postdoc at UT San Antonio (apply by December 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We have a two year postdoctoral position at UT San Antonio in geometry, probability, optimization, and computing. Applications are on a rolling basis; to be considered in the first round please send your application by 12/01/20 using the mathjobs link.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/16692">https://www.mathjobs.org/jobs/list/16692</a><br />
Email: alperen.ergur@utsa.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/04/postdoc-at-ut-san-antonio-apply-by-december-1-2020/"><span class="datestr">at November 04, 2020 02:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7875">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/">Yet another backpropagation tutorial</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am teaching deep learing this week in Harvard’s CS 182 (Artificial Intelligence) course. As I’m preparing the back-propagation lecture, Preetum Nakkiran told me about <a href="https://github.com/karpathy/micrograd">Andrej Karpathy’s awesome micrograd package</a> which implements automatic differentiation for scalar variables in very few lines of code.</p>



<p>I couldn’t resist using this to show how simple back-propagation and stochastic gradient descents are. To make sure we leave nothing “under the hood” we will not import anything from the package but rather only copy paste the few things we need. I hope that the text below is generally accessible to anyone familiar with partial derivatives. See this <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing">colab notebook</a> for all the code in this tutorial. In particular, aside from libraries for plotting and copy pasting a few dozen lines from Karpathy this code uses absolutely no libraries (no numpy, no pytorch, etc..) and can train (slowly..) neural networks using stochastic gradient descent.  (This <a href="https://deepnote.com/publish/f898cdd4-4815-42ad-ba57-ae0b8b733492">notebook</a> builds the code more incrementally.)</p>



<p><strong>Automatic differentiation</strong> is a mechanism that allows you to write a Python functions such as</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def f(x,y): return (x+y)+x**3
</pre>


<p>and enables one to automatically obtain the partial derivatives <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7Bx%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial \text{x}}" class="latex" title="\tfrac{\partial f}{\partial \text{x}}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7By%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial \text{y}}" class="latex" title="\tfrac{\partial f}{\partial \text{y}}" />. Numerically we could do this by choosing some small value <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> and computing both <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2B%5Cdelta%2Cy%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}" class="latex" title="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2Cy%2B%5Cdelta%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}" class="latex" title="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}" />.<br />However, if we generalize this approach to <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> variables, we get an algorithm that requires roughly <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> evaluations of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />. <em>Back-propagation</em> enables computing <em>all</em> of the partial derivatives at only constant overhead over the cost of a single evaluation of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</p>



<h2>Back propagation and the chain rule</h2>



<p>Back-propagation is a direct implication of the <strong>multi-variate chain rule</strong>. Let’s illustrate this for the case of two variables. Suppose that <img src="https://s0.wp.com/latex.php?latex=v%2Cw%3A+%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v,w: \mathbb{R} \rightarrow \mathbb{R}" class="latex" title="v,w: \mathbb{R} \rightarrow \mathbb{R}" /> and <img src="https://s0.wp.com/latex.php?latex=z%3A%5Cmathbb%7BR%7D%5E2+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z:\mathbb{R}^2 \rightarrow \mathbb{R}" class="latex" title="z:\mathbb{R}^2 \rightarrow \mathbb{R}" /> are functions, and define</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28u%29+%3D+z%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u) = z(v(u),w(u))" class="latex" title="f(u) = z(v(u),w(u))" />.</p>



<p>That is, we have the following situation:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/KGVphzL.png" alt="" /></figure>



<p>where <img src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u)" class="latex" title="f(u)" /> is the value <img src="https://s0.wp.com/latex.php?latex=z%3Dz%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z=z(v(u),w(u))" class="latex" title="z=z(v(u),w(u))" /></p>



<p>Then the <strong>chain rule</strong> states that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+u%7D+%3D+%28+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )" class="latex" title="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )" /></p>



<p>You can take this on faith, but it also has a simple proof. To see the intuition, note that for small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" />, <img src="https://s0.wp.com/latex.php?latex=v%28u%2B%5Cdelta%29+%5Capprox+v%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)" class="latex" title="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)" /> and <img src="https://s0.wp.com/latex.php?latex=w%28u%2B%5Cdelta%29+%5Capprox+w%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)" class="latex" title="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)" />. For small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_1%2C%5Cdelta_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta_1,\delta_2" class="latex" title="\delta_1,\delta_2" />, <img src="https://s0.wp.com/latex.php?latex=z%28v%2B%5Cdelta_1%2Cw%2B%5Cdelta_2%29+%5Capprox+z%28v%2Cw%29+%2B+%5Cdelta_1+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D%28z%2Cw%29+%2B+%5Cdelta_2+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}" class="latex" title="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}" />. Hence, if we ignore terms with powers of delta two or higher,</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28u+%2B%5Cdelta%29%3D+z%28w%28u%2B%5Cdelta%29%2Cv%28u%2B%5Cdelta%29%29+%5Capprox+f%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}" class="latex" title="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}" /></p>



<p></p>



<p>The chain rule generalizes naturally to the case that <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> is a function of more variables than <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. Generally, if the value <img src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(u)" class="latex" title="f(u)" /> is obtained by first computing some intermediate values <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" /> from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and then computing <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in some arbitrary way from <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" />, then <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Csum_%7Bi%3D1%7D%5Ek+%5Ctfrac%7B%5Cpartial+v_i%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}" class="latex" title="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}" />.</p>



<p>As a corollary, if you already managed to compute the values <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_1%7D%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}" class="latex" title="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}" />, and you kept track of the way that <img src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v_1,\ldots,v_k" class="latex" title="v_1,\ldots,v_k" /> were obtained from <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />, then you can compute <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u}" class="latex" title="\tfrac{\partial z}{\partial u}" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/35jj2wz.png" alt="" /></figure>



<p>This suggests a simple recursive algorithm by which you compute the derivative of the final value <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> with respect to an intermediate value <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> in the computation using recursive calls to compute the values <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial w'}" class="latex" title="\tfrac{\partial z}{\partial w'}" /> for all the values <img src="https://s0.wp.com/latex.php?latex=w%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'" class="latex" title="w'" /> that were directly computed from <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />. Back propagation is this algorithm.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/11/backprop_example.png"><img src="https://windowsontheory.files.wordpress.com/2020/11/backprop_example.png?w=1024" alt="" class="wp-image-7887" /></a>Computing <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D%2C%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D%2C+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u},\tfrac{\partial z}{\partial v}, \tfrac{\partial z}{\partial w}" class="latex" title="\tfrac{\partial z}{\partial u},\tfrac{\partial z}{\partial v}, \tfrac{\partial z}{\partial w}" />  for the assignment <img src="https://s0.wp.com/latex.php?latex=u%3D5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u=5" class="latex" title="u=5" /> using back propagation</figure>



<h2>Implementing automatic differentiation using back propagation in Python</h2>



<p>We now describe how to do this in Python, following Karpathy’s code. The basic class we use is <code>Value</code>. Every member <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> of <code>Value</code> is a container that holds:</p>



<ol><li>The actual scalar (i.e., floating point) value that <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> holds. We call this <code>data</code>.</li><li>The gradient of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> with respect to some future unknown value <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that will use it. We call this <code>grad</code> and it is initialized to zero.</li><li>Pointers to all the values that were used in the computation of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. We call this <code>_prev</code></li><li>The method that adds (using the current value of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and other values) the contribution of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> to the gradient of all its previous values <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> to their gradients. We call this function <code>_backward</code>. Specifically, at the time we call <code>_backward</code> we assume that <code>u.grad</code> already contains <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u}" class="latex" title="\tfrac{\partial z}{\partial u}" /> where <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> is the final value we are interested in. For every value <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> that was used to compute <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />, we add to <code>v.grad</code> the quantity <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+u%7D%7B%5Cpartial+v%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}" class="latex" title="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}" />. For the latter quantity we need to keep track of <em>how</em> <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> was computed from <img src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b" class="latex" title="b" />.</li><li>If we call the method <code>backwards</code> (without an underscore) on a variable <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> then this will compute the derivative of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> with respect to <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> for all values <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> that were used in the computation of <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. We do this by applying <code>_backward</code> to <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and then recursively (just like in DFS) going over the “children” (values used to compute <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />), calling <code>_backward</code> on each one and keeping track the ones we visited just like the Depth First Search (DFS) algorithm.</li></ol>



<p>Let’s now describe this in code. We start off with a simple version that only supports addition and multiplication. The constructor for the class is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Value:
    """ stores a single scalar value and its gradient """

    def __init__(self, data, _children=()):
        self.data = data
        self.grad = 0
        self._backward = lambda: None
        self._prev = set(_children)
</pre>


<p>which fairly directly matches the description above. This constructor creates a value not using prior ones, which is why the <code>_backward</code> function is empty.<br />However, we can also create values by adding or multiplying prior ones, by adding the following methods:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">  def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other))

        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward

        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other))

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward

        return out
</pre>


<p>That is, if we create <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> by adding the values <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />, then the <code>_backward</code> function of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> works by adding <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w}" class="latex" title="= \tfrac{\partial z}{\partial w}" /> to both <code>u.grad</code> and <code>v.grad</code>.<br />If we <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> is obtain by multiplying <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> then we add <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cdot" class="latex" title="\cdot" /> <code>v.data</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w} v" class="latex" title="= \tfrac{\partial z}{\partial w} v" /> to <code>u.grad</code> and similarly add <code>w.grad</code> <img src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cdot" class="latex" title="\cdot" /> <code>u.data</code> <img src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="= \tfrac{\partial z}{\partial w} u" class="latex" title="= \tfrac{\partial z}{\partial w} u" /> to <code>v.grad</code>.</p>



<p>The <code>backward</code> function is obtained by setting the gradient of the current value to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> and then running <code>_backwards</code> on all other values in reverse topological order:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def backward(self, visited= None): # slightly shorter code to fit in the blog 
    if visited is None:
        visited= set([self])
        self.grad = 1
    self._backward()
    for child in self._prev:
        if not child in visited:
            visited.add(child)
            child.backward(visited)
</pre>


<p>For example, if we run the following code</p>


<pre class="brush: python; gutter: false; title: ; notranslate">a = Value(5)
print(a.grad)
def f(x): return (x+2)**2 + x**3
f(a).backward()
print(a.grad)
</pre>


<p>then the values printed will be <code>0</code> and <code>89</code> since the derivative of <img src="https://s0.wp.com/latex.php?latex=%28x%2B2%29%5E2+%2B+x%5E3+%3D+x%5E3+%2B+x%5E2+%2B+4x+%2B+4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4" class="latex" title="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4" /> equals <img src="https://s0.wp.com/latex.php?latex=3x%5E2+%2B+2x+%2B42&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3x^2 + 2x +42" class="latex" title="3x^2 + 2x +42" />.</p>



<p>In the <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing#scrollTo=0_nveKyfxXQK">notebook</a> you can see that we implement also the power function, and have some “convenience methods” (division etc..).</p>



<h3>Linear regression using back propagation and stochastic gradient descent</h3>



<p>In <em>stochastic gradient descent</em> we are given some data <img src="https://s0.wp.com/latex.php?latex=%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_1,y_1),\ldots,(x_n,y_n)" class="latex" title="(x_1,y_1),\ldots,(x_n,y_n)" /> and want to find an hypothesis <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> that minimizes the empirical loss <img src="https://s0.wp.com/latex.php?latex=L%28h%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+L%28h%28x_i%29%2Cy_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)" class="latex" title="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)" /> where <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> is a <em>loss function</em> mapping two labels <img src="https://s0.wp.com/latex.php?latex=y%2C+y%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y, y'" class="latex" title="y, y'" /> to a real number. If we let <img src="https://s0.wp.com/latex.php?latex=L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_i(h)" class="latex" title="L_i(h)" /> be the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />-th term of this sum, then, identifying <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> with the parameters (i.e., real numbers) that specify it, stochastic gradient descent is the following algorithm:</p>



<ol><li>Set <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> to be a random vector. Set <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> to be some small number (e.g., <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3D+0.1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta = 0.1" class="latex" title="\eta = 0.1" />)</li><li>For <img src="https://s0.wp.com/latex.php?latex=t+%5Cin+%7B1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \in {1,\ldots, T}" class="latex" title="t \in {1,\ldots, T}" /> (where <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> is the number of <em>epochs</em>):</li></ol>



<ul><li>For <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%7B1%2C%5Cldots%2C+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i \in {1,\ldots, n}" class="latex" title="i \in {1,\ldots, n}" />: (in random order)<ul><li>Let <img src="https://s0.wp.com/latex.php?latex=h+%5Cleftarrow+h+-+%5Ceta+%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \leftarrow h - \eta \nabla_h L_i(h)" class="latex" title="h \leftarrow h - \eta \nabla_h L_i(h)" /></li></ul></li></ul>



<p>If <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" title="h" /> is specified by the parameters <img src="https://s0.wp.com/latex.php?latex=h_1%2C%5Cldots%2Ch_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h_1,\ldots,h_k" class="latex" title="h_1,\ldots,h_k" /> <img src="https://s0.wp.com/latex.php?latex=%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla_h L_i(h)" class="latex" title="\nabla_h L_i(h)" /> is the vector <img src="https://s0.wp.com/latex.php?latex=%28+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_1%7D%28h%29%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_2%7D%28h%29%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_k%7D%28h%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))" class="latex" title="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))" />. This is exactly the vector we can obtain using back propagation.</p>



<p>For example, if we want a linear model, we can use <img src="https://s0.wp.com/latex.php?latex=%28a%2Cb%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a,b)" class="latex" title="(a,b)" /> as our parameters and the function will be <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+a%5Ccdot+%2B+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \mapsto a\cdot + b" class="latex" title="x \mapsto a\cdot + b" />. We can generate random points <code>X</code>,<code>Y</code> as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/4Do0KI4.png" alt="" /></figure>



<p>Now we can define a linear model as follows:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Linear:
  def __init__(self):
    self.a,self.b = Value(random.random()),Value(random.random())
  def __call__(self,x): return self.a*x+self.b

  def zero_grad(self):
    self.a.grad, self.b.grad = 0,0
</pre>


<p>And train it directly by using SGD:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">η = 0.03, epochs = 20
for t in range(epochs):
  for x,y in zip(X,Y):
    model.zero_grad()
    loss = (model(x)-y)**2
    loss.backward()
    model.a , model.b = (model.a - η*model.a.grad  , model.b - η*model.b.grad)
</pre>


<p>Which as you can see works very well:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/19kDCPM.gif" alt="" /></figure>



<h2>From linear classifiers to Neural Networks.</h2>



<p>The above was somewhat of an “overkill” for linear models, but the beautify of automatic differentiation is that we can easily use more complex computation.</p>



<p>We can follow <a href="https://github.com/karpathy/micrograd/blob/master/demo.ipynb">Karpathy’s demo</a> and us the same approach to train a neural network.</p>



<p>We will use a neural network that takes two inputs and has two hidden layers of width 16. A neuron that takes input <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_k" class="latex" title="x_1,\ldots,x_k" /> will apply the ReLU function (<img src="https://s0.wp.com/latex.php?latex=max%7B0%2Cx%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="max{0,x}" class="latex" title="max{0,x}" />) to <img src="https://s0.wp.com/latex.php?latex=%5Csum+w_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum w_i x_i" class="latex" title="\sum w_i x_i" /> where <img src="https://s0.wp.com/latex.php?latex=w_1%2C%5Cldots%2Cw_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_1,\ldots,w_k" class="latex" title="w_1,\ldots,w_k" /> are its <em>weight</em> parameters. (It’s easy to add support for relu for our <code>Value</code> class. Also we won’t have a bias term in this example.)</p>



<p>The code for this Neural Network is as follows: (when <code>Value() </code>is called without a parameter the value is random number in <img src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[-1,1]" class="latex" title="[-1,1]" />)</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def Neuron(weights,inputs, relu =True):
  # Evaluate neuron with given weights on given inputs
  v =  sum(weights[i]*x for i,x in enumerate(inputs))
  return v.relu() if relu else v


class Net:
  # Depth 3 fully connected neural net with one two inputs and output
  def __init__(self,  N=16):
    self.layer_1 = [[Value(),Value()] for i in range(N)]
    self.layer_2 = [ [Value() for j in range(N)] for i in range(N)]
    self.output =  [ Value() for i in range(N)]
    self.parameters = [v for L in [self.layer_1,self.layer_2,[self.output]] for w in L for v in w]


  def __call__(self,x):
    layer_1_vals = [Neuron(w,x) for w in self.layer_1]
    layer_2_vals = [Neuron(w,layer_1_vals) for w in self.layer_2]
    return Neuron(self.output,layer_2_vals,relu=False) 
    # the last output does not have the ReLU on top

  def zero_grad(self):
    for p in self.parameters:
      p.grad=0
</pre>


<p>We can train it in the same way as above.<br />We will follow Karpathy and train it to classify the following points:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/cXv93KU.png" alt="" /></figure>



<p>The training code is very similar, with the following differences:</p>



<ul><li>Instead of the square loss, we use the function <img src="https://s0.wp.com/latex.php?latex=L%28y%2Cy%27%29%3D+%5Cmax%7B+1-+y%5Ccdot+y%27%2C+0+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(y,y')= \max{ 1- y\cdot y', 0 }" class="latex" title="L(y,y')= \max{ 1- y\cdot y', 0 }" /> which is <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> if <img src="https://s0.wp.com/latex.php?latex=y+%5Ccdot+y%27+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y \cdot y' \geq 1" class="latex" title="y \cdot y' \geq 1" />. This makes sense since our data labels will be <img src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\pm 1" class="latex" title="\pm 1" /> and we say we classify correctly if we get the same sign. We get zero loss if we classify correctly all samples with a margin of at least <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" />.</li><li>Instead of stochastic gradient descent we will do standard gradient descent, using all the datapoints before taking a gradient step. The optimal for neural networks is actually often something in the middle – <em>batch gradient descent</em> where we take a batch of samples and perform the gradient over them.</li></ul>



<p>The resulting code is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">for t in range(epochs):
  loss = sum([(1+ -y*model(x)).relu() for (x,y) in zip(X,Y)])/len(X)
  model.zero_grad()
  loss.backward()
  for p in model.parameters:
    p.data -= η*p.grad
</pre>


<p>If we use this, we get a decent approximation for this training set (see image below). As Karpathy shows, by adjusting the learning rate and using regularization, one can in fact get 100% accuracy.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/nbpNZyu.png" alt="" /></figure></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/"><span class="datestr">at November 03, 2020 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/">Postdoc at University of Augsburg (apply by November 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The group of Tobias Mömke at University of Augsburg is inviting applications for a three-year Postdoc position, starting at the earliest possible date. The topic of the position is research on Approximation Algorithm.</p>
<p>Website: <a href="https://www.uni-augsburg.de/en/fakultaet/fai/informatik/prof/raa/hiring/#postdoc">https://www.uni-augsburg.de/en/fakultaet/fai/informatik/prof/raa/hiring/#postdoc</a><br />
Email: moemke@informatik.uni-augsburg.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/03/postdoc-at-university-of-augsburg-apply-by-november-30-2020/"><span class="datestr">at November 03, 2020 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17775">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/">The Election Night Time Warp</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Has Election Night—not just the election—been modeled adequately?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/godelinthought/" rel="attachment wp-att-17777"><img src="https://rjlipton.files.wordpress.com/2020/11/godelinthought.jpg?w=600" alt="" class="alignright size-full wp-image-17777" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">The Conversation <a href="https://theconversation.com/kurt-godel-from-loopholes-and-dictators-to-the-incompleteness-theorems-72376">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Kurt Gödel famously found solutions to Albert Einstein’s equations of general relativity that allow trajectories to loop through time. </p>
<p>
Today, early on Election Day and a usual time for our posts on Gödel, I talk about the trajectory of counting votes after tomorrow’s polls close and time-warp effects that everyone watching the returns will see.</p>
<p>
I am picking up the vein of ethical algorithms in yesterday’s <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">post</a>, but with a different take about ethical modeling in novel situations where reliable training data is unavailable. I believe there is a responsibility for running simulations not just of tomorrow’s <em>election</em>, such as FiveThirtyEight <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">conducts</a>, but also of tomorrow’s <em>count</em> as it may unfold hour by hour and stretch over many following days. </p>
<p>
Gödel also famously believed he had found a logical flaw in the U.S. Constitution that allowed a mechanism for legally instituting a dictatorship, but his argument was never reported. We <a href="https://rjlipton.wordpress.com/2013/12/06/judging-a-book-by-its-coverage/">discussed</a> this at length in 2013, including noting a 2012 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2010183">paper</a> by Enrique Guerra-Pujol of the University of Central Florida College of Business, who is a frequent reader of this blog. Guerra-Pujol offers a detailed construction of a mechanism based on self-reference applied to the short <a href="https://www.archives.gov/federal-register/constitution/article-v.html">Article V</a> on amending the Constitution. This topic has been <a href="https://slate.com/technology/2018/08/is-there-a-logical-inconsistency-in-the-constitution.html">addressed</a> <a href="https://harvardlawreview.org/2020/01/pack-the-union-a-proposal-to-admit-new-states-for-the-purpose-of-amending-the-constitution-to-ensure-equal-representation/">several</a> <a href="https://theconversation.com/kurt-godel-from-loopholes-and-dictators-to-the-incompleteness-theorems-72376">times</a> <a href="https://thehill.com/opinion/civil-rights/473613-a-contradiction-at-the-heart-of-the-american-system">since</a>, but what stands out to us is a 2019 <a href="http://ceur-ws.org/Vol-2632/MIREL-19_paper_1.pdf">paper</a> by Valeria Zahoransky and Christoph Benzmüller. This paper attempts to find a proof of Guerra-Pujol’s mechanism by automated logical inference using the <a href="https://en.wikipedia.org/wiki/Isabelle_(proof_assistant)">Isabelle</a>/<a href="https://en.wikipedia.org/wiki/HOL_(proof_assistant)">HOL</a> proof assistant. They found a model that satisfies the argument. </p>
<p></p><h2> Election Evolution Over Time </h2><p></p>
<p></p><p>
My <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">post</a> on Election Day 2016 contrasted the relatively stable time evolution of polls in 2012 with the gyrations of 2016. That post began by defending Nate Silver of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> and his 30% likelihood for Donald Trump against those who had Hillary Clinton over 90%. At the time of our 2012 <a href="https://rjlipton.wordpress.com/2012/11/05/the-election-outcome/">post</a> on Barack Obama versus Mitt Romney, I had thought Silver’s error bars were too wide, but in 2016 they looked right on the basis of last-minute decision making by impulse that my chess model also registers.</p>
<p>
This year, the polls have been even steadier than in 2012, and undecided voters are found to be scarce. Of course, the pandemic has been the greatest factor in the poll numbers, but my first point is that election forecasting uses only the numbers as primary. The algorithms do not have an input that could take values Covid 19, Covid 23, Covid 17… In a normal election, the polling numbers would seem to point to an easier call than in 2012. Silver has, however, expressed <a href="https://fivethirtyeight.com/features/im-here-to-remind-you-that-trump-can-still-win/">caution</a> in explaining why his odds stay just short of 90% for Joe Biden as I write at midnight. My first point of further disquiet begins with a simple fact:</p>
<blockquote><p><b> </b> <em> The models have been trained on data from past elections. </em>
</p></blockquote>
<p></p><p>
A major difference on our flight path to Election Day is the upsurge in early voting and turnout overall. Texas reports that it has already registered more early votes than total votes cast in 2016. We are not saying this difference has been overlooked—of course, models are being adjusted for it every hour. What we are doubting is the existence of a basis for making those adjustments with high confidence. Before we discuss the major issue of the flight path <em>after</em> the polls close, let me insert an analogy from my current chess work.</p>
<p>
</p><p></p><h2> My Own Ethical Interpolation </h2><p></p>
<p></p><p>
My statistical chess model is trained on millions of moves from games at the hours-long pace of standard in-person chess. The pandemic has led to chess moving online where fast time controls are the norm. For instance, the hallowed US Championships were just held in a format of three games per day at a pace of 25 minutes for the whole game plus 5 seconds for each move played, which equates to G/30 (game in 30 minutes) under a simplification used also <a href="http://universalrating.com/about-us.php">here</a>. The Online Olympiad held in August by the International Chess Federation (FIDE) gave only 15 minutes plus 5 seconds per move, equating to G/20. I have been asked for input on games at paces all the way down to 1-minute “Bullet” chess. </p>
<p>
My solid estimates of how less time affects skill come from the annual FIDE World Rapid and Blitz Championships, which are contested at equivalents of G/25 and G/5, respectively, by hundreds of elite male and female players. For time controls in-between I face twin problems of scant data from in-person chess—basically none for player below master level—and online chess having evident contamination from cheating, as I discussed in a <a href="https://rjlipton.wordpress.com/2020/06/07/the-doomsday-argument-in-chess/">post</a> last June. Hence I <em>interpolate</em> to determine model settings for other time controls. </p>
<p>
This <a href="https://cse.buffalo.edu/~regan/chess/RatingTimeCurves.jpg">diagram</a> shows internal evidence supporting the orange curve obtained via <a href="https://en.wikipedia.org/wiki/Aitken's_delta-squared_process">Aitken extrapolation</a>, in that inverse polynomial curves based on other reasoning converge to it. The curve has been supported by recent field tests, including a restricted invitational tournament recently run by Chess.com at G/3 and a large junior league in Britain at G/15 equivalent. Still, the <em>ethical</em> status is:</p>
<ol>
<li>
The pandemic has injected me into online fast chess ahead of the year-long timeframe that would be needed to clean the available large data and rebuild my model directly for all the gamut of different time controls. <p></p>
</li><li>
So it is ethical for me to give my best estimates, as supported by field tests and <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">cross-checks</a> in my model, but with caveat of their being “an extrapolation of an interpolation.” <p></p>
</li><li>
But in principle there are more-reliable ways to do the modeling.
</li></ol>
<p>
My second point is that this year’s election models are in similar boats: The pandemic forces their cantilevered use in new situations. As with chess the data needed for direct training may not be available. As we’ve said in the <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">post</a>, this year’s election <em>task</em> may be “Ethics-Hard.” Now we come to my third and main point about responsibility.</p>
<p>
</p><p></p><h2> A Night of Waves and Blue/Red Shifts </h2><p></p>
<p></p><p>
The new dimension of time is the order in which all the following categories of votes will be counted, in the states that variously allow them:</p>
<ol>
<li>
Early votes cast in-person, such as my wife and I did on the first possible <a href="https://buffalonews.com/news/local/government-and-politics/nearly-15-000-turn-out-on-first-day-of-early-voting-in-erie-county/article_f35cc7f0-153b-11eb-bbd6-43fce059203c.html">day</a> in New York. <p></p>
</li><li>
Early votes sent by mail. <p></p>
</li><li>
In-person votes on Election Day. <p></p>
</li><li>
Regular votes by mail in states that vote that way. <p></p>
</li><li>
Absentee ballots, which are the only non-Tuesday option in some states.
</li></ol>
<p>
This is approximately the order in which votes will be counted, again with state-by-state differences, which especially may lead to votes in category 2 being counted later. The issue is that the Democratic and Republican shares are expected to vary greatly across the categories, enough to cause large shifts in the perceived leader over real time. </p>
<p>
Geoffrey Skelley of FiveThirtyEight has an <a href="https://fivethirtyeight.com/features/why-pennsylvanias-vote-count-could-change-after-election-night/">article</a> showing how a 5-point win for Biden in Pennsylvania might still present as a <b>16-point</b> lead for President Trump on Election Night, when all in-person votes are counted but only some of the votes by mail. Here are the article’s key graphics, the left one showing actual proportions from Pennsylvania’s June primary.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/skelleyblueshiftcombo/" rel="attachment wp-att-17778"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2020/11/skelleyblueshiftcombo.png?w=550&amp;h=243" class="alignright wp-image-17778" height="243" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of two figures from FiveThirtyEight Skelley <a href="https://fivethirtyeight.com/features/why-pennsylvanias-vote-count-could-change-after-election-night/">article</a></font>
</td>
</tr>
</tbody></table>
<p>
Other states that allow early tallying of early votes may show an initial Biden lead before a red-shift toward Trump on Election Night, with more of Biden’s vote share remaining to be counted. Still others are less clear. FiveThirtyEight on Saturday posted a useful state-by-state <a href="https://projects.fivethirtyeight.com/election-results-timing/">guide</a>.</p>
<p>
</p><p></p><h2> Why Important to Model the Count? </h2><p></p>
<p></p><p>
Awareness of the time-shifts is important not only for perception but also for the timing of legal challenges that are expected to arise. For instance, there is a continued specter of <a href="https://www.texastribune.org/2020/11/01/texas-drive-thru-votes-harris-county/">invalidating</a> 127,000 early votes already cast by a novel drive-up system in Harris County Texas; despite its <a href="https://www.nytimes.com/live/2020/11/02/us/trump-vs-biden/a-federal-judge-denies-a-bid-to-throw-out-more-than-127000-votes-in-texas-the-republicans-who-sued-have-already-appealed">rejection</a> by a district judge today, it has been appealed higher. There is expectation of legal <a href="https://www.nytimes.com/2020/11/02/us/politics/election-day-ballot-counting.html">battles</a> nationwide over procedures that have been altered by measures to cope with the pandemic.</p>
<p>
Public perception, however, is the most immediate concern. President Trump <a href="https://rjlipton.wordpress.com/feed/is totally inappropriate, and I don't believe that's by our laws">stated</a> in a flagged Tweet that we “must have final total on November 3rd” and followed up by saying, “…instead of counting ballots for two weeks, which is totally inappropriate, and I don’t believe that’s by our laws.” Justice Brett Kavanaugh wrote in a formal <a href="https://www.supremecourt.gov/opinions/20pdf/20a66_new_m6io.pdf#page=6">opinion</a> that states “want to avoid the chaos and suspicions of impropriety that can ensue if thousands of absentee ballots flow in after Election Day and potentially flip the results of an election.” Note his phrase “flip the results”—a sure sign that perception is reality.</p>
<p>
Of course many outlets besides FiveThirtyEight are aware of the new election-reporting physics and <a href="https://www.nytimes.com/interactive/2020/10/27/upshot/election-results-timing.html">have</a> <a href="https://www.brookings.edu/blog/fixgov/2020/10/30/what-to-watch-for-on-election-night-2020/">published</a> <a href="https://www.washingtonpost.com/politics/2020/10/26/timing-election-results/">their</a> <a href="https://www.wsj.com/articles/will-we-know-who-is-elected-president-on-election-night-a-guide-to-possible-delays-11596629410">own</a> <a href="https://www.cnn.com/2020/10/30/politics/red-blue-mirage-election-results/index.html">guides</a>. They are adjusting their election-night projection models accordingly. Our main question goes further in terms of responsibility:</p>
<blockquote><p><b> </b> <em> Has anyone been running simulations of how Election Night vote-counting may unfold? </em>
</p></blockquote>
<p></p><p>
We believe such simulations are just as important as the ones they have run of a timeless election. Showing them is not only important to inform the many who will be watching, it would be a vaccine against pressure that exploits unexpected perception. </p>
<p>
Yet it must be acknowledged that there is neither hard data nor sure knowledge of county-level vote-tallying policies and schedules on which to train such simulations. Nor may there be as many cross-checks as my chess interpolation situation. </p>
<p>
This may be another “Ethics-Hard” problem. But it is one already involved in adjusting the projection models that definitely are being deployed. Aside from the many novel and vital modeling problems from the pandemic, it may be the most important one of our near future. And we are thinking of this less than 48 hours—now less than 24 hours—before the first polls close.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How do you think Election Night results will unfold, apart from your estimate of the time-independent “ground truth” of the electorate’s intentions?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2020/11/03/the-election-night-time-warp/"><span class="datestr">at November 03, 2020 06:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5061">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5061">A Drawing for Singularity Eve</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<figure class="wp-block-image size-large"><a href="https://www.scottaaronson.com/hamburgur2.jpg"><img src="https://www.scottaaronson.com/hamburgur2-sm.jpg" alt="" /></a></figure>



<p>Lily, my 7-year-old, asked me to share the above on my blog.  She says it depicts the US Army luring Trump out of the White House with a hamburger, in order to lock the front door once he’s out—what she proposes should happen if Trump refuses to acknowledge a loss.</p>



<p>If you haven’t yet voted, especially if you live in a contested state, <strong>please do so tomorrow</strong>.  Best wishes to us all!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Nov. 3):</span></strong> Even if it comes 4-5 years late, <a href="https://samharris.org/podcasts/224-key-trumps-appeal/">this 8-minute podcast by Sam Harris</a> gives perhaps the sharpest solution ever articulated to the mystery of how tens of millions of Americans could enthusiastically support an obvious fraud, liar, incompetent, and threat to civilization.  Briefly, it’s not <em>despite</em> his immense failings but <em>because of</em> them—because by flaunting his failings he absolves his supporters for their own, even while the other side serves those same supporters relentless moral condemnation and scorn.  I <em>think</em> I had known this—I even said something similar as the tagline of this blog (“The Far Right is destroying the world, and the Far Left thinks it’s my fault!”).  But Sam Harris expresses it as only he can.  If this analysis is right—and I feel virtually certain it is—then it bodes well that Biden, unlike Hillary Clinton, isn’t seen as especially sanctimonious or judgmental.  Biden’s own gaffes and failings probably help him.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5061"><span class="datestr">at November 03, 2020 01:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/">Faculty at Tufts University (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Tufts Computer Science seeks Assistant/Associate Professor with research in Quantum Computation and Information, and a strong background in theoretical computer science whose research connects with current faculty in quantum information and beyond. Candidates should demonstrate attention to diversity and inclusion as related to teaching, research, and engagement. Tufts is an EO/AA employer.</p>
<p>Website: <a href="https://apply.interfolio.com/78094">https://apply.interfolio.com/78094</a><br />
Email: ttsearch@cs.tufts.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/"><span class="datestr">at November 02, 2020 10:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/11/02/constant-width-involutes">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/11/02/constant-width-involutes.html">Constant width from involutes of pseudotriangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In his <a href="https://www.eecs.yorku.ca/~jeff/courses/fun/">online collection of fun stuff</a>, Jeff Edmonds recently posted <a href="https://www.eecs.yorku.ca/~jeff/courses/fun/Equal_Distance.docx">a method of constructing curves of constant width</a> by spinning a pencil on a flat surface, with a varying axis, and tracking the movement of its ends. It is pretty similar to the classical method of crossed lines described by Martin Gardner in <em>The Unexpected Hanging</em>, in which one constructs an arrangement of lines in the plane, sorts them in circular order by slope, and builds a curve out of circular arcs centered at the crossing points of consecutive lines in this sorted order. However, it grows the curve at both ends simultaneously, rather than only at one end, and chooses the lines dynamically rather than in advance. Regardless, the result is the same: a piecewise-circular constant-width curve.</p>

<p>This got me wondering how we might go about constructing curves of constant width that are not piecewise-circular. Instead of a finite set of lines, we could use a continuous family of lines, one of each slope, but that’s a little difficult to visualize. Instead, there’s a simpler method that works more like Jeff’s spinning pencil, which Robinson (<a href="https://doi.org/10.1112/blms/16.3.264">“Smooth curves of constant width and transnormality”, <em>Bull. LMS</em> 1984</a>) attributes to Euler (<a href="https://scholarlycommons.pacific.edu/euler-works/513/">“De curvis triangularibus”, 1778</a>):</p>

<ul>
  <li>
    <p>Draw a closed curve in the plane that has only one tangent line of each slope.</p>
  </li>
  <li>
    <p>Rotate a “long enough” tangent line segment of some fixed length around this curve without sliding it.</p>
  </li>
  <li>
    <p>Trace the paths of the endpoints of the line segment.</p>
  </li>
</ul>

<p>The first step may already seem a little confusing. Don’t curves usually have at least two tangent lines of each slope, their support lines from opposite sides? Well, yes, for convex curves. But for a <a href="https://en.wikipedia.org/wiki/Pseudotriangle">pseudotriangle</a>, a curve whose boundary is concave everywhere except at three extreme points, there might only be one tangent line of each slope. A standard example of such a curve is the <a href="https://en.wikipedia.org/wiki/Deltoid_curve">deltoid</a>, and the following animation stolen from the <a href="https://mathcurve.com/courbes2d.gb/deltoid/deltoid.shtml">mathcurve.com page on deltoids</a> shows a tangent line segment (black) rotating around a deltoid (blue) and tracing out a curve of constant width (red).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/involute-deltoid.gif" alt="Animation of a tangent line segment rolling around a deltoid, with its endpoints tracing out a curve of constant width, from https://mathcurve.com/courbes2d.gb/deltoid/deltoid.shtml" /></p>

<p>The traced curve is always perpendicular to the rotating line segment, so locally at least this segment behaves like the width of the curve in each given direction. And since we don’t change the length of the segment while it rotates, the width stays constant. The same deltoid, the same curve of constant width, and two positions of its tangent line segment can also be seen in the illustration below from a 1954 mathematics paper, <a href="https://doi.org/10.2307/2307215">“Rotors within rotors” by Michael Goldberg in the <em>Amer. Math. Monthly</em></a>. I’ve overlaid a red Reuleaux triangle to show that, like <a href="https://11011110.github.io/blog/2020/08/30/linkage.html">so</a> <a href="https://11011110.github.io/blog/2020/07/05/shape-wankel-rotor.html">many</a> <a href="https://11011110.github.io/blog/2020/06/30/linkage.html">other</a> <a href="https://11011110.github.io/blog/2018/06/24/la-maddalena-non-reuleaux.html">curvy</a> <a href="https://11011110.github.io/blog/2018/04/17/mythical-reuleaux-manhole.html">triangles</a>, this is not a Reuleaux triangle, even though it has constant width. Although its corners are drawn to look kind of pointy, they should actually be smooth, and the rest of the curve bulges farther out from its sides than a Reuleaux triangle would.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/rotors-within-rotors.png" alt="The involute of deltoid, as depicted by Goldberg in &quot;Rotors within rotors&quot;, with an overlaid Reuleaux triangle" /></p>

<p>More formally, this process of rotating and tracing tangent line segments produces a curve called the <a href="https://en.wikipedia.org/wiki/Involute">involute</a> of the deltoid. An involute of a curve is more typically described as what you get when you fix one end of a length of string at a point on the curve, wrap it tightly around the curve, and then unwrap it while keeping it taut, tracing a curve with the other end of the string as you do. The two ends of the rotating tangent line segment can both be thought of as being formed in the same way from two strings, with one of them unwrapping the deltoid from one direction while the other wraps it back up in the other direction. In the deltoid example the segment was the same length as the sides of the deltoid, which were all equal, but it also works with unequal sides or longer segments, as long as the rotating segment is long enough to reach all three cusps.</p>

<p>You might worry whether the segment always comes back to its starting position after each rotation, and this does require a little care in the initial choice of length and placement of the line segment. If the length of the rotating segment and eventual width of the traced curve are \(w\), the pseudotriangle sides have lengths \(a\), \(b\), and \(c\), and the segment starts with \(x\) units of extra length extending past the cusp prior to side \(a\) in its rotation, then it will have \(w-a-x\) units of extra length at the next cusp, \(w-b-(w-a-x)=x+a-b\) units at the third cusp, and \(w-c-(x+a-b)=w-x-a+b-c\) units at the last cusp. To make a curve of constant width we need the amount of extra length at the start and end to be equal, which happens when we set this length to be \(x=(w-a+b-c)/2\).</p>

<p>If the pseudotriangle that the tangent segment rotates around includes a line segment, there will be a discontinuity in its rotating movement as the axis of rotation shifts from one end of the segment to the other, much like the changes of axis described by Edmonds, but the same process still works.  If the pseudotriangle has a point where its slope changes discontinuously (for instance, if it is a polygon rather than a smooth curve), then the rotating segment will rotate around this point, with its ends tracing circular arcs, as it continuously moves between the same slopes; this can happen either at the three convex points of the pseudotriangle or along the concave curves between them. In particular, if your pseudotriangle is actually an equilateral triangle and the rotating segment has the same length as its sides, you get a Reuleaux triangle.</p>

<p>It’s also possible to form closed curves with only one tangent line of each slope that are not pseudotriangles. An example is the standard pentagram (whose involute is the Reuleaux pentagon), or a cuspy and irregular pentagram like the one below (whose involute is another curve of constant width without circular arcs). The same process works for these, with a slightly more complicated calculation of how to place a rotating segment of a given length for a given starting curve, involving alternating sums of side lengths.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/cuspy-star.svg" alt="A curved five-point star with one tangent line in each direction" /></p>

<p>What happens when the rotating line segment is too short, so that it doesn’t reach one or more of the cusps? I’m not sure in general, but for the deltoid the result can be the same deltoid (for which the rotating line segment is one possibility for Goldberg’s “rotor within a rotor”, although the rotor he describes is larger) or another similar but smaller deltoid inside it. See the mathcurve link for details.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105144037558097550">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/11/02/constant-width-involutes.html"><span class="datestr">at November 02, 2020 05:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9123933489422342245">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html">I polled my class about the election</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In 2016 I had the Sophomore discrete math class do a poll of who they wanted for president.</p><p>In 2020 I had the  both my  Senior Crypto class and Clyde's Sophomore algorithms course do a poll of who they wanted for president.</p><p>All of these polls were anonymous. One big difference- in 2016 it was paper, they could check offwho they wanted or put in a write in, whereas in 2020 it was on elms without a mechanism for a write in--- so no votes for Bernie or Bill or Kruskal (not sure if they were voting for the man or the algorithm) were possible. In all cases I included everyone who was on the Maryland Ballot (so Libertarian and Green votes were possible). </p><p><br /></p><p>Discrete Math 2016: 428 students took the poll. Write ins allowed. </p><p>Clinton- 305 which is 71%</p><p>Trump- 44 which is  10%</p><p>Johnson (Libertarian)- 21 which is 5%</p><p>Stein (Green)-  11which is 3%</p><p>Sanders-6 which is 1%</p><p>Silly answers: 41 which is 10%</p><p>I was NOT surprised that Trump got 44 votes- every year I do this and every year the </p><p>republican gets between 10 and 20 percent. Romney go 17% in 2012 (see <a href="https://blog.computationalcomplexity.org/2012/11/random-thoughts-on-election.html">here</a>). </p><p><br /></p><p>Algorithms, 2020, 161 students took the poll</p><p>Biden: 127 (79%)</p><p>Trump: 25 (16%)</p><p>Hawkins (Green): 4 (2%)</p><p>Jorgenson (Libertarian): 4 (2%)</p><p>Segal (Bread and Roses Party) 1 (1%)</p><p><br /></p><p>Cryptography in 2020: </p><p>Biden- 40 which is 78%</p><p>Trump-6 which is 12%</p><p>Hawkins (Green ) 3 which is 6%</p><p>Jorgenson (Libertarian) 2 which is 4%</p><p>Segal (Bread and Roses) 0 which is 0%</p><p><br /></p><p>I have no idea what these numbers mean. College students tend to be liberal- we knew that. That Trump went from 10% to 16% would be interesting if it was a larger sample size. I wonder if forcing them to NOT have a write-in had an effect. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html"><span class="datestr">at November 02, 2020 03:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/160">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/160">TR20-160 |  Non-adaptive vs Adaptive Queries in the Dense Graph Testing Model | 

	Oded Goldreich, 

	Avi Wigderson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the relation between the query complexity of adaptive and non-adaptive testers in the dense graph model. 
It has been known for a couple of decades that the query complexity of non-adaptive testers is at most quadratic in the query complexity of adaptive testers. 
We show that this general result is essentially tight; that is, there exist graph properties for which any non-adaptive tester must have query complexity that is almost quadratic in the query complexity of the best general (i.e., adaptive) tester. 

More generally, for every $q:\N\to\N$ such that $q(n)\leq{\sqrt n}$ and constant $c\in[1,2]$, we show a graph property that is testable in $\Theta(q(n))$ queries, but its non-adaptive query complexity is $\Theta(q(n)^c)$, omitting $\poly(\log n)$ factors and ignoring the effect of the proximity parameter $\epsilon$. Furthermore, the upper bounds hold for one-sided error testers,
and are at most quadratic in $1/\epsilon$. 

These results are obtained through the use of general reductions that transport properties of ordered structured (like bit strings) to those of unordered structures (like unlabeled graphs). 
The main features of these reductions are query-efficiency and preservation of distance to the properties.
This method was initiated in our prior work ({\em ECCC}, TR20-149), and we significantly extend it here.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/160"><span class="datestr">at November 02, 2020 03:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/159">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/159">TR20-159 |  Relating existing powerful proof systems for QBF | 

	Leroy Chew</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We advance the theory of QBF proof systems by showing the first simulation of the universal checking format QRAT by a theory-friendly system. We show that the sequent system G fully p-simulates QRAT, including the Extended Universal Reduction (EUR) rule which was recently used to show QRAT does not have strategy extraction. Because EUR heavily uses resolution paths our technique also brings resolution path dependency and sequent systems closer together. 
	While we do not recommend G for practical applications this work can potentially show what features are needed for a new QBF checking format stronger than QRAT.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/159"><span class="datestr">at November 02, 2020 09:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17765">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/">The Night of the Ethical Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Algorithms for the Election</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png"><img width="142" alt="" src="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png?w=142&amp;h=110" class="alignright wp-image-17767" height="110" /></a></p>
<p>
Michael Kearns and Aaron Roth are the authors of the <a href="https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205">book</a> <em>Ethical Algorithms and the The Science of Socially Aware Algorithm Design</em>. It has earned strong reviews including this <a href="https://www.nature.com/articles/s41567-019-0768-1.epdf?shared_access_token=PLLHPNTn5pByQUPaJ-n_KdRgN0jAjWel9jnR3ZoTv0OmI9hygrW1-UKQon3ZVGlMNsZowBl_5psAIvdU-Dic4gtf_j0e7S_897lCTN3vdbqv3cbSQwUtbHG78r5ycJNB2VKFndoPOT_8w0OYjS-pjQ%3D%3D">one</a> in <i>Nature</i>—impressive.</p>
<p>
Michael is a long-time friend who is a leader in machine learning, artificial intelligence, and much <a href="https://www.cis.upenn.edu/~mkearns/">more</a>. He also overlapped with Ken at Oxford while visiting Les Valiant there in the mid-1980s. He is at the University of Pennsylvania in computer science along with his co-author Roth. Cynthia Dwork and Roth wrote an earlier <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">book</a> on the related issue of Differential Privacy.</p>
<p>
Today we will talk about making algorithms ethical.</p>
<p>
Tuesday is the 2020 US national election for President, for Congress, and for state and local offices. Every four years we have a national election, and we cannot imagine a better motivation for making sure that algorithms are ethical. </p>
<p>
The word “algorithm” appears 157 times in their book. Two words used hand-in-hand with it are “data” (132 times) and “model” (103 times), both spread through all of the book’s 232 pages. Models of electorates, trained on data from past elections, inform the algorithms used by news agencies to make election-night projections. These carry more responsibilities than election-eve forecasts. There have been infamous mistakes, most notably the premature calls of Florida both ways in the 2000 election. </p>
<p>
We believe that Tuesday’s election in our novel pandemic situation requires attention to ethics from first principles. We will discuss why this is important. What it means to be ethical here? And how one can make an algorithm ethical? </p>
<p>
</p><p></p><h2> The Issue </h2><p></p>
<p></p><p>
Algorithms have been around forever. Euclid devised his <a href="https://en.wikipedia.org/wiki/Euclidean_algorithm">gcd</a> algorithm in 300 BCE. In the first half of the last century, the central issue was how to define that an algorithm is <b>effective</b>. This led to showing that some problems are <em>uncomputable</em>, so that algorithms for them are impossible.</p>
<p>
In the second half, the emphasis shifted to whether algorithms are <b>efficient</b>. This led to classifying problems as <em>feasible</em> or (contingently) <em>hard</em>. Although many algorithms for feasible problems have been improved in ways that redouble the effect of faster and cheaper hardware, the study of complexity classes such as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> has given reasons why algorithms for hard problems may never be improvable.</p>
<p>
The new territory, that of Kearns and Roth, is whether algorithms are <b>ethical</b>. Current ones that they and <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">others</a> have critqued as unethical accompany models for the likes of mortgages, small-business loans, parole decisions, and college admissions. The training data for these models often bakes in past biases. Besides problems of racial and gender bias and concerns of societal values, the raw fact is that past biases cause the models to miss the mark for today’s applications. For algorithms with such direct application to society, ethical design is critical. </p>
<p>
But this requirement is further reaching that one might initially imagine, so that as with computability and complexity, the factors can be ingrained in the <em>problems</em>. </p>
<p>
Consider a simple problem: We given a collection of pairs of numbers <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x,y)}" class="latex" title="{(x,y)}" />. We are to predict whether this number pair has the property </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%2B+y+%5Cge+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x + y \ge 0. " class="latex" title="\displaystyle  x + y \ge 0. " /></p>
<p>This is pretty easy if we can use both <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" />. But imagine a world where <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> is allowed to be viewed but <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" /> is secret. Perhaps the law requires that we cannot use <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" />—it is illegal. Now we might do as poorly as <img src="https://s0.wp.com/latex.php?latex=%7B50%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{50\%}" class="latex" title="{50\%}" />. Suppose that the data consists of </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%281%2C-1%29%2C+%281%2C1%29%2C+%282%2C-2%29%2C+%282%2C2%29+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots " class="latex" title="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots " /></p>
<p>Then seeing only <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> gives no advantage, while giving both is perfect. Thus what in these simplified terms counts as an ethical algorithm is a poor predictor, whereas an unethical one is perfect. </p>
<p>
The blurb for the Kearns-Roth book says that they “…explain how we can better embed human principles into machine code—without halting the advance of data-driven scientific exploration.” While we agree their approach is vital, we suspect that as with complexity there will be indelibly ethically hard tasks. We wonder whether election modeling has already become one of them. </p>
<p>
Ken and I have two separate takes on this. We will do the first and then the other in a second post.</p>
<p>
</p><p></p><h2> Red/Blue Leakage, Bias, and Ethics </h2><p></p>
<p></p><p>
One question on everyone’s minds is whether we will see a repeat of the forecasting misses from 2016. Let us remind that our own Election Day 2016 <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">post</a> started by defending Nate Silver of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for giving Donald Trump as much as a 30% chance to defeat Hillary Clinton. He had been attacked by representatives of many news and opinion agencies whose models had Clinton well over 90%. </p>
<p>
We wonder whether these models were affected by the kind of biases highlighted in the book by Kearns and Roth. We must say right away that we are neither alleging conscious biases nor questioning the desire for prediction accuracy. One issue in ethical modeling (for parole, loans, admissions) is the divergence between algorithm outcomes that are most <em>predictive</em> versus those that are best for society. Here we agree that accurate prediction—and accurate projections as results come in after the polls close—is paramount. However, the algorithms used for the latter projections (which were not at fault in 2016 but have been wrong previously) may be even more subject to what we as computer scientists with crypto background see as a “leakage” issue.</p>
<p>
Here is the point. Ideally, models using polling data and algorithms reading the Election Night returns should read the numbers as if they did not have ‘R’ and ‘D’ attached to them. Their own workings should be invariant under transformations that interchange Joe Biden and Donald Trump, or whoever are opposed in a local race. However, a crucial element in the projection models in particular is knowledge of voting geography. They must use data on the general voting preferences of regions where much of the vote is still extant. Thus they cannot avoid intimate knowledge of who is ‘R’ and who is ‘D.’ There is no double-blind or zero-knowledge approach to the subjects being projected.</p>
<p>
There is also the question of error bars. A main point of our 2016 post (and of Silver’s analysis) was the high uncertainty factor that could be read from how the Clinton-Trump race unfolded. Underestimating uncertainty causes overconfidence in models. This can result from “groupthink” of the kind we perceive in newsrooms of many of the same outlets that are doing the projections. The algorithms ought to be isolated from opinions of those in the organization, but again there is reason from the last election to wonder about leakage.</p>
<p>
Unlike cases addressed by Kearns and Roth, we do not see a solution to suggest. As in our simple <img src="https://s0.wp.com/latex.php?latex=%7Bx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x+y}" class="latex" title="{x+y}" /> example, prior knowledge of the data in full may be needed for prediction. This may just be an “Ethics-Hard” problem. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>
The word “election” does <i>not</i> appear in Kearns and Roth’s book.  What further application of their standpoint to elections would you make?</p>
<p>Ken sees a larger question of ethical modeling decisions given the unprecedented circumstances of the current election.  This comes not from spatial geography distorted by the pandemic but rather from the dimension of time injected by massive early voting and late counting of many mailed ballots.  He will address this next.</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/"><span class="datestr">at November 02, 2020 05:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
