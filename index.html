<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 02, 2019 07:25 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4199">NP-complete Problems and Physics: A 2019 View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr />

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center, a leading supercomputing facility in North Austin that’s affiliated with UT, is seeking to hire a Research Scientist focused on quantum computing.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting</a>.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4199"><span class="datestr">at June 02, 2019 01:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/082">TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/082"><span class="datestr">at June 02, 2019 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/081">TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/081"><span class="datestr">at June 02, 2019 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/080">TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/080"><span class="datestr">at June 01, 2019 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/079">TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/079"><span class="datestr">at June 01, 2019 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/078">TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/078"><span class="datestr">at June 01, 2019 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13196">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13196">Persistent homology detects curvature</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bubenik:Peter.html">Peter Bubenik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hull:Michael.html">Michael Hull</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Patel:Dhruv.html">Dhruv Patel</a>, Benjamin Whittle <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13196">PDF</a><br /><b>Abstract: </b>Persistent homology computations are completely characterized by a set of
intervals called a bar code. It is often said that the long intervals represent
the "topological signal" and the short intervals represent "noise". We give
evidence to dispute this thesis, showing that the short intervals encode
geometric information. Specifically, we show that persistent homology detects
the curvature of disks from which points have been sampled. We describe a
general computational framework for solving inverse problems using average
persistence landscapes. In the present application, the average persistence
landscapes of points sampled from disks of constant curvature produce a path in
a Hilbert space which may be learned.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13196"><span class="datestr">at June 01, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13095">Quantum Speedup Based on Classical Decision Trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beigi:Salman.html">Salman Beigi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Taghavi:Leila.html">Leila Taghavi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13095">PDF</a><br /><b>Abstract: </b>Lin and Lin have recently shown how starting with a classical query algorithm
(decision tree) for a function, we may find upper bounds on its quantum query
complexity. More precisely, they have shown that given a decision tree for a
function $f:\{0,1\}^n\to[m]$ whose input can be accessed via queries to its
bits, and a guessing algorithm that predicts answers to the queries, there is a
quantum query algorithm for $f$ which makes at most $O(\sqrt{GT})$ quantum
queries where $T$ is the depth of the decision tree and $G$ is the maximum
number of mistakes of the guessing algorithm. In this paper we give a simple
proof of and generalize this result for functions $f:[\ell]^n \to [m]$ with
non-binary input as well as output alphabets. Our main tool for this
generalization is non-binary span program which has recently been developed for
non-binary functions, as well as the dual adversary bound. As applications of
our main result we present several quantum query upper bounds, some of which
are new. In particular, we show that topological sorting of vertices of a
directed graph $\mathcal{G}$ can be done with $O(n^{3/2})$ quantum queries in
the adjacency matrix model. Also, we show that the quantum query complexity of
the maximum bipartite matching is upper bounded by $O(n^{3/4}\sqrt m + n)$ in
the adjacency list model.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13095"><span class="datestr">at June 01, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13064">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13064">The Bloom Clock</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Lum Ramabaja <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13064">PDF</a><br /><b>Abstract: </b>The bloom clock is a space-efficient, probabilistic data structure designed
to determine the partial order of events in highly distributed systems. The
bloom clock, like the vector clock, can autonomously detect causality
violations by comparing its logical timestamps. Unlike the vector clock, the
space complexity of the bloom clock does not depend on the number of nodes in a
system. Instead it depends on a set of chosen parameters that determine its
confidence interval, i.e. false positive rate. To reduce the space complexity
from which the vector clock suffers, the bloom clock uses a 'moving window' in
which the partial order of events can be inferred with high confidence. If two
clocks are not comparable, the bloom clock can always deduce it, i.e. false
negatives are not possible. If two clocks are comparable, the bloom clock can
calculate the confidence of that statement, i.e. it can compute the false
positive rate between comparable pairs of clocks. By choosing an acceptable
threshold for the false positive rate, the bloom clock can properly compare the
order of its timestamps, with that of other nodes in a highly accurate and
space efficient way.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13064"><span class="datestr">at June 01, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.13011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.13011">Don't Persist All : Efficient Persistent Data Structures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahapatra:Pratyush.html">Pratyush Mahapatra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hill:Mark_D=.html">Mark D. Hill</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Swift:Michael_M=.html">Michael M. Swift</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.13011">PDF</a><br /><b>Abstract: </b>Data structures used in software development have inbuilt redundancy to
improve software reliability and to speed up performance. Examples include a
Doubly Linked List which allows a faster deletion due to the presence of the
previous pointer. With the introduction of Persistent Memory, storing the
redundant data fields into persistent memory adds a significant write overhead,
and reduces performance. In this work, we focus on three data structures -
Doubly Linked List, B+Tree and Hashmap, and showcase alternate partly
persistent implementations where we only store a limited set of data fields to
persistent memory. After a crash/restart, we use the persistent data fields to
recreate the data structures along with the redundant data fields. We compare
our implementation with the base implementation and show that we achieve
speedups around 5-20% for some data structures, and up to 165% for a
flush-dominated data structure.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.13011"><span class="datestr">at June 01, 2019 11:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12990">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12990">Data Complexity and Rewritability of Ontology-Mediated Queries in Metric Temporal Logic under the Event-Based Semantics (Full Version)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ryzhikov:Vladislav.html">Vladislav Ryzhikov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walega:Przemyslaw_Andrzej.html">Przemyslaw Andrzej Walega</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zakharyaschev:Michael.html">Michael Zakharyaschev</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12990">PDF</a><br /><b>Abstract: </b>We investigate the data complexity of answering queries mediated by metric
temporal logic ontologies under the event-based semantics assuming that data
instances are finite timed words timestamped with binary fractions. We identify
classes of ontology-mediated queries answering which can be done in AC0, NC1,
L, NL, P, and coNP for data complexity, provide their rewritings to first-order
logic and its extensions with primitive recursion, transitive closure or
datalog, and establish lower complexity bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12990"><span class="datestr">at June 01, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12987">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12987">Inducing the Lyndon Array</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Felipe A. Louza, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mantaci:Sabrina.html">Sabrina Mantaci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sciortino:Marinella.html">Marinella Sciortino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telles:Guilherme_P=.html">Guilherme P. Telles</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12987">PDF</a><br /><b>Abstract: </b>In this paper we propose a variant of the induced suffix sorting algorithm by
Nong (TOIS, 2013) that computes simultaneously the Lyndon array and the suffix
array of a text in $O(n)$ time using $\sigma + O(1)$ words of working space,
where $n$ is the length of the text and $\sigma$ is the alphabet size. Our
result improves the previous best space requirement for linear time computation
of the Lyndon array. In fact, all the known linear algorithms for Lyndon array
computation use suffix sorting as a preprocessing step and use $O(n)$ words of
working space in addition to the Lyndon array and suffix array. Experimental
results with real and synthetic datasets show that our algorithm is not only
space-efficient but also fast in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12987"><span class="datestr">at June 01, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12935">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12935">Consistency of circuit lower bounds with bounded theories</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jan Bydzovsky, Jan Krajicek, Igor C. Oliveira <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12935">PDF</a><br /><b>Abstract: </b>Proving that there are problems in $\mathsf{P}^\mathsf{NP}$ that require
boolean circuits of super-linear size is a major frontier in complexity theory.
While such lower bounds are known for larger complexity classes, existing
results only show that the corresponding problems are hard on infinitely many
input lengths. For instance, proving almost-everywhere circuit lower bounds is
open even for problems in $\mathsf{MAEXP}$. Giving the notorious difficulty of
proving lower bounds that hold for all large input lengths, we ask the
following question: Can we show that a large set of techniques cannot prove
that $\mathsf{NP}$ is easy infinitely often? Motivated by this and related
questions about the interaction between mathematical proofs and computations,
we investigate circuit complexity from the perspective of logic.
</p>
<p>Among other results, we prove that for any parameter $k \geq 1$ it is
consistent with theory $T$ that computational class $\mathcal{C} \nsubseteq
\textit{i.o.}\mathsf{SIZE}(n^k)$, where $(T, \mathcal{C})$ is one of the pairs:
</p>
<p>$T = \mathsf{T}^1_2\;$ and $\;{\cal C} = \mathsf{P}^\mathsf{NP}$, $\quad T =
\mathsf{S}^1_2\;$ and $\;{\cal C} = \mathsf{NP}$, $\quad T = \mathsf{PV}\;$ and
$\;{\cal C} = \mathsf{P}$.
</p>
<p>In other words, these theories cannot establish infinitely often circuit
upper bounds for the corresponding problems. This is of interest because the
weaker theory $\mathsf{PV}$ already formalizes sophisticated arguments, such as
a proof of the PCP Theorem (Pich, 2015). These consistency statements are
unconditional and improve on earlier theorems of Krajicek and Oliveira (2017)
and Bydzovsky and Muller (2018) on the consistency of lower bounds with
$\mathsf{PV}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12935"><span class="datestr">at June 01, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12913">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12913">Information Source Detection with Limited Time Knowledge</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xuecheng.html">Xuecheng Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fu:Luoyi.html">Luoyi Fu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Bo.html">Bo Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Xiaojun.html">Xiaojun Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Xinbing.html">Xinbing Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12913">PDF</a><br /><b>Abstract: </b>This paper investigates the problem of utilizing network topology and partial
timestamps to detect the information source in a network. The problem incurs
prohibitive cost under canonical maximum likelihood estimation (MLE) of the
source due to the exponential number of possible infection paths. Our main idea
of source detection, however, is to approximate the MLE by an alternative
infection path based estimator, the essence of which is to identify the most
likely infection path that is consistent with observed timestamps. The source
node associated with that infection path is viewed as the estimated source
$\hat{v}$. We first study the case of tree topology, where by transforming the
infection path based estimator into a linear integer programming, we find a
reduced search region that remarkably improves the time efficiency.
</p>
<p>Within this reduced search region, the estimator $\hat{v}$ is provably always
on a path which we term as \emph{candidate path}. This notion enables us to
analyze the distribution of $d(v^{\ast},\hat{v})$, the error distance between
$\hat{v}$ and the true source $v^{\ast}$, on arbitrary tree, which allows us to
obtain for the first time, in the literature provable performance guarantee of
the estimator under limited timestamps. Specifically, on the infinite
$g$-regular tree with uniform sampled timestamps, we get a refined performance
guarantee in the sense of a constant bounded $d(v^{\ast},\hat{v})$. By virtue
of time labeled BFS tree, the estimator still performs fairly well when
extended to more general graphs. Experiments on both synthetic and real
datasets further demonstrate the superior performance of our proposed
algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12913"><span class="datestr">at June 01, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12854">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12854">Compact Data Structures for Shortest Unique Substring Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mieno:Takuya.html">Takuya Mieno</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12854">PDF</a><br /><b>Abstract: </b>Given a string T of length n, a substring u = T[i.. j] of T is called a
shortest unique substring (SUS) for an interval [s, t] if (a) u occurs exactly
once in T, (b) u contains the interval [s, t] (i.e. i \leq s \leq t \leq j),
and (c) every substring v of T with |v| &lt; |u| containing [s, t] occurs at least
twice in T. Given a query interval [s, t] \subset [1, n], the interval SUS
problem is to output all the SUSs for the interval [s, t]. In this article, we
propose a 4n + o(n) bits data structure answering an interval SUS query in
output-sensitive O(occ) time, where occ is the number of returned SUSs.
Additionally, we focus on the point SUS problem, which is the interval SUS
problem for s = t. Here, we propose a \lceil (log2 3 + 1)n \rceil + o(n) bits
data structure answering a point SUS query in the same output-sensitive time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12854"><span class="datestr">at June 01, 2019 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12778">Online Matching with Stochastic Rewards: Optimal Competitive Ratio via Path Based Formulation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vineet.html">Vineet Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Udwani:Rajan.html">Rajan Udwani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12778">PDF</a><br /><b>Abstract: </b>The problem of online matching with stochastic rewards is a variant of the
online bipartite matching problem where each edge has a probability of
"success". When a match is made it succeeds with the probability of the
corresponding edge. Introducing this model, Mehta and Panigrahi (FOCS 2012)
focused on the special case of identical and vanishingly small edge
probabilities and gave an online algorithm which is 0.567 competitive against a
deterministic offline LP. For the case of vanishingly small but heterogeneous
probabilities Mehta et al. (SODA 2015), gave a 0.534 competitive algorithm
against the same LP benchmark.
</p>
<p>We study a generalization of the problem to vertex-weighted graphs and
compare against clairvoyant algorithms that know the sequence of arrivals and
the edge probabilities in advance, but not the outcomes of potential matches.
To the best of our knowledge, no results beating $1/2$ were previously known
for this setting, even for identical probabilities. By introducing a novel
path-based formulation, we show that a natural variant of the RANKING algorithm
achieves the best possible competitive ratio of $(1-1/e)$, for heterogeneous
but vanishingly small edge probabilities. Our result also holds for
non-vanishing probabilities that decompose as a product of two factors, one
corresponding to each vertex of the edge. The idea of a path-based program may
be of independent interest in other online matching problems with a stochastic
component.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12778"><span class="datestr">at June 01, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12753">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12753">Clustering without Over-Representation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmadian:Sara.html">Sara Ahmadian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epasto:Alessandro.html">Alessandro Epasto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ravi.html">Ravi Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahdian:Mohammad.html">Mohammad Mahdian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12753">PDF</a><br /><b>Abstract: </b>In this paper we consider clustering problems in which each point is endowed
with a color. The goal is to cluster the points to minimize the classical
clustering cost but with the additional constraint that no color is
over-represented in any cluster. This problem is motivated by practical
clustering settings, e.g., in clustering news articles where the color of an
article is its source, it is preferable that no single news source dominates
any cluster.
</p>
<p>For the most general version of this problem, we obtain an algorithm that has
provable guarantees of performance; our algorithm is based on finding a
fractional solution using a linear program and rounding the solution
subsequently. For the special case of the problem where no color has an
absolute majority in any cluster, we obtain a simpler combinatorial algorithm
also with provable guarantees. Experiments on real-world data shows that our
algorithms are effective in finding good clustering without
over-representation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12753"><span class="datestr">at June 01, 2019 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12730">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12730">Recursive Sketches for Modular Deep Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghazi:Badih.html">Badih Ghazi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahy:Rina.html">Rina Panigrahy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Joshua_R=.html">Joshua R. Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12730">PDF</a><br /><b>Abstract: </b>We present a mechanism to compute a sketch (succinct summary) of how a
complex modular deep network processes its inputs. The sketch summarizes
essential information about the inputs and outputs of the network and can be
used to quickly identify key components and summary statistics of the inputs.
Furthermore, the sketch is recursive and can be unrolled to identify
sub-components of these components and so forth, capturing a potentially
complicated DAG structure. These sketches erase gracefully; even if we erase a
fraction of the sketch at random, the remainder still retains the `high-weight'
information present in the original sketch. The sketches can also be organized
in a repository to implicitly form a `knowledge graph'; it is possible to
quickly retrieve sketches in the repository that are related to a sketch of
interest; arranged in this fashion, the sketches can also be used to learn
emerging concepts by looking for new clusters in sketch space. Finally, in the
scenario where we want to learn a ground truth deep network, we show that
augmenting input/output pairs with these sketches can theoretically make it
easier to do so.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12730"><span class="datestr">at June 01, 2019 11:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://plus.maths.org/content/democratic-dilemmas">No maths for Europe</a> (<a href="https://mathstodon.xyz/@11011110/102109693915830408"></a>). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) <a href="https://en.wikipedia.org/wiki/Highest_averages_method">formula for its apportionment</a> of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</p>
  </li>
  <li>
    <p>Prominent cryptographers <a href="https://en.wikipedia.org/wiki/Adi_Shamir">Adi Shamir</a> and <a href="https://en.wikipedia.org/wiki/Ross_J._Anderson">Ross J. Anderson</a> were both <a href="https://www.schneier.com/blog/archives/2019/05/why_are_cryptog.html">denied visas to travel to the US</a> for a conference and a book awards ceremony respectively (<a href="https://mathstodon.xyz/@11011110/102112360619663485"></a>, <a href="https://boingboing.net/2019/05/17/denying-cryptographers-problem.html">see also</a>). Bruce Schneier mentions “two other prominent cryptographers who are in the same boat”. Odd and troubling.</p>
  </li>
  <li>
    <p><a href="https://mathlesstraveled.com/2019/05/09/computing-the-euler-totient-function-part-1/">Three</a> <a href="https://mathlesstraveled.com/2019/05/18/computing-the-euler-totient-function-part-2-seeing-phi-is-multiplicative/">new</a> <a href="https://mathlesstraveled.com/2019/05/27/computing-the-euler-totient-function-part-3-proving-phi-is-multiplicative/">blog posts</a> by Brent Yorgey concern the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> (<a href="https://mathstodon.xyz/@11011110/102118180704402052"></a>). Computing it quickly would break RSA; Brent describes using factoring to do better than brute force. The problem is clearly in , and I think it may be a natural candidate for being -intermediate. Igor Pak (who asked me for -intermediate problems when I recently visited UCLA) <a href="https://cstheory.stackexchange.com/q/43954/95">thinks the prime-counting function may be another</a>, but neither function is very combinatorial. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">In a recent blog post I found a couple of combinatorial candidates</a>, but others would be interesting.</p>
  </li>
  <li>
    <p>The image below (<a href="https://commons.wikimedia.org/wiki/File:Gr%C3%BCnbaum-Rigby_configuration,_vector_graphics.svg">as redrawn by Brammers</a>) is the 
<a href="https://en.wikipedia.org/wiki/Gr%C3%BCnbaum%E2%80%93Rigby_configuration">Grünbaum–Rigby configuration</a> (<a href="https://mathstodon.xyz/@11011110/102119858635464298"></a>) with 21 points and lines, 4 points per line, and 4 lines per point. Klein studied it in the complex projective plane in 1879, but it wasn’t known to have this nice real heptagonal realization until Grünbaum and Rigby (1990). The new Wikipedia article on it was started by “Tomo” (whose real-world identity Wikipedia’s arcane outing rules bar me from disclosing, but he just turned 70, so if you figure it out wish him a happy birthday).</p>

    <p style="text-align: center;"><img width="60%" alt="The Grünbaum–Rigby configuration" src="https://11011110.github.io/blog/assets/2019/grunrig.svg" /></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Garden_of_Eden_(cellular_automaton)">Garden of Eden</a> (<a href="https://mathstodon.xyz/@11011110/102129199678568606"></a>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.pnas.org/content/early/2019/05/20/1902572116. Via https://mathstodon.xyz/@helger/102138884170343694">Ono et al prove that almost all Jensen-Pólya polynomials have only real roots</a> (<a href="https://mathstodon.xyz/@11011110/102143915068349382"></a>, <a href="https://mathstodon.xyz/@helger/102138884170343694">via</a>). The Riemann Hypothesis is equivalent to the statement that they all do. The same thing works for similar families of polynomials associated with partition functions and proves a conjecture of Chen. See also <a href="https://phys.org/news/2019-05-mathematicians-revive-abandoned-approach-riemann.html">a popularized account</a> and <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">Ono’s talk slides</a>.</p>
  </li>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/imu-abacus-medal/">The International Mathematical Union is renaming</a> its <a href="https://en.wikipedia.org/wiki/Nevanlinna_Prize">Nevanlinna Prize</a> to be the IMU Abacus Medal (<a href="https://mathstodon.xyz/@11011110/102149453984232922"></a>). The prize is given every four years for major accomplishments in theoretical computer science. The article doesn’t say why rename but it’s because Nevanlinna was a Nazi sympathizer and collaborator. The prize was named after him in the early 1980s because its funding came from Finland, but Nevanlinna also never had much to do with TCS.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Gasarch on proofreading</a> (<a href="https://mathstodon.xyz/@11011110/102154856271421940"></a>). Just as in programming, there’s always one more bug.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ellen_Fetter">Ellen Fetter</a> and <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)">Margaret Hamilton</a>: <a href="https://www.quantamagazine.org/hidden-heroines-of-chaos-ellen-fetter-and-margaret-hamilton-20190520/">Uncredited collaborators with Edward Lorenz at the birth of chaos theory</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/102163812229252010"></a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2019/05/17/science/math-physics-knitting-matsumoto.html">Elisabetta Matsumoto is studying the mathematics of knitting</a> (<a href="https://mathstodon.xyz/@11011110/102177647389031957"></a>, <a href="https://twitter.com/Sabetta_">via</a>), with the hope that it can lead to new programmable metamaterials.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/news/2019/05/ieee-major-science-publisher-bans-huawei-scientists-reviewing-papers">IEEE bans Huawei employees from reviewing submissions to its journals</a> (<a href="https://mathstodon.xyz/@11011110/102183137967208347"></a>, <a href="https://news.ycombinator.com/item?id=20046771">via</a>), saying it is forced to do so by US government sanctions.</p>
  </li>
  <li>
    <p>The UCI University Club (which in other places might be called a faculty club) is next door to the building I work in, and has a bustling side business hosting weddings. Here’s the view that greeted me as I left the office this evening, looking across their lawn towards the gazebo (<a href="https://mathstodon.xyz/@11011110/102188200347058355"></a>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/uclub/index.html"><img src="https://www.ics.uci.edu/~eppstein/pix/uclub/uclub-m.jpg" alt="UCI University Club lawn" style="border-style: solid; border-color: black;" /></a></p>
  </li>
  <li>
    <p>Line arrangements in architecture (<a href="https://mathstodon.xyz/@11011110/102193430755771327"></a>): the beams of <a href="https://en.wikipedia.org/wiki/Mathematical_Bridge">Cambridge’s Mathematical Bridge</a> form tangent lines to its arch and then extend through and support its trusswork, while another set of radial lines tie the structure together. The bridge just looks like a wood truss bridge in real life but <a href="https://commons.wikimedia.org/wiki/File:Mathematical_Bridge_tangents.jpg">this artificially-colored image</a> makes the underlying structure clearer.</p>

    <p style="text-align: center;"><img width="80%" alt="Cambridge's Mathematical Bridge" style="border-style: solid; border-color: black;" src="https://11011110.github.io/blog/assets/2019/cambridgebridge.jpg" /></p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/31/linkage.html"><span class="datestr">at May 31, 2019 09:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/077">TR19-077 |  Consistency of circuit lower bounds with bounded theories | 

	Jan Bydzovsky, 

	Jan  Krajicek, 

	Igor Carboni Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proving that there are problems in $P^{NP}$ that require boolean circuits of super-linear size is a major frontier in complexity theory. While such lower bounds are known for larger complexity classes, existing results only show that the corresponding problems are hard on infinitely many input lengths. For instance, proving almost-everywhere circuit lower bounds is open even for problems in MAEXP. Giving the notorious difficulty of proving lower bounds that hold for all large input lengths, we ask the following question: 

Can we show that a large set of techniques cannot prove that NP is easy infinitely often? 

Motivated by this and related questions about the interaction between mathematical proofs and computations, we investigate circuit complexity from the perspective of logic.

  Among other results, we prove that for any parameter $k \geq 1$ it is consistent with theory $T$ that computational class $C$ is not contained infinitely often in SIZE$(n^k)$, where $(T, C)$ is one of the pairs:

  $T = T^1_2\;$ and $\;C = P^{NP}$, $\quad T = S^1_2\;$ and $\;C = NP$, $\quad T =~ $PV$\;$ and $C = P$.

  In other words, these theories cannot establish infinitely often circuit upper bounds for the corresponding problems. This is of interest because the weaker theory PV already formalizes sophisticated arguments, such as a proof of the PCP Theorem (Pich, 2015). These consistency statements are unconditional and improve on earlier theorems of Krajicek and Oliveira (2017) and Bydzovsky and Muller (2018) on the consistency of lower bounds with PV.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/077"><span class="datestr">at May 30, 2019 10:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img src="http://grigory.github.io/blog/pics/theory-jobs-2019.png" />
Apparently, it’s a busy life being an assistant prof so there were no posts here all year. However, while some of us are decompressing after the NeurIPS deadline, <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
Congratulations to both job seekers and departments/labs who are done with their searches!</p>

<p>In the past my academic uncle Lance Fortnow set this spreadsheet up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from two years ago which also has links to all the previous years). This year the first entry is Lance himself who is moving back to Chicago to be the Dean of the College of Science at the Illinois Institute of Technology. Did Lance get the idea from his advisor <a href="https://en.wikipedia.org/wiki/Michael_Sipser">Michael Sipser</a> who is also a Dean of Science but at MIT? In any case, great to see theoretical computer scientists stepping up to be the deans of science, congratulations!</p>

<p>Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 30, 2019.</p></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2019/"><span class="datestr">at May 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1040381893569171546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html">NSF Panels</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The government shut down in January led to delays at the National Science Foundation and only recently announcing decisions on grants submitted last fall. For those who successfully received awards, congratulations! For those who didn't, don't take it personally, buckle up and try again.<br />
<br />
For those who don't know how the process works, for each grant program, the program directors organize one or more panels which typically meets in person at NSF headquarters in Alexandria, Virginia. A typical panel has about a dozen panelists and twenty or so proposals. Before the panels, each proposal gets at least three reviews by the panelists. Discussions ensue over a day or two, proposals get sorted into categories: Highly Competitive, Competitive, Low Competitive and Not Competitive and then ranked ordered in the top categories.<br />
<br />
There are tight rules for Conflict-of-Interest and those who are conflicted have to leave the room during the discussions on those papers.<br />
<br />
If you do get asked to serve on a panel, you should definitely do so. You get to see how the process works and help influence funding and research directions in your field. You can't reveal when you serve on a particular panel but you can say "Served on NSF Panels" on your CV.<br />
<br />
Panels tend to take proposals that will likely make progress and not take ones less risky. Funding risky proposals is specifically mentioned to the panel but when push comes to shove and there is less funding than worthy proposals, panelists gravitate towards proposals that don't take chances.<br />
<br />
Panels are not unlike conference program committees. It didn't always work this way, it used to be more like journal publications. I remember when the program director would send out proposals for outside reviews and then make funding decisions. That gave the program director more discretion to fund a wider variety of proposals.<br />
<br />
The NSF budget for computing goes up slowly while the number of academic computer scientists grows at a much larger clip. Until this changes, we'll have more and more worthy proposals unfunded, particularly proposals of bold risky projects. That's the saddest part of all.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html"><span class="datestr">at May 29, 2019 08:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-831739446833439686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html">Individual Notions of Fairness You Can Use</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="text-align: center;"><span style="font-size: x-large;"><u>Individual Notions of Fairness You Can Use</u></span></div><br />Our group at Penn has been thinking about when <i>individual </i>notions of fairness might be practically achievable for awhile, and we have <a href="https://arxiv.org/abs/1905.10660">two</a> <a href="https://arxiv.org/abs/1905.10607">new</a> approaches.<br /><br /><span style="font-size: large;"><u>Background</u>:</span><br /><u>Statistical Fairness</u><br />I've written about this before, <a href="http://aaronsadventures.blogspot.com/2017/11/between-statistical-and-individual.html">here</a>. But briefly: there are two families of definitions in the fairness in machine learning literature. The first group of definitions, which I call <i>statistical</i> fairness notions, is far and away the most popular. If you want to come up with your own statistical fairness notion, you can follow this recipe:<br /><ol><li>Partition the world into a small number of "protected sub-groups". You will probably be thinking along the lines of race or gender or something similar when you do this.</li><li>Pick your favorite error/accuracy metric for a classifier. This might literally be classification error, or false positive or false negative rate, or positive predictive value, or something else. Lots of options here. </li><li>Ask that this metric be approximately equalized across your protected groups.</li><li>Finally, enjoy your new statistical fairness measure! Congratulations!</li></ol><div>These definitions are far and away the most popular in this literature, in large part (I think) because they are so immediately actionable. Because they are defined as conditions on a small number of expectations, you can easily check whether your classifier is "fair" according to these metrics, and (although there are some interesting computational challenges) go and try and learn classifiers subject to these constraints. </div><div><br /></div><div>Their major problem is related to the reason for their success: they are defined as conditions on a small number of expectations or <i>averages</i> over people, and so they don't promise much to particular individuals. I'll borrow an example from our <a href="https://arxiv.org/abs/1711.05144">fairness gerrymandering</a> paper from a few years ago to put this in sharp relief. Imagine that we are building a system to decide who to incarcerate, and we want to be "fair" with respect to both gender (men and women) and race (green and blue people). We decide that in our scenario, it is the false positives who are harmed (innocent people sent to jail), and so to be fair, we decide should equalize the false positive rate: across men and women, and across greens and blues. But one way to do this is to jail all green men and blue women. This does indeed equalize the false positive rate (at 50%) across all four of the groups we specified, but is cold comfort if you happen to be a green man --- since then you will be jailed with certainty. The problem was our fairness constraint was never a promise to an individual to begin with, just a promise about the average behavior of our classifier over a large group. And although this is a toy example constructed to make a point, things like this happen in real data too. </div><br /><u>Individual Fairness</u><br />Individual notions of fairness, on the other hand, really do correspond to promises made to individuals. There are at least two kinds of individual fairness definitions that have been proposed: <a href="https://dl.acm.org/citation.cfm?id=2090255">metric fairness</a>, and <a href="http://papers.nips.cc/paper/6355-fairness-in-learning-classic-and-contextual-bandits">weakly meritocratic fairness</a>. Metric fairness proposes that the learner will be handed a <i>task specific similarity metric</i>, and requires that individuals who are close together in the metric should have a similar probability of being classified as positive. Weakly meritocratic fairness, on the other hand, takes the (unknown) labels of an individual as a measure of merit, and requires that individuals who have a higher probability of really having a positive label should have only a higher probability of being classified as positive. This in particular implies that false positive and false negative rates should be equalized <i>across individuals</i>, where now the word <i>rate</i> is averaging over only the randomness of the classifier, not over people. What makes both of these <i>individual</i> notions of fairness is that they impose constraints that bind on all pairs of individuals and not just over averages of people.<br /><br />Definitions like this have the advantage of strong individual-level semantics, which the statistical definitions don't have. But they also have big problems: for metric fairness, the obvious question is: <i>where does the metric come from</i>? Even granting that fairness should be some Lipschitz condition on a metric, it seems hard to pin down what the metric is, and different people will disagree: coming up with the metric seems to encapsulate a large part of the original problem of defining fairness. For weakly meritocratic fairness, the obvious problem is that we don't know what the labels are. Its possible to do non-trivial things if you make assumptions about the label generating process, but its not at all clear you can do any non-trivial learning subject to this constraint if you don't make strong assumptions.<br /><br /><span style="font-size: large;"><u>Two New Approaches:</u></span><br />We have two new approaches, building off of metric fairness and weakly meritocratic fairness respectively. Both have the advantages of statistical notions of fairness in that they can be put into practice without making unrealistic assumptions about the data, and without needing to wait on someone to hand us a metric. But they continue to make meaningful promises to individuals.<br /><br /><u>Subjective Individual Fairness</u><br />Lets start with our variant of metric fairness, which we call subjective individual fairness. (This is joint work with Michael Kearns, our PhD students Chris Jung and Seth Neel, our former PhD student Steven Wu, and Steven's student (our grand student!) Logan Stapleton). The paper is here: <a href="https://arxiv.org/abs/1905.10660">https://arxiv.org/abs/1905.10660</a>. We stick with the premise that "similar people should be treated similarly", and that whether or not it is correct/just/etc., it is at least fair to treat two people the same way, in the sense that we classify them as positive with the same probability. But we don't want to assume anything else.<br /><br />Suppose I were to create a machine learning fairness panel: I could recruit "AI Ethics" experts, moral philosophers, hyped up consultants, people off the street, toddlers, etc. I would expect that there would be as many different conceptions of fairness as there were people on the panel, and that none of them could precisely quantify what they meant by fairness --- certainly not in the form of a "fairness metric". But I could still ask these people, in particular cases, if they thought it was fair that two particular individuals be treated differently or not.<br /><br />Of course, I would have no reason to expect that the responses that I got from the different panelists would be consistent with one another --- or possibly even internally consistent (we won't assume, e.g. that the responses satisfy any kind of triangle inequality). Nevertheless, once we fix a data distribution and a group of people who have opinions about fairness, we have a well defined tradeoff we can hope to manage: any classifier we could choose will have both:<br /><ol><li>Some error rate, and</li><li>Some frequency with which it makes a pair of decisions that someone in the group finds unfair. </li></ol><div>We can hope to find classifiers that optimally trade off 1 and 2: note this is a coherent tradeoff even though we haven't forced the people to try and express their conceptions of fairness into some consistent metric. What we show is that you can do this. </div><div><br /></div><div>Specifically, given a set of pairs that we have determined should be treated similarly, there is an <i>oracle efficient </i>algorithm that can find the optimal classifier subject to the constraint that no pair of individuals that has been specified as a constraint should have a substantially different probability of positive classification. Oracle efficiency means that what we can do is reduce the "fair learning" problem to a regular old learning problem, without fairness constraints. If we can solve the regular learning problem, we can also solve the fair learning problem. This kind of fairness constraint also generalizes in the standard way: if you ask your fairness panel about a reasonably small number of pairs, and then solve the in-sample problem subject to these constraints, the classifier you learn will also satisfy the fairness constraints out of sample. And it works: we implement the algorithm and try it out on the COMPAS data set, with fairness constraints that we elicited from 43 human (undergrad) subjects. The interesting thing is that once you have an algorithm like this, it isn't only a tool to create "fair" machine learning models: its also a new instrument to investigate human conceptions of fairness. We already see quite a bit of variation among our 43 subjects in our preliminary experiments. We plan to pursue this direction more going forward.</div><div><br /></div><div><u>Average Individual Fairness</u></div><div>Next, our variant of weakly meritocratic fairness. This is joint work with Michael Kearns and our student Saeed Sharifi. The paper is here: <a href="https://arxiv.org/abs/1905.10607">https://arxiv.org/abs/1905.10607</a>. In certain scenarios, it really does seem tempting to think about fairness in terms of false positive rates. Criminal justice is a great example, in the sense that it is clear that everyone agrees on which outcome they <i>want</i> (they would like to be released from jail), and so the people we are being unfair to really do seem to be the false positives: the people who should have been released from jail, but who were mistakenly incarcerated for longer. So in our "fairness gerrymandering" example above, maybe the problem with thinking about false positive rates wasn't a problem with <i>false positives</i>, but with <i>rates</i>: i.e. the problem was that the word rate averaged over many people, and so it didn't promise <i>you</i> anything. Our idea is to redefine the word rate. </div><div><br /></div><div>In some (but certainly not all) settings, people are subject to not just one, but many classification tasks. For example, consider online advertising: you might be shown thousands of targeted ads each month. Or applying for schools (a process that is centralized in cities like New York): you apply not just to one school, but to many. In situations like this, we can model the fact that we have not just a distribution over people, but also a distribution over (or collection of) problems. </div><div><br /></div><div>Once we have a distribution over problems, we can define the error rate, or false positive rate, or any other rate you like <i>for individuals. </i>It is now sensible to talk about Alice's false positive rate, or Bob's error rate, because rate has been redefined as an average over problems, for a particular individual. So we can now ask for individual fairness notions in the spirit of the statistical notions of fairness we discussed above! We no longer need to define protected groups: we can now ask that the false positive rates, or error rates, be equalized across all pairs of people. </div><div><br /></div><div>It turns out that given a reasonably sized sample of people, and a reasonably sized sample of problems, it is tractable to find the optimal classifier subject to constraints like this in sample, and that these guarantees generalize out of sample. The in-sample algorithm is again an oracle-efficient algorithm, or in other words, a reduction to standard, unconstrained learning. The generalization guarantee here is a little interesting, because now we are talking about simultaneous generalization in two different directions: to people we haven't seen before, and also to problems we haven't seen before. This requires thinking a little bit about what kind of object we are even trying to output: a mapping from new problems to classifiers. The details are in the paper (spoiler --- the mapping is defined by the optimal dual variables for the empirical risk minimization problem): here, I'll just point out that again, the algorithm is practical to implement, and we perform some simple experiments with it. </div><div><br /></div><div><br /></div><br /><br /><br /></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html"><span class="datestr">at May 28, 2019 10:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19015) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>WINDMILL Early Stage Researcher 9: Optimizing URLLC metadata/data flows using machine learning</p>
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19014) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">Shattering and quasipolynomiality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>An inadequately-explained phenomenon in computational complexity theory is that there are so few natural candidates for <a href="https://en.wikipedia.org/wiki/NP-intermediate">-intermediate problems</a>, problems in  but neither in  nor -complete. Of course, if  there are none, and the <a href="https://11011110.github.io/blog/2019/05/27/Schaefer's dichotomy theorem">dichotomy theorem</a> implies that there are no intermediate Boolean constraint satisfaction problems. But there are a lot of other types of problems in , and a theorem of Ladner<sup id="fnref:l"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:l" class="footnote">1</a></sup> shows that there should be an infinite hierarchy of degrees of hardness within . So where are all the members of this hierarchy, and why are they so shy?</p>

<p>The same thing happens not just for  but for other related complexity classes like <a href="https://en.wikipedia.org/wiki/%E2%99%AFP"></a>. There should be many -intermediate classes but we know even fewer than for . <a href="https://mathstodon.xyz/@11011110/102118180704402052">I recently posted</a> about a discussion I had with Igor Pak on this issue, in which we suggested to each other two number-theoretic candidates for being -intermediate, the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> and the <a href="https://en.wikipedia.org/wiki/Prime-counting_function">prime-counting function</a> (see also <a href="https://cstheory.stackexchange.com/q/43954/95">Igor’s StackExchange question on this</a>). But although they’re in , neither of these functions is very combinatorial.</p>

<p>So anyway, the point of all this is to discuss more candidates for being -intermediate that are, I think, natural and combinatorial. They’re part of a family of problems that include a couple of related candidates for being -intermediate, and even a candidate for being -intermediate. These problems come from computational learning theory, or alternatively they can be seen as coming from mathematical logic, hereditary graph theory, and the theory of the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. And they’re all at what is in some sense the shallow end of the intermediate problems: they’re solvable in quasi-polynomial time, meaning , but not known to be solvable in polynomial time. So this is pretty strong evidence that they’re not complete for their respective complexity classes, but weaker evidence than usual that they’re not polynomial.</p>

<p>In learning theory, a family of sets  is said to <em>shatter</em> another set  (not necessarily belonging to ) if every subset of , including the empty set and  itself, can be obtained by intersecting  with some member of . The <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">Vapnik–Chervonenkis dimension</a> of  is just the size of the largest set that is shattered by  . If we let  (the number of sets in the family) and  (the number of distinct elements in those sets), then the dimension is clearly at most , because sets of size larger than  have too many subsets for them all to be formed by intersection with a member of . Therefore, the following problem can be solved in quasipolynomial time, by a brute-force search of the  small-enough subsets of :<sup id="fnref:lmr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:lmr" class="footnote">2</a></sup></p>

<dl>
  <dt>VC-dimension (largest shattered set)</dt>
  <dd>Input: family of sets , number 

    <p>Output: true if  shatters a set of size , false otherwise.</p>
  </dd>
</dl>

<p>The same quasipolynomial time bound applies to the following related problems,
the first of which is also in  and the second of which is in :</p>

<dl>
  <dt>Smallest non-shattered set</dt>
  <dd>Input: family of sets , number 

    <p>Output: True if there exists a subset  of 
of size  that is not shattered by , false otherwise.</p>
  </dd>
  <dt>Number of shattered sets</dt>
  <dd>Input: family of sets 

    <p>Output: the number of sets shattered by .</p>
  </dd>
</dl>

<p>For the first two problems, being non--complete hinges on the assumption that , but for the number of shattered sets, being non--complete (under <a href="https://en.wikipedia.org/wiki/Polynomial-time_counting_reduction">counting reductions</a>) is unconditional: the output doesn’t provide enough bits of information to encode the answers to all other  problems.
The VC-dimension is hard to approximate under a form of the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, strongly suggesting that it cannot be computed exactly in polynomial time.<sup id="fnref:mr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:mr" class="footnote">3</a></sup></p>

<p>To see that the two existence problems can sometimes both have answers that are logarithmic, it’s helpful to turn to the theory of random graphs, and of <em>the</em> random graph, the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. This graph obeys a collection of <em>extension axioms</em> according to which, for every two disjoint finite subsets of vertices, there exists another vertex adjacent to everything in the first subset and to nothing in the second subset. Using these axioms, we can build up induced copies of any finite or countable subgraph, one vertex at a time, using a greedy algorithm. Based on this property, let’s define a subset  of the vertices in an undirected graph to be <em>extensible</em> if, for every partition of  into two disjoint subsets, there exists another vertex outside  that is adjacent to everything in the first subset and to nothing in the second subset. This is nothing more than being shattered by the neighborhoods of the vertices outside . So we have the following corresponding problems.</p>

<dl>
  <dt>Largest extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has an extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest non-extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has a non-extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest missing induced subgraph</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if there is a graph  on at most  vertices that
is not an induced subgraph of , false otherwise.</p>
  </dd>
  <dt>Number of extensible sets</dt>
  <dd>Input: Undirected graph 

    <p>Output: The number of extensible sets of vertices of .</p>
  </dd>
</dl>

<p>The smallest missing induced subgraph size naturally falls into the complexity class  of problems for which you can guess a solution (the missing subgraph) but then verifying it involves solving a co- problem (is this subgraph missing).
It is greater than the size of the smallest non-extensible set, because if you try to build up a given induced subgraph by adding one vertex at a time greedily you can only get stuck at a non-extensible set. There must be a missing induced subgraph of size at most , because there are  isomorphism classes of -vertex labeled graphs and fewer than  ways of choosing which of the  labeled vertices correspond to vertices of , so for larger values of  than this bound there are more labeled graphs than placements of them as induced subgraphs. Another way of thinking about the smallest missing induced subgraph problem is that we are asking for the largest  for which  is <a href="https://en.wikipedia.org/wiki/Universal_graph">-universal</a>: it contains all graphs on at most  vertices as induced subgraphs.</p>

<p>The smallest non-extensible set and the smallest missing subgraph are both easy on any hereditary class of graphs, because these classes always have a missing subgraph of size . On the other hand, if  is chosen uniformly at random among the -vertex graphs, then any small subset of its vertices is extensible with high probability, so the smallest non-extensible set has expected size .</p>

<p>If these problems are not - and -complete, what are they? Papadimitriou and Yannakakis<sup id="fnref:py"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:py" class="footnote">4</a></sup> define a complexity class , and show that VC-dimension is -complete. Presumably, because it’s so similar, the same is true for the largest extensible set. Maybe it’s possible to prove completeness for the smallest missing induced subgraph in an analogue of  at the level of , and to prove completeness for the number of shattered sets and number of extensible sets in an analogue of  at this level.</p>

<div class="footnotes">
  <ol>
    <li id="fn:l">
      <p>Ladner, Richard (1975), “<a href="https://doi.org/10.1145/321864.321877">On the structure of polynomial time reducibility</a>”, <em>J. ACM</em> 22 (1): 155–171. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:l" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:lmr">
      <p>Linial, Nathan, Mansour, Yishay, and Rivest, Ronald L. (1991), “<a href="https://doi.org/10.1016/0890-5401(91)90058-A">Results on learnability and the Vapnik–Chervonenkis dimension</a>”, <em>Inf. Comput.</em> 90 (1): 33–49. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:lmr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:mr">
      <p>Manurangsi, Pasin, and Rubinstein, Aviad (2017), “<a href="http://proceedings.mlr.press/v65/manurangsi17a.html">Inapproximability of VC dimension and Littlestone’s dimension</a>”, <em>Proc. 2017 Conf. Learning Theory (COLT 2017)</em>, Proceedings of Machine Learning Research 65, pp. 1432–1460. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:mr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:py">
      <p>Papadimitriou, Christos H., and Yannakakis, Mihalis (1996), “<a href="https://doi.org/10.1006/jcss.1996.0058">On limited nondeterminism and the complexity of the V–C dimension</a>”, <em>J. Comput. Syst. Sci.</em> 53 (2): 161–170. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:py" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102170815471019923">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html"><span class="datestr">at May 27, 2019 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3247584017741087776">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html">separating fact from fiction with the 56% of Americans say Arabic Numerals should not be taught in school</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
On the excellent TV show Veep there was a subplot about a political candidate (who himself had failed algebra in HS) objecting to Algebra since it was invented by the Muslims. I don't recall the exact line, but he said something like `Math teachers are terrorists'<br />
This was, of course, fiction.<br />
<br />
The same week I read that 56% of survey respondents say `<u><i>Arabic Numerals' shouldn't be taught in</i></u> <i><u>schools'</u></i> Obviously also a fiction. Perhaps a headline from <i>The Onion</i>.<br />
<br />
No. The story is true.<br />
<br />
See snopes entry on this: <a href="https://www.snopes.com/fact-check/teaching-arabic-numerals/">here</a><br />
<br />
but also see many FALSE but FUNNY websites:<br />
<br />
Sarah Palin wants Arabic Numerals out of the schools: <a href="http://nationalreport.net/sarah-palin-wants-arabic-numerals-banned-americas-schools/">here</a> Funny but false.<br />
<br />
Jerry Brown is forcing students in California to learn Arabic Numerals as part of multi-culturism False by funny:  <a href="https://me.me/i/sharia-law-must-be-stopped-under-gov-brown-students-in-20990368">here</a><br />
<br />
A website urging us to use Roman Numerals (which Jesus used!) False but funny:  <a href="http://freedomnumerals.com/">here</a><br />
<br />
OKAY, what to make of the truth that really, really, 56% of Americans are against Arab Numerals<br />
<br />
1) Bigotry combined with ignorance.<br />
<br />
2) Some of the articles I read about this say its a problem with polls and people. There may be some of that, but still worries me.<br />
<br />
3) In Nazi Germany (WOW- Goodwin's law popped up rather early!) they stopped teaching relativity because Albert Einstein was Jewish (the story is more complicated than that, see <a href="https://www.scientificamerican.com/article/how-2-pro-nazi-nobelists-attacked-einstein-s-jewish-science-excerpt1/">her</a>e). That could of course never happen in America now (or could it, see <a href="https://www.tabletmag.com/jewish-news-and-politics/50097/time-warp">here</a> and <a href="https://www.conservapedia.com/index.php?title=Counterexamples_to_Relativity">here</a>).<br />
<br />
4) There is no danger that we will dump Arabic Numerals. I wonder if we will change there name to Freedom Numerals.<br />
<br />
5) Ignorance of science is a more immediate problem with the anti-vax people. See <a href="https://www.thedailybeast.com/measles-outbreak-grows-with-60-new-cases-across-26-states?ref=home">here</a><br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html"><span class="datestr">at May 27, 2019 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/076">TR19-076 |  The Equivalences of Refutational QRAT | 

	Leroy Chew, 

	Judith Clymo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The solving of Quantified Boolean Formulas (QBF) has been advanced considerably in the last two decades. In response to this, several proof systems have been put forward to universally verify QBF solvers. 
QRAT by Heule et al. is one such example of this and builds on technology from DRAT, a checking format used in propositional logic. 
Recent advances have shown conditional optimality results for QBF systems that use extension variables.
Since QRAT can simulate Extended Q-Resolution, we know it is strong, but we do not know if QRAT has the strategy extraction property as Extended Q-Resolution does. In this paper, we partially answer this question by showing that QRAT with a restricted reduction rule has strategy extraction (and consequentially is equivalent to Extended Q-Resolution modulo NP).
We also extend equivalence to another system, as we show an augmented version of QRAT known as QRAT+, developed by Lonsing and Egly, is in fact equivalent to the basic QRAT. We achieve this by constructing a line-wise simulation of QRAT+ using only steps valid in QRAT.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/076"><span class="datestr">at May 26, 2019 10:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html">More matching-mimicking networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My <a href="https://11011110.github.io/blog/2018/02/01/parallel-matching-in.html">paper with Vijay Vazirani on parallel matching</a> (soon to appear in <a href="http://spaa.acm.org/2019/">SPAA</a>) is based on the idea of a “matching-mimicking network”. If  is a graph with a designated set  of terminal vertices, then a matching-mimicking network for  is another graph  with the same terminals that has the same pattern of matchings. Here, by a <em>pattern of matchings</em>, I mean a family of subsets of , the subsets that can be covered by a matching that also covers all non-terminal vertices. We included a messy case analysis that, after some simplifications due to symmetry, had 21 cases for the matching mimicking networks on at most three terminals.</p>

<p>By now, I think I understand patterns of matchings a lot better, enough to do the three-terminal case in only four cases and to extend the analysis to four terminals in only seven more cases. The starting point is the observation that these patterns of matchings are <a href="https://en.wikipedia.org/wiki/Delta-matroid">even Δ-matroids</a>.</p>

<p>One way to think of a Δ-matroid is that it’s just a convex polyhedron or polytope in Euclidean space of some dimension , with the properties that all vertex coordinates are  or  and all edge lengths are  or . An even Δ-matroid has the stronger property that all edge lengths are , as is true for the three-dimensional regular tetrahedron with vertex coordinates (written in a more compact form as bitvectors) , , , and .</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/tet-in-cube.svg" alt="Regular tetrahedron formed from alternating vertices of a cube" /></p>

<p>Alternatively one can consider the same kind of structure to be a family of sets, drawn from a universe of  elements that correspond to the dimensions of the space. Each set in the family corresponds to a vertex of the polytope and includes the elements whose coordinates are one. So over the three-element set  the same regular tetrahedron can be written as the family of sets</p>



<p>When expressed in this way, the sets of a Δ-matroid obey an exchange axiom: if two sets  and  differ on whether they include some element , then there must exist an element  on which they also differ, so that the symmetric difference of sets  also belongs to the Δ-matroid. By repeatedly applying this axiom one can connect  to  by a geodesic path (in Hamming distance) of two-element moves. For the bases of a matroid, we have a stronger requirement that one of the two elements belongs to  and the other belongs to , or equivalently that all sets have the same size, but a Δ-matroid relaxes this requirement. It’s not even required that ! But in an even Δ-matroid  and  must be distinct, because otherwise the step would be along an edge of length one. Another way of expressing the extra requirements of an even Δ-matroid over an arbitrary Δ-matroid is that all sets must have the same parity (all have even size, or all have odd size).</p>

<p>So anyway, back to matching. Suppose that both  and  are sets drawn from a pattern of matchings. Choose arbitrarily a matching representing each set. Then the symmetric difference of these matchings is a collection of disjoint alternating paths and cycles, and we can get from  to  by 
a sequence of steps in which we take the symmetric difference of the current matching by one of the alternating paths. So this gives us not just one geodesic from  to  but a lot of different geodesics, one for each ordering of the alternating paths. Expressed as an exchange axiom, this means that when two sets  and  differ, the elements on which they differ can be partitioned into pairs, the symmetric differences with which can be performed independently. You can pick any subset of the pairs of differing elements, and change each of those pairs, leaving the rest alone. Because this is a strengthening of the even Δ-matroid axiom, every matching pattern is an even Δ-matroid.</p>

<p>Expressed in polyhedral terms, this stronger exchange axiom means that every two vertices at distance  from each other are connected by a -dimensional hypercube with side length . This is a little weird, because we started with a hypercube but then eliminated half of its vertices (by the parity condition) to get something else. Now we have hypercubes again, of lower dimension. They must be tilted with respect to the coordinate axes: each axis of one of these lower-dimensional hypercubes is tilted at a 45 degree angle with respect to the coordinate system of the overall polytope.</p>

<p>Not every even Δ-matroid obeys this sub-hypercube property. On the other hand, I was expecting the matching patterns that are matroids (all sets are the same size) to be transversal matroids (maximal subsets of vertices on one side of a bipartite graph that can be covered by a matching), and they aren’t. There is a six-element non-transversal matroid, whose six elements are the edges of a triangle with doubled edges and whose sets are pairs of edges from different sides of the triangle. But it is the pattern of matchings of a tree in which the (non-terminal) root has three children, each of which has two terminals as its children.</p>

<p>Conveniently, whether a 0-1 polyhedron can be represented by a pattern of matchings depends only on its shape and not on its orientation. You can obviously permute the coordinates of a polyhedron that represents a pattern of matchings, by relabeling which coordinate corresponds to which terminal vertex. But you can also reflect the polyhedron across any one of its coordinates by modifying the graph whose matchings represent it in the following way: turn the  terminal vertex for that coordinate into a non-terminal, and attach a new degree-one terminal vertex to it. These permutations and reflections generate all the symmetries of the hypercube in which the 0-1 polyhedron lives. So to find small matching-mimicking networks, we only need to look at one representative 0-1 polyhedron in each symmetry class. If we find a small network for this representative, we can modify it to create a different small matching-mimicking network for every other 0-1 polyhedron with the same shape.</p>

<p>So what are the possible shapes? Let’s define the dimension of a Δ-matroid to be the number of coordinates of the polytope that take both values,  and , at different vertices. Then a 0-dimensional even Δ-matroid must be a single point (), there are no 1-dimensional even Δ-matroids, and a two-dimensional even Δ-matroid must be a line segment (). There are two three-dimensional even Δ-matroids: a triangle  and the tetrahedron shown above, . The cube exchange axiom for patterns of matching starts to kick in for four-dimensional even Δ-matroids, whose vertices must be subsets of the four-dimensional hyperoctahedron . (This is the shape formed from a four-dimensional hypercube by keeping only vertices with the same parity as each other, just as we formed a regular tetrahedron by doing the same thing to a three-dimensional cube.) Here’s a drawing of  from <a href="https://11011110.github.io/blog/2010/09/26/in-response-to.html">an earlier post</a>:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2010/k7/cocktail2.svg" alt="The hyperoctahedral graph K_{2,2,2,2}" /></p>

<p>If there are no two opposite vertices, we get  (a regular tetrahedron, again, but embedded in a four-dimensional way into the hypercube). Otherwise, we must take at least two pairs of opposite vertices to form a square, and the cases are  (only the square),  (a square pyramid),  (an octahedron), ,  (an octahedral pyramid), and  (the hyperoctahedron). All of these polyhedra can be represented as matching-mimicking networks with the additional property that all vertices are terminals:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/4-terminal-mm.svg" alt="Matching-mimicking networks for up to four terminals" /></p>

<p>Based on these small examples, it’s tempting to guess that when a pattern of matchings includes the empty set, the whole pattern is just the set of matchings on a graph whose edges are the pairs of terminals in the pattern. But it isn’t true. The square pyramid , for instance, can represent the pattern of matchings</p>



<p>with the empty set at the apex of the pyramid. In this pattern, even though one can match terminal pairs <span style="white-space: nowrap;">— or —,</span> one can’t take the union of those two matchings and cover all four terminals. (This is what you get by reflecting the two middle terminals of the network shown above for ; its matching-mimicking network is a tree with two interior non-terminals and four terminal leaves.) My guess is that the number of patterns of matching should grow quickly relative to the number of graphs, so for large enough numbers of terminals it should not be possible to use graphs without non-terminals or their complements. But I haven’t taken the case analysis far enough to find an example of this.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102160605632804102">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html"><span class="datestr">at May 25, 2019 09:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/075">TR19-075 |  Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems | 

	Lijie Chen, 

	Dylan McKay, 

	Cody Murray, 

	Ryan Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems

A frontier open problem in circuit complexity is to prove P^NP is not in SIZE[n^k] for all k; this is a necessary intermediate step towards NP is not in P/poly. Previously, for several classes containing P^NP, including NP^NP, ZPP^NP, and S_2 P, such lower bounds have been proved via Karp-Lipton-style Theorems: to prove C is not in SIZE[n^k] for all k, we show that C subset Ppoly implies a ``collapse'' D = C for some larger class D, where we already know D is not in SIZE[n^k] for all k. 

It seems obvious that one could take a different approach to prove circuit lower bounds for P^NP that does not require proving any Karp-Lipton-style theorems along the way. We show this intuition is wrong: (weak) Karp-Lipton-style theorems for P^NP are equivalent to fixed-polynomial size circuit lower bounds for P^NP. That is, P^NP not subset SIZE[n^k] for all k if and only if (NP is in P/poly implies PH is in i.o.-P^NP/n).
		
Next, we present new consequences of the assumption NP is in P/poly, towards proving similar results for NP circuit lower bounds. We show that under the assumption, fixed-polynomial circuit lower bounds for NP, nondeterministic polynomial-time derandomizations, and various fixed-polynomial time simulations of NP are all equivalent. Applying this equivalence, we show that circuit lower bounds for NP imply better Karp-Lipton collapses. That is, if NP is not in SIZE[n^k] for all k, then for all C in { ParP, PP, PSPACE, EXP }, C is in P/poly implies C is in i.o.-NP/n^eps for all eps &gt; 0. Note that unconditionally, the collapses are only to MA and not NP.
		
We also explore consequences of circuit lower bounds for a sparse language in NP. Among other results, we show if a polynomially-sparse NP language does not have n^(1+eps)-size circuits, then MA is in i.o.-NP/O(log n), MA is in i.o.-P^{NP[O(log n)]}, and NEXP is not in SIZE[2^o(m)]. Finally, we observe connections between these results and the ``hardness magnification'' phenomena described in recent works.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/075"><span class="datestr">at May 25, 2019 06:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15910">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/">Selected Papers at CCC 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Some papers from the accepted list of this year’s Computational Complexity Conference</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/unknown-122/" rel="attachment wp-att-15912"><img width="150" alt="" class="alignright  wp-image-15912" src="https://rjlipton.files.wordpress.com/2019/05/unknown-1.jpeg?w=150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ UB CSE ]</font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman is a long-time friend of Ken and I, and is a long-time researcher in complexity theory. Alan was the first president of the organizing <a href="https://www.computationalcomplexity.org/governance.php">body</a> for the Computational Complexity Conferences (CCC). </p>
<p>
Today we salute the <img src="https://s0.wp.com/latex.php?latex=%7B0b100010%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0b100010}" class="latex" title="{0b100010}" />th edition of the conference and discuss some of the accepted papers.</p>
<p>
The conference, and the governing body, have changed names over the years; by any name it remains an important conference. Alan <a href="https://www.computationalcomplexity.org/documents/first-cfp.pdf">chaired</a> the first program committee with Steve Mahaney and <a href="https://dl.acm.org/citation.cfm?id=648296&amp;picked=prox">edited</a> the first proceedings, in 1986. </p>
<p>
Ken recently saw Alan two weeks ago at the banquet for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> honoring Steve Cook at the University of Toronto. We will cover event, once <a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/">exams</a> are done. Ken saw another of the CCC past presidents there—if you wish to guess who, a hint is it was one of Cook’s past students.</p>
<ul>
<li>
Dieter van Melkebeek, 2012-2018 <p></p>
</li><li>
Peter Bro Miltersen, 2009-2012 <p></p>
</li><li>
Pierre McKenzie, 2006-2009 <p></p>
</li><li>
Lance Fortnow, 2000-2006 <p></p>
</li><li>
Eric Allender, 1997-2000 <p></p>
</li><li>
Steven Homer, 1994-1997 <p></p>
</li><li>
Timothy Long, 1992-1994 <p></p>
</li><li>
Stephen Mahaney, 1988-1992 <p></p>
</li><li>
Alan Selman, 1985-1988
</li></ul>
<p>
Although we do not usually do announcements, we note from the conference <a href="https://computationalcomplexity.org">website</a>:</p>
<blockquote><p><b> </b> <em> Details of the local arrangements for CCC 2019 and the preceding events, including the DIMACS Day of Tutorials, are available. Early registration runs till June 26. </em>
</p></blockquote>
<p>
</p><p></p><h2> Six Papers with Some Comments </h2><p></p>
<p></p><p>
Here are some papers that I, Dick, found interesting from the list of accepted papers. All accepted papers are interesting, of course. I selected six that were on topics that were directly connected with my interests.</p>
<p>
<b>Criticality of Regular Formulas</b>—<a href="http://www.math.toronto.edu/rossman/criticality.pdf">paper</a><br />
Benjamin Rossman<br />
<i>I thought this was about regular expressions. Shows something about me.</i> Here “regular” means the in-degree of gates being the same at each level of the circuit. This condition seems likely to be removable as Rossman conjectures, but I doubt it will be easy. The term “criticality” is a parameter that measures how much a random restriction reduces the size of a formula. Think switching lemma.</p>
<p>
<b>Typically-Correct Derandomization for Small Time and Space</b>—<a href="https://arxiv.org/abs/1711.00565">paper</a><br />
William Hoza<br />
<i>I like the notion of typically-correct.</i> Their algorithms work by treating the input as a source of randomness. This idea was pioneered by Oded Goldreich and Avi Wigderson. The title of their 2002 <a href="http://www.wisdom.weizmann.ac.il/~oded/p_rnd02.html">article</a> “Derandomization that is rarely wrong from short advice that is typically good”, gives away how one can prove such results. </p>
<p>
<b>Optimal Short-Circuit Resilient Formulas</b>—<a href="https://arxiv.org/abs/1807.05014">paper</a><br />
Mark Braverman, Klim Efremenko, Ran Gelles, and Michael Yitayew <br />
<i>This is on a kind of fault-tolerance.</i> They consider fault-tolerant boolean formulas in which the output of a faulty gate is stuck at one of the gate’s inputs. This is an interesting model of errors, and they show roughly: any formula can be converted into a formula that is not too much bigger and survives even if about <img src="https://s0.wp.com/latex.php?latex=%7B1%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/5}" class="latex" title="{1/5}" /> of the gates are faulty. A surprise is that they use a method related to <i>blockchains</i>. Hmmmmm. Interesting.</p>
<p>
<b>Fourier and Circulant Matrices are Not Rigid</b>—<a href="https://arxiv.org/pdf/1902.07334.pdf">paper</a><br />
Allen Liu and Zeev Dvir <br />
<i>A matrix is rigid if its rank cannot be reduced significantly by changing a small number of entries.</i> As you probably know there are plenty of rigid matrices—take random ones—but no provable examples of explicit ones. Their beautiful results prove that specific families of matrices are not rigid. These families include ones that were long thought to be rigid. The highlight of this work could be that it suggests new families that may be rigid. </p>
<p>
<b>Average-Case Quantum Advantage with Shallow Circuits</b>—<a href="https://arxiv.org/pdf/1810.12792.pdf">paper</a><br />
François Le Gall <br />
<i>A quest, the quest that tops all others—is the search for evidence that quantum computers are better than classic ones.</i> Of course, this is nearly impossible, since P=PSPACE is an open problem. So one looks at special classes of computations. See <a href="https://arxiv.org/pdf/1612.05903.pdf">here</a> for how the quest for “quantum advantage” meets up with computational complexity.</p>
<p>
<b>Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems</b><br />
<a href="https://eccc.weizmann.ac.il/report/2019/075/">paper</a><br />
Lijie Chen, Dylan McKay, Cody Murray, and Ryan Williams <br />
<i>Of course I think this is an interesting paper.</i> There is the famous H-score. Perhaps there could be a T-score. This would be the number of times your name is in the title of a published paper. Thus Ron Rivest, for example, has a huge T-score.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your selected papers? </p>
<p>
[Added link to Relations and Equivalences… paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/"><span class="datestr">at May 25, 2019 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4982821151012814632">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html">Logic Then and Now</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This week I attended the <a href="https://asl2019.commons.gc.cuny.edu/">Association of Symbolic Logic North American Annual Meeting</a> in New York City, giving a talk on P v NP.<br />
<br />
First I must share the announcement that ASL member Tuna Antinel of Lyon 1 University has been arrested in Turkey for his political beliefs. <a href="http://math.univ-lyon1.fr/SoutienTunaAltinel/">This website</a> (<a href="https://webusers.imj-prg.fr/~adrien.deloro/">English version</a>) has details and how to show your support.<br />
<br />
I last attended the ASL annual meeting at Notre Dame in 1993 as a young assistant professor. Back then I talked about <a href="https://doi.org/10.1137/S0097539793248305">then recent work</a> using a special kind of generic oracle to make the Berman-Hartmanis isomorphism conjecture true. I remember someone coming up to me after the talk saying how excited they were to see such applications of logic. I'm not a theoretical computer scientist, I'm a applied logician.<br />
<br />
I asked at my talk this year and maybe 2-3 people were at that 1993 meeting. The attendance seemed smaller and younger, though that could be my memory playing tricks. I heard that the 2018 meeting in Macomb, Illinois drew a larger crowd. New York is expensive and logicians don't get large travel budgets.<br />
<br />
Logic like theoretical computer science has gotten more specialized so I was playing catch up trying to follow many of the talks. Mariya Soskova of Wisconsin talked about enumeration degrees that brought me back to the days I sat in logic classes and talks at the University of Chicago. A set A is enumeration reducible to B if from an enumeration of B you can compute an enumeration of A and Mariya gave a great overview of this area.<br />
<br />
I learned about the status of an open problem for Turing reducibility: Is there a non-trivial automorphism of the Turing Degrees? A degree is the equivalence class where each class are the languages all computably Turing-reducible to each other. So the question asks if there is a bijection f mapping degrees to degrees, other than identity, that preserves reducibility or lack thereof.<br />
<br />
Here's what's known: There are countably many such automorphisms. There is a definable degree C in the arithmetic hierarchy, such that if f(C) = C then f is the identity. Also if f is the identity on all the c.e.-degrees (those equivalence classes containing a computably enumerable set), then f is the identity on all the degrees. Still open if there is more than one automorphism.<br />
<br />
<br /></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html"><span class="datestr">at May 24, 2019 01:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=355">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/05/22/tcs-talk-wednesday-may-29th-lior-kamma-aarhus-university/">TCS+ talk: Wednesday, May 29th — Lior Kamma, Aarhus University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 29th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). <strong>Lior Kamma</strong> from Aarhus University will speak about “<em>Lower Bounds for Multiplication via Network Coding</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Multiplication is one of the most fundamental computational problems, yet its true complexity remains elusive. The best known upper bound, very recently proved by Harvey and Van Der Hoven shows that two <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n" class="latex" title="n" />-bit numbers can be multiplied via a boolean circuit of size <img src="https://s0.wp.com/latex.php?latex=O%28n+%5Clg+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="O(n \lg n)" class="latex" title="O(n \lg n)" />.</p>
<p>We prove that if a central conjecture in the area of network coding is true, then any constant degree Boolean circuit for multiplication must have size <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n+%5Clg+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\Omega(n \lg n)" class="latex" title="\Omega(n \lg n)" />, thus (conditioned on the conjecture) completely settling the complexity of multiplication circuits. We additionally revisit classic conjectures in circuit complexity, due to Valiant, and show that the network coding conjecture also implies one of Valiant’s conjectures.</p>
<p>Joint work with Peyman Afshani, Casper Freksen and Kasper Green Larsen</p></blockquote>
<p><span id="more-355"></span></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/05/22/tcs-talk-wednesday-may-29th-lior-kamma-aarhus-university/"><span class="datestr">at May 22, 2019 08:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=639">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2019/05/22/statement-of-concern-regarding-marijuana-in-massachusetts/">Statement of concern regarding Marijuana in Massachusetts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>You can read it <a href="https://gallery.mailchimp.com/d93a0b5fcff1daf7b38ecd20c/files/4a4118a7-45ed-4910-b3d1-c64a5e822417/MA_MJ_Policy_Statement_of_Concern_5_9_19_FINAL.pdf">here</a>. If you don’t want to click, some key takeaways are:</p>
<ul>
<li><span class="fontstyle0">We disagree with how marijuana policy is being shaped in the Commonwealth.</span></li>
<li><span class="fontstyle0">The science is clear; marijuana, specifically the psychoactive chemical THC (delta-9-tetrahydrocannabinol), has the potential to do significant harm to public health.</span></li>
<li><span class="fontstyle0">Diversion of high THC products (≥10%), vapes and edibles, to MA youth is a growing concern.</span></li>
<li><span class="fontstyle0">When public health is not prioritized in the regulation of addictive substances, the public and our young people are put at risk.</span></li>
</ul>
<p>You can also find in the statement a list of negative effects of THC.  This is all signed by a dozen+ doctors. The various marijuana players with zero medical knowledge will probably dismiss the experts’ opinion with, at best, a shrug. Instead, they are looking into opening <a href="https://www.boston.com/news/local-news/2019/05/16/massachusetts-marijuana-cafes-social-consumption">marijuana cafes</a>. And the first marijuana retail store will open in Newton <a href="https://patch.com/massachusetts/newton/newtons-first-recreational-marijuana-shop-has-grand-opening-date">this Saturday</a>.</p>
<p>If you want to get even more worked up about marijuana reading my <a href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/">previous post</a> might help.</p>
<p>Finally, <a href="http://www.mapreventionalliance.org/">on June 5^th there will be a luncheon event at the JFK Library titled: Marijuana: Addiction, Mental Health and Policy – Advances in Research…What have we learned in the past 5 years?</a></p></div>







<p class="date">
by Emanuele <a href="https://emanueleviola.wordpress.com/2019/05/22/statement-of-concern-regarding-marijuana-in-massachusetts/"><span class="datestr">at May 22, 2019 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/074">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/074">TR19-074 |  Finding a Nash Equilibrium Is No Easier Than Breaking Fiat-Shamir | 

	Chethan Kamath, 

	Arka Rai Choudhuri, 

	Pavel Hubacek, 

	Krzysztof Pietrzak, 

	Alon Rosen, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Fiat-Shamir heuristic transforms a public-coin interactive proof into a non-interactive argument, by replacing the verifier with a cryptographic hash function that is applied to the protocol’s transcript. Constructing hash functions for which this transformation is sound is a central and long-standing open question in cryptography.

We show that solving the End-of-Metered-Line problem is no easier than breaking the soundness of the Fiat-Shamir transformation when applied to the sumcheck protocol. In particular, if the transformed protocol is sound, then any hard problem in #P gives rise to a hard distribution in the class CLS, which is contained in PPAD.

Our main technical contribution is a stateful incrementally verifiable procedure that, given a SAT instance over n variables, counts the number of satisfying assignments. This is accomplished via an exponential sequence of small steps, each computable in time poly(n). Incremental verifiability means that each intermediate state includes a sumcheck-based proof of its correctness, and the proof can be updated and verified in time poly(n).

Combining our construction with a hash family proposed by Canetti et al. [STOC 2019] gives rise to a distribution in the class CLS, which is provably hard under the assumption that any one of a class of fully homomorphic encryption (FHE) schemes has almost-optimal security against quasi-polynomial time adversaries, and under the additional worst-case assumption that there is no polynomial time algorithm for counting the number of satisfying assignments for formulas over a polylogarithmic number of variables.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/074"><span class="datestr">at May 22, 2019 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/21/congratulations-dr-tillman">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/21/congratulations-dr-tillman.html">Congratulations, Dr. Tillman!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Today I participated in the successful dissertation defense of Bálint Tillman, a student of Athina Markopoulou in the <a href="http://www.networkedsystems.uci.edu/">UCI Graduate Program in Networked Systems</a>.</p>

<p>Bálint has been investigating problems connected with the <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem">Erdős–Gallai theorem</a>, which states that it is possible to test whether a sequence of numbers is the degree distribution of a graph (the sequence of numbers of vertices of each possible degree) and if so, find a graph with that degree distribution, in polynomial time. The degree distribution can be extended to a matrix called the <em>joint degree distribution</em>, which specifies the number of pairs of adjacent vertices with each combination of degrees, and to higher-order tensors specifying the degrees of subgraphs with more than two vertices.</p>

<p>In his INFOCOM 2015 paper “<a href="https://doi.org/10.1109/INFOCOM.2015.7218534">Construction of simple graphs with a target joint degree matrix and beyond</a>”, Bálint showed that one can recognize the joint degree distributions of simple graphs, and reconstruct a graph with that distribution, in polynomial time. The algorithm works equally well when the vertices are distinguished in other ways than by degree and the input matrix specifies the target number of edges with each pair of degrees between each class of vertices. Later, in KDD 2017, he extended these results to directed graphs and at NetSci 2018 he showed how to find a realization with as few connected components as possible.</p>

<p>On the other hand, if one adds only a little bit of extra information to the joint degree distribution, such as the total number of triangles in the graph, it becomes NP-complete to recognize whether the input describes a valid graph and NP-hard to reconstruct a graph that realizes a given description. This comes from a poster by Bálint with Will Devanny and me at NetSci 2016, where we found the reduction from graph 3-coloring depicted below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/jdm-tri-hard.svg" alt="NP-completeness reduction from 3-coloring to realizability of joint degree matrices with numbers of triangles" /></p>

<p>To perform an NP-hardness reduction, one should start with a graph for which it’s hard to test 3-colorability, and translate it into an instance of whatever other problem you want to prove hard. But instead let’s pretend for now that we start with a little more information: a graph that’s known to be 3-colorable, and a specific 3-coloring of it.
To turn this into a hard problem for realizability of joint distribution plus number of triangles, we add a triangle to the graph, representing the three colors, and connect each original graph vertex to the new triangle vertex for its color. Then we add enough hair (in the form of degree-one vertices) to the augmented graph to make all vertices have distinct degrees, except within the triangle of new vertices where all three degrees should be the same. Now take as the result of the reduction the pair  of the joint degree distribution and number of triangles in the augmented graph.</p>

<p>But now the trick is that  can be computed directly from your starting graph, without knowing its coloring or even whether it is colorable.
Because the degrees are distinct, and  tells you the number of edges for each combination of degrees, any realization of  must contain a copy of your starting graph augmented by a triangle. The graph and the triangle might be connected to each other differently than they were before, but their connection pattern must still correspond to a different valid 3-coloring, because otherwise you would form some extra triangles in the graph (one for each invalidly-colored edge) and not correctly match the value of . So  is realizable if and only if your starting graph has a 3-coloring. This reduction proves that testing realizability of pairs  is NP-complete.</p>

<p>There’s even more material along these lines in Bálint’s dissertation, but some of it is not yet published. I think the plan is to get all of that submitted over the summer before
Bálint starts a new position at Google.
Congratulations, Bálint!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102137081740561085">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/21/congratulations-dr-tillman.html"><span class="datestr">at May 21, 2019 06:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15894">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/">Making Up Tests</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>It’s harder to make up tests than to take them</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/ken/" rel="attachment wp-att-15897"><img width="150" alt="" class="alignright  wp-image-15897" src="https://rjlipton.files.wordpress.com/2019/05/ken.png?w=150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Recent photo ]</font></td>
</tr>
</tbody>
</table>
<p>
Ken Regan has been busy these last few days working on making a final exam, giving the exam, and now grading the exam. </p>
<p>
Today Ken and I want to talk about tests.</p>
<p>
I also have a test for you. You can jump right to our test of knowledge. Do not, please, use any search tools, especially Google.<span id="more-15894"></span></p>
<p>
</p><p></p><h2> Test Theory </h2><p></p>
<p></p><p>
Ken recently made up a final exam. We both have had to make countless tests over the years. I was never trained in how to make a good test. Nor how to make a test at all. I am still puzzled about how to do it.</p>
<p>
Avi Wigderson once told me that Michael Rabin only asked questions on his exams that he had stated already in lectures. Is there a theory of what makes a proper test? I do not know any.</p>
<p>
Suppose that before the exam we lectured and the students learned <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />: Here <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> are true statements and <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> are false statements. A rote type question might be: </p>
<blockquote><p><b> </b> <em> <i>Is the statement <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> true or false?</i> </em>
</p></blockquote>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> would be in either <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. This type of question is purely a memory problem. </p>
<p>
A more difficult test would have questions like: </p>
<blockquote><p><b> </b> <em> <i>Is <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> true or false</i> </em>
</p></blockquote>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> would be equivalent to some <img src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S'}" class="latex" title="{S'}" /> that is in <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> or in <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. The equivalence between <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S'}" class="latex" title="{S'}" /> would require only the application of a few simple logical rules. This is much harder for students. In the limit we could have <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S'}" class="latex" title="{S'}" /> far apart, even could have it an open problem if <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S'}" class="latex" title="{S'}" /> are equivalent. </p>
<p>
</p><p></p><h2> Our Test </h2><p></p>
<p></p><p>
<b>No looking at Google please.</b></p>
<p>
<i>Question 1</i>: We all know that Dick Karp created the P=NP question. What is Dick’s middle name? </p>
<ol>
<li>
Mark<p></p>
<p></p></li><li>
Manning <p></p>
</li><li>
Mathew <p></p>
</li><li>
Richard
</li></ol>
<p>
<i>Question 2</i>: This year is the fifty-first anniversary of STOC. Where was the first one held? </p>
<ol>
<li>
Marina del Rey, CA <p></p>
</li><li>
Massapequa, NY <p></p>
</li><li>
Boston, MA <p></p>
</li><li>
Chicago, IL
</li></ol>
<p>
<i>Question 3</i>: Which of these did <b>not</b> happen in 1969? </p>
<ol>
<li>
The first automatic teller machine in the United States is installed. <p></p>
</li><li>
The $500 bills are officially removed from circulation. <p></p>
</li><li>
The first The Limited store opens, in San Francisco. <p></p>
</li><li>
The New York Mets win the World Series.
</li></ol>
<p>
<i>Question 4</i>: The first STOC conference program committee included: </p>
<ol>
<li>
No women. <p></p>
</li><li>
A person named Mike. <p></p>
</li><li>
A person named Pat. <p></p>
</li><li>
All the above.
</li></ol>
<p>
<i>Question 5</i>: How do you tell if you are a “theoretical computer scientist”? </p>
<ol>
<li>
You wear flip-flops in the winter. <p></p>
</li><li>
You regularly attend STOC. <p></p>
</li><li>
You wear glasses. <p></p>
</li><li>
You cannot program a computer.
</li></ol>
<p>
<i>Question 6</i>: “Cooking” a chess problem means: </p>
<ol>
<li>
Showing it is in a family of NP-complete problems on <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> boards. <p></p>
</li><li>
Showing it has two or more solutions (or no solutions). <p></p>
</li><li>
Showing it cannot be solved by Steve Cook. <p></p>
</li><li>
Showing that it cannot be solved by the best-move strategy.
</li></ol>
<p>
<i>Question 7</i>: The other theory conference is called FOCS. Which of these is true about this conference: </p>
<ol>
<li>
The name was selected by a person named Edward. <p></p>
</li><li>
It has never had parallel sessions. <p></p>
</li><li>
It was originally called Symposium on Switching Circuit Theory and Logical Design. <p></p>
</li><li>
The artwork for the proceedings cover is by an artist named Smith, who never published in the conference.
</li></ol>
<p>
<i>Question 8</i>: What do the STOC conferences have in common with last night’s final episode of <i>Game of Thrones</i>? </p>
<ol>
<li>
Both had flying horses and whistling pigs. <p></p>
</li><li>
No dragons were harmed during either. <p></p>
</li><li>
Both have left many big questions unanswered. <p></p>
</li><li>
Both are explained by the “Prisoner’s Dilemma” game solution.
</li></ol>
<p>
<i>Question 9</i>: STOC has been held on each of these islands except: </p>
<ol>
<li>
Long Island, NY. <p></p>
</li><li>
Puerto Rico. <p></p>
</li><li>
Crete. <p></p>
</li><li>
Vancouver Island.
</li></ol>
<p>
<i>Question 10</i>: What term appears in the titles of three award-winning STOC/FOCS papers since 2016? </p>
<ol>
<li>
Quantum. <p></p>
</li><li>
Quadratic/subquadratic. <p></p>
</li><li>
Quadtree. <p></p>
</li><li>
Quasi/quasipolynomial.
</li></ol>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Answers: Note 1a means question 1 and answer 1 and so on. This is a wordpress issue.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/answers4/" rel="attachment wp-att-15899"><img src="https://rjlipton.files.wordpress.com/2019/05/answers4.png?w=300&amp;h=47" alt="" width="300" class="aligncenter size-medium wp-image-15899" height="47" /></a></p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/"><span class="datestr">at May 21, 2019 12:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4249">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">Online Optimization Post 5: Bregman Projections and Mirror Descent</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 In this post we return to the generic form of the FTRL online optimization algorithm. If the cost functions are linear, as they will be in all the applications that I plan to talk about, the algorithm is:</p>
<p>
<a name="eq.ftrl.def"></a></p><a name="eq.ftrl.def">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k%2C+x+%5Crangle+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle   x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} \langle \ell_k, x \rangle \ \ \ \ \ (1)" class="latex" title="\displaystyle   x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} \langle \ell_k, x \rangle \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.ftrl.def"></a> where <img src="https://s0.wp.com/latex.php?latex=%7BK%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K\subseteq {\mathbb R}^n}" class="latex" title="{K\subseteq {\mathbb R}^n}" /> is the convex set of feasible solutions that the algorithm is allowed to produce, <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+%5Clangle+%5Cell_k+%2C+x+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \rightarrow \langle \ell_k , x \rangle}" class="latex" title="{x \rightarrow \langle \ell_k , x \rangle}" /> is the linear loss function at time <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BR%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R: K \rightarrow {\mathbb R}}" class="latex" title="{R: K \rightarrow {\mathbb R}}" /> is the strictly convex regularizer.</p>
<p>
If we have an unconstrained problem, that is, if <img src="https://s0.wp.com/latex.php?latex=%7BK%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K= {\mathbb R}^n}" class="latex" title="{K= {\mathbb R}^n}" />, then the optimization problem <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> has a unique solution: the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " /></p>
<p> and we can usually both compute <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> efficiently in an algorithm and reason about <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> effectively in an analysis.</p>
<p>
Unfortunately, we are almost always interested in constrained settings, and then it becomes difficult both to compute <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> and to reason about it.</p>
<p>
A very nice special case happens when the regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> acts as a <em>barrier function</em> for <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, that is, the (norm of the) gradient of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> goes to infinity when one approaches the boundary of <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />. In such a case, it is impossible for the minimum of <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> to occur at the boundary and the solution will be again the unique <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " /></p>
<p>
We swept this point under the rug when we studied FTRL with negative-entropy regularizer in the settings of experts, in which <img src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K = \Delta}" class="latex" title="{K = \Delta}" /> is the set of probability distributions. When we proceeded to solve <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> using Lagrange multipliers, we ignored the non-negativity constraints. The reason why it was ok to do so was that the negative-entropy is a barrier function for the non-negative orthant <img src="https://s0.wp.com/latex.php?latex=%7B%28%7B%5Cmathbb+R%7D_%7B%5Cgeq+0%7D%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{({\mathbb R}_{\geq 0})^n}" class="latex" title="{({\mathbb R}_{\geq 0})^n}" />.</p>
<p>
Another important special case occurs when the regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) = c || x||^2}" class="latex" title="{R(x) = c || x||^2}" /> is a multiple of length-squared. In this case, we saw that we could “decouple” the optimization problem by first solving the unconstrained optimization problem, and then projecting the solution of the unconstrained problem to <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+y_%7Bt%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+c+%7C%7C+y%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle y_{t} = \arg\min_{y\in {\mathbb R}^n} \ c || y||^2 + \sum_{k=1}^{t-1} \langle \ell_k, y \rangle " class="latex" title="\displaystyle y_{t} = \arg\min_{y\in {\mathbb R}^n} \ c || y||^2 + \sum_{k=1}^{t-1} \langle \ell_k, y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5CPi_K+%28y_t%29+%3D+%5Carg%5Cmin+_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \Pi_K (y_t) = \arg\min _{x\in K} || x - y_t || " class="latex" title="\displaystyle  x_t = \Pi_K (y_t) = \arg\min _{x\in K} || x - y_t || " /></p>
<p> Then we have the closed-form solution <img src="https://s0.wp.com/latex.php?latex=%7By_t+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell+_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t = - \frac 1{2c} \sum_{k=1}^{t-1} \ell _k}" class="latex" title="{y_t = - \frac 1{2c} \sum_{k=1}^{t-1} \ell _k}" /> and, depending on the set <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />, the projection might also have a nice closed-form, as in the case <img src="https://s0.wp.com/latex.php?latex=%7BK%3D+%5B0%2C1%5D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K= [0,1]^n}" class="latex" title="{K= [0,1]^n}" /> that comes up in results related to regularity lemmas.</p>
<p>
As we will see today, this approach of solving the unconstrained problem and then projecting on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> works for every regularizer, for an appropriate notion of projection called the <em>Bregman projection</em> (the projection will depend on the regularizer). </p>
<p>
To define the Bregman projection, we will first define the <em>Bregman divergence</em> with respect to the regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />, which is a non-negative “distance” <img src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D(x,y)}" class="latex" title="{D(x,y)}" /> defined on <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" /> (or possibly a subset of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" /> for which the regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is a barrier function). Then, the Bregman projection of <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is defined as <img src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+D%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\arg\min_{x\in K} \ D(x,y)}" class="latex" title="{\arg\min_{x\in K} \ D(x,y)}" />.</p>
<p>
Unfortunately, it is not so easy to reason about Bregman projections either, but the notion of Bregman divergence offers a way to reinterpret the FTRL algorithm from another point of view, called <em>mirror descent</em>. Via this reinterpretation, we will prove the regret bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cy_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " /></p>
<p> which carries the intuition that the regret comes from a combination of the “distance” of our initial solution from the offline optimum and of the “stability” of the algorithm, that is, the “distance” between consecutive soltuions. Nicely, the above bound measures both quantities using the same “distance” function.</p>
<p>
<span id="more-4249"></span> </p>
<p>
</p><p><b>1. Bregman Divergence and Bregman Projection </b></p>
<p></p><p>
For a strictly convex function <img src="https://s0.wp.com/latex.php?latex=%7BR%3A+%7B%5Cmathbb+R%7D%5En+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R: {\mathbb R}^n \rightarrow {\mathbb R}}" class="latex" title="{R: {\mathbb R}^n \rightarrow {\mathbb R}}" />, we define the <em>Bregman divergence</em> associated to <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> as </p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D_R%28x%2Cy%29+%3A%3D+R%28x%29+-+R%28y%29+-+%5Clangle+%5Cnabla+R%28y%29%2C+x-y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D_R(x,y) := R(x) - R(y) - \langle \nabla R(y), x-y \rangle " class="latex" title="\displaystyle  D_R(x,y) := R(x) - R(y) - \langle \nabla R(y), x-y \rangle " /></p>
<p> that is, the difference between the value of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and the value of the linear approximation of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> (centered at <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />). By the strict convexity of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D_R(x,y) \geq 0}" class="latex" title="{D_R(x,y) \geq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D_R(x,y) = 0}" class="latex" title="{D_R(x,y) = 0}" /> iff <img src="https://s0.wp.com/latex.php?latex=%7Bx%3Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x=y}" class="latex" title="{x=y}" />. These properties suggest that we may think of <img src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D_R(x,y)}" class="latex" title="{D_R(x,y)}" /> as a kind of “distance” between <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, which is a useful intuition although it is important to keep in mind that the divergence need not be symmetric and need not satisfy the triangle inequality.</p>
<p>
Now we show that, assuming that <img src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(\cdot)}" class="latex" title="{R(\cdot)}" /> is well defined and strictly convex on all <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" />, and that the losses are linear, the constrained optimization problem <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> can be solved by first solving the unconstrained problem and then “projecting” the solution on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> by finding the point in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of smallest Bregman divergence from the unconstrained optimum:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin+_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28y%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_t = \arg\min _{y\in {\mathbb R}^n} \ R(y) + \sum_{k=1}^{t-1} \langle \ell_k , y \rangle " class="latex" title="\displaystyle  y_t = \arg\min _{y\in {\mathbb R}^n} \ R(y) + \sum_{k=1}^{t-1} \langle \ell_k , y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+%5C+D_R%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) " class="latex" title="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) " /></p>
<p> The proof is very simple. The optimum of the unconstrained optimization problem is the unique <img src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t}" class="latex" title="{y_t}" /> such that</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%5Cleft%28+R%28y_t%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla \left( R(y_t) + \sum_{k=1}^{t-1} \ell_k \right) = 0 " class="latex" title="\displaystyle  \nabla \left( R(y_t) + \sum_{k=1}^{t-1} \ell_k \right) = 0 " /></p>
<p> that is, the unique <img src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t}" class="latex" title="{y_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(y_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" title="\displaystyle  \nabla R(y_t) = - \sum_{k=1}^{t-1} \ell_k " /></p>
<p> On the other hand, <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+%5C+D_R%28x%2Cy_t%29+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+-+R%28y_t%29+-+%5Clangle+%5Cnabla+R%28y_t%29+%2C+x+-+y_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) = \arg\min_{x \in K} R(x) - R(y_t) - \langle \nabla R(y_t) , x - y_t \rangle " class="latex" title="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) = \arg\min_{x \in K} R(x) - R(y_t) - \langle \nabla R(y_t) , x - y_t \rangle " /></p>
<p> that is, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+-+R%28y_t%29+%2B+%5Clangle+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%2C+x+-+y_t+%5Crangle+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+%2B+%5Clangle+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x \in K} R(x) - R(y_t) + \langle \sum_{k=1}^{t-1} \ell_k , x - y_t \rangle = \arg\min_{x \in K} R(x) + \langle \sum_{k=1}^{t-1} \ell_k , x \rangle " class="latex" title="\displaystyle  x_t = \arg\min_{x \in K} R(x) - R(y_t) + \langle \sum_{k=1}^{t-1} \ell_k , x - y_t \rangle = \arg\min_{x \in K} R(x) + \langle \sum_{k=1}^{t-1} \ell_k , x \rangle " /></p>
<p> where the second equality above follows from the fact that two functions that differ by a constant have the same optimal solutions.</p>
<p>
Indeed we see that the above “decoupled” characterization of the FTRL algorithm would have worked for any definition of a function of the form</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+R%28x%29+-+%5Clangle+%5Cnabla+R%28y%29%2C+x+%5Crangle+%2B+%5Clangle+%5Cmbox%7B+stuff+that+depends+only+on+%7D+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x,y) = R(x) - \langle \nabla R(y), x \rangle + \langle \mbox{ stuff that depends only on } y \rangle " class="latex" title="\displaystyle  D(x,y) = R(x) - \langle \nabla R(y), x \rangle + \langle \mbox{ stuff that depends only on } y \rangle " /></p>
<p> and that our particular choice of what “stuff dependent only on <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />” to add makes <img src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D(x,x) = 0}" class="latex" title="{D(x,x) = 0}" /> which is reasonable for something that we want to think of as a “distance function.”</p>
<p>
Note that, in all of the above, we can replace <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" /> with a convex set <img src="https://s0.wp.com/latex.php?latex=%7BK+%5Csubseteq+S+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K \subseteq S \subseteq {\mathbb R}^n}" class="latex" title="{K \subseteq S \subseteq {\mathbb R}^n}" /> provided that <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is a barrier function for <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />. In that case</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7By%5Cin+S%7D+R%28y%29+%2B+%5Csum_k+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_t = \arg\min_{y\in S} R(y) + \sum_k \ell_k " class="latex" title="\displaystyle  y_t = \arg\min_{y\in S} R(y) + \sum_k \ell_k " /></p>
<p> is the unique <img src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t}" class="latex" title="{y_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_t%29+%3D+-+%5Csum+_k+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(y_t) = - \sum _k \ell_k " class="latex" title="\displaystyle  \nabla R(y_t) = - \sum _k \ell_k " /></p>
<p> and everything else follows analogously.</p>
<p>
</p><p><b>2. Examples </b></p>
<p>
</p><p><b>  2.1. Bregman Divergence of Length-Squared </b></p>
<p></p><p>
If <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x) = ||x ||^2}" class="latex" title="{R(x) = ||x ||^2}" />, then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+%7C%7Cx%7C%7C%5E2+-+%7C%7Cy%7C%7C%5E2+-+%5Clangle+2+y+%2C+x-+y+%5Crangle+%3D+%7C%7Cx+-+y%7C%7C%5E2+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x,y) = ||x||^2 - ||y||^2 - \langle 2 y , x- y \rangle = ||x - y||^2 \ , " class="latex" title="\displaystyle  D(x,y) = ||x||^2 - ||y||^2 - \langle 2 y , x- y \rangle = ||x - y||^2 \ , " /></p>
<p> so Bregman divergence is distance-squared, and Bregman projection is just (Euclidean) projection.</p>
<p>
</p><p><b>  2.2. Bregman Divergence of Negative Entropy </b></p>
<p></p><p>
If, for <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%28%7B%5Cmathbb+R%7D_%7B%5Cgeq+0%7D%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in ({\mathbb R}_{\geq 0})^n}" class="latex" title="{x\in ({\mathbb R}_{\geq 0})^n}" />, we define</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) = \sum_{i=1}^n x_i \ln x_i " class="latex" title="\displaystyle  R(x) = \sum_{i=1}^n x_i \ln x_i " /></p>
<p> then the associated Bregman divergence is the generalized <em>KL divergence.</em></p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+%7Bx_i%7D+%5C+-+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Clangle+%5Cnabla+R%28y%29%2C+x-y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x,y) = \sum_{i=1}^n x_i \ln {x_i} \ - \sum_i y_i \ln y_i \ - \langle \nabla R(y), x-y \rangle " class="latex" title="\displaystyle  D(x,y) = \sum_{i=1}^n x_i \ln {x_i} \ - \sum_i y_i \ln y_i \ - \langle \nabla R(y), x-y \rangle " /></p>
<p> where <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cnabla+R%28y%29%29_i+%3D+1+%2B+%5Cln+y_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\nabla R(y))_i = 1 + \ln y_i}" class="latex" title="{(\nabla R(y))_i = 1 + \ln y_i}" /> so that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28x%2Cy%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5C+-+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Csum_i+x_i+%5Cln+y_i+%5C+%2B+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Csum_i+x_i+%2B+%5Csum_i+y_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle D(x,y) = \sum_{i=1}^n x_i \ln x_i \ - \sum_i y_i \ln y_i \ - \sum_i x_i \ln y_i \ + \sum_i y_i \ln y_i \ - \sum_i x_i + \sum_i y_i " class="latex" title="\displaystyle D(x,y) = \sum_{i=1}^n x_i \ln x_i \ - \sum_i y_i \ln y_i \ - \sum_i x_i \ln y_i \ + \sum_i y_i \ln y_i \ - \sum_i x_i + \sum_i y_i " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+%5Cfrac%7Bx_i%7D+%7By_i%7D+%5C+-+%5Csum_i+x_i+%2B+%5Csum_i+y_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n x_i \ln \frac{x_i} {y_i} \ - \sum_i x_i + \sum_i y_i " class="latex" title="\displaystyle  = \sum_{i=1}^n x_i \ln \frac{x_i} {y_i} \ - \sum_i x_i + \sum_i y_i " /></p>
<p>
Note that, if <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> are probability distributions, then the final two terms above cancel out, leaving just the KL divergence <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%5Cln+%5Cfrac+%7Bx_i%7D%7By_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_i x_i \ln \frac {x_i}{y_i}}" class="latex" title="{\sum_i x_i \ln \frac {x_i}{y_i}}" />.</p>
<p>
</p><p><b>3. Mirror Descent </b></p>
<p></p><p>
We now introduce a new perspective on FTRL.</p>
<p>
In the unconstrained setting, if <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is a strictly convex function and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is the associated Bregman divergence, the <em>mirror descent</em> algorithm for online optimization has the update rule</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in {\mathbb R}^n} D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " class="latex" title="\displaystyle  x_t = \arg\min_{x\in {\mathbb R}^n} D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " /></p>
<p> The idea is that we want to find a solution that is good for the past loss functions, but that does not “overfit” too much. If, in past steps, <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t-1}}" class="latex" title="{x_{t-1}}" /> had been chosen to be such a solution for the loss functions <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%2C%5Cldots%2C%5Cell_%7Bt-2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_1,\ldots,\ell_{t-2}}" class="latex" title="{\ell_1,\ldots,\ell_{t-2}}" />, then, in choosing <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />, we want to balance staying close to <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t-1}}" class="latex" title="{x_{t-1}}" /> but also doing well with respect to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_{t-1}}" class="latex" title="{\ell_{t-1}}" />, hence the above definition.</p>
<blockquote><p><b>Theorem 1</b> <em> Initialized with <img src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+R%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1 = \arg\min_{x\in {\mathbb R}^n} R(x)}" class="latex" title="{x_1 = \arg\min_{x\in {\mathbb R}^n} R(x)}" />, the unconstrained mirror descent algorithm is identical to FTRL with regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />. </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  We will proceed by induction on <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. At <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1}" class="latex" title="{t=1}" />, the definition of <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> is the same. For larger <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, we know that FTRL will choose the unique <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k}" class="latex" title="{\nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k}" />, so we will assume that this is true for the mirror descent algorithm for <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_{t-1}}" class="latex" title="{x_{t-1}}" /> and prove it for <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" />. </p>
<p>
First, we note that the function <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D+%2C+x+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \rightarrow D(x,x_{t-1}) + \langle \ell_{t-1} , x \rangle}" class="latex" title="{x \rightarrow D(x,x_{t-1}) + \langle \ell_{t-1} , x \rangle}" /> is strictly convex, because it equals </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+-+R%28x_%7Bt-1%7D%29+-+%5Clangle+%5Cnabla+R%28x_%7Bt-1%7D%29%2C+x+-+x_t+%5Crangle+%2B+%5Clangle+%5Cell_%7Bt-1%7D+%2C+x+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  R(x) - R(x_{t-1}) - \langle \nabla R(x_{t-1}), x - x_t \rangle + \langle \ell_{t-1} , x \rangle" class="latex" title="\displaystyle  R(x) - R(x_{t-1}) - \langle \nabla R(x_{t-1}), x - x_t \rangle + \langle \ell_{t-1} , x \rangle" /></p>
<p> and so it is a sum of a strictly convex function <img src="https://s0.wp.com/latex.php?latex=%7BR%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R(x)}" class="latex" title="{R(x)}" />, linear functions in <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, and constants independent of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. This means that <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is the unique point at which the gradient of the above function is zero, that is, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28x_%7Bt-1%7D%29+%2B+%5Cell_%7Bt-1%7D+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_t) - \nabla R(x_{t-1}) + \ell_{t-1} = 0 " class="latex" title="\displaystyle  \nabla R(x_t) - \nabla R(x_{t-1}) + \ell_{t-1} = 0 " /></p>
<p> and so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+%5Cnabla+R%28x_%7Bt-1%7D%29+-+%5Cell_%7Bt-1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_t) = \nabla R(x_{t-1}) - \ell_{t-1} " class="latex" title="\displaystyle  \nabla R(x_t) = \nabla R(x_{t-1}) - \ell_{t-1} " /></p>
<p> and, using the inductive hypothesis, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " /></p>
<p> as desired. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
In the constrained case, there are two variants of mirror descent. Using the terminology from Elad Hazan’s survey, <em>agile</em> mirror descent is the natural generalization of the unconstrained algorithm:</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in K} \ D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " class="latex" title="\displaystyle  x_t = \arg\min_{x\in K} \ D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " /></p>
<p> Following the same steps as the proof in the previous section, it is possible to show that agile mirror descent is equivalent to solving, at each iteration, the “decoupled” optimization problems</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_t = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t-1}) + \langle \ell_{t-1}, y \rangle " class="latex" title="\displaystyle  y_t = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t-1}) + \langle \ell_{t-1}, y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+D%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " class="latex" title="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " /></p>
<p> That is, we can first solve the unconstrained problem and then project on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />. (Again, we can always replace <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" /> by a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> for which <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is a barrier function and such that <img src="https://s0.wp.com/latex.php?latex=%7BK+%5Csubseteq+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K \subseteq S}" class="latex" title="{K \subseteq S}" />.)</p>
<p>
The <em>lazy</em> mirror descent algorithm has the update rule </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cy_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_t = \arg\min_{x\in {\mathbb R}^n} D(y,y_{t-1}) + \langle \ell_{t-1}, y \rangle " class="latex" title="\displaystyle  y_t = \arg\min_{x\in {\mathbb R}^n} D(y,y_{t-1}) + \langle \ell_{t-1}, y \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+D%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " class="latex" title="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " /></p>
<p> The initialization is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28x%29+%5C+%5C+%5C+x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) \ \ \ x_1 = \arg\min_{x\in K} R(x)" class="latex" title="\displaystyle  y_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) \ \ \ x_1 = \arg\min_{x\in K} R(x)" /></p>
<blockquote><p><b>Fact 2</b> <em> Lazy mirror descent is equivalent to FTRL. </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  The solutions <img src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t}" class="latex" title="{y_t}" /> are the unconstrained optimum of FTRL, and <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> is the Bregman projection of <img src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_t}" class="latex" title="{y_t}" /> on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" />. We proved in the previous section that this characterizes constrained FTRL. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
What about agile mirror descent? In certain special cases it is equivalent to lazy mirror descent, and hence to FTRL, but it usually leads to a different set of solutions. </p>
<p>
We will provide an analysis of lazy mirror descent, but first we will give an analysis of the regret of unconstrained FTRL in terms of Bregman divergence, which will be the model on which we will build the proof for the constrained case.</p>
<p>
</p><p><b>4. A Regret Bound for FTRL in Terms of Bregman Divergence </b></p>
<p></p><p>
In this section we prove the following regret bound.</p>
<blockquote><p><b>Theorem 3</b> <em> Unconstrained FTRL with regularizer <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> satisfies the regret bound </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cx_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,x_{t+1}) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,x_{t+1}) " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is the Bregman divergence associated with <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />. </em></p></blockquote>
<p></p><p>
We will take the mirror descent view of unconstrained FTRL, so that</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) " class="latex" title="\displaystyle  x_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+D%28x%2Cx_t%29+%2B+%5Clangle+%5Cell_t+%2C+x%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_{t+1} = \arg\min_{x\in {\mathbb R}^n} \ D(x,x_t) + \langle \ell_t , x\rangle " class="latex" title="\displaystyle  x_{t+1} = \arg\min_{x\in {\mathbb R}^n} \ D(x,x_t) + \langle \ell_t , x\rangle " /></p>
<p> We proved that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_%7Bt%2B1%7D+%29+%3D+%5Cnabla+R%28x_t%29+-+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(x_{t+1} ) = \nabla R(x_t) - \ell_t " class="latex" title="\displaystyle  \nabla R(x_{t+1} ) = \nabla R(x_t) - \ell_t " /></p>
<p> This means that we can rewrite the regret suffered at step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> with respect to <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%3D+%5Clangle+%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28x_%7Bt%2B1%7D%29%2C+x_t+-+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(x_{t+1}), x_t - x \rangle " class="latex" title="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(x_{t+1}), x_t - x \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+D%28x%2Cx_t%29+-+D%28x%2Cx_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = D(x,x_t) - D(x,x_{t+1} ) +D(x_t, x_{t+1}) " class="latex" title="\displaystyle  = D(x,x_t) - D(x,x_{t+1} ) +D(x_t, x_{t+1}) " /></p>
<p> and the theorem follows by adding up the above expression for <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,\ldots,T}" class="latex" title="{t=1,\ldots,T}" /> and recalling that <img src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx_%7BT%2B1%7D%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D(x,x_{T+1}) \geq 0}" class="latex" title="{D(x,x_{T+1}) \geq 0}" />.</p>
<p>
Unfortunately I have no geometric intuition about the above identity, although, as you can check yourself, the algebra works neatly.</p>
<p>
</p><p><b>5. A Regret Bound for Agile Mirror Descent </b></p>
<p></p><p>
In this section we prove the following generalization of the regret bound from the previous section.</p>
<blockquote><p><b>Theorem 4</b> <em> Agile mirror descent satisfies the regret bound </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cy_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " class="latex" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
The first part of the update rule of agile mirror descent is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cx_%7Bt%7D%29+%2B+%5Clangle+%5Cell_%7Bt%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t}) + \langle \ell_{t}, y \rangle " class="latex" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t}) + \langle \ell_{t}, y \rangle " /></p>
<p> and, following steps that we have already carried out before, <img src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_{t+1}}" class="latex" title="{y_{t+1}}" /> satisfies </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_%7Bt%2B1%7D+%29+%3D+%5Cnabla+R%28x_t%29+-+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \nabla R(y_{t+1} ) = \nabla R(x_t) - \ell_t " class="latex" title="\displaystyle  \nabla R(y_{t+1} ) = \nabla R(x_t) - \ell_t " /></p>
<p> This means that we can rewrite the regret suffered at step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> with respect to <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%3D+%5Clangle+%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28y_%7Bt%2B1%7D%29%2C+x_t+-+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(y_{t+1}), x_t - x \rangle " class="latex" title="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(y_{t+1}), x_t - x \rangle " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+D%28x%2Cx_t%29+-+D%28x%2Cy_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+y_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = D(x,x_t) - D(x,y_{t+1} ) +D(x_t, y_{t+1}) " class="latex" title="\displaystyle  = D(x,x_t) - D(x,y_{t+1} ) +D(x_t, y_{t+1}) " /></p>
<p> where the same mystery cancellations as before make the above identity true.</p>
<p>
Now I will wield another piece of magic, and I will state without proof the following fact about Bregman projections</p>
<blockquote><p><b>Lemma 5</b> <em> If <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in K}" class="latex" title="{x\in K}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> is the Bregman projection on <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> of a point <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, then </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%5Cgeq+D%28x%2Cz%29+%2B+D%28z%2Cy%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x,y) \geq D(x,z) + D(z,y) " class="latex" title="\displaystyle  D(x,y) \geq D(x,z) + D(z,y) " /></p>
</em><p><em> </em></p></blockquote>
<p> That is, if we think of <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> as a “distance,” the distance from <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> to its closest point <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> plus the distance from <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> is at most the distance from <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Note that this goes in the opposite direction as the triangle inequality (which ok, because <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> typically does not satisfy the triangle inequality).</p>
<p>
In particular, the above lemma gives us </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy_%7Bt%2B1%7D%29+%5Cgeq+D%28x%2Cx_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D(x,y_{t+1}) \geq D(x,x_{t+1}) " class="latex" title="\displaystyle  D(x,y_{t+1}) \geq D(x,x_{t+1}) " /></p>
<p> and so </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%5Cleq+D%28x%2Cx_t%29+-+D%28x%2Cx_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+y_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \ell_t , x_t - x \rangle \leq D(x,x_t) - D(x,x_{t+1} ) +D(x_t, y_{t+1}) " class="latex" title="\displaystyle  \langle \ell_t , x_t - x \rangle \leq D(x,x_t) - D(x,x_{t+1} ) +D(x_t, y_{t+1}) " /></p>
<p> Now summing over <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and recalling that <img src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx_%7BT%2B1%7D%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D(x,x_{T+1}) \geq 0}" class="latex" title="{D(x,x_{T+1}) \geq 0}" /> we have our theorem.</p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/"><span class="datestr">at May 21, 2019 01:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/20/lecturer-tenured-assistant-professor-at-royal-holloway-university-of-london-apply-by-june-7-2019-at-royal-holloway-university-of-london-apply-by-june-7-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/20/lecturer-tenured-assistant-professor-at-royal-holloway-university-of-london-apply-by-june-7-2019-at-royal-holloway-university-of-london-apply-by-june-7-2019/">Lecturer (Tenured Assistant Professor) at Royal Holloway, University of London (apply by June 7, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Royal Holloway, University of London, invites applications for a Lecturer position (a full-time and permanent (tenured) post). We are recruiting for an academic member of staff who can strengthen our research, which falls broadly within Algorithms and Complexity, Artificial Intelligence, Distributed and Global Computing, and Software Language Engineering.</p>
<p>Website: <a href="https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0519-179">https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0519-179</a><br />
Email: Jose.Fiadeiro@rhul.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/20/lecturer-tenured-assistant-professor-at-royal-holloway-university-of-london-apply-by-june-7-2019-at-royal-holloway-university-of-london-apply-by-june-7-2019/"><span class="datestr">at May 20, 2019 06:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6282781001271163342">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Notorious L.A.H or Notorious LAH? OR You always need one more proofread</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I noticed a while back that even on the nth proofread of a document there are still corrections. So I decided to keep track of how many corrections there are in a paper I was working on. I chose a non-technical paper so that errors-in-the-math would not be the issue.  I chose<br />
<br />
                                           Guest Column: The Third P =?NP Poll (see <a href="https://blog.computationalcomplexity.org/2019/03/third-poll-on-p-vs-np-and-related.html">here</a>)<br />
<br />
that appeared in Lane Hemaspaandra's SIGACT News Complexity Column.<br />
<br />
I kept track of the following:<br />
<br />
1) Number of corrections. Anything that I changed. Could be style, a new thought, need not be (though could be) an error.<br />
<br />
2) Errors. These are things that really need to be corrected, like having `think' instead of  `thing' .<br />
<br />
Corrections vs  Errors, an Example:<br />
<br />
If I refer to Lane Hemaspaandra as <i>Notorious L.A.N</i> that is a correction and an  error, as he is Notorous <i>L.A.H.</i><br />
<br />
If I refer to Lane Hemaspaandra as<i> Notorious L.A.H</i> and decide to change it to <i>LAH </i>that is a correction that is not an error.<br />
<br />
I didn't keep track of serious errors vs typos, but after the first 3 proofreads there were no more serious errors--- sort of- --you'll see. Most serious was a f<b>onts-gone-wild thing where half the paper was in boldface.</b><br />
<br />
Here is a history of the number of corrections<br />
<br />
1) Lane proofread the first draft. κ corrections where κ is some cardinal between the cardinality of N and the cardinality of  2<sup>N</sup> . Its value depends on which model of set theory you are in. (My spellchecker thinks that cardinality is not a word. I checked and I am spelling it correctly but perhaps it's one of those things where I stare at it too much and keep misreading it.)<br />
<br />
Henceforth I omit the word <i>proofread</i> as it is understood<br />
<br />
<br />
2) Bill G:  81 corrections, 29 of which were errors.<br />
<br />
3) Clyde: 64 corrections,  of which 17 were errors.<br />
<br />
4) Bill G: 40 corrections, of which 21 were errors (I had added a new section causing more errors)<br />
<br />
5) Clyde: 30 corrections of which 10 were errors.<br />
<br />
6) Bill G: 24 corrections of which 6 were errors.<br />
<br />
7) Clyde: 18 corrections of which 8 were errors.<br />
<br />
8) David Sekora (A CS grad student at Maryland who at one time wanted to be an English Major): f15 corrections of which 15 were errors. Really! Typos dagnabbit! (Spell check thinks that dagnabbit is spelled wrong. Um---in that case what is the correct spelling?)<br />
<br />
9) Nathan Grammel (A CS grad student at Maryland) :6 corrections of which  3 were errors.<br />
<br />
10) Bill G, proofreading backwards, a paragraph at a time: 29 corrections of which 5 were errors.<br />
<br />
11) Justin Hontz, an ugrad who TAs for me: 10 corrections of which 7 were errors.<br />
<br />
12) Karthik Abinav, a grad student in theory at Maryland: 2 corrections both of which were errors. Was this the end or are there still issues?<br />
<br />
13) Josh Twitty, an ugrad who TAs for me: 0 corrections. YEAH!<br />
<br />
14) Dan Smolyak, an ugrad CS and Eco major:4 corrections, all 4 errors. <i>Error  </i>sounds too strong. For example, one of them was to replace ?. with ?  Yes, its an error, but not that important. It DOES point to his carefulness as a proofreader.<br />
<br />
15) Clyde Kruskal :20 corrections, 10 of which were errors. To call them errors seems wrong when he corrects  <i>Group theory' </i>to  <i>Group Theory</i>. None of these corrections were caused by prior comments. I think all of the errors were in the paper early on, undetected until now!<br />
<br />
16)  Backwards Bill G again:  28 corrections,  14 of which were errors. Again, the errors were minor. Most of the errors were relatively recent. As an example, if I list out topics in math like:<br />
<br />
a) Group Theory, Set Theory, and  Ramsey Theory<br />
<br />
then I am supposed to use capital letters, but if I say in prose<br />
<br />
Lance Fortnow thinks that the techniques used will be group theory, set theory, and Ramsey theory<br />
<br />
then only the R in Ramsey Theory is in caps.  Makes me glad I'm in math.<br />
<br />
17) Lane got penultimate proofread. Lane found 75 (yes 75 WOW) of which 66 (yes 66 WOW) were errors. Many of these were spacing and latex things that I would never have noticed (indeed- I didn't notice) and most readers would not have noticed (hmmm- how do I know that?) but only an editor could catch (hmmm- when I've edited the book review column and now the open problems column and I never found more than 10 errors). So when all is said and done: KUDOS to Lane! And My point was that you can never get all the errors out. On that I am correct. I wonder if there are still errors? Yeah, but at most 10. However, I said that BEFORE giving it to Lane.<br />
<br />
18) Stephen Fenner, the editor of SIGACT news got FINAL proofread. He found that I spelled his name wrong . How many errors are left? I would bet at most 10. I would bet that I would lose that bet.<br />
------------------------------------------------------------------------------------------------------------<br />
<br />
Why after multiple proofreadings are there still errors? (My spell check thinks proofreadings is not a word. Maybe my spell check is worried that if people get documents proofread a lot then they won't be needed anymore. This blog post refutes that thought.)<br />
<br />
1)  An error can occur from a correction. This caused a massive problem with another paper. Lane's next column will be by me and co-authors on The Muffin Problem. We had all kinds of problems with the colors and sizes--- Massive Magenta Muffins or Miniature Magenta Muffins? Sizes gone wild! Again Kudos to my proofreaders and to Lane for catching this rather important error.<br />
<br />
2) If some passage is added late in the process it will surely have errors.<br />
<br />
3) An error correction may clear away the brush so you can see other errors.<br />
<br />
4) With LaTeX (or Word for some) we have the ability to get things perfect. So there is no cost to keeping on perfecting things. This lead so many corrections that are not errors.<br />
<br />
5) I know of an adviser who would first say change A to B, and later change B back to A. (None of that happened with the paper discussed above).<br />
<br />
Are errors inevitable? Clyde Kruskal tells me that his father Martin Kruskal, as a teenager, read Courant and Robbins book <i>What is Mathematics</i> and found some errors in it. Martin's mother didn't believe him and marched him over to Courant's house:<br />
<br />
MARTIN MOTHER: Martin claims to have found errors in your book.<br />
<br />
COURANT:  (laughs) There are errors in every book.<br />
<br />
Courant was so impressed that ten (or so) years later Courant became Martin's PhD adviser.<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html"><span class="datestr">at May 20, 2019 02:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
