<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 24, 2020 10:21 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4772">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4772">Martinis, The Plot Against America, Kill Chain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>As if we didn’t have enough to worry us, this week brought the <a href="https://www.wired.com/story/googles-head-quantum-computing-hardware-resigns/">sad news</a> that John Martinis, who for five years was the leader and public face of Google’s experimental quantum computing effort, has quit Google and returned to his earlier post at UC Santa Barbara.  I’ve spoken about what happened both with John and with Hartmut Neven, the head of Google’s Quantum AI Lab.  Without betraying confidences, or asserting anything that either side would disagree with, I think I can say that it came down to a difference in management philosophies.  Google tends to be consensus-driven, whereas John is of the view that building a million-qubit, error-corrected quantum computer will take more decisive leadership.  I can add: I’d often wondered how John had time to travel the world, giving talks about quantum supremacy, while also managing the lab’s decisions on a day-to-day basis.  It looks now like I was right to wonder!  Potential analogies flood the mind: is this like a rock band that breaks up right after its breakout hit?  Is it like Steve Jobs leaving Apple?  Anyway, I wish the Google team the best in John’s absence, and I also wish John the best with whatever he does next.</p>



<p>I was never big on HBO (e.g., I still haven’t seen a single minute of <em>Game of Thrones</em>), but in the last couple of weeks, Dana and I found ourselves watching two absolutely compelling HBO shows—one a fictional miniseries and the other a documentary, but both on the theme of the fragility of American democracy.</p>



<p><a href="https://www.hbo.com/the-plot-against-america"><em>The Plot Against America</em></a>, based on the <a href="https://en.wikipedia.org/wiki/The_Plot_Against_America">2004 Philip Roth novel</a> of the same name (which Dana read and which I now plan to read), is about an alternate history where the aviator Charles Lindbergh defeats FDR in the 1940 presidential election, on a fascist and isolationist platform, in events that—as countless people have pointed out—are eerily, terrifyingly prescient of what would actualy befall the US in 2016.  The series follows a Jewish insurance salesman and his family in Newark, NJ—isn’t that what it always is with Philip Roth?—as they try to cope with the country’s gradual, all-too-plausible slide downward, from the genteel antisemitism that already existed in <em>our</em> timeline’s 1940 all the way to riots, assassinations, and pogroms (although never to an American Holocaust).  One of the series’ final images is of paper ballots, in a rematch presidential election, being carted away and burned, underscoring just how much depends here on the mundane machinery of democracy.</p>



<p>Which brings me to <a href="https://www.hbo.com/documentaries/kill-chain-the-cyber-war-on-americas-elections"><em>Kill Chain: The Cyber War on America’s Elections</em></a>, a documentary about the jaw-droppingly hackable electronic voting machines used in US elections and the fight to do something about them.  The show mostly follows the journey of <a href="https://en.wikipedia.org/wiki/Harri_Hursti">Harri Hursti</a>, a Finnish-born programmer who’s made this issue his life’s work, but it also extensively features my childhood best friend <a href="https://en.wikipedia.org/wiki/Alex_Halderman">Alex Halderman</a>.  OK, but isn’t this a theoretical issue, one that (perhaps rightly) exercises security nerds like Alex, but surely hasn’t changed the outcomes of actual elections?</p>



<p>Yeah, so about that.  You know Brian Kemp, the doofus governor of Georgia, who’s infamously announced plans to reopen the state right away, ignoring the pleading of public health experts—a  act that will fill Georgia’s ICUs and morgues as surely as night follows day?  And you know how Kemp defeated the Democrat, Stacey Abrams, by a razor-thin margin, in a 2018 election of which Kemp himself was the overseer?  It turns out that Kemp’s office distributed defective memory cards to African-American and Democratic precincts, though not to white and Republican ones.  There’s also striking statistical evidence that at least some voting machines were hacked, although because there was no paper trail it can never be proved.</p>



<p>In short, what <em>The Plot Against America</em> and <em>Kill Chain</em> have in common is that they <em>would be</em> desperately needed warnings about the ease with which democracy could collapse in the US, except for the detail that much of what they warn about has already happened, and now it’s not clear how we get back.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4772"><span class="datestr">at April 23, 2020 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=426">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/04/23/tcs-talk-wednesday-april-29-sepideh-mahabadi-ttic/">TCS+ talk: Wednesday, April 29 — Sepideh Mahabadi, TTIC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 29th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Sepideh Mahabadi</strong> from TTIC will speak about “<em>Non-Adaptive Adaptive Sampling in Turnstile Streams</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a>the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a>on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a>suggest</a> a possible topic or speaker, please see <a>the website</a>.</p>
<blockquote><p>Abstract: Adaptive sampling is a useful algorithmic tool for data summarization problems in the classical centralized setting, where the entire dataset is available to the single processor performing the computation. Adaptive sampling repeatedly selects rows of an underlying <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n" class="latex" title="n" /> by <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" alt="d" class="latex" title="d" /> matrix <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" alt="A" class="latex" title="A" />, where <img src="https://s0.wp.com/latex.php?latex=n+%5Cgg+d&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n \gg d" class="latex" title="n \gg d" />, with probabilities proportional to their distances to the subspace of the previously selected rows. Intuitively, adaptive sampling seems to be limited to trivial multi-pass algorithms in the streaming model of computation due to its inherently sequential nature of assigning sampling probabilities to each row only after the previous iteration is completed. Surprisingly, we show this is not the case by giving the first one-pass algorithms for adaptive sampling on turnstile streams and using space <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28d%2Ck%2C%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\text{poly}(d,k,\log n)" class="latex" title="\text{poly}(d,k,\log n)" />, where <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0" alt="k" class="latex" title="k" /> is the number of adaptive sampling rounds to be performed.</p>
<p>Our adaptive sampling procedure has a number of applications to various data summarization problems on turnstile streams that either improve state-of-the-art or have only been previously studied in the more relaxed row-arrival model. This includes column subset selection, subspace approximation, projective clustering, and volume maximization. We complement our volume maximization algorithmic results with lower bounds that are tight up to lower order terms, even for multi-pass algorithms. By a similar construction, we also obtain lower bounds for volume maximization in the row-arrival model, which we match with competitive upper bounds.</p>
<p>This is a joint work with Ilya Razenshteyn, David Woodruff, and Samson Zhou.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/04/23/tcs-talk-wednesday-april-29-sepideh-mahabadi-ttic/"><span class="datestr">at April 23, 2020 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/055">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/055">TR20-055 |  Bounded Collusion Protocols, Cylinder-Intersection Extractors and Leakage-Resilient Secret Sharing | 

	Ashutosh Kumar, 

	Raghu Meka, 

	David Zuckerman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work we study bounded collusion protocols (BCPs) recently introduced in the context of secret sharing by Kumar, Meka, and Sahai (FOCS 2019). These are multi-party communication protocols on $n$ parties where in each round a subset of $p$-parties (the collusion bound) collude together and write a function of their inputs on a public blackboard. 

BCPs interpolate elegantly between the well-studied number-in-hand (NIH) model ($p=1$) and the number-on-forehead (NOF) model ($p=n-1$). Motivated by questions in communication complexity, secret sharing, and pseudorandomness we investigate BCPs more thoroughly answering several questions about them. 

* We prove a polynomial (in the input-length) lower bound for an explicit function against BCPs where any constant fraction of players can collude. Previously, non-trivial lower bounds were only known when the collusion bound was at most logarithmic in the input-length (owing to bottlenecks in NOF lower bounds). 

* For all $t \leq n$, we construct efficient $t$-out-of-$n$ secret sharing schemes where the secret remains hidden even given the transcript of a BCP with collusion bound $O(t/\log t)$. Prior work could only handle collusions of size $O(\log n)$. Along the way, we construct leakage-resilient schemes against disjoint and adaptive leakage,  resolving a question asked by Goyal and Kumar (STOC 2018).

* An explicit $n$-source cylinder intersection extractor whose output is close to uniform even when given the transcript of a BCP with a constant fraction of parties colluding. The min-entropy rate we require is $0.3$ (independent of collusion bound $p \ll n$). 

Our results rely on a new class of exponential sums that interpolate between the ones considered in additive combinatorics by Bourgain (Geometric and Functional Analysis 2009) and Petridis and Shparlinski (Journal d'Analyse Mathématique 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/055"><span class="datestr">at April 23, 2020 09:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/054">TR20-054 |  Communication Complexity with Defective Randomness  | 

	Marshall Ball, 

	Oded Goldreich, 

	Tal Malkin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Starting with the two standard model of randomized communication complexity, we study the communication complexity of functions when the protocol has access to a defective source of randomness. 
Specifically, we consider both the public-randomness and private-randomness cases, while replacing the commonly postulated perfect randomness with distributions over $\ell$ bit strings that have min-entropy at least $k\leq\ell$. 
We present general upper and lower bounds on the communication complexity in these cases, where the bounds are typically linear in $\ell-k$ and also depend on the size of the fooling set for the function being computed and on its standard randomized complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/054"><span class="datestr">at April 22, 2020 07:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1650">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/04/22/private-libraries/">Reading in Private</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><i>(This blog post is based on joint work with </i><a href="https://cs.stanford.edu/~dkogan/"><i>Dima Kogan</i></a><i>.)</i></p>
<p>One of the great unsung perks of being a college student is having access to the university library. There is something thrilling about hunting down exactly the right reference deep in the stacks, or reading through the archived papers of a public figure from years back.</p>
<p>The pandemic has closed all of our libraries for the time being. Even so, through the fruits of computer science—databases, the Internet, e-readers, and so on—we can get access to much of the same information even when we are cooped up at home.</p>
<p>But for me, one of the true pleasures of using a library is the fact that I can browse through any book I want in complete privacy. If I want to go up to the stacks and read about tulip gardening, or road-bike maintenance, or strategies for managing anxiety, I can do that pretty much without anyone else knowing.</p>
<p>In contrast, if I go online today and search for “tulip gardening,” Google will take careful note of my interest in tulips and I will be seeing ads about gardening tools for months.</p>
<p>An ideal digital library would let us download and read books without anyone—not even the library itself—learning which books we are reading. How could we build such a privacy-respecting digital library?</p>
<p>In this post, we will discuss the private-library problem and how <a href="https://eprint.iacr.org/2019/1075">our recent work on private information retrieval</a> might be able to help solve it.</p>
<h3><b>The Private-Library Problem</b></h3>
<p>Let us define the problem a little more precisely. We will imagine a protocol running between a library, which holds the books, and a student, who wants to download a particular book.</p>
<p>Say that the library has <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> books—let’s call the books <img src="https://s0.wp.com/latex.php?latex=x%3D%28x_1%2C+%5Cdots%2C+x_N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x=(x_1, \dots, x_N)" class="latex" title="x=(x_1, \dots, x_N)" />. To keep things simple, let’s pretend that each book consists of just a single bit of information, so <img src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_i \in \{0,1\}" class="latex" title="x_i \in \{0,1\}" /> for all <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5C%7B1%2C+%5Cdots%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i \in \{1, \dots, N\}" class="latex" title="i \in \{1, \dots, N\}" />.</p>
<p>The student starts out holding the index <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5C%7B1%2C+%5Cdots%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i \in \{1, \dots, N\}" class="latex" title="i \in \{1, \dots, N\}" /> of her desired book. To fetch the digital book from the library, the student and library exchange some messages. At the end of the interaction, we want the following two properties to hold:</p>
<ul>
<li><b>Correctness.</b> The student should have her desired book (i.e., the bit <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_i" class="latex" title="x_i" />).</li>
<li><b>Privacy. </b>The library should have not learned any information, in a cryptographic sense, about which book the student downloaded.</li>
</ul>
<p>Of course, we have grossly simplified the problem: a real book is more than a single bit in length, book titles are not consecutive integers, maybe the student would like to find a book using a keyword search, etc. But even this simplified private-information-retrieval problem, which <a href="http://www.tau.ac.il/~bchor/PIR.pdf">Chor, Goldreich, Kushilevitz, and Sudan introduced</a> in the 90s, is already interesting enough.</p>
<h3><b>A simple but inefficient solution</b></h3>
<p>There is a simple solution to this problem: the student can just ask the library to send her the contents of all <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> books. This solution achieves both correctness and privacy, so what’s the problem? Are we done?</p>
<p>Well, there are two problems:</p>
<ol>
<li>The amount of <b>communication</b> is large: Just to read a single book, the client must download the contents of the entire library! So this is terribly inefficient.</li>
<li>The amount of <b>computation</b> is large: Just to fetch a single book, the library must do work proportional to the size of the entire library. So “checking out” a book from this digital library will take a long time.</li>
</ol>
<p>Research on private information retrieval typically focuses on the first problem: how can we reduce the <i>communication</i> cost? Using a <a href="https://dl.acm.org/doi/abs/10.1145/2968443">variety</a> of <a href="https://ieeexplore.ieee.org/abstract/document/646125">clever</a> <a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978429">techniques</a>, it is possible to drive down the communication cost to something very small—sub-polynomial or even logarithmic in the library size <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />.</p>
<p>But today we are interested in the <i>computational</i> burden on the library. Is there any way that the student can privately download a book from the library while requiring the library to do only <img src="https://s0.wp.com/latex.php?latex=o%28N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="o(N)" class="latex" title="o(N)" /> work in the process?</p>
<h3><b>Doing the hard work in advance</b></h3>
<p>To have both correctness and privacy, it seems that the library needs to touch each of the <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" /> books in the process of responding to each student’s request. And, in some sense, <a href="http://groups.csail.mit.edu/cis/pubs/malkin/BIM.ps">this is true</a>. So, to allow the library to run in time sublinear in <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />, we will have to tweak the problem slightly.</p>
<p>Our idea is to have the library do the <img src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="O(N)" class="latex" title="O(N)" />-time computation in an <b>offline phase</b>, which takes place <i>before</i> the student decides which book she wants to read. For example, this offline phase might happen overnight while the library’s servers would otherwise be idle.</p>
<p>Later on, once the student decides which book in the library she wants to read, the student and library can run a <img src="https://s0.wp.com/latex.php?latex=o%28N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="o(N)" class="latex" title="o(N)" />-time <b>online phase</b> in which the student is able to retrieve her desired book. The total communication cost, in both offline and online phases, will be <img src="https://s0.wp.com/latex.php?latex=o%28N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="o(N)" class="latex" title="o(N)" />.</p>
<p>So, by pushing the library’s expensive linear scan to an offline phase, the library can service the student’s request for a book in sublinear online time.</p>
<h3><b>Our offline/online private information retrieval scheme</b></h3>
<p>Let’s see how to construct such an offline/online scheme. To make things simple for the purposes of this post, let’s assume that the student has access to two non-colluding libraries that hold the same set of books. To be concrete, let’s call the two libraries “Stanford” and “Berkeley.”</p>
<p>The privacy property will hold as long as the librarians at Stanford and Berkeley don’t get together and share the information that they learned while running the protocol with the student. So Stanford and Berkeley here are “non-colluding.” (Equivalently, our scheme that protects privacy against an adversary that controls one of the two libraries—but not both.)</p>
<div style="width: 571px;" class="wp-caption aligncenter" id="attachment_1670"><img width="561" alt="Offline-online PIR" src="https://theorydish.files.wordpress.com/2020/04/offlineonline.png?w=561&amp;h=365" class=" wp-image-1670" height="365" /><p class="wp-caption-text" id="caption-attachment-1670">In the offline phase, which happens before the student knows which book she wants to read, the Stanford library does linear work. In the online phase, which runs once the student has the index of her desired book, the Berkeley library runs in sublinear time. (We are suppressing log factors here.)</p></div>
<p>Now, let’s describe an offline/online protocol by which the student can privately fetch a book from the digital library:</p>
<p><b>Offline Phase.</b></p>
<ul>
<li>The student partitions the integers <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+..%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{1, .., N\}" class="latex" title="\{1, .., N\}" /> into <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\sqrt{N}" class="latex" title="\sqrt{N}" /> non-overlapping sets chosen at random, where each set has size <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\sqrt{N}" class="latex" title="\sqrt{N}" />. Call these sets <img src="https://s0.wp.com/latex.php?latex=S_1%2C+%5Cdots%2C+S_%7B%5Csqrt%7BN%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1, \dots, S_{\sqrt{N}}" class="latex" title="S_1, \dots, S_{\sqrt{N}}" />.</li>
<li>The student sends these sets <img src="https://s0.wp.com/latex.php?latex=S_1%2C+%5Cdots%2C+S_%7B%5Csqrt%7BN%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_1, \dots, S_{\sqrt{N}}" class="latex" title="S_1, \dots, S_{\sqrt{N}}" /> to Stanford (the first library). To reduce the communication cost here, the student can compress these sets using pseudorandomness.</li>
<li>For each set <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_j" class="latex" title="S_j" />, the Stanford library computes the <i>parity</i> of all of the books indexed by set <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_j" class="latex" title="S_j" /> and returns the parity bits <img src="https://s0.wp.com/latex.php?latex=%28b_1%2C+%5Cdots%2C+b_%7B%5Csqrt%7BN%7D%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(b_1, \dots, b_{\sqrt{N}})" class="latex" title="(b_1, \dots, b_{\sqrt{N}})" /> to the student. In other words, if the books are <img src="https://s0.wp.com/latex.php?latex=x%3D%28x_1%2C+%5Cdots%2C+x_N%29+%5Cin+%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x=(x_1, \dots, x_N) \in \{0,1\}^n" class="latex" title="x=(x_1, \dots, x_N) \in \{0,1\}^n" />, then <img src="https://s0.wp.com/latex.php?latex=b_j+%3D+%5Csum_%7Bk+%5Cin+S_j%7D+x_k+%5Cbmod+2&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="b_j = \sum_{k \in S_j} x_k \bmod 2" class="latex" title="b_j = \sum_{k \in S_j} x_k \bmod 2" />.</li>
</ul>
<p>The total communication in this phase is only <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="O(\sqrt{N})" class="latex" title="O(\sqrt{N})" /> bits and the student and the Stanford library can run this step <i>before</i> the student decides which book she wants to read.</p>
<p><b>Online Phase.</b> Once the student decides that she wants to read book <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5C%7B1%2C+%5Cdots%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i \in \{1, \dots, N\}" class="latex" title="i \in \{1, \dots, N\}" />, the student and Berkeley (the second library) run the following steps:</p>
<ul>
<li>The student finds the set <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_j" class="latex" title="S_j" /> that contains the index <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> of her desired book.</li>
<li>The student flips a coin that is weighted to come up heads with some probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" />, to be fixed later.</li>
<li>If the coin lands <span style="text-decoration: underline;">heads</span>:
<ul>
<li>The student sends <img src="https://s0.wp.com/latex.php?latex=S+%5Cgets+S_j+%5Csetminus+%5C%7Bi%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S \gets S_j \setminus \{i\}" class="latex" title="S \gets S_j \setminus \{i\}" /> to the Berkeley library.</li>
</ul>
</li>
<li>If the coin lands <span style="text-decoration: underline;">tails</span>:
<ul>
<li>The student samples <img src="https://s0.wp.com/latex.php?latex=i%27+%5Cgets_R+S_j+%5Csetminus+%5C%7Bi%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i' \gets_R S_j \setminus \{i\}" class="latex" title="i' \gets_R S_j \setminus \{i\}" />.</li>
<li>The student sends <img src="https://s0.wp.com/latex.php?latex=S+%5Cgets+S_j+%5Csetminus+%5C%7Bi%27%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S \gets S_j \setminus \{i'\}" class="latex" title="S \gets S_j \setminus \{i'\}" /> to the Berkeley library.</li>
</ul>
</li>
<li>The Berkeley library receives the set <img src="https://s0.wp.com/latex.php?latex=S+%5Csubseteq+%5C%7B1%2C+%5Cdots%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S \subseteq \{1, \dots, N\}" class="latex" title="S \subseteq \{1, \dots, N\}" /> from the student. The Berkeley library returns the contents of all books whose indices appear in set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S" class="latex" title="S" /> to the student.</li>
<li>Now, the student can recover its desired book as follows:
<ul>
<li>If <span style="text-decoration: underline;">heads</span>: the student now has the parity of the books in <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_j" class="latex" title="S_j" /> (from the offline phase) and the value of all books in <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S_j" class="latex" title="S_j" /> that are not book <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />. This is enough to recover the contents of book <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />.</li>
<li>If <span style="text-decoration: underline;">tails</span>: <img src="https://s0.wp.com/latex.php?latex=i+%5Cin+S&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i \in S" class="latex" title="i \in S" />. In this case the Berkeley library has sent the contents of book <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" /> to the student in the online phase.</li>
</ul>
</li>
</ul>
<p>Even before we fix the weight <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" /> of the coin, we see that the protocol satisfies <b>correctness</b>, since no matter how the coin lands the client recovers its desired book. Also, the total communication cost is <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%7D+%5Clog+N%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="O(\sqrt{N} \log N)" class="latex" title="O(\sqrt{N} \log N)" /> bits, which is sublinear as we had hoped. Finally, the <b>online computation cost</b> is also sublinear: the Berkeley library just needs to return <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\sqrt{N}" class="latex" title="\sqrt{N}" /> books to the client, which it can do in time roughly <img src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7BN%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="O(\sqrt{N})" class="latex" title="O(\sqrt{N})" />.</p>
<p>The last matter to address is <b>privacy</b>. Again, we are assuming that the adversary controls only one of the two libraries.</p>
<ul>
<li>In the offline phase, the student’s message to the Stanford library is independent of <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i" class="latex" title="i" />, so the protocol is perfectly private with respect to Stanford.</li>
<li>In the online phase, we must be more careful. It turns out that if we choose the weight <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p" class="latex" title="p" /> of the coin as <img src="https://s0.wp.com/latex.php?latex=p+%3D+1+-+%28%5Csqrt%7BN%7D+-+1%29%2FN&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="p = 1 - (\sqrt{N} - 1)/N" class="latex" title="p = 1 - (\sqrt{N} - 1)/N" />, then the set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="S" class="latex" title="S" /> that the student sends to UC Berkeley in the online phase is just a uniformly random size-<img src="https://s0.wp.com/latex.php?latex=%28%5Csqrt%7BN%7D-1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(\sqrt{N}-1)" class="latex" title="(\sqrt{N}-1)" /> subset of <img src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C+%5Cdots%2C+N%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\{1, \dots, N\}" class="latex" title="\{1, \dots, N\}" />.</li>
</ul>
<h3><b>Open problems</b></h3>
<p>So, the student can privately fetch a book from our digital libraries in sublinear online time. What else is left to do?</p>
<ul>
<li>Getting rid of the need for two non-colluding libraries is a clear next step. <a href="https://eprint.iacr.org/2019/1075">Our work</a> has some results along these lines, but they pay a price either in (a) asymptotic efficiency or in (b) the strength of the cryptographic assumptions required.</li>
<li>A beautiful paper of <a href="https://www.cs.bgu.ac.il/~beimel/Papers/BIM.pdf">Beimel, Ishai, and Malkin</a> shows that if the library can store its collection of books using a special type of error-correcting encoding, the <b>total</b> computational time at the libraries (not just the online time) can be sublinear in <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="N" class="latex" title="N" />. As far as we know, these schemes are not concretely efficient enough to use in practice. Could they be made so?</li>
<li>Privacy is just one of the many pleasures of using a physical library. During this period of confinement, I also miss the smell of the books, the beauty of light filtering through the stacks, and the peacefulness of thinking in a study carrel. Can a digital library ever give us these things too?</li>
</ul>
<p>If any of these questions catch your fancy, please check out <a href="https://eprint.iacr.org/2019/1075">our Eurocrypt paper</a> for more background, pointers, and results.</p>
<p>Don Knuth <a href="http://jmlr.csail.mit.edu/reviewing-papers/knuth_mathematical_writing.pdf">has reportedly said</a> “Using a great library to solve a specific problem… Now <i>that</i> […] is real living.” With better digital libraries, maybe we could all live a little bit more during these challenging days.</p></div>







<p class="date">
by Henry Corrigan-Gibbs <a href="https://theorydish.blog/2020/04/22/private-libraries/"><span class="datestr">at April 22, 2020 07:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3472">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/04/21/ec-2020-will-be-virtual/">SIGecom Announcement: EC 2020 will be virtual</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>As many of you have probably anticipated, due to concerns regarding the novel coronavirus COVID-19, the <a href="http://ec20.sigecom.org/">2020 ACM Conference on Economics and Computation (EC 2020)</a> will be held virtually.</p>
<p>This change of format will of course present us with difficult challenges, but we believe it will offer exciting new opportunities as well.  (And not to worry, your opportunity to attend EC in Budapest is just deferred to 2021.)</p>
<p>The <a href="http://sigecom.org/officers.html">SIGecom Executive Committee</a> has appointed and will serve on a Virtual Transition Team that additionally includes the following new conference officers:</p>
<ul>
<li>Virtual General Chair: <a href="https://sites.northwestern.edu/hartline/">Jason Hartline</a></li>
<li>Virtual Local Chair: <a href="https://yannai.gonch.name/">Yannai Gonczarowski</a></li>
<li>Virtual Global Outreach Chairs: <a href="https://www.cs.cornell.edu/~red/">Rediet Abebe</a> and <a href="https://research.fb.com/people/sodomka-eric/">Eric Sodomka</a></li>
</ul>
<p>This team is working with the <a href="http://ec20.sigecom.org/committees-acm/organizing-committee/">EC 2020 organizing committee</a> and <a href="http://ec20.sigecom.org/committees-acm/program-committee/">EC 2020 PC chairs</a> to put together a plan that leverages the opportunities of the virtual format to the fullest extent. Though these plans are still in the works, we have identified the following “minimal commitment” for authors of accepted papers to the main EC conference: at least one author will need to</p>
<ul>
<li>register for the conference;</li>
<li>be available virtually on the conference dates (July 14-16);</li>
<li>provide a camera-ready paper or abstract by the camera-ready deadline;</li>
<li>provide a pre-recorded talk presenting the paper two weeks in advance (by June 28).</li>
</ul>
<p>We are optimistic that, while a virtual EC may lack some of the positive features of a classical conference, the format will also provide opportunities that improve on the classical experience.  As with any conference there will be opportunities to participate beyond the “minimal commitment.”  We hope that speakers and participants will join in other activities, which may include preview sessions for talks before the conference proper, watch parties for speakers and attendees, and mechanisms for reaching a wider audience with the technical program. With many academic interactions moving virtual, the barriers to collaboration with distant colleagues have lowered, and we hope that EC 2020 will kindle and rekindle global collaborations.</p>
<div>
<p>Further details about these activities as well as the minimal requirements will be circulated by June 1.</p>
<p>Tutorial speakers and workshop organizers will receive separate emails from the Tutorial and Workshop Chairs about plans for moving these events online.</p>
</div></div>







<p class="date">
by Jason Hartline <a href="https://agtb.wordpress.com/2020/04/21/ec-2020-will-be-virtual/"><span class="datestr">at April 21, 2020 02:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=118">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2020/04/21/a-primer-on-private-statistics-part-ii/">A Primer on Private Statistics – Part II</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>By <a href="http://www.gautamkamath.com/">Gautam Kamath</a> and <a href="http://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
<p>The second part of our brief survey of differentially private statistics. This time, we show how to privately estimate the CDF of a distribution (i.e., estimate the distribution in Kolmogorov distance), and conclude with pointers to some other work in the space.</p>
<p>The first part of this series is <a href="https://kamathematics.wordpress.com/2020/04/14/a-primer-on-private-statistics-part-i/">here</a>, and you can download both parts in PDF form <a href="http://www.gautamkamath.com/writings/primer.pdf">here</a>.</p>
<p><b>1. CDF Estimation for Discrete, Univariate Distributions </b></p>
<p>Suppose we have a distribution <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> over the ordered, discrete domain <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2CD%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\dots,D\}}" class="latex" title="{\{1,\dots,D\}}" /> and let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> be the family of all such distributions. The CDF of the distribution is the function <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BP%7D+%3A+%5C%7B1%2C%5Cdots%2CD%5C%7D+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{P} : \{1,\dots,D\} \rightarrow [0,1]}" class="latex" title="{\Phi_{P} : \{1,\dots,D\} \rightarrow [0,1]}" /> given by</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPhi_%7BP%7D%28j%29+%3D+%5Cmathop%7B%5Cmathbb+P%7D%28P+%5Cleq+j%29.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Phi_{P}(j) = \mathop{\mathbb P}(P \leq j). \ \ \ \ \ (1)" class="latex" title="\displaystyle \Phi_{P}(j) = \mathop{\mathbb P}(P \leq j). \ \ \ \ \ (1)" /></p>
<p>A natural measure of distance between CDFs is the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_\infty}" class="latex" title="{\ell_\infty}" /> distance, as this is the sort of convergence guarantee that the empirical CDF satisfies. That is, in the non-private setting, the empirical CDF will achieve the minimax rate, which it known by [<a href="https://kamathematics.wordpress.com/feed/#DKW56">DKW56</a>, <a href="https://kamathematics.wordpress.com/feed/#Mas90">Mas90</a>] to be <a name="eqdkw"></a></p>
<p><a name="eqdkw"></a></p>
<p><a name="eqdkw"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+%5CPhi_%7BX%7D+-+%5CPhi_%7BP%7D+%5C%7C_%7B%5Cinfty%7D%29+%3D+O%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D+%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \Phi_{X} - \Phi_{P} \|_{\infty}) = O\left(\sqrt{\frac{1}{n}} \right). \ \ \ \ \ (2)" class="latex" title="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \Phi_{X} - \Phi_{P} \|_{\infty}) = O\left(\sqrt{\frac{1}{n}} \right). \ \ \ \ \ (2)" /></p>
<p><a name="eqdkw"></a><a name="eqdkw"></a><a name="eqdkw"></a></p>
<p><b> 1.1. Private CDF Estimation </b></p>
<blockquote><p><b>Theorem 1</b> <em> <a name="thmcdf-ub"></a> For every <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cin+%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \in {\mathbb N}}" class="latex" title="{n \in {\mathbb N}}" /> and every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%2C%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon,\delta &gt; 0}" class="latex" title="{\epsilon,\delta &gt; 0}" />, there exists an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> such that </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BP%7D+%5C%7C_%7B%5Cinfty%7D%29+%3D+O%5Cleft%28%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7D+%2B+%5Cfrac%7B%5Clog%5E%7B3%2F2%7D%28D%29+%5Clog%5E%7B1%2F2%7D%281%2F%5Cdelta%29%7D%7B%5Cepsilon+n%7D+%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}) = O\left(\sqrt{\frac{1}{n}} + \frac{\log^{3/2}(D) \log^{1/2}(1/\delta)}{\epsilon n} \right). \ \ \ \ \ (3)" class="latex" title="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}) = O\left(\sqrt{\frac{1}{n}} + \frac{\log^{3/2}(D) \log^{1/2}(1/\delta)}{\epsilon n} \right). \ \ \ \ \ (3)" /></p>
</blockquote>
<p><em>Proof:</em> Assume without loss of generality that <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+2%5E%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = 2^{d}}" class="latex" title="{D = 2^{d}}" /> for an integer <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d \geq 1}" class="latex" title="{d \geq 1}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_{1 \cdots n} \sim P}" class="latex" title="{X_{1 \cdots n} \sim P}" /> be a sample. By the triangle inequality, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brll%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%7B%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BP%7D+%5C%7C_%7B%5Cinfty%7D%7D+%26%5Cleq%7B%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+%5CPhi_%7BX%7D+-+%5CPhi_%7BP%7D+%5C%7C_%7B%5Cinfty%7D+%2B+%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BX%7D+%5C%7C_%7B%5Cinfty%7D%29+%5C%5C+%26%5Cleq%7B%7D+O%28%5Csqrt%7B1%2Fn%7D%29+%2B+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BX%7D+%5C%7C_%7B%5Cinfty%7D%29%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rll} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}{\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}} &amp;\leq{} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \Phi_{X} - \Phi_{P} \|_{\infty} + \| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}) \\ &amp;\leq{} O(\sqrt{1/n}) + \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}), \end{array} " class="latex" title="\displaystyle \begin{array}{rll} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}{\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}} &amp;\leq{} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \Phi_{X} - \Phi_{P} \|_{\infty} + \| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}) \\ &amp;\leq{} O(\sqrt{1/n}) + \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}), \end{array} " /></p>
<p>so we will focus on constructing <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> to approximate <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{X}}" class="latex" title="{\Phi_{X}}" />.</p>
<p>For any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+0%2C%5Cdots%2Cd-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell = 0,\dots,d-1}" class="latex" title="{\ell = 0,\dots,d-1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+1%2C%5Cdots%2C2%5E%7Bd+-+%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j = 1,\dots,2^{d - \ell}}" class="latex" title="{j = 1,\dots,2^{d - \ell}}" />, consider the statistics</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f_%7B%5Cell%2Cj%7D%28X_%7B1+%5Ccdots+n%7D%29+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%7B%5Cbf+1%7D%5C%7B+%28j-1%292%5E%7B%5Cell%7D+%2B+1+%5Cleq+X_i+%5Cleq+j+2%5E%7B%5Cell%7D+%5C%7D.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle f_{\ell,j}(X_{1 \cdots n}) = \frac{1}{n} \sum_{i=1}^{n} {\bf 1}\{ (j-1)2^{\ell} + 1 \leq X_i \leq j 2^{\ell} \}. \ \ \ \ \ (4)" class="latex" title="\displaystyle f_{\ell,j}(X_{1 \cdots n}) = \frac{1}{n} \sum_{i=1}^{n} {\bf 1}\{ (j-1)2^{\ell} + 1 \leq X_i \leq j 2^{\ell} \}. \ \ \ \ \ (4)" /></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7Bf+%3A+%5C%7B1%2C%5Cdots%2CD%5C%7D%5En+%5Crightarrow+%5B0%2C1%5D%5E%7B2D+-+2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f : \{1,\dots,D\}^n \rightarrow [0,1]^{2D - 2}}" class="latex" title="{f : \{1,\dots,D\}^n \rightarrow [0,1]^{2D - 2}}" /> be the function whose output consists of all <img src="https://s0.wp.com/latex.php?latex=%7B2D-2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2D-2}" class="latex" title="{2D-2}" /> such counts. To decipher this notation, for a given <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" />, the counts <img src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Cell%2C%5Ccdot%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_{\ell,\cdot}}" class="latex" title="{f_{\ell,\cdot}}" /> form a histogram of <img src="https://s0.wp.com/latex.php?latex=%7BX_%7B1+%5Ccdots+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_{1 \cdots n}}" class="latex" title="{X_{1 \cdots n}}" /> using consecutive bins of width <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{\ell}}" class="latex" title="{2^{\ell}}" />, and we consider the <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(D)}" class="latex" title="{\log(D)}" /> histograms of geometrically increasing width <img src="https://s0.wp.com/latex.php?latex=%7B1%2C2%2C4%2C%5Cdots%2CD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1,2,4,\dots,D}" class="latex" title="{1,2,4,\dots,D}" />. First, we claim that the function <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> has low sensitivity—for adjacent samples <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X'}" class="latex" title="{X'}" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C+f%28X%29+-+f%28X%27%29+%5C%7C_2%5E2+%5Cleq+%5Cfrac%7B2+%5Clog%28D%29%7D%7Bn%5E2%7D.+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \| f(X) - f(X') \|_2^2 \leq \frac{2 \log(D)}{n^2}. \ \ \ \ \ (5)" class="latex" title="\displaystyle \| f(X) - f(X') \|_2^2 \leq \frac{2 \log(D)}{n^2}. \ \ \ \ \ (5)" /></p>
<p>Thus, we can use the Gaussian mechanism:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M%27%28X_%7B1+%5Ccdots+n%7D%29+%3D+f%28X_%7B1+%5Ccdots+n%7D%29+%2B+%5Cmathcal%7BN%7D%5Cleft%280%2C+%5Cfrac%7B2+%5Clog%28D%29+%5Clog%281%2F%5Cdelta%29%7D%7B%5Cepsilon%5E2+n%5E2%7D+%5Ccdot+%5Cmathbb%7BI%7D_%7B2D+%5Ctimes+2D%7D%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle M'(X_{1 \cdots n}) = f(X_{1 \cdots n}) + \mathcal{N}\left(0, \frac{2 \log(D) \log(1/\delta)}{\epsilon^2 n^2} \cdot \mathbb{I}_{2D \times 2D}\right). \ \ \ \ \ (6)" class="latex" title="\displaystyle M'(X_{1 \cdots n}) = f(X_{1 \cdots n}) + \mathcal{N}\left(0, \frac{2 \log(D) \log(1/\delta)}{\epsilon^2 n^2} \cdot \mathbb{I}_{2D \times 2D}\right). \ \ \ \ \ (6)" /></p>
<p>As we will argue, there exists a matrix <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%7B%5Cmathbb+R%7D%5E%7B2D+%5Ctimes+2D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A \in {\mathbb R}^{2D \times 2D}}" class="latex" title="{A \in {\mathbb R}^{2D \times 2D}}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BX%7D+%3D+A+%5Ccdot+f%28X_%7B1+%5Ccdots+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{X} = A \cdot f(X_{1 \cdots n})}" class="latex" title="{\Phi_{X} = A \cdot f(X_{1 \cdots n})}" />. We will let <img src="https://s0.wp.com/latex.php?latex=%7BM%28X_%7B1+%5Ccdots+n%7D%29+%3D+A+%5Ccdot+M%27%28X_%7B1+%5Ccdots+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X_{1 \cdots n}) = A \cdot M'(X_{1 \cdots n})}" class="latex" title="{M(X_{1 \cdots n}) = A \cdot M'(X_{1 \cdots n})}" />. Since differential privacy is closed under post-processing, <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> inherits the privacy of <img src="https://s0.wp.com/latex.php?latex=%7BM%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M'}" class="latex" title="{M'}" />.</p>
<p>We will now show how to construct the matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and analyze the error of <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" />. For any <img src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+1%2C%5Cdots%2CD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j = 1,\dots,D}" class="latex" title="{j = 1,\dots,D}" />, we can form the interval <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2Cj%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\dots,j\}}" class="latex" title="{\{1,\dots,j\}}" /> as the union of at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log D}" class="latex" title="{\log D}" /> disjoint intervals of the form we’ve computed, and therefore we can obtain <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BX%7D%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{X}(j)}" class="latex" title="{\Phi_{X}(j)}" /> as the sum of at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log D}" class="latex" title="{\log D}" /> of the entries of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X)}" class="latex" title="{f(X)}" />. For example, if <img src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j = 5}" class="latex" title="{j = 5}" /> then we can write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7B1%2C%5Cdots%2C7%5C%7D+%3D+%5C%7B1%2C%5Cdots%2C4%5C%7D+%5Ccup+%5C%7B5%2C6%5C%7D+%5Ccup+%5C%7B7%5C%7D+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \{1,\dots,7\} = \{1,\dots,4\} \cup \{5,6\} \cup \{7\} \ \ \ \ \ (7)" class="latex" title="\displaystyle \{1,\dots,7\} = \{1,\dots,4\} \cup \{5,6\} \cup \{7\} \ \ \ \ \ (7)" /></p>
<p>and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPhi_%7BX%7D%285%29+%3D+f_%7B2%2C1%7D+%2B+f_%7B1%2C3%7D+%2B+f_%7B0%2C7%7D.+%5C+%5C+%5C+%5C+%5C+%288%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Phi_{X}(5) = f_{2,1} + f_{1,3} + f_{0,7}. \ \ \ \ \ (8)" class="latex" title="\displaystyle \Phi_{X}(5) = f_{2,1} + f_{1,3} + f_{0,7}. \ \ \ \ \ (8)" /></p>
<p>See the following diagram for a visual representation of the decomposition.</p>
<p><img width="547" alt="bin-tree-mech" src="https://kamathematics.files.wordpress.com/2020/04/bin-tree-mech.png?w=547&amp;h=300" class=" wp-image-142 aligncenter" height="300" /></p>
<p>This shows hierarchical decomposition of the domain <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2C8%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\dots,8\}}" class="latex" title="{\{1,\dots,8\}}" /> using 14 intervals. The highlighted squares represent the interval <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2C7%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\dots,7\}}" class="latex" title="{\{1,\dots,7\}}" /> and the highlighted circles show the decomposition of this interval into a union of <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> intervals in the tree.</p>
<p>Thus we can construct the matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> using this information. Note that each entry of <img src="https://s0.wp.com/latex.php?latex=%7BA+f%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A f(X)}" class="latex" title="{A f(X)}" /> is the sum of at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(D)}" class="latex" title="{\log(D)}" /> entries of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X)}" class="latex" title="{f(X)}" />. Thus, if we use the output of <img src="https://s0.wp.com/latex.php?latex=%7BM%27%28X_%7B1+%5Ccdots+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M'(X_{1 \cdots n})}" class="latex" title="{M'(X_{1 \cdots n})}" /> in place of <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X_%7B1+%5Ccdots+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X_{1 \cdots n})}" class="latex" title="{f(X_{1 \cdots n})}" />, for every <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> we obtain</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPhi_%7BX%7D%28j%29+%2B+%5Cmathcal%7BN%7D%280%2C+%5Csigma%5E2%29+%5Cquad+%5Ctextrm%7Bfor%7D+%5Cquad+%5Csigma%5E2+%3D+%5Cfrac%7B+2+%5Clog%5E2%28D%29+%5Clog%281%2F%5Cdelta%29%7D%7B%5Cepsilon%5E2+n%5E2%7D.+%5C+%5C+%5C+%5C+%5C+%289%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Phi_{X}(j) + \mathcal{N}(0, \sigma^2) \quad \textrm{for} \quad \sigma^2 = \frac{ 2 \log^2(D) \log(1/\delta)}{\epsilon^2 n^2}. \ \ \ \ \ (9)" class="latex" title="\displaystyle \Phi_{X}(j) + \mathcal{N}(0, \sigma^2) \quad \textrm{for} \quad \sigma^2 = \frac{ 2 \log^2(D) \log(1/\delta)}{\epsilon^2 n^2}. \ \ \ \ \ (9)" /></p>
<p>Applying standard bounds on the expected supremum of a Gaussian process, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BX%7D+%5C%7C_%7B%5Cinfty%7D%29+%3D+O%28+%5Csigma+%5Csqrt%7B%5Clog+D%7D%29+%3D+O%5Cleft%28%5Cfrac%7B%5Clog%5E%7B3%2F2%7D%28D%29+%5Clog%5E%7B1%2F2%7D%281%2F%5Cdelta%29%7D%7B%5Cepsilon+n%7D+%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%2810%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}) = O( \sigma \sqrt{\log D}) = O\left(\frac{\log^{3/2}(D) \log^{1/2}(1/\delta)}{\epsilon n} \right). \ \ \ \ \ (10)" class="latex" title="\displaystyle \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \Phi_{X} \|_{\infty}) = O( \sigma \sqrt{\log D}) = O\left(\frac{\log^{3/2}(D) \log^{1/2}(1/\delta)}{\epsilon n} \right). \ \ \ \ \ (10)" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p><b> 1.2. Why Restrict the Domain? </b></p>
<p>A drawback of the estimator we constructed is that it only applies to distributions of finite support <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C2%2C%5Cdots%2CD%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,2,\dots,D\}}" class="latex" title="{\{1,2,\dots,D\}}" />, albeit with a relatively mild dependence on the support size. If privacy isn’t a concern, then no such restriction is necessary, as the bound <a href="https://kamathematics.wordpress.com/feed/#eqdkw">(2)</a> applies equally well to any distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}}" class="latex" title="{{\mathbb R}}" />. Can we construct a differentially private estimator for distributions with infinite support?</p>
<p>Perhaps surprisingly, the answer to this question is no! Any differentially private estimator for the CDF of the distribution has to have a rate that depends on the support size, and cannot give non-trivial rates for distributions with infinite support.</p>
<blockquote><p><b>Theorem 2 ([<a href="https://kamathematics.wordpress.com/feed/#BNSV15">BNSV15</a>])</b> <em> <a name="thmcdf-lb"></a> If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> consists of all distributions on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2CD%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\dots,D\}}" class="latex" title="{\{1,\dots,D\}}" />, then </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7BM+%5Cin+%5Cmathcal%7BM%7D_%7B1%2C+%5Cfrac%7B1%7D%7Bn%7D%7D%7D+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5CPhi_%7BP%7D+%5C%7C_%7B%5Cinfty%7D%29+%3D+%5COmega%5Cleft%28%5Cfrac%7B%5Clog%5E%2A+D%7D%7Bn%7D+%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%2811%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{M \in \mathcal{M}_{1, \frac{1}{n}}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}) = \Omega\left(\frac{\log^* D}{n} \right). \ \ \ \ \ (11)" class="latex" title="\displaystyle \min_{M \in \mathcal{M}_{1, \frac{1}{n}}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \Phi_{P} \|_{\infty}) = \Omega\left(\frac{\log^* D}{n} \right). \ \ \ \ \ (11)" /></p>
</blockquote>
<p>The notation <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5E%2A+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log^* D}" class="latex" title="{\log^* D}" /> refers to the <a href="https://en.wikipedia.org/wiki/Iterated_logarithm">iterated logarithm</a>.</p>
<p>We emphasize that this theorem shouldn’t meet with too much alarm, as <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5E%2A+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log^* D}" class="latex" title="{\log^* D}" /> grows remarkably slowly with <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />. There are differentially private CDF estimators that achieve very mild dependence on <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> [<a href="https://kamathematics.wordpress.com/feed/#BNS13">BNS13</a>, <a href="https://kamathematics.wordpress.com/feed/#BNSV15">BNSV15</a>], including one nearly matching the lower bound in Theorem <a href="https://kamathematics.wordpress.com/feed/#thmcdf-lb">2</a>. Moreover, if we want to estimate a distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}}" class="latex" title="{{\mathbb R}}" />, and are willing to make some mild regularity conditions on the distribution, then we can approximate it by a distribution with finite support and only increase the rate slightly. However, what Theorem <a href="https://kamathematics.wordpress.com/feed/#thmcdf-lb">2</a> shows is that there is no “one-size-fits-all” solution to private CDF estimation that achieves similar guarantees to the empirical CDF. That is, the right algorithm has to be tailored somewhat to the application and the assumptions we can make about the distribution.</p>
<p><b>2. More Private Statistics </b></p>
<p>Of course, the story doesn’t end here! There’s a whole wide world of differentially private statistics beyond what we’ve mentioned already. We proceed to survey just a few other directions of study in private statistics.</p>
<p><b> 2.1. Parameter and Distribution Estimation </b></p>
<p>A number of the early works in differential privacy give methods for differentially private statistical estimation for i.i.d. data. The earliest works [<a href="https://kamathematics.wordpress.com/feed/#DN03">DN03</a>, <a href="https://kamathematics.wordpress.com/feed/#DN04">DN04</a>, <a href="https://kamathematics.wordpress.com/feed/#BDMN05">BDMN05</a>, <a href="https://kamathematics.wordpress.com/feed/#DMNS06">DMNS06</a>], which introduced the Gaussian mechanism, among other foundational results, can be thought of as methods for estimating the mean of a distribution over the hypercube <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{0,1\}^d}" class="latex" title="{\{0,1\}^d}" /> in the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_\infty}" class="latex" title="{\ell_\infty}" /> norm. Tight lower bounds for this problem follow from the tracing attacks introduced in [<a href="https://kamathematics.wordpress.com/feed/#BUV14">BUV14</a>, <a href="https://kamathematics.wordpress.com/feed/#DSSUV15">DSSUV15</a>, <a href="https://kamathematics.wordpress.com/feed/#BSU17">BSU17</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17a">SU17a</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17b">SU17b</a>]. A very recent work of Acharya, Sun, and Zhang [<a href="https://kamathematics.wordpress.com/feed/#ASZ20">ASZ20</a>] adapts classical tools for proving estimation and testing lower bounds (lemmata of Assouad, Fano, and Le Cam) to the differentially private setting. Steinke and Ullman [<a href="https://kamathematics.wordpress.com/feed/#SU17b">SU17b</a>] give tight minimax lower bounds for the weaker guarantee of selecting the largest coordinates of the mean, which were refined by Cai, Wang, and Zhang [<a href="https://kamathematics.wordpress.com/feed/#CWZ19">CWZ19</a>] to give lower bounds for sparse mean-estimation problems.</p>
<p>Nissim, Raskhodnikova, and Smith introduced the highly general sample-and-aggregate paradigm, which they apply to several learning problems (e.g., learning mixtures of Gaussians) [<a href="https://kamathematics.wordpress.com/feed/#NRS07">NRS07</a>]. Later, Smith [<a href="https://kamathematics.wordpress.com/feed/#Smi11">Smi11</a>] showed that this paradigm can be used to transform any estimator for any asymptotically normal, univariate statistic over a bounded data domain into a differentially private one with the same asymptotic convergence rate.</p>
<p>Subsequent work has focused on both relaxing the assumptions in [<a href="https://kamathematics.wordpress.com/feed/#Smi11">Smi11</a>], particularly boundedness, and on giving finite-sample guarantees. Karwa and Vadhan investigated the problem of Gaussian mean estimation, proving the first near-optimal bounds for this setting [<a href="https://kamathematics.wordpress.com/feed/#KV18">KV18</a>]. In particular, exploiting concentration properties of Gaussian data allows us to achieve non-trivial results even with unbounded data, which is impossible in general. Following this, Kamath, Li, Singhal, and Ullman moved to the multivariate setting, investigating the estimation of Gaussians and binary product distributions in total variation distance [<a href="https://kamathematics.wordpress.com/feed/#KLSU19">KLSU19</a>]. In certain cases (i.e., Gaussians with identity covariance), this is equivalent to mean estimation in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2}" class="latex" title="{\ell_2}" />-distance, though not always. For example, for binary product distribution, one must estimate the mean in a type of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\chi^2}" class="latex" title="{\chi^2}" />-distance instead. The perspective of distribution estimation rather than parameter estimation can be valuable. Bun, Kamath, Steinke, and Wu [<a href="https://kamathematics.wordpress.com/feed/#BKSW19">BKSW19</a>] develop a primitive for private hypothesis selection, which they apply to learn any coverable class of distributions under pure differential privacy. Through the lens of distribution estimation, their work implies an upper bound for mean estimation of binary product distributions that bypasses lower bounds for the same problem in the empirical setting. In addition to work on mean estimation in the sub-Gaussian setting, such as the results discussed earlier, mean estimation has also been studied under weaker moment conditions [<a href="https://kamathematics.wordpress.com/feed/#BS19">BS19</a>, <a href="https://kamathematics.wordpress.com/feed/#KSU20">KSU20</a>]. Beyond these settings, there has also been study of estimation of discrete multinomials, including estimation in Kolmogorov distance [<a href="https://kamathematics.wordpress.com/feed/#BNSV15">BNSV15</a>] and in total variation distance for structured distributions [<a href="https://kamathematics.wordpress.com/feed/#DHS15">DHS15</a>], and parameter estimation of Markov Random Fields [<a href="https://kamathematics.wordpress.com/feed/#ZKKW20">ZKKW20</a>].</p>
<p>A different approach to constructing differentially private estimators is based on robust statistics. This approah begins with the influential work of Dwork and Lei [<a href="https://kamathematics.wordpress.com/feed/#DL09">DL09</a>], which introduced the propose-test-release framework, and applied to estimating robust statistics such as the median and interquartile range. While the definitions in robust statistics and differential privacy are semantically similar, formal connections between the two remain relatively scant, which suggests a productive area for future study.</p>
<p><b> 2.2. Hypothesis Testing </b></p>
<p>An influential work of Homer et al. [<a href="https://kamathematics.wordpress.com/feed/#HSRDTMPSNC08">HSRDTMPSNC08</a>] demonstrated the vulnerability of classical statistics in a genomic setting, showing that certain <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\chi^2}" class="latex" title="{\chi^2}" />-statistics on many different variables could allow an attacker to determine the presence of an individual in a genome-wide association study (GWAS). Motivated by these concerns, an early line of work from the statistics community focused on addressing these issues [<a href="https://kamathematics.wordpress.com/feed/#VS09">VS09</a>, <a href="https://kamathematics.wordpress.com/feed/#USF13">USF13</a>, <a href="https://kamathematics.wordpress.com/feed/#YFSU14">YFSU14</a>].</p>
<p>More recently, work on private hypothesis testing can be divided roughly into two lines. The first focuses on the minimax sample complexity, in a line initiated by Cai, Daskalakis, and Kamath [<a href="https://kamathematics.wordpress.com/feed/#CDK17">CDK17</a>], who give an algorithm for privately testing goodness-of-fit (more precisely, a statistician might refer to this problem as one-sample testing of multinomial data). A number of subsequent works have essentially settled the complexity of this problem [<a href="https://kamathematics.wordpress.com/feed/#ASZ18">ASZ18</a>, <a href="https://kamathematics.wordpress.com/feed/#ADR18">ADR18</a>], giving tight upper and lower bounds. Other papers in this line study related problems, including the two-sample version of the problem, independence testing, and goodness-of-fit testing for multivariate product distributions [<a href="https://kamathematics.wordpress.com/feed/#ASZ18">ASZ18</a>, <a href="https://kamathematics.wordpress.com/feed/#ADR18">ADR18</a>, <a href="https://kamathematics.wordpress.com/feed/#ADKR19">ADKR19</a>, <a href="https://kamathematics.wordpress.com/feed/#CKMUZ19">CKMUZ19</a>]. A related paper studies the minimax sample complexity of property <em>estimation</em>, rather than testing of discrete distributions, including support size and entropy [<a href="https://kamathematics.wordpress.com/feed/#AKSZ18">AKSZ18</a>]. Other recent works in this vein focus on testing of simple hypotheses [<a href="https://kamathematics.wordpress.com/feed/#CKMTZ18">CKMTZ18</a>, <a href="https://kamathematics.wordpress.com/feed/#CKMSU19">CKMSU19</a>]. In particular [<a href="https://kamathematics.wordpress.com/feed/#CKMSU19">CKMSU19</a>] proves an analogue of the Neyman-Pearson Lemma for differentially private testing of simple hypotheses. A paper of Awan and Slavkovic [<a href="https://kamathematics.wordpress.com/feed/#AS18">AS18</a>] gives a universally optimal test when the domain size is two, however Brenner and Nissim [<a href="https://kamathematics.wordpress.com/feed/#BN14">BN14</a>] shows that such universally optimal tests cannot exist when the domain has more than two elements. A related problem in this space is private change-point detection [<a href="https://kamathematics.wordpress.com/feed/#CKMTZ18">CKMTZ18</a>, <a href="https://kamathematics.wordpress.com/feed/#CKMSU19">CKMSU19</a>, <a href="https://kamathematics.wordpress.com/feed/#CKLZ19">CKLZ19</a>] — in this setting, we are given a time series of datapoints which are sampled from a distribution, which at some point, changes to a different distribution. The goal is to (privately) determine when this point occurs.</p>
<p>Complementary to minimax hypothesis testing, a line of work [<a href="https://kamathematics.wordpress.com/feed/#WLK15">WLK15</a>, <a href="https://kamathematics.wordpress.com/feed/#GLRV16">GLRV16</a>, <a href="https://kamathematics.wordpress.com/feed/#KR17">KR17</a>, <a href="https://kamathematics.wordpress.com/feed/#KSF17">KSF17</a>, <a href="https://kamathematics.wordpress.com/feed/#CBRG18">CBRG18</a>, <a href="https://kamathematics.wordpress.com/feed/#SGGRGB19">SGGRGB19</a>, <a href="https://kamathematics.wordpress.com/feed/#CKSBG19">CKSBG19</a>] designs differentially private versions of popular test statistics for testing goodness-of-fit, closeness, and independence, as well as private ANOVA, focusing on the performance at small sample sizes. Work by Wang et al. [<a href="https://kamathematics.wordpress.com/feed/#WKLK18">WKLK18</a>] focuses on generating statistical approximating distributions for differentially private statistics, which they apply to hypothesis testing problems.</p>
<p><b> 2.3. Differential Privacy on Graphs </b></p>
<p>There is a significant amount of work on differentially private analysis of graphs. We remark that these algorithms can satisfy either edge or node differential privacy. The former (easier) guarantee defines a neighboring graph to be one obtained by adding or removing a single edge, while in the latter (harder) setting, a neighboring graph is one that can be obtained by modifying the set of edges connected to a single node. The main challenge in this area is that most graph statistics can have high sensitivity in the worst-case.</p>
<p>The initial works in this area focused on the empirical setting, and goals range from counting subgraphs [<a href="https://kamathematics.wordpress.com/feed/#KRSY11">KRSY11</a>, <a href="https://kamathematics.wordpress.com/feed/#BBDS13">BBDS13</a>, <a href="https://kamathematics.wordpress.com/feed/#KNRS13">KNRS13</a>, <a href="https://kamathematics.wordpress.com/feed/#CZ13">CZ13</a>, <a href="https://kamathematics.wordpress.com/feed/#RS16">RS16</a>] to outputting a privatized graph which approximates the original [<a href="https://kamathematics.wordpress.com/feed/#GRU12">GRU12</a>, <a href="https://kamathematics.wordpress.com/feed/#BBDS12">BBDS12</a>, <a href="https://kamathematics.wordpress.com/feed/#Upa13">Upa13</a>, <a href="https://kamathematics.wordpress.com/feed/#AU19">AU19</a>, <a href="https://kamathematics.wordpress.com/feed/#EKKL20">EKKL20</a>]. In contrast to the setting discussed in most of this series, it seems that there are larger qualitative differences between the study of empirical and population statistics due to the fact that many graph statistics have high worst-case sensitivity, but may have smaller sensitivity on typical graphs from many natural models.</p>
<p>In the population statistics setting, recent work has focused on parameter estimation of the underlying random graph model. So far this work has given estimators for the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\beta}" class="latex" title="{\beta}" />-model [<a href="https://kamathematics.wordpress.com/feed/#KS16">KS16</a>] and graphons [<a href="https://kamathematics.wordpress.com/feed/#BCS15">BCS15</a>,<a href="https://kamathematics.wordpress.com/feed/#BCSZ18">BCSZ18</a>]. Graphons are a generalization of the stochastic block model, which is, in turn, a generalization of the Erdös-Rényi model. Interestingly, the methods of Lipschitz-extensions introduced in the empirical setting by [<a href="https://kamathematics.wordpress.com/feed/#BBDS13">BBDS13</a>, <a href="https://kamathematics.wordpress.com/feed/#KNRS13">KNRS13</a>] are the main tool used in the statistical setting as well. While the first works on private graphon estimation were not computationally efficient, a recent focus has been on obviating these issues for certain important cases, such as the Erdös-Rényi setting [<a href="https://kamathematics.wordpress.com/feed/#SU19">SU19</a>].</p>
<p><b>Bibliography</b></p>
<p><a name="ADKR19"></a>[ADKR19] Maryam Aliakbarpour, Ilias Diakonikolas, Daniel M. Kane, and Ronitt Rubinfeld. Private testing of distributions via sample permutations. NeurIPS ’19.</p>
<p><a name="ADR18"></a>[ADR18] Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity and closeness testing of discrete distributions. ICML ’18.</p>
<p><a name="AKSZ18"></a>[AKSZ18] Jayadev Acharya, Gautam Kamath, Ziteng Sun, and Huanyu Zhang. Inspectre: Privately estimating the unseen. ICML ’18.</p>
<p><a name="AS18"></a>[AS18] Jordan Awan and Aleksandra Slavković. Differentially private uniformly most powerful tests for binomial data. NeurIPS ’18.</p>
<p><a name="ASZ18"></a>[ASZ18] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and closeness of discrete distributions. NeurIPS ’18.</p>
<p><a name="ASZ20"></a>[ASZ20] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private Assouad, Fano, and Le Cam. arXiv, 2004.06830, 2020.</p>
<p><a name="AU19"></a>[AU19] Raman Arora and Jalaj Upadhyay. On differentially private graph sparsification and applications. NeurIPS ’19.</p>
<p><a name="BBDS12"></a>[BBDS12] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. The Johnson-Lindenstrauss transform itself preserves differential privacy. FOCS ’12.</p>
<p><a name="BBDS13"></a>[BBDS13] Jeremiah Blocki, Avrim Blum, Anupam Datta, and Or Sheffet. Differentially private data analysis of social networks via restricted sensitivity. ITCS ’13.</p>
<p><a name="BCS15"></a>[BCS15] Christian Borgs, Jennifer Chayes, and Adam Smith. Private graphon estimation for sparse graphs. NIPS ’15.</p>
<p><a name="BCSZ18"></a>[BCSZ18] Christian Borgs, Jennifer Chayes, Adam Smith, and Ilias Zadik. Revealing network structure, confidentially: Improved rates for node-private graphon estimation. FOCS ’18.</p>
<p><a name="BDMN05"></a>[BDMN05] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: The SuLQ framework. PODS ’05.</p>
<p><a name="BKSW19"></a>[BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. NeurIPS ’19.</p>
<p><a name="BN14"></a>[BN14] Hai Brenner and Kobbi Nissim. Impossibility of differentially private universally optimal mechanisms. SIAM Journal on Computing, 43(5), 2014.</p>
<p><a name="BNS13"></a>[BNS13] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM ’13.</p>
<p><a name="BNSV15"></a>[BNSV15] Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS ’15.</p>
<p><a name="BS19"></a>[BS19] Mark Bun and Thomas Steinke. Average-case averages: Private algorithms for smooth sensitivity and mean estimation. NeurIPS ’19.</p>
<p><a name="BSU17"></a>[BSU17] Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in differential privacy. SODA ’17.</p>
<p><a name="BUV14"></a>[BUV14] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. STOC ’14.</p>
<p><a name="CBRG18"></a>[CBRG18] Zachary Campbell, Andrew Bray, Anna Ritz, and Adam Groce. Differentially private ANOVA testing. ICDIS ’18.</p>
<p><a name="CDK17"></a>[CDK17] Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Priv’it: Private and sample efficient identity testing. ICML ’17.</p>
<p><a name="CKLZ19"></a>[CKLZ19] Rachel Cummings, Sara Krehbiel, Yuliia Lut, and Wanrong Zhang. Privately detecting changes in unknown distributions. arXiv, 1910.01327, 2019.</p>
<p><a name="CKMSU19"></a>[CKMSU19] Clément L. Canonne, Gautam Kamath, Audra McMillan, Adam Smith, and Jonathan Ullman. The structure of optimal private tests for simple hypotheses. STOC ’19.</p>
<p><a name="CKMTZ18"></a>[CKMTZ18] Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, and Wanrong Zhang. Differentially private change-point detection. NeurIPS ’18.</p>
<p><a name="CKMUZ19"></a>[CKMUZ19] Clément L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou. Private identity testing for high-dimensional distributions. arXiv, 1905.11947, 2019.</p>
<p><a name="CKSBG19"></a>[CKSBG19] Simon Couch, Zeki Kazan, Kaiyan Shi, Andrew Bray, and Adam Groce. Differentially private nonparametric hypothesis testing. CCS ’19.</p>
<p><a name="CWZ19"></a>[CWZ19] T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. arXiv, 1902.04495, 2019.</p>
<p><a name="CZ13"></a>[CZ13] Shixi Chen and Shuigeng Zhou. Recursive mechanism: Towards node differential privacy and unrestricted joins. SIGMOD ’13.</p>
<p><a name="DHS15"></a>[DHS15] Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured discrete distributions. NIPS ’15.</p>
<p><a name="DKW56"></a>[DKW56] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, 27(3), 1956.</p>
<p><a name="DL09"></a>[DL09] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. STOC ’09.</p>
<p><a name="DMNS06"></a>[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. TCC ’06.</p>
<p><a name="DN03"></a>[DN03] Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. PODS ’03.</p>
<p><a name="DN04"></a>[DN04] Cynthia Dwork and Kobbi Nissim. Privacy-preserving datamining on vertically partitioned databases. CRYPTO ’04.</p>
<p><a name="DSSUV15"></a>[DSSUV15] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. FOCS ’15.</p>
<p><a name="EKKL20"></a>[EKKL20] Marek Eliáš, Michael Kapralov, Janardhan Kulkarni, and Yin Tat Lee. Differentially private release of synthetic graphs. SODA ’20.</p>
<p><a name="GLRV16"></a>[GLRV16] Marco Gaboardi, Hyun-Woo Lim, Ryan M. Rogers, and Salil P. Vadhan. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. ICML ’16.</p>
<p><a name="GRU12"></a>[GRU12] Anupam Gupta, Aaron Roth, and Jonathan Ullman. Iterative constructions and private data release. TCC ’12.</p>
<p><a name="HSRDTMPSNC08"></a>[HSRDTMPSNC08] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. PLoS Genetics, 4(8), 2008.</p>
<p><a name="KLSU19"></a>[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional distributions. COLT ’19.</p>
<p><a name="KNRS13"></a>[KNRS13] Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Analyzing graphs with node differential privacy. TCC ’13.</p>
<p><a name="KR17"></a>[KR17] Daniel Kifer and Ryan M. Rogers. A new class of private chi-square tests. AISTATS ’17.</p>
<p><a name="KRSY11"></a>[KRSY11] Vishesh Karwa, Sofya Raskhodnikova, Adam Smith, and Grigory Yaroslavtsev. Private analysis of graph structure. VLDB ’11.</p>
<p><a name="KS16"></a>[KS16] Vishesh Karwa and Aleksandra Slavković. Inference using noisy degrees: Differentially private β-model and synthetic graphs. The Annals of Statistics, 44(1), 2016.</p>
<p><a name="KSF17"></a>[KSF17] Kazuya Kakizaki, Jun Sakuma, and Kazuto Fukuchi. Differentially private chi-squared test by unit circle mechanism. ICML ’17.</p>
<p><a name="KSU20"></a>[KSU20] Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distributions. arXiv, 2002.09464, 2020.</p>
<p><a name="KV18"></a>[KV18] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. ITCS ’18.</p>
<p><a name="Mas90"></a>[Mas90] Pascal Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. The Annals of Probability, 18(3), 1990.</p>
<p><a name="NRS07"></a>[NRS07] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. STOC ’07.</p>
<p><a name="RS16"></a>[RS16] Sofya Raskhodnikova and Adam D. Smith. Lipschitz extensions for node-private graph statistics and the generalized exponential mechanism. FOCS ’16.</p>
<p><a name="Smi11"></a>[Smi11] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. STOC ’11.</p>
<p><a name="SGGRGB19"></a>[SGGRGB19] Marika Swanberg, Ira Globus-Harris, Iris Griffith, Anna Ritz, Adam Groce, and Andrew Bray. Improved differentially private analysis of variance. PETS ’19.</p>
<p><a name="SU17a"></a>[SU17a] Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. Journal of Privacy and Confidentiality, 7(2), 2017.</p>
<p><a name="SU17b"></a>[SU17b] Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. FOCS ’17.</p>
<p><a name="SU19"></a>[SU19] Adam Sealfon and Jonathan Ullman. Efficiently estimating Erdos-Renyi graphs with node differential privacy. NeurIPS ’19.</p>
<p><a name="Upa13"></a>[Upa13] Jalaj Upadhyay. Random projections, graph sparsification, and differential privacy. ASIACRYPT ’13.</p>
<p><a name="USF13"></a>[USF13] Caroline Uhler, Aleksandra Slavković, and Stephen E. Fienberg. Privacy-preserving data sharing for genome-wide association studies. The Journal of Privacy and Confidentiality, 5(1), 2013.</p>
<p><a name="VS09"></a>[VS09] Duy Vu and Aleksandra Slavković. Differential privacy for clinical trial data: Preliminary evaluations. ICDMW ’09.</p>
<p><a name="WKLK18"></a>[WKLK18] Yue Wang, Daniel Kifer, Jaewoo Lee, and Vishesh Karwa. Statistical approximating distributions under differential privacy. The Journal of Privacy and Confidentiality, 8(1), 2018.</p>
<p><a name="WLK15"></a>[WLK15] Yue Wang, Jaewoo Lee, and Daniel Kifer. Revisiting differentially private hypothesis tests for categorical data. arXiv, 1511.03376, 2015.</p>
<p><a name="YFSU14"></a>[YFSU14] Fei Yu, Stephen E. Fienberg, Aleksandra B. Slavković, and Caroline Uhler. Scalable privacy-preserving data sharing methodology for genome-wide association studies. Journal of Biomedical Informatics, 50, 2014.</p>
<p><a name="ZKKW20"></a>[ZKKW20] Huanyu Zhang, Gautam Kamath, Janardhan Kulkarni, and Zhiwei Steven Wu. Privately learning Markov random fields. arXiv, 2002.09463, 2020.</p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2020/04/21/a-primer-on-private-statistics-part-ii/"><span class="datestr">at April 21, 2020 01:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4768">AirToAll: Another guest post by Steve Ebin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Scott’s foreword:</span></strong> Today I’m honored to host another guest post by friend-of-the-blog Steve Ebin, who not only published a <a href="https://www.scottaaronson.com/blog/?p=4675">beautiful essay</a> here a month ago (the one that I titled “First it came from Wuhan”), but also posted an extremely informative <a href="https://www.scottaaronson.com/blog/?p=4695#comment-1834991">timeline</a> of what he understood when about the severity of the covid crisis, from early January until March 31st.  By the latter date, Steve had quit his job, having made a hefty sum shorting airline stocks, and was devoting his full time to a new nonprofit to manufacture low-cost ventilators, called AirToAll.  A couple weeks ago, Steve was kind enough to include me in one of AirToAll’s regular Zoom meetings; I learned more about pistons than I had in my entire previous life (admittedly, still not much).  Which brings me to what Steve wants to talk about today: what he and others are doing and how <em>you</em> can help.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Without further ado, Steve’s guest post:</span></strong></p>



<p>In my <a href="https://www.scottaaronson.com/blog/?p=4675">last essay</a> on Coronavirus, I argued that Coronavirus will radically change society.  In this blog post, I’d like to propose a structure for how we can organize to fight the virus.  I will also make a call to action for readers of this blog to help a non-profit I co-founded, <a href="http://airtoall.org">AirToAll</a>, build safe, low-cost ventilators and other medical devices and distribute them across the world at scale.</p>



<p>There are four ways we can help fight coronavirus:</p>



<ol><li><strong>Reduce exposure to the virus. </strong>Examples: learn where the virus is through better testing; attempt to be where the virus isn’t through social distancing, quarantining, and other means.</li><li><strong>Reduce the chance of exposure leading to infection.</strong> Examples: Wash your hands; avoid touching your face; wear personal protective equipment.</li><li><strong>Reduce the chance of infection leading to serious illness. </strong>Examples: improve your aerobic and pulmonary health; make it more difficult for coronavirus’s spike protein to bind to ACE-2 receptors; scale antibody therapies; consume adequate vitamin D; get more sleep; develop a vaccine.</li><li><strong>Reduce the chance of serious illness leading to death. </strong>Examples: ramp up the production and distribution of certain drugs; develop better drugs; build more ventilators; help healthcare workers.</li></ol>



<p>Obviously, not every example I listed is practical, advisable, or will work, and some options, like producing a vaccine, may be better solutions than others. But we must pursue all approaches.</p>



<p>I’ve been devoting my own time to pursuing the fourth approach, reducing the chance that the illness will lead to death.  Specifically, along with Neil Thanedar, I co-founded AirToAll, a nonprofit that helps bring low-cost, reliable, and clinically tested ventilators to market.  I know lots of groups are working on this problem, so I thought I’d talk about it briefly.</p>



<p>First, like many groups, we’re designing our own ventilators.  Although designing ventilators and bringing them to market at scale poses unique challenges, particularly in an environment where supply chains are strained, this is <em>much </em>easier than it must have been to build iron lungs in the early part of the 20th century, when Zoom conferencing wasn’t yet invented.  When it comes to the ventilators we’re producing, we’re focused on safety and clinical validation rather than speed to market.  We are not the farthest along here, but we’ve made good progress.</p>



<p>Second, our nonprofit is helping other groups produce safe and reliable ventilators by doing direct consultations with them and also by producing <a href="https://591654b2-a393-4da6-9d87-f168fb514898.filesusr.com/ugd/9fa1d3_9f8fa025b877468e91b8ba575c054815.pdf">whitepapers</a> to help them think through the issues at hand (h/t to Harvey Hawes, Abdullah Saleh, and our friends at <a href="https://icchange.ca/">ICChange</a>).</p>



<p>Third, we’re working to increase the manufacturing capacity for currently approved ventilators.</p>



<p>The current shortage of ventilators is a symptom of a greater underlying problem: namely, the world is not good at recognizing healthcare crises early and responding to them quickly.  While our nonprofit helps bring more ventilators to market, we are also trying to solve this greater underlying problem.  I look at our work in ventilator-land as a first step towards our ultimate goal of making medical devices cheaper and more available through an open-source nonprofit model.</p>



<p>I am writing this post as a call to action to you, dear <em>Shtetl-Optimized</em> reader, to get involved.</p>



<p>You don’t have to be an engineer, pulmonologist, virologist, or epidemiologist to help us, although those skillsets are of course helpful and if you are we’d love to have you.  If you have experience in data science and modeling, supply chain and manufacturing, public health, finance, operations, community management, or anything else a rapidly scaling organization needs, you can help us too. </p>



<p>We are a group of 700+ volunteers and growing rapidly.  If you’d like to help, we’d love to have you.  If you might be interested in volunteering, click <a href="https://airtoall.org/community/">here</a>.  Donors click <a href="https://airtoall.org/donate/">here</a>.  Everyone else, please email me at <a href="mailto:steven@airtoall.org">steven@airtoall.org</a> and include a clear subject line so I can direct you to the right person.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4768"><span class="datestr">at April 20, 2020 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/04/20/workshop-on-local-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/04/20/workshop-on-local-algorithms/">Workshop on Local Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 20-21, 2020 Virtual (8am — 12pm Pacific Time) https://www.mit.edu/~mahabadi/workshops/WOLA-2020.html Submission deadline: May 15, 2020 Registration deadline: June 30, 2020 Due to the current situation with COVID-19, we have decided to hold a virtual and shorter version of WOLA this year. WOLA 2020 will run for two days between 8am – 12pm PT to maximize … <a href="https://cstheory-events.org/2020/04/20/workshop-on-local-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">Workshop on Local Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/04/20/workshop-on-local-algorithms/"><span class="datestr">at April 20, 2020 08:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/053">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/053">TR20-053 |  Understanding the Relative Strength of QBF CDCL Solvers and QBF Resolution | 

	Benjamin Böhm, 

	Olaf Beyersdorff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
QBF solvers implementing the QCDCL paradigm are powerful algorithms that
successfully tackle many computationally complex applications. However, our
theoretical understanding of the strength and limitations of these QCDCL
solvers is very limited.

In this paper we suggest to formally model QCDCL solvers as proof systems. We
define different policies that can be used for decision heuristics and unit
propagation and give rise to a number of sound and complete QBF proof systems
(and hence new QCDCL algorithms). With respect to the standard policies used
in practical QCDCL solving, we show that the corresponding QCDCL proof system
is incomparable (via exponential separations) to Q-resolution, the classical
QBF resolution system used in the literature. This is in stark contrast to the
propositional setting where CDCL and resolution are known to be p-equivalent.

This raises the question what formulas are hard for standard QCDCL, since
Q-resolution lower bounds do not necessarily apply to QCDCL as we show here.
In answer to this question we prove several lower bounds for QCDCL, including
exponential lower bounds for a large class of random QBFs.

We also introduce a strengthening of the decision heuristic used in classical
QCDCL, which does not necessarily decide variables in order of the prefix, but
still allows to learn asserting clauses. We show that with this decision
policy, QCDCL can be exponentially faster on some formulas.

We further exhibit a QCDCL proof system that is p-equivalent to Q-resolution.
In comparison to classical QCDCL, this new QCDCL version adapts both decision
and unit propagation policies.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/053"><span class="datestr">at April 20, 2020 07:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4762">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4762">Lockdown day 39</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>This is <em>really</em> getting depressing.  One of the only things that makes it bearable—even though in some sense it shouldn’t—is that most of humanity is in this together.  For once, there’s no question of “why me?”</li><li>Having watched the eighth and final episode of <em>Devs</em>, the thought occurred to me: if I’d had the opportunity to restart the world from 8 months ago, even inside a simulation, I’d seize the chance and never look back.</li><li>I think I finally figured out how to explain the issue with <em>Devs</em> to my literary sophisticate readers.  Namely: <em>Devs</em> consists, precisely, of the <strong>cultural appropriation </strong>of quantum computing.  Now, I never felt like cultural appropriation was the world’s worst problem—not even <em>before</em> a pandemic started overflowing the morgues—so I wouldn’t say I was <em>offended</em> by Alex Garland appropriating the images and buzzwords of my quantum computing tribe for a basically unrelated purpose, but it is what it is.  Again: <em>Devs</em> is the show for you, if you want a haunting, slow-paced, well-produced meditation about free will and determinism and predicting the future and parallel worlds and “what if the whole universe is a simulation?,” and the various ideas I would’ve had about such topics around the age of 11.  It’s just not a show about quantum computing.  I hope that makes it clear.</li><li>I read with interest <a href="https://project-evidence.github.io/">this anonymous but PGP-signed article</a>, laying out the case that it’s <em>plausible</em> that covid accidentally leaked from either the Wuhan Institute of Virology or the Wuhan CDC, rather than originating at the Huanan seafood market.  Or, as an intermediate hypothesis, that an infected animal from one of those labs ended up at the seafood market.  (Note that this is completely different from the hypothesis that covid was purposefully engineered—the authors of the article find that totally implausible, and I agree with them.)  Notably, the Wuhan labs are known to have experimented with bat coronaviruses very much like covid, and are known to have performed “gain-of-function” experiments on them, and were probably the central labs in China for such experiments.  And viruses are known to have leaked from other labs in China on other occasions, and the nature → seafood market route has unresolved issues, like where exactly the crossover from bats to pangolins (or some other intermediate species) is supposed to have happened, such that people would only start getting infected at the seafood market and not at its faraway suppliers, and … well, anyway, read the article and form your own judgment!</li><li>I find it interesting that three months ago, I would’ve hesitated even to share such a link, because my internal critic would’ve screamed “this looks too much like tinfoil-hat stuff—are you ready for all the people you respect sneering at you?”  But the me of three months ago is not the me of today.  I make no apologies for adapting my thoughts to the freak branch of the multiverse where I actually find myself.</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4762"><span class="datestr">at April 19, 2020 10:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=415">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/04/19/tcs-talk-wednesday-april-22-huacheng-yu-princeton/">TCS+ talk: Wednesday, April 22 — Huacheng Yu, Princeton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 22nd at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Huacheng Yu</strong> from Princeton will speak about a “<em>Nearly Optimal Static Las Vegas Succinct Dictionary</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk">the online form</a>. Due to security concerns, <strong>registration is required</strong> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a>on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Given a set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n" class="latex" title="n" /> (distinct) keys from key space <img src="https://s0.wp.com/latex.php?latex=%5BU%5D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="[U]" class="latex" title="[U]" />, each associated with a value from <img src="https://s0.wp.com/latex.php?latex=%5CSigma&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\Sigma" class="latex" title="\Sigma" />, the <em>static dictionary problem</em> asks to preprocess these (key, value) pairs into a data structure, supporting value-retrieval queries: for any given <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5BU%5D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="x \in [U]" class="latex" title="x \in [U]" />, valRet<img src="https://s0.wp.com/latex.php?latex=%28x%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="(x)" class="latex" title="(x)" /> must return the value associated with <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0" alt="x" class="latex" title="x" /> if <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0" alt="x" class="latex" title="x" /> is in <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />, or return “N/A” if <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0" alt="x" class="latex" title="x" /> is not in <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />. The special case where <img src="https://s0.wp.com/latex.php?latex=%7C%5CSigma%7C%3D1&amp;bg=fff&amp;fg=444444&amp;s=0" alt="|\Sigma|=1" class="latex" title="|\Sigma|=1" /> is called the membership problem. The “textbook” solution is to use a hash table, which occupies linear space and answers each query in constant time. On the other hand, the minimum possible space to encode all (key, value) pairs is only <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BOPT%7D+%3A%3D+%5Clg+%5Cbinom%7BU%7D%7Bn%7D+%2B+n+%5Clg+%7C%5CSigma%7C&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textrm{OPT} := \lg \binom{U}{n} + n \lg |\Sigma|" class="latex" title="\textrm{OPT} := \lg \binom{U}{n} + n \lg |\Sigma|" /> bits, which could be much less.</p>
<p>In this talk, we will talk about a randomized dictionary data structure using <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BOPT%7D%2B%5Ctextrm%7Bpoly%7D%5Clg+n%2BO%28%5Clg%5Clg%5Ccdots%5Clg+U%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textrm{OPT}+\textrm{poly}\lg n+O(\lg\lg\cdots\lg U)" class="latex" title="\textrm{OPT}+\textrm{poly}\lg n+O(\lg\lg\cdots\lg U)" /> bits of space and expected constant query time, assuming the query algorithm have access to an external data-independent lookup table of size <img src="https://s0.wp.com/latex.php?latex=n%5E%7B0.001%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="n^{0.001}" class="latex" title="n^{0.001}" />. Previously, even for membership queries and when <img src="https://s0.wp.com/latex.php?latex=U%5Cleq+n%5E%7BO%281%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="U\leq n^{O(1)}" class="latex" title="U\leq n^{O(1)}" />, the best known data structure with constant query time requires <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BOPT%7D%2Bn%2F%5Ctextrm%7Bpoly%7D+%5Clg+n&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textrm{OPT}+n/\textrm{poly} \lg n" class="latex" title="\textrm{OPT}+n/\textrm{poly} \lg n" /> bits of space (due to Pagh [Pagh’01] and Pătraşcu [Pat’08]). It has <img src="https://s0.wp.com/latex.php?latex=O%28%5Clg+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" alt="O(\lg n)" class="latex" title="O(\lg n)" /> query time when the space is at most <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BOPT%7D%2Bn%5E%7B0.999%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textrm{OPT}+n^{0.999}" class="latex" title="\textrm{OPT}+n^{0.999}" />.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/04/19/tcs-talk-wednesday-april-22-huacheng-yu-princeton/"><span class="datestr">at April 19, 2020 06:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=19738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/">To cheer you up in difficult times II: Mysterious matching news by Gal Beniamini, Naom Nisan, Vijay Vazirani and Thorben Tröbst!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Matching is one of the richest gold mines for ideas and results in mathematics, computer science and other areas.  Today I want to briefly tell you about a curious, surprising, mysterious, and <a href="https://arxiv.org/abs/2001.07642">cheerful recent result</a> by Gal Beniamini and Noam Nisan and a subsequent work of Vijay Vazirani. It is a result that will cheer up combinatorialists on both sides of the aisle: graph theorists and researchers in extremal and probabilistic combinatorics as well as algebraic and enumerative combinatorialists.  (And it is related to query complexity, Eulerian lattices, Birkhoff’s polytope, a theorem of Lou Billera and Aravamuthan Sarangarajan, evasiveness, analysis of Boolean functions, and various other things.) At the end of the post I will remind you of a central problem in matching theory: that of extending Lovasz’ randomized algorithm for matching to general graphs. (Perhaps methods from algebraic combinatorics can help.)</p>
<p>I will start with sad news. <a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John Horton Conway</a>, an amazing mathematical hero,  passed away a few days ago. There are very nice posts on Conway’s work <a href="https://www.scottaaronson.com/blog/?p=4732">by Scott Aaronson</a> (with many nice memories in the comments section), <a href="https://terrytao.wordpress.com/2020/04/12/john-conway/">by Terry Tao</a>, and by <a href="https://rjlipton.wordpress.com/2020/04/14/john-horton-conway-1937-2020/">Dick Lipton and Ken Regan</a>. And<a href="https://xkcd.com/2293/"> a moving obituary on xkcd</a> with a touch of ingenuity of Conway’s style. (There is also a question on MO  “<a href="https://mathoverflow.net/questions/357197/conways-lesser-known-results">Conway’s less known results</a>,” and two questions on the game of life (<a href="https://mathoverflow.net/questions/132402/conways-game-of-life-for-random-initial-position">I</a>, <a href="https://cstheory.stackexchange.com/questions/17914/does-a-noisy-version-of-conways-game-of-life-support-universal-computation">II</a>).)</p>
<p>Another reading material to cheer you up is my paper: <a href="https://gilkalai.files.wordpress.com/2020/04/laws-blog.pdf">The argument against quantum computers, the quantum laws of nature, and Google’s supremacy claims.</a> It is for <em>Laws, Rigidity and Dynamics,</em> Proceedings of the <a href="https://gilkalai.wordpress.com/2018/06/10/conference-in-singapore-vietnam-appeasement-restorative-justice-laws-of-history-and-neutrinos/">ICA workshops</a> 2018 &amp; 2019 in Singapore and Birmingham. <span style="color: #993366;">Remarks are most welcome.</span></p>
<p><strong>Update:</strong> starting today, <a href="https://sites.google.com/view/acow2020/home">the algebraic combinatorics online workshop.</a>  Here is the schedule <a href="https://drive.google.com/file/d/17us1_lpZk1XLdQ1IewxS9cp-fBPA6TKw/view">for the first week</a>, and <a href="https://drive.google.com/file/d/1wJlUyLkdQ1Uvz5OxZGCNuE2SWyRk5_te/view">for the second week</a>.</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/mt.jpg"><img src="https://gilkalai.files.wordpress.com/2020/04/mt.jpg?w=640" alt="" class="alignnone size-full wp-image-19748" /></a></p>
<p><span style="color: #ff0000;">Matching theory by Lovasz and Plummer is probably one of the best mathematics books ever written. </span></p>
<h2>Bipartite Perfect Matching as a Real Polynomial</h2>
<p><a href="https://arxiv.org/abs/2001.07642">Bipartite Perfect Matching as a Real Polynomial,</a> by Gal Beniamini and Noam Nisan</p>
<p><strong>Abstract:</strong> We obtain a description of the Bipartite Perfect Matching decision problem as a multilinear polynomial over the Reals. We show that it has full degree and <img src="https://s0.wp.com/latex.php?latex=%281-o_n%281%29%29%5Ccdot+2%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1-o_n(1))\cdot 2^{n^2}" class="latex" title="(1-o_n(1))\cdot 2^{n^2}" />  monomials with non-zero coefficients. In contrast, we show that in the dual representation (switching the roles of 0 and 1) the number of monomials is only exponential in <img src="https://s0.wp.com/latex.php?latex=%5CTheta%28n+%5Clog+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Theta(n \log n)" class="latex" title="\Theta(n \log n)" />  Our proof relies heavily on the fact that the lattice of graphs which are “matching-covered” is Eulerian.</p>
<p>And here is how the paper starts</p>
<p>Every Boolean function <img src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f:\{0,1\}^n\to\{0,1\}" class="latex" title="f:\{0,1\}^n\to\{0,1\}" /> can be represented in a unique way as a Real multilinear polynomial. This representation and related ones (e.g. using the {1,−1} basis rather than {0,1}– the “Fourier transform” over the hypercube, or approximation variants) have many applications for various complexity and algorithmic purposes. See, e.g., [O’D14] for a recent textbook. In this paper we derive the representation of the bipartite-perfect-matching decision problem as a Real polynomial.</p>
<p><strong>Deﬁnition.</strong> The Boolean function <img src="https://s0.wp.com/latex.php?latex=BPM_n%28x_%7B1%2C1%7D%2C%5Cdots%2Cx_%7Bn%2Cn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="BPM_n(x_{1,1},\dots,x_{n,n})" class="latex" title="BPM_n(x_{1,1},\dots,x_{n,n})" /> is deﬁned to be 1 if and only if the bipartite graph whose edges are<img src="https://s0.wp.com/latex.php?latex=%5C%7B%28i%2Cj%29%3Ax_%7Bi%2Cj%7D%3D1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\{(i,j):x_{i,j}=1\}" class="latex" title="\{(i,j):x_{i,j}=1\}" /> has a perfect matching, and 0 otherwise.</p>
<p>And here are the two main theorems regarding this polynomial and the polynomial for the dual representation:</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/bn2.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/04/bn2.png?w=640&amp;h=138" class="alignnone size-full wp-image-19769" height="138" /></a></p>
<p>(For the second theorem you need the notion of totally ordered bipartite graphs.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/bn3.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/04/bn3.png?w=640&amp;h=143" class="alignnone size-full wp-image-19770" height="143" /></a></p>
<p>And here is a nice picture!</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/bn4.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/04/bn4.png?w=640&amp;h=421" class="alignnone size-full wp-image-19771" height="421" /></a></p>
<p>A very interesting open problem is:</p>
<p><strong>Problem:</strong> Can the Beniamini-Nisan results be extended to general (non-bipartite) graphs</p>
<p>This reminds me of an old great problem:</p>
<p><strong>Problem:</strong> Does Lovasz’ randomized algorithm for matching extend to the non-bipartite case?</p>
<p>For both problems methods of algebraic combinatorics may be helpful.</p>
<h2>An Extension by Vijay Vazirani and Thorben Tröbst</h2>
<p class="title mathjax"><a href="https://arxiv.org/abs/2003.08917">A Real Polynomial for Bipartite Graph Minimum Weight Perfect Matchings,</a> Thorben Tröbst, Vijay V. Vazirani</p>
<p><strong>Abstract:</strong></p>
<p>In a recent paper, Beniamini and Nisan gave a closed-form formula for the unique multilinear polynomial for the Boolean function determining whether a given bipartite graph <img src="https://s0.wp.com/latex.php?latex=G+%5Csubset+K_%7Bn%2Cn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \subset K_{n,n}" class="latex" title="G \subset K_{n,n}" /> has a perfect matching, together with an efficient algorithm for computing the coefficients of the monomials of this polynomial. We give the following generalization: Given an arbitrary non-negative weight function <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-12"><span class="mrow" id="MathJax-Span-13"><span class="mi" id="MathJax-Span-14">w</span></span></span></span> on the edges of <img src="https://s0.wp.com/latex.php?latex=K_%7Bn%2Cn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="K_{n,n}" class="latex" title="K_{n,n}" />, consider its set of minimum weight perfect matchings. We give the real multilinear polynomial for the Boolean function which determines if a graph <img src="https://s0.wp.com/latex.php?latex=G+%5Csubset+K_%7Bn%2Cn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="G \subset K_{n,n}" class="latex" title="G \subset K_{n,n}" /> contains one of these minimum weight perfect matchings.</p>
<h3>Three more remarks about VVV</h3>
<p>Three more VVV remarks: in the Tel Aviv theory <del>fast</del> fest three months ago (it seems like ages ago) Vijay Vazirani gave a lecture about matching. Here is the link to <a href="https://youtu.be/DFGsIOVGOIs">Vijay’s lecture</a>, and to <a href="https://www.youtube.com/playlist?list=PLGRBwz8taWHgpFOqbKQLvm-eAaZz33zM7">all plenary lectures</a>.  At the end, I asked him how he explains that matching theory is such inexhaustable gold mine and Vijay mentioned the fact that a polynomial-time algorithm for the assignment problem (which is closely related to matching) was <a href="http://www.lix.polytechnique.fr/~ollivier/JACOBI/presentationlEngl.htm">already found by Jacobi in 1890</a>. (Unfortunately VJ’s inspiring answer was not recorded). A few years ago Vijay<a href="https://arxiv.org/abs/1210.4594"> published a simplified proof</a> of a fantastic famous result he first proved with Silvio Micaly 34 years earlier. And here is a most amazing story: a few years ago I went to the beach in Tel Aviv and I discovered Vijay swimming just next to me.  We were quite happy to see each other and Vijay told me a few things about matching, economics and biology. This sounds now like a truly surrealistic story, and perhaps we even shook hands.</p>
<h3></h3>
<p> </p>
<p> </p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/"><span class="datestr">at April 19, 2020 08:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16965">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/">Proof and Cake Envy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Our proofs can be big too</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/screen-shot-2020-04-18-at-12-47-07-pm/" rel="attachment wp-att-16968"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/04/screen-shot-2020-04-18-at-12.47.07-pm.png?w=300&amp;h=163" class="alignright size-medium wp-image-16968" height="163" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Mackenzie and Aziz]</font></td>
</tr>
</tbody>
</table>
<p>
Haris Aziz and Simon Mackenzie are computer scientists at UNSW and CMU respectively. Of course UNSW is the University of New South Wales and CMU is the Carnegie Mellon University. </p>
<p>
Today we will discuss cake cutting and more.</p>
<p>
Aziz and Mackenzie have solved an open problem concerning how to cut cakes. Their <a href="https://cacm.acm.org/magazines/2020/4/243651-a-bounded-and-envy-free-cake-cutting-algorithm/fulltext">paper</a> is in the April issue of the CACM: “A Bounded and Envy-Free Cake Cutting Algorithm.” </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/cake/" rel="attachment wp-att-16970"><img src="https://rjlipton.files.wordpress.com/2020/04/cake.jpg?w=600" alt="" class="aligncenter size-full wp-image-16970" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p></p><h2> Why Cake Cutting? </h2><p></p>
<p></p><p>
Before we talk about cake cutting from a theory viewpoint let’s take a look at why it is interesting. The real answer probably is it is a beautiful math problem. It is easy to state without lots of background. It is simple like Fermat’s Last Theorem: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bn%7D+%2B+y%5E%7Bn%7D+%3D+z%5E%7Bn%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x^{n} + y^{n} = z^{n} " class="latex" title="\displaystyle  x^{n} + y^{n} = z^{n} " /></p>
<p>has no solutions over the integers with <img src="https://s0.wp.com/latex.php?latex=%7Bxyz+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{xyz \neq 0}" class="latex" title="{xyz \neq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=n%3E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n&gt;2" class="latex" title="n&gt;2" />. Cake cutting is hard. We like problems that strike back: problems that are not easy to solve. This is a strange view. In real life we might prefer problems that we can easily solve. But not in math. We like problems that are not trivial. The cake cutting problem is hard, so we like it. </p>
<p>
</p><p></p><h2> Cutting Cakes </h2><p></p>
<p></p><p>
We are theorists so our cakes are one-dimensional line segments. The problem involves a finite set of agents, say Alice, Bob, and so on. They want to divide the cake, the line segment, into a finite number of pieces. The pieces are then allocated to the agents. The goal is to get a fair division of the cake. </p>
<p>
The notion of “fair” is what makes the problem interesting. Often agents will not have the same tastes: Some like icing more than others, some like the end pieces, while others do not. The fact that the agents assign different values to a piece of the cake is what makes the problem challenging. </p>
<p>
If there are two agents the problem has long been solved. Let Bob divide the cake into two pieces, so that he is happy to get either of these pieces. Then have Alice chose which piece she wants. It is easy to see that both Bob and Alice are happy. Both are <i>envy-free</i>: neither would exchange their piece for the others piece. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/two/" rel="attachment wp-att-16971"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/04/two.png?w=300&amp;h=144" class="aligncenter size-medium wp-image-16971" height="144" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
There is a large literature on the <a href="https://en.wikipedia.org/wiki/Fair_cake-cutting">cake-cutting</a> problem. Its creator, Hugo Steinhaus, noted:</p>
<blockquote><p><b> </b> <em> Interesting mathematical problems arise if we are to determine the minimal numbers of “cuts” necessary for fair division. </em>
</p></blockquote>
<p></p><p>
We have taken the quote from an <a href="https://medium.com/cantors-paradise/envy-free-cake-cutting-procedures-de3cf13c5d3d">article</a> on <em>Medium</em> that neatly conveys details on various protocols. Some main results are: </p>
<ul>
<li>
The Selfridge-Conway discrete procedure produces an envy-free division for <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> people using at most <img src="https://s0.wp.com/latex.php?latex=%7B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{5}" class="latex" title="{5}" /> cuts. <p></p>
</li><li>
The Brams-Taylor-Zwicker moving knives procedure produces an envy-free division for <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> people using at most <img src="https://s0.wp.com/latex.php?latex=%7B11%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{11}" class="latex" title="{11}" /> cuts. <p></p>
</li><li>
Three different procedures produce an envy-free division for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> people. Both algorithms require a finite but unbounded number of cuts. That is to say, the number of cuts may depend on details of their preference functions. <p></p>
</li><li>
The procedure by Aziz and Mackenzie finds an envy-free division for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> people in a bounded number of cuts.
</li></ul>
<p>The last is the result in the CACM paper. Note, the number of cuts can be large: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7Bn%5E%7Bn%5E%7Bn%5E%7Bn%5E%7Bn%7D%7D%7D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  n^{n^{n^{n^{n^{n}}}}}. " class="latex" title="\displaystyle  n^{n^{n^{n^{n^{n}}}}}. " /></p>
<p>Even for <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> this is immense, galactic. This should be compared to the best lower bound that is order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{2}}" class="latex" title="{n^{2}}" />. This gap is even larger than the usual gaps we find in complexity theory. The P=NP question is only one exponential not five.</p>
<p>
This has started me thinking: what exactly is the relationship between this and <em>proof complexity</em>? The latter has well-established relationships to complexity-class questions. The link from proofs in various systems of <a href="https://en.wikipedia.org/wiki/Bounded_arithmetic">bounded arithmetic</a> goes through the heart of P=NP. See for instance these <a href="https://www.math.ucsd.edu/~sbuss/ResearchWeb/StPetersburg_BoundedArith_2016/talkAllSlides_Corrected.pdf">slides</a> by Sam Buss and <a href="https://www.math.ucsd.edu/~sbuss/ResearchWeb/Barbados95Notes/reporte.pdf">notes</a> that were scribed by Ken and others. What I am puzzled by is that in most cases the blowup is only one or two exponentials. The setting with cake-cutting is different, but how different? </p>
<p>
</p><p></p><h2> Easy Cases </h2><p></p>
<p></p><p>
The Aziz and Mackenzie algorithm takes a long time. It is a nontrivial result, but not one that applies in any practical case. It always takes way too long. The cake will be stale by the time the agents have agreed on their pieces. </p>
<p>
This raises a question, that also applies to many computational problems. Is there a way cut a cake faster on some interesting examples? We can explain this by the analogy to sorting. The fastest sorting algorithms run in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n \log n)}" class="latex" title="{O(n \log n)}" /> time where there are <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> objects. But what happens if the objects are already in sorted order? Or at least close to sorted order? The answer is it depends:</p>
<ol>
<li>
Some sorting algorithms always take the same time, independent of the input structure. <p></p>
</li><li>
There are other sorting algorithms that can take advantage of the nature of the input.
</li></ol>
<p>
That is some sorting algorithms can run say in linear time if the input is almost sorted. For the cake cutting problem we ask:</p>
<blockquote><p><b> </b> <em> <i>Is there a way to cut cakes that is envy-free when the agents have some property <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" />?</i> </em>
</p></blockquote>
<p></p><p>
We do not know the answer, but we think it is an interesting question. Here is an example. Suppose that the agents have the same measures. That is, they evaluate every piece of cake in the same way. </p>
<p>
If we <em>know</em> this—and if we continue our supposition above that Bob can cut with exact precision—then there is an easy answer: Have Bob do the cuts. Then all agents will be equally happy since they have the same measures. The question is, what if we do not know? I believe there should be some theorem like this:</p>
<blockquote><p><b>Theorem 1 (Conjecture)</b> <em> There is an envy-free algorithm that operates in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^{2})}" class="latex" title="{O(n^{2})}" /> time the algorithm so that either: </em></p><em>
<ol>
<li>
It yields an envy-free solution, or <p></p>
</li><li>
It determines that some agents have different measures.
</li></ol>
</em><p><em></em>
</p></blockquote>
<p></p><p>
In the second case the cake will be cut as before. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I originally planned on discussing size and complexity of proofs. This is driven by the complexity of the cake cutting algorithms. They tend to have lots of cases and are difficult to understand.<br />
<a href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/over/" rel="attachment wp-att-16973"><img src="https://rjlipton.files.wordpress.com/2020/04/over.png?w=600" alt="" class="aligncenter size-full wp-image-16973" /></a><br />
They are also difficult to find—this is why cake cutting questions have been resistance to progress. More on this in the future. </p>
<p>[Edit <img src="https://s0.wp.com/latex.php?latex=n%3E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n&gt;2" class="latex" title="n&gt;2" /> in Fermat example]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/04/18/proof-and-cake-envy/"><span class="datestr">at April 18, 2020 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/052">TR20-052 |  On One-way Functions and Kolmogorov Complexity | 

	Rafael Pass, 

	Yanyi Liu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove the equivalence of two fundamental problems in the theory of computation:

- Existence of one-way functions: the existence of one-way functions (which in turn are equivalent to pseudorandom generators, pseudorandom functions, private-key encryption schemes, digital signatures, commitment schemes, and more).
  
- Mild average-case hardness of $K^{poly}$-complexity: the existence of polynomials $t,p$ such that no PPT algorithm can determine the $t$-time bounded Kolmogorov Complexity, $K^t$, for more than a $1-\frac{1}{p(n)}$ fraction of $n$-bit strings.

In doing so, we present the first natural, and well-studied, computational problem characterizing ``non-trivial'' complexity-based Cryptography: ``Non-trivial'' complexity-based Cryptography is possible iff $K^{poly}$ is mildly hard-on average.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/052"><span class="datestr">at April 18, 2020 09:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/051">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/051">TR20-051 |  Is it Easier to Prove Theorems that are Guaranteed to be True? | 

	Rafael Pass, 

	Muthuramakrishnan Venkitasubramaniam</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the following two fundamental open problems in complexity theory: (a) Does a hard-on-average language in $\NP$ imply the existence of one-way functions?, or (b) Does a hard-on-average language in NP imply a hard-on-average problem in TFNP (i.e., the class of total NP search problem)? Our main result is that the answer to (at least) one of these questions is yes.
 
Both one-way functions and problems in TFNP can be interpreted as promise-true distributional NP search problems---namely, distributional search problems where the sampler only samples true statements. As a direct corollary of the above result, we thus get that the existence of a hard-on-average distributional NP search problem implies a hard-on-average promise-true distributional NP search problem. In other words,” It is no easier to find witnesses (a.k.a. proofs) for efficiently-sampled statements (theorems) that are guaranteed to be true.”?

This result follows from a more general study of interactive puzzles---a generalization of average-case hardness in NP—and in particular, a novel round-collapse theorem for computationally-sound protocols, analogous to Babai-Moran's celebrated round-collapse theorem for information-theoretically sound protocols. As another consequence of this treatment, we show that the existence of O(1)-round public-coin non-trivial arguments (i.e., argument systems that are not proofs) imply the existence of a hard-on-average problem in NP/poly.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/051"><span class="datestr">at April 18, 2020 09:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/050">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/050">TR20-050 |  Unexpected Hardness Results for Kolmogorov Complexity Under Uniform Reductions | 

	Shuichi Hirahara</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Hardness of computing the Kolmogorov complexity of a given string is closely tied to a security proof of hitting set generators, and thus understanding hardness of Kolmogorov complexity is one of the central questions in complexity theory.  In this paper, we develop new proof techniques for showing hardness of computing Kolmogorov complexity under *surprisingly efficient reductions*, which were previously conjectured to be impossible.  It is known that the set $R_K$ of Kolmogorov-random strings is PSPACE-hard under polynomial-time Turing reductions, i.e., $PSPACE \subset P^{R_K}$, and that $NEXP \subset NP^{R_K}$, which was conjectured to be tight by Allender (CiE 2012).  We prove that $EXP^{NP} \subset P^{R_K}$, which simultaneously improves these hardness results and refutes the conjecture of Allender under the plausible assumption that $EXP^{NP} \neq NEXP$.  At the core of our results is a new security proof of a pseudorandom generator via a black-box uniform reduction, which overcomes an impossibility result of Gutfreund and Vadhan (RANDOM/APPROX 2008).

Our proof techniques have further consequences, including:

1.  Applying our proof techniques to the case of resource-bounded Kolmogorov complexity, we obtain NP-hardness of the problem $MINcKT^{SAT}$ of computing conditional polynomial-time-bounded SAT-oracle Kolmogorov complexity under polynomial-time deterministic reductions.  In contrast, the Minimum SAT-Oracle Circuit Size Problem, which is a version of sublinear-time-bounded Kolmogorov complexity, cannot be NP-hard under polynomial-time deterministic reductions without resolving $EXP \neq ZPP$.  Our hardness result is the first result that overcomes the non-NP-hardness results of MCSP.  We also prove DistNP-hardness of $MINKT^{SAT}$, which is a partial converse of the approach of Hirahara (FOCS 2018) for proving the equivalence between worst-case and average-case complexity of NP.

2.  We prove $S_2^p$-hardness of Kolmogorov complexity under quasi-polynomial-time *nonadaptive* reductions.  This is the first result that overcomes a P/poly barrier result of Allender, Buhrman, Friedman, and Loff (MFCS 2012).

We also establish a firm link between non-trivial satisfiability algorithms and immunity of random strings, and obtain the following unconditional lower bounds.

1.  It has been a long-standing open question whether the set of subexponential-time-bounded Kolmogorov-random strings is decidable in P.  We resolve this open question, by showing that the set of super-polynomial-time-bounded Kolmogorov-random strings is P-immune, which is a much stronger lower bound than an average-case lower bound.

2.  The set of Levin's Kolmogorov-random strings is (P-uniform ACC)-immune.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/050"><span class="datestr">at April 18, 2020 07:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/049">TR20-049 |  Automating Cutting Planes is NP-Hard | 

	Mika Göös, 

	Sajin Koroth, 

	Ian Mertz, 

	Toniann Pitassi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that Cutting Planes (CP) proofs are hard to find: Given an unsatisfiable formula $F$,

(1) it is NP-hard to find a CP refutation of $F$ in time polynomial in the length of the shortest such refutation; and

(2) unless Gap-Hitting-Set admits a nontrivial algorithm, one cannot find a tree-like CP refutation of $F$ in time polynomial in the length of the shortest such refutation.

The first result extends the recent breakthrough of Atserias and Muller (FOCS 2019) that established an analogous result for Resolution. Our proofs rely on two new lifting theorems: (1) Dag-like lifting for gadgets with many output bits. (2) Tree-like lifting that simulates an $r$-round protocol with gadgets of query complexity $O(\log r)$ independent of input length.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/049"><span class="datestr">at April 16, 2020 10:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/048">TR20-048 |  Improved lifting theorems via robust sunflowers | 

	Jiapeng Zhang, 

	Shachar Lovett, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Lifting theorems are a generic way to lift lower bounds in query complexity to lower bounds in communication complexity, with applications in diverse areas, such as combinatorial optimization, proof complexity, game theory. Lifting theorems rely on a gadget, where smaller gadgets give stronger lower bounds. However, existing proof techniques are known to require somewhat large gadgets.

We focus on one of the most widely used gadgets, the index gadget. For this gadget, existing lifting techniques are known to require at least a quadratic gadget size. We develop a new approach to prove lifting theorems for the indexing gadget, based on a novel connection to the recently developed robust sunflower lemmas. We show that this allows to reduce the gadget size to linear. We conjecture that it can be further improved to poly logarithmic, similar to the known bounds for the corresponding robust sunflower lemmas.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/048"><span class="datestr">at April 16, 2020 05:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/047">TR20-047 |  Explicit Uniquely Decodable Codes for Space Bounded Channels That Achieve List-Decoding Capacity | 

	Jad Silbak, 

	Ronen Shaltiel</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider codes for space bounded channels. This is a model for communication under noise that was introduced by Guruswami and Smith (J. ACM 2016) and lies between the Shannon (random) and Hamming (adversarial) models. In this model, a channel is a space bounded procedure that reads the codeword in one pass, and modifies at most a $p$ fraction of the bits of the codeword.

Explicit uniquely decodable codes for space bounded channels: Our main result is that for every $0 \le p \le \frac{1}{4}$, there exists a constant $\delta\ge 0$ and a \emph{uniquely decodable} code that is \emph{explicit} (meaning that encoding and decoding are in poly-time) and has rate $1-H(p)$ for channels with space $n^{\delta}$.

This improves upon previous explicit codes by Guruswami and Smith, and Kopparty, Shaltiel and Silbak (FOCS 2019). Specifically, we obtain the same space and rate as earlier works, even though prior work gave only list-decodable codes (rather than uniquely decodable codes).

Complete characterization of the capacity of space bounded channels: Together with a result by Guruswami and Smith showing the impossibility of unique decoding for $p \ge \frac{1}{4}$, our techniques also give a complete characterization of the capacity $R(p)$ of space $n^{1-o(1)}$ channels, specifically: $R(p)=1-H(p)$ for $0 \le p \le 1/4$ and $R(p)=0$ for $p \ge 1/4$.

In particular, $R(\cdot)$ is not continuous at $p=1/4$. This capacity is strictly larger than the capacity of Hamming channels for every $0 \le p \le \frac{1}{4}$, and matches the capacity of list decoding, and binary symmetric channels in this range.


Our results are incomparable to recent work on casual channels. Casual channels are stronger channels in which the channel reads the codeword in one pass, but there is no space restriction. The best known codes for casual channels, due to Chen, Jaggi and Langberg (STOC 2015), are shown to exist by the probabilistic method, and no explicit codes are known. Furthermore, our results imply that for $p\ge p_0 \approx 0.0804$, there is a separation between the capacities of space bounded channels and casual channels, and the capacity of the former is strictly larger than that of the latter.


Our approach builds on previous explicit list decodable codes for space bounded channels. We introduce and study a notion of ``\emph{evasivenss}'' of codes, which is concerned with whether a decoding algorithm rejects a word that is obtained when a channel induces few errors to a \emph{uniformly chosen} string. We use evasiveness (as well as several additional new ideas related to coding theory and pseudorandomness) to identify the ``correct'' message in the list. Loosely speaking, this is achieved by arguing that on ``incorrect messages'' the decoding algorithm cannot distinguish the codeword from a uniform string.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/047"><span class="datestr">at April 16, 2020 10:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/046">TR20-046 |  A Robust Version of Heged\H{u}s&amp;#39;s Lemma, with Applications | 

	Srikanth Srinivasan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Heged\H{u}s's lemma is the following combinatorial statement regarding polynomials over finite fields. Over a field $\mathbb{F}$ of characteristic $p &gt; 0$ and for $q$ a  power of $p$, the lemma says that any multilinear polynomial $P\in \mathbb{F}[x_1,\ldots,x_n]$ of degree less than $q$ that vanishes at all points in $\{0,1\}^n$ of Hamming weight $k\in [q,n-q]$ must also vanish at all points in $\{0,1\}^n$ of weight $k + q$. This lemma was used by Heged\H{u}s (2009) to give a solution to \emph{Galvin's problem}, an extremal problem about set systems; by Alon, Kumar and Volk (2018) to improve the best-known multilinear circuit lower bounds; and by Hrube\v{s}, Ramamoorthy, Rao and Yehudayoff (2019) to prove optimal lower bounds against depth-$2$ threshold circuits for computing some symmetric functions. 
		
		In this paper, we formulate a robust version of Heged\H{u}s's lemma. Informally, this version says that if a polynomial of degree $o(q)$ vanishes at most points of weight $k$, then it vanishes at many points of weight $k+q$. We prove this lemma and give the following three different applications.
		
		1. Degree lower bounds for the coin problem: The \emph{$\delta$-Coin Problem} is the problem of distinguishing between a coin that is heads with probability $((1/2) + \delta)$ and a coin that is heads with probability $1/2$. We show that over a field of positive (fixed) characteristic, any polynomial that solves the $\delta$-coin problem with error $\varepsilon$ must have degree $\Omega(\frac{1}{\delta}\log(1/\varepsilon)),$ which is tight up to constant factors.
		
		2. Probabilistic degree lower bounds: The \emph{Probabilistic degree} of a Boolean function is the minimum $d$ such that there is a random polynomial of degree $d$ that agrees with the function at each point with high probability. We give tight lower bounds on the probabilistic degree of \emph{every} symmetric Boolean function over positive (fixed) characteristic. As far as we know, this was not known even for some very simple functions such as unweighted Exact Threshold functions, and constant error.
		
		3. A robust version of the combinatorial result of Heged\H{u}s (2009) mentioned above.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/046"><span class="datestr">at April 15, 2020 04:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4740">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4740">The quantum computer that knows all</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This is my first post in more than a month that’s totally unrelated to the covid crisis.  Or rather, it’s related only insofar as it’s about a Hulu miniseries, the sort of thing that many of us have more occasion to watch while holed up at home.</p>



<p>Three weeks ago, a journalist named Ben Lindbergh—who’d previously asked me to <a href="https://www.scottaaronson.com/blog/?p=4184">comment on the scientific accuracy of <em>Avengers: Endgame</em></a>—asked me the same question about the miniseries <a href="https://en.wikipedia.org/wiki/Devs_(miniseries)">Devs</a>, which I hadn’t previously heard of.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">[Warning: Spoilers follow]</span></strong></p>



<p>‘Devs,’ I learned, is a spooky sci-fi action thriller about a secretive Silicon Valley company that builds a quantum computer that can perfectly reconstruct the past, down to what Jesus looked like on the cross, and can also (at least up to a point) predict the future.</p>



<p>And I was supposed, not only to endure such a show, but to comment on the <em>accuracy</em> of its invocations of quantum computing?  This didn’t sound promising.</p>



<p>But, y’know, I was at home quarantined.  So I agreed to watch the first episode.  Which quickly turned into the second, third, fourth, fifth, sixth, and seventh episodes (the eighth and final one isn’t out yet).</p>



<p>It turns out that ‘Devs’ isn’t too bad, <em>except</em> that it’s not particularly about quantum computers.  The latter is simply a buzzword chosen by the writers for a plot concept that would’ve been entirely familiar to the ancient Greeks, who called it the Delphic Oracle.  You know, the mysterious entity that prophesies your fate, so then you try to escape the prophecy, but your very evasive maneuvers make the prophecy come true?  Picture that, except with qubits—and for some reason, in a gleaming golden laboratory that has components that float in midair.</p>



<figure class="wp-block-image"><img src="https://cdn1-www.comingsoon.net/assets/uploads/2020/01/Screen-Shot-2020-01-09-at-2.52.52-PM.png" alt="Devs Trailer Reveals New Look at FX-Hulu's Upcoming Limited Series" />If you’re never visited a real quantum computing lab: they’re messier and a lot less golden.</figure>



<p>At this point, I’ll just link you to Ben Lindbergh’s article about the show: <a href="https://www.theringer.com/tv/2020/4/10/21216149/devs-hulu-quantum-physics-philosophy-alex-garland">Making Sense of the Science and Philosophy of ‘Devs.’</a>  His long and excellent piece quotes me extensively enough that I see no need <em>also</em> to analyze the show in this blog post.  (It also quotes several academic philosophers.)</p>



<p>Instead, I’ll just share a few tidbits that Ben left out, but that might be amusing to quantum computing fans.</p>



<ul><li>The first episode opens with a conversation between two characters about how even “elliptical curve” cryptography is insecure against attack by quantum computers.  So I immediately knew <em>both</em> that the writers had one or more consultants who actually knew something about QC, and also that those consultants were not as heavily involved as they could’ve been.</li></ul>



<ul><li>Similarly: in a later scene, some employees at the secretive company hold what appears to be a reading group about Shor’s algorithm.  They talk about waves that interfere and cancel each other out, which is great, but beyond that their discussion sounded to me like nonsense.  In particular, their idea seemed to be that the waves would reinforce at the prime factors p and q themselves, rather than at inverse multiples of the period of a periodic function that only indirectly encodes the factoring problem.  (What do you say: should we let this one slide?)</li></ul>



<ul><li>“How many qubits does this thing have?” “A number that there would be no point in describing as a number.”  ROFL</li></ul>



<ul><li>In the show, a crucial break comes when the employees abandon a prediction algorithm based on the deBroglie-Bohm pilot wave interpretation, and substitute one based on Everett’s many-worlds interpretation.  Which I could actually <em>almost</em> believe, except that the many-worlds interpretation seems to contradict the entire premise of the rest of the show?</li></ul>



<ul><li>A new employee, after he sees the code of the superpowerful quantum computer for the first time, is so disoriented and overwhelmed that he runs and vomits into a toilet.  I, too, have had that reaction to the claims of certain quantum computing companies, although in some sense for the opposite reason.</li></ul>



<p>Anyway, none of the above addresses the show’s central conceit: namely, that the <a href="https://en.wikipedia.org/wiki/Laplace%27s_demon">Laplace demon</a> can be made real, the past and future rendered fully knowable (with at most occasional breaks and exceptions) by a machine that’s feasible to build.  This conceit is fascinating to explore, but also <em>false</em>.</p>



<p>In the past, if you’d asked me to justify its falsity, I would’ve talked about chaos, and quantum mechanics, and the unknowability of the fine details of the universe’s state; I might’ve even pointed you to my <a href="https://arxiv.org/abs/1306.0159">Ghost in the Quantum Turing Machine</a> essay.  I also would’ve mentioned the severe conceptual difficulties in forcing Nature to find a fixed-point of a universe where you get to see your own future and act on that information (these difficulties are just a variant of the famous <a href="https://en.wikipedia.org/wiki/Grandfather_paradox">Grandfather Paradox</a>).</p>



<p>But it occurs to me that, just as the coronavirus has now made plain the nature of exponential growth, even to the world’s least abstract-minded person, so too it’s made plain the universe’s unpredictability.  Let’s put it this way: do you find it plausible that the quantum computer from ‘Devs,’ had you booted it up six months ago, would’ve known the exact state of every nucleotide in every virus in every bat in Wuhan?  No?  Then it wouldn’t have known our future.</p>



<p>And I see now that I’ve violated my promise that this post would have nothing to do with covid.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4740"><span class="datestr">at April 15, 2020 12:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16931">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/04/14/john-horton-conway-1937-2020/">John Horton Conway 1937–2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>An appreciation</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/04/johnhortonconway1987.jpg"><img width="192" alt="" src="https://rjlipton.files.wordpress.com/2020/04/johnhortonconway1987.jpg?w=192&amp;h=240" class="alignright wp-image-16933" height="240" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Names for large numbers <a href="https://sites.google.com/site/largenumbers/home/2-4/6">source</a></font></td>
</tr>
</tbody>
</table>
<p>
John Horton Conway just passed away from complications of COVID-19. We are all saddened by this news, and we hope you all are doing your best to stay safe and help others cope.</p>
<p>
Today Ken and I thought we would reflect on some of Conway’s many contributions and emphasize three in which we see connections to computational complexity. </p>
<p>
Conway was a Fellow of the Royal Society, and was the first recipient of the London Mathematical Society’s Pólya Prize. His nomination to the Royal Society reads:</p>
<blockquote><p><b> </b> <em> A versatile mathematician who combines a deep combinatorial insight with algebraic virtuosity, particularly in the construction and manipulation of “off-beat” algebraic structures which illuminate a wide variety of problems in completely unexpected ways. He has made distinguished contributions to the theory of finite groups, to the theory of knots, to mathematical logic (both set theory and automata theory) and to the theory of games (as also to its practice). </em>
</p></blockquote>
<p>
</p><p></p><h2> A Life Force </h2><p></p>
<p></p><p>
Conway may be most noted for his game of <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Life</a>. This is a two-dimensional cellular automaton. Conway invented it in 1970, which he rounded up from 1969. The game—and Martin Gardner’s 1970 column on it in <em>Scientific American</em>—made him famous in the wider community. The website <a href="https://www.conwaylife.com/">conwaylife.com</a> and <a href="https://catagolue.appspot.com/home">several</a> <a href="https://tebs-game-of-life.com/">others</a> link to more information than we could digest in a lifetime.</p>
<p>
We want to emphasize instead how Conway was a special force in mathematics. He applied an almost elementary approach to deep hard problems of mathematics. This is a unique combination. There have been mathematicians who worked on deep problems and also on recreational math, but few who established integral flows across the boundary between them. Conway infused both with magic in a way conveyed by an iconic photograph of his Princeton office in 1993:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/04/conwayoffice.jpg"><img width="450" alt="" src="https://rjlipton.files.wordpress.com/2020/04/conwayoffice.jpg?w=450&amp;h=270" class="aligncenter wp-image-16934" height="270" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><i>Guardian<i> via Dith Pran, <i>NY Times</i> <a href="https://www.theguardian.com/science/2015/jul/23/john-horton-conway-the-most-charismatic-mathematician-in-the-world">source</a> </i></i></font>
</td>
</tr>
</tbody></table>
<p>
What Ken remembers is how accessible Conway was <em>outside</em> his office. “I know I met him at least once while I was an undergraduate at Princeton in 1979 or 1980, though this is overlaid by a memory of finding just him and a few others in the Fine Hall tea room when I was there for my tenth reunion in 1991. My most evocative memory is when Conway gave an evening talk to the undergraduate mathematics club at Oxford when I was there sometime after 1981. It was relatively sparsely attended, perhaps because it was literally a dark and stormy winter night. But after his lecture we all got to huddle around him for another hour in the tea room as he regaled us with stories and mathematical problems.” </p>
<p>
We also remember that Conway was one of Andrew Wiles’s main confidants during the months before Wiles announced his proof of Fermat’s Last Theorem in June 1993. Here is a <a href="https://www.pbs.org/wgbh/nova/transcripts/2414proof.html">transcript</a> of a PBS Nova documentary on the proof in which Conway appears prominently. Ken has picked out two of Conway’s other contributions that we feel may have untapped use for research in complexity theory.</p>
<p>
</p><p></p><h2> Conway’s Numbers </h2><p></p>
<p></p><p>
One of this blog’s “invariants” is first-name last-name style, thus “Godfrey Hardy” not “G.H. Hardy.” But we make an exception in Conway’s case. Partly this owes to how his initials were amplified by Donald Knuth in his novella <em>Surreal Numbers</em>:</p>
<blockquote><p><b> </b> <em> In the beginning, everything was void, and J.H.W.H. Conway began to create numbers. </em>
</p></blockquote>
<p></p><p>
Besides the void (that is, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cemptyset%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\emptyset}" class="latex" title="{\emptyset}" />), the creation uses the idea of a <em>left set</em> <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> and a <em>right set</em> <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />. Every number has the form <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+L%7E%7C%7ER+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle L~|~R \rangle}" class="latex" title="{\langle L~|~R \rangle}" />. The initial number is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cemptyset+%7E%7C%7E+%5Cemptyset%5Crangle+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle \emptyset ~|~ \emptyset\rangle = 0. " class="latex" title="\displaystyle  \langle \emptyset ~|~ \emptyset\rangle = 0. " /></p>
<p>
Once a number is generated, it can be in the <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> of other numbers. Thus, next come </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Clangle+0+%7E%7C%7E+%5Cemptyset+%5Crangle+%26%3D%26+1%5C%5C+%5Clangle+%5Cemptyset+%7E%7C%7E+0+%5Crangle+%26%3D%26+-1.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  \langle 0 ~|~ \emptyset \rangle &amp;=&amp; 1\\ \langle \emptyset ~|~ 0 \rangle &amp;=&amp; -1. \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  \langle 0 ~|~ \emptyset \rangle &amp;=&amp; 1\\ \langle \emptyset ~|~ 0 \rangle &amp;=&amp; -1. \end{array} " /></p>
<p>
You might think of <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0+%7E%7C%7E+0+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0 ~|~ 0 \rangle}" class="latex" title="{\langle 0 ~|~ 0 \rangle}" /> next, but it violates the invariant </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5Cforall+%5Cell+%5Cin+L%29%28%5Cforall+r+%5Cin+R%29%5Cneg+%28r+%5Cleq+%5Cell%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (\forall \ell \in L)(\forall r \in R)\neg (r \leq \ell). " class="latex" title="\displaystyle  (\forall \ell \in L)(\forall r \in R)\neg (r \leq \ell). " /></p>
<p>which defines an <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+L%7E%7C%7ER+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle L~|~R \rangle}" class="latex" title="{\langle L~|~R \rangle}" /> <em>form</em> to be a <em>number</em>. </p>
<p>
The relation <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq}" class="latex" title="{\leq}" /> is inductively defined for <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%5Clangle+L_a+%7E%7C%7E+R_a+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a = \langle L_a ~|~ R_a \rangle}" class="latex" title="{a = \langle L_a ~|~ R_a \rangle}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb+%3D+%5Clangle+L_b+%7E%7C%7E+R_b+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b = \langle L_b ~|~ R_b \rangle}" class="latex" title="{b = \langle L_b ~|~ R_b \rangle}" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a+%5Cleq+b+%5Cquad%5Cequiv%5Cquad+%28%5Cforall+%5Cell_a+%5Cin+L_a%29%28%5Cforall+r_b+%5Cin+R_b%29%5Cneg%28b+%5Cleq+%5Cell_a+%5C%3B%5Clor%5C%3B+r_b+%5Cleq+a%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a \leq b \quad\equiv\quad (\forall \ell_a \in L_a)(\forall r_b \in R_b)\neg(b \leq \ell_a \;\lor\; r_b \leq a). " class="latex" title="\displaystyle  a \leq b \quad\equiv\quad (\forall \ell_a \in L_a)(\forall r_b \in R_b)\neg(b \leq \ell_a \;\lor\; r_b \leq a). " /></p>
<p>
That is, no member of the left-set of <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> “bumps” <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> (in the sense of rowing races) and <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> does not bump any member of the right-set of <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />.  Note that <img src="https://s0.wp.com/latex.php?latex=%7BR_a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R_a}" class="latex" title="{R_a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BL_b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L_b}" class="latex" title="{L_b}" /> are not involved—they already behave correctly owing to the invariant. The numbers <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b}" class="latex" title="{a,b}" /> are equal if <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Cleq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a \leq b}" class="latex" title="{a \leq b}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cleq+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b \leq a}" class="latex" title="{b \leq a}" /> both hold. The rule for addition is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a+%2B+b+%3D+%5Clangle+%28L_a+%5Cboxplus+b%29+%5Ccup+%28a+%5Cboxplus+L_b%29+%7E%7C%7E+%28a+%5Cboxplus+R_b%29+%5Ccup+%28R_a+%5Cboxplus+b%29+%5Crangle%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a + b = \langle (L_a \boxplus b) \cup (a \boxplus L_b) ~|~ (a \boxplus R_b) \cup (R_a \boxplus b) \rangle, " class="latex" title="\displaystyle  a + b = \langle (L_a \boxplus b) \cup (a \boxplus L_b) ~|~ (a \boxplus R_b) \cup (R_a \boxplus b) \rangle, " /></p>
<p>
where <img src="https://s0.wp.com/latex.php?latex=%7BL_a+%5Cboxplus+b+%3D+%5C%7B%5Cell_a+%2B+b%3A+%5Cell_a+%5Cin+L_a%5C%7D+%3D+b+%5Cboxplus+L_a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L_a \boxplus b = \{\ell_a + b: \ell_a \in L_a\} = b \boxplus L_a}" class="latex" title="{L_a \boxplus b = \{\ell_a + b: \ell_a \in L_a\} = b \boxplus L_a}" /> and so on. The logical rule <img src="https://s0.wp.com/latex.php?latex=%7B%5Cemptyset+%5Cboxplus+a+%3D+%5Cemptyset%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\emptyset \boxplus a = \emptyset}" class="latex" title="{\emptyset \boxplus a = \emptyset}" /> for any <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> makes the definition of addition well-founded. This yields the numerical fact </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%2B+0+%3D+%5Clangle+%28%5Cemptyset+%5Cboxplus+0%29+%5Ccup+%280+%5Cboxplus+%5Cemptyset%29+%7E%7C%7E+%28%5Cemptyset+%5Cboxplus+0%29+%5Ccup+%280+%5Cboxplus+%5Cemptyset%29+%5Crangle+%3D+%5Clangle%5Cemptyset+%7E%7C%7E+%5Cemptyset%5Crangle+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  0 + 0 = \langle (\emptyset \boxplus 0) \cup (0 \boxplus \emptyset) ~|~ (\emptyset \boxplus 0) \cup (0 \boxplus \emptyset) \rangle = \langle\emptyset ~|~ \emptyset\rangle = 0. " class="latex" title="\displaystyle  0 + 0 = \langle (\emptyset \boxplus 0) \cup (0 \boxplus \emptyset) ~|~ (\emptyset \boxplus 0) \cup (0 \boxplus \emptyset) \rangle = \langle\emptyset ~|~ \emptyset\rangle = 0. " /></p>
<p>
It is immediate that <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" /> is commutative. There is also a rule for multiplication but addition gives us enough to talk about here.</p>
<p>
</p><p></p><h2> Redundancy and Simplicity </h2><p></p>
<p></p><p>
It is straightforward to compute that <img src="https://s0.wp.com/latex.php?latex=%7B0+%2B+1+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 + 1 = 1}" class="latex" title="{0 + 1 = 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B-1+%2B+0+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-1 + 0 = -1}" class="latex" title="{-1 + 0 = -1}" />. Now consider: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++-1+%2B+1+%3D+%5Clangle+%28%5Cemptyset+%5Cboxplus+1%29+%5Ccup+%28-1+%5Cboxplus+%5C%7B0%5C%7D%29+%7E%7C%7E+%28-1+%5Cboxplus+%5Cemptyset+%29+%5Ccup+%28%5C%7B0%5C%7D+%5Cboxplus+1%29%5Crangle+%3D+%5Clangle+-1+%7E%7C%7E+1%5Crangle.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  -1 + 1 = \langle (\emptyset \boxplus 1) \cup (-1 \boxplus \{0\}) ~|~ (-1 \boxplus \emptyset ) \cup (\{0\} \boxplus 1)\rangle = \langle -1 ~|~ 1\rangle. " class="latex" title="\displaystyle  -1 + 1 = \langle (\emptyset \boxplus 1) \cup (-1 \boxplus \{0\}) ~|~ (-1 \boxplus \emptyset ) \cup (\{0\} \boxplus 1)\rangle = \langle -1 ~|~ 1\rangle. " /></p>
<p>This is a legal number. You can check that the relations <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+-1+%7E%7C%7E+1%5Crangle+%5Cleq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle -1 ~|~ 1\rangle \leq 0}" class="latex" title="{\langle -1 ~|~ 1\rangle \leq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Clangle+-1+%7E%7C%7E+1%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 \leq \langle -1 ~|~ 1\rangle}" class="latex" title="{0 \leq \langle -1 ~|~ 1\rangle}" /> both hold. Thus—as a number rather than a “form”—the number <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+-1+%7E%7C%7E+1%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle -1 ~|~ 1\rangle}" class="latex" title="{\langle -1 ~|~ 1\rangle}" /> equals <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. </p>
<p>
That seems to make sense since <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> is the average of <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-1}" class="latex" title="{-1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, but now compute <img src="https://s0.wp.com/latex.php?latex=%7B2+%3D+1+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 = 1 + 1}" class="latex" title="{2 = 1 + 1}" /> as a formal Conway number and consider <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Clangle+-1+%7E%7C%7E+2%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = \langle -1 ~|~ 2\rangle}" class="latex" title="{c = \langle -1 ~|~ 2\rangle}" />. This also satisfies the relations <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cleq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c \leq 0}" class="latex" title="{c \leq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 \leq c}" class="latex" title="{0 \leq c}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> must likewise equal <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+L+%7E%7C%7E+R+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle L ~|~ R \rangle}" class="latex" title="{\langle L ~|~ R \rangle}" /> is not some kind of numerical interpolation between <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />. The interpretation that grabbed my imagination as a teenager in 1976 is that:</p>
<blockquote><p><b> </b> <em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+L+%7E%7C%7E+R+%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\langle L ~|~ R \rangle}" class="latex" title="{\langle L ~|~ R \rangle}" /> equals the <b>simplest</b> number that is between <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />. </em>
</p></blockquote>
<p></p><p>
This is especially evocative in cases like <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+1+%7E%7C%7E+%5Cemptyset+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 1 ~|~ \emptyset \rangle}" class="latex" title="{\langle 1 ~|~ \emptyset \rangle}" />, which is what one gets by computing <img src="https://s0.wp.com/latex.php?latex=%7B1+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 + 1}" class="latex" title="{1 + 1}" />. In general, <img src="https://s0.wp.com/latex.php?latex=%7Bm%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m+1}" class="latex" title="{m+1}" /> is the simplest number between <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cemptyset%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\emptyset}" class="latex" title="{\emptyset}" />. Conway made this a theorem by giving each number a set-theoretic ordinal for its “time of generation” and proved that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+L+%7E%7C%7E+R+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle L ~|~ R \rangle}" class="latex" title="{\langle L ~|~ R \rangle}" /> always equals a (the) least-ordinal number <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cleq+c+%5Cleq+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell \leq c \leq r}" class="latex" title="{\ell \leq c \leq r}" /> for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cin+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell \in L}" class="latex" title="{\ell \in L}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Cin+R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r \in R}" class="latex" title="{r \in R}" />. </p>
<p>
Conway’s rules allow <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> to be infinite sets—any sets of numbers built by the rules of set theory. Then not only do all real numbers emerge at ordinal times, so do infinitesimals and further richness of structure. We should remember that Conway began as a set theorist with a dissertation under Harold Davenport titled <em>Homogeneous ordered sets</em>. All Conway numbers with finite creation times are dyadic rational numbers, which may seem trivial from the standpoint of set theory, but those are akin to binary strings. </p>
<p>
What became magic was how Conway’s rules characterize <em>games</em>. Through games we can also interpret forms like <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0+%7E%7C%7E+0+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0 ~|~ 0 \rangle}" class="latex" title="{\langle 0 ~|~ 0 \rangle}" /> that are not numbers. I did not know about complexity when I purchased Conway’s <a href="https://en.wikipedia.org/wiki/On_Numbers_and_Games">book</a> <em>On Numbers and Games</em> around 1980, let alone the connections between games and complexity. The book has a lot of depth that might be useful to complexity theory. To quote Peter Sarnak, per this <a href="https://www.ias.edu/ideas/2015/roberts-john-horton-conway">article</a> by Conway’s biographer Siobhan Roberts on Conway’s meeting with Kurt Gödel:</p>
<blockquote><p><b> </b> <em> The surreal numbers will be applied. It’s just a question of how and when. </em>
</p></blockquote>
<p>
</p><p>
</p><p>
</p><p></p><h2> Modular Programming </h2><p></p>
<p></p><p>
Most of us know that the conditional-jump instruction</p>
<p>
<font size="+1"><tt><b><br />
if (x == 0) goto k<br />
</b></tt></font></p>
<p></p><p><br />
where <tt><b>k</b></tt> is the label of another instruction, creates a universal programming language when added to the usual programming primitives of assignment, sequencing, and simple arithmetic. Conway was a maven of the “modular-jump”:</p>
<p>
<font size="+1"><tt><b><br />
if (x == 0 mod m) goto k.<br />
</b></tt></font></p>
<p></p><p><br />
In complexity theory we know that mod-<img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> gates having 0-1 inputs define the idea of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BACC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ACC}}" class="latex" title="{\mathsf{ACC}}" /> circuits, with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BACC%7D%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ACC}^0}" class="latex" title="{\mathsf{ACC}^0}" /> denoting problems solved by families of these circuits having fixed depth and polynomial size. If we don’t insist on fixed depth and unary inputs, we get modular programs. They are more complex than <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BACC%7D%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ACC}^0}" class="latex" title="{\mathsf{ACC}^0}" /> circuits, but we can learn from what can be done <em>concretely</em> with them.</p>
<p>
Conway created a particular form of modular programs in a language he called <a href="https://link.springer.com/chapter/10.1007/978-1-4612-4808-8_2">FRACTRAN</a>. A program is just a list of positive fractions <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Ba_r%7D%7Bb_r%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{a_r}{b_r}}" class="latex" title="{\frac{a_r}{b_r}}" /> in lowest terms. The input is an integer <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> held in a separate register. Each fraction represents the code line</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7Bif+%7D+%28n%2Aa_r+%5Cequiv+0+%5Cpmod%7Bb_r%7D%29+%5C%7B+n+%3D+n%5Cfrac%7Ba_r%7D%7Bb_r%7D%3B+%5Ctext%7Bgoto+start%7D+%5C%7D%3B+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \text{if } (n*a_r \equiv 0 \pmod{b_r}) \{ n = n\frac{a_r}{b_r}; \text{goto start} \}; " class="latex" title="\displaystyle  \text{if } (n*a_r \equiv 0 \pmod{b_r}) \{ n = n\frac{a_r}{b_r}; \text{goto start} \}; " /></p>
<p>
In other words, each iteration takes the first fraction <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bnf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{nf}" class="latex" title="{nf}" /> is an integer and updates <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bnf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{nf}" class="latex" title="{nf}" />; if there is no such fraction then the program exits and outputs <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.</p>
<p>
For example, the following FRACTRAN program given in Wikipedia’s <a href="https://en.wikipedia.org/wiki/FRACTRAN">article</a> implicitly computes integer division: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%5B%5Cfrac%7B91%7D%7B66%7D%2C%7E%5Cfrac%7B11%7D%7B13%7D%2C%7E%5Cfrac%7B1%7D%7B33%7D%2C%7E%5Cfrac%7B85%7D%7B11%7D%2C%7E%5Cfrac%7B57%7D%7B119%7D%2C%7E%5Cfrac%7B17%7D%7B19%7D%2C%7E%5Cfrac%7B11%7D%7B17%7D%2C%7E%5Cfrac%7B1%7D%7B3%7D%5Cright%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left[\frac{91}{66},~\frac{11}{13},~\frac{1}{33},~\frac{85}{11},~\frac{57}{119},~\frac{17}{19},~\frac{11}{17},~\frac{1}{3}\right]. " class="latex" title="\displaystyle  \left[\frac{91}{66},~\frac{11}{13},~\frac{1}{33},~\frac{85}{11},~\frac{57}{119},~\frac{17}{19},~\frac{11}{17},~\frac{1}{3}\right]. " /></p>
<p>The notation is unary: The input <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> has the form <img src="https://s0.wp.com/latex.php?latex=%7B2%5En+3%5Ed+11%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n 3^d 11}" class="latex" title="{2^n 3^d 11}" /> and the ouput is <img src="https://s0.wp.com/latex.php?latex=%7B5%5Eq+7%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{5^q 7^r}" class="latex" title="{5^q 7^r}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+qd+%2B+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n = qd + r}" class="latex" title="{n = qd + r}" /> with remainder <img src="https://s0.wp.com/latex.php?latex=%7Br+%3C+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r &lt; d}" class="latex" title="{r &lt; d}" />. This already hints the fact that FRACTRAN is a universal programming language. Powers of primes serve as memory registers. The following program computes the Hamming weight <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> of the binary expansion of a natural number <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> encoded as <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^x}" class="latex" title="{2^x}" />, returning the value <img src="https://s0.wp.com/latex.php?latex=%7B13%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{13^k}" class="latex" title="{13^k}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%5B%5Cfrac%7B33%7D%7B20%7D%2C%7E%5Cfrac%7B5%7D%7B11%7D%2C%7E%5Cfrac%7B13%7D%7B10%7D%2C%7E%5Cfrac%7B1%7D%7B5%7D%2C%7E%5Cfrac%7B2%7D%7B3%7D%2C%7E%5Cfrac%7B10%7D%7B7%7D%2C%7E%5Cfrac%7B7%7D%7B2%7D%5Cright%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left[\frac{33}{20},~\frac{5}{11},~\frac{13}{10},~\frac{1}{5},~\frac{2}{3},~\frac{10}{7},~\frac{7}{2}\right]. " class="latex" title="\displaystyle  \left[\frac{33}{20},~\frac{5}{11},~\frac{13}{10},~\frac{1}{5},~\frac{2}{3},~\frac{10}{7},~\frac{7}{2}\right]. " /></p>
<p>This might help bridge to our notions of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BACC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ACC}}" class="latex" title="{\mathsf{ACC}}" />. The Wikipedia article does a good job of de-mystifying the fractions in terms of their actions on the prime-power registers under the unary-style encoding. We wonder what happens when we try to work directly with binary encodings. </p>
<p>
</p><p></p><h2> The Collatz Example </h2><p></p>
<p></p><p>
The famous “<img src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3n+1}" class="latex" title="{3n+1}" />” problem of Lothar Collatz is a case in point. It iterates the function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++T%28n%29+%3D+%5Cbegin%7Bcases%7D+%5Cfrac%7B3n%2B1%7D%7B2%7D+%26+%5Ctext%7Bif+%7D+n+%5Ctext%7B+is+odd%7D+%5C%5C+%5Cfrac%7Bn%7D%7B2%7D+%26+%5Ctext%7Bif+%7D+n+%5Ctext%7B+is+even%7D+%5Cend%7Bcases%7D++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  T(n) = \begin{cases} \frac{3n+1}{2} &amp; \text{if } n \text{ is odd} \\ \frac{n}{2} &amp; \text{if } n \text{ is even} \end{cases}  " class="latex" title="\displaystyle  T(n) = \begin{cases} \frac{3n+1}{2} &amp; \text{if } n \text{ is odd} \\ \frac{n}{2} &amp; \text{if } n \text{ is even} \end{cases}  " /></p>
<p>The following FRACTRAN program <a href="https://hal.inria.fr/hal-00958971/document">given</a> by Kenneth Monks iterates <img src="https://s0.wp.com/latex.php?latex=%7BT%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T(n)}" class="latex" title="{T(n)}" /> under the unary encoding <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%5B%5Cfrac%7B1%7D%7B11%7D%2C%7E%5Cfrac%7B136%7D%7B15%7D%2C%7E%5Cfrac%7B5%7D%7B17%7D%2C%7E%5Cfrac%7B4%7D%7B5%7D%2C%7E%5Cfrac%7B26%7D%7B21%7D%2C%7E%5Cfrac%7B7%7D%7B13%7D%2C%7E%5Cfrac%7B1%7D%7B7%7D%2C%7E%5Cfrac%7B33%7D%7B4%7D%2C%7E%5Cfrac%7B5%7D%7B2%7D%2C%7E%5Cfrac%7B7%7D%7B1%7D%5Cright%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left[\frac{1}{11},~\frac{136}{15},~\frac{5}{17},~\frac{4}{5},~\frac{26}{21},~\frac{7}{13},~\frac{1}{7},~\frac{33}{4},~\frac{5}{2},~\frac{7}{1}\right]. " class="latex" title="\displaystyle  \left[\frac{1}{11},~\frac{136}{15},~\frac{5}{17},~\frac{4}{5},~\frac{26}{21},~\frac{7}{13},~\frac{1}{7},~\frac{33}{4},~\frac{5}{2},~\frac{7}{1}\right]. " /></p>
<p>Note that since the last fraction is an integer the program runs forever. If <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n = 1}" class="latex" title="{n = 1}" /> so that the input is <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, it would go <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Crightarrow+5+%5Crightarrow+4+%5Crightarrow+33+%5Crightarrow+3+%5Crightarrow+21+%5Crightarrow+26+%5Crightarrow+14+%5Crightarrow+2+%5Ccdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \rightarrow 5 \rightarrow 4 \rightarrow 33 \rightarrow 3 \rightarrow 21 \rightarrow 26 \rightarrow 14 \rightarrow 2 \cdots}" class="latex" title="{2 \rightarrow 5 \rightarrow 4 \rightarrow 33 \rightarrow 3 \rightarrow 21 \rightarrow 26 \rightarrow 14 \rightarrow 2 \cdots}" /> and thus cycle, unless we stop it. The powers of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> that appear in its output give the desired sequence. </p>
<p>
More natural to us, however, is the following modular program—which can use binary or any notation:</p>
<p>
<font size="+1"><tt><b><br />
start: if (n == 1) { halt; }<br />
if (n == 0 mod 2) { goto div; }<br />
n = 3*n + 1;<br />
div: n = n/2;<br />
goto start;<br />
</b></tt></font></p>
<p></p><p><br />
One can generalize the Collatz problem to moduli <img src="https://s0.wp.com/latex.php?latex=%7Bm+%3E+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m &gt; 2}" class="latex" title="{m &gt; 2}" />. For each <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3C+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k &lt; m}" class="latex" title="{k &lt; m}" /> we have a linear transformation <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cmapsto+c_k+n+%2B+d_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \mapsto c_k n + d_k}" class="latex" title="{n \mapsto c_k n + d_k}" /> that always gives an integer value when <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cequiv+k+%5Cpmod%7Bm%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \equiv k \pmod{m}}" class="latex" title="{n \equiv k \pmod{m}}" />. We want to know about the orbits of numbers <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> under this iteration.</p>
<p>
In fact, this is exactly what FRACTRAN does. Take <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> to be the least common multiple of the denominators <img src="https://s0.wp.com/latex.php?latex=%7Bb_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b_r}" class="latex" title="{b_r}" /> in a FRACTRAN program <img src="https://s0.wp.com/latex.php?latex=%7B%5B%5Cfrac%7Ba_r%7D%7Bb_r%7D%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[\frac{a_r}{b_r}]}" class="latex" title="{[\frac{a_r}{b_r}]}" />. Then for each <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> we can list the remainders <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> that are multiples of <img src="https://s0.wp.com/latex.php?latex=%7Bb_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b_r}" class="latex" title="{b_r}" /> and we get <img src="https://s0.wp.com/latex.php?latex=%7Bc_k+%3D+%5Cfrac%7Ba_r%7D%7B%5Cgcd%28k%2Cm%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_k = \frac{a_r}{\gcd(k,m)}}" class="latex" title="{c_k = \frac{a_r}{\gcd(k,m)}}" />, with <img src="https://s0.wp.com/latex.php?latex=%7Bd_k+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_k = 0}" class="latex" title="{d_k = 0}" />. The Turing-universality of FRACTRAN then proves a general theorem Conway stated in 1972:</p>
<blockquote><p><b>Theorem 1</b> <em> Generalized Collatz-type problems for moduli <img src="https://s0.wp.com/latex.php?latex=%7Bm+%3E+2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{m &gt; 2}" class="latex" title="{m &gt; 2}" /> are undecidable. </em>
</p></blockquote>
<p></p><p>
<a href="https://link.springer.com/chapter/10.1007/978-3-540-72504-6_49">Several</a> <a href="http://julienmalka.me/collatz.pdf">followup</a> <a href="https://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Lagarias3-23.pdf">papers</a> have proved stronger and more particular forms of the undecidability. The paper by Monks linked above leverages the unary encoding to show that having <img src="https://s0.wp.com/latex.php?latex=%7Bd_k+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_k = 0}" class="latex" title="{d_k = 0}" /> is essentially without loss of generality for universality; it is titled “<img src="https://s0.wp.com/latex.php?latex=%7B3x%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3x+1}" class="latex" title="{3x+1}" /> Minus the <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" />.” </p>
<p>
Having digested universality, it is natural to wonder about complexity. Can we use modular programming to achieve stronger connections between number theory and complexity classes—classes above the level of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BACC%7D%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ACC}^0}" class="latex" title="{\mathsf{ACC}^0}" />, say? One possible mode of connection is exemplified by this <a href="https://www.researchgate.net/publication/220994869_One_Binary_Horn_Clause_is_Enough">paper</a> from STACS 1994, which both Dick and I attended. We wonder whether the kind of connection noted by Terry Tao in his <a href="https://terrytao.wordpress.com/2020/04/12/john-conway/">tribute</a> to Conway can also smooth the way to understanding <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BMIP%5E%2A+%3D+RE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{MIP^* = RE}}" class="latex" title="{\mathsf{MIP^* = RE}}" />.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Conway posed many open problems himself. Here is a <a href="https://oeis.org/A248380/a248380.pdf">list</a> of five for which he posted cash rewards in the manner of Paul Erdős. The fifth was recently solved. The fourth can be stated in one sentence:</p>
<blockquote><p><b> </b> <em> If a set of points in the plane intersects every convex region of area 1, then must it have pairs of points at arbitrarily small distances? </em>
</p></blockquote>
<p></p><p>
Our condolences go out to his family and all who were enthralled by him in the mathematical world. We could talk endlessly about his impact on mathematics education—even about simple things like how to <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=3111964">prove</a> that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{2}}" class="latex" title="{\sqrt{2}}" /> is irrational—or try to tangle with his <a href="https://en.wikipedia.org/wiki/Monstrous_moonshine">applications</a> of the “Monster” group to modular forms, but those must be for another time. Also see Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=4732">tribute</a> and its comments section for many more stories and items.</p>
<p></p><p><br />
[some small word and format changes, added link to Scott and may add others as time allows]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/04/14/john-horton-conway-1937-2020/"><span class="datestr">at April 14, 2020 08:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=49">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2020/04/14/a-primer-on-private-statistics-part-i/">A Primer on Private Statistics – Part I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>By <a href="http://www.gautamkamath.com/">Gautam Kamath</a> and <a href="http://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
<p>Differentially private statistics is a very lively research area, and has seen a lot of activity in the last couple years. While the phrasing is a slight departure from previous work which focused on estimation with worst-case datasets, it turns out that the differences are often superficial. In a short series of blog posts, we hope to educate readers on some of the recent advancements in this area, as well as shed light on some of the connections between the old and the new. We’ll describe the settings, cover a couple of technical examples, and give pointers to some other directions in the area. Thanks to <a href="https://cs-people.bu.edu/ads22/">Adam Smith</a> for helping kick off this project, <a href="http://www.cs.columbia.edu/~ccanonne/">Clément Canonne</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, and <a href="http://www.thomas-steinke.net/">Thomas Steinke</a> for helpful comments, and <a href="https://lucatrevisan.github.io/">Luca Trevisan</a> for his <a href="https://lucatrevisan.wordpress.com/latex-to-wordpress/">LaTeX2WP script</a>.</p>
<p><b>1. Introduction </b></p>
<p>Statistics and machine learning are now ubiquitous in data analysis. Given a dataset, one immediately wonders what it allows us to infer about the underlying population. However, modern datasets don’t exist in a vacuum: they often contain sensitive information about the individuals they represent. Without proper care, statistical procedures will result in gross violations of privacy. Motivated by the shortcomings of ad hoc methods for data anonymization, Dwork, McSherry, Nissim, and Smith introduced the celebrated notion of differential privacy [<a href="https://kamathematics.wordpress.com/feed/#DMNS06">DMNS06</a>].</p>
<p>From its inception, some of the driving motivations for differential privacy were applications in statistics and the social sciences, notably disclosure limitation for the US Census. And yet, the lion’s share of differential privacy research has taken place within the computer science community. As a result, the specific applications being studied are often not formulated using statistical terminology, or even as statistical problems. Perhaps most significantly, much of the early work in computer science (though definitely not all) focus on estimating some property <em>of a dataset</em> rather than estimating some property <em>of an underlying population</em>.</p>
<p>Although the earliest works exploring the interaction between differential privacy and classical statistics go back to at least 2009 [<a href="https://kamathematics.wordpress.com/feed/#VS09">VS09</a>,<a href="https://kamathematics.wordpress.com/feed/#FRY10">FRY10</a>], the emphasis on differentially private statistical inference in the computer science literature is somewhat more recent. However, while earlier results on differential privacy did not always formulate problems in a statistical language, statistical inference was a key motivation for most of this work. As a result many of the techniques that were developed have direct applications in statistics, for example establishing minimax rates for estimation problems.</p>
<p>The purpose of this series of blog posts is to highlight some of those results in the computer science literature, and present them in a more statistical language. Specifically, we will discuss:</p>
<ul>
<li>Tight minimax lower bounds for privately estimating the mean of a multivariate distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^d}" class="latex" title="{{\mathbb R}^d}" />, using the technique of <em>tracing attacks</em> developed in [<a href="https://kamathematics.wordpress.com/feed/#BUV14">BUV14</a>,<a href="https://kamathematics.wordpress.com/feed/#DSSUV15">DSSUV15</a>, <a href="https://kamathematics.wordpress.com/feed/#BSU17">BSU17</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17a">SU17a</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17b">SU17b</a>, <a href="https://kamathematics.wordpress.com/feed/#KLSU19">KLSU19</a>].
<p> </p>
</li>
<li>Upper bounds for estimating a distribution in Kolmogorov distance, using the ubiquitous <em>binary-tree mechanism</em> introduced in [<a href="https://kamathematics.wordpress.com/feed/#DNPR10">DNPR10</a>,<a href="https://kamathematics.wordpress.com/feed/#CSS11">CSS11</a>].</li>
</ul>
<p>In particular, we hope to encourage computer scientists working on differential privacy to pay more attention to the applications of their methods in statistics, and share with statisticians many of the powerful techniques that have been developed in the computer science literature.</p>
<p> </p>
<p><b> 1.1. Formulating Private Statistical Inference </b></p>
<p>Essentially every differentially private statistical estimation task can be phrased using the following setup. We are given a dataset <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%28X_1%2C+%5Cdots%2C+X_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X = (X_1, \dots, X_n)}" class="latex" title="{X = (X_1, \dots, X_n)}" /> of size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, and we wish to design an algorithm <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Cin+%5Cmathcal%7BM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M \in \mathcal{M}}" class="latex" title="{M \in \mathcal{M}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{M}}" class="latex" title="{\mathcal{M}}" /> is the class of mechanisms that are both:</p>
<ol>
<li>differentially private, and</li>
<li>accurate, either in expectation or with high probability, according to some task-specific measure.</li>
</ol>
<p>A few comments about this framework are in order. First, although the accuracy requirement is stochastic in nature (i.e., an algorithm might not be accurate depending on the randomness of the algorithm and the data generation process), the privacy requirement is worst-case in nature. That is, the algorithm must protect privacy for every dataset <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />, even those we believe are very unlikely.</p>
<p>Second, the accuracy requirement is stated rather vaguely. This is because the notion of accuracy of an algorithm is slightly more nuanced, depending on whether we are concerned with <em>empirical</em> or <em>population</em> statistics. A particular emphasis of these blog posts is to explore the difference (or, as we will see, the lack of a difference) between these two notions of accuracy. The former estimates a quantity of the observed dataset, while the latter estimates a quantity of an unobserved distribution which is assumed to have generated the dataset.</p>
<p>More precisely, the former can be phrased in terms of empirical loss, of the form:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7BM+%5Cin+%5Cmathcal%7BM%7D%7D%7E%5Cmax_%7BX+%5Cin+%5Cmathcal%7BX%7D%7D%7E%5Cmathop%7B%5Cmathbb+E%7D_M%28%5Cell%28M%28X%29%2C+f%28X%29%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{M \in \mathcal{M}}~\max_{X \in \mathcal{X}}~\mathop{\mathbb E}_M(\ell(M(X), f(X))), " class="latex" title="\displaystyle \min_{M \in \mathcal{M}}~\max_{X \in \mathcal{X}}~\mathop{\mathbb E}_M(\ell(M(X), f(X))), " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{M}}" class="latex" title="{\mathcal{M}}" /> is some class of <em>randomized estimators</em> (e.g., differentially private estimators), <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{X}}" class="latex" title="{\mathcal{X}}" /> is some class of <em>datasets</em>, <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is some quantity of interest, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> is some <em>loss function</em>. That is, we’re looking to find an estimator that has small expected loss on <em>any dataset</em> in some class.</p>
<p>In contrast, statistical minimax theory looks at statements about population loss, of the form:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7BM+%5Cin+%5Cmathcal%7BM%7D%7D%7E%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D%7E%5Cmathop%7B%5Cmathbb+E%7D_%7BX+%5Csim+P%2C+M%7D%28%5Cell%28M%28X%29%2Cf%28P%29%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{M \in \mathcal{M}}~\max_{P \in \mathcal{P}}~\mathop{\mathbb E}_{X \sim P, M}(\ell(M(X),f(P))), " class="latex" title="\displaystyle \min_{M \in \mathcal{M}}~\max_{P \in \mathcal{P}}~\mathop{\mathbb E}_{X \sim P, M}(\ell(M(X),f(P))), " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> is some family of <em>distributions</em> over datasets (typically consisting of i.i.d. samples). That is, we’re looking to find an estimator that has small expected loss on random data from <em>any distribution</em> in some class. In particular, note that the randomness in this objective additionally includes the data generating procedure <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Csim+P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X \sim P}" class="latex" title="{X \sim P}" />.</p>
<p>These two formulations are formally very different in several ways. First, the empirical formulation requires an estimator to have small loss on <em>worst-case</em> datasets, whereas the statistical formulation only requires the estimator to have small loss <em>on average</em> over datasets drawn from certain distributions. Second, the statistical formulation requires that we estimate the unknown quantity <img src="https://s0.wp.com/latex.php?latex=%7Bf%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(P)}" class="latex" title="{f(P)}" />, and thus necessitates a solution to the non-private estimation problem. On the other hand, the empirical formulation only asks us to estimate the known quantity <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X)}" class="latex" title="{f(X)}" />, and thus if there were no privacy constraint it would always be possible to compute <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X)}" class="latex" title="{f(X)}" /> exactly. Third, typically in the statistical formulation, we require that the dataset is drawn i.i.d., which means that we are more constrained when proving lower bounds for estimation than we are in the empirical problem.</p>
<p>However, in practice (more precisely, in the practice of doing theoretical research), these two formulations are more alike than they are different, and results about one formulation often imply results about the other formulation. On the algorithmic side, classical statistical results will often tell us that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%28f%28X%29%2Cf%28P%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell(f(X),f(P))}" class="latex" title="{\ell(f(X),f(P))}" /> is small, in which case algorithms that guarantee <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%28M%28X%29%2Cf%28X%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell(M(X),f(X))}" class="latex" title="{\ell(M(X),f(X))}" /> is small also guarantee <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%28M%28X%29%2Cf%28P%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell(M(X),f(P))}" class="latex" title="{\ell(M(X),f(P))}" /> is small.</p>
<p>Moreover, typical lower bound arguments for empirical quantities are often statistical in nature. These typically involving constructing some simple “hard distribution” over datasets such that no private algorithm can estimate well on average for this distribution, and thus these lower bound arguments also apply to estimating population statistics for some simple family of distributions. We will proceed to give some examples of estimation problems that were originally studied by computer scientists with the empirical formulation in mind. These results either implicitly or explicitly provide solutions to the corresponding population versions of the same problems—our goal is to spell out and illustrate these connections.</p>
<p><b>2. DP Background </b></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%28X_1%2CX_2%2C%5Cdots%2CX_n%29+%5Cin+%5Cmathcal%7BX%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X = (X_1,X_2,\dots,X_n) \in \mathcal{X}^n}" class="latex" title="{X = (X_1,X_2,\dots,X_n) \in \mathcal{X}^n}" /> be a collection of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> samples where each individual sample comes from the domain <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{X}}" class="latex" title="{\mathcal{X}}" />. We say that two samples <img src="https://s0.wp.com/latex.php?latex=%7BX%2CX%27+%5Cin+%5Cmathcal%7BX%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X,X' \in \mathcal{X}^*}" class="latex" title="{X,X' \in \mathcal{X}^*}" /> are <em>adjacent</em>, denoted <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Csim+X%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X \sim X'}" class="latex" title="{X \sim X'}" />, if they differ on at most one individual sample. Intuitively, a randomized algorithm <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" />, which is often called a <em>mechanism</em> for historical reasons, is <em>differentially private</em> if the distribution of <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X)}" class="latex" title="{M(X)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X')}" class="latex" title="{M(X')}" /> are similar for every pair of adjacent samples <img src="https://s0.wp.com/latex.php?latex=%7BX%2CX%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X,X'}" class="latex" title="{X,X'}" />.</p>
<blockquote>
<p><b>Definition 1 ([<a href="https://kamathematics.wordpress.com/feed/#DMNS06">DMNS06</a>])</b><em> A mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Ccolon+%5Cmathcal%7BX%7D%5En+%5Crightarrow+%5Cmathcal%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M \colon \mathcal{X}^n \rightarrow \mathcal{R}}" class="latex" title="{M \colon \mathcal{X}^n \rightarrow \mathcal{R}}" /> is <em><img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private</em> if for every pair of adjacent datasets <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Csim+X%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X \sim X'}" class="latex" title="{X \sim X'}" />, and every (measurable) <img src="https://s0.wp.com/latex.php?latex=%7BR+%5Csubseteq+R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R \subseteq R}" class="latex" title="{R \subseteq R}" /> </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+P%7D%28M%28X%29+%5Cin+R%29+%5Cleq+e%5E%7B%5Cepsilon%7D+%5Ccdot+%5Cmathop%7B%5Cmathbb+P%7D%28M%28X%27%29+%5Cin+R%29+%2B+%5Cdelta.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb P}(M(X) \in R) \leq e^{\epsilon} \cdot \mathop{\mathbb P}(M(X') \in R) + \delta. " class="latex" title="\displaystyle \mathop{\mathbb P}(M(X) \in R) \leq e^{\epsilon} \cdot \mathop{\mathbb P}(M(X') \in R) + \delta. " /></p>
<p> </p>
</blockquote>
<p>We let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D_%7B%5Cepsilon%2C%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{M}_{\epsilon,\delta}}" class="latex" title="{\mathcal{M}_{\epsilon,\delta}}" /> denote the set of mechanisms that satisfy <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differential privacy.</p>
<blockquote>
<p><b>Remark 1</b> <em> To simplify notation, and to maintain consistency with the literature, we adopt the convention of defining the mechanism only for a fixed sample size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. What this means in practice is that the mechanisms we describe treat the sample size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is <em>public information</em> that need not be kept private. While one could define a more general model where <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is not fixed, it wouldn’t add anything to this discussion other than additional complexity. </em></p>
</blockquote>
<blockquote>
<p><b>Remark 2</b> <em> In these blog posts, we stick to the most general formulation of differential privacy, so-called <em>approximate differential privacy</em>, i.e. <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differential privacy for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta &gt; 0}" class="latex" title="{\delta &gt; 0}" /> essentially because this is the notion that captures the widest variety of private mechanisms. Almost all of what follows would apply equally well, with minor technical modifications, to slightly stricter notions of <em>concentrated differential privacy [</em><a href="https://kamathematics.wordpress.com/feed/#DR16">DR16</a>, <a href="https://kamathematics.wordpress.com/feed/#BS16">BS16</a>, <a href="https://kamathematics.wordpress.com/feed/#BDRS18">BDRS18</a>], Rényi differential privacy [<a href="https://kamathematics.wordpress.com/feed/#Mir17">Mir17</a>], or <em>Gaussian differential privacy [<a href="https://kamathematics.wordpress.com/feed/#DRS19">DRS19</a>]</em>. While so-called <em>pure differential privacy</em>, i.e. <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,0)}" class="latex" title="{(\epsilon,0)}" />-differential privacy has also been studied extensively, this notion is artificially restrictive and excludes many differentially private mechanisms. </em></p>
</blockquote>
<p>A key property of differential privacy that helps when desinging efficient estimators is <em>closure under postprocessing</em>:</p>
<blockquote>
<p><b>Lemma 2 (Post-Processing [<a href="https://kamathematics.wordpress.com/feed/#DMNS06">DMNS06</a>])</b><em> <a name="lempost-processing"></a> If <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Ccolon+%5Cmathcal%7BX%7D%5En+%5Crightarrow+%5Cmathcal%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M \colon \mathcal{X}^n \rightarrow \mathcal{R}}" class="latex" title="{M \colon \mathcal{X}^n \rightarrow \mathcal{R}}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private and <img src="https://s0.wp.com/latex.php?latex=%7BM%27+%5Ccolon+%5Cmathcal%7BR%7D+%5Crightarrow+%5Cmathcal%7BR%7D%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M' \colon \mathcal{R} \rightarrow \mathcal{R}'}" class="latex" title="{M' \colon \mathcal{R} \rightarrow \mathcal{R}'}" /> is any randomized algorithm, then <img src="https://s0.wp.com/latex.php?latex=%7BM%27+%5Ccirc+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M' \circ M}" class="latex" title="{M' \circ M}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private. </em></p>
</blockquote>
<p>The estimators we present in this work will use only one tool for achieving differential privacy, the <em>Gaussian Mechanism</em>.</p>
<blockquote>
<p><b>Lemma 3 (Gaussian Mechanism)</b> <em> <a name="lemgauss-mech"></a> Let <img src="https://s0.wp.com/latex.php?latex=%7Bf+%5Ccolon+%5Cmathcal%7BX%7D%5En+%5Crightarrow+%7B%5Cmathbb+R%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f \colon \mathcal{X}^n \rightarrow {\mathbb R}^d}" class="latex" title="{f \colon \mathcal{X}^n \rightarrow {\mathbb R}^d}" /> be a function and let </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CDelta_%7Bf%7D+%3D+%5Csup_%7BX%5Csim+X%27%7D+%5C%7C+f%28X%29+-+f%28X%27%29+%5C%7C_2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Delta_{f} = \sup_{X\sim X'} \| f(X) - f(X') \|_2 " class="latex" title="\displaystyle \Delta_{f} = \sup_{X\sim X'} \| f(X) - f(X') \|_2 " /></p>
<p>denote its <em><img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2}" class="latex" title="{\ell_2}" />-sensitivity</em>. The <em>Gaussian mechanism</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M%28X%29+%3D+f%28X%29+%2B+%5Cmathcal%7BN%7D%5Cleft%280+%2C+%5Cfrac%7B2+%5Clog%282%2F%5Cdelta%29%7D%7B%5Cepsilon%5E2%7D+%5Ccdot+%5CDelta_%7Bf%7D%5E2+%5Ccdot+%7B%5Cmathbb+I%7D_%7Bd+%5Ctimes+d%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle M(X) = f(X) + \mathcal{N}\left(0 , \frac{2 \log(2/\delta)}{\epsilon^2} \cdot \Delta_{f}^2 \cdot {\mathbb I}_{d \times d} \right) " class="latex" title="\displaystyle M(X) = f(X) + \mathcal{N}\left(0 , \frac{2 \log(2/\delta)}{\epsilon^2} \cdot \Delta_{f}^2 \cdot {\mathbb I}_{d \times d} \right) " /></p>
<p><em> satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differential privacy. </em></p>
</blockquote>
<p><b>3. Mean Estimation in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^d}" class="latex" title="{{\mathbb R}^d}" /> </b></p>
<p>Let’s take a dive into the problem of <em>private mean estimation</em> for some family <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> of multivariate distributions over <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^d}" class="latex" title="{{\mathbb R}^d}" />. This problem has been studied for various families <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> and various choices of loss function. Here we focus on perhaps the simplest variant of the problem, in which <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> contains distributions of bounded support <img src="https://s0.wp.com/latex.php?latex=%7B%5B%5Cpm+1%5D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[\pm 1]^d}" class="latex" title="{[\pm 1]^d}" /> and the loss is the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2^2}" class="latex" title="{\ell_2^2}" /> error. We emphasize, however, that the methods we discuss here are quite versatile and can be used to derive minimax bounds for other variants of the mean-estimation problem.</p>
<p>Note that, by a simple argument, the non-private minimax rate for this class is achieved by the empirical mean, and is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+%5Coverline%7BX%7D+-+%5Cmu%5C%7C_2%5E2%29+%3D+%5Cfrac%7Bd%7D%7Bn%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \overline{X} - \mu\|_2^2) = \frac{d}{n}. \ \ \ \ \ (1)" class="latex" title="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| \overline{X} - \mu\|_2^2) = \frac{d}{n}. \ \ \ \ \ (1)" /></p>
<p>The main goal of this section is to derive the minimax bound <a name="eqRd-minimax"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7BM+%5Cin+%5Cmathcal%7BM%7D_%7B%5Cepsilon%2C%5Cfrac%7B1%7D%7Bn%7D%7D%7D+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Cmu+%5C%7C_2%5E2%29+%3D+%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Ctilde%5CTheta%5Cleft%28%5Cfrac%7Bd%5E2%7D%7B%5Cepsilon%5E2+n%5E2%7D%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{M \in \mathcal{M}_{\epsilon,\frac{1}{n}}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \mu \|_2^2) = \frac{d}{n} + \tilde\Theta\left(\frac{d^2}{\epsilon^2 n^2}\right). \ \ \ \ \ (2)" class="latex" title="\displaystyle \min_{M \in \mathcal{M}_{\epsilon,\frac{1}{n}}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \mu \|_2^2) = \frac{d}{n} + \tilde\Theta\left(\frac{d^2}{\epsilon^2 n^2}\right). \ \ \ \ \ (2)" /></p>
<p><a name="eqRd-minimax"></a> Recall that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+%5CTheta%28f%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tilde \Theta(f(n))}" class="latex" title="{\tilde \Theta(f(n))}" /> refers to a function which is both <img src="https://s0.wp.com/latex.php?latex=%7BO%28f%28n%29+%5Clog%5E%7Bc_1%7D+f%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(f(n) \log^{c_1} f(n))}" class="latex" title="{O(f(n) \log^{c_1} f(n))}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28f%28n%29+%5Clog%5E%7Bc_2%7D+f%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(f(n) \log^{c_2} f(n))}" class="latex" title="{\Omega(f(n) \log^{c_2} f(n))}" /> for some constants <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%2C+c_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1, c_2}" class="latex" title="{c_1, c_2}" />. The proof of this lower bound is based on <em>robust tracing attacks</em>, also called <em>membership inference attacks</em>, which were developed in a chain of papers [<a href="https://kamathematics.wordpress.com/feed/#BUV14">BUV14</a>, <a href="https://kamathematics.wordpress.com/feed/#DSSUV15">DSSUV15</a>, <a href="https://kamathematics.wordpress.com/feed/#BSU17">BSU17</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17a">SU17a</a>, <a href="https://kamathematics.wordpress.com/feed/#SU17b">SU17b</a>, <a href="https://kamathematics.wordpress.com/feed/#KLSU19">KLSU19</a>]. We remark that this lower bound is almost identical to the minimax bound for mean estimation proven in the much more recent work of Cai, Wang, and Zhang [<a href="https://kamathematics.wordpress.com/feed/#CWZ19">CWZ19</a>], but it lacks tight dependence on the parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />, which we discuss in the following remark.</p>
<blockquote>
<p><b>Remark 3</b> <em> The choice of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3D+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta = 1/n}" class="latex" title="{\delta = 1/n}" /> in <a href="https://kamathematics.wordpress.com/feed/#eqRd-minimax">(2)</a> may look strange at first. For the upper bound this choice is arbitrary—as we will see, we can upper bound the rate for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta &gt; 0}" class="latex" title="{\delta &gt; 0}" /> at a cost of a factor of <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog%281%2F%5Cdelta%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\log(1/\delta))}" class="latex" title="{O(\log(1/\delta))}" />. The lower bound applies only when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cleq+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta \leq 1/n}" class="latex" title="{\delta \leq 1/n}" />. Note that the rate is qualitatively different when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cgg+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta \gg 1/n}" class="latex" title="{\delta \gg 1/n}" />. However, we emphasize that <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differential privacy is not a meaningful privacy notion unless <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cll+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta \ll 1/n}" class="latex" title="{\delta \ll 1/n}" />. In particular, the mechanism that randomly outputs <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta n}" class="latex" title="{\delta n}" /> elements of the sample satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%280%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(0,\delta)}" class="latex" title="{(0,\delta)}" />-differential privacy. However, when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cgg+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta \gg 1/n}" class="latex" title="{\delta \gg 1/n}" />, this mechanism completely violates the privacy of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgg+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\gg 1}" class="latex" title="{\gg 1}" /> person in the dataset. Moreover, taking the empirical mean of these <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta n}" class="latex" title="{\delta n}" /> samples gives rate <img src="https://s0.wp.com/latex.php?latex=%7Bd%2F%5Cdelta+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d/\delta n}" class="latex" title="{d/\delta n}" />, which would violate our lower bound when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> is large enough. On the other hand, we would expect the minimax rate to become slower when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%5Cll+1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta \ll 1/n}" class="latex" title="{\delta \ll 1/n}" />. This expectation is, in fact, correct, however the proof we present does not give the tight dependence on the parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />. See [<a href="https://kamathematics.wordpress.com/feed/#SU17a">SU17a</a>] for a refinement that can obtain the right dependence on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />, and [<a href="https://kamathematics.wordpress.com/feed/#CWZ19">CWZ19</a>] for the details of how to apply this refinement in the i.i.d. setting. </em></p>
</blockquote>
<p><b> 3.1. A Simple Upper Bound </b></p>
<blockquote>
<p><b>Theorem 4</b> <em> For every <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cin+%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \in {\mathbb N}}" class="latex" title="{n \in {\mathbb N}}" />, and every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%2C%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon,\delta &gt; 0}" class="latex" title="{\epsilon,\delta &gt; 0}" />, there exists an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private private mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> such that <a name="eqmean-est-ub"></a></em></p>
<p><em><em><a name="eqmean-est-ub"></a></em></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Cmu+%5C%7C_2%5E2%29+%5Cleq+%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Cfrac%7B2+d%5E2+%5Clog%282%2F%5Cdelta%29%7D%7B%5Cepsilon%5E2+n%5E2%7D.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \mu \|_2^2) \leq \frac{d}{n} + \frac{2 d^2 \log(2/\delta)}{\epsilon^2 n^2}. \ \ \ \ \ (3)" class="latex" title="\displaystyle \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P}(\| M(X_{1 \cdots n}) - \mu \|_2^2) \leq \frac{d}{n} + \frac{2 d^2 \log(2/\delta)}{\epsilon^2 n^2}. \ \ \ \ \ (3)" /></p>
<p><em><a name="eqmean-est-ub"></a></em></p>
<p><em><a name="eqmean-est-ub"></a> </em></p>
</blockquote>
<p><em>Proof:</em> Define the mechanism</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M%28X_%7B1+%5Ccdots+n%7D%29+%3D+%5Coverline%7BX%7D+%2B+%5Cmathcal%7BN%7D%5Cleft%280%2C+%5Cfrac%7B2+d+%5Clog%282%2F%5Cdelta%29%7D%7B%5Cvarepsilon%5E2+n%5E2%7D+%5Ccdot+%5Cmathbb%7BI%7D_%7Bd+%5Ctimes+d%7D+%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle M(X_{1 \cdots n}) = \overline{X} + \mathcal{N}\left(0, \frac{2 d \log(2/\delta)}{\varepsilon^2 n^2} \cdot \mathbb{I}_{d \times d} \right). \ \ \ \ \ (4)" class="latex" title="\displaystyle M(X_{1 \cdots n}) = \overline{X} + \mathcal{N}\left(0, \frac{2 d \log(2/\delta)}{\varepsilon^2 n^2} \cdot \mathbb{I}_{d \times d} \right). \ \ \ \ \ (4)" /></p>
<p>This mechanism satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differential privacy by Lemma <a href="https://kamathematics.wordpress.com/feed/#lemgauss-mech">3</a>, noting that for any pair of adjacent samples <img src="https://s0.wp.com/latex.php?latex=%7BX_%7B1+%5Ccdots+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_{1 \cdots n}}" class="latex" title="{X_{1 \cdots n}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%27_%7B1+%5Ccdots+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X'_{1 \cdots n}}" class="latex" title="{X'_{1 \cdots n}}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7C+%5Coverline%7BX%7D+-+%5Coverline%7BX%7D%27%5C%7C_2%5E2+%5Cleq+%5Cfrac%7Bd%7D%7Bn%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\| \overline{X} - \overline{X}'\|_2^2 \leq \frac{d}{n^2}}" class="latex" title="{\| \overline{X} - \overline{X}'\|_2^2 \leq \frac{d}{n^2}}" />.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%5E2+%3D+%5Cfrac%7B2+d+%5Clog%282%2F%5Cdelta%29%7D%7B%5Cvarepsilon%5E2+n%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma^2 = \frac{2 d \log(2/\delta)}{\varepsilon^2 n^2}}" class="latex" title="{\sigma^2 = \frac{2 d \log(2/\delta)}{\varepsilon^2 n^2}}" />. Note that since the Gaussian noise has mean <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> and is independent of <img src="https://s0.wp.com/latex.php?latex=%7B%5Coverline%7BX%7D+-+%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\overline{X} - \mu}" class="latex" title="{\overline{X} - \mu}" />, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brll%7D+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Cmu+%5C%7C_2%5E2%29+%3D%7B%7D+%26%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+%5Coverline%7BX%7D+-+%5Cmu+%5C%7C_2%5E2%29+%2B+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Coverline%7BX%7D+%5C%7C_2%5E2+%29+%5C%5C+%5Cleq%7B%7D+%26%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Coverline%7BX%7D+%5C%7C_2%5E2+%29+%5C%5C+%3D%7B%7D+%26%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+%5Cmathcal%7BN%7D%280%2C+%5Csigma%5E2+%5Cmathbb%7BI%7D_%7Bd+%5Ctimes+d%7D%29+%5C%7C_2%5E2+%29+%5C%5C+%3D%7B%7D+%26%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Csigma%5E2+d+%5C%5C+%3D%7B%7D+%26%5Cfrac%7Bd%7D%7Bn%7D+%2B+%5Cfrac%7B2+d%5E2+%5Clog%282%2F%5Cdelta%29%7D%7B%5Cepsilon%5E2+n%5E2%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rll} \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \mu \|_2^2) ={} &amp;\mathop{\mathbb E}(\| \overline{X} - \mu \|_2^2) + \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \overline{X} \|_2^2 ) \\ \leq{} &amp;\frac{d}{n} + \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \overline{X} \|_2^2 ) \\ ={} &amp;\frac{d}{n} + \mathop{\mathbb E}(\| \mathcal{N}(0, \sigma^2 \mathbb{I}_{d \times d}) \|_2^2 ) \\ ={} &amp;\frac{d}{n} + \sigma^2 d \\ ={} &amp;\frac{d}{n} + \frac{2 d^2 \log(2/\delta)}{\epsilon^2 n^2}. \end{array} " class="latex" title="\displaystyle \begin{array}{rll} \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \mu \|_2^2) ={} &amp;\mathop{\mathbb E}(\| \overline{X} - \mu \|_2^2) + \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \overline{X} \|_2^2 ) \\ \leq{} &amp;\frac{d}{n} + \mathop{\mathbb E}(\| M(X_{1 \cdots n}) - \overline{X} \|_2^2 ) \\ ={} &amp;\frac{d}{n} + \mathop{\mathbb E}(\| \mathcal{N}(0, \sigma^2 \mathbb{I}_{d \times d}) \|_2^2 ) \\ ={} &amp;\frac{d}{n} + \sigma^2 d \\ ={} &amp;\frac{d}{n} + \frac{2 d^2 \log(2/\delta)}{\epsilon^2 n^2}. \end{array} " /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p> </p>
<p><b> 3.2. Minimax Lower Bounds via Tracing </b></p>
<blockquote>
<p><b>Theorem 5</b> <em> <a name="thmmean-lb"></a> For every <img src="https://s0.wp.com/latex.php?latex=%7Bn%2C+d+%5Cin+%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n, d \in {\mathbb N}}" class="latex" title="{n, d \in {\mathbb N}}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3C+1%2F96n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta &lt; 1/96n}" class="latex" title="{\delta &lt; 1/96n}" />, if <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" /> is the class of all product distributions on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%5E%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}^{d}}" class="latex" title="{\{\pm 1\}^{d}}" />, then for some constant <img src="https://s0.wp.com/latex.php?latex=%7BC+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C &gt; 0}" class="latex" title="{C &gt; 0}" />, </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmin_%7BM+%5Cin+%5Cmathcal%7BM%7D_%7B%5Cepsilon%2C%5Cdelta%7D%7D+%5Cmax_%7BP+%5Cin+%5Cmathcal%7BP%7D%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7BX_%7B1+%5Ccdots+n%7D+%5Csim+P%2CM%7D%28%5C%7C+M%28X_%7B1+%5Ccdots+n%7D%29+-+%5Cmu+%5C%7C_2%5E2%29+%3D+%5COmega%5Cleft%28%5Cmin+%5Cleft%5C%7B+%5Cfrac%7Bd%5E2%7D%7B+%5Cepsilon%5E2+n%5E2%7D%2C+d+%5Cright%5C%7D%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \min_{M \in \mathcal{M}_{\epsilon,\delta}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P,M}(\| M(X_{1 \cdots n}) - \mu \|_2^2) = \Omega\left(\min \left\{ \frac{d^2}{ \epsilon^2 n^2}, d \right\}\right). " class="latex" title="\displaystyle \min_{M \in \mathcal{M}_{\epsilon,\delta}} \max_{P \in \mathcal{P}} \mathop{\mathbb E}_{X_{1 \cdots n} \sim P,M}(\| M(X_{1 \cdots n}) - \mu \|_2^2) = \Omega\left(\min \left\{ \frac{d^2}{ \epsilon^2 n^2}, d \right\}\right). " /></p>
<p> </p>
</blockquote>
<p>Note that it is trivial to achieve error <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> for any distribution using the mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM%28X_%7B1+%5Ccdots+n%7D%29+%5Cequiv+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X_{1 \cdots n}) \equiv 0}" class="latex" title="{M(X_{1 \cdots n}) \equiv 0}" />, so the result says that the error must be <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28d%5E2%2F%5Cepsilon%5E2+n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(d^2/\epsilon^2 n^2)}" class="latex" title="{\Omega(d^2/\epsilon^2 n^2)}" /> whenever this error is significantly smaller than the trivial error of <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />.</p>
<p><b>Tracing Attacks.</b></p>
<p>Before giving the formal proof, we will try to give some intuition for the high-level proof strategy. The proof can be viewed as constructing a <em>tracing attack </em>[<a href="https://kamathematics.wordpress.com/feed/#DSSU17">DSSU17</a>] (sometimes called a <em>membership inference attack</em>) of the following form. There is an attacker who has the data of some individual <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> chosen in one of the two ways: either <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is a random element of the sample <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />, or <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is an independent random sample from the population <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" />. The attacker is given access to the true distribution <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> and the outcome of the mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X)}" class="latex" title="{M(X)}" />, and wants to determine which of the two is the case. If the attacker can succeed, then <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> cannot be differentially private. To understand why this is the case, if <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is a member of the dataset, then the attacker should say <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is in the dataset, but if we consider the adjacent dataset <img src="https://s0.wp.com/latex.php?latex=%7BX%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X'}" class="latex" title="{X'}" /> where we replace <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> with some independent sample from <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" />, then the attacker will now say <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is independent of the dataset. Thus, <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X)}" class="latex" title="{M(X)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X')}" class="latex" title="{M(X')}" /> cannot be close in the sense required by differential privacy.</p>
<p>Thus, the proof works by constructing a test statistic <img src="https://s0.wp.com/latex.php?latex=%7BZ+%3D+Z%28M%28X%29%2CY%2CP%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z = Z(M(X),Y,P),}" class="latex" title="{Z = Z(M(X),Y,P),}" /> that the attacker can use to distinguish the two possibilities for <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" />. In particular, we show that there is a distribution over populations <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28Z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(Z)}" class="latex" title="{\mathop{\mathbb E}(Z)}" /> is small when <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is independent of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />, but for <em>every</em> sufficiently accurate mechanism <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28Z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(Z)}" class="latex" title="{\mathop{\mathbb E}(Z)}" /> is large when <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> is a random element of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />.</p>
<p><b>Proof of Theorem <a href="https://kamathematics.wordpress.com/feed/#thmmean-lb">5</a>.</b></p>
<p>The proof that we present closely follows the one that appears in Thomas Steinke’s Ph.D. thesis [<a href="https://kamathematics.wordpress.com/feed/#Ste16">Ste16</a>].</p>
<p>We start by constructing a “hard distribution” over the family of product distributions <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{P}}" class="latex" title="{\mathcal{P}}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu+%3D+%28%5Cmu%5E1%2C%5Cdots%2C%5Cmu%5Ed%29+%5Cin+%5B-1%2C1%5D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu = (\mu^1,\dots,\mu^d) \in [-1,1]^d}" class="latex" title="{\mu = (\mu^1,\dots,\mu^d) \in [-1,1]^d}" /> consist of <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> independent draws from the uniform distribution on <img src="https://s0.wp.com/latex.php?latex=%7B%5B-1%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[-1,1]}" class="latex" title="{[-1,1]}" /> and let <img src="https://s0.wp.com/latex.php?latex=%7BP_%7B%5Cmu%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_{\mu}}" class="latex" title="{P_{\mu}}" /> be the product distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%5E%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}^{d}}" class="latex" title="{\{\pm 1\}^{d}}" /> with mean <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu}" class="latex" title="{\mu}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cdots%2CX_n+%5Csim+P_%7B%5Cmu%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_1,\dots,X_n \sim P_{\mu}}" class="latex" title="{X_1,\dots,X_n \sim P_{\mu}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%28X_1%2C%5Cdots%2CX_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X = (X_1,\dots,X_n)}" class="latex" title="{X = (X_1,\dots,X_n)}" />.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Ccolon+%5C%7B%5Cpm+1%5C%7D%5E%7Bn+%5Ctimes+d%7D+%5Crightarrow+%5B%5Cpm+1%5D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M \colon \{\pm 1\}^{n \times d} \rightarrow [\pm 1]^d}" class="latex" title="{M \colon \{\pm 1\}^{n \times d} \rightarrow [\pm 1]^d}" /> be any <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%2C%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\epsilon,\delta)}" class="latex" title="{(\epsilon,\delta)}" />-differentially private mechanism and let</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Calpha%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%2CX%2CM%7D%28%5C%7C+M%28X%29+-+%5Cmu%5C%7C_2%5E2+%29+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \alpha^2 = \mathop{\mathbb E}_{\mu,X,M}(\| M(X) - \mu\|_2^2 ) \ \ \ \ \ (5)" class="latex" title="\displaystyle \alpha^2 = \mathop{\mathbb E}_{\mu,X,M}(\| M(X) - \mu\|_2^2 ) \ \ \ \ \ (5)" /></p>
<p>be its expected loss. We will prove the desired lower bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2}" class="latex" title="{\alpha^2}" />.</p>
<p>For every element <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />, we define the random variables</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Z_i+%3D+Z_i%28M%28X%29%2CX_i%2C%5Cmu%29+%3D+%5Cleft%5Clangle+M%28X%29+-+%5Cmu%2C+X_i+-+%5Cmu+%5Cright%5Crangle+%5C%5C+Z%27_%7Bi%7D+%3D+Z%27_i%28M%28X_%7B%5Csim+i%7D%29%2C+X_i%2C+%5Cmu%29+%3D+%5Cleft%5Clangle+M%28X_%7B%5Csim+i%7D%29+-+%5Cmu%2C+X_i+-+%5Cmu+%5Cright%5Crangle%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle Z_i = Z_i(M(X),X_i,\mu) = \left\langle M(X) - \mu, X_i - \mu \right\rangle \\ Z'_{i} = Z'_i(M(X_{\sim i}), X_i, \mu) = \left\langle M(X_{\sim i}) - \mu, X_i - \mu \right\rangle, " class="latex" title="\displaystyle Z_i = Z_i(M(X),X_i,\mu) = \left\langle M(X) - \mu, X_i - \mu \right\rangle \\ Z'_{i} = Z'_i(M(X_{\sim i}), X_i, \mu) = \left\langle M(X_{\sim i}) - \mu, X_i - \mu \right\rangle, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7BX_%7B%5Csim+i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_{\sim i}}" class="latex" title="{X_{\sim i}}" /> denotes <img src="https://s0.wp.com/latex.php?latex=%7B%28X_1%2C%5Cdots%2CX%27_i%2C%5Cdots%2CX_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X_1,\dots,X'_i,\dots,X_n)}" class="latex" title="{(X_1,\dots,X'_i,\dots,X_n)}" /> where <img src="https://s0.wp.com/latex.php?latex=%7BX%27_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X'_i}" class="latex" title="{X'_i}" /> is an independent sample from <img src="https://s0.wp.com/latex.php?latex=%7BP_%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_\mu}" class="latex" title="{P_\mu}" />. Our goal will be to show that, privacy and accuracy imply both upper and lower bounds on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28%5Csum_i+Z_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(\sum_i Z_i)}" class="latex" title="{\mathop{\mathbb E}(\sum_i Z_i)}" /> that depend on <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha}" class="latex" title="{\alpha}" />, and thereby obtain a bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2}" class="latex" title="{\alpha^2}" />.</p>
<p>The first claim says that, when <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" /> is <em>not</em> in the sample, then the likelihood random variable has mean <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> and variance controlled by the expected <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_2^2}" class="latex" title="{\ell_2^2}" /> error of the mechanism.</p>
<blockquote>
<p><b>Claim 1</b> <em> <a name="clmmean-lb-1"></a> For every <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28Z%27_i%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(Z'_i) = 0}" class="latex" title="{\mathop{\mathbb E}(Z'_i) = 0}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BVar%7D%28Z%27_i%29+%5Cleq+4%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathrm{Var}(Z'_i) \leq 4\alpha^2}" class="latex" title="{\mathrm{Var}(Z'_i) \leq 4\alpha^2}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7CZ%27_i%5C%7C_%5Cinfty+%5Cleq+4d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\|Z'_i\|_\infty \leq 4d}" class="latex" title="{\|Z'_i\|_\infty \leq 4d}" />. </em></p>
</blockquote>
<p><em>Proof:</em> Conditioned on any value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu}" class="latex" title="{\mu}" />, <img src="https://s0.wp.com/latex.php?latex=%7BM%28X_%7B%5Csim+i%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X_{\sim i})}" class="latex" title="{M(X_{\sim i})}" /> is independent from <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" />. Moreover, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28X_i+-+%5Cmu%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(X_i - \mu) = 0}" class="latex" title="{\mathop{\mathbb E}(X_i - \mu) = 0}" />, so we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brll%7D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%2CX%2CM%7D%28%5Clangle+M%28X_%7B%5Csim+i%7D%29+-+%5Cmu%2C+X_i+-+%5Cmu+%5Crangle%29+%5C%5C+%3D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%7D%28%5Cmathop%7B%5Cmathbb+E%7D_%7BX%2CM%7D%28%5Clangle+M%28X_%7B%5Csim+i%7D%29+-+%5Cmu%2C+X_i+-+%5Cmu+%5Crangle%29%29+%5C%5C+%3D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%7D%28%5Cleft%5Clangle+%5Cmathop%7B%5Cmathbb+E%7D_%7BX%2CM%7D%28M%28X_%7B%5Csim+i%7D%29+-+%5Cmu%29%2C+%5Cmathop%7B%5Cmathbb+E%7D_%7BX%2CM%7D%28X_i+-+%5Cmu%29+%5Cright+%5Crangle+%29+%5C%5C+%3D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%7D%28%5Cleft%5Clangle+%5Cmathop%7B%5Cmathbb+E%7D_%7BX%2CM%7D%28M%28X_%7B%5Csim+i%7D%29+-+%5Cmu%29%2C+0+%5Cright+%5Crangle+%29+%5C%5C+%3D+%260.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rll} &amp;\mathop{\mathbb E}_{\mu,X,M}(\langle M(X_{\sim i}) - \mu, X_i - \mu \rangle) \\ = &amp;\mathop{\mathbb E}_{\mu}(\mathop{\mathbb E}_{X,M}(\langle M(X_{\sim i}) - \mu, X_i - \mu \rangle)) \\ = &amp;\mathop{\mathbb E}_{\mu}(\left\langle \mathop{\mathbb E}_{X,M}(M(X_{\sim i}) - \mu), \mathop{\mathbb E}_{X,M}(X_i - \mu) \right \rangle ) \\ = &amp;\mathop{\mathbb E}_{\mu}(\left\langle \mathop{\mathbb E}_{X,M}(M(X_{\sim i}) - \mu), 0 \right \rangle ) \\ = &amp;0. \end{array} " class="latex" title="\displaystyle \begin{array}{rll} &amp;\mathop{\mathbb E}_{\mu,X,M}(\langle M(X_{\sim i}) - \mu, X_i - \mu \rangle) \\ = &amp;\mathop{\mathbb E}_{\mu}(\mathop{\mathbb E}_{X,M}(\langle M(X_{\sim i}) - \mu, X_i - \mu \rangle)) \\ = &amp;\mathop{\mathbb E}_{\mu}(\left\langle \mathop{\mathbb E}_{X,M}(M(X_{\sim i}) - \mu), \mathop{\mathbb E}_{X,M}(X_i - \mu) \right \rangle ) \\ = &amp;\mathop{\mathbb E}_{\mu}(\left\langle \mathop{\mathbb E}_{X,M}(M(X_{\sim i}) - \mu), 0 \right \rangle ) \\ = &amp;0. \end{array} " /></p>
<p>For the second part of the claim, since <img src="https://s0.wp.com/latex.php?latex=%7B%28X_i+-+%5Cmu%29%5E2+%5Cleq+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X_i - \mu)^2 \leq 4}" class="latex" title="{(X_i - \mu)^2 \leq 4}" />, we have <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BVar%7D%28Z%27_i%29+%5Cleq+4+%5Ccdot+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X%29+-+%5Cmu+%5C%7C_2%5E2%29+%3D+4%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathrm{Var}(Z'_i) \leq 4 \cdot \mathop{\mathbb E}(\| M(X) - \mu \|_2^2) = 4\alpha^2}" class="latex" title="{\mathrm{Var}(Z'_i) \leq 4 \cdot \mathop{\mathbb E}(\| M(X) - \mu \|_2^2) = 4\alpha^2}" />. The final part of the claim follows from the fact that every entry of <img src="https://s0.wp.com/latex.php?latex=%7BM%28X_%7B%5Csim+i%7D%29+-+%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X_{\sim i}) - \mu}" class="latex" title="{M(X_{\sim i}) - \mu}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX_i+-+%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i - \mu}" class="latex" title="{X_i - \mu}" /> is bounded by <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> in absolute value, and <img src="https://s0.wp.com/latex.php?latex=%7BZ%27_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z'_i}" class="latex" title="{Z'_i}" /> is a sum of <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> such entries, so its absolute value is always at most <img src="https://s0.wp.com/latex.php?latex=%7B4d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4d}" class="latex" title="{4d}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>The next claim says that, because <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> is differentially private, <img src="https://s0.wp.com/latex.php?latex=%7BZ_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_i}" class="latex" title="{Z_i}" /> has similar expectation to <img src="https://s0.wp.com/latex.php?latex=%7BZ%27_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z'_i}" class="latex" title="{Z'_i}" />, and thus its expectation is also small.</p>
<blockquote>
<p><b>Claim 2</b> <em><a name="clmmean-lb-2"></a> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+Z_i%29+%5Cleq+4n%5Calpha+%5Cepsilon+%2B+8n+%5Cdelta+d.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) \leq 4n\alpha \epsilon + 8n \delta d.}" class="latex" title="{\mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) \leq 4n\alpha \epsilon + 8n \delta d.}" /> </em></p>
</blockquote>
<p><em>Proof:</em> The proof is a direct calculation using the following inequality, whose proof is relatively simple using the definition of differential privacy:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D%28Z_i%29+%5Cleq+%5Cmathop%7B%5Cmathbb+E%7D%28Z%27_i%29+%2B+2%5Cepsilon+%5Csqrt%7B%5Cmathrm%7BVar%7D%28Z%27_i%29%7D+%2B+2%5Cdelta+%5C%7C+Z%27_i+%5C%7C_%5Cinfty.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}(Z_i) \leq \mathop{\mathbb E}(Z'_i) + 2\epsilon \sqrt{\mathrm{Var}(Z'_i)} + 2\delta \| Z'_i \|_\infty. " class="latex" title="\displaystyle \mathop{\mathbb E}(Z_i) \leq \mathop{\mathbb E}(Z'_i) + 2\epsilon \sqrt{\mathrm{Var}(Z'_i)} + 2\delta \| Z'_i \|_\infty. " /></p>
<p>Given the inequality and Claim <a href="https://kamathematics.wordpress.com/feed/#clmmean-lb-1">1</a>, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D%28Z_i%29+%5Cleq+0+%2B+%282%5Cepsilon%29%282%5Calpha%29+%2B+%282%5Cdelta%29%282d%29+%3D+4%5Cepsilon+%5Calpha+%2B+8+%5Cdelta+d+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}(Z_i) \leq 0 + (2\epsilon)(2\alpha) + (2\delta)(2d) = 4\epsilon \alpha + 8 \delta d . " class="latex" title="\displaystyle \mathop{\mathbb E}(Z_i) \leq 0 + (2\epsilon)(2\alpha) + (2\delta)(2d) = 4\epsilon \alpha + 8 \delta d . " /></p>
<p>The claim now follows by summing over all <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>The final claim says that, because <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> is accurate, the expected sum of the random variables <img src="https://s0.wp.com/latex.php?latex=%7BZ_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_i}" class="latex" title="{Z_i}" /> is large.</p>
<blockquote>
<p><b>Claim 3</b> <em> <a name="clmmean-lb-3"></a> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+Z_i%29+%5Cgeq+%5Cfrac%7Bd%7D%7B3%7D+-+%5Calpha%5E2.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) \geq \frac{d}{3} - \alpha^2.}" class="latex" title="{\mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) \geq \frac{d}{3} - \alpha^2.}" /> </em></p>
</blockquote>
<p>The proof relies on the following key lemma, whose proof we omit.</p>
<blockquote>
<p><b>Lemma 6 (Fingerprinting Lemma [<a href="https://kamathematics.wordpress.com/feed/#BSU17">BSU17</a>])</b><em> <a name="lemfp"></a> If <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu+%5Cin+%5B%5Cpm+1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu \in [\pm 1]}" class="latex" title="{\mu \in [\pm 1]}" /> is sampled uniformly, <img src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cdots%2CX_n+%5Cin+%5C%7B%5Cpm+1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_1,\dots,X_n \in \{\pm 1\}^{n}}" class="latex" title="{X_1,\dots,X_n \in \{\pm 1\}^{n}}" /> are sampled independently with mean <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu}" class="latex" title="{\mu}" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bf+%5Ccolon+%5C%7B%5Cpm+1%5C%7D%5En+%5Crightarrow+%5B%5Cpm+1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f \colon \{\pm 1\}^n \rightarrow [\pm 1]}" class="latex" title="{f \colon \{\pm 1\}^n \rightarrow [\pm 1]}" /> is any function, then </em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%2CX%7D%28%28f%28X%29+-+%5Cmu%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28X_i+-+%5Cmu%29%29+%5Cgeq+%5Cfrac%7B1%7D%7B3%7D+-+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%2CX%7D%28%28f%28X%29+-+%5Cmu%29%5E2%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{\mu,X}((f(X) - \mu) \cdot \sum_{i=1}^{n} (X_i - \mu)) \geq \frac{1}{3} - \mathop{\mathbb E}_{\mu,X}((f(X) - \mu)^2). " class="latex" title="\displaystyle \mathop{\mathbb E}_{\mu,X}((f(X) - \mu) \cdot \sum_{i=1}^{n} (X_i - \mu)) \geq \frac{1}{3} - \mathop{\mathbb E}_{\mu,X}((f(X) - \mu)^2). " /></p>
<p> </p>
</blockquote>
<p>The lemma is somewhat technical, but for intuition, consider the case where <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%7D+X_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X) = \frac{1}{n}\sum_{i} X_i}" class="latex" title="{f(X) = \frac{1}{n}\sum_{i} X_i}" /> is the empirical mean. In this case we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%2CX%7D%28%28f%28X%29+-+%5Cmu%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+%28X_i+-+%5Cmu%29%29+%3D%7B%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%7D%28%5Cfrac%7B1%7D%7Bn%7D+%5Csum_i+%5Cmathop%7B%5Cmathbb+E%7D_%7BX%7D%28+%28X_i+-+%5Cmu%29%5E2%29+%29+%3D%7B%7D+%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Cmu%7D%28%5Cmathrm%7BVar%7D%28X_i%29%29+%3D+%5Cfrac%7B1%7D%7B3%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} \mathop{\mathbb E}_{\mu,X}((f(X) - \mu) \cdot \sum_{i=1}^n (X_i - \mu)) ={} \mathop{\mathbb E}_{\mu}(\frac{1}{n} \sum_i \mathop{\mathbb E}_{X}( (X_i - \mu)^2) ) ={} \mathop{\mathbb E}_{\mu}(\mathrm{Var}(X_i)) = \frac{1}{3}. \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} \mathop{\mathbb E}_{\mu,X}((f(X) - \mu) \cdot \sum_{i=1}^n (X_i - \mu)) ={} \mathop{\mathbb E}_{\mu}(\frac{1}{n} \sum_i \mathop{\mathbb E}_{X}( (X_i - \mu)^2) ) ={} \mathop{\mathbb E}_{\mu}(\mathrm{Var}(X_i)) = \frac{1}{3}. \end{array} " /></p>
<p>The lemma says that, when <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mu}" class="latex" title="{\mu}" /> is sampled this way, then any modification of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> that reduces the correlation between <img src="https://s0.wp.com/latex.php?latex=%7Bf%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(X)}" class="latex" title="{f(X)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+X_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_i X_i}" class="latex" title="{\sum_i X_i}" /> will increase the mean-squared-error of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> proportionally.</p>
<p>We now prove Claim <a href="https://kamathematics.wordpress.com/feed/#clmmean-lb-3">3</a>.</p>
<p><em>Proof:</em> We can apply the lemma to each coordinate of the estimate <img src="https://s0.wp.com/latex.php?latex=%7BM%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M(X)}" class="latex" title="{M(X)}" />.</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brll%7D+%5Cmathop%7B%5Cmathbb+E%7D%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+Z_i%29+%3D%7B%7D+%26%5Cmathop%7B%5Cmathbb+E%7D%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cleft%5Clangle+M%28X%29+-+%5Cmu%2C+X_i+-+%5Cmu+%5Cright%5Crangle%29+%5C%5C+%3D%7B%7D+%26%5Csum_%7Bj%3D1%7D%5E%7Bd%7D+%5Cmathop%7B%5Cmathbb+E%7D%28%28M%5Ej%28X%29+-+%5Cmu%5Ej%29%5Ccdot+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28X_i%5Ej+-+%5Cmu%5Ej%29%29+%5C%5C+%5Cgeq%7B%7D+%26%5Csum_%7Bj%3D1%7D%5E%7Bd%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B3%7D+-+%5Cmathop%7B%5Cmathbb+E%7D%28%28M%5Ej%28X%29+-+%5Cmu%5Ej%29%5E2%29+%5Cright%29+%5C%5C+%3D%7B%7D+%26%5Cfrac%7Bd%7D%7B3%7D+-+%5Cmathop%7B%5Cmathbb+E%7D%28%5C%7C+M%28X%29+-+%5Cmu+%5C%7C_2%5E2%29+%3D%7B%7D+%5Cfrac%7Bd%7D%7B3%7D+-+%5Calpha%5E2.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rll} \mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) ={} &amp;\mathop{\mathbb E}(\sum_{i=1}^{n} \left\langle M(X) - \mu, X_i - \mu \right\rangle) \\ ={} &amp;\sum_{j=1}^{d} \mathop{\mathbb E}((M^j(X) - \mu^j)\cdot \sum_{i=1}^{n} (X_i^j - \mu^j)) \\ \geq{} &amp;\sum_{j=1}^{d} \left( \frac{1}{3} - \mathop{\mathbb E}((M^j(X) - \mu^j)^2) \right) \\ ={} &amp;\frac{d}{3} - \mathop{\mathbb E}(\| M(X) - \mu \|_2^2) ={} \frac{d}{3} - \alpha^2. \end{array} " class="latex" title="\displaystyle \begin{array}{rll} \mathop{\mathbb E}(\sum_{i=1}^{n} Z_i) ={} &amp;\mathop{\mathbb E}(\sum_{i=1}^{n} \left\langle M(X) - \mu, X_i - \mu \right\rangle) \\ ={} &amp;\sum_{j=1}^{d} \mathop{\mathbb E}((M^j(X) - \mu^j)\cdot \sum_{i=1}^{n} (X_i^j - \mu^j)) \\ \geq{} &amp;\sum_{j=1}^{d} \left( \frac{1}{3} - \mathop{\mathbb E}((M^j(X) - \mu^j)^2) \right) \\ ={} &amp;\frac{d}{3} - \mathop{\mathbb E}(\| M(X) - \mu \|_2^2) ={} \frac{d}{3} - \alpha^2. \end{array} " /></p>
<p>The inequality is Lemma <a href="https://kamathematics.wordpress.com/feed/#lemfp">6</a>. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>Combining Claims <a href="https://kamathematics.wordpress.com/feed/#clmmean-lb-2">2</a> and <a href="https://kamathematics.wordpress.com/feed/#clmmean-lb-3">3</a> gives</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7Bd%7D%7B3%7D+-+%5Calpha%5E2+%5Cleq+4n%5Calpha+%5Cepsilon+%2B+8n+%5Cdelta+d.+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \frac{d}{3} - \alpha^2 \leq 4n\alpha \epsilon + 8n \delta d. \ \ \ \ \ (6)" class="latex" title="\displaystyle \frac{d}{3} - \alpha^2 \leq 4n\alpha \epsilon + 8n \delta d. \ \ \ \ \ (6)" /></p>
<p>Now, if <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2+%5Cgeq+%5Cfrac%7Bd%7D%7B6%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2 \geq \frac{d}{6}}" class="latex" title="{\alpha^2 \geq \frac{d}{6}}" /> then we’re done, so we’ll assume that <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2+%5Cleq+%5Cfrac%7Bd%7D%7B6%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2 \leq \frac{d}{6}}" class="latex" title="{\alpha^2 \leq \frac{d}{6}}" />. Further, by our assumption on the value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />, <img src="https://s0.wp.com/latex.php?latex=%7B8n+%5Cdelta+d+%5Cleq+%5Cfrac%7Bd%7D%7B12%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8n \delta d \leq \frac{d}{12}}" class="latex" title="{8n \delta d \leq \frac{d}{12}}" />. In this case we can rearrange terms and square both sides to obtain</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Calpha%5E2+%5Cgeq%7B%7D+%5Cfrac%7B1%7D%7B16+%5Cepsilon%5E2+n%5E2%7D+%5Cleft%28%5Cfrac%7Bd%7D%7B3%7D+-+%5Calpha%5E2+-+8+n%5Cdelta+d%5Cright%29%5E2+%5Cgeq+%5Cfrac%7B1%7D%7B16+%5Cepsilon%5E2+n%5E2%7D+%5Cleft%28%5Cfrac%7Bd%7D%7B12%7D%5Cright%29%5E2+%3D+%5Cfrac%7Bd%5E2%7D%7B2304+%5Cepsilon%5E2+n%5E2%7D.+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \alpha^2 \geq{} \frac{1}{16 \epsilon^2 n^2} \left(\frac{d}{3} - \alpha^2 - 8 n\delta d\right)^2 \geq \frac{1}{16 \epsilon^2 n^2} \left(\frac{d}{12}\right)^2 = \frac{d^2}{2304 \epsilon^2 n^2}. \ \ \ \ \ (7)" class="latex" title="\displaystyle \alpha^2 \geq{} \frac{1}{16 \epsilon^2 n^2} \left(\frac{d}{3} - \alpha^2 - 8 n\delta d\right)^2 \geq \frac{1}{16 \epsilon^2 n^2} \left(\frac{d}{12}\right)^2 = \frac{d^2}{2304 \epsilon^2 n^2}. \ \ \ \ \ (7)" /></p>
<p>Combining the two cases for <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2}" class="latex" title="{\alpha^2}" /> gives <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E2+%5Cgeq+%5Cmin%5C%7B+%5Cfrac%7Bd%7D%7B6%7D%2C+%5Cfrac%7Bd%5E2%7D%7B2304+%5Cepsilon%5E2+n%5E2%7D+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha^2 \geq \min\{ \frac{d}{6}, \frac{d^2}{2304 \epsilon^2 n^2} \}}" class="latex" title="{\alpha^2 \geq \min\{ \frac{d}{6}, \frac{d^2}{2304 \epsilon^2 n^2} \}}" />, as desired.</p>
<p><b>Bibliography</b></p>
<p>[BDRS18]<a name="BDRS18"></a> Mark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC ’18.</p>
<p>[BS16]<a name="BS16"></a> Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. TCC ’16-B.</p>
<p>[BSU17]<a name="BSU17"></a> Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in differential privacy. SODA ’17.</p>
<p>[BUV14]<a name="BUV14"></a> Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate differential privacy. STOC ’14.</p>
<p>[CSS11]<a name="CSS11"></a> T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transactions on Information and System Security, 14(3):26, 2011.</p>
<p>[CWZ19]<a name="CWZ19"></a> T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. arXiv, 1902.04495, 2019.</p>
<p>[DMNS06]<a name="DMNS06"></a> Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. TCC ’06.</p>
<p>[DNPR10]<a name="DNPR10"></a> Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual observation. STOC ’10.</p>
<p>[DR16]<a name="DR16"></a> Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. arXiv, 1603.01887, 2016.</p>
<p>[DRS19]<a name="DRS19"></a> Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. arXiv, 1905.02383, 2019.</p>
<p>[DSSU17]<a name="DSSU17"></a> Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. FOCS ’15.</p>
<p>[DSSUV15]<a name="DSSUV15"></a> Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application, 4:61–84, 2017.</p>
<p>[FRY10]<a name="FRY10"></a> Stephen E. Fienberg, Alessandro Rinaldo, and Xiaolin Yang. Differential privacy and the risk-utility tradeoff for multi-dimensional contingency tables. PSD ’10.</p>
<p>[KLSU19]<a name="KLSU19"></a> Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional distributions. COLT ’19.</p>
<p>[Mir17]<a name="Mir17"></a> Ilya Mironov. Rényi differential privacy. CSF ’17.</p>
<p>[Ste16]<a name="Ste16"></a> Thomas Alexander Steinke. Upper and Lower Bounds for Privacy and Adaptivity in Algorithmic Data Analysis. PhD thesis, 2016.</p>
<p>[SU17a]<a name="SU17a"></a> Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. Journal of Privacy and Confidentiality, 7(2), 2017.</p>
<p>[SU17b]<a name="SU17b"></a> Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. FOCS ’17.</p>
<p>[VS09]<a name="VS09"></a> Duy Vu and Aleksandra Slavković. Differential privacy for clinical trial data: Preliminary evaluations. ICDMW ’09.</p>


<p></p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2020/04/14/a-primer-on-private-statistics-part-i/"><span class="datestr">at April 14, 2020 02:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4732">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4732">John Horton Conway (1937-2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (4/13):</span></strong> Check out the comments on this post for some wonderful firsthand Conway stories.  Or for the finest tribute I’ve seen so far, see a MathOverflow thread entitled <a href="https://mathoverflow.net/questions/357197/conways-lesser-known-results">Conway’s lesser known results</a>.  Virtually everything there is a gem to be enjoyed by amateurs and experts alike.  And if you actually click through to any of Conway’s papers … oh my god, what a rebuke to the way most of us write papers!</p>



<p><a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John Horton Conway</a>, one of the great mathematicians and math communicators of the past half-century, has died at age 82.</p>



<blockquote class="wp-block-quote"><p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> John’s widow, Diana Conway, left a <a href="https://www.scottaaronson.com/blog/?p=4732#comment-1836789">nice note</a> in the comments section of this post.  I wish to express my condolences to her and to all of the Conway children and grandchildren.</p></blockquote>



<p>Just a week ago, as part of her quarantine homeschooling, I introduced my seven-year-old daughter Lily to the famous <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a>.  Compared to the other stuff we’ve been doing, like fractions and right triangles and the distributive property of multiplication, the Game of Life was a <em>huge</em> hit: Lily spent a full hour glued to the screen, watching the patterns evolve, trying to guess when they’d finally die out.  So this first-grader knew who John Conway was, when I told her the sad news of his passing.</p>



<p>“Did he die from the coronavirus?” Lily immediately asked.</p>



<p>“I doubt it, but I’ll check,” I said.</p>



<p>Apparently it <em>was</em> the coronavirus.  Yes, the self-replicating snippet of math that’s now terrorizing the whole human race, in part because those in power couldn’t or wouldn’t understand exponential growth.  Conway is perhaps the nasty bugger’s most distinguished casualty so far.</p>



<p>I regrettably never knew Conway, although I did attend a few of his wildly popular and entertaining lectures.  His <a href="https://www.amazon.com/Book-Numbers-John-H-Conway/dp/038797993X">The Book of Numbers</a> (coauthored with Richard Guy, who himself recently passed away at age 103) made a huge impression on me as a teenager.  I worked through every page, gasping at gems like e<sup>π√163</sup> (“no, you can’t be serious…”), embarrassed to be learning so much from a “fun, popular” book but grateful that my ignorance of such basic matters was finally being remedied.</p>



<p>A little like Pascal with his triangle or Möbius with his strip, Conway was fated to become best-known to the public not for his deepest ideas but for his most accessible—although for Conway, a principal puzzle-supplier to Martin Gardner for decades, the boundary between the serious and the recreational may have been more blurred than for any other contemporary mathematician.  Conway invented the <a href="https://en.wikipedia.org/wiki/Surreal_number">surreal number system</a>, discovered three of the 26 <a href="https://en.wikipedia.org/wiki/Sporadic_group">sporadic simple groups</a>, was instrumental in the discovery of <a href="https://en.wikipedia.org/wiki/Monstrous_moonshine">monstrous moonshine</a>, and did many other things that bloggers more qualified than I will explain in the coming days.</p>



<p>Closest to my wheelhouse, Conway together with Simon Kochen waded into the foundations of quantum mechanics in 2006, with their <a href="https://en.wikipedia.org/wiki/Free_will_theorem">“Free Will Theorem”</a>—a result Conway liked to summarize provocatively as “if human experimenters have free will, then so do the elementary particles they measure.”  I confess that I wasn’t a fan at the time—partly because Conway and Kochen’s theorem was really about “freshly-generated randomness,” rather than free will in any sense related to agency, but also partly because I’d already known the conceptual point at issue, but had considered it folklore (see, e.g., my <a href="https://arxiv.org/abs/quant-ph/0206089">2002 review</a> of Stephen Wolfram’s <em>A New Kind of Science</em>).  Over time, though, the “Free Will Theorem” packaging grew on me.  Much like with the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning Theorem</a> and other simple enormities, sometimes it’s worth making a bit of folklore so memorable and compelling that it will never be folklore again.</p>



<p>At a lecture of Conway’s that I attended, someone challenged him that his proposed classification of knots worked only in special cases.  “Oh, of course, this only classifies 0% of knots—but 0% is a start!” he immediately replied, to roars from the audience.  That’s just one line that I remember, but nearly everything out of his mouth was of a similar flavor.  I noted that part of it was in the delivery.</p>



<p>As a mathematical jokester and puzzler who could delight and educate anyone from a Fields Medalist to a first-grader, Conway had no equal.  For no one else who I can think of, even going back centuries and millennia, were entertainment and mathematical depth so closely marbled together.  Here’s to a well-lived Life.</p>



<p>Feel free to share your own Conway memories in the comments.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4732"><span class="datestr">at April 12, 2020 07:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=43">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/04/11/friday-april-17-shachar-lovett-from-uc-san-diego/">Friday, April 17 — Shachar Lovett from UC San Diego</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The third Foundations of Data Science virtual talk will take place next Friday, April 17th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Shachar Lovett</strong> from UC San Diego will speak about “<em>The power of asking more informative questions about the data</em>”.</p>



<p><strong>Abstract</strong>: Many supervised learning algorithms (such as deep learning) need a large collection of labelled data points in order to perform well. However, what is easy to get are large amounts of unlabelled data. Labeling data is an expensive procedure, as it usually needs to be done manually, often by a domain expert. Active learning provides a mechanism to bridge this gap. Active learning algorithms are given a large collection of unlabelled data points. They need to smartly choose a few data points to query their label. The goal is then to automatically infer the labels of many other data points.</p>



<p>In this talk, we will explore the option of giving active learning algorithms additional power, by allowing them to have richer interaction with the data. We will see how allowing for even simple types of queries, such as comparing two data points, can exponentially improve the number of queries needed in various settings. Along the way, we will see interesting connections to both geometry and combinatorics, and a surprising application to fine grained complexity.</p>



<p>Based on joint works with Daniel Kane, Shay Moran and Jiapeng Zhang.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/04/11/friday-april-17-shachar-lovett-from-uc-san-diego/"><span class="datestr">at April 11, 2020 08:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16921">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/">Nina Balcan Wins</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Congrats and More</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/unknown-137/" rel="attachment wp-att-16924"><img width="170" alt="" class="alignright  wp-image-16924" src="https://rjlipton.files.wordpress.com/2020/04/unknown.jpeg?w=170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ CMU ]</font></td>
</tr>
</tbody>
</table>
<p>
Nina Balcan is a leading researcher in the theory of machine learning. Nina is at Carnegie-Mellon and was previously at Georgia Tech—it was a major loss to have her leave Tech.</p>
<p>
Today we applaud her winning the ACM Hopper Award.<br />
<span id="more-16921"></span></p>
<p>
ACM President Cherri Pancake <a href="https://www.ml.cmu.edu/news/news-archive/2020/april/machine-learning-professor-balcan-receives-acm-grace-hopper-award.html">says</a>: </p>
<blockquote><p><b> </b> <em> Although she is still in the early stages of her career, she has already established herself as the world leader in the theory of how AI systems can learn with limited supervision. More broadly, her work has realigned the foundations of machine learning, and consequently ushered in many new applications that have brought about leapfrog advances in this exciting area of artificial intelligence. </em>
</p></blockquote>
<p>
</p><p></p><h2> The Hopper Award </h2><p></p>
<p></p><p>
The ACM Grace Murray Hopper <a href="https://awards.acm.org/hopper/award-winners">Award</a> is given to: An outstanding young computer professional, on the basis of a single major contribution before the age of 35. Here are five of the most recent winners: </p>
<ul>
<li>
Constantinos Daskalakis, 	(<a href="https://awards.acm.org/award_winners/daskalakis_4121823">2018</a>)	 <p></p>
</li><li>
Michael Freedman, 		(<a href="https://awards.acm.org/award_winners/freedman_6665293">2018</a>)	 <p></p>
</li><li>
Amanda Randles, 		(<a href="https://awards.acm.org/award_winners/randles_0365390">2017</a>)	 <p></p>
</li><li>
Jeffrey Heer, 			(<a href="https://awards.acm.org/award_winners/heer_1520709">2016</a>)	 <p></p>
</li><li>
Brent Waters,			(<a href="https://awards.acm.org/award_winners/waters_3058089.cfm">2015</a>)
</li></ul>
<p>
</p><p></p><h2> Nina’s Contribution </h2><p></p>
<p></p><p>
The Hopper award says it is for a “single” major contribution. I believe that Nina is almost a disproof of this statement: I fail to see how she only did one major contribution. In fact, the <a href="https://awards.acm.org/hopper">citation</a> lists three. In any event I thought we might look at one of her top results on learning. It is a <a href="http://www.cs.cmu.edu/~ninamf/papers/agnostic-active.pdf">paper</a> from 2006 with over 400 citations. The ttile is “Agnostic Active Learning” and is joint with Alina Beygelzimer and John Langford.</p>
<p>
<em>Active learning</em> follows a classic idea in computer theory: Making a protocol interactive can often decrease the cost, and almost always makes the protocol more complex to understand. In active learning one is given unlabeled examples. As usual the goal is to classify the samples. However, as the samples are unlabelled, the learning can ask for labels for elected samples—this is the active part of the learning. As you might imagine asking for labels has a cost, so the learner strives to ask for the fewest labels possible. </p>
<p>
The savings can be large when the labels are perfect—that is, noise-free. In general it is much more complex to understand when active learning helps. Nina’s work found examples where noise can be tamed. Her award citation says:</p>
<blockquote><p><b> </b> <em> Balcan established performance guarantees for active learning that hold even in challenging cases when “noise” is present in the data. These guarantees hold under arbitrary forms of noise, that is, anything that distorts or corrupts the data. This can include anything from a blurry photo, a unit of data that is improperly labeled, meaningless information, or data that the algorithm cannot interpret. </em>
</p></blockquote>
<p></p><p>
See her papers for the details. </p>
<p>
</p><p></p><h2> Other Awards </h2><p></p>
<p></p><p>
There are various awards for computer scientists, many are from the ACM. Since Alan Perlis won the first Turing award, there have been 69 more winners. Only three have been women:</p>
<ul>
<li>
Shafi Goldwasser, (2012) <p></p>
</li><li>
Barbara Liskov, (2008) <p></p>
</li><li>
Frances Allen, (2006)
</li></ul>
<p>Here is another quote from the president of the ACM: </p>
<blockquote><p><b> </b> <em> We typically receive one woman nominee [for the Turing Award] every five years. It’s very disturbing. </em>
</p></blockquote>
<p></p><p>
The number of nominations is too small. There are plenty of strong women candidates for the Turing award, and for other awards. We need to do a better job. See <a href="https://slate.com/technology/2020/01/turing-award-acm-women-recipients.html">this</a> for more thoughts on this issue.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We do not know about the situation with nominations for the Hopper Award. Nina is only the seventh woman to win since 1971, but four of the last ten Hopper Award winners have been women. How can we recognize more women under 35 who are doing great work?</p>
<p>
Again we congratulate Nina Balcan on her richly deserved honor. </p>
<p>[Fixed typo “Congrats” ]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/"><span class="datestr">at April 10, 2020 01:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/04/09/experts-shmexperts/">Experts shmexperts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(If you’re not already following him, I highly recommend reading <a href="https://lucatrevisan.wordpress.com/">Luca Trevisan’s dispatches from Milan</a>, much more interesting than what I write below.)</em></p>



<p>On the topic of my <a href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/">last post</a>, Ross Douthat writes in the New York Times that <a href="https://www.nytimes.com/2020/04/07/opinion/coronavirus-science-experts.html?smid=fb-share&amp;fbclid=IwAR1KIcMblvNsaKaAQTh2vCpB2yARYL8O5TVy9j3ZxSqsXsRAMK2GGcvF81o">“In the fog of coronavirus, there are no experts”</a>, even citing <a href="https://www.scottaaronson.com/blog/?p=4695">Scott Aaronson’s post</a>. Both Aaronson and Douthat make the point that the COVID-19 crisis is so surprising and unprecedented, and experts were so much in the dark, that there is no reason to trust them over non expert “common sense” or “armchair epidemiologists”. <br /><br />It’s true that the “expert models” have significant uncertainty, hardwired constants, noisy data, and dubious assumptions. It is also true that many countries (especially those that didn’t learn from the 2003 SARS epidemic) bungled their initial response. But do we really need to challenge the notion of expertise itself? To what extent was this pandemic not predicted by experts or progressed in ways defying their expectations?</p>



<p>Here is what some of these experts and institutions were saying in the recent past:</p>



<p><em>“The thing I’m most concerned about … is the emergence of a new virus that the body doesn’t have any background experience with, that is very transmissible, highly transmissible from person to person, and has a high degree of morbidity and mortality … a respiratory illness that can spread even before someone is so sick that you want to keep them in bed.”</em>  <a href="https://fivethirtyeight.com/features/dr-fauci-has-been-dreading-a-pandemic-like-covid-19-for-years/">Dr. Anthoni Fauci, 2019</a>.</p>



<p><em>“High-impact respiratory pathogens … pose particular global risks … [they] are spread via respiratory droplets; they can infect a large number of people very quickly and with today’s transportation infrastructure, move rapidly across multiple geographies. … There is insufficient R&amp;D investment and planning for innovative vaccine development and manufacture, broad-spectrum antivirals, … In addition, such a pandemic requires advance planning across multiple sectors … Epidemic control costs would completely overwhelm the current financing arrangements for emergency response.” </em><a href="https://apps.who.int/gpmb/assets/annual_report/GPMB_annualreport_2019.pdf">WHO world at risk report</a>, 2019.</p>



<p><em>“respiratory transmission …. is the transmission route posing the greatest pandemic risk   … [since it] can occur with coughing or simply breathing (in aerosol transmission), making containment much more challenging. …  If a pathogen is capable of causing asymptomatic or mildly symptomatic infections that either do not or only minimally interrupt activities of daily living, many individuals may be exposed. Viruses that cause the common cold, including coronaviruses, have this attribute.”</em> <a href="https://apps.who.int/gpmb/assets/thematic_papers/tr-6.pdf">JHU report</a>, 2019.</p>



<p>As an experiment, I also tried to compare the response of experts and “contrarians” in real time as the novel coronavirus was discovered, trying to see if it’s really the case that, as Douthat says, <em>“up until mid-March you were better off trusting the alarmists of anonymous Twitter than the official pronouncements from the custodians of public health”</em>.  I chose both experts and contrarians that are active on Twitter. I was initially planning to look at several people but due to laziness am just taking Imperial college’s <a href="https://twitter.com/Imperial_JIDEA/">J-IDEA institute</a> for the expert, and <a href="https://twitter.com/robinhanson">Robin Hanson</a> for the contrarian.  I also looked at <a href="https://twitter.com/DouthatNYT">Douthat’s twitter feed</a>, to see if he followed his own advice. Initially I thought I would go all the way to March but have no time so just looked at the period from January 1 till February 14th. I leave any conclusions to the reader.</p>



<h2><strong>January 1-19:</strong> </h2>



<p>(Context: novel coronavirus confirmed in Wuhan, initially unclear if there is human to human transmission – this was confirmed by China on January 20 though suspected before.)</p>



<p>Here is one of the many tweets by Imperial from this period:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Substantial human to human transmission cannot be ruled out – size of novel <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> in Wuhan outbreak likely over 1700 cases. <a href="https://twitter.com/MailOnline?ref_src=twsrc%5Etfw">@MailOnline</a> on <a href="https://twitter.com/MRC_Outbreak?ref_src=twsrc%5Etfw">@MRC_Outbreak</a>, <a href="https://twitter.com/Imperial_JIDEA?ref_src=twsrc%5Etfw">@Imperial_JIDEA</a>, <a href="https://twitter.com/imperialcollege?ref_src=twsrc%5Etfw">@imperialcollege</a> report today.<a href="https://t.co/Iq4hBmx4JL">https://t.co/Iq4hBmx4JL</a> <a href="https://t.co/3n5OMPYNdL">pic.twitter.com/3n5OMPYNdL</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1218295967683874816?ref_src=twsrc%5Etfw">January 17, 2020</a></blockquote></div>
</div></figure>



<p>I didn’t see any tweets from Hanson or Douthat on this topic.</p>



<h2><strong>January 20-31:</strong> </h2>



<p>(Context: first confirmed cases in several countries, including the US, WHO declares emergency in Jan 30, US restricts travel from China on Jan 31. By then there are about 10K confirmed cases and 213 deaths worldwide.)</p>



<p>On January 25th Imperial college estimated the novel coronavirus “R0” parameter as 2.6:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">UPDATE: Transmissibility estimates of <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> <a href="https://twitter.com/hashtag/2019nCoV?src=hash&amp;ref_src=twsrc%5Etfw">#2019nCoV</a> at 2.6<br /><br />Identification &amp; testing potential cases to be as extensive as permitted by healthcare &amp; testing capacity<br /><br /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f530.png" alt="🔰" style="height: 1em;" class="wp-smiley" /><a href="https://t.co/7zpZzG2JOs">https://t.co/7zpZzG2JOs</a><a href="https://twitter.com/neil_ferguson?ref_src=twsrc%5Etfw">@neil_ferguson</a> <a href="https://twitter.com/dr_anne_cori?ref_src=twsrc%5Etfw">@dr_anne_cori</a> <a href="https://twitter.com/SRileyIDD?ref_src=twsrc%5Etfw">@SRileyIDD</a> <a href="https://twitter.com/MarcBaguelin?ref_src=twsrc%5Etfw">@MarcBaguelin</a> <a href="https://twitter.com/IlariaDorigatti?ref_src=twsrc%5Etfw">@IlariaDorigatti</a> <a href="https://t.co/nmhjWsWpfa">pic.twitter.com/nmhjWsWpfa</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1221033477824532480?ref_src=twsrc%5Etfw">January 25, 2020</a></blockquote></div>
</div></figure>



<p>Hanson tweeted approvingly about China’s response and that this situation might help the “more authoritarian” U.S. presidential candidate:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Seems to me the "parasite stress hypothesis of authoritarianism" suggests that if this China Coronavirus ends up being a big deal, that would push US voters toward the more authoritarian presidential candidate. Which one is that?<a href="https://t.co/cWmV2WkroJ">https://t.co/cWmV2WkroJ</a><a href="https://t.co/ysNiZIkfYD">https://t.co/ysNiZIkfYD</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1222184281050697728?ref_src=twsrc%5Etfw">January 28, 2020</a></blockquote></div>
</div></figure>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I doubt we in US would be as fast to restrict travel in the face of such a still-small pandemic. If so, China is to be praised for their better abilities to coordinate in the face of such threats.<a href="https://t.co/Lro5kGVTvz">https://t.co/Lro5kGVTvz</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1220704857922985984?ref_src=twsrc%5Etfw">January 24, 2020</a></blockquote></div>
</div></figure>



<p>Still no tweet from Douthat on this topic though he did say in January 29th that compared to issues in the past the U.S.’s problems in the 2020’s are “problem of decadence” rather than any crisis like the late 1970’s:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Belated, yes, America's problems in '20 are problems of decadence rather than late-1970s crisis, and economically '16 might have been a better year to gamble on Bernie …<a href="https://t.co/3DaHWXC1k0">https://t.co/3DaHWXC1k0</a></p>— Ross Douthat (@DouthatNYT) <a href="https://twitter.com/DouthatNYT/status/1222566830642024448?ref_src=twsrc%5Etfw">January 29, 2020</a></blockquote></div>
</div></figure>



<h2><strong>February 1-14: </strong></h2>



<p>(Context: Diamond princess cruise ship quaranteed, disease gets COVID-19 official name, first death in Europe)</p>



<p>Imperial continues to tweet extensively, including the following early estimates of the case fatality rates:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">UPDATE: <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> <a href="https://twitter.com/hashtag/2019nCoV?src=hash&amp;ref_src=twsrc%5Etfw">#2019nCoV</a> Severity<br /><br /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" alt="➡" style="height: 1em;" class="wp-smiley" />Estimated fatality ratio for infections 1%<br /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" alt="➡" style="height: 1em;" class="wp-smiley" />Estimated CFR for travellers outside mainland China (mix severe &amp; milder cases) 1%-5%<br /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" alt="➡" style="height: 1em;" class="wp-smiley" />Estimated CFR for detected cases in Hubei (severe cases) 18%<br /><br /><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f530.png" alt="🔰" style="height: 1em;" class="wp-smiley" /><a href="https://t.co/7zpZzG2JOs">https://t.co/7zpZzG2JOs</a> <a href="https://t.co/gtmzq1vOhq">pic.twitter.com/gtmzq1vOhq</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1226766907396718597?ref_src=twsrc%5Etfw">February 10, 2020</a></blockquote></div>
</div></figure>



<p>Robin Hanson correctly realizes this is going to spread wide:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Look at this table and tell me is isn't all over China now, beyond recall: <a href="https://t.co/Ne9UgDlx9G">pic.twitter.com/Ne9UgDlx9G</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1227701511469387777?ref_src=twsrc%5Etfw">February 12, 2020</a></blockquote></div>
</div></figure>



<p>Hanson tweets quite a lot about this, including potential social implications. Up to February 13th there is nothing too “contrarian” at this point, but also no information that could not be gotten from the experts:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">My poll so far estimates ~40% chance that <a href="https://twitter.com/hashtag/COVID19?src=hash&amp;ref_src=twsrc%5Etfw">#COVID19</a> infects large % of world. So seems worth it for social scientists to ask themselves: using social sci, what non-obvious predictions can we make about social outcomes in that case?</p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1227656071021334528?ref_src=twsrc%5Etfw">February 12, 2020</a></blockquote></div>
</div></figure>



<p>In February 14 Hansons makes a very contrarian position when he proposes “controlled infection” as a solution:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Though it is a disturbing &amp; extreme option, we should seriously consider deliberately infecting folks with coronavirus, to spread out the number of critically ill people over time, and to ensure that critical infrastructure remains available to help sick. <a href="https://t.co/giIfo8z8v0">https://t.co/giIfo8z8v0</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1228400896507367424?ref_src=twsrc%5Etfw">February 14, 2020</a></blockquote></div>
</div></figure>



<p>To the anticipated “you first” objection he responds <em>“I proposed compensating volunteers via cash or medical priority for associates, &amp; I’d seriously consider such offers.”</em>.  He doesn’t mention that he is much less strapped for cash than some of the would be “volunteers”. </p>



<p>Still no tweet from Douthat about COVID-19 though he does write that we live in an “age of decadence”:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">My Sunday essay excerpts my new book: The Age of Decadence: <a href="https://t.co/3cvLqNVQpv">https://t.co/3cvLqNVQpv</a></p>— Ross Douthat (@DouthatNYT) <a href="https://twitter.com/DouthatNYT/status/1225908487198330880?ref_src=twsrc%5Etfw">February 7, 2020</a></blockquote></div>
</div></figure>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/04/09/experts-shmexperts/"><span class="datestr">at April 09, 2020 07:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4361">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/04/09/the-peak-the-plateau-and-the-phase-two/">The peak, the plateau, and the phase two</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>What has been happening in Italy in the last few days, and what can other Western countries expect in the next week or two?</p>
<p>The national discourse has been obsessed with “The Peak,” that is, the time when things reach their worst point, and start improving after that. For the last several days, all indicators, such as new cases, deaths, and ICU occupancy, have been improving. Apparently, then,  “The Peak” is behind us. Virologists have been cautious to say that “peak” is the wrong mountain metaphor to use, and that we have rather reached a “plateau” in which things will change very slowly for a while. </p>
<p>Below is the number of confirmed covid-19 deaths in Italy updated with today’s data, showing that we reached the plateau a couple of weeks ago, meaning that the number of new cases started to plateau about a month ago, when the lockdown started.</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/04/italy-deaths.png?w=584" alt="italy-deaths" class="alignnone size-full wp-image-4365" /></p>
<p>The data from New York City continues to track the data from Lombardy, so NYC should be just a few days away from its own plateau, if the match continues.</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/04/lombardy-nyc.png?w=584" alt="lombardy-nyc" class="alignnone size-full wp-image-4366" /></p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/04/lombardy-nyc-daily.png?w=584" alt="lombardy-nyc-daily" class="alignnone size-full wp-image-4367" /></p>
<p>Given all this, people have been wondering when and how we will get out of the lockdown, and reach what everybody has been calling the “Phase Two” of this emergency.</p>
<p><span id="more-4361"></span></p>
<p>The lockdown is set to expire this coming Monday, and it is expected that tomorrow or Saturday the prime minister will announce new measures. (Perhaps, according to precedent, he will do so on Sunday night.) It is expected that the stay-at-home order will be extended to early May, or even mid-May, but that the definition of “essential activities” will be relaxed to allow some manufacturing to restart sooner.</p>
<p>Meanwhile, an infrastructure to isolate new cases and trace their contacts, which should have been frantically under construction over the last month, is still non-existent. Last week, the government nominated a committee of 70+ experts to “begin thinking about mapping out possibilities” for what such an infrastructure might be like.</p>
<p>To be honest, I am not too confident that the “Phase Two” will be organized with Taiwanese, or even Korean, efficiency, and my only hope is that the number of cases in Lombardy has been so under-reported that we may already be close to herd immunity.</p>
<p>This is probably not the case, but not by a wide margin. The Italian Institute of Statistics has released 2019 vs 2020 all-cause mortality data from a representative sample of Italian towns. Apparently, during the worst days of March, all-cause mortality roughly doubled nation-wide, while the reported deaths caused by covid-19 account for only about half of the excess deaths. This might mean that there have been 20,000 covid-19 deaths and maybe 2 million infected people out of 10 million in Lombardy. A study of the Imperial College estimates, at the high end, that 6 million Italians have been infected, and since Lombardy’s  data has consistently accounted for half the national data on all measures, it would mean 3 million infected people in Lombardy, or 30%, which is within a factor of two of what might suffice for herd immunity. In any case we will not know until there is a randomized serologic study, which is something else for which experts are almost ready to begin mapping out ways of thinking about how to explore plans for … </p>
<p>What will life be like in “Phase Two”? If the epidemic continues at a slow burn, will we have to continue to keep a one-meter distance from strangers? Will trains and planes run with only every third seat occupied? Will tickets cost three times as much? Will beaches be open during the summer? Will there be riots if Italians are not allowed to go to the beach in August? Apart from the last question, whose answer is obviously yes, everything is up in the air.</p>
<p>What about me, after 32 days of lockdown? I was already in need of a haircut at the end of February, and lately the hair situation had become untenable, so I used my beard trimmer to blindly cut my hair. Mistakes were made, but I would not even rate it among my top ten worst haircuts ever. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/04/09/the-peak-the-plateau-and-the-phase-two/"><span class="datestr">at April 09, 2020 04:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4719">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4719">When events make craziness sane</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This post is simply to say the following (and thereby to make it common knowledge that I said it, and that I no longer give a shit who thinks less of me for saying it):</p>



<p>If the pandemic has radicalized you, I won’t think that makes you crazy.  It’s radicalized me, noticeably shifted my worldview.  And in some sense, I no more apologize for that, than I apologize for my worldview presumably differing from what it would’ve been in some parallel universe with no WWII.</p>



<p>If you suspect that all those earnest, well-intentioned plans and slogans about “flattening the curve” are wonderful and essential, but still, <em>“flattening” is only a desperate gambit to buy some time and nothing more</em>; still, flattening or no flattening, the fundamentals of the situation are that either</p>



<p>(1) a vaccine or cure gets discovered and deployed, or else</p>



<p>(2) we continue in quasi-lockdown mode for the rest of our lives, or else</p>



<p>(3) the virus spreads to the point where it definitely kills some people you know,</p>



<p>—if you suspect this, then at least in my book you’re not crazy.  I suspect the same.</p>



<p>If you still don’t understand, no matter how patiently it’s explained to you, why ~18 months is the <em>absolute bare minimum</em> needed to get a vaccine out; if all the talk of Phase 1, 2, and 3 trials and the need to learn more about rare side effects and so forth seems hard to square with the desperate world war that this is; if you wonder whether the Allied commanders and Allied medical authorities in WWII, transported to the present, would <em>agree</em> that 18 months is the bare minimum, or whether they’d already be distributing vaccines a month ago that probably work well enough and do bounded damage if they don’t—I hereby confess that I don’t understand it either.</p>



<p>If you wonder how the US will possibly hold an election in November that the world won’t rightly consider a sham—given that the only safe way will be universal vote-by-mail, but Trump and his five Vichy justices will never allow it—know that I wonder this too.</p>



<p>If you think that all those psychiatrists now doing tele-psychiatry should tell their patients, “listen, I’ve been noticing an unhealthy absence of panic attacks, obsessions about the government trying to kill your family, or compulsive disinfecting of doorknobs, so I think we’d better up your dose of pro-anxiety medication”—I’m with you.</p>



<p>If you see any US state that wants to avoid &gt;2% deaths, being pushed to the brink of openly defying the FDA, smuggling in medical supplies to escape federal confiscation, using illegal tests and illegal masks and illegal ventilators and illegal everything else, and you also see military commanders getting fired for going outside the chain of command to protect their soldiers’ lives, and you wonder whether this is the start of some broader dissolution of the Union—well, <em>I</em> don’t intend to repeat the mistake of underestimating this crisis.</p>



<p>If you think that the feds who <em>literally confiscate medical supplies before they can reach the hospitals</em>, might as well just shoot the patients as they’re wheeled into the ICU and say “we’re sorry, but this action was obligatory under directive 48c(7)”—I won’t judge you for feeling that way. </p>



<p>If you feel like, while there are still pockets of brilliance and kindness and inspiration and even heroism all over US territory, still, as a federal entity the United States <em>effectively no longer exists or functions</em>, at least not if you treat “try to stop the mass death of the population” as a nonnegotiable component of the “life, liberty, and happiness” foundation for the nation’s existence—if you think this, I won’t call you crazy.  I feel more like a citizen of nowhere every day.</p>



<p>If you’d jump, should the opportunity arise (as it won’t), to appoint Bill Gates as temporary sovereign for as long as this crisis lasts, and thereafter hold a new Constitutional Convention to design a stronger democracy, attempting the first-ever Version 2.0 (as opposed to 1.3, 1.4, etc.) of the American founders’ vision, this time with <em>even more</em> safeguards against destruction by know-nothings and demagogues—if you’re in for that, I don’t think you’re crazy.  I’m wondering where to sign up.</p>



<p>Finally, if you’re one of the people who constantly emails me wrong P=NP proofs or local hidden-variable explanations of quantum mechanics … sorry, I still think you’re crazy.  That stuff hasn’t been affected.</p>



<p>Happy Passover and Easter!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4719"><span class="datestr">at April 08, 2020 02:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3470">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/04/07/call-for-papers-the-2nd-workshop-on-behavioral-economics-and-computation/">Call for Papers: The 2nd Workshop on Behavioral Economics and Computation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div><span><a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br />
July 17, 2020, Budapest, Hungary<br />
At the 21st ACM Conference on Economics and Computation (ACM EC ’20)</span></div>
<div><span>**In the event the in-person conference does not happen due to the COVID-19 pandemic, we will hold the workshop virtually.</span></div>
<div></div>
<div><strong><span style="color: #ff0000;">SUBMISSIONS DUE May 18, 2020, 11:59pm PDT.</span></strong></div>
<div></div>
<div></div>
<div><strong>Call for Papers: the 2nd Workshop on Behavioral Economics and Computation</strong></div>
<div></div>
<div><span>We solicit research contributions and participants for the 2nd Workshop on Behavioral Economics and Computation, to be held in conjunction with the Twenty-First ACM Conference on Economics and Computation (ACM EC ’20). </span></div>
<div></div>
<div><span>Based on the successful workshop last year, we aim to bring together again researchers and practitioners from diverse subareas of EC, who are interested in the intersection of human economic behavior and computation, to share new results and to discuss future directions for behavioral research related to economics and computation. It will be a full-day workshop, and will feature invited speakers, contributed paper presentations and a panel discussion. </span></div>
<div></div>
<div><span>The gap between rationality-based analysis that assumes utility-maximizing agents and actual human behavior in the real world has been well recognized in economics, psychology and other social sciences. In recent years, there has been growing interest in conducting behavioral research across many of the sub-areas related to economics and computation to address this gap. In one direction, some of these studies leverage insights on human decision making from the behavioral economics and psychology literature to study economic and computational systems with human users. In the other direction, computational tools are used to study and gain insights on human behavior and a data-driven approach is used to learn behavior models from user-generated data.</span></div>
<div><span><br />
The 2nd Behavioral EC workshop aims to provide a venue for researchers and practitioners from diverse fields, including but not limited to computer science, economics, psychology and sociology, to exchange ideas related to behavioral research in economics and computation. In addition to sharing new results, we hope the workshop will foster a lively discussion of future directions and methodologies for behavioral research related to economics and computation as well as fruitful cross-pollination of behavioral economics, cognitive psychology and computer science. </span></div>
<div><span><br />
We welcome studies at the intersection of economic behavior and computation from a rich set of theoretical, experimental and empirical perspectives. The topics of interest for the workshop are behavioral research in all settings covered by EC, including but not limited to:</span></div>
<ul>
<li><span>Behavioral mechanism design and applied mechanism design</span></li>
<li><span>Boundedly-rational models of economic decision making</span></li>
<li><span>Empirical studies of human economic behavior</span></li>
<li><span>Model evaluation and selection based on behavioral data</span></li>
<li><span>Data-driven modelling</span></li>
<li><span>Online prediction markets, online experiments, and crowdsourcing platforms</span></li>
<li><span>Hybrid human-machine systems</span></li>
<li><span>Models and experiments about social considerations (e.g. fairness and trust) in decision making</span></li>
<li><span>Methods for behavioral EC: information aggregation, probability elicitation, quality control</span></li>
</ul>
<div></div>
<div></div>
<div><span><strong>Submission Instructions</strong><br />
</span></div>
<div></div>
<div><span style="color: #ff0000;">Submission deadline: May 18, 2020, 11:59pm PDT.</span></div>
<div><span style="color: #ff0000;">Notification: June 11, 2020</span></div>
<div></div>
<div><span>All submissions will be peer reviewed. We will give priority to new (unpublished) research papers but will also consider ongoing research and recently published papers that may be of interest to the workshop audience. For submissions of published papers, authors must clearly state the venue of publication. Position papers and panel discussion proposals are also welcome. Papers will be reviewed for relevance, significance, originality, research contribution, and likelihood to catalyze discussion. </span></div>
<div><span><br />
Submissions can be in any format and any length. We recommend the EC submission format. </span><span>The workshop will not have archival proceedings but will post accepted papers on the workshop website. At least one author of each accepted paper will be expected to attend and present their findings at the workshop.<p></p>
<p></p></span></div>
<div><span>Submissions should be uploaded to Easychair no later than May 18th, 2020, 11:59pm PDT. </span></div>
<div></div>
<div></div>
<div><span><strong>Organizing Committee</strong><br />
</span></div>
<div><span><br />
Yiling Chen, Harvard University<br />
Dan Goldstein, Microsoft Research<br />
Kevin Leyton-Brown, University of British Columbia<br />
Shengwu Li, Harvard University<br />
Gali Noti, Hebrew University</span></div>
<div></div>
<div><span><br />
<strong>More Information</strong><br />
</span></div>
<div><span><br />
For more information or questions, visit the workshop website:<br />
<a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br />
or email the organizing committee: <a href="mailto:behavioralec2020@easychair.org">behavioralec2020@easychair.org</a></span></div></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2020/04/07/call-for-papers-the-2nd-workshop-on-behavioral-economics-and-computation/"><span class="datestr">at April 07, 2020 02:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/">Professorship in Theoretical Computer Science at University of Copenhagen (apply by May 24, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>University of Copenhagen is seeking candidates for a full professorship in Theoretical Computer Science. More specifically, we are inviting exceptional candidates from the broad fields of algorithms, complexity, and cryptography including privacy. The application deadline is May 24, 2020. Enquiries are welcome and may be sent to Mikkel Thorup (mthorup@di.ku.dk) or Mads Nielsen (madsn@di.ku.dk).</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668">https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668</a><br />
Email: mthorup@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/"><span class="datestr">at April 06, 2020 07:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16911">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/">Not As Easy As ABC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Is the claimed proof of the ABC conjecture correct?</em><br />
<font color="#000000"></font></p><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/screen-shot-2020-04-05-at-8-22-38-pm/" rel="attachment wp-att-16914"><img width="300" alt="" src="https://rjlipton.files.wordpress.com/2020/04/screen-shot-2020-04-05-at-8.22.38-pm.png?w=300&amp;h=238" class="alignright size-medium wp-image-16914" height="238" /></a><p></p>
<p>
<font color="#0044cc">
</font></p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Photo courtesy of Kyodo University ]</font></td>
</tr>
</tbody>
</table>
<p>
Shinichi Mochizuki is about to have his proof of the ABC conjecture <a href="https://futurism.com/the-byte/mathematicians-shocked-paper-published">published</a> in a journal. The proof needs more than a ream of paper—that is, it is over 500 pages long. </p>
<p>
Today I thought we would discuss his claimed proof of this famous conjecture.</p>
<p>
The decision to published is also discussed in an <a href="https://www.nature.com/articles/d41586-020-00998-2">article</a> in <em>Nature</em>. Some of the discussion we have seen <a href="https://www.math.columbia.edu/~woit/wordpress/?p=11709">elsewhere</a> has been about personal factors. We will just comment briefly on the problem, the proof, and how to tell if a proof has problems. </p>
<p>
</p><p></p><h2> The Problem </h2><p></p>
<p></p><p>
Number theory is hard because addition and multiplication do not play well together. Adding numbers is not too complex by its self; multiplication by its self is also not too hard. For those into formal logic the theory of addition for example is decidable. So in principle there is no hard problem that only uses addition. None. A similar point follows for multiplication. </p>
<p>
But together addition and multiplication is hard. Of course Kurt Gödel proved that the formal theory of arithmetic is hard. It is not complete, for example. There must be statements about addition and multiplication that are unprovable in Peano Arithmetic. </p>
<p>
The ABC conjecture states a property that is between addition and multiplication. Suppose that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  A + B = C, " class="latex" title="\displaystyle  A + B = C, " /></p>
<p>for some integers <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+A+%5Cle+B+%5Cle+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \le A \le B \le C}" class="latex" title="{1 \le A \le B \le C}" />. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le ABC " class="latex" title="\displaystyle  C \le ABC " /></p>
<p>is trivial. The ABC conjecture says that one can do better and get 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le F(ABC), " class="latex" title="\displaystyle  C \le F(ABC), " /></p>
<p>for a function <img src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(X)}" class="latex" title="{F(X)}" /> that is sometimes much smaller than <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. The function <img src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(X)}" class="latex" title="{F(X)}" /> depends not on the size of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> but on the multiplicative structure of <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. That is the function depends on the multiplicative structure of the integers. Note, the bound 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le ABC " class="latex" title="\displaystyle  C \le ABC " /></p>
<p>only needed that <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B,C}" class="latex" title="{A,B,C}" /> were numbers larger than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. The stronger bound 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le F(ABC), " class="latex" title="\displaystyle  C \le F(ABC), " /></p>
<p>relies essentially on the finer structure of the integers. </p>
<p>
Roughly <img src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(X)}" class="latex" title="{F(X)}" /> operates as follows: Compute all the primes <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> that divide <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> be the product of all these primes. Then <img src="https://s0.wp.com/latex.php?latex=%7BF%28X%29+%5Cle+Q%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(X) \le Q^{2}}" class="latex" title="{F(X) \le Q^{2}}" /> works: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+Q%5E%7B2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le Q^{2}. " class="latex" title="\displaystyle  C \le Q^{2}. " /></p>
<p>The key point is: <i>Even if <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B100%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p^{100}}" class="latex" title="{p^{100}}" />, for example, divides <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />, we only include <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> in the product <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" />.</i> This is where the savings all comes from. This is why the ABC conjecture is hard: repeated factors are thrown away.</p>
<p>
Well not exactly, there is a constant missing here, the bound is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+%5Calpha+Q%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le \alpha Q^{2} " class="latex" title="\displaystyle  C \le \alpha Q^{2} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\alpha&gt;0}" class="latex" title="{\alpha&gt;0}" /> is a universal constant. We can replace <img src="https://s0.wp.com/latex.php?latex=%7BQ%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q^{2}}" class="latex" title="{Q^{2}}" /> by a smaller number—the precise statement can be found <a href="https://en.wikipedia.org/wiki/Abc_conjecture">here</a>. This is the ABC conjecture. </p>
<p>
The point here is that in many cases <img src="https://s0.wp.com/latex.php?latex=%7BF%28ABC%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(ABC)}" class="latex" title="{F(ABC)}" /> is vastly smaller than <img src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ABC}" class="latex" title="{ABC}" /> and so that inequality 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le F(ABC), " class="latex" title="\displaystyle  C \le F(ABC), " /></p>
<p>is much better than the obvious one of 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  C \le ABC. " class="latex" title="\displaystyle  C \le ABC. " /></p>
<p>For example, suppose that one wishes to know if 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++5%5E%7Bz%7D+%3D+2%5E%7Bx%7D+%2B+3%5E%7By%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  5^{z} = 2^{x} + 3^{y}, " class="latex" title="\displaystyle  5^{z} = 2^{x} + 3^{y}, " /></p>
<p>is possible. The ABC conjecture shows that this cannot happen for <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" /> large enough. Note 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%282%5E%7Bx%7D+3%5E%7By%7D+5%5E%7Bz%7D%29+%3D+30+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 " class="latex" title="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 " /></p>
<p>for positive integers <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y,z}" class="latex" title="{x,y,z}" />.</p>
<p>
</p><p></p><h2> Is He Correct? </h2><p></p>
<p></p><p>
Eight years ago Mochizuki announced his <a href="http://www.kurims.kyoto-u.ac.jp/~motizuki/papers-english.html">proof</a>. Now it is about to be published in a journal. He is famous for work in part of number theory. He solved a major open problem there years ago. This gave him instant credibility and so his claim of solving the ABC conjecture was taken seriously. </p>
<p>
For example, one of his papers is <a href="https://www.math.uni-bielefeld.de/documenta/vol-kato/mochizuki.dm.pdf">The Absolute Anabelian Geometry of Canonical Curves</a>. The paper says: </p>
<blockquote><p><b> </b> <em> How much information about the isomorphism class of the variety <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is contained in the knowledge of the étale fundamental group? </em>
</p></blockquote>
<p></p><p>
A glance at this paper shows that it is for specialists only. But it does seem to be math of the type that we see all the time. And indeed the proof in his paper is long believed to be correct. This is in sharp contrast to his proof of the ABC conjecture. </p>
<p>
</p><p></p><h2> Indicators of Correctness </h2><p></p>
<p></p><p>
The question is: Are there ways to detect if a proof is (in)correct? Especially <a href="https://en.wikipedia.org/wiki/List_of_long_mathematical_proofs">long</a> proofs? Are there ways that rise above just checking the proof line by line? By the way:</p>
<blockquote><p><b> </b> <em> The length of unusually long proofs has increased with time. As a rough rule of thumb, 100 pages in 1900, or 200 pages in 1950, or 500 pages in 2000 is unusually long for a proof. </em>
</p></blockquote>
<p></p><p>
There are some ways to gain confidence. Here are some in my opinion that are useful.</p>
<ol>
<li>
Is the proof understood by the experts? <p></p>
</li><li>
Has the proof been generalized? <p></p>
</li><li>
Have new proofs been found? <p></p>
</li><li>
Does the proof have a clear roadmap?
</li></ol>
<p>
The answer to the first question (1) seems to be no for the ABC proof. At least two world experts have raised concerns—see this <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">article</a> in <em>Quanta</em>—that appear serious. The proof has not yet been generalized. This is an important milestone for any proof. Andrew Wiles famous proof that the Fermat equation 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bp%7D+%2B+y%5E%7Bp%7D+%3D+z%5E%7Bp%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x^{p} + y^{p} = z^{p}, " class="latex" title="\displaystyle  x^{p} + y^{p} = z^{p}, " /></p>
<p>has no solutions in integers for <img src="https://s0.wp.com/latex.php?latex=%7Bxyz+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{xyz \neq 0}" class="latex" title="{xyz \neq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p \ge 3}" class="latex" title="{p \ge 3}" /> a prime has been extended. This certainly adds confidence to our belief that it is correct.</p>
<p>
Important problems eventually get other proofs. This can take some time. But there is almost always success in finding new and different proofs. Probably it is way too early for the ABC proof, but we can hope. Finally the roadmap issue: This means does the argument used have a nice logical flow. Proofs, even long proofs, often have a logic flow that is not too complex. A proof that says: Suppose there is a object <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> with this property. Then it follows that there must be an object <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> Is more believable than one with a much more convoluted logical flow. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Ivan Fesenko of Nottingham has written an <a href="https://www.maths.nottingham.ac.uk/plp/pmzibf/rpp.pdf">essay</a> about the proof and the decision to publish. Among factors he notes is “the potential lack of mathematical infrastructure and language to communicate novel concepts and methods”—noting the steep learning curve of trying to grasp the language and framework in which Mochizuki has set his proof. Will the decision to publish change the dynamics of this effort?</p>
<p>[Fixed typo]</p></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/"><span class="datestr">at April 06, 2020 02:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4714">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4714">If I used Twitter…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I’m thinking of writing a novel where human civilization is threatened by a global pandemic, and is then almost singlehandedly rescued by one man … a man who reigned for decades as the world’s prototypical ruthless and arrogant tech billionaire, but who was then transformed by the love of his wife.  That is, <em>if</em> the billionaire can make it past government regulators as evil as they are stupid.  I need some advice: how can I make my storyline a bit subtler, so critics don’t laugh it off as some immature nerd fantasy?</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Updates (April 5):</span></strong> Thanks to several commenters for emphasizing that the wife needs to be a central character here: I agree!  The other thing is, I don’t want Fox News cheering my novel for its <em>Atlas Shrugged</em> vibe.  So maybe the pandemic is only surging out of control in the US because of the incompetence of a Republican president?  I don’t want to go ridiculously overboard, but like, maybe the president is some thuggish conman with the diction of a 5-year-old, who the deluded Republicans cheer anyway?  And maybe he’s also a Bible-thumping fundamentalist?  OK, that’s too much, so maybe the fundamentalist is like the <em>vice</em> president or something, and he gets put in charge of the pandemic response and then sets about muzzling the scientists?  As I said, I really need advice on making the messages subtler.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4714"><span class="datestr">at April 04, 2020 11:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=413">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/04/04/tcs-talk-wednesday-april-8-ramon-van-handel-princeton/">TCS+ talk: Wednesday, April 8 — Ramon van Handel, Princeton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, April 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Ramon van Handel</strong> from Princeton will speak about how “<em>Rademacher type and Enflo type coincide</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In Banach space theory, Rademacher type is an important invariant that controls many geometric and probabilistic properties of normed spaces. It is of considerable interest in various settings to understand to what extent such powerful tools extend to general metric spaces. A natural metric analogue of Rademacher type was proposed by Enflo in the 1960s-70s, and has found a number of interesting applications. Despite much work in the intervening years, however, the relationship between Rademacher type and Enflo type has remained unclear. This basic question is settled in joint work with Paata Ivanisvili and Sasha Volberg: in the setting of Banach spaces, Rademacher type and Enflo type coincide. The proof is based on a very simple but apparently novel insight on how to prove dimension-free inequalities on the Boolean cube. I will not assume any prior background in Banach space theory in the talk.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/04/04/tcs-talk-wednesday-april-8-ramon-van-handel-princeton/"><span class="datestr">at April 04, 2020 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7673">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/">In defense of expertise</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Scott Aaronson blogged in <a href="https://www.scottaaronson.com/blog/?p=4695">defense of “armchair epidemiology”</a>. Scott makes several points I agree with, but he also advocates that rather than discounting ideas from “contrarians” who have no special expertise in the matter, each one of us should evaluate the input of such people on its merits.</p>



<p>I disagree. I can judge on their merits the validity of a proposed P vs NP proof or a quantum algorithm for SAT, but I have seen time and again smart and educated non-experts misjudge such proposals. As much as I’d like to think otherwise, I would probably be fooled just as easily by a well-presented proposal in area X that “brushes under the rug” subtleties that experts in X would immediately notice.</p>



<p>This is not to say that non experts should stay completely out of the matter. Just like scientific journalists such as Erica Klarreich and Kevin Hartnett of quanta can do a great job of explaining computer science topics to lay audience, so can other well-read people serve as “signal boosters” and highlight works of experts in epidemiology. Journalist Helen Branswell of <a href="https://www.statnews.com/">stat news</a> has been following the <a href="https://www.nytimes.com/2020/03/30/business/media/stat-news-boston-coronavirus.html">novel coronavirus since January 4th</a>. </p>



<p>The difference is that these journalists don’t pretend to see what the experts are missing but rather to highlight and simplify the works that experts are already doing. This is unlike “contrarians” such as Robin Hanson that do their own analysis on a spreadsheet and come up with a “home brewed” policy proposal such as <a href="http://www.overcomingbias.com/2020/02/consider-controlled-infection.html">deliberate infection</a> or <a href="http://www.overcomingbias.com/2020/03/variolation-may-cut-covid19-deaths-3-30x.html">variolation</a> (with “hero hotels” in which people go to be deliberately infected). I am not saying that such proposals are necessarily wrong, but I am saying that I (or anyone else without the experience in this field) am not qualified to judge them. Even if they did “make sense” to me (they don’t) I would not feel any more confident in judging them than I would in reviewing a paper in astronomy. There is a reason why Wikipedia has a <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">“no original research”</a> policy.</p>



<p>Moreover, the attitude of dismissing expertise can be dangerous, whether it comes in the form of <a href="https://en.wikipedia.org/wiki/Teach_the_Controversy">“teach the debate”</a> in the context of evolution, or <a href="https://en.wikipedia.org/wiki/Climatic_Research_Unit_email_controversy">“ClimateGate”</a> in the context of climate change. Unlike the narrative of few brave “dissenters” or “contrarians”, in the case of COVID-19, experts as well as the world health organization have been literally sounding the alarm (see also <a href="https://www.nytimes.com/article/coronavirus-timeline.html">timeline</a>, as well as this NPR story on the <a href="https://www.npr.org/2020/04/03/826945368/how-the-united-states-failed-to-see-the-coronavirus-crisis-coming">US response</a>). Yes, some institutions, and especially the U.S., failed in several aspects (most importantly in the early production of testing). But one of the most troubling aspects is the constant sense of  <a href="https://www.nytimes.com/2020/04/03/us/politics/coronavirus-trump-medical-advisers.html">“daylight”</a>  and <a href="https://www.politico.com/news/2020/02/26/trump-backers-coronavirus-conspiracy-117781">distrust</a> between the current U.S. administration and its own medical experts. Moreover, the opinions of people such as  <a href="https://www.newyorker.com/news/q-and-a/the-contrarian-coronavirus-theory-that-informed-the-trump-administration">law professor Richard Epstein</a> are listened to even when they are far out of their depth. It is one thing to entertain the opinion of non-expert contrarians when we have all the time in the world to debate, discuss and debunk. It’s quite another to do so in the context of a fast-moving health emergency.  COVID-19 is an emergency that has medical, social, economical, and technological aspects, but it would best be addressed if each person contributes according to their skill set and collaborates with people of complementary backgrounds.</p>



<p></p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/"><span class="datestr">at April 04, 2020 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2445">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/computer-aided-analyses/">Computer-aided analyses in optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In this blog post, I want to illustrate how computers can be great allies in designing (and verifying) convergence proofs for first-order optimization methods. This task can be daunting, and highly non-trivial, but nevertheless usually unavoidable when performing complexity analyses. A notable example is probably the convergence analysis of the stochastic average gradient (SAG) [<a href="https://arxiv.org/pdf/1309.2388.pdf">1</a>], whose original proof was computer assisted.</p>



<p class="justify-text">To this end, we will mostly spend time on what is referred to as <em>performance estimation problems</em> (PEPs), introduced by Yoel Drori and Marc Teboulle [<a href="https://link.springer.com/article/10.1007/s10107-013-0653-0">2</a>]. Performance estimation is also closely related to the topic of <em>integral quadratic constraints</em> (IQCs), introduced in the context of optimization by Laurent Lessard, Benjamin Recht and Andrew Packard [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>]. In terms of presentations, IQCs  leverages control theory, whereas PEPs might seem more natural in the optimization community. This blog post essentially presents PEPs from the point of view of [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>], instantiated on a running example.</p>



<h2>Overview, motivations</h2>



<p class="justify-text">First-order methods for continuous optimization belong to the large panel of algorithms that are usually approached via worst-case analyses. In this context, analyses rely on combining inequalities (that are due to assumptions on the problem classes), in potentially long, non-intuitive, and technical, proofs. For the insiders, those proofs all look very similar. For the outsiders, those proofs all look rather repelling, technical (long pages of chained inequalities), probably not interesting, and like computer codes: usually intuitive mostly for their authors.</p>



<p class="justify-text">In what follows, I want to show how (and why) those proofs are indeed all very similar. On the way, I want to emphasize how those combinations of inequalities are related to the “true essence” of worst-case analyses (which rely on computing worst-case scenarios), and to provide examples on how to constructively obtain them.</p>



<p class="justify-text">We take the stand of illustrating the PEP approach on a single iteration of gradient descent, as it essentially contains all necessary ingredients to understand the methodology in other contexts as well. Certain details of the following text are (probably unavoidably) a bit technical. However, going through the detailed computations is not essential, and the text should contain the necessary ingredients for understanding the essence of the methodology.</p>



<h2>Running example: gradient descent</h2>



<p class="justify-text">Let us consider a naive, but standard, example: unconstrained convex minimization $$x_\star= \underset{x\in\mathbb{R}^d}{\mathrm{arg min}} f(x)$$with gradient descent: \(x_{k+1}=x_k-\gamma \nabla f(x_k)\). Let us assume \(f(\cdot)\) to be continuously differentiable, to have a \(L\)-Lipschitz gradient (a.k.a., \(L\)-smoothness), and to be \(\mu\)-strongly convex. Those functions satisfy, for all \(x,y\in\mathbb{R}^d\):<br />– strong convexity, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Definition 2.1.2]: $$\tag{1}f(x) \geqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{\mu}{2} \lVert x-y\rVert^2,$$- smoothness, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.5]: $$\tag{2} f(x) \leqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{L}{2}\lVert x-y\rVert^2.$$Let us recall that in the case of twice continuously differentiable functions, smoothness and strong convexity amount to requiring  that $$\mu I \preccurlyeq \nabla^2 f(x) \preccurlyeq L I,$$ for some \(0&lt; \mu&lt;L&lt; \infty\) and for all \(x\in\mathbb{R}^d\) (in other words, all eigenvalues of \(\nabla^2 f(x)\) are between \(\mu\) and \(L\)). In what follows, we denote by \(\mathcal{F}_{\mu,L}\) the class of \(L\)-smooth \(\mu\)-strongly convex functions (irrespective of the dimension \(d\)).</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="473" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/SmoothStronglyConvex.png" height="204" />Figure 1: the blue function is \(L\)-smooth and \(\mu\)-strongly convex (it is possible to create respectively global upper and lower quadratic bounds from every \(x\in\mathbb{R}^d\) with respectively curvatures \(L\) and \(\mu\)).</figure></div>
</div></div>



<p class="justify-text">In this context, convergence of gradient descent can be studied in many ways. Here, for the sake of the example, we will do it in terms of two base quantities: distance to optimality \(\lVert x_k-x_\star\rVert\), and function value accuracy \(f(x_k)-f(x_\star)\). There are, of course, infinitely many other possibilities, such as gradient norm \(\rVert \nabla f(x_k)\lVert\), Bregman divergence \(f(x_\star)-f(x_k)-\langle{\nabla f(x_k)};{x_\star-x_k}\rangle\), or even best function value observed throughout the iterations \(\min_{0\leq i\leq k} \{f(x_i)-f(x_\star)\}\): the reader can adapt the lines below for his/her favorite criterion. </p>



<p class="justify-text">For later reference, let us provide another inequality that is known to  hold for all \(x,y\in\mathbb{R}^d\) for any \(L\)-smooth \( \mu\)-strongly convex function: <br />– bound on inner product, see, e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>,  Theorem 2.1.11]: $$\langle{\nabla f(x)-\nabla f(y)};{x-y}\rangle  \geqslant        \tfrac{1}{L+\mu} \lVert{\nabla f(x)-\nabla f(y)}\rVert^2+\tfrac{\mu  L}{L+\mu}\lVert{x-y}\rVert^2.\tag{3}$$ In the case \(\mu=0\) this inequality is known as “cocoercivity”. This (perhaps mysterious) inequality happens to play an important role in convergence proofs.</p>



<h3>A standard convergence result</h3>



<p class="justify-text">Let us start by stating two known results along with their simple proofs (see, e.g.,  [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.14] or [6, Section 1.4.2, Theorems 2 &amp; 3]):<br />– convergence in distance:  $$\begin{array}{rl}    \rVert{x_{k+1}-x_\star}\lVert^2&amp;= \lVert{x_k-x_\star}\rVert^2+\gamma^2\lVert{\nabla f(x_k)}\rVert-2\gamma\langle{\nabla f(x_k)};{x_k-x_\star}\rangle \\ \     &amp; \leqslant      \left(1-\tfrac{2\gamma L \mu}{L+\mu}\right)\lVert{x_k-x_\star}\rVert^2+\gamma\left(\gamma-\tfrac2{L+\mu}\right)\lVert{\nabla f(x_k)}\rVert^2, \end{array} $$ where the second line follows from smoothness and strong convexity of \(f\) via the bound (3) on the inner product (with \(x=x_k\) and \(y=x_\star\)). For the particular choice \(\gamma=\tfrac2{L+\mu}\), the second term on the right hand side disappears, and we end up with<br />     $$\lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^2\lVert{x_k-x_\star}\rVert^2,$$ which, following from \(0&lt;\mu&lt;L&lt;\infty\), satisfies \(0&lt; \tfrac{L-\mu}{L+\mu}&lt;1\), hence proving linear convergence of gradient descent in this setup, by recursively applying the previous inequality: $$ \lVert{x_{k}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^{2k}\lVert{x_0-x_\star}\rVert^2.$$ – Convergence in function values: one can simply use the result in distance along with the previous basic inequalities (1) and (2) characterizing smoothness and strong convexity (both with \(y=x_\star\)):<br />     $$f(x_k)-f(x_\star) \leqslant \hspace{-.15cm}\tfrac{L}{2}\hspace{-.1cm}\rVert{x_k-x_\star}\lVert^2  \leqslant  \hspace{-.15cm}    \tfrac{L}{2}\hspace{-.1cm}\left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k} \rVert{x_0-x_\star}\lVert^2 \leqslant  \hspace{-.15cm}     \tfrac{L}{\mu}\hspace{-.1cm} \left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k}(f(x_0)-f(x_\star)).$$  It is also possible to directly look for convergence in terms of function values, but it is then usually unclear in the literature what inequalities to use, and I am not aware of any such proof leading to the same rate without the leading \(\tfrac{L}{\mu}\) (except the proof presented below).</p>



<p class="justify-text">At this point, even in this toy example, a few very legitimate questions can be raised:<br />– can we improve anything? Can gradient descent really behaves like that on this class of functions?<br />– How could we have guessed the inequality to use, and the shape of the corresponding proof? Obviously, the obscure fact is to arrive to inequality (1).  Therefore, is there a principled way for choosing the right inequalities to use, for example for studying convergence in terms of other quantities, such as  function values?<br />– Is this the unique way to arrive to the desired result? If yes, how likely are we to find such proofs for more complicated cases (algorithms and/or function class)?</p>



<p class="justify-text">For the specific step size choice \(\gamma=\tfrac2{L+\mu}\), a partial answer to the first question is obtained by the observation that the rate is actually achieved on the quadratic function<br /> $$f(x)=\tfrac12 \, x^\top \begin{bmatrix}<br /> L &amp; 0\\ 0 &amp; \mu<br /> \end{bmatrix}x.$$ The following lines precisely target the missing answers.</p>



<h2>Worst-case analysis through worst-case scenarios</h2>



<p class="justify-text">Let us start by rephrasing our goal, and restrict ourselves to the study of a single iteration. We fix our target to finding the smallest possible value of \(\rho\) such that the inequality<br /> $$ \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant       \rho^2 \lVert{x_k-x_\star}\rVert^2 $$ is valid for all \(x_k\) and \(x_{k+1}=x_k-\gamma \nabla f(x_k)\) (hence \(\rho\) is a function of \(\gamma\)). In other words, our goal is to solve<br />$$ \rho^2(\gamma):= \sup \left\{ \frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0\right\}.$$<br />Alternatively, we could be interested in studying convergence in other forms: for function values, we could target to solve the slightly modified problem:<br /> $$ \sup  \left\{ \frac{f(x_{k+1})-f(x_\star)}{f(x_{k})-f(x_\star)}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0 \right\}.$$ It turns out that in both cases, the problem can be solved both numerically to high precision, and analytically, and that the answer is \(\rho^2(\gamma)=\max\{(1-\mu\gamma)^2,(1-L\gamma)^2\}\).</p>



<p class="justify-text">The only thing we did, so far, was to explicitly reformulate the problem of finding the best (smallest) convergence rate as the problem of finding the worst-case scenario, nothing more. In what follows, some parts might become slightly technical, but the overall idea is only to reformulate this problem of finding the worst-case scenarios, for solving it.</p>



<h3>Dealing with an infinite-dimensional variable: the function \(f\)</h3>



<p class="justify-text">The first observation is that the problem of computing \(\rho\) is stated as an infinite-dimensional optimization problem: we are looking for the worst possible problem instance (a function \(f\) and an initial point \(x_k\)) within a predefined class of problems. The first step we take to work around this is to reformulate it in the following equivalent  form (note that we maximize also over the dimension \(d\)—we discuss later how to remove it):<br />$$\begin{array}{rl} \rho^2:= \underset{f,\, x_k,\,x_\star,\, g_k,\,d}{\sup} &amp;\displaystyle \frac{\lVert{x_{k}-\gamma g_k-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\\<br /> \text{s.t. }    &amp; \exists f\in\mathcal{F}_{\mu,L}:\, g_k= \nabla f(x_k),\, 0=\nabla f(x_\star).<br />\end{array}$$<br />This problem intrinsically does not look better (it contains an  existence constraint), but it allows using mathematical tools which are referred to as  <em>interpolation,</em> or <em>extension</em>, theorems [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1603.00241.pdf">7</a>, <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">8</a>]. The problem is depicted on Figure 2:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="490" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/Interpolation.png" height="209" />Figure 2: discrete interpolation (or extension) problem: given a set of triplets \(\{(\text{coordinate}, \text{gradient}, \text{function value})\}\) can we recover a function within a determined class that explains those triplets?</figure></div>



<p class="justify-text">It turns out that convex interpolation (that is, neglecting smoothness and strong convexity) is actually rather simple:</p>



<ul class="justify-text"><li>given a convex function and an index set \(I\), any set of samples \(\{(x_i,g_i,f_i)\}_{i\in I}\) of the form \(\{(\text{coordinate}, \text{(sub)gradient}, \text{function value})\}\)) satisfies, for all \(i,j\in I\): $$f_i \geqslant      f_j+\langle g_j; x_i-x_j\rangle,$$ by definition of subgradient, as illustrated on Figure 3.</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="485" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/SamplingCvx.png" height="205" />Figure 3: sampling from a convex function.</figure></div>



<ul class="justify-text"><li>In the other direction, given a set of triplets \(\{(x_i,g_i,f_i)\}_{i\in I}\) satisfying the previous inequality for all pairs \(i,j\in I\), one can simply recover a  convex function by the following construction: $$f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\},$$ which is depicted on Figure 4.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="487" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/InterpolateCvx.png" height="207" />Figure 4: some set \(\{(x_i,g_i,f_i)\}_{i\in I}\) and its piecewise affine interpolant \(f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\}\).</figure></div>



<ul class="justify-text"><li>Formally, the reasoning allows arriving to the following “convex interpolation” (or “convex extension”) result, where we denote the set of (closed, proper) convex functions by \(\mathcal{F}_{0,\infty}\) (to be understood as \(L\)-smooth \(\mu\)-strongly convex functions with \(\mu=0\) and \(L=\infty\)): $$\begin{array}{c}\exists f\in\mathcal{F}_{0,\infty}: \,  g_k\in\partial f(x_k) \text{ and } f_k=f(x_k) \ \ \forall k\in  I\\ \Leftrightarrow\\ f_i \geqslant       f_j+\langle{g_j};{x_i-x_j}\rangle\quad \forall i,j\in I,\end{array}$$ where \(\partial f(x)\) denotes the subdifferential of \(f\) at \(x\).</li></ul>



<p class="justify-text">In the next section, we use a similar interpolation result for taking smoothness and strong convexity into account. The result is a bit more technical, but follows from similar constructions as those for convex interpolation—the main difference being that the interpolation is done on the Fenchel conjugate instead, in order to incorporate smoothness, see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Section 2].</p>



<h3>Reformulation through convex interpolation</h3>



<p class="justify-text">Back to the problem of computing worst-case scenarios, we can now reformulate the existence constraint <em>exactly</em> using the following result (see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>,  Theorem 4]): let \(I\) be a finite index set and let \( S=\{(x_i,g_i,f_i)\}_{i\in I}\) be a set of triplets, then<br />  $$\begin{array}{c}\exists f\in\mathcal{F}_{\mu,L}: \,  g_i=\nabla f(x_i) \text{ and } f_i=f(x_i) \text{ for all } i\in  I\\ \Leftrightarrow\\  f_i \geqslant      f_j+\langle{g_j};{x_i-x_j}\rangle+\frac{1}{2L}\lVert{g_i-g_j}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_i-x_j-\frac{{1}}{L}(g_i-g_j)}\rVert^2  \,\,\, \forall i,j\in I.\end{array}$$ Therefore, the previous problem can be reformulated as (recalling that \(g_\star=0\))<br />$$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle\frac{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}{\rVert{x_k-x_\star}\lVert^2}\\<br />      \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br />      &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2.<br />\end{array}$$ </p>



<h3>Quadratic reformulation</h3>



<p class="justify-text">The next step is to remove the ratio appearing in the objective function, which we do via an homogeneity argument, as follows.</p>



<p class="justify-text">Starting from a feasible point, scale \(x_k,\,x_\star,g_k\) by some \(\alpha&gt;0\) and \(f_k,\,f_\star\) by \(\alpha^2\) and observe it does not change the value of the objective, while still being a feasible point. Therefore, the problem can be reformulated as a nonconvex QCQP (quadratically constrained quadratic program): $$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}\\<br />       \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br />       &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\ &amp;{\rVert{x_k-x_\star}\lVert^2} \leqslant      1,<br /> \end{array}$$  which is quadratic in \(x_k\), \(x_\star\) and  \(g_k\), and linear in \(f_\star\) and \(f_k\). Actually, in the current form, nonconvexity comes from the term “\(\langle{g_k};{x_\star-x_k}\rangle\)” in the second constraint (and from the objective, due to maximization). It turns out that this problem can be reformulated <em>losslessly</em> using semidefinite programming (this is due to the maximization over \(d\), as commented at the end of the next section). </p>



<h3>Semidefinite reformulation</h3>



<p class="justify-text">At the end of this section, we will be able to compute, numerically, the values of the rate \(\rho^2(\gamma)\) for given values of the parameters \(\mu,\,L\), and \(\gamma\).</p>



<p class="justify-text">The last step in the reformulation goes as follows: the previous problem can be reformulated as a semidefinite program, as it is linear in terms of the entries of the following Gram matrix<br />$$G = \begin{pmatrix}<br />     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle \\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2<br />     \end{pmatrix}\succcurlyeq 0,$$ and in terms of the function values \(f_k\) and \(f_\star\). From those variables, one reformulate the previous problem as $$\begin{array}{rl} \underset{f_k,\,f_\star,\, G\succeq 0}{\sup} \, &amp;{\mathrm{Tr} (A_\text{num} G)}\\<br />      \text{s.t. }    &amp; f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0\\<br />      &amp;  f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0 \\<br />      &amp;\mathrm{Tr} (A_\text{denom} G) \leqslant      1,\end{array}$$ which is a gentle semidefinite program where we picked matrices \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) for encoding the previous terms. That is, we choose those matrices such that<br /> $$\begin{array}{rl}<br />\mathrm{Tr}(A_{\text{denom}} G)&amp;=\lVert{x_k-x_\star}\rVert^2,\\  \mathrm{Tr}(A_{\text{num}} G)&amp;=\lVert{x_k-\gamma g_k-x_\star}\rVert^2,\\ \mathrm{Tr}(A_1G)&amp;=\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2,\\ \mathrm{Tr}(A_2G)&amp;=\tfrac{1}{2L}\lVert g_k\rVert^2+\tfrac{\mu}{2(1-\mu/L)}\lVert x_k-x_\star-\tfrac1L g_k\rVert^2.\end{array}$$ One possibility is to choose \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) as symmetric matrices, as follows: $$\begin{array}{cc}<br /> A_{\text{denom}}=\begin{pmatrix}     1 &amp; 0\\ 0 &amp; 0     \end{pmatrix}, &amp; A_{\text{num}}=\begin{pmatrix}     1 &amp; -\gamma\\ -\gamma &amp; \gamma^2     \end{pmatrix}, \\ A_1=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac12-\tfrac{\mu}{2(L-\mu)} \\ -\tfrac12-\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)}\end{pmatrix}, &amp;  A_2=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac{\mu}{2(L-\mu)} \\ -\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)} \end{pmatrix}.\end{array}$$</p>



<p class="justify-text">All those steps can be carried out in the exact same way for the problem of computing the convergence rate for function values, reaching a similar problem with \(6\) inequality constraints instead—because interpolation conditions have to be imposed on all pairs of points in a set of \(3\) points: \(x_k\), \(x_{k+1}\) and \(x_\star\), instead of only \(2\) for the distance problem. The objective function is then \(f_{k+1}-f_\star\), the de-homogenization constraint (arising from the denominator of the objective function) is \(f_{k}-f_\star \leqslant     1\), and the Gram matrix is \(3\times 3\):<br />$$G=\begin{pmatrix}<br />     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle&amp; \langle{g_{k+1}};{x_k-x_\star}\rangle\\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2 &amp; \langle{g_k};{g_{k+1}}\rangle \\<br />     \langle{g_{k+1}};{x_k-x_\star}\rangle &amp; \langle{g_{k+1}};{g_k}\rangle &amp; \lVert{g_{k+1}}\rVert^2<br />     \end{pmatrix}\succcurlyeq 0, $$ and the function values variables are \(f_k\), \(f_{k+1}\) and \(f_\star\).</p>



<p class="justify-text">We provide the numerical optimal values of those semidefinite programs on Figure 5 for both convergence in distances and in function values. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="534" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/ObjectiveValues.png" height="324" />Figure 5: worst-cases of the ratio \(\frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\) (red) and \(\frac{f(x_{k+1})-f_\star}{f(x_k)-f_\star}\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match exactly the expected \(\max\{(1-\gamma L)^2,(1-\gamma\mu)^2\}\) in both cases. Note that the corresponding SDPs can be solved both for “good and bad” choices of step sizes: if the step size is chosen wisely then \(\rho(\gamma)&lt;1\), and otherwise \(\rho(\gamma)\geqslant 1\). The SDP confirms the common knowledge that \(\gamma\in (0,2/L)\Rightarrow \rho(\gamma)&lt; 1\). Numerical values obtained through YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p class="justify-text">As a conclusion for this section, let us note that we showed how to compute the “best” rates that are dimension independent. In general, requiring the iterates and gradient (e.g., \(x_k\) and \(g_k\) for the problem in terms of distance, and \(x_k\), \(g_k\) and \(g_{k+1}\) for function values, and potentially more vectors when dealing with more complex settings) to lie in \(\mathbb{R}^d\) is equivalent to adding a rank constraint in the SDP. </p>



<h2>Duality between worst-case scenarios and combinations of inequalities</h2>



<p class="justify-text">Any feasible point to the previous SDP corresponds to a <em>lower bound</em>: a sampled version of a potentially difficult function for gradient descent. If we want to find <em>upper bounds</em> on the rate, a natural way to proceed is to go to the dual side of the previous SDPs, where any feasible point will naturally correspond to an upper bound on the convergence rate (by <em>weak duality</em>). As the primal problems were SDPs, their Lagrangian duals are SDPs as well. Let us associate one multiplier per constraint: $$ \begin{array}{rl}<br />f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0&amp;:\lambda_1\\<br />f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0&amp;:\lambda_2\\<br />\mathrm{Tr}(A_\text{denom} G) \leqslant 1&amp;: \tau.<br />\end{array}$$The dual is then<br />$$\begin{array}{rl}<br /> \underset{\tau,\,\lambda_1,\,\lambda_2}{\min} &amp; \, \tau \\<br /> \text{s.t. } &amp; \lambda_1=\lambda_2,\\<br /> &amp; S:=A_\text{num}-\tau A_\text{denom}-\lambda_1A_1-\lambda_2A_2 \preccurlyeq 0,\\<br /> &amp;\tau,\lambda_1,\lambda_2 \geqslant  0.<br /> \end{array}$$ Hence, by weak duality, any feasible point to this last SDP corresponds to an upper bound on the rate: \(\tau \geqslant \rho^2\). A mere rephrasing of weak duality can be obtained through the following reasoning: assume we received some feasible \(\tau,\lambda_1,\lambda_2\) (and hence \(\lambda_1=\lambda_2\) and a corresponding \(S\preccurlyeq 0\)), we then get, for any primal feasible \(G\succcurlyeq0\):<br />$$\begin{array}{rl}\mathrm{Tr}(SG)&amp;=\mathrm{Tr}(A_{\text{num}}G)-\tau\mathrm{Tr}(A_{\text{denom}}G)-\lambda_1\mathrm{Tr}(A_1G)-\lambda_2\mathrm{Tr}(A_2G)\\&amp;=\lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\\ \,&amp;\,\,\,-\lambda_1[     f_k-f_\star+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\,\,\,-\lambda_2 [f_\star-f_k+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\geqslant \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2, \end{array}$$ where the first equality follows from the definition of \(S\), the second equality corresponds to the definitions of \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\), and the last inequality follows from the sign of the interpolation inequalities (constraints in the primal) for any primal feasible point. Hence, we indeed have that any feasible \(\tau\) corresponds to a valid upper bound on the convergence rate, as $$S\preccurlyeq 0 \,\,\Rightarrow \,\, \mathrm{Tr}(SG)\leqslant 0\,\,\Rightarrow  \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\leqslant 0.$$ In order to obtain analytical proofs, we therefore need to find analytical dual feasible points, and numerics can of course help in this process! Let’s look at what the optimal dual solutions look like for our two running examples.</p>



<ul class="justify-text"><li> in Figure 6, we provide the numerical values for \(\lambda_1\) and \(\lambda_2\) for the distance problem.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="467" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_distance.png" height="316" />Figure 6: numerical values of optimal dual variables: \(\lambda_1\) (red) and \(\lambda_2\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match \(\lambda_1=\lambda_2=2\gamma \rho(\gamma)\) with \(\rho(\gamma)=\max\{|1-\gamma L|,|1-\gamma\mu|\}\). Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<ul class="justify-text"><li>For function values, the SDP is slightly more complicated, as more inequalities are involved (6 interpolation inequalities). We provide raw numerical values for the six multipliers in Figure 7.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="463" alt="" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_function.png" height="348" />Figure 7: numerical values of optimal dual variables (for the rate in function values): \(\lambda_1,\lambda_2, …,\lambda_6\) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p>For those who want a bit more details, here are a few additional pointers:</p>



<ul class="justify-text"><li>Strong duality holds—a way to prove it is to show that there exists a Slater point in the primal, see e.g., [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Theorem 6]—, and hence primal and dual optimal values match.</li><li>There might be different ways to optimaly combine the interpolation inequalities for proving the desired results. In other words: dual optimal solutions are often not unique—which is, in fact, quite a good news: I am sure nobody want to find the analytical version of the multipliers provided in Figure 7.</li><li>It is often possible to simplify the proofs by using fewer, or weaker, inequalities. This might lead to ”cleaner” results, typically (but not always) at the cost of ”weaker” rates. This was done for designing the proof for function values, later in this text.</li></ul>



<h2>Combinations of inequalities: same proofs without SDPs</h2>



<p class="justify-text">So far, we showed that computing convergence rates can be done in a very principled way. To this end, one can solve semidefinite programs—which may have arbitrarily complicated analytical solutions. Here, I want to emphasize that the process of <em>verifying</em> a solution can be quite different to that of<em> finding</em> a solution. Put in other words, although the dual certificates (a.k.a., the proofs) might have been found by solving SDPs, they can be formulated in ways that do not require the reader to know anything about the PEP methodology, nor on any SDP material, for verifying them. This fact might actually not be very surprising to the reader, as many proofs arising in the first-order optimization literature actually “only consists” in linear combinations of (quadratic) inequalities. On the one hand, those proofs can be seen as feasible points to “dual SDPs”, although generally not explicitely proved as such. On the other hand, proofs arising from the SDPs might therefore be expected to be writable without any explicit reference to semidefinite programing and performance estimation problems.<br /></p>



<p class="justify-text">In what follows, we provide the proofs for gradient descent, using the previous numerical inspiration, but without explicitly relying on any semidefinite program. The reader is not expected to verify any of those computations, as our goal is rather to emphasize that the principles underlying both proofs are exactly the same: reformulating linear combinations of inequalities.</p>



<p class="justify-text">For both proofs below, we limit ourselves to the step size regime \(0\leq   \gamma \leq \tfrac{2}{L+\mu}\), and we prove  that, in  this regime, \(\rho(\gamma)=(1-\gamma\mu)\)—actually we only proof the upper bounds, but one can easily verify that they are <em>tight</em> on simple quadratic functions.  The complete proofs (for the proximal gradient method),  can be found in [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>]. </p>



<h3>Example 1: distance to optimality</h3>



<p class="justify-text">Recall the notations: \(g_k:=\nabla f(x_k)\), \(f_k:= f(x_k)\), \(g_\star:=\nabla f(x_\star)\), and \(f_\star:= f(x_\star)\).</p>



<p class="justify-text">For distance to optimality, sum the following inequalities with their corresponding weights: $$\begin{array}{r}     f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2  :\lambda_1,  \\     f_k \geqslant      f_\star+\langle{g_\star};{x_k-x_\star}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2:\lambda_2.     \end{array}$$ We use the following values for the multipliers: \(\lambda_1=\lambda_2=2\gamma\rho(\gamma) \geqslant      0\) (see Figure 6). </p>



<p class="justify-text">After appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), and with little effort, one can check that the previous weighted sum of inequalities can be written in the form: $$ \begin{array}{rl}    \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant      &amp; \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2 -\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2. \end{array}$$ This statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation) and verifying that all terms indeed match.</p>



<p class="justify-text">Finally, using $$\gamma(2-\gamma (L+\mu)) \geqslant      0,  \text{ and } L-\mu \geqslant      0,$$ which are nonnegative by assumptions on the values of \(L\in(0,\infty)\), \(\mu\in (0,L)\) and \(\gamma\in(0,2/(L+\mu))\), we arrive to the desired $$ \lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2.$$</p>



<p class="justify-text">Note that, by using \(\lambda_1=\lambda_2\), the weighted sum exactly corresponds to the (scaled by a positive constant) inequality introduced in the early stage of this note for studying distance to optimality. However, the resulting expression is tight for all values of the step size here, whereas it was only tight for \(\gamma=2/(L+\mu)\) earlier, due to a different choice of weights! </p>



<p class="justify-text">The curious reader might wonder how to find such a reformulation. Actually, back in terms of SDPs, and using the expressions for the multipliers, it simply corresponds to $$\mathrm{Tr}(SG)=-\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2.$$ In the example below, the reformulation is a bit more tricky—as \(\mathrm{Tr}(SG)\)  has two nonnegative terms, which were simply obtained by doing an analytical Cholesky factorization of the term \(\mathrm{Tr}(SG)\)—, but the idea is exactly the same.</p>



<h3>Example 2: function values</h3>



<p class="justify-text">For function values, we combine the following inequalities after multiplication with their respective coefficients:    </p>



<p class="justify-text">$$\scriptsize \begin{array}{lr}     f_k \geqslant      f_{k+1}+\langle{g_{k+1}};{x_k-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_k-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_{k+1}-\frac{1}{L}(g_k-g_{k+1})}\rVert^2    &amp;:\lambda_1,\\<br />f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{1}{L}(g_k-g_\star)}\rVert^2  &amp;:\lambda_2, \\<br />f_\star \geqslant      f_{k+1}+\langle{g_{k+1}};{x_\star-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_\star-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_\star-x_{k+1}-\frac{1}{L}(g_\star-g_{k+1})}\rVert^2  &amp;:\lambda_3.     \end{array}$$ We use the following multipliers \(\lambda_1=\rho(\gamma)\), \(\lambda_2=(1-\rho(\gamma))\rho(\gamma)\), and  \(\lambda_3=1-\rho(\gamma)\) (obtained by greedily trying to set different combinations of multipliers to \(0\) in the SDP—see Figure 7 for the values without such simplifications).</p>



<p class="justify-text">Again, after appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), we obtain that the weighted sum of inequalities can be reformulated exactly as $$  \begin{array}{rl}          f(x_{k+1})-f_\star \leqslant      &amp;\left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right)\\&amp;-\frac{1}{2 (L-\mu)}\lVert \nabla f(x_{k+1})-(1-\gamma  (L+\mu))\nabla f(x_k) +\gamma  \mu  L (x_\star-x_k)\rVert^2\\<br />&amp;-\frac{\gamma  L(2- \gamma  (L+\mu))}{2 (L-\mu )}\lVert \nabla f(x_k)+\mu  (x_\star-x_k)\rVert^2.\end{array}$$ Again, this statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation), and verifying that all terms match. The desired conclusion $$ f(x_{k+1})-f_\star \leqslant \left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right), $$ follows from the signs of the leading coefficients: \(\gamma(2-\gamma (L+\mu)) \geqslant      0\), and \(L-\mu \geqslant      0\).</p>



<h3>To go further</h3>



<p class="justify-text">Before finishing, let us mention that we only dealt with linear convergence through a single iteration of gradient descent.</p>



<p class="justify-text">There are quite a few ways to handle both more iterations and sublinear convergence rates. Using SDPs, probably the most natural approach is to directly incorporate several iterations in the problem by  studying, for example, ratios of the form  $$\sup_{f\in\mathcal{F}_{\mu,L},\, x_0}  \frac{f(x_{N})-f_\star}{\lVert x_0-x_\star\rVert^2}. $$ This type of approach was used in the work of Drori and Teboulle [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>] and in most consecutive PEP-related works: it has the advantage of providing  comfortable “non-improvable results” [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>]   (by providing matching lower bounds) for any given \(N\), but requires solving larger and larger SDPs. Alternatively, simpler proofs can often be obtained through the use of  Lyapunov (or potential) functions—i.e., study a single iteration to produce recursable inequalities; a nice introduction is provided in [<a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">13</a>]. This idea can be exploited in PEPs [<a href="https://arxiv.org/pdf/1902.00947.pdf">14</a>] by enforcing the proofs to have a certain structure. Those principles are also at the heart of the related approach using integral quadratic constraints [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>, <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">15</a>]. </p>



<h2>Take-home message and conclusions</h2>



<p class="justify-text">The overall message of this note is that first-order methods can often be studied directly using the definition of their “worst-cases” (i.e., by trying to find worst-case scenarios), along with their dual counterparts (linear combinations of inequalities), by translating them into semidefinite programs.</p>



<p class="justify-text">What we saw might look like an overkill for studying gradient descent. However, as long as we deal with Euclidean spaces, the same approach actually works beyond this simple case. In particular, the same technique applies to first-order methods performing explicit, projected, proximal, conditional, and inexact (sub)gradient steps [<a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>].</p>



<p class="justify-text">Finally, let us mention a few previous works illustrating that the use of such computer-assisted proofs allowed obtaining results that are apparently too complicated for us to find bare-handed—even in apparently simple contexts.  Reasonable examples include the direct proof for convergence rates in  function values [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>] presented above, but also proofs arising in the context of optimized numerical schemes [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>, <a href="https://arxiv.org/abs/1409.2636">16</a>, <a href="https://arxiv.org/pdf/1406.5468.pdf">17</a>, <a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>]—in particular [<a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>] presents a method for minimizing the gradient norm at the last iterate, in smooth convex minimization—,  in the context of monotone inclusions,  and even for more general fixed-point problems (e.g., for Halpern iterations [<a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">19</a>]).</p>



<h3>Toolbox</h3>



<p class="justify-text">The PErformance EStimation TOolbox (PESTO) [<a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">20</a>, see <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/graphs/traffic">Github</a>] allows a quick access to the methodology without worrying about details of semidefinite reformulations. The toolbox contains many  examples (about 50) in different settings, and include progresses on the approach, and results, by other groups (which are much more thoroughly referenced in the <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/blob/master/UserGuide.pdf">user guide</a>). In particular, we included standard classes of functions and operators, along with examples for analyzing recent optimized methods.</p>



<h2>References</h2>



<p class="justify-text">[1] Mark Schmidt, Nicolas Le Roux, Francis Bach. <a href="https://arxiv.org/pdf/1309.2388.pdf">Minimizing finite sums with the stochastic average gradient</a>. <em>Mathematical Programming</em>, <em>162</em>(1-2), 83-112, 2017.<br />[2] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/pdf/1206.3209.pdf">Performance of first-order methods for smooth convex minimization: a novel approach</a>. <em>Mathematical Programming</em>, 145(1-2), 451-482, 2014.<br />[3] Laurent Lessard, Benjamin Recht, Andrew Packard. <a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">Analysis and design of  optimization algorithms via integral quadratic constraints</a>. <em>SIAM Journal on Optimization</em>, 26(1), 57-95, 2016.<br />[4] Adrien Taylor,  Julien Hendrickx, François Glineur. <a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">Smooth strongly convex interpolation and exact worst-case performance of first-order methods</a>. <em>Mathematical Programming</em>, 161(1-2), 307-345, 2017.<br />[5] Yurii Nesterov. <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">Introductory Lectures on Convex Optimization : a Basic Course</a>. <em>Applied optimization</em>. Kluwer Academic Publishing, 2004.<br />[6]  Boris Polyak. Introduction to Optimization. Optimization Software New York, 1987.<br />[7] Daniel Azagra, Carlos Mudarra. <a href="https://arxiv.org/pdf/1603.00241.pdf">An extension theorem for convex functions of class \(C^{1,1}\) on Hilbert spaces</a>. <em>Journal of Mathematical Analysis and Applications</em>, 446(2):1167–1182, 2017.<br />[8] Aris Daniilidis, Mounir Haddou, Erwan Le Gruyer, Olivier Ley. <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">Explicit formulas for \(C^{1,1}\) Glaeser-Whitney extensions of 1-Taylor fields in Hilbert spaces</a>. <em>Proceedings of the American Mathematical Society</em>, 146(10):4487–4495, 2018.<br />[9] Johan Löfberg. <a href="https://yalmip.github.io/">YALMIP : A toolbox for modeling and optimization in MATLAB</a>. <em>Proceedings of the CACSD Conference</em>, 2004.<br />[10] APS Mosek. <a href="https://www.mosek.com/">The MOSEK optimization software</a>. Online at http://www.mosek.com, 54, 2010.<br />[11] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1705.04398.pdf">Exact worst-case convergence rates of the proximal gradient method for  composite convex minimization</a>. <em>Journal of Optimization Theory and Applications</em>, vol. 178, no 2, p. 455-476, 2018.<br />[12] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1512.07516.pdf">Exact worst-case performance of first-order methods for composite convex optimization</a>. <em>SIAM Journal on Optimization</em>, vol. 27, no 3, p. 1283-1313, 2017.<br />[13] Nikhil Bansal, Anupam Gupta. <a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">Potential-Function Proofs for Gradient Methods. <em>Theory of Computing</em></a>, <em>15</em>(1), 1-32, 2019.<br />[14] Adrien Taylor, Francis Bach. <a href="https://arxiv.org/pdf/1902.00947.pdf">Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions</a>, <em>Proceedings of the 32nd Conference on Learning Theory (COLT)</em>, 99:2934-2992, 2019.  <br />[15] Bin Hu, Laurent Lessard. <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">Dissipativity theory for Nesterov’s accelerated method</a>, <em>Proceedings of the 34th International Conference on Machine Learning</em>, 70:1549-1557, 2017. <br />[16] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/abs/1409.2636">An optimal variant of Kelley’s cutting-plane method</a>. <em>Mathematical Programming</em> 160.1-2: 321-351, 2016.<br />[17] Donghwan<strong> </strong>Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1406.5468.pdf">Optimized first-order methods for smooth convex minimization</a>, <em>Mathematical programming</em>, <em>159</em>(1-2), 81-107, 2016.<br />[18] Donghwan Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1803.06600.pdf">Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions</a>, <em>preprint arXiv:1803.06600</em>, 2018.   <br />[19] Felix Lieder. <a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">On the convergence rate of the Halpern-iteration</a>. Technical Report, 2019.<br />[20] Adrien Taylor, Julien Hendrickx, François Glineur.  <a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">Performance estimation toolbox (PESTO): automated worst-case analysis of  first-order optimization methods</a>,<em> Proceedings of the 56th Annual Conference on Decision and Control (CDC)</em>, pp. 1278-1283, 2017.</p></div>







<p class="date">
by Adrien Taylor <a href="https://francisbach.com/computer-aided-analyses/"><span class="datestr">at April 03, 2020 11:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=19411">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/">Trees not Cubes! Memories of Boris Tsirelson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This post is devoted to a few memories of Boris Tsirelson who passed away at the end of January. I would like to mention that a few days ago graph theorist Robin Thomas passed away after long battle with ALS. I hope  to tell about Robin’s stunning mathematics in a future post.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg?w=640&amp;h=480" class="alignnone size-full wp-image-19552" height="480" /></a></p>
<p>Boris Tsirelson (1950 – 2020); <a href="http://www.math.tau.ac.il/~tsirel/">Boris’ home-page</a>, and <a href="https://en.wikipedia.org/wiki/Boris_Tsirelson">Wikipedia</a>. (More links, below.)</p>
<p>The title of the post is taken from the title of a very interesting 1999 paper by Boris Tsirelson and Oded Schramm: <a href="https://arxiv.org/abs/math/9902116">Trees, not cubes: hypercontractivity, cosiness, and noise stability</a></p>
<p>I was very sad and shocked to hear that Boris Tsirelson had passed away. Boris was one of the greatest Israeli mathematicians, and since 1997 or so we established mathematical connections surrounding several matters of common interest.  Here are a few memories.</p>
<h3>Love for coding</h3>
<p>1) One thing that Boris told me was that he loves to code. Being a “refusnik”, he could not get into Academia and (luckily) he could work as a programmer. And he told me that afterwards deciding what he liked more – programming or doing mathematical research – was no longer a trivial question for him.  Boris chose to go back to mathematical research, but he continued to enjoy programming, and when he needed it in his mathematical research, he could easily program.</p>
<h3>Love for quantum</h3>
<p>2) Another thing that Boris loved is “quantum”, the mathematics and physics of quantum mechanics and various connections to mathematics. Early on he proved his famous Tsirelson’s bound related to Bell’s inequalities, and later he was enthusiastic about the area of quantum computing. (And he learned it quickly, taught a course about it in 1997, and his 1997 lecture notes are still considered very useful.)</p>
<h3>Black Noise and noise sensitivity</h3>
<p>3) Perhaps the most significant mathematical connection between us was in the late 90s, and was centered around the theory of noise stability and noise sensitivity by Benjamini, Schramm and myself, which was closely related to a theory initiated by Boris Tsirelson and Anatoly Vershik. The translation between the different languages that we used and that Boris used was awkward, since the analog of Boolean functions that we studied was the “noise” that Boris studied, and the analog of noise sensitive Boolean functions in our language was “black noise” in Boris’s language. In any case, we had email discussions and we also met a few times with Itai and Oded regarding this connection.</p>
<h3>Black Noise and noise sensitivity II</h3>
<p>4)  Boris developed a very rich theory of black noise with relations to various areas of probability theory and operator algebras. He also found hypercontractivity that we used in our work quite useful to his applications, and also in this theory, he considered both classical and quantum aspects. I know only a little about Boris Tsirelson’s theory and its applications, but as far as tangent points with our Boolean interests are concerned, I can mention that Boris was enthusiastic by the <a href="https://arxiv.org/abs/1101.5820">result</a> of Schramm and Stas Smirnov that percolation is a “black noise” and also that, in 1999, Boris and Oded Schramm wrote a paper whose title started with “Trees not cubes!”, presenting a different angle on this theory.</p>
<p>Tsirelson saw white noise (what we call noise stability) as manifesting “linearity” while “black noise” (what we call noise sensitivity) as manifesting “non-linearity”. Over the years, I often asked him to explain this to me.</p>
<h3>Tsireslon’s Banach spaces</h3>
<p>5) Geometry of Banach spaces is a very strong area in Israel so naturally I heard as a graduate student about “Tsirelson’s space” from 1974 and some subsequent developments in the 80s. Boris Tsirelson constructed a  Banach space that does not contain an imbedding of <img src="https://s0.wp.com/latex.php?latex=%5Cell_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\ell_p" class="latex" title="\ell_p" /> or <img src="https://s0.wp.com/latex.php?latex=c_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="c_0" class="latex" title="c_0" />.</p>
<h3>Bible codes</h3>
<p>6) My first personal connection with Boris was related to claims regarding a hidden Bible Code, and a 1994 paper claiming a statistical proof of the existence of these Bible codes. For many years my attitude was that these claims should be ignored, but around 1997, I changed my mind and did some work to see what was going on. Now, Boris kept a site linked in his homepage devoted to developments regarding the Bible Code claims. In this site Boris kindly reported about my first 1997 paper on the topic, my observation that the proximity of two reported p-values for the two Bible code experiments was “too good to be true”, and my interpretation that this suggests that the claimed results manifest naïve statistical expectations rather than scientific reality.  A few weeks later, Boris reported about a much stronger evidence (by McKay and Bar Nathan) against the Bible Code claims (they demonstrated codes of similar quality in Tolstoy’s “War and Peace”) and subsequently after some time he lost interest in this topic.</p>
<h3>Quantum computing skepticism</h3>
<p>6) In 2005 we had some correspondence and meetings regarding my quantum computing skepticism. In his first email he told me that my reference to “decoherence” seemed strange and I realized that I consistently referred to “entanglement” as “decoherence” and to “decoherence” as “entanglement”.</p>
<p>7) In his 1997 lecture notes on quantum computing (that I cannot find on the web, so I count on my memory), Boris addressed the concerns of early quantum computers skeptics like Rolf Landauer. He did not accept the analogy between quantum computing and analog computation, but he also regarded the analogy with digital computation as problematic. Rather, he regarded quantum information based on qubits as something (at least a priori) different from both these examples. (Update: I found one non-broken link to the lecture notes; indeed the subtitle of Chapter 9 is “neither analog nor digital”.)</p>
<p>A joke that I heard from Boris at that time</p>
<p>8) I remember that once when I asked him about some aspects of quantum fault tolerance he told me the following joke: A student is entitled to a special exam, he arrives at the professor’s office, is given three questions to answer and he fails to do so. He request and is granted a make-up exam two weeks later. When the student shows up at the office two weeks later the professor, who forgot all about it, gave him the same three questions. “This is extremely unfair”, said the student “you ask me questions that you already know that I cannot answer.”</p>
<h3>Noise sensitivity and high energy physics</h3>
<p>9) In 2006 I came up with the idea that noise sensitivity might be a great idea for physics. Knowing very little physics, I wrote a little manifesto with this idea and tried it, among other people, on Boris. As it turned out, Boris had the idea that noise sensitivity could add a useful modeling power to physics (especially high energy physics) well before that time. (And by 2006 he was already a bit skeptical regarding his own idea.) He also told me that one of the motivations of his 1998 paper with Tolya Vershik came from some mathematical ideas related to physics of the big bang. When I asked him if this was written somewhere in the paper itself, he answered: “Of course, not!”</p>
<h3>Boris Tsirelson’s lecture at Oded’s memorial school</h3>
<p>10) in 2009 we organized a meeting in memory of Oded Schramm and Boris gave a lecture related to the Schramm-Smirnov “percolation is black noise” result with a single theorem. And what was remarkable about it that it was that he presented a classical theorem with a quantum proof. You can find the videotaped lecture here  (And here are the slides. Boris never wrote up this result.) Following this lecture we had a short correspondence with Scott A. and Greg K. about quantum proofs to classical theorems. (Namely theorems that do not mention quantum in the statement).</p>
<h3>Tsirelson’s problem</h3>
<p>11) Our last correspondence in 2019 was about Thomas Vidick’s  Notices AMS article about Tsirelson’s problem. (This was a couple of months before the announcement of the solution.) Boris was pleased to hear about these developments, as he was regarding earlier developments in this area. He humorously refers to the history of his problem on his homepage and this interview.</p>
<p>12) People who knew Boris regarded him as a genius from a very early age, and former students have fond memories of his classes.</p>
<p> </p>
<h3>More resources:</h3>
<p>Boris’s <a href="http://www.math.tau.ac.il/~tsirel/">home page</a> contains  “Museum to my courses” with many useful lecture notes; link to a small <a href="http://www.math.tau.ac.il/~tsirel/Research/qcomp/main.html">page on quantum computation</a> with a link to Boris’ <a href="http://www.math.tau.ac.il/~tsirel/Courses/QuantInf/syllabus.html">1997 lecture notes on quantum computing</a>.  Links to comments on some of Tsirelson’s famous papers. <a href="http://www.math.tau.ac.il/~tsirel/Research/mybound/main.html">Tsirelson’s 1980 bound</a>. Boris <a href="http://www.math.tau.ac.il/~tsirel/Research/refereed.html">published papers</a>, and his “<a href="http://www.math.tau.ac.il/~tsirel/Research/self-publ.html">self-published</a>” papers.</p>
<p>Boris was a devoted Wikipedian and his Wikipidea <a href="https://en.wikipedia.org/wiki/User:Tsirel">user page</a> is now devoted to his memory; Here is <a href="https://www.iqoqi-vienna.at/en/blog/article/boris-tsirelson/?fbclid=IwAR1PrVvK0u5XmnFLLoPMzMN3x9rY1WIdp1wrYZ_yYPlqSGRpXDkollYTCR0">a great interview</a> with Boris; A very nice <a href="https://www.scottaaronson.com/blog/?p=4626">memorial post</a> on Freeman Dyson and Boris Tsirelson on the Shtetl Optimized; Tim Gowers explains some ideas behind Tsirelson’s space <a href="https://twitter.com/wtgowers/status/1231704267641249794">over Twitter;</a> and here in <a href="https://gowers.wordpress.com/2009/02/17/must-an-explicitly-defined-banach-space-contain-c_0-or-ell_p/">Polymath2</a>.</p>
<p>Below the fold some emails of interest from Boris, mainly where he explained to me various mathematics. (More can be found in this page.)</p>
<p><span id="more-19411"></span></p>
<h2>Some email correspondence</h2>
<h3>Oct. 2019 Vidick’s paper</h3>
<p>Dear Boris<br />
<a href="https://www.ams.org/journals/notices/201910/rnoti-p1618.pdf">This paper</a> by Thomas Vidick may interest you,<br />
best regards and shana tova Gil</p>
<p>Oh yes, sure!<br />
Thank you.<br />
Shana metuka, Boris</p>
<p>(Remark: “metuka” means “sweet” in Hebrew.)</p>
<h3>Dec 2006 (about noise sensitivity and physics)</h3>
<p>(Dec 2006) My very first idea in this field (inspired by conversations with Vershik) was<br />
rather physical (that Big Bang could be a natural occurrence of black noise),<br />
and in fact the main example of “Tsirelson and Vershik 1998” follows this line<br />
(not explicitly, of course).</p>
<p>In local (not Big Bang related) physics, I think, nonlinearity could produce<br />
such effect. And then the very idea of `the field operator at a point’ (on<br />
the level of operator-valued Schwartz distributions or something like that)<br />
will fail. However, physicists do not want to consider this possibility<br />
without very serious indications that it really is used by the nature. And<br />
they are right…</p>
<h3>2005 quantum computer skepticism</h3>
<p>Subject: Re: Noise and more</p>
<p>Dear Gil:<br />
Yes, of course, we can meet and speak.</p>
<p>For now, I am not much bothered. I am not an expert in quantum error<br />
correction, but anyway, my feeling is that all physically reasonable<br />
“attacks” of Nature are repelled. Especially, your three-qubit attack<br />
looks to me not dangerous. And, “der Herr Gott is raffiniert, aber<br />
boschaft ist er nicht”; Nature never attacks like an enemy.</p>
<p>Yours, Boris.</p>
<h3>Quick 2000 comments on a (sloppy) draft of my survey paper</h3>
<p>Dear Gil:</p>
<p>Thank you for the text; I am reading it.<br />
For now, only a trivial remark: “Tsilerson” should be “Tsirelson” in<br />
[133] and [134]; and in [132] “” should be “Tsirelson”…</p>
<p>Shabat shalom,<br />
Yours, Boris.</p>
<h3>November 1998: Noise sensitivity and black noise</h3>
<p>Dear Gil,</p>
<p>I am reading your (with Itai and Oded) paper. Thanks.</p>
<p>Moreover, I am thinking about changing the title of my future talk in<br />
Vien accordingly: from “The five noises” to “The six noises” (or even<br />
more).<br />
To this end, however, I need to answer the following question.</p>
<p>Is there a mesh refinement limit for the percolation?</p>
<p>That is, take the lattice with a small pitch \eps. Choose two<br />
“electrodes”, say, two vertical intervals on two parallel vertical<br />
lines, and ask about the probability that they are connected (via the<br />
bond percolation on the whole band between the two vertical lines).<br />
Let the electrodes be macroscopic; that is, they do not depend on<br />
\eps. Does the probability of the event have a limit for \eps \to 0 ?<br />
If it does, then one more question: what about the joint distribution<br />
for a finite collection of such events? That is, I want to see a weak<br />
limit of these “discrete” random processes. It seems to me, the<br />
question is well-known and was discussed. However, do you know the<br />
answer?</p>
<p>Yours, Boris.</p>
<p>Dear Itai, Oded, and Gil,</p>
<p>Thank you for the information. I see that for now we have a<br />
conditional result: if there exists “the noise of percolation”, then<br />
it is not a white noise.<br />
Yours, Boris.</p>
<p>Dear Gil:<br />
&gt; I dont understand yet the concept of “noise” precisely</p>
<p>One of ways is this. A noise is a scaling limit for coin tossing.<br />
You choose a class of “macroscopic observables” and look, whether<br />
their joint distribution converges, when n\to\infty. If it does, you<br />
get a noise. (For percolation we do not know, does it or not.)<br />
Now, if all “macroscopic observables” are noise insensitive, it means<br />
that the noise is white. For a white noise, there is only one<br />
invariant, its dimension (or multiplicity).<br />
If all “macroscopic observables” are noise sensitive, the noise is<br />
black. Probably, there are a lot of black noises, but for now we have<br />
only two examples, without knowing, whether they are isomorphic, or<br />
not. Spectra may be used for classifying black noises. Say, it may<br />
happen that for each “macroscopic observable”, its spectrum is<br />
concentrated on sets having Hausdorf dimension less than something.<br />
If some “macroscopic observables” are noise sensitive but some others<br />
are not (except for constants, of course), then the noise is neither<br />
white nor black. It may happen that it is a direct sum of a white<br />
noise and a black noise. However, it may happen that it is not. We<br />
have for now two such examples: “noise if splitting” and “noise of<br />
stickiness” (they are probably non-isomorphic); both are found by Jonathan<br />
Warren. I am trying to understand, whether your matter can give more<br />
examples.</p>
<p> </p>
<h3>August 1998: Bible code story</h3>
<p>Dear Gil,</p>
<p>Thanks for the text.<br />
As for me, it is already an `overkill’, since for me the WRR94<br />
is basically dead. But maybe for others…</p>
<p>Yours, Boris.</p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/"><span class="datestr">at April 03, 2020 08:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1644">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2020/04/01/approx-random-2020-is-virtual-from-the-get-go/">APPROX/RANDOM 2020 is Virtual From the Get Go</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>While <a href="https://randomconference.com/random-2020-home/">APPROX/RANDOM 2020</a> was scheduled for September, we decided to reduce uncertainty in these stormy days and declare it to be virtual already in the <a href="https://randomconference.files.wordpress.com/2020/04/randomapprox2020cfp-3.pdf">CFP</a>. We hope to have APPROX/RANDOM 2021 hosted in Seattle by UW, instead of this year,  So if you never sent papers to APPROX/RANDOM because you hate travel, this is your year to start!</p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2020/04/01/approx-random-2020-is-virtual-from-the-get-go/"><span class="datestr">at April 01, 2020 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=19719">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/">A small update from Israel and memories from Singapore: Partha Dasgupta, Robin Mason, Frank Ramsey, and 007</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h3>A small update about the situation here in Israel</h3>
<p>Eight weeks ago <a href="https://gilkalai.wordpress.com/2020/02/03/thinking-about-the-people-of-wuhan-and-china/">I wrote</a> that my heart goes out to the people of Wuhan and China, and these days my heart goes out to people in Italy, Spain, the US, Iran, France, the United Kingdom, Germany, Netherland and many other countries all over the world. Of course, I am especially worried about the situation here in my beloved country Israel, and let me tell you a little about it.</p>
<p>The pandemic started here late but it hit us pretty hard with 5,358 identified cases yesterday. Severe measures of social distancing were gradually introduced, and right now it is too early to tell if the pandemic is under control.</p>
<p>My part in this struggle is to stay at home. (Many Israeli scientists are making various endeavors and proposing ideas of various kind for fighting the disease and I salute them all for their efforts.) Like all of us I am very thankful to medical and other essential workers who are in the front-lines. As a scientist, I am especially impressed by the people from the Ministry of Health who manage the crisis and communicate with the public. They represent the very best we can offer in terms of science and medicine, decision making, gathering information, communicating with the public, and managing the crisis. In the picture below you can see three of the leaders – Moshe Bar Siman Tov (middle) Prof. Itamar Grotto (right) and Professor Sigal Sadetzki (left).</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/listen.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/03/listen.png?w=640&amp;h=371" class="alignnone size-full wp-image-19695" height="371" /></a></p>
<h2>And now for today’s post</h2>
<p>We had a tradition of sharing entertaining taxi-and-more stories and this post belongs to this category. We note that our highest quality story teller Michal Linial, a prominent Israeli biologist, is now involved in various aspects of the struggle against the disease. Our post today is part of<a href="https://gilkalai.files.wordpress.com/2020/03/gil-michal.docx"> a report by Michal Feldman and me on our experience from the ICA3 conferences in Singapore and Birmingham</a>.</p>
<h2>Partha Dasgupta, Robin Mason, Frank Ramsey, and James Bond</h2>
<p>After hearing about him for many years, it was a great pleasure for both Michal Feldman and myself to finally meet Partha Dasgupta in person and to listen to his lecture. Partha who is the Frank Ramsey Professor of Economics at Cambridge was introduced by a person, who entered the room directly from an intercontinental flight, whom we did not know but who made a strong impression on us. He devoted part of his introduction to Frank Ramsey who was a mathematician, philosopher and economist, and who had made fundamental contributions to algebra and had developed the canonical model of saving in economic growth, before he died at the young age of 26. (And yes! also Ramsey’s theorem!)</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/james-bond.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/03/james-bond.png?w=640&amp;h=423" class="alignnone size-full wp-image-19699" height="423" /></a></p>
<p>Seeing the introducer, Robin Mason, three words came into our minds (more precisely two words, one repeated twice): “Bond, James Bond.”</p>
<p>Indeed, this has led to the following sequence of profound ideas:</p>
<p>1) Robin Mason is a perfect choice for a new generation James Bond.</p>
<p>2) The name “James Bond” is overused. “Robin Mason” is a perfect name to replace the name “James Bond”.</p>
<p>3) Espionage is a little obsolete and it lost much of its prestige and charm. Science and academia is the new thing! An international interdisciplinary academics is the perfect profession which, at present, deserves the prestige formely associated with espionage.</p>
<p>In summary, we came a full circle. Robin Mason is the perfect new choice for James Bond, “Robin Mason” is the perfect new name to replace the name “James Bond,” and Mason’s academic activities and title of Pro-Vice-Chancellor (International) are the perfect replacement for Bond’s activities and the title ‘007’.</p>
<p>(The option of Mason playing his role on the movies rather than in real life should be considered. ‘Q’ could be handy for science as well. )</p>
<p><a href="https://youtu.be/dMSHmHc0z-E">Clique here</a> for Robin’s introduction and Partha’s lectur</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/"><span class="datestr">at April 01, 2020 07:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
