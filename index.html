<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.blogger.com/feeds/25562705/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 30, 2019 09:23 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12477">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12477">Understanding the Effectiveness of Data Reduction in Public Transportation Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischbeck:Philipp.html">Philipp Fischbeck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schirneck:Martin.html">Martin Schirneck</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12477">PDF</a><br /><b>Abstract: </b>Given a public transportation network of stations and connections, we want to
find a minimum subset of stations such that each connection runs through a
selected station. Although this problem is NP-hard in general, real-world
instances are regularly solved almost completely by a set of simple reduction
rules. To explain this behavior, we view transportation networks as hitting set
instances and identify two characteristic properties, locality and
heterogeneity. We then devise a randomized model to generate hitting set
instances with adjustable properties. While the heterogeneity does influence
the effectiveness of the reduction rules, the generated instances show that
locality is the significant factor. Beyond that, we prove that the
effectiveness of the reduction rules is independent of the underlying graph
structure. Finally, we show that high locality is also prevalent in instances
from other domains, facilitating a fast computation of minimum hitting sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12477"><span class="datestr">at May 30, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12461">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12461">On the Clique-Width of Unigraphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakahata:Yu.html">Yu Nakahata</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12461">PDF</a><br /><b>Abstract: </b>Clique-width is a well-studied graph parameter. For graphs of bounded
clique-width, many problems that are NP-hard in general can be polynomial-time
solvable. The fact motivates many studies to investigate whether the
clique-width of graphs in a certain class is bounded or not. We focus on
unigraphs, that is, graphs uniquely determined by their degree sequences up to
isomorphism. We show that every unigraph has clique-width at most 5. It follows
that many problems that are NP-hard in general are polynomial-time solvable for
unigraphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12461"><span class="datestr">at May 30, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12442">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12442">Rank-one Multi-Reference Factor Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aizenbud:Yariv.html">Yariv Aizenbud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landa:Boris.html">Boris Landa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shkolnisky:Yoel.html">Yoel Shkolnisky</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12442">PDF</a><br /><b>Abstract: </b>In recent years, there is a growing need for processing methods aimed at
extracting useful information from large datasets. In many cases the challenge
is to discover a low-dimensional structure in the data, often concealed by the
existence of nuisance parameters and noise. Motivated by such challenges, we
consider the problem of estimating a signal from its scaled, cyclically-shifted
and noisy observations. We focus on the particularly challenging regime of low
signal-to-noise ratio (SNR), where different observations cannot be
shift-aligned. We show that an accurate estimation of the signal from its noisy
observations is possible, and derive a procedure which is proved to
consistently estimate the signal. The asymptotic sample complexity (the number
of observations required to recover the signal) of the procedure is
$1/\operatorname{SNR}^4$. Additionally, we propose a procedure which is
experimentally shown to improve the sample complexity by a factor equal to the
signal's length. Finally, we present numerical experiments which demonstrate
the performance of our algorithms, and corroborate our theoretical findings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12442"><span class="datestr">at May 30, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12412">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12412">A unified variance-reduced accelerated gradient method for convex optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lan:Guanghui.html">Guanghui Lan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zhize.html">Zhize Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Yi.html">Yi Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12412">PDF</a><br /><b>Abstract: </b>We propose a novel randomized incremental gradient algorithm, namely,
VAriance-Reduced Accelerated Gradient (Varag), for finite-sum optimization.
Equipped with a unified step-size policy that adjusts itself to the value of
the conditional number, Varag exhibits the unified optimal rates of convergence
for solving smooth convex finite-sum problems directly regardless of their
strong convexity. Moreover, Varag is the first of its kind that benefits from
the strong convexity of the data-fidelity term, and solves a wide class of
problems only satisfying an error bound condition rather than strong convexity,
both resulting in the optimal linear rate of convergence. Varag can also be
extended to solve stochastic finite-sum problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12412"><span class="datestr">at May 30, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12379">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12379">Reallocating Multiple Facilities on the Line</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fotakis:Dimitris.html">Dimitris Fotakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kavouras:Loukas.html">Loukas Kavouras</a>, Panagiotis Kostopanagiotis, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lazos:Philip.html">Philip Lazos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skoulakis:Stratis.html">Stratis Skoulakis</a>, Nikolas Zarifis <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12379">PDF</a><br /><b>Abstract: </b>We study the multistage $K$-facility reallocation problem on the real line,
where we maintain $K$ facility locations over $T$ stages, based on the
stage-dependent locations of $n$ agents. Each agent is connected to the nearest
facility at each stage, and the facilities may move from one stage to another,
to accommodate different agent locations. The objective is to minimize the
connection cost of the agents plus the total moving cost of the facilities,
over all stages. $K$-facility reallocation was introduced by de Keijzer and
Wojtczak, where they mostly focused on the special case of a single facility.
Using an LP-based approach, we present a polynomial time algorithm that
computes the optimal solution for any number of facilities. We also consider
online $K$-facility reallocation, where the algorithm becomes aware of agent
locations in a stage-by-stage fashion. By exploiting an interesting connection
to the classical $K$-server problem, we present a constant-competitive
algorithm for $K = 2$ facilities.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12379"><span class="datestr">at May 30, 2019 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12372">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12372">Resolution Lower Bounds for Refutation Statements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garl=iacute=k:Michal.html">Michal Garlík</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12372">PDF</a><br /><b>Abstract: </b>For any unsatisfiable CNF formula we give an exponential lower bound on the
size of resolution refutations of a propositional statement that the formula
has a resolution refutation. We describe three applications. (1) An open
question in (Atserias, M\"uller 2019) asks whether a certain natural
propositional encoding of the above statement is hard for Resolution. We answer
by giving an exponential size lower bound. (2) We show exponential resolution
size lower bounds for reflection principles, thereby improving a result in
(Atserias, Bonet 2004). (3) We provide new examples of CNFs that exponentially
separate Res(2) from Resolution (an exponential separation of these two proof
systems was originally proved in (Segerlind, Buss, Impagliazzo 2004)).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12372"><span class="datestr">at May 30, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12233">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12233">Improved Analysis of Highest-Degree Branching for Feedback Vertex Set</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iwata:Yoichi.html">Yoichi Iwata</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yusuke.html">Yusuke Kobayashi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12233">PDF</a><br /><b>Abstract: </b>Recent empirical evaluations of exact algorithms for Feedback Vertex Set have
demonstrated the efficiency of a highest-degree branching algorithm with a
degree-based pruning heuristic. In this paper, we prove that this empirically
fast algorithm runs in $O(3.460^k n)$ time, where $k$ is the solution size.
This improves the previous best $O(3.619^k n)$-time deterministic algorithm
obtained by Kociumaka and Pilipczuk.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12233"><span class="datestr">at May 30, 2019 01:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12145">Minimizing approximately submodular functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Halabi:Marwa_El.html">Marwa El Halabi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jegelka:Stefanie.html">Stefanie Jegelka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12145">PDF</a><br /><b>Abstract: </b>The problem of minimizing a submodular function is well studied; several
polynomial-time algorithms have been developed to solve it exactly or up to
arbitrary accuracy. However, in many applications, the objective functions are
not exactly submodular. In this paper, we show that a classical algorithm used
for submodular minimization performs well even for a class of non-submodular
functions, namely weakly DR-submodular functions. We provide the first
approximation guarantee for non-submodular minimization. This broadly expands
the range of applications of submodular minimization techniques.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12145"><span class="datestr">at May 30, 2019 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12143">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12143">The Impact of RDMA on Agreement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aguilera:Marcos_K=.html">Marcos K. Aguilera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=David:Naama.html">Naama Ben-David</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guerraoui:Rachid.html">Rachid Guerraoui</a>, Virendra Marathe, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zablotchi:Igor.html">Igor Zablotchi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12143">PDF</a><br /><b>Abstract: </b>Remote Direct Memory Access (RDMA) is becoming widely available in data
centers. This technology allows a process to directly read and write the memory
of a remote host, with a mechanism to control access permissions. In this
paper, we study the fundamental power of these capabilities. We consider the
well-known problem of achieving consensus despite failures, and find that RDMA
can improve the inherent trade-off in distributed computing between failure
resilience and performance. Specifically, we show that RDMA allows algorithms
that simultaneously achieve high resilience and high performance, while
traditional algorithms had to choose one or another. With Byzantine failures,
we give an algorithm that only requires $n \geq 2f_P + 1$ processes (where
$f_P$ is the maximum number of faulty processes) and decides in two (network)
delays in common executions. With crash failures, we give an algorithm that
only requires $n \geq f_P + 1$ processes and also decides in two delays. Both
algorithms tolerate a minority of memory failures inherent to RDMA, and they
provide safety in asynchronous systems and liveness with standard additional
assumptions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12143"><span class="datestr">at May 30, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.12118">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.12118">Cyclicality, Periodicity and the Topology of Time Series</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Paweł Dłotko, Wanling Qiu, Simon Rudkin <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.12118">PDF</a><br /><b>Abstract: </b>Periodic and semi periodic patterns are very common in nature. In this paper
we introduce a topological toolbox aiming in detecting and quantifying
periodicity. The presented technique is of a general nature and may be employed
wherever there is suspected cyclic behaviour in a time series with no trend.
The approach is tested on a number of real-world examples enabling us to
consistently demonstrate an ability to recognise periodic behaviour where
conventional techniques fail to do so. Quicker to react to changes in time
series behaviour, and with a high robustness to noise, the toolbox offers a
powerful way to deeper understanding of time series dynamics.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.12118"><span class="datestr">at May 30, 2019 01:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11830">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11830">A Graph Theoretic Additive Approximation of Optimal Transport</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lahn:Nathaniel.html">Nathaniel Lahn</a>, Deepika Mulchandani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghvendra:Sharath.html">Sharath Raghvendra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11830">PDF</a><br /><b>Abstract: </b>Transportation cost is an attractive similarity measure between probability
distributions due to its many useful theoretical properties. However, solving
optimal transport exactly can be prohibitively expensive. Therefore, there has
been significant effort towards the design of scalable approximation
algorithms. Previous combinatorial results [Sharathkumar, Agarwal STOC '12,
Agarwal, Sharathkumar STOC '14] have focused primarily on the design of
strongly polynomial multiplicative approximation algorithms. There has also
been an effort to design approximate solutions with additive errors [Cuturi
NIPS '13, Altschuler et. al NIPS '17, Dvurechensky et al., ICML '18, Quanrud,
SOSA '19] within a time bound that is linear in the size of the cost matrix and
polynomial in $C/\delta$; here $C$ is the largest value in the cost matrix and
$\delta$ is the additive error. We present an adaptation of the classical graph
algorithm of Gabow and Tarjan and provide a novel analysis of this algorithm
that bounds its execution time by $O(\frac{n^2 C}{\delta}+
\frac{nC^2}{\delta^2})$. Our algorithm is extremely simple and executes, for an
arbitrarily small constant $\varepsilon$, only $\lfloor
\frac{2C}{(1-\varepsilon)\delta}\rfloor + 1$ iterations, where each iteration
consists only of a Dijkstra search followed by a depth-first search. We also
provide empirical results that suggest our algorithm significantly outperforms
existing approaches in execution time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11830"><span class="datestr">at May 30, 2019 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1040381893569171546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html">NSF Panels</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The government shut down in January led to delays at the National Science Foundation and only recently announcing decisions on grants submitted last fall. For those who successfully received awards, congratulations! For those who didn't, don't take it personally, buckle up and try again.<br />
<br />
For those who don't know how the process works, for each grant program, the program directors organize one or more panels which typically meets in person at NSF headquarters in Alexandria, Virginia. A typical panel has about a dozen panelists and twenty or so proposals. Before the panels, each proposal gets at least three reviews by the panelists. Discussions ensue over a day or two, proposals get sorted into categories: Highly Competitive, Competitive, Low Competitive and Not Competitive and then ranked ordered in the top categories.<br />
<br />
There are tight rules for Conflict-of-Interest and those who are conflicted have to leave the room during the discussions on those papers.<br />
<br />
If you do get asked to serve on a panel, you should definitely do so. You get to see how the process works and help influence funding and research directions in your field. You can't reveal when you serve on a particular panel but you can say "Served on NSF Panels" on your CV.<br />
<br />
Panels tend to take proposals that will likely make progress and not take ones less risky. Funding risky proposals is specifically mentioned to the panel but when push comes to shove and there is less funding than worthy proposals, panelists gravitate towards proposals that don't take chances.<br />
<br />
Panels are not unlike conference program committees. It didn't always work this way, it used to be more like journal publications. I remember when the program director would send out proposals for outside reviews and then make funding decisions. That gave the program director more discretion to fund a wider variety of proposals.<br />
<br />
The NSF budget for computing goes up slowly while the number of academic computer scientists grows at a much larger clip. Until this changes, we'll have more and more worthy proposals unfunded, particularly proposals of bold risky projects. That's the saddest part of all.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html"><span class="datestr">at May 29, 2019 08:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11968">Chasing Convex Bodies Optimally</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sellke:Mark.html">Mark Sellke</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11968">PDF</a><br /><b>Abstract: </b>In the chasing convex bodies problem, an online player receives a request
sequence of $N$ convex sets $K_1,\dots, K_N$ contained in a normed space
$\mathbb R^d$. The player starts at $x_0\in \mathbb R^d$, and after observing
each $K_n$ picks a new point $x_n\in K_n$. At each step the player pays a
movement cost of $||x_n-x_{n-1}||$. The player aims to maintain a constant
competitive ratio against the minimum cost possible in hindsight, i.e. knowing
all requests in advance. The existence of a finite competitive ratio for convex
body chasing was first conjectured in 1991 by Friedman and Linial. This
conjecture was recently resolved with an exponential $2^{O(d)}$ upper bound on
the competitive ratio.
</p>
<p>In this paper, we drastically improve the exponential upper bound. We give an
algorithm achieving competitive ratio $d$ for arbitrary normed spaces, which is
exactly tight for $\ell^{\infty}$. In Euclidean space, our algorithm achieves
nearly optimal competitive ratio $O(\sqrt{d\log N})$, compared to a lower bound
of $\sqrt{d}$. Our approach extends another recent work which chases nested
convex bodies using the classical Steiner point of a convex body. We define the
functional Steiner point of a convex function and apply it to the work function
to obtain our algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11968"><span class="datestr">at May 29, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11947">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11947">Private Identity Testing for High-Dimensional Distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canonne:Cl=eacute=ment_L=.html">Clément L. Canonne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McMillan:Audra.html">Audra McMillan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ullman:Jonathan.html">Jonathan Ullman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zakynthinou:Lydia.html">Lydia Zakynthinou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11947">PDF</a><br /><b>Abstract: </b>In this work we present novel differentially private identity
(goodness-of-fit) testers for natural and widely studied classes of
multivariate product distributions: Gaussians in $\mathbb{R}^d$ with known
covariance and product distributions over $\{\pm 1\}^{d}$. Our testers have
improved sample complexity compared to those derived from previous techniques,
and are the first testers whose sample complexity matches the order-optimal
minimax sample complexity of $O(d^{1/2}/\alpha^2)$ in many parameter regimes.
We construct two types of testers, exhibiting tradeoffs between sample
complexity and computational complexity. Finally, we provide a two-way
reduction between testing a subclass of multivariate product distributions and
testing univariate distributions, and thereby obtain upper and lower bounds for
testing this subclass of product distributions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11947"><span class="datestr">at May 29, 2019 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11924">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11924">Paper Matching with Local Fairness Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobren:Ari.html">Ari Kobren</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saha:Barna.html">Barna Saha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McCallum:Andrew.html">Andrew McCallum</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11924">PDF</a><br /><b>Abstract: </b>Automatically matching reviewers to papers is a crucial step of the peer
review process for venues receiving thousands of submissions. Unfortunately,
common paper matching algorithms often construct matchings suffering from two
critical problems: (1) the group of reviewers assigned to a paper do not
collectively possess sufficient expertise, and (2) reviewer workloads are
highly skewed. In this paper, we propose a novel local fairness formulation of
paper matching that directly addresses both of these issues. Since optimizing
our formulation is not always tractable, we introduce two new algorithms,
FairIR and FairFlow, for computing fair matchings that approximately optimize
the new formulation. FairIR solves a relaxation of the local fairness
formulation and then employs a rounding technique to construct a valid matching
that provably maximizes the objective and only compromises on fairness with
respect to reviewer loads and papers by a small constant. In contrast, FairFlow
is not provably guaranteed to produce fair matchings, however it can be 2x as
efficient as FairIR and an order of magnitude faster than matching algorithms
that directly optimize for fairness. Empirically, we demonstrate that both
FairIR and FairFlow improve fairness over standard matching algorithms on real
conference data. Moreover, in comparison to state-of-the-art matching
algorithms that optimize for fairness only, FairIR achieves higher objective
scores, FairFlow achieves competitive fairness, and both are capable of more
evenly allocating reviewers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11924"><span class="datestr">at May 29, 2019 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11888">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11888">Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11888">PDF</a><br /><b>Abstract: </b>We consider the problems of distribution estimation and heavy hitter
(frequency) estimation under privacy and communication constraints. While these
constraints have been studied separately, optimal schemes for one are
sub-optimal for the other. We propose a sample-optimal $\varepsilon$-locally
differentially private (LDP) scheme for distribution estimation, where each
user communicates only one bit, and requires no public randomness. We show that
Hadamard Response, a recently proposed scheme for $\varepsilon$-LDP
distribution estimation is also utility-optimal for heavy hitter estimation.
Finally, we show that unlike distribution estimation, without public randomness
where only one bit suffices, any heavy hitter estimation algorithm that
communicates $o(\min \{\log n, \log k\})$ bits from each user cannot be
optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11888"><span class="datestr">at May 29, 2019 11:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11877">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11877">Chasing Convex Bodies with Linear Competitive Ratio</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Argue:C=_J=.html">C. J. Argue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruganesh:Guru.html">Guru Guruganesh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Ziye.html">Ziye Tang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11877">PDF</a><br /><b>Abstract: </b>We study the problem of chasing convex bodies online: given a sequence of
convex bodies $K_t\subseteq \mathbb{R}^d$ the algorithm must respond with
points $x_t\in K_t$ in an online fashion (i.e., $x_t$ is chosen before
$K_{t+1}$ is revealed). The objective is to minimize the total distance between
successive points in this sequence. Recently, Bubeck et al. (STOC 2019) gave a
$2^{O(d)}$-competitive algorithm for this problem. We give an algorithm that is
$O(\min(d, \sqrt{d \log T}))$-competitive for any sequence of length $T$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11877"><span class="datestr">at May 29, 2019 11:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11838">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11838">A Parameterized Perspective on Protecting Elections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dey:Palash.html">Palash Dey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Misra:Neeldhara.html">Neeldhara Misra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nath:Swaprava.html">Swaprava Nath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shakya:Garima.html">Garima Shakya</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11838">PDF</a><br /><b>Abstract: </b>We study the parameterized complexity of the optimal defense and optimal
attack problems in voting. In both the problems, the input is a set of voter
groups (every voter group is a set of votes) and two integers $k_a$ and $k_d$
corresponding to respectively the number of voter groups the attacker can
attack and the number of voter groups the defender can defend. A voter group
gets removed from the election if it is attacked but not defended. In the
optimal defense problem, we want to know if it is possible for the defender to
commit to a strategy of defending at most $k_d$ voter groups such that, no
matter which $k_a$ voter groups the attacker attacks, the outcome of the
election does not change. In the optimal attack problem, we want to know if it
is possible for the attacker to commit to a strategy of attacking $k_a$ voter
groups such that, no matter which $k_d$ voter groups the defender defends, the
outcome of the election is always different from the original (without any
attack) one.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11838"><span class="datestr">at May 29, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11822">Robotic bees: Algorithms for collision detection and prevention</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Vincent Arcila, Isabel Piedrahita <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11822">PDF</a><br /><b>Abstract: </b>In the following paper we will discuss data structures suited for distance
threshold queries keeping in mind real life application such as collision
detection on robotic bees. We will focus on spatial hashes designed to store 3D
points and capable of fastly determining which of them surpass a specific
threshold from any other. In this paper we will discuss related literature,
explain in depth the data structure chosen with its design criteria, operations
and speed and memory efficiency analysis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11822"><span class="datestr">at May 29, 2019 11:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11743">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11743">Certified lattice reduction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Espitau:Thomas.html">Thomas Espitau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Joux:Antoine.html">Antoine Joux</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11743">PDF</a><br /><b>Abstract: </b>Quadratic form reduction and lattice reduction are fundamental tools in
computational number theory and in computer science, especially in
cryptography. The celebrated Lenstra-Lenstra-Lov\'asz reduction algorithm
(so-called LLL) has been improved in many ways through the past decades and
remains one of the central methods used for reducing integral lattice basis. In
particular, its floating-point variants-where the rational arithmetic required
by Gram-Schmidt orthogonalization is replaced by floating-point arithmetic-are
now the fastest known. However, the systematic study of the reduction theory of
real quadratic forms or, more generally, of real lattices is not widely
represented in the literature. When the problem arises, the lattice is usually
replaced by an integral approximation of (a multiple of) the original lattice,
which is then reduced. While practically useful and proven in some special
cases, this method doesn't offer any guarantee of success in general. In this
work, we present an adaptive-precision version of a generalized LLL algorithm
that covers this case in all generality. In particular, we replace
floating-point arithmetic by Interval Arithmetic to certify the behavior of the
algorithm. We conclude by giving a typical application of the result in
algebraic number theory for the reduction of ideal lattices in number fields.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11743"><span class="datestr">at May 29, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11635">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11635">Complexity lower bounds for computing the approximately-commuting operator value of non-local games to high precision</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coudron:Matthew.html">Matthew Coudron</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Slofstra:William.html">William Slofstra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11635">PDF</a><br /><b>Abstract: </b>We study the problem of approximating the commuting-operator value of a
two-player non-local game. It is well-known that it is $\mathrm{NP}$-complete
to decide whether the classical value of a non-local game is 1 or $1-
\epsilon$. Furthermore, as long as $\epsilon$ is small enough, this result does
not depend on the gap $\epsilon$. In contrast, a recent result of Fitzsimons,
Ji, Vidick, and Yuen shows that the complexity of computing the quantum value
grows without bound as the gap $\epsilon$ decreases. In this paper, we show
that this also holds for the commuting-operator value of a game. Specifically,
in the language of multi-prover interactive proofs, we show that the power of
$\mathrm{MIP}^{co}(2,1,1,s)$ (proofs with two provers, one round, completeness
probability $1$, soundness probability $s$, and commuting-operator strategies)
can increase without bound as the gap $1-s$ gets arbitrarily small.
</p>
<p>Our results also extend naturally in two ways, to perfect zero-knowledge
protocols, and to lower bounds on the complexity of computing the
approximately-commuting value of a game. Thus we get lower bounds on the
complexity class $\mathrm{PZK}$-$\mathrm{MIP}^{co}_{\delta}(2,1,1,s)$ of
perfect zero-knowledge multi-prover proofs with approximately-commuting
operator strategies, as the gap $1-s$ gets arbitrarily small. While we do not
know any computable time upper bound on the class $\mathrm{MIP}^{co}$, a result
of the first author and Vidick shows that for $s = 1-1/\text{poly}(f(n))$ and
$\delta = 1/\text{poly}(f(n))$, the class $\mathrm{MIP}^{co}_\delta(2,1,1,s)$,
with constant communication from the provers, is contained in
$\mathrm{TIME}(\exp(\text{poly}(f(n))))$. We give a lower bound of
$\mathrm{coNTIME}(f(n))$ (ignoring constants inside the function) for this
class, which is tight up to polynomial factors assuming the exponential time
hypothesis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11635"><span class="datestr">at May 29, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11612">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11612">Average Bias and Polynomial Sources</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharyya:Arnab.html">Arnab Bhattacharyya</a>, Philips George John, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghoshal:Suprovat.html">Suprovat Ghoshal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meka:Raghu.html">Raghu Meka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11612">PDF</a><br /><b>Abstract: </b>We identify a new notion of pseudorandomness for randomness sources, which we
call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average
bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim
Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has
min-entropy at least $k$, and so low average bias is a stronger condition than
high min-entropy. We observe that the inner product function is an extractor
for any source with average bias less than $2^{-n/2}$.
</p>
<p>The notion of average bias especially makes sense for polynomial sources,
i.e., distributions sampled by low-degree $n$-variate polynomials over
$\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see
that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show
that for quadratic sources, min-entropy $k$ implies that the average bias is at
most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for
separable quadratic sources with a min-entropy guarantee.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11612"><span class="datestr">at May 29, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11580">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11580">A near-optimal algorithm for approximating the John Ellipsoid</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Michael_B=.html">Michael B. Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cousins:Ben.html">Ben Cousins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Xin.html">Xin Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11580">PDF</a><br /><b>Abstract: </b>We develop a simple and efficient algorithm for approximating the John
Ellipsoid of a symmetric polytope. Our algorithm is near optimal in the sense
that our time complexity matches the current best verification algorithm. We
also provide the MATLAB code for further research.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11580"><span class="datestr">at May 29, 2019 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11573">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11573">On the Complexity of Distributed Splitting Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamberger:Philipp.html">Philipp Bamberger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uitto:Jara.html">Jara Uitto</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11573">PDF</a><br /><b>Abstract: </b>One of the fundamental open problems in the area of distributed graph
algorithms is the question of whether randomization is needed for efficient
symmetry breaking. While there are fast, $\text{poly}\log n$-time randomized
distributed algorithms for all of the classic symmetry breaking problems, for
many of them, the best deterministic algorithms are almost exponentially
slower. The following basic local splitting problem, which is known as the
\emph{weak splitting} problem takes a central role in this context: Each node
of a graph $G=(V,E)$ has to be colored red or blue such that each node of
sufficiently large degree has at least one node of each color among its
neighbors. Ghaffari, Kuhn, and Maus [STOC '17] showed that this seemingly
simple problem is complete w.r.t. the above fundamental open question in the
following sense: If there is an efficient $\text{poly}\log n$-time determinstic
distributed algorithm for weak splitting, then there is such an algorithm for
all locally checkable graph problems for which an efficient randomized
algorithm exists. In this paper, we investigate the distributed complexity of
weak splitting and some closely related problems. E.g., we obtain efficient
algorithms for special cases of weak splitting, where the graph is nearly
regular. In particular, we show that if $\delta$ and $\Delta$ are the minimum
and maximum degrees of $G$ and if $\delta=\Omega(\log n)$, weak splitting can
be solved deterministically in time
$O\big(\frac{\Delta}{\delta}\cdot\text{poly}(\log n)\big)$. Further, if $\delta
= \Omega(\log\log n)$ and $\Delta\leq 2^{\varepsilon\delta}$, there is a
randomized algorithm with time complexity
$O\big(\frac{\Delta}{\delta}\cdot\text{poly}(\log\log n)\big)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11573"><span class="datestr">at May 29, 2019 11:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11566">Adaptive Reduced Rank Regression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Qiong.html">Qiong Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Felix_Ming_Fai.html">Felix Ming Fai Wong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Zhenming.html">Zhenming Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yanhua.html">Yanhua Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanade:Varun.html">Varun Kanade</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11566">PDF</a><br /><b>Abstract: </b>Low rank regression has proven to be useful in a wide range of forecasting
problems. However, in settings with a low signal-to-noise ratio, it is known to
suffer from severe overfitting. This paper studies the reduced rank regression
problem and presents algorithms with provable generalization guarantees. We use
adaptive hard rank-thresholding in two different parts of the data analysis
pipeline. First, we consider a low rank projection of the data to eliminate the
components that are most likely to be noisy. Second, we perform a standard
multivariate linear regression estimator on the data obtained in the first
step, and subsequently consider a low-rank projection of the obtained
regression matrix. Both thresholding is performed in a data-driven manner and
is required to prevent severe overfitting as our lower bounds show.
Experimental results show that our approach either outperforms or is
competitive with existing baselines.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11566"><span class="datestr">at May 29, 2019 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11564">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11564">Adversarially Robust Learning Could Leverage Computational Hardness</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Sanjam.html">Sanjam Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jha:Somesh.html">Somesh Jha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahloujifar:Saeed.html">Saeed Mahloujifar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahmoody:Mohammad.html">Mohammad Mahmoody</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11564">PDF</a><br /><b>Abstract: </b>Over recent years, devising classification algorithms that are robust to
adversarial perturbations has emerged as a challenging problem. In particular,
deep neural nets (DNNs) seem to be susceptible to small imperceptible changes
over test instances. In this work, we study whether there is any learning task
for which it is possible to design classifiers that are only robust against
polynomial-time adversaries. Indeed, numerous cryptographic tasks (e.g.
encryption of long messages) are only be secure against computationally bounded
adversaries, and are indeed mpossible for computationally unbounded attackers.
Thus, it is natural to ask if the same strategy could help robust learning.
</p>
<p>We show that computational limitation of attackers can indeed be useful in
robust learning by demonstrating a classifier for a learning task in which
computational and information theoretic adversaries of bounded perturbations
have very different power. Namely, while computationally unbounded adversaries
can attack successfully and find adversarial examples with small perturbation,
polynomial time adversaries are unable to do so unless they can break standard
cryptographic hardness assumptions. Our results, therefore, indicate that
perhaps a similar approach to cryptography (relying on computational hardness)
holds promise for achieving computationally robust machine learning. We also
show that the existence of such learning task in which computational robustness
beats information theoretic robustness implies (average case) hard problems in
$\mathbf{NP}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11564"><span class="datestr">at May 29, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11512">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11512">A New Algorithm for Decremental Single-Source Shortest Paths with Applications to Vertex-Capacitated Flow and Cut Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chuzhoy:Julia.html">Julia Chuzhoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11512">PDF</a><br /><b>Abstract: </b>We study the vertex-decremental Single-Source Shortest Paths (SSSP) problem:
given an undirected graph $G=(V,E)$ with lengths $\ell(e)\geq 1$ on its edges
and a source vertex $s$, we need to support (approximate) shortest-path queries
in $G$, as $G$ undergoes vertex deletions. In a shortest-path query, given a
vertex $v$, we need to return a path connecting $s$ to $v$, whose length is at
most $(1+\epsilon)$ times the length of the shortest such path, where
$\epsilon$ is a given accuracy parameter. The problem has many applications,
for example to flow and cut problems in vertex-capacitated graphs.
</p>
<p>Our main result is a randomized algorithm for vertex-decremental SSSP with
total expected update time $O(n^{2+o(1)}\log L)$, that responds to each
shortest-path query in $O(n\log L)$ time in expectation, returning a
$(1+\epsilon)$-approximate shortest path. The algorithm works against an
adaptive adversary. The main technical ingredient of our algorithm is an
$\tilde O(|E(G)|+ n^{1+o(1)})$-time algorithm to compute a \emph{core
decomposition} of a given dense graph $G$, which allows us to compute short
paths between pairs of query vertices in $G$ efficiently. We believe that this
core decomposition algorithm may be of independent interest. We use our result
for vertex-decremental SSSP to obtain $(1+\epsilon)$-approximation algorithms
for maximum $s$-$t$ flow and minimum $s$-$t$ cut in vertex-capacitated graphs,
in expected time $n^{2+o(1)}$, and an $O(\log^4n)$-approximation algorithm for
the vertex version of the sparsest cut problem with expected running time
$n^{2+o(1)}$. These results improve upon the previous best known results for
these problems in the regime where $m= \omega(n^{1.5 + o(1)})$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11512"><span class="datestr">at May 29, 2019 11:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1905.11458">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1905.11458">Noise sensitivity of Boson Sampling and density of bosons</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Valery Shchesnovich <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1905.11458">PDF</a><br /><b>Abstract: </b>Inevitable experimental noise lies on the way to demonstrate the
computational advantage of quantum devices over digital computers in some
specific tasks. One of the proposals is Boson Sampling of Aaronson &amp; Arkhipov,
where the specific classically hard task is sampling from the many-body quantum
interference of $N$ indistinguishable single bosons on a $M$-dimensional
unitary network. Can a noisy realisation of Boson Sampling be efficiently and
faithfully simulated classically? We consider how the output distribution of
noisy Boson Sampling can be distinguished from that of classical simulation
accounting for the many-body interference only up to a fixed order. It is shown
that one can distinguish the output distribution of noisy Boson Sampling from
that of classical simulation with a number of samples that depends solely on
the highest order of quantum interference accounted for by the classical
simulation, noise amplitude, and density of bosons $\rho = N/M$. The results
indicate that noisy Boson Sampling in a regime of finite density of bosons,
$\rho= \Theta(1)$, i.e., on a small network $M = N/\rho$, retains quantum
advantage over digital computers if the amplitude of noise remains bounded as
$N$ scales up.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1905.11458"><span class="datestr">at May 29, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-831739446833439686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html">Individual Notions of Fairness You Can Use</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="text-align: center;"><span style="font-size: x-large;"><u>Individual Notions of Fairness You Can Use</u></span></div><br />Our group at Penn has been thinking about when <i>individual </i>notions of fairness might be practically achievable for awhile, and we have <a href="https://arxiv.org/abs/1905.10660">two</a> <a href="https://arxiv.org/abs/1905.10607">new</a> approaches.<br /><br /><span style="font-size: large;"><u>Background</u>:</span><br /><u>Statistical Fairness</u><br />I've written about this before, <a href="http://aaronsadventures.blogspot.com/2017/11/between-statistical-and-individual.html">here</a>. But briefly: there are two families of definitions in the fairness in machine learning literature. The first group of definitions, which I call <i>statistical</i> fairness notions, is far and away the most popular. If you want to come up with your own statistical fairness notion, you can follow this recipe:<br /><ol><li>Partition the world into a small number of "protected sub-groups". You will probably be thinking along the lines of race or gender or something similar when you do this.</li><li>Pick your favorite error/accuracy metric for a classifier. This might literally be classification error, or false positive or false negative rate, or positive predictive value, or something else. Lots of options here. </li><li>Ask that this metric be approximately equalized across your protected groups.</li><li>Finally, enjoy your new statistical fairness measure! Congratulations!</li></ol><div>These definitions are far and away the most popular in this literature, in large part (I think) because they are so immediately actionable. Because they are defined as conditions on a small number of expectations, you can easily check whether your classifier is "fair" according to these metrics, and (although there are some interesting computational challenges) go and try and learn classifiers subject to these constraints. </div><div><br /></div><div>Their major problem is related to the reason for their success: they are defined as conditions on a small number of expectations or <i>averages</i> over people, and so they don't promise much to particular individuals. I'll borrow an example from our <a href="https://arxiv.org/abs/1711.05144">fairness gerrymandering</a> paper from a few years ago to put this in sharp relief. Imagine that we are building a system to decide who to incarcerate, and we want to be "fair" with respect to both gender (men and women) and race (green and blue people). We decide that in our scenario, it is the false positives who are harmed (innocent people sent to jail), and so to be fair, we decide should equalize the false positive rate: across men and women, and across greens and blues. But one way to do this is to jail all green men and blue women. This does indeed equalize the false positive rate (at 50%) across all four of the groups we specified, but is cold comfort if you happen to be a green man --- since then you will be jailed with certainty. The problem was our fairness constraint was never a promise to an individual to begin with, just a promise about the average behavior of our classifier over a large group. And although this is a toy example constructed to make a point, things like this happen in real data too. </div><br /><u>Individual Fairness</u><br />Individual notions of fairness, on the other hand, really do correspond to promises made to individuals. There are at least two kinds of individual fairness definitions that have been proposed: <a href="https://dl.acm.org/citation.cfm?id=2090255">metric fairness</a>, and <a href="http://papers.nips.cc/paper/6355-fairness-in-learning-classic-and-contextual-bandits">weakly meritocratic fairness</a>. Metric fairness proposes that the learner will be handed a <i>task specific similarity metric</i>, and requires that individuals who are close together in the metric should have a similar probability of being classified as positive. Weakly meritocratic fairness, on the other hand, takes the (unknown) labels of an individual as a measure of merit, and requires that individuals who have a higher probability of really having a positive label should have only a higher probability of being classified as positive. This in particular implies that false positive and false negative rates should be equalized <i>across individuals</i>, where now the word <i>rate</i> is averaging over only the randomness of the classifier, not over people. What makes both of these <i>individual</i> notions of fairness is that they impose constraints that bind on all pairs of individuals and not just over averages of people.<br /><br />Definitions like this have the advantage of strong individual-level semantics, which the statistical definitions don't have. But they also have big problems: for metric fairness, the obvious question is: <i>where does the metric come from</i>? Even granting that fairness should be some Lipschitz condition on a metric, it seems hard to pin down what the metric is, and different people will disagree: coming up with the metric seems to encapsulate a large part of the original problem of defining fairness. For weakly meritocratic fairness, the obvious problem is that we don't know what the labels are. Its possible to do non-trivial things if you make assumptions about the label generating process, but its not at all clear you can do any non-trivial learning subject to this constraint if you don't make strong assumptions.<br /><br /><span style="font-size: large;"><u>Two New Approaches:</u></span><br />We have two new approaches, building off of metric fairness and weakly meritocratic fairness respectively. Both have the advantages of statistical notions of fairness in that they can be put into practice without making unrealistic assumptions about the data, and without needing to wait on someone to hand us a metric. But they continue to make meaningful promises to individuals.<br /><br /><u>Subjective Individual Fairness</u><br />Lets start with our variant of metric fairness, which we call subjective individual fairness. (This is joint work with Michael Kearns, our PhD students Chris Jung and Seth Neel, our former PhD student Steven Wu, and Steven's student (our grand student!) Logan Stapleton). The paper is here: <a href="https://arxiv.org/abs/1905.10660">https://arxiv.org/abs/1905.10660</a>. We stick with the premise that "similar people should be treated similarly", and that whether or not it is correct/just/etc., it is at least fair to treat two people the same way, in the sense that we classify them as positive with the same probability. But we don't want to assume anything else.<br /><br />Suppose I were to create a machine learning fairness panel: I could recruit "AI Ethics" experts, moral philosophers, hyped up consultants, people off the street, toddlers, etc. I would expect that there would be as many different conceptions of fairness as there were people on the panel, and that none of them could precisely quantify what they meant by fairness --- certainly not in the form of a "fairness metric". But I could still ask these people, in particular cases, if they thought it was fair that two particular individuals be treated differently or not.<br /><br />Of course, I would have no reason to expect that the responses that I got from the different panelists would be consistent with one another --- or possibly even internally consistent (we won't assume, e.g. that the responses satisfy any kind of triangle inequality). Nevertheless, once we fix a data distribution and a group of people who have opinions about fairness, we have a well defined tradeoff we can hope to manage: any classifier we could choose will have both:<br /><ol><li>Some error rate, and</li><li>Some frequency with which it makes a pair of decisions that someone in the group finds unfair. </li></ol><div>We can hope to find classifiers that optimally trade off 1 and 2: note this is a coherent tradeoff even though we haven't forced the people to try and express their conceptions of fairness into some consistent metric. What we show is that you can do this. </div><div><br /></div><div>Specifically, given a set of pairs that we have determined should be treated similarly, there is an <i>oracle efficient </i>algorithm that can find the optimal classifier subject to the constraint that no pair of individuals that has been specified as a constraint should have a substantially different probability of positive classification. Oracle efficiency means that what we can do is reduce the "fair learning" problem to a regular old learning problem, without fairness constraints. If we can solve the regular learning problem, we can also solve the fair learning problem. This kind of fairness constraint also generalizes in the standard way: if you ask your fairness panel about a reasonably small number of pairs, and then solve the in-sample problem subject to these constraints, the classifier you learn will also satisfy the fairness constraints out of sample. And it works: we implement the algorithm and try it out on the COMPAS data set, with fairness constraints that we elicited from 43 human (undergrad) subjects. The interesting thing is that once you have an algorithm like this, it isn't only a tool to create "fair" machine learning models: its also a new instrument to investigate human conceptions of fairness. We already see quite a bit of variation among our 43 subjects in our preliminary experiments. We plan to pursue this direction more going forward.</div><div><br /></div><div><u>Average Individual Fairness</u></div><div>Next, our variant of weakly meritocratic fairness. This is joint work with Michael Kearns and our student Saeed Sharifi. The paper is here: <a href="https://arxiv.org/abs/1905.10607">https://arxiv.org/abs/1905.10607</a>. In certain scenarios, it really does seem tempting to think about fairness in terms of false positive rates. Criminal justice is a great example, in the sense that it is clear that everyone agrees on which outcome they <i>want</i> (they would like to be released from jail), and so the people we are being unfair to really do seem to be the false positives: the people who should have been released from jail, but who were mistakenly incarcerated for longer. So in our "fairness gerrymandering" example above, maybe the problem with thinking about false positive rates wasn't a problem with <i>false positives</i>, but with <i>rates</i>: i.e. the problem was that the word rate averaged over many people, and so it didn't promise <i>you</i> anything. Our idea is to redefine the word rate. </div><div><br /></div><div>In some (but certainly not all) settings, people are subject to not just one, but many classification tasks. For example, consider online advertising: you might be shown thousands of targeted ads each month. Or applying for schools (a process that is centralized in cities like New York): you apply not just to one school, but to many. In situations like this, we can model the fact that we have not just a distribution over people, but also a distribution over (or collection of) problems. </div><div><br /></div><div>Once we have a distribution over problems, we can define the error rate, or false positive rate, or any other rate you like <i>for individuals. </i>It is now sensible to talk about Alice's false positive rate, or Bob's error rate, because rate has been redefined as an average over problems, for a particular individual. So we can now ask for individual fairness notions in the spirit of the statistical notions of fairness we discussed above! We no longer need to define protected groups: we can now ask that the false positive rates, or error rates, be equalized across all pairs of people. </div><div><br /></div><div>It turns out that given a reasonably sized sample of people, and a reasonably sized sample of problems, it is tractable to find the optimal classifier subject to constraints like this in sample, and that these guarantees generalize out of sample. The in-sample algorithm is again an oracle-efficient algorithm, or in other words, a reduction to standard, unconstrained learning. The generalization guarantee here is a little interesting, because now we are talking about simultaneous generalization in two different directions: to people we haven't seen before, and also to problems we haven't seen before. This requires thinking a little bit about what kind of object we are even trying to output: a mapping from new problems to classifiers. The details are in the paper (spoiler --- the mapping is defined by the optimal dual variables for the empirical risk minimization problem): here, I'll just point out that again, the algorithm is practical to implement, and we perform some simple experiments with it. </div><div><br /></div><div><br /></div><br /><br /><br /></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html"><span class="datestr">at May 28, 2019 10:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19015) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>WINDMILL Early Stage Researcher 9: Optimizing URLLC metadata/data flows using machine learning</p>
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19014) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">Shattering and quasipolynomiality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>An inadequately-explained phenomenon in computational complexity theory is that there are so few natural candidates for <a href="https://en.wikipedia.org/wiki/NP-intermediate">-intermediate problems</a>, problems in  but neither in  nor -complete. Of course, if  there are none, and the <a href="https://11011110.github.io/blog/2019/05/27/Schaefer's dichotomy theorem">dichotomy theorem</a> implies that there are no intermediate Boolean constraint satisfaction problems. But there are a lot of other types of problems in , and a theorem of Ladner<sup id="fnref:l"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:l" class="footnote">1</a></sup> shows that there should be an infinite hierarchy of degrees of hardness within . So where are all the members of this hierarchy, and why are they so shy?</p>

<p>The same thing happens not just for  but for other related complexity classes like <a href="https://en.wikipedia.org/wiki/%E2%99%AFP"></a>. There should be many -intermediate classes but we know even fewer than for . <a href="https://mathstodon.xyz/@11011110/102118180704402052">I recently posted</a> about a discussion I had with Igor Pak on this issue, in which we suggested to each other two number-theoretic candidates for being -intermediate, the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> and the <a href="https://en.wikipedia.org/wiki/Prime-counting_function">prime-counting function</a> (see also <a href="https://cstheory.stackexchange.com/q/43954/95">Igor’s StackExchange question on this</a>). But although they’re in , neither of these functions is very combinatorial.</p>

<p>So anyway, the point of all this is to discuss more candidates for being -intermediate that are, I think, natural and combinatorial. They’re part of a family of problems that include a couple of related candidates for being -intermediate, and even a candidate for being -intermediate. These problems come from computational learning theory, or alternatively they can be seen as coming from mathematical logic, hereditary graph theory, and the theory of the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. And they’re all at what is in some sense the shallow end of the intermediate problems: they’re solvable in quasi-polynomial time, meaning , but not known to be solvable in polynomial time. So this is pretty strong evidence that they’re not complete for their respective complexity classes, but weaker evidence than usual that they’re not polynomial.</p>

<p>In learning theory, a family of sets  is said to <em>shatter</em> another set  (not necessarily belonging to ) if every subset of , including the empty set and  itself, can be obtained by intersecting  with some member of . The <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">Vapnik–Chervonenkis dimension</a> of  is just the size of the largest set that is shattered by  . If we let  (the number of sets in the family) and  (the number of distinct elements in those sets), then the dimension is clearly at most , because sets of size larger than  have too many subsets for them all to be formed by intersection with a member of . Therefore, the following problem can be solved in quasipolynomial time, by a brute-force search of the  small-enough subsets of :<sup id="fnref:lmr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:lmr" class="footnote">2</a></sup></p>

<dl>
  <dt>VC-dimension (largest shattered set)</dt>
  <dd>Input: family of sets , number 

    <p>Output: true if  shatters a set of size , false otherwise.</p>
  </dd>
</dl>

<p>The same quasipolynomial time bound applies to the following related problems,
the first of which is also in  and the second of which is in :</p>

<dl>
  <dt>Smallest non-shattered set</dt>
  <dd>Input: family of sets , number 

    <p>Output: True if there exists a subset  of 
of size  that is not shattered by , false otherwise.</p>
  </dd>
  <dt>Number of shattered sets</dt>
  <dd>Input: family of sets 

    <p>Output: the number of sets shattered by .</p>
  </dd>
</dl>

<p>For the first two problems, being non--complete hinges on the assumption that , but for the number of shattered sets, being non--complete (under <a href="https://en.wikipedia.org/wiki/Polynomial-time_counting_reduction">counting reductions</a>) is unconditional: the output doesn’t provide enough bits of information to encode the answers to all other  problems.
The VC-dimension is hard to approximate under a form of the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, strongly suggesting that it cannot be computed exactly in polynomial time.<sup id="fnref:mr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:mr" class="footnote">3</a></sup></p>

<p>To see that the two existence problems can sometimes both have answers that are logarithmic, it’s helpful to turn to the theory of random graphs, and of <em>the</em> random graph, the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. This graph obeys a collection of <em>extension axioms</em> according to which, for every two disjoint finite subsets of vertices, there exists another vertex adjacent to everything in the first subset and to nothing in the second subset. Using these axioms, we can build up induced copies of any finite or countable subgraph, one vertex at a time, using a greedy algorithm. Based on this property, let’s define a subset  of the vertices in an undirected graph to be <em>extensible</em> if, for every partition of  into two disjoint subsets, there exists another vertex outside  that is adjacent to everything in the first subset and to nothing in the second subset. This is nothing more than being shattered by the neighborhoods of the vertices outside . So we have the following corresponding problems.</p>

<dl>
  <dt>Largest extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has an extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest non-extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has a non-extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest missing induced subgraph</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if there is a graph  on at most  vertices that
is not an induced subgraph of , false otherwise.</p>
  </dd>
  <dt>Number of extensible sets</dt>
  <dd>Input: Undirected graph 

    <p>Output: The number of extensible sets of vertices of .</p>
  </dd>
</dl>

<p>The smallest missing induced subgraph size naturally falls into the complexity class  of problems for which you can guess a solution (the missing subgraph) but then verifying it involves solving a co- problem (is this subgraph missing).
It is greater than the size of the smallest non-extensible set, because if you try to build up a given induced subgraph by adding one vertex at a time greedily you can only get stuck at a non-extensible set. There must be a missing induced subgraph of size at most , because there are  isomorphism classes of -vertex labeled graphs and fewer than  ways of choosing which of the  labeled vertices correspond to vertices of , so for larger values of  than this bound there are more labeled graphs than placements of them as induced subgraphs. Another way of thinking about the smallest missing induced subgraph problem is that we are asking for the largest  for which  is <a href="https://en.wikipedia.org/wiki/Universal_graph">-universal</a>: it contains all graphs on at most  vertices as induced subgraphs.</p>

<p>The smallest non-extensible set and the smallest missing subgraph are both easy on any hereditary class of graphs, because these classes always have a missing subgraph of size . On the other hand, if  is chosen uniformly at random among the -vertex graphs, then any small subset of its vertices is extensible with high probability, so the smallest non-extensible set has expected size .</p>

<p>If these problems are not - and -complete, what are they? Papadimitriou and Yannakakis<sup id="fnref:py"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:py" class="footnote">4</a></sup> define a complexity class , and show that VC-dimension is -complete. Presumably, because it’s so similar, the same is true for the largest extensible set. Maybe it’s possible to prove completeness for the smallest missing induced subgraph in an analogue of  at the level of , and to prove completeness for the number of shattered sets and number of extensible sets in an analogue of  at this level.</p>

<div class="footnotes">
  <ol>
    <li id="fn:l">
      <p>Ladner, Richard (1975), “<a href="https://doi.org/10.1145/321864.321877">On the structure of polynomial time reducibility</a>”, <em>J. ACM</em> 22 (1): 155–171. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:l" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:lmr">
      <p>Linial, Nathan, Mansour, Yishay, and Rivest, Ronald L. (1991), “<a href="https://doi.org/10.1016/0890-5401(91)90058-A">Results on learnability and the Vapnik–Chervonenkis dimension</a>”, <em>Inf. Comput.</em> 90 (1): 33–49. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:lmr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:mr">
      <p>Manurangsi, Pasin, and Rubinstein, Aviad (2017), “<a href="http://proceedings.mlr.press/v65/manurangsi17a.html">Inapproximability of VC dimension and Littlestone’s dimension</a>”, <em>Proc. 2017 Conf. Learning Theory (COLT 2017)</em>, Proceedings of Machine Learning Research 65, pp. 1432–1460. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:mr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:py">
      <p>Papadimitriou, Christos H., and Yannakakis, Mihalis (1996), “<a href="https://doi.org/10.1006/jcss.1996.0058">On limited nondeterminism and the complexity of the V–C dimension</a>”, <em>J. Comput. Syst. Sci.</em> 53 (2): 161–170. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:py" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102170815471019923">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html"><span class="datestr">at May 27, 2019 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3247584017741087776">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html">separating fact from fiction with the 56% of Americans say Arabic Numerals should not be taught in school</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
On the excellent TV show Veep there was a subplot about a political candidate (who himself had failed algebra in HS) objecting to Algebra since it was invented by the Muslims. I don't recall the exact line, but he said something like `Math teachers are terrorists'<br />
This was, of course, fiction.<br />
<br />
The same week I read that 56% of survey respondents say `<u><i>Arabic Numerals' shouldn't be taught in</i></u> <i><u>schools'</u></i> Obviously also a fiction. Perhaps a headline from <i>The Onion</i>.<br />
<br />
No. The story is true.<br />
<br />
See snopes entry on this: <a href="https://www.snopes.com/fact-check/teaching-arabic-numerals/">here</a><br />
<br />
but also see many FALSE but FUNNY websites:<br />
<br />
Sarah Palin wants Arabic Numerals out of the schools: <a href="http://nationalreport.net/sarah-palin-wants-arabic-numerals-banned-americas-schools/">here</a> Funny but false.<br />
<br />
Jerry Brown is forcing students in California to learn Arabic Numerals as part of multi-culturism False by funny:  <a href="https://me.me/i/sharia-law-must-be-stopped-under-gov-brown-students-in-20990368">here</a><br />
<br />
A website urging us to use Roman Numerals (which Jesus used!) False but funny:  <a href="http://freedomnumerals.com/">here</a><br />
<br />
OKAY, what to make of the truth that really, really, 56% of Americans are against Arab Numerals<br />
<br />
1) Bigotry combined with ignorance.<br />
<br />
2) Some of the articles I read about this say its a problem with polls and people. There may be some of that, but still worries me.<br />
<br />
3) In Nazi Germany (WOW- Goodwin's law popped up rather early!) they stopped teaching relativity because Albert Einstein was Jewish (the story is more complicated than that, see <a href="https://www.scientificamerican.com/article/how-2-pro-nazi-nobelists-attacked-einstein-s-jewish-science-excerpt1/">her</a>e). That could of course never happen in America now (or could it, see <a href="https://www.tabletmag.com/jewish-news-and-politics/50097/time-warp">here</a> and <a href="https://www.conservapedia.com/index.php?title=Counterexamples_to_Relativity">here</a>).<br />
<br />
4) There is no danger that we will dump Arabic Numerals. I wonder if we will change there name to Freedom Numerals.<br />
<br />
5) Ignorance of science is a more immediate problem with the anti-vax people. See <a href="https://www.thedailybeast.com/measles-outbreak-grows-with-60-new-cases-across-26-states?ref=home">here</a><br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html"><span class="datestr">at May 27, 2019 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-955320605612447790">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/05/an-interview-with-jamie-gabbay-and.html">An interview with Jamie Gabbay and Andrew Pitts, 2019 Alonzo Church Award recipients</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The 2019 Alonzo Church Award committee consisting of Thomas Eiter, Javier Esparza, Radha Jagadeesan, Catuscia Palamidessi, and Natarajan Shankar, have selected <a href="http://www.gabbay.org.uk/">Murdoch J. Gabbay </a>and <a href="https://www.cl.cam.ac.uk/~amp12/">Andrew M. Pitts </a>for the <a href="http://eatcs.org/index.php/component/content/article/1-news/2812-the-2019-alonzo-church-award">2019 Alonzo Church Award</a>, for introducing the theory of nominal representations, a powerful and elegant mathematical model for computing with data involving atomic names. In particular, the nomination for the Alonzo Church Award singled out the following two papers:<br /><ul><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/newaas/newaas-jv.pdf">A new approach to abstract syntax with variable binding</a>” by Murdoch J. Gabbay and Andrew M. Pitts, Formal Aspects of Computing 13(3):341– 363, 2002; and</li><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/nomlfo/nomlfo-draft.pdf">Nominal logic, a first order theory of names and binding</a>” by Andrew M. Pitts, Information and Computation 186(2):165–193, 2003.</li></ul>For the conference version of the first article, Andy and Jamie will also be receiving the Test-of-Time Award from LICS 1999.<br /><br />The award recipients kindly agreed to answer some questions of mine via email. You can find the transcript of the interview below. My questions are labelled with <b>LA</b>, Andy's answers with <b>AP</b> and Jamie's with <b>JG</b>. I hope that you'll enjoy reading their insights and the story of their award-receiving work as much as I did myself. <br /><br /><div dir="ltr"><b>LA: </b>You are receiving the 2019 Alonzo Church Award  for  Outstanding Contributions to Logic and Computation as well as the  Test-of-Time Award from LICS 1999 for your invention of nominal  techniques to provide a semantic understanding of abstract syntax with  binding.   Could you briefly describe the history of the ideas that led  you to use the <a href="https://en.wikipedia.org/wiki/Permutation_model">permutation model of set theory with atoms</a> due to  Fraenkel and Mostowski to represent name abstraction and fresh name  generation? What were the  main inspirations and motivations for your work? In your opinion, how  did nominal techniques advance the state of the art at that time?</div><div dir="ltr"></div><br /><div dir="ltr"><b>AP: </b>I have had a long-standing interest in the mathematical semantics of programming language features that restrict resources to a specific scope, or hide information from a program's environment; think local mutable state in languages like <a href="http://ocaml.org/">OCaml</a>, or channel-name restriction in the <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus">pi-calculus</a>. When <a href="http://homepages.inf.ed.ac.uk/stark/">Ian Stark</a> was doing his PhD with me in the 90s we tried to understand a simple instance: the observable properties of higher-order functions combined with dynamically generated atomic names that can be tested for equality, but don't have any other attribute -- we called this the "nu-calculus". Ian gave a denotational semantics for the nu-calculus using Moggi's monad for modelling dynamic allocation. That monad is defined on the category of pullback-preserving functors from the category of injective functions between finite ordinals to the category of sets. This functor category was well-known to me from topos theory, where it is called <a href="https://ncatlab.org/nlab/show/Schanuel+topos">Schanuel's topos </a>and hosts the generic model of a geometric theory of an infinite decidable set.  A few years later, when Jamie joined me as a PhD student in 1998, I suggested we look at the Schanuel topos as a setting for initial algebra semantics of syntax involving binding operations, modulo alpha-equivalence. I think Jamie prefers set theory over category theory, so he pushed us to use another known equivalent presentation of the Schanuel topos, in terms of continuous actions of the group of permutations of the set N of natural numbers (topologized as a subspace of the product of countably many copies of N). In this form there is an obvious connection with the cumulative hierarchy of sets (with atoms) that are hereditarily finitely supported with respect to the action of permuting atoms. This universe of sets was devised by Fraenkel and Mostowski in the first part of the twentieth century to model ZFA set theory without axioms of choice.  Whether one emphasises set theory or category theory, the move to making permutations of names, rather than injections between sets of names, the primary concept was very fruitful. For example, it certainly makes higher-order constructions (functions and powersets) in the topos/set-theory easier to describe and use. We ended up with a generic construction for name-abstraction modulo <a href="https://en.wikipedia.org/wiki/Lambda_calculus#Alpha_equivalence">alpha-equivalence</a> compatible with classical higher-order logic or set theory, so long as one abstains from unrestricted use of choice. </div><div dir="ltr"></div><div dir="ltr"><br /><b>JG:</b> At the time it wasn't an idea to consider names as elements of a distinctive datatype of names, with properties just like other datatypes such as the natural numbers Nat. If we want to add 1 to 1, we take 1:Nat and invoke the "plus" function, which is a specific thing associated to Nat; so why not abstract a in x by assuming a:Atm (where Atm is a distinct thing in our mathematical universe) and x:X and invoking a function "abstract", which is a thing associated to Atm?  We unfolded the implications of this idea in set theory and rediscovered FM sets.  I was inspired by the way I saw mathematics built up in ZF set theory as an undergraduate, starting from a simple basis and building up the cumulative hierarchy.  When I saw the chance to do this for a universe with names, I jumped at the chance.  It turns out FM sets are not required.  Nominal techniques can be built in ZFA set theory, which contains more functions and permits unrestricted choice. </div><div dir="ltr"><br /><b>LA: </b>Over  the last fifteen years, nominal techniques have become a fundamental  tool for modelling locality in computation, underlying research  presented in over a hundred papers, new programming languages and models  of computation. They have applications to the syntax and semantics of  programming languages, to logics for machine-assisted reasoning about  programming-language semantics and to the automatic verification of  specifications in process calculi. Variations on nominal sets are used  in automata theory over infinite alphabets, with applications to  querying XML and databases, and also feature in work on models of  Homotopy Type Theory. When did it dawn on you that you had succeeded in  finding a very good model for name abstraction and fresh name  generation, and one that would have a  lot of impact? Did you imagine that your model would generate such a  large amount of follow-up work, leading to a whole body of work on  nominal computation theory? <br /><br /><b>AP: </b>No, to begin with I was very focussed on getting better techniques for computing and reasoning about syntax with bound names. But that only represents a part of the current broad landscape of nominal techniques, the part that mainly depends on the mathematical notion of "finite support" (a way of expressing, via name-permutation, that an object only involves finitely many names). Independently of us, some people realised that a related notion of finiteness, "orbit-finiteness" (which expresses that an object is finite modulo symmetries) is crucial for many applications of nominal techniques. I am referring to the work of Montanari and Pistore on pi-calculus and <a href="https://core.ac.uk/download/pdf/82414059.pdf">HD automata </a>using named sets (yet another equivalent of the Schanuel topos) and the work on automata theory over infinite alphabets (and much else besides) using "sets with atoms" by the Warsaw group (Bojanczyk, Klin, Lasota, Torunczyk,...). The latter is particularly significant because it considers groups of symmetry for atoms other than the full permutation group (in which the only property of an atom preserved under symmetry is its identity). <br /><br /><b>JG:</b> Yes, I did.  Nobody could anticipate the specific applications but I knew we were on to something, which is why I stayed on to build the field after the PhD.  The amount of structure was just too striking.  This showed early: e.g. in the equivariance properties, and the commutation of nominal atoms-abstraction with function-spaces.  When I sent the proof of this property to Andrew, at first he didn't believe it!  I had a sense that there was something deep going on and I still do. <br /><br /><b>LA: </b>What is the result of  yours on nominal techniques you are most proud of? And what are your  favourite results amongst those achieved by others on nominal computation?<b></b><br /><b><br /></b><b>AP:</b> Not so much a specific result, but rather a logical concept, the freshness quantifier (which we wrote using an upside down "N" -- N stands for "New"). In informal practice when reasoning about syntax involving binders, one often chooses <i>some</i> fresh name for the bound variable, but then has to revise that choice in view of later ones; but fortunately <i>any </i>fresh name does as well as some particular one. This distinctive "some/any" property occurs all over the place when computing and reasoning about languages with binders and the freshness quantifier formalises it, in terms of the freshness ("not in the support of") relation and conventional quantifiers.  For the second part of your question I would choose two things. One is the work by Jamie with Fernandez and Mackie on <a href="https://www.sciencedirect.com/science/article/pii/S0890540106001635">nominalrewriting systems</a>, which won the PPDP Most Influential Paper 10-year Award in 2014. The second is the characterisation of orbit-finite sets with atoms in terms of "set-builder expressions"---see Klin et al, "<a href="https://www.mimuw.edu.pl/~szymtor/papers/locfin.pdf">Locally Finite Constraint Satisfaction Problems</a>", Proc. LICS 2015); it's a nice application of the classical model theory of homogeneous structures with interesting applications for languages that compute with finite structures. <br /><br /><b>JG:</b> Thanks for asking.  Aside from the initial papers, my work on nominal rewriting with Fernandez has probably had most impact.  However, I am rather fond of the thread of research going from Nominal Algebra, through the axiomatisation of substitution and first-order logic and the characterisation of quantification as a limit in nominal sets, and on to Stone duality.  It's a mathematical foundation built from a nominal perspective of naming and quantification and I hope that as the state of the art in nominal techniques advances and broadens, it might prove useful.  Andrew's book has been helpful in marking out nominal techniques as a field.  I also agree with Andrew that orbit-finiteness and the applications of this idea to transition systems and automata, is important.  I like the automata work for another concrete reason: nominal techniques were discovered in the context of names and binding in syntax, which has bequeathed a misconception that nominal techniques are <i>only</i> about this.  The Warsaw school of nominal techniques gives an independent illustration of the other applications of these ideas. <br /><b><br /></b><b><b>LA: </b></b>Twenty years have passed since your LICS 1999 paper and the  literature on variations on nominal techniques now contains over a hundred papers. Do you expect any further  development related to the theory and application of nominal techniques in the  coming years? What advice would you give to a PhD student who is  interested in working on topics related to nominal computation today?</div><div dir="ltr"><br /><b>AP: </b> For the purpose of answering your question, let's agree to divide LICS topics into Programming Languages and Semantics (PLS) versus Logic and Algorithms (LAS). (So long as we don't think of it as a dichotomy!) Then it seems to me that applications of nominal techniques to LAS are currently in the ascendant and show no sign of slowing down. My own interests are with PLS and there is still work to be done there. In particular, I would like better support for using nominal techniques within the mainstream interactive theorem proving systems: we have the Nominal Package of Urban and Berghofer for classical higher-order logic within Isabelle (which lead to Urban and Tasson winning the CADE Skolem Award in 2015), but nothing analogous for systems based on dependent type theory, such as Agda, Coq and Lean. Recent work of Swan (arXiv:1702.01556) gives us a better understanding of how to develop nominal sets within constructive logic; but I have yet to see a dependent type theory that both corresponds to some form of constructive nominal logic under Curry-Howard and is sufficiently simple that it appeals to users of systems lke Coq who want to mechanise programming language meta-theory in a nameful style. Really, I would like the utility of the FreshML programming language that Jamie, Mark Shinwell and I proposed in 2003 (and which Mark implemented as a patch of OCaml) restricted to total functional programming in the style of Agda; but I don't quite know how to achieve that. <br /><br /><b>JG:</b> Yes.  We are far from understanding nominal techniques and the field has a lot of life and will continue to surprise.  I've always believed that.  A key sticking-point right now is implementations.  I wrote a paper about this recently, on equivariance and the foundations of nominal techniques.  One point in the paper is a sketch for a next-generation nominal theorem-prover (based on ZFA + equivariance).  I'd like to see this carried out, so if anybody reading this is interested then please be in touch.  I'd also like to see nominal techniques implemented as a package in a language like Haskell, ML, or even Python!  If we can get this stuff into the working programmer's toolbox, in a way that just works and does not require special configuration, then that would be helpful.  I suspect that nominal techniques as currently presented in the maths papers, might not fit into a programming language at the moment.  The theory is too strong and may need weakened first.  We need a subset of nominal techniques weak enough to squeeze into an existing language, yet expressive enough for interesting applications.  Some general advice, specifically for the PhD student.  If you have an idea which most people around you don't understand, consider this may be a gap in the collective imagination.  There can be peer pressure when faced by incomprehension to blame yourself, back down, and think about something else.  By all means do this, but only if you yourself judge it right to do so. <br /><br /></div><b>LA: </b>Is there any general research-related lesson you have learnt in the process of working on nominal techniques?<br /><br /><b>AP: </b>On the one hand, don't lose sight of what application your theory is supposed to be good for; but on the other hand, let beauty and simplicity be your guide.<br /><br /><b>JG:</b> Yes:<br /><ul><li>Proving stuff is 30% of the work; convincing people is 70%. </li><li>It's the basic ideas that are hard, not the complicated theorems.</li><li>Competence and imagination are orthogonal. </li><li>It's doesn't have to be complex to be clever. </li><li>Elegant + applicable is a potent combination. </li><li>Seek out good listeners.  Give up quickly on bad ones.  Try to be a good listener. </li><li>Other people have a lot to teach you, but it might not be the things you expected. </li><li>Writing papers is fun.   </li></ul><b>LA: </b>Thanks to both of you for your willingness to answer my questions and congratulations for the awards you will be receiving this summer!</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/05/an-interview-with-jamie-gabbay-and.html"><span class="datestr">at May 26, 2019 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/076">TR19-076 |  The Equivalences of Refutational QRAT | 

	Leroy Chew, 

	Judith Clymo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The solving of Quantified Boolean Formulas (QBF) has been advanced considerably in the last two decades. In response to this, several proof systems have been put forward to universally verify QBF solvers. 
QRAT by Heule et al. is one such example of this and builds on technology from DRAT, a checking format used in propositional logic. 
Recent advances have shown conditional optimality results for QBF systems that use extension variables.
Since QRAT can simulate Extended Q-Resolution, we know it is strong, but we do not know if QRAT has the strategy extraction property as Extended Q-Resolution does. In this paper, we partially answer this question by showing that QRAT with a restricted reduction rule has strategy extraction (and consequentially is equivalent to Extended Q-Resolution modulo NP).
We also extend equivalence to another system, as we show an augmented version of QRAT known as QRAT+, developed by Lonsing and Egly, is in fact equivalent to the basic QRAT. We achieve this by constructing a line-wise simulation of QRAT+ using only steps valid in QRAT.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/076"><span class="datestr">at May 26, 2019 10:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html">More matching-mimicking networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My <a href="https://11011110.github.io/blog/2018/02/01/parallel-matching-in.html">paper with Vijay Vazirani on parallel matching</a> (soon to appear in <a href="http://spaa.acm.org/2019/">SPAA</a>) is based on the idea of a “matching-mimicking network”. If  is a graph with a designated set  of terminal vertices, then a matching-mimicking network for  is another graph  with the same terminals that has the same pattern of matchings. Here, by a <em>pattern of matchings</em>, I mean a family of subsets of , the subsets that can be covered by a matching that also covers all non-terminal vertices. We included a messy case analysis that, after some simplifications due to symmetry, had 21 cases for the matching mimicking networks on at most three terminals.</p>

<p>By now, I think I understand patterns of matchings a lot better, enough to do the three-terminal case in only four cases and to extend the analysis to four terminals in only seven more cases. The starting point is the observation that these patterns of matchings are <a href="https://en.wikipedia.org/wiki/Delta-matroid">even Δ-matroids</a>.</p>

<p>One way to think of a Δ-matroid is that it’s just a convex polyhedron or polytope in Euclidean space of some dimension , with the properties that all vertex coordinates are  or  and all edge lengths are  or . An even Δ-matroid has the stronger property that all edge lengths are , as is true for the three-dimensional regular tetrahedron with vertex coordinates (written in a more compact form as bitvectors) , , , and .</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/tet-in-cube.svg" alt="Regular tetrahedron formed from alternating vertices of a cube" /></p>

<p>Alternatively one can consider the same kind of structure to be a family of sets, drawn from a universe of  elements that correspond to the dimensions of the space. Each set in the family corresponds to a vertex of the polytope and includes the elements whose coordinates are one. So over the three-element set  the same regular tetrahedron can be written as the family of sets</p>



<p>When expressed in this way, the sets of a Δ-matroid obey an exchange axiom: if two sets  and  differ on whether they include some element , then there must exist an element  on which they also differ, so that the symmetric difference of sets  also belongs to the Δ-matroid. By repeatedly applying this axiom one can connect  to  by a geodesic path (in Hamming distance) of two-element moves. For the bases of a matroid, we have a stronger requirement that one of the two elements belongs to  and the other belongs to , or equivalently that all sets have the same size, but a Δ-matroid relaxes this requirement. It’s not even required that ! But in an even Δ-matroid  and  must be distinct, because otherwise the step would be along an edge of length one. Another way of expressing the extra requirements of an even Δ-matroid over an arbitrary Δ-matroid is that all sets must have the same parity (all have even size, or all have odd size).</p>

<p>So anyway, back to matching. Suppose that both  and  are sets drawn from a pattern of matchings. Choose arbitrarily a matching representing each set. Then the symmetric difference of these matchings is a collection of disjoint alternating paths and cycles, and we can get from  to  by 
a sequence of steps in which we take the symmetric difference of the current matching by one of the alternating paths. So this gives us not just one geodesic from  to  but a lot of different geodesics, one for each ordering of the alternating paths. Expressed as an exchange axiom, this means that when two sets  and  differ, the elements on which they differ can be partitioned into pairs, the symmetric differences with which can be performed independently. You can pick any subset of the pairs of differing elements, and change each of those pairs, leaving the rest alone. Because this is a strengthening of the even Δ-matroid axiom, every matching pattern is an even Δ-matroid.</p>

<p>Expressed in polyhedral terms, this stronger exchange axiom means that every two vertices at distance  from each other are connected by a -dimensional hypercube with side length . This is a little weird, because we started with a hypercube but then eliminated half of its vertices (by the parity condition) to get something else. Now we have hypercubes again, of lower dimension. They must be tilted with respect to the coordinate axes: each axis of one of these lower-dimensional hypercubes is tilted at a 45 degree angle with respect to the coordinate system of the overall polytope.</p>

<p>Not every even Δ-matroid obeys this sub-hypercube property. On the other hand, I was expecting the matching patterns that are matroids (all sets are the same size) to be transversal matroids (maximal subsets of vertices on one side of a bipartite graph that can be covered by a matching), and they aren’t. There is a six-element non-transversal matroid, whose six elements are the edges of a triangle with doubled edges and whose sets are pairs of edges from different sides of the triangle. But it is the pattern of matchings of a tree in which the (non-terminal) root has three children, each of which has two terminals as its children.</p>

<p>Conveniently, whether a 0-1 polyhedron can be represented by a pattern of matchings depends only on its shape and not on its orientation. You can obviously permute the coordinates of a polyhedron that represents a pattern of matchings, by relabeling which coordinate corresponds to which terminal vertex. But you can also reflect the polyhedron across any one of its coordinates by modifying the graph whose matchings represent it in the following way: turn the  terminal vertex for that coordinate into a non-terminal, and attach a new degree-one terminal vertex to it. These permutations and reflections generate all the symmetries of the hypercube in which the 0-1 polyhedron lives. So to find small matching-mimicking networks, we only need to look at one representative 0-1 polyhedron in each symmetry class. If we find a small network for this representative, we can modify it to create a different small matching-mimicking network for every other 0-1 polyhedron with the same shape.</p>

<p>So what are the possible shapes? Let’s define the dimension of a Δ-matroid to be the number of coordinates of the polytope that take both values,  and , at different vertices. Then a 0-dimensional even Δ-matroid must be a single point (), there are no 1-dimensional even Δ-matroids, and a two-dimensional even Δ-matroid must be a line segment (). There are two three-dimensional even Δ-matroids: a triangle  and the tetrahedron shown above, . The cube exchange axiom for patterns of matching starts to kick in for four-dimensional even Δ-matroids, whose vertices must be subsets of the four-dimensional hyperoctahedron . (This is the shape formed from a four-dimensional hypercube by keeping only vertices with the same parity as each other, just as we formed a regular tetrahedron by doing the same thing to a three-dimensional cube.) Here’s a drawing of  from <a href="https://11011110.github.io/blog/2010/09/26/in-response-to.html">an earlier post</a>:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2010/k7/cocktail2.svg" alt="The hyperoctahedral graph K_{2,2,2,2}" /></p>

<p>If there are no two opposite vertices, we get  (a regular tetrahedron, again, but embedded in a four-dimensional way into the hypercube). Otherwise, we must take at least two pairs of opposite vertices to form a square, and the cases are  (only the square),  (a square pyramid),  (an octahedron), ,  (an octahedral pyramid), and  (the hyperoctahedron). All of these polyhedra can be represented as matching-mimicking networks with the additional property that all vertices are terminals:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/4-terminal-mm.svg" alt="Matching-mimicking networks for up to four terminals" /></p>

<p>Based on these small examples, it’s tempting to guess that when a pattern of matchings includes the empty set, the whole pattern is just the set of matchings on a graph whose edges are the pairs of terminals in the pattern. But it isn’t true. The square pyramid , for instance, can represent the pattern of matchings</p>



<p>with the empty set at the apex of the pyramid. In this pattern, even though one can match terminal pairs <span style="white-space: nowrap;">— or —,</span> one can’t take the union of those two matchings and cover all four terminals. (This is what you get by reflecting the two middle terminals of the network shown above for ; its matching-mimicking network is a tree with two interior non-terminals and four terminal leaves.) My guess is that the number of patterns of matching should grow quickly relative to the number of graphs, so for large enough numbers of terminals it should not be possible to use graphs without non-terminals or their complements. But I haven’t taken the case analysis far enough to find an example of this.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102160605632804102">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html"><span class="datestr">at May 25, 2019 09:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/075">TR19-075 |  Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems | 

	Lijie Chen, 

	Dylan McKay, 

	Cody Murray, 

	Ryan Williams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems

A frontier open problem in circuit complexity is to prove P^NP is not in SIZE[n^k] for all k; this is a necessary intermediate step towards NP is not in P/poly. Previously, for several classes containing P^NP, including NP^NP, ZPP^NP, and S_2 P, such lower bounds have been proved via Karp-Lipton-style Theorems: to prove C is not in SIZE[n^k] for all k, we show that C subset Ppoly implies a ``collapse'' D = C for some larger class D, where we already know D is not in SIZE[n^k] for all k. 

It seems obvious that one could take a different approach to prove circuit lower bounds for P^NP that does not require proving any Karp-Lipton-style theorems along the way. We show this intuition is wrong: (weak) Karp-Lipton-style theorems for P^NP are equivalent to fixed-polynomial size circuit lower bounds for P^NP. That is, P^NP not subset SIZE[n^k] for all k if and only if (NP is in P/poly implies PH is in i.o.-P^NP/n).
		
Next, we present new consequences of the assumption NP is in P/poly, towards proving similar results for NP circuit lower bounds. We show that under the assumption, fixed-polynomial circuit lower bounds for NP, nondeterministic polynomial-time derandomizations, and various fixed-polynomial time simulations of NP are all equivalent. Applying this equivalence, we show that circuit lower bounds for NP imply better Karp-Lipton collapses. That is, if NP is not in SIZE[n^k] for all k, then for all C in { ParP, PP, PSPACE, EXP }, C is in P/poly implies C is in i.o.-NP/n^eps for all eps &gt; 0. Note that unconditionally, the collapses are only to MA and not NP.
		
We also explore consequences of circuit lower bounds for a sparse language in NP. Among other results, we show if a polynomially-sparse NP language does not have n^(1+eps)-size circuits, then MA is in i.o.-NP/O(log n), MA is in i.o.-P^{NP[O(log n)]}, and NEXP is not in SIZE[2^o(m)]. Finally, we observe connections between these results and the ``hardness magnification'' phenomena described in recent works.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/075"><span class="datestr">at May 25, 2019 06:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15910">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/">Selected Papers at CCC 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Some papers from the accepted list of this year’s Computational Complexity Conference</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/unknown-122/" rel="attachment wp-att-15912"><img width="150" alt="" class="alignright  wp-image-15912" src="https://rjlipton.files.wordpress.com/2019/05/unknown-1.jpeg?w=150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ UB CSE ]</font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman is a long-time friend of Ken and I, and is a long-time researcher in complexity theory. Alan was the first president of the organizing <a href="https://www.computationalcomplexity.org/governance.php">body</a> for the Computational Complexity Conferences (CCC). </p>
<p>
Today we salute the <img src="https://s0.wp.com/latex.php?latex=%7B0b100010%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0b100010}" class="latex" title="{0b100010}" />th edition of the conference and discuss some of the accepted papers.</p>
<p>
The conference, and the governing body, have changed names over the years; by any name it remains an important conference. Alan <a href="https://www.computationalcomplexity.org/documents/first-cfp.pdf">chaired</a> the first program committee with Steve Mahaney and <a href="https://dl.acm.org/citation.cfm?id=648296&amp;picked=prox">edited</a> the first proceedings, in 1986. </p>
<p>
Ken recently saw Alan two weeks ago at the banquet for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> honoring Steve Cook at the University of Toronto. We will cover event, once <a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/">exams</a> are done. Ken saw another of the CCC past presidents there—if you wish to guess who, a hint is it was one of Cook’s past students.</p>
<ul>
<li>
Dieter van Melkebeek, 2012-2018 <p></p>
</li><li>
Peter Bro Miltersen, 2009-2012 <p></p>
</li><li>
Pierre McKenzie, 2006-2009 <p></p>
</li><li>
Lance Fortnow, 2000-2006 <p></p>
</li><li>
Eric Allender, 1997-2000 <p></p>
</li><li>
Steven Homer, 1994-1997 <p></p>
</li><li>
Timothy Long, 1992-1994 <p></p>
</li><li>
Stephen Mahaney, 1988-1992 <p></p>
</li><li>
Alan Selman, 1985-1988
</li></ul>
<p>
Although we do not usually do announcements, we note from the conference <a href="https://computationalcomplexity.org">website</a>:</p>
<blockquote><p><b> </b> <em> Details of the local arrangements for CCC 2019 and the preceding events, including the DIMACS Day of Tutorials, are available. Early registration runs till June 26. </em>
</p></blockquote>
<p>
</p><p></p><h2> Six Papers with Some Comments </h2><p></p>
<p></p><p>
Here are some papers that I, Dick, found interesting from the list of accepted papers. All accepted papers are interesting, of course. I selected six that were on topics that were directly connected with my interests.</p>
<p>
<b>Criticality of Regular Formulas</b>—<a href="http://www.math.toronto.edu/rossman/criticality.pdf">paper</a><br />
Benjamin Rossman<br />
<i>I thought this was about regular expressions. Shows something about me.</i> Here “regular” means the in-degree of gates being the same at each level of the circuit. This condition seems likely to be removable as Rossman conjectures, but I doubt it will be easy. The term “criticality” is a parameter that measures how much a random restriction reduces the size of a formula. Think switching lemma.</p>
<p>
<b>Typically-Correct Derandomization for Small Time and Space</b>—<a href="https://arxiv.org/abs/1711.00565">paper</a><br />
William Hoza<br />
<i>I like the notion of typically-correct.</i> Their algorithms work by treating the input as a source of randomness. This idea was pioneered by Oded Goldreich and Avi Wigderson. The title of their 2002 <a href="http://www.wisdom.weizmann.ac.il/~oded/p_rnd02.html">article</a> “Derandomization that is rarely wrong from short advice that is typically good”, gives away how one can prove such results. </p>
<p>
<b>Optimal Short-Circuit Resilient Formulas</b>—<a href="https://arxiv.org/abs/1807.05014">paper</a><br />
Mark Braverman, Klim Efremenko, Ran Gelles, and Michael Yitayew <br />
<i>This is on a kind of fault-tolerance.</i> They consider fault-tolerant boolean formulas in which the output of a faulty gate is stuck at one of the gate’s inputs. This is an interesting model of errors, and they show roughly: any formula can be converted into a formula that is not too much bigger and survives even if about <img src="https://s0.wp.com/latex.php?latex=%7B1%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/5}" class="latex" title="{1/5}" /> of the gates are faulty. A surprise is that they use a method related to <i>blockchains</i>. Hmmmmm. Interesting.</p>
<p>
<b>Fourier and Circulant Matrices are Not Rigid</b>—<a href="https://arxiv.org/pdf/1902.07334.pdf">paper</a><br />
Allen Liu and Zeev Dvir <br />
<i>A matrix is rigid if its rank cannot be reduced significantly by changing a small number of entries.</i> As you probably know there are plenty of rigid matrices—take random ones—but no provable examples of explicit ones. Their beautiful results prove that specific families of matrices are not rigid. These families include ones that were long thought to be rigid. The highlight of this work could be that it suggests new families that may be rigid. </p>
<p>
<b>Average-Case Quantum Advantage with Shallow Circuits</b>—<a href="https://arxiv.org/pdf/1810.12792.pdf">paper</a><br />
François Le Gall <br />
<i>A quest, the quest that tops all others—is the search for evidence that quantum computers are better than classic ones.</i> Of course, this is nearly impossible, since P=PSPACE is an open problem. So one looks at special classes of computations. See <a href="https://arxiv.org/pdf/1612.05903.pdf">here</a> for how the quest for “quantum advantage” meets up with computational complexity.</p>
<p>
<b>Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems</b><br />
<a href="https://eccc.weizmann.ac.il/report/2019/075/">paper</a><br />
Lijie Chen, Dylan McKay, Cody Murray, and Ryan Williams <br />
<i>Of course I think this is an interesting paper.</i> There is the famous H-score. Perhaps there could be a T-score. This would be the number of times your name is in the title of a published paper. Thus Ron Rivest, for example, has a huge T-score.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your selected papers? </p>
<p>
[Added link to Relations and Equivalences… paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/"><span class="datestr">at May 25, 2019 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4982821151012814632">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html">Logic Then and Now</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This week I attended the <a href="https://asl2019.commons.gc.cuny.edu/">Association of Symbolic Logic North American Annual Meeting</a> in New York City, giving a talk on P v NP.<br />
<br />
First I must share the announcement that ASL member Tuna Antinel of Lyon 1 University has been arrested in Turkey for his political beliefs. <a href="http://math.univ-lyon1.fr/SoutienTunaAltinel/">This website</a> (<a href="https://webusers.imj-prg.fr/~adrien.deloro/">English version</a>) has details and how to show your support.<br />
<br />
I last attended the ASL annual meeting at Notre Dame in 1993 as a young assistant professor. Back then I talked about <a href="https://doi.org/10.1137/S0097539793248305">then recent work</a> using a special kind of generic oracle to make the Berman-Hartmanis isomorphism conjecture true. I remember someone coming up to me after the talk saying how excited they were to see such applications of logic. I'm not a theoretical computer scientist, I'm a applied logician.<br />
<br />
I asked at my talk this year and maybe 2-3 people were at that 1993 meeting. The attendance seemed smaller and younger, though that could be my memory playing tricks. I heard that the 2018 meeting in Macomb, Illinois drew a larger crowd. New York is expensive and logicians don't get large travel budgets.<br />
<br />
Logic like theoretical computer science has gotten more specialized so I was playing catch up trying to follow many of the talks. Mariya Soskova of Wisconsin talked about enumeration degrees that brought me back to the days I sat in logic classes and talks at the University of Chicago. A set A is enumeration reducible to B if from an enumeration of B you can compute an enumeration of A and Mariya gave a great overview of this area.<br />
<br />
I learned about the status of an open problem for Turing reducibility: Is there a non-trivial automorphism of the Turing Degrees? A degree is the equivalence class where each class are the languages all computably Turing-reducible to each other. So the question asks if there is a bijection f mapping degrees to degrees, other than identity, that preserves reducibility or lack thereof.<br />
<br />
Here's what's known: There are countably many such automorphisms. There is a definable degree C in the arithmetic hierarchy, such that if f(C) = C then f is the identity. Also if f is the identity on all the c.e.-degrees (those equivalence classes containing a computably enumerable set), then f is the identity on all the degrees. Still open if there is more than one automorphism.<br />
<br />
<br /></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html"><span class="datestr">at May 24, 2019 01:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img src="http://grigory.github.io/blog/pics/theory-jobs-2019.png" />
Apparently, it’s a busy life being an assistant prof so there were no posts here all year. However, while some of us are decompressing after the NeurIPS deadline, <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
Congratulations to both job seekers and departments/labs who are done with their searches!</p>

<p>In the past my academic uncle Lance Fortnow set this spreadsheet up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from two years ago which also has links to all the previous years). This year the first entry is Lance himself who is moving back to Chicago to be the Dean of the College of Science at the Illinois Institute of Technology. Did Lance get the idea from his advisor <a href="https://en.wikipedia.org/wiki/Michael_Sipser">Michael Sipser</a> who is also a Dean of Science but at MIT? In any case, great to see theoretical computer scientists stepping up to be the deans of science, congratulations!</p>

<p>Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 23, 2019.</p></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2019/"><span class="datestr">at May 23, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
