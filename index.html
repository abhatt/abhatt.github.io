<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at February 04, 2021 02:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2021/02/03/focs-2021/">FOCS 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A (very) preliminary call for papers for FOCS 2021 now available: </p>



<p><a href="https://t.co/1SAg6jInlH?amp=1" target="_blank" rel="noreferrer noopener">https://cs.yale.edu/homes/vishnoi/focs-2021-cfp.html…</a> </p>



<figure class="wp-block-image"><img src="https://pbs.twimg.com/media/EtQ2VzPXIAIOFiE?format=jpg&amp;name=large" alt="Image" /></figure>



<p>This year,  the submission deadline has been delayed to early June. This is in order to maximize the chances of a physical conference (sometime in early 2022). </p>



<p></p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2021/02/03/focs-2021/"><span class="datestr">at February 03, 2021 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/">Postdoc in Dynamic Graph Algorithms at Technical University of Denmark, Copenhagen, Denmark (apply by March 21, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Together, we will study different hypotheses, problems, and ideas concerning dynamic graph algorithms, in the pursuit of new, efficient algorithms.<br />
Here, the fun challenge is to find just the right partial answers to maintain as the graph changes, and often, the road to efficient dynamic algorithms goes via new graph theoretic insights.<br />
Contact: Eva Rotenberg.</p>
<p>Website: <a href="https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21">https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21</a><br />
Email: erot@dtu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/"><span class="datestr">at February 03, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7991231367461036724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html">A Blood Donation Puzzle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the US you can donate whole blood every eight weeks. Suppose Elvira does exactly that. Will she hit every date of the year? For example, if Elvira gave blood today, will she in some future year give blood on the 4th of July? Can we figure it out without having to rely on a computer simulation or even a calculator?</p><p>Let's make the assumptions that the blood center is open every day and that Elvira gives blood exactly every 56 days for eternity. </p><p>A year has 365 days which is relatively prime to 56=2<sup>3</sup>*7 since 365 mod 2 =1 and 365 mod 7 = 1. By the <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a> her next 365 blood donations will be on 365 distinct dates. If Elvira started giving blood at age 17, she will have hit every date at age 73.</p><p>That was easy but wrong. We have to account for those pesky leap years.</p><p>In a four year span, there will be one leap day. The total days in four years (using modular arithmetic) will still be odd and 5 mod 7, so still relatively prime to 56. So Elvira will donate on every day on the calendar exactly four times, except February 29th which she will hit once, over a period 56*4=224 years. </p><p>Alas not quite. Years ending 00 are not leap years, unless the year is divisible by 400. 2000 was a leap year but 2100 won't be. Any stretch of 224 years will hit at least one of those 00 non-leap years.</p><p>In 400 years, there will be 97 leap years. Since a regular year is 1 mod 7 days and a leap year is 2 mod 7 days, 400 years will be 497 mod 7 days. Since 497=71*7, 400 years has a multiple of 7 days. Every 400 years we have exactly the same calendar. February 3, 2421 is also a Wednesday.</p><p>The cycle of blood donations will repeat every 3200 years, the number of years in the least common multiple of 56 and the odd multiple of seven number of days in 400 years. But we can no longer directly apply the Chinese remainder theorem and argue that every day of the year will be hit. In those 3200 years Elvira will have over 20,000 blood donations. If the dates were chosen randomly the expected number to hit all dates would be 2372 by <a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem">coupon collector</a>. So one would expect Elvira would hit every day, but that's not a proof.</p><p>So I had to dust off my Python skills and do the computer simulation after all. No matter what day Elvira starts donating she will eventually hit every date of the year. If Elvira starts donating today, she would give blood on the 4th of July for the first time in 2035 and <a href="https://docs.google.com/document/d/10aTFv5UUWfsrkKmVeud_r2VObfS1_J1tJflMeWld2w4/edit?usp=sharing">hit all dates</a> on January 8, 2087 after 431 donations. The longest sequence is 3235 donations starting April 25, 2140 and hitting all dates on February 29, 2636.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html"><span class="datestr">at February 03, 2021 01:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/">To cheer you up in difficult times 19: Nati Linial and Adi Shraibman construct larger corner-free sets from better numbers-on-the-forehead protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">What will be the next polymath project? click here for our previous post. </a></p>
<h2>Number on the forehead, communication complexity, and additive combinatorics</h2>
<p class="title mathjax"><a href="https://arxiv.org/abs/2102.00421">Larger Corner-Free Sets from Better NOF Exactly-<span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">N</span></span></span></span> Protocols</a>, by Nati Linial and Adi Shraibman</p>
<p><span style="color: #0000ff;"><strong>Abstract:</strong> A subset of the integer planar grid <em><span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6">[</span><span class="mi" id="MathJax-Span-7">N</span><span class="mo" id="MathJax-Span-8">]</span><span class="mo" id="MathJax-Span-9">×</span><span class="mo" id="MathJax-Span-10">[</span><span class="mi" id="MathJax-Span-11">N</span><span class="mo" id="MathJax-Span-12">]</span></span></span></span></em> is called <strong>corner-free</strong> if it contains no triple of the form <em><span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mo" id="MathJax-Span-15">(</span><span class="mi" id="MathJax-Span-16">x</span><span class="mo" id="MathJax-Span-17">,</span><span class="mi" id="MathJax-Span-18">y</span><span class="mo" id="MathJax-Span-19">)</span><span class="mo" id="MathJax-Span-20">,</span><span class="mo" id="MathJax-Span-21">(</span><span class="mi" id="MathJax-Span-22">x</span><span class="mo" id="MathJax-Span-23">+</span><span class="mi" id="MathJax-Span-24">δ</span><span class="mo" id="MathJax-Span-25">,</span><span class="mi" id="MathJax-Span-26">y</span><span class="mo" id="MathJax-Span-27">)</span><span class="mo" id="MathJax-Span-28">,</span><span class="mo" id="MathJax-Span-29">(</span><span class="mi" id="MathJax-Span-30">x</span><span class="mo" id="MathJax-Span-31">,</span><span class="mi" id="MathJax-Span-32">y</span><span class="mo" id="MathJax-Span-33">+</span><span class="mi" id="MathJax-Span-34">δ</span><span class="mo" id="MathJax-Span-35">)</span></span></span></span></em>. It is known that such a set has a vanishingly small density, but how large this density can be remains unknown. The best previous construction was based on Behrend’s large subset of <em><span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-36"><span class="mrow" id="MathJax-Span-37"><span class="mo" id="MathJax-Span-38">[</span><span class="mi" id="MathJax-Span-39">N</span><span class="mo" id="MathJax-Span-40">]</span></span></span></span></em> with no <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-41"><span class="mrow" id="MathJax-Span-42"><span class="mn" id="MathJax-Span-43">3</span></span></span></span>-term arithmetic progression. Here we provide the first substantial improvement to this lower bound in decades. Our approach to the problem is based on the theory of communication complexity.</span></p>
<p><span style="color: #0000ff;">In the <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-44"><span class="mrow" id="MathJax-Span-45"><span class="mn" id="MathJax-Span-46">3</span></span></span></span>-players exactly-<span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-47"><span class="mrow" id="MathJax-Span-48"><span class="mi" id="MathJax-Span-49">N</span></span></span></span> problem the players need to decide whether <em><span class="MathJax" id="MathJax-Element-8-Frame"><span class="math" id="MathJax-Span-50"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52">x</span><span class="mo" id="MathJax-Span-53">+</span><span class="mi" id="MathJax-Span-54">y</span><span class="mo" id="MathJax-Span-55">+</span><span class="mi" id="MathJax-Span-56">z</span><span class="mo" id="MathJax-Span-57">=</span><span class="mi" id="MathJax-Span-58">N</span></span></span></span> </em>for inputs <em><span class="MathJax" id="MathJax-Element-9-Frame"><span class="math" id="MathJax-Span-59"><span class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61">x</span><span class="mo" id="MathJax-Span-62">,</span><span class="mi" id="MathJax-Span-63">y</span><span class="mo" id="MathJax-Span-64">,</span><span class="mi" id="MathJax-Span-65">z</span></span></span></span> </em>and fixed <em><span class="MathJax" id="MathJax-Element-10-Frame"><span class="math" id="MathJax-Span-66"><span class="mrow" id="MathJax-Span-67"><span class="mi" id="MathJax-Span-68">N</span></span></span></span></em>. This is the first problem considered in the multiplayer Number On the Forehead (NOF) model. Despite the basic nature of this problem, no progress has been made on it throughout the years. Only recently have explicit protocols been found for the first time, yet no improvement in complexity has been achieved to date. The present paper offers the first improved protocol for the exactly-<em><span class="MathJax" id="MathJax-Element-11-Frame"><span class="math" id="MathJax-Span-69"><span class="mrow" id="MathJax-Span-70"><span class="mi" id="MathJax-Span-71">N</span></span></span></span></em> problem. This is also the first significant example where algorithmic ideas in communication complexity bear fruit in additive combinatorics.</span></p>
<p>This is remarkable for various reasons. On the additive combinatorics side, improved constructions are rare. For example, <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/">we reported</a> here in 2008 Elkin’s (small) improvements of Behrend’s bound. For the corner-free problem the paper of Nati and Adi goes beyond the Behrend’s (and Elkin’s) constructions. On the communication complexity side this is significant progress on a classical 1983 problem of Chandra, Furst and Lipton. The connection that goes from improved result on communication complexity to additive combinatorics is exciting — certainly a <a href="https://gilkalai.wordpress.com/2013/11/01/natifest-is-coming/">new frontier for Nati and Adi</a>. On the blogging side, I cannot compete with the beautifully written introduction. <a href="https://arxiv.org/abs/2102.00421">Click here to read the paper</a>.</p>
<p><strong>Remark 1:</strong> The number of the forehead problem is  related to Levine’s hat problem that we discussed in <a href="https://gilkalai.wordpress.com/2020/12/12/open-problem-session-of-huji-combsem-problem-3-ehud-friedgut-independent-sets-and-lionel-levins-infamous-hat-problem/">this post</a>.</p>
<p><strong>Remark 2: </strong>Ryan Alweiss just told me about Ben Green’s new paper <a href="https://arxiv.org/abs/2102.01543">New lower bounds for van der Waerden numbers.</a> It gives a construction of a red blue colouring of {1,2,…,N} with no monochromatic arithmetic progression of length k, where N is super-polynomial! Stay tune for a fuller report.</p>
<p> </p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/"><span class="datestr">at February 03, 2021 11:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-1310669999043503597">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2021/02/forc-2021-call-for-papers.html">FORC 2021 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Reminder to anyone who has forgotten about FORC 2021 --- its a very nice venue --- and also a nice place to highlight recent work that is published or submitted elsewhere, via the non-archival track.</p><p><br /></p><p><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Symposium on Foundations of Responsible Computing (FORC) 2021 Call for Papers - Deadline February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The second annual Symposium on Foundations of Responsible Computing (FORC) is planned to be held on June 9-11, 2021, *online*. FORC is a forum for mathematically rigorous research in computation and society writ large.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics, and other relevant analytical fields to problems of pressing and anticipated societal concern.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Topics that fall in scope include, but are not restricted to, formal approaches to privacy, including differential privacy; theoretical approaches to fairness in machine learning, including the investigation of definitions, algorithms and lower bounds, tradeoffs, and economic incentives; computational and mathematical social choice (including apportionment and redistricting); theoretical foundations of sustainability; mechanism design for social good; mathematical approaches to bridging computer science, law and ethics; and theory related to modeling and mitigating the spread of epidemics. The Program Committee also warmly welcomes mathematically rigorous work on societal problems that have not traditionally received attention in the theoretical computer science literature. Whatever the topic, submitted papers should communicate their contributions towards responsible computing, broadly construed.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The symposium itself will feature a mixture of talks by authors of accepted papers and invited talks. At least one author of each accepted paper should be present at the symposium to present the work (with an option for virtual attendance, as needed).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Dual Submission Policy. Authors must indicate at the time of submission whether they are submitting to the archival-option track or the non-archival track.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the non-archival track, it is permitted to submit papers that have appeared in a peer-reviewed conference or journal since the last FORC. It is also permitted to simultaneously or subsequently submit substantially similar work to another conference or to a journal. Accepted papers in the non-archival track will receive talks at the symposium and will appear as one-page abstracts on the symposium website. They will not appear in the proceedings.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the archival-option track, papers that are substantially similar to papers that have been previously published, accepted for publication, or submitted in parallel to other peer-reviewed conferences with proceedings may not be submitted. Also, submissions that are substantially similar to papers that are already published in a journal at the time of submission may not be submitted to the archival-option track. Accepted papers in the archival-option track will receive talks at the symposium. Authors of papers accepted to the archival-option track will be given the option to choose whether to convert to a one-page abstract (which will not appear in the proceedings) or publish a 10-page version of their paper in the proceedings. The proceedings of FORC 2021 will be published by LIPIcs.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Authors are also responsible for ensuring that submitting to FORC would not be in violation of other journals’ or conferences’ submission policies.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">PC members and reviewers will be aware during the review process of whether papers have been submitted as archival-option or non-archival. The PC reserves the right to hold non-archival papers to a different standard than archival-option papers.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission Instructions.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors should upload a PDF of the paper here: </span><a style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" href="https://easychair.org/conferences/?conf=forc2021" target="_blank">https://easychair.org/conferences/?conf=forc2021</a><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* A footnote on the title of the paper should indicate whether the paper is a submission to the archival-option track or the non-archival track. Submissions to the non-archival track should also indicate in this footnote any archival venues (conferences or journals) at which the paper has appeared, a link to the publication, and the date on which it was published.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* The font size should be at least 11 point and the format should be single-column.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Author names and affiliations should appear at the top of the paper (reviewing for FORC is single, not double blind).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Beyond these, there are no formatting or length requirements, but reviewers will only be asked to read the first 10 pages of the submission. It is the authors’ responsibility that the main results of the paper and their significance be clearly stated within the first 10 pages. For both the archival-option track and the non-archival track, submissions should include proofs of all central claims, and the committee will put a premium on writing that conveys clearly and in the simplest possible way what the paper is accomplishing.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors are free to post their submissions on arXiv or other online repositories.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">All questions about submissions should be emailed to the PC chair, Katrina Ligett, at </span><a style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" href="mailto:katrina@cs.huji.ac.il" target="_blank">katrina@cs.huji.ac.il</a><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC Steering Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Avrim Blum</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Cynthia Dwork      </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Shafi Goldwasser  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Sampath Kannan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jon Kleinberg</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Toni Pitassi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Omer Reingold</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Guy Rothblum  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salvatore Ruggieri</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adrian Weller</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC 2021 Program Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Borja Balle</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Raef Bassily</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Mark Bun</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Elisa Celis</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Aloni Cohen</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Moon Duchin</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Vitaly Feldman</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kira Goldner</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Krishna Gummadi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Swati Gupta</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Gautam Kamath</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Michael Kearns</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Scott Kominers</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Himabindu Lakkaraju</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Katrina Ligett (chair)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jamie Morgenstern</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Seth Neel</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adam Smith</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kunal Talwar</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Important Dates</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission deadline: February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Author notification: March 31, 2021</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Conference: June 9-11, 2021</span></p></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2021/02/forc-2021-call-for-papers.html"><span class="datestr">at February 03, 2021 02:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/">Postdoc at PUC Chile &amp; Millennium Institute for Foundational Research on Data (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IMFD Chile, <a href="http://www.imfd.cl" rel="nofollow">http://www.imfd.cl</a> offers an open position for a postdoc to advance the understanding of theoretical aspects of neural networks.<br />
IMFD is a joint initiative held by several universities in Chile. It is a vibrant and truly interdisciplinary environment, which gathers together over 40 researchers and more than 100 students working on theoretical and applied aspects of data science.</p>
<p>Website: <a href="https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing">https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing</a><br />
Email: pbarcelo@uc.cl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/"><span class="datestr">at February 03, 2021 01:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01646">Online Learning with Simple Predictors and a Combinatorial Characterization of Minimax in 0/1 Games</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanneke:Steve.html">Steve Hanneke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livni:Roi.html">Roi Livni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moran:Shay.html">Shay Moran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01646">PDF</a><br /><b>Abstract: </b>Which classes can be learned properly in the online model? -- that is, by an
algorithm that at each round uses a predictor from the concept class. While
there are simple and natural cases where improper learning is necessary, it is
natural to ask how complex must the improper predictors be in such cases. Can
one always achieve nearly optimal mistake/regret bounds using "simple"
predictors?
</p>
<p>In this work, we give a complete characterization of when this is possible,
thus settling an open problem which has been studied since the pioneering works
of Angluin (1987) and Littlestone (1988). More precisely, given any concept
class C and any hypothesis class H, we provide nearly tight bounds (up to a log
factor) on the optimal mistake bounds for online learning C using predictors
from H. Our bound yields an exponential improvement over the previously best
known bound by Chase and Freitag (2020).
</p>
<p>As applications, we give constructive proofs showing that (i) in the
realizable setting, a near-optimal mistake bound (up to a constant factor) can
be attained by a sparse majority-vote of proper predictors, and (ii) in the
agnostic setting, a near-optimal regret bound (up to a log factor) can be
attained by a randomized proper algorithm.
</p>
<p>A technical ingredient of our proof which may be of independent interest is a
generalization of the celebrated Minimax Theorem (von Neumann, 1928) for binary
zero-sum games. A simple game which fails to satisfy Minimax is "Guess the
Larger Number", where each player picks a number and the larger number wins.
The payoff matrix is infinite triangular. We show this is the only obstruction:
if a game does not contain triangular submatrices of unbounded sizes then the
Minimax Theorem holds. This generalizes von Neumann's Minimax Theorem by
removing requirements of finiteness (or compactness), and captures precisely
the games of interest in online learning.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01646"><span class="datestr">at February 03, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01626">Sub-Linear Point Counting for Variable Separated Curves over Prime Power Rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Robelle:Caleb.html">Caleb Robelle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rojas:J=_Maurice.html">J. Maurice Rojas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Yuyu.html">Yuyu Zhu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01626">PDF</a><br /><b>Abstract: </b>Let $k,p\in \mathbb{N}$ with $p$ prime and let $f\in\mathbb{Z}[x_1,x_2]$ be a
bivariate polynomial with degree $d$ and all coefficients of absolute value at
most $p^k$. Suppose also that $f$ is variable separated, i.e., $f=g_1+g_2$ for
$g_i\in\mathbb{Z}[x_i]$. We give the first algorithm, with complexity
sub-linear in $p$, to count the number of roots of $f$ over $\mathbb{Z}$ mod
$p^k$ for arbitrary $k$: Our Las Vegas randomized algorithm works in time
$(dk\log p)^{O(1)}\sqrt{p}$, and admits a quantum version for smooth curves
working in time $(d\log p)^{O(1)}k$. Save for some subtleties concerning
non-isolated singularities, our techniques generalize to counting roots of
polynomials in $\mathbb{Z}[x_1,\ldots,x_n]$ over $\mathbb{Z}$ mod $p^k$.
</p>
<p>Our techniques are a first step toward efficient point counting for varieties
over Galois rings (which is relevant to error correcting codes over
higher-dimensional varieties), and also imply new speed-ups for computing Igusa
zeta functions of curves. The latter zeta functions are fundamental in
arithmetic geometry.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01626"><span class="datestr">at February 03, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01570">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01570">Symmetric Boolean Factor Analysis with Applications to InstaHide</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, Zhao Song, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tao:Runzhou.html">Runzhou Tao</a>, Ruizhe Zhang <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01570">PDF</a><br /><b>Abstract: </b>In this work we examine the security of InstaHide, a recently proposed scheme
for distributed learning (Huang et al.). A number of recent works have given
reconstruction attacks for InstaHide in various regimes by leveraging an
intriguing connection to the following matrix factorization problem: given the
Gram matrix of a collection of m random k-sparse Boolean vectors in {0,1}^r,
recover the vectors (up to the trivial symmetries). Equivalently, this can be
thought of as a sparse, symmetric variant of the well-studied problem of
Boolean factor analysis, or as an average-case version of the classic problem
of recovering a k-uniform hypergraph from its line graph.
</p>
<p>As previous algorithms either required m to be exponentially large in k or
only applied to k = 2, they left open the question of whether InstaHide
possesses some form of "fine-grained security" against reconstruction attacks
for moderately large k. In this work, we answer this in the negative by giving
a simple O(m^{\omega + 1}) time algorithm for the above matrix factorization
problem. Our algorithm, based on tensor decomposition, only requires m to be at
least quasi-linear in r. We complement this result with a quasipolynomial-time
algorithm for a worst-case setting of the problem where the collection of
k-sparse vectors is chosen arbitrarily.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01570"><span class="datestr">at February 03, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01541">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01541">Tree trace reconstruction using subtraces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brailovskaya:Tatiana.html">Tatiana Brailovskaya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R=aacute=cz:Mikl=oacute=s_Z=.html">Miklós Z. Rácz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01541">PDF</a><br /><b>Abstract: </b>Tree trace reconstruction aims to learn the binary node labels of a tree,
given independent samples of the tree passed through an appropriately defined
deletion channel. In recent work, Davies, R\'acz, and Rashtchian used
combinatorial methods to show that $\exp(\mathcal{O}(k \log_{k} n))$ samples
suffice to reconstruct a complete $k$-ary tree with $n$ nodes with high
probability. We provide an alternative proof of this result, which allows us to
generalize it to a broader class of tree topologies and deletion models. In our
proofs, we introduce the notion of a subtrace, which enables us to connect with
and generalize recent mean-based complex analytic algorithms for string trace
reconstruction.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01541"><span class="datestr">at February 03, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01540">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01540">Targeted Branching for the Maximum Independent Set Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hespe:Demian.html">Demian Hespe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lamm:Sebastian.html">Sebastian Lamm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schorr:Christian.html">Christian Schorr</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01540">PDF</a><br /><b>Abstract: </b>Finding a maximum independent set is a fundamental NP-hard problem. Given an
unweighted graph, this problem asks for a maximum cardinality set of pairwise
non-adjacent vertices. Some of the most successful algorithms for solving this
problem use the branch-and-bound or branch-and-reduce paradigms. Results on
branch-and-reduce algorithms were to a large part achieved by developing new,
more practical reduction rules. However, other components that have been shown
to have a significant impact on the performance of these algorithms have not
received as much attention. One of these is the branching strategy, which
determines what vertex is included or excluded in a potential solution. Even
now, the most commonly used strategy selects vertices solely based on their
degree and does not take into account other factors that contribute to the
performance of the algorithm. In this work, we develop and evaluate several
novel branching strategies for both branch-and-bound and branch-and-reduce
algorithms. Our strategies are based on one of two approaches which are
motivated by existing research. They either (1) aim to decompose the graph into
two or more connected components which can then be solved independently, or (2)
try to remove vertices that hinder the application of a reduction rule which
can lead to smaller graphs. Our experimental evaluation on a large set of
real-world instances indicates that our strategies are able to improve the
performance of the state-of-the-art branch-and-reduce algorithm by Akiba and
Iwata. To be more specific, we are able to compute a solution faster than the
default branching strategy of selecting a vertex of highest degree on 90% of
all instances tested. Furthermore, our decomposition-based strategies are able
to achieve a speedup of 90% on sparse networks (33% on all instances).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01540"><span class="datestr">at February 03, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01378">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01378">Multilevel Hypergraph Partitioning with Vertex Weights Revisited</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heuer:Tobias.html">Tobias Heuer</a>, Nikolai Maas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schlag:Sebastian.html">Sebastian Schlag</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01378">PDF</a><br /><b>Abstract: </b>The balanced hypergraph partitioning problem (HGP) is to partition the vertex
set of a hypergraph into k disjoint blocks of bounded weight, while minimizing
an objective function defined on the hyperedges. Whereas real-world
applications often use vertex and edge weights to accurately model the
underlying problem, the HGP research community commonly works with unweighted
instances.
</p>
<p>In this paper, we argue that, in the presence of vertex weights, current
balance constraint definitions either yield infeasible partitioning problems or
allow unnecessarily large imbalances and propose a new definition that
overcomes these problems. We show that state-of-the-art hypergraph partitioners
often struggle considerably with weighted instances and tight balance
constraints (even with our new balance definition). Thus, we present a
recursive-bipartitioning technique that is able to reliably compute balanced
(and hence feasible) solutions. The proposed method balances the partition by
pre-assigning a small subset of the heaviest vertices to the two blocks of each
bipartition (using an algorithm originally developed for the job scheduling
problem) and optimizes the actual partitioning objective on the remaining
vertices. We integrate our algorithm into the multilevel hypergraph partitioner
KaHyPar and show that our approach is able to compute balanced partitions of
high quality on a diverse set of benchmark instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01378"><span class="datestr">at February 03, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01149">A Tight Bound for Stochastic Submodular Cover</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellerstein:Lisa.html">Lisa Hellerstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kletenik:Devorah.html">Devorah Kletenik</a>, Srinivasan Parthasarathy <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01149">PDF</a><br /><b>Abstract: </b>We show that the Adaptive Greedy algorithm of Golovin and Krause (2011)
achieves an approximation bound of $(\ln (Q/\eta)+1)$ for Stochastic Submodular
Cover: here $Q$ is the "goal value" and $\eta$ is the smallest non-zero
marginal increase in utility deliverable by an item. (For integer-valued
utility functions, we show a bound of $H(Q)$, where $H(Q)$ is the $Q^{th}$
Harmonic number.) Although this bound was claimed by Golovin and Krause in the
original version of their paper, the proof was later shown to be incorrect by
Nan and Saligrama (2017). The subsequent corrected proof of Golovin and Krause
(2017) gives a quadratic bound of $(\ln(Q/\eta) + 1)^2$. Other previous bounds
for the problem are $56(\ln(Q/\eta) + 1)$, implied by work of Im et al. (2016)
on a related problem, and $k(\ln (Q/\eta)+1)$, due to Deshpande et al. (2016)
and Hellerstein and Kletenik (2018), where $k$ is the number of states. Our
bound generalizes the well-known $(\ln~m + 1)$ approximation bound on the
greedy algorithm for the classical Set Cover problem, where $m$ is the size of
the ground set.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01149"><span class="datestr">at February 03, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.01124">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.01124">Role Coloring Bipartite Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pandey:Sukanya.html">Sukanya Pandey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sahlot:Vibha.html">Vibha Sahlot</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.01124">PDF</a><br /><b>Abstract: </b>A k-role coloring alpha of a graph G is an assignment of k colors to the
vertices of G such that if any two vertices are assigned the same color, then
their neighborhood are assigned the same set of colors. That is, if alpha(u) =
alpha(v) for a pair of vertices u and v, then the set of colors assigned to
N(u) and N(v) are the same (where N(u) is the set of neighbors of u). By
definition, every graph on n vertices admits an n-role coloring. While for
every graph on n vertices, it is trivial to decide if it admits a 1-role
coloring, determining whether a graph admits a k-role coloring is a notoriously
hard problem for k greater than 1. In fact, it is known that k-Role coloring is
NP-complete for k greater than 1 on arbitrary graphs. There has been extensive
research on the complexity of k-role coloring on various hereditary graph
classes. Furthering this direction of research, we show that k-Role coloring is
NP-complete on bipartite graphs for k greater than 2 (while it is trivial for k
= 2). We complement the hardness result by characterizing 3-role colorable
bipartite chain graphs, leading to a polynomial-time algorithm for 3-Role
coloring for this class of graphs. We further show that 2-Role coloring is
NP-complete for graphs that are d vertices or edges away from the class of
bipartite graphs, even when d = 1.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.01124"><span class="datestr">at February 03, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=212">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/">Learning Theory Alliance and Mentoring Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://www.cs.utexas.edu/~surbhi/">Surbhi Goel</a>, <a href="https://people.eecs.berkeley.edu/~nika/">Nika Haghtalab</a>, and <a href="https://vitercik.github.io/">Ellen Vitercik</a> are the organizers of an excellent new initiative called the <a href="https://www.let-all.com/">Learning Theory Alliance</a>. They have the following inspiring mission statement:</p>



<p><em>Our mission is to develop a strong, supportive learning theory community and ensure its healthy growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers.</em></p>



<p>Their first event is a mentoring workshop, to be held at ALT 2021. I’ll be helping out by mentoring the creation of some written ALT highlights. Read on for more details from the organizers.</p>



<hr class="wp-block-separator" />



<p>We are pleased to announce the first<strong> <a href="https://www.let-all.com/alt.html">Learning Theory Mentorship Workshop</a></strong> in collaboration with the <a href="http://algorithmiclearningtheory.org/alt2021/">Conference on Algorithmic Learning Theory (ALT) 2021</a> to be held virtually on <strong>March 4-5, 2021</strong>. The workshop will focus on building technical and networking skills while giving participants an opportunity to interact with fellow researchers in the field. </p>



<p>The workshop is intended for upper-level undergraduate and all-level graduate students as well as postdoctoral researchers who are excited about the possibility of learning theory research. No prior research experience in the field is expected.</p>



<p>We have several planned events including:</p>



<ul><li><strong>How-to talks </strong>which will provide general advice about giving talks, structuring papers, writing reviews, networking, and attending conferences.</li><li>A small group discussion <strong>dissecting a short talk</strong> with feedback from a senior researcher.</li><li>An informal and interactive<strong> “Ask Me Anything” </strong>sessionwith a senior member of the learning theory community.</li><li><strong>General audience talks</strong> about recent learning theory research which will be accessible to new researchers.</li><li><strong>Social events </strong>such as board games.</li></ul>



<p>Our lineup includes Jacob Abernethy, Kamalika Chaudhuri, Nadav Cohen, Rafael Frongillo, Shafi Goldwasser, Zhiyi Huang, Robert Kleinberg, Pravesh Kothari, Po-Ling Loh, Lester Mackey, Jamie Morgenstern, Praneeth Netrapalli, Vatsal Sharan and Mary Wootters.</p>



<p>Together with Gautam Kamath, we will also organize a written account of ALT titled <strong>“ALT Highlights”</strong> which will summarize the research presented at ALT. We will assist students and postdocs to set up interviews with presenters and keynote speakers as part of the highlights.</p>



<p>A short application<a href="https://forms.gle/v8b8aeJMgWxJ1Bbx9" target="_blank" rel="noreferrer noopener"> form</a> is required to participate with an <strong>application deadline of Friday, Feb. 19, 2021</strong>. Students with backgrounds that are underrepresented or underserved in related fields are especially encouraged to apply. <strong>We will be accommodating all time zones.</strong> More information can be found on the event’s website:<a href="http://let-all.com/alt.html" target="_blank" rel="noreferrer noopener"> http://let-all.com/alt.html</a>.</p>



<p>This workshop is part of our broader community building initiative called the Learning Theory Alliance (advised by Peter Bartlett, Avrim Blum, Stefanie Jegelka, Po-Ling Loh and Jenn Wortman Vaughan). Check out <a href="http://let-all.com/" target="_blank" rel="noreferrer noopener">http://let-all.com/</a> for more details and to sign up to volunteer.</p>



<p>Best,<br />Surbhi Goel, Nika Haghtalab and Ellen Vitercik</p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/"><span class="datestr">at February 02, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/009">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/009">TR21-009 |  One-way Functions and Partial MCSP | 

	Eric Allender, 

	Mahdi Cheraghchi, 

	Dimitrios Myrisiotis, 

	Harsha Tirumala, 

	Ilya Volkovich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
One-way functions (OWFs) are central objects of study in cryptography and computational complexity theory. In a seminal work, Liu and Pass (FOCS 2020) proved that the average-case hardness of computing time-bounded Kolmogorov complexity is equivalent to the existence of OWFs. It remained an open problem to establish such an equivalence for the average-case hardness of some NP-complete problem. In this paper, we make progress on this question by studying a polynomially-sparse variant of Partial Minimum Circuit Size Problem (Partial MCSP), which we call Sparse Partial MCSP, as follows.

1. First, we prove that if Sparse Partial MCSP is zero-error average-case hard on a polynomial fraction of its instances, then there exist OWFs.
2. Then, we observe that Sparse Partial MCSP is NP-complete under polynomial-time deterministic reductions. That is, there are NP-complete problems whose average-case hardness implies the existence of OWFs.
3. Finally, we prove that the existence of OWFs implies the nontrivial zero-error average-case hardness of Sparse Partial MCSP.

Thus the existence of OWFs is inextricably linked to the average-case hardness of this NP-complete problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/009"><span class="datestr">at February 01, 2021 09:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5433">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/self-concordant-analysis-newton/">Going beyond least-squares – I : self-concordant analysis of Newton method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Least-squares is a workhorse of optimization, machine learning, statistics, signal processing, and many other scientific fields. I find it particularly appealing (too much, according to some of my students and colleagues…), because all algorithms, such as stochastic gradient [<a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">1</a>], and analyses, such as for kernel ridge regression [<a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">2</a>], are much simpler and rely on reasonably simple linear algebra.</p>



<p class="justify-text">Despite the unique appeal of least-squares, most interesting optimization or machine problems go beyond quadratic functions. While there are many algorithms and analyses dedicated to more general situations, it is tempting to use least-squares for analysis or algorithms by considering the functions at hand to be approximately quadratic.</p>



<p class="justify-text">Using bounds on the third-order derivatives and <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansions</a> are the usual ways to go, but they are not ideal for sharp non-asymptotic results where the deviation from quadratic functions has to be precisely quantified. In many situations, more finer structures can be used. In a series of posts, I will describe <em>self-concordance</em> properties, that relate the third order derivatives to second order ones.</p>



<p class="justify-text">There are many great books that cover this topic where the material below is taken from [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>, <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">6</a>].</p>



<h2>Self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ You may wonder why the power \(3/2\) or why the constant \(2\). The constant is just a convention (multiplying the function \(f\) by \(c\) would replace \(2\) by \(2/\sqrt{c}\)), while the power \(3/2\) is fundamental, as it makes the definition “affine-invariant”, that is, if \(f\) is self-concordant, so is \(y \mapsto f(ay)\) for any \(a \in \mathbb{R}\).</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), this has to be true along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h,h]= \sum_{i,j,k=1}^d h_i h_j h_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the symmetric third-order tensor and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the second-order one, then $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h,h]| \leqslant 2 f^{\prime\prime}(x)^{3/2}[h,h].$$</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are self-concordant, then so is \(f+g\) (but not their average). Moreover, if \(f\) is self-concordant, so is \(y\mapsto f(Ay)\) for any matrix \(A\). The property is also preserved by Fenchel conjugation. Classical examples are all linear and quadratic functions, the negative logarithm, the negative log-determinant, or the negative logarithm of quadratic functions. The three previous examples are particularly important because they are “barrier functions”, with non-full domains, and are instrumental to interior-point methods (see below).</p>



<p class="justify-text"><strong>Properties in one dimension.</strong>  A nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( f^{\prime\prime}(x)^{-1/2} \big) \big| = \big| \frac{1}{2} f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-3/2} \big| \leqslant 1,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ – x \leqslant f^{\prime\prime}(x)^{-1/2} \, – f^{\prime\prime}(0)^{-1/2} \leqslant x,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3} \frac{f^{\prime\prime}(0)}{\big(1 + x f^{\prime\prime}(0)^{1/2}\big)^2} \leqslant f^{\prime\prime}(x) \leqslant \frac{f^{\prime\prime}(0)}{\big(1 – x f^{\prime\prime}(0)^{1/2}\big)^2}.$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$-f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1+x f^{\prime\prime}(0)^{1/2}} \leqslant f^\prime(x)-f^\prime(0) \leqslant -f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1-x f^{\prime\prime}(0)^{1/2}},$$ and  $$ \tag{4} \rho \big( – f^{\prime\prime}(0)^{1/2} x \big) \leqslant f(x) \ – f(0) \ – f^\prime(0) x \leqslant   \rho \big( f^{\prime\prime}(0)^{1/2} x \big),$$ with \(\displaystyle \rho(u) =\  – \log(1-u) \ – u \sim \frac{u^2}{2} \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. This upper-bound is valid as long as \(\delta = f^{\prime\prime}(0)^{1/2} x \in [0,1]\), while the lower-bound on \(f\) is always true. The function \(\rho\) is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="303" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/rho.png" class="wp-image-5631" height="233" /></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(h \in \mathbb{R}^d\) such that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\), we have upper and lower bounds for the Hessian, the gradient and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm (see detailed proofs at the end of the post) $$\tag{5}(1-\delta)^2 f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq  \frac{1}{(1-\delta)^2}  f^{\prime \prime}(x),$$ $$ \tag{6}\big\| f^{\prime\prime}(x)^{-1/2} \big(f^\prime(x+\Delta)-f^\prime(x) -f^{\prime \prime}(x)\Delta \big) \big\|  \leqslant \frac{\delta^2}{1-\delta},$$ and $$\tag{7} \rho(-\delta) \leqslant f(x+\Delta)\ -f(x) \ – f^\prime(x)^\top \Delta \leqslant \rho(\delta).$$ A nice consequence is that if \(\delta &lt; 1\), then \(x+\Delta \in C\), that is, we get “for free” a feasible point. Moreover, these approximations are “second-order tight” at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact.</p>



<p class="justify-text"><strong>Dikin ellipsoid.</strong> The condition that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\) defines an ellipsoid around \(x\) which is always strictly inside the domain of \(f\) (see example in the plot below). The results above essentially state that when inside the Dikin ellipsoid, the locally quadratic approximation can be used. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="388" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/dykin.gif" class="wp-image-5642" height="307" />Dikin ellipsoids for the function \(f(x) = \sum_{i=1}^k \log (b_i – a_i^\top x)\), which is self-concordant with a domain which is a polytope.</figure></div>



<p class="justify-text">In this post, I will focus primarily on the use self-concordant functions in optimization through the analysis of Newton method.</p>



<h2>Why should you care about Newton method?</h2>



<ol class="justify-text"><li>For fun: Newton method is one of the classics of optimization!</li><li>For high precisions: as we will see, it is quadratically convergent and attains machine precision after solving a few linear systems.</li><li>Even in high dimensions where the linear system can be expensive, Newton method may still be the method of choice for severely ill-conditioned problems where even accelerated first-order methods are too slow to obtain low precision solutions.</li><li>It sometimes comes for free in situations where gradients are expensive to evaluate compared to \(d\).</li></ol>



<h2>Classical analysis of Newton method</h2>



<p class="justify-text">Given a function \(f: \mathbb{R}^d \to \mathbb{R}\), Newton method is an iterative optimization algorithm consisting in locally approximating the function \(f\) around the iterate \(x_{t}\) by a second-order Taylor expansion $$f(x_t) + f^\prime(x_t)^\top(x-x_t) + \frac{1}{2} (x-x_t)^\top f^{\prime \prime}(x_t) ( x – x_t),$$ whose minimum can be found in closed form as $$\tag{8} x_{t+1} = x_t \ – f^{\prime \prime}(x_t)^{-1} f^{\prime}(x_t).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="323" alt="" src="https://francisbach.com/wp-content/uploads/2021/02/approx_taylor.gif" class="wp-image-5705" height="275" />Quadratic approximation and Newton step (in green) for varying starting points (in red). When the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer.</figure></div>



<p class="justify-text">Newton method is classically analyzed for three times differentiable convex functions with bounded Hessians and third-order derivatives. The method is only locally convergent, that is, far away from the global minimizer \(x_\ast\) (even for very regular convex function), the method may diverge. In the non-convex setting, this leads to nice <a href="https://en.wikipedia.org/wiki/Newton_fractal">fractal plots</a>, but even for convex functions, the method can be unstable (see plot above).</p>



<p class="justify-text">Locally, it is quadratically convergent, that is, there exists \(c&gt;0\), such that if \(\| x_t \ – x_\ast\| \leqslant c\), then \(\| x_{t+1} \ – x_\ast \| /c  \leqslant \big( \|x_t\  – x_\ast\| / c \big)^2\). Roughly, the number of significant digits doubles at every iteration.</p>



<p class="justify-text">This leads to $$ \| x_{t} \ – x_\ast \| \leqslant c \big( \|x_{t_0} \ – x_\ast\| / c \big)^{2^{t-t_0}} ,$$ for \(t \geqslant t_0\) and \(t_0\) an index for which \(\| x_{t_0} \ – x_\ast\| \leqslant c\). It is less than \(\varepsilon\), as soon as \(2^{t-t_0} \log ( c / \| x_{t_0} – x_\ast \| ) \geqslant \log ( c / \varepsilon)\), that is, $$ t \geqslant t_0 + \frac{ \log \log ( c / \varepsilon)}{ \log 2} \  –  \frac{\log \log ( c / \| x_{t_0} – x_\ast \| )}{\log 2}.$$</p>



<p class="justify-text">That is, once we enter the quadratic phase, we obtain a number of iterations in \( \log \log ( 1/ \varepsilon)\), that is, only very few iterations. For example, for \(\varepsilon = 10^{-16}\), \(\log \log ( 1/ \varepsilon) \leqslant 4\).</p>



<p class="justify-text">Two major issues may be solved elegantly using self-concordant analysis: </p>



<ol class="justify-text"><li>Dealing with the two phases of Newton method, the quadratically convergent final phase, as well as the initial phase.</li><li>Obtaining convergence rates which are affine-invariant, that is, minimizing \(f(x)\) of \(f(Ax+b)\) for \(A\) an invertible matrix should lead to exactly the same convergence rate (this is not the case for the classical analysis, where for example the constant \(c\) depends on non affine-invariant quantities).</li></ol>



<h2>Self-concordant analysis of Newton method</h2>



<p class="justify-text">Consider \(f: \mathcal{C} \to \mathbb{R}\) which is self-concordant. Since Newton method in Eq. (8) is not globally convergent, we need to study a version where the Newton step is performed partially. There are several possible strategies. Here I present the so-called “damped Newton” iteration and thus study the iteration $$ x^+ = x\  – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x),$$ where we define the “Newton decrement” \(\lambda(x)\) at \(x \in C\), as $$ \lambda^2 = \lambda(x)^2 =f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x) = \|f^{\prime \prime}(x)^{-1/2} f^\prime(x) \|^2.$$</p>



<p class="justify-text">The Newton decrement is a key quantity in the analysis of Newton method, as \(\frac{1}{2} \lambda(x)^2\) is exactly the decrease in the quadratic approximation obtained by a full Newton step. Moreover, </p>



<ol class="justify-text"><li>If \(\lambda(x) &lt; 1\), then \(x^+\) is in the Dikin ellipsoid where we can expect the local quadratic approximation to be relevant.</li><li>If \(\lambda(x) &lt; 1\), then one can show that \(f(x) \ – f(x_\ast) \leqslant \rho( \lambda(x)) \sim \frac{1}{2} \lambda(x)^2\) when \(\lambda(x)\) is close to zero, that is, the Newton decrement provides an upper bound on the distance to optimum.</li></ol>



<p class="justify-text">The update corresponds to \(\Delta = \ – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x)\), and \(\delta  = \frac{\lambda(x)}{1+\lambda(x)}  \in [0,1]\), thus \(x^+\) is automatically feasible (which is important for constrained case, see below).</p>



<p class="justify-text">Moreover, using Eq. (7), we get $$f(x^+)-f(x) \leqslant \ – \frac{f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x)}{1+\lambda(x)} + \rho \Big( \frac{\lambda(x)}{1+\lambda(x)} \Big) = \log (1+ \lambda(x)) \ – \lambda(x).$$ This immediately leads to a fixed decrease of \(\frac{1}{4} \, – \log \frac{5}{4} \geqslant 0.0268\) if \(\lambda(x) \geqslant \frac{1}{4}\). </p>



<p class="justify-text">We can now compute the Newton decrement at \(x^+\), to see how it decreases, by first bounding $$\lambda(x^+) =\|f^{\prime \prime}(x^+)^{-1/2} f^\prime(x^+) \| \leqslant \frac{1}{1-\delta} \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \|,$$ using Eq. (5). We then have, using Eq. (6): $$ \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \| \leqslant \big\| f^{\prime \prime}(x)^{-1/2} \big( f^\prime(x) + f^{\prime\prime}(x) \Delta  \big) \big\| + \frac{\delta^2}{ 1-\delta } \leqslant \frac{\lambda(x)^2}{1+\lambda(x)} + \frac{\delta^2}{ 1-\delta  }.$$ This exactly leads to $$\lambda(x^+) \leqslant 2  \lambda(x)^2, $$ which leads to quadratic convergence if \(\lambda(x)\) is small enough.</p>



<p class="justify-text">We can then divide the analysis in two phases: before \(\lambda(x) \leqslant 1/4\) and after. The first integer \(t_0\) such that \(\lambda(x) \leqslant 1/4\) is less than \(\frac{ f(x_0) – f(x_\ast)}{0.0268} \leqslant 38 [ f(x_0) – f(x_\ast) ]\). Then, for the second phase, \(2\lambda(x_t) \leqslant (1/2)^{2^{t-t_0}}\). Given that for \(\lambda \leqslant 1/4\), \(\rho(\lambda) \leqslant 2\lambda\), we reach precision \(\varepsilon\) as soon as \(2^{t-t_0} \log 2 \geqslant \log \frac{1}{\varepsilon}\), that is, \(t \geqslant t_0 + \frac{1}{\log 2} \log \log \frac{1}{\varepsilon} -1\). This leads to number of iterations to reach a precision \(\varepsilon\)  which is less than $$38[ f(x_0) \ – f(x_\ast)]  +2 \log \log \frac{1}{\varepsilon}.$$</p>



<h2>Interior point methods</h2>



<p class="justify-text">Self-concordant functions are also key in the analysis of interior point methods. Consider a function \(f\) defined on \(\mathbb{R}^d\) and the constrained optimization problem $$\min_{ x \in C} f(x),$$ where \(C\) is a convex set. Barrier methods are appending a so-called “barrier function” \(g(x)\) to the objective function. A function \(g\) is a barrier function if \(g\) is convex and with domain containing the relative interior of \(C\), with gradients that explode when reaching the boundary of \(C\). We then solve instead $$\tag{9} \min_{x \in \mathbb{R}^d}  \varepsilon^{-1} f(x) +  g(x), $$ where \(\varepsilon &gt; 0\). Typically, the minimizer \(x_\varepsilon\) is in the relative interior of \(C\) (hence the name interior point method), and, when \(\varepsilon\) tends to zero, \(x_\varepsilon\) tends to the minimizer of \(f\) on \(C\).</p>



<p class="justify-text">When both the original function \(f\) and the barrier function \(g\) are self-concordant, the (damped) Newton method is particularly useful as it ensures feasibility of the iterates. Moreover, the interplay between the progressive reduction of \(\varepsilon\) towards zero and the approximate resolution of Eq. (9) can be completely characterized (see [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>]). This applies directly to linear programming, second-order cone programming and semidefinite programming.</p>



<h2>Applications in machine learning</h2>



<p class="justify-text">If you have reached this point, you are probably a big fan of self-concordance. While this property is crucial in optimization, is it really relevant for machine learning or statistics? The sad truth is that most of the non-quadratic functions within machine learning are <em>not</em> self-concordant in the sense of Eq. (1) or Eq. (2). In particular, log-sum-exp functions, such that the logistic loss \(f(t) = \log( 1 + \exp(-t) )\),  do satisfy a relationship between third and second-order derivatives, but of the form $$| f^{\prime \prime \prime}(t)| \leqslant f^{\prime \prime }(t),$$ without the power \(3/2\). This seemingly small difference leads to several variations [7] which will the topic of next month blog post. Meanwhile, it is worth mentioning two applications in machine learning of classical self-concordance.</p>



<p class="justify-text"><strong>Maximum likelihood estimation for covariance matrices.</strong> Beyond its use in interior point methods, self-concordant functions arise naturally when estimating the covariance matrix using maximum likelihood estimation with a Gaussian model. Indeed, the negative log-likelihood can be written as $$ – \log p (x| \mu ,\Sigma) =\frac{d}{2} \log(2\pi) +  \frac{1}{2} \log \det \Sigma + \frac{1}{2} ( x -\mu)^\top \Sigma^{-1} ( x – \mu),$$  which leads to a negative log-determinant of the inverse \(\Sigma^{-1}\) of the covariance matrix \(\Sigma\), which is a self-concordant function, on which the guarantees discussed above apply.</p>



<p class="justify-text"><strong>Self-concordant losses.</strong> One can also design losses which are self-concordant. For example, a self-concordant version of the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> is $$f(t) = \sqrt{1+t^2} \ – 1 \ –  \log \frac{ \sqrt{1+t^2} +1 }{2} .$$ It can be seen as the Fenchel-conjugate of \(– \log(1-t^2)\), and the proximity with a quadratic problem can be leveraged to obtain generalization performances using this loss function which are essentially the same as for the square loss. It can also be used for binary classification (see [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">8</a>] for details, and the two nice <a href="https://ostrodmit.github.io/blog/2018/11/12/self-concordance-part-1/">blog posts</a> of Dmitrii Ostrovskii).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="359" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/loss_selfc.png" class="wp-image-5654" height="251" /></figure></div>



<p class="justify-text"><strong>One-step-estimation.</strong> Given the focus of this post on Newton method, I cannot resist mentioning a great technique coming from statistics that relies on a single Newton step. We consider a classical empirical risk minimization problem (statisticians would call it an M-estimation problem), with empirical risk \(\displaystyle \widehat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i) )\). Given an estimator \(\hat{\theta}\), obtained by any means, then if \(\hat{\theta}\) is \(\frac{1}{\sqrt{n}}\) away from the optimal parameter (with optimal performance on unseen data), then one Newton step on the function \(\widehat{R}\) started from \(\hat{\theta}\) will lead to an estimator achieving asymptotically the usual <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao</a> lower bound. In a nutshell, a single Newton step on the empirical risk transforms a good estimator into a very good estimator. See [9, Section 5.7] for more details.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Adrien Taylor and Dmitrii Ostrovskii for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] F. Bach and E. Moulines. <a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate</a> \(O(1/n)\). Advances in Neural Information Processing Systems (NIPS), 2013.<br />[2] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7(3):331-368, 2007.<br />[3] Yurii Nesterov, and Arkadii Nemirovskii. <em><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">Interior</a><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611970791.bm">-Point Polynomial Algorithms in Convex Programming</a></em>, SIAM, 1994.<br />[4] Arkadii Nemirovski. <em><a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">Interior Point Polynomial Time Methods in Convex Programming</a></em>. Lecture notes, 1996.<br />[5] Yurii Nesterov. <em><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">Lectures on Convex Optimization</a></em> (Vol. 137). Springer, 2018.<br />[6] Stephen P. Boyd, and Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br />[7] Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1271941980">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010<br />[8] Dmitrii Ostrovskii, and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br />[9] Aad W. Van der Vaart. <em><a href="http://Aad W. Van der Vaart. Asymptotic Statistics">Asymptotic Statistics</a></em>, volume 3. Cambridge University Press, 2000.</p>



<h2>Detailed proofs for self-concordance properties</h2>



<p class="justify-text">We first show Eq. (7), by considering the function \(a(t) = f(x+t\Delta)\), which is a one-dimensional self-concordant function, for which \(a^\prime(t) = \Delta^\top f^\prime(x+t\Delta)\), and \(a^{\prime\prime}(t) = \Delta^\top f^{\prime\prime}(x+t\Delta) \Delta\). Then \(a^{\prime\prime}(0) = \delta^2\), and Eq. (4) for \(x=1\) and \(a\) exactly leads to Eq. (7).</p>



<p class="justify-text">In order to show Eq. (5), we consider \(h \in \mathbb{R}^d\), and the function \(b(t) = h^\top f^{\prime\prime}(x+t\Delta) h\). We have, \(b'(t) = f^{\prime\prime\prime}(x+t\Delta)[h,h,\Delta]\), which can be bounded using Eq. (2) as $$ |b'(t) | \leqslant 2 f^{\prime\prime}(x+t\Delta)[h,h] f^{\prime\prime}(x+t\Delta)[\Delta,\Delta]^{1/2} = 2 b(t) a^{\prime \prime}(t)^{1/2} \leqslant 2 b(t) \frac{\delta}{1-t\delta}, $$ using Eq. (3). This implies that for \(\delta t \in [0,1)\): $$\frac{d}{dt} \big[ (1-\delta t)^2 b(t) \big] = -2\delta (1-\delta t) b(t) + (1-\delta t)^2 b'(t) \leqslant 0, $$  which implies \(b(t) \leqslant \frac{b(0)}{(1-\delta t)^2} = \frac{h^\top f^{\prime\prime}(x) h}{(1-\delta t)^2}\), which leads to the right-hand side of Eq. (5) since this is true for all \(h \in \mathbb{R}^d\). The left-hand side is proved similarly.</p>



<p class="justify-text">In order to show Eq. (6), we consider the function \(g(t) = h^\top f^\prime(x+t\Delta)\), for which, \(g^\prime(t) = h^\top f^{\prime\prime}(x+t\Delta) \Delta\), and \(g^{\prime\prime}(t) =   f^{\prime\prime\prime}(x+t\Delta) [h,\Delta,\Delta]\), which satisfies: $$|g^{\prime\prime}(t)| \leqslant 2 f^{\prime\prime}(x+t\Delta)[\Delta,\Delta] f^{\prime\prime}(x+t\Delta)[h,h]^{1/2} \leqslant 2 b(t)^{1/2} a^{\prime \prime}(t). $$ This leads to $$ |g^{\prime\prime}(t)| \leqslant 2 \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{(1-\delta t)^3}.$$ We can then integrate twice, using \(g(0) = h^\top f^\prime(x)\) and \(g^\prime(0) = h^\top f^{\prime\prime}(x) \Delta\), to get: $$h^\top \big(f^\prime(x+t\Delta)-f^\prime(x) \ – f^{\prime \prime}(x)\Delta \big) \leqslant    \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{1-\delta},$$ which leads to Eq. (5) after maximizing with respect to \(h\).</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/self-concordant-analysis-newton/"><span class="datestr">at February 01, 2021 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6639458808942063282">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/two-phd-positions-at-department-of.html">Two PhD positions at the Department of Computer Science, Reykjavik University: Model-driven SE for blockchain and smart contracts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">The<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Software and Emerging Technology Lab at the Department of Computer Science,</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Reykjavik University</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">, is looking for two PhD candidates to work on an ongoing research project on the application of Model Driven Software Engineering principles, methodologies, technologies and abstractions to Blockchain and Smart Contracts. While both positions require strong software development skills and familiarity with the<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">model-driven software engineering</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span>approach, the first position will focus on domain analysis and code generation, while the second position is concerned with contract safety and validity and requires knowledge in model<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">verification</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"> and validation. The project is based in Iceland. It will be directed by Mohammad Hamdaqa in collaboration with Luca Aceto and Gísli Hjálmtýsson and in close collaboration with Polytechnique Montréal in Canada. The positions</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">are fully funded and<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">include full tuition waiver and a salary in accordance with the Icelandic<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">Research Fund guidelines. Particularly the funding is covering full tuition as well as a stipend of 383,000 ISK</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span>per month before taxes for a minimum of three years. </span></span></span></div><div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">If you are interested to apply, please send the documents below to<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">the following email addresses:<span> </span></span></span><a href="mailto:mhamdaqa@ru.is" target="_blank" rel="noopener noreferrer"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">mhamdaqa@ru.is</span></span></a><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">; </span></span><a href="mailto:luca@ru.is" target="_blank" rel="noopener noreferrer">luca@ru.is</a>; <a href="mailto:gisli@ru.is" target="_blank" rel="noopener noreferrer">gisli@ru.is</a></span></div><ul><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of your CV and research interests</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of all your transcripts</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A sample publication</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A maximum of one page research statement of your plans for research in your PhD.</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">Your intended starting date / and if you need a visa</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">For more information about the position and the research topics, do not hesitate to send your enquiries<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">to any of the project collaborators.</span></span></span></li></ul><div><span style="background-color: white;">Mohammad Hamdaqa  (<a href="https://en.ru.is/cress/" target="_blank" rel="noopener noreferrer">https://en.ru.is/cress/</a>)<br /><a href="mailto:mhamdaqa@ru.is" target="_blank" rel="noopener noreferrer">mhamdaqa@ru.is</a></span></div><div><span style="background-color: white;">Luca Aceto (<a href="http://icetcs.ru.is/" target="_blank" rel="noopener noreferrer">http://icetcs.ru.is/</a>)<br /><a href="mailto:luca@ru.is" target="_blank" rel="noopener noreferrer">email: luca@ru.is</a><br /><br />Gísli Hjálmtýsson (https://en.ru.is/fintech/)<br />email: gisli@ru.is</span></div><div><br /></div><div>Informal inquiries about the project and the conditions of work are very welcome. We will start reviewing applications as soon as they arrive and will continue to accept applications until each position is filled. We strongly encourage interested applicants to send their applications as soon as possible and no later than 28 February 2021.</div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/two-phd-positions-at-department-of.html"><span class="datestr">at February 01, 2021 09:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/">Postdoc at Georgia Institute of Technology (apply by February 25, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms and Randomness Center (ARC) at Georgia Tech is recruiting postdocs for a 1-year position with the possibility of extension by another year starting August 1, 2021. Area of expertise include algorithms, discrete mathematics, optimization, theoretical machine learning. Candidates with a PhD in Computer Science, Math, Operations Research are encouraged to apply by February 25.</p>
<p>Website: <a href="http://arc.gatech.edu/postdoc21">http://arc.gatech.edu/postdoc21</a><br />
Email: lisa.cox@isye.gatech.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/"><span class="datestr">at February 01, 2021 12:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8809394001786387491">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html">Grading policies during Covid-No easy answers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Because of COVID  (my spellecheck says covid and Covid are not works, but COVID is) various schools have done various things to make school less traumatic. Students already have problems, either getting COVID or having their friends got family get it (I've had four relatives get it, and one died) . Some do not adjust to learning online.  Some do not have good computer connection to learn on line. So what is a good policy? Here are some things I have either seen schools do or heard that they might do.</p><p><br /></p><p>1) Be more generous with Tuition-Refunds if a student has to withdraw. </p><p>2) Be more generous with Housing-Refunds if a students comes to campus thinking it will be courses on campus and there are no courses on campus. Or if a student has to withdraw. </p><p>3) Make the deadlines for dropping-without-a-W, or taking-it-pass-fail, later in the semester. </p><p>4) Tell the teachers to `just teach them the bare min they need for the next course.'</p><p>5) Allow students to take courses P/F in their major and still allow them to count, so a student might get a D in Discrete Math and be able to go on in the major. </p><p>6) How far to extend deadlines? How is this: extend deadline to make it P/F until the last day of classes (but before the final) and then after the final is given, the school changes its mind and says - OH, you can change to P/F now if you want to.</p><p>7) Allow either an absolute number (say 7) or a fraction (say 1/3) of the courses to be changed to P/F by the last day of class.</p><p>8) Combine 6 or 7 with saying NO- a D is an F for a P/F course. Perhaps only if its in the major, but that maybe hard to work out. since majors can change. Some schools do A-B-C-NO CREDIT, where the NC grade does not go into the GPA.</p><p>9) Give standard letter grades and tell the students to tough it out. Recall the following inspirational quotes</p><p>When the going gets tough, the tough go shopping</p><p>When the going gets tough, the tough take a nap</p><p>If at first you don't succeed, quit. Why make a damn fool of yourself. </p><p>If at first you don't succeed, then skydiving is not for you. </p><p>10) Decide later in the term what to do depending on who yells the loudest. </p><p>11) Any combination of the above that makes sense, and even some that don't. </p><p><br /></p><p>On the one hand, there are students who are going through very hard times because of covid and should be given a break. On the other hand, we want to give people a good education and give grades that are meaningful (the logic of how to give grades in normal times is another issue for another blog post). </p><p>What is your school doing? Is it working? What does it mean to be working?</p><p>The problems I am talking about are first-world problems or even champagne-problems. I know there are people who have far worse problems then getting a bad grade or dropping courses.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html"><span class="datestr">at January 31, 2021 09:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7972">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through classical statistical learning theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">ML theory with bad drawings</a> <strong>Next post:</strong> TBD,  see also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5c6a9e86-bca7-42df-a04a-acc200ed2c2d">Lecture video</a> (starts in slide 2 since I hit record button 30 seconds too late – sorry!)</p>



<p>These are rough notes for the first lecture in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">my advanced topics in machine learning seminar</a>. See the <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">previous post</a> for the introduction.</p>



<p>This lecture’s focus was on <strong>“classical” learing theory</strong>. The distinction between “classical learning” and “deep learning” is semantic/philosophical, and doesn’t matter much for this seminar. I personally view this difference as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/dzDbio7.png" alt="" /></figure>



<p>That is, deep learning is a framework that allows you to translate more resources (data and computation) into bettter performance. “Classical” methods often have a “threshold effect” where a certain amount of data and computation is needed, and more would not really help. For example, in parametric methods there will typically be a sharp threshold for the amount of data required for saturating the potential performance. Even in non-parametric models such as nearest neighbors or kernel methods, the computational cost is fixed for a fixed amount of data, and there is no way to profitably trade more computation for better performance.</p>



<p>In contrast, for deep learning, we often can get better performance using the same data by using bigger models or more computation. For example, I doubt this <a href="http://karpathy.github.io/2019/04/25/recipe/">story of Andrej Karpathy</a> could have happened with a non deep-learning method:</p>



<p><em>“One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).”</em></p>



<h2>Leaky pipelines</h2>



<p>We can view machine learning (deep or not) as a series of “leaky pipelines”:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/0dpRqYa.png" alt="" /></figure>



<p>We want to create an adaptive system that performs well in the wild, but to do so, we:</p>



<ol><li>Set up a benchmark of a test distribution, so we have some way to compare different systems.</li><li>We typically can’t optimize directly on the benchmark, both because losses like accuracy are not differentiable and because we don’t have access to an unbounded number of samples from the distribution. (Though there are exceptions, such as when optimizing for playing video games.) Hence we set up the task of optimizing some proxy loss function <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}" class="latex" title="\mathcal{L}" /> on some finite samples of training data.</li><li>We then run an optimization algorithm whose ostensible goal is to find the <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{F}" class="latex" title="f \in \mathcal{F}" /> that minimizes the loss function over the training data. (<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> is a set of models, sometimes known as <em>architecture</em>, and sometimes we also add other restrictions such norms of weights, which is known as <em>regularization</em>)</li></ol>



<p>All these steps are typically “leaky.” Test performance on benchmarks is not the same as real-world performance. Minimizing the loss over the training set is not the same as test performance. Moreover, we typically can’t solve the loss minimization task optimally, and there isn’t a unique minimizer, so the choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> depends on the algorithm.</p>



<p>Much of machine learning theory is about obtaining guarantees bounding the “leakiness” of the various steps. These are often easier to do in “classical” contexts of statistical learning theory than for deep learning. In this lecture, we will make a short blitz through classical learning theory. This material is covered in several sources, including the excellent book <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">understanding machine learning</a> and the upcoming Hardt-Recht text.</p>



<p>We will be very rough, using proofs by picture and making some simplifications (e.g., working in one dimension, assuming functions are always differentiable, etc.)</p>



<h2>Convexity</h2>



<p>A (nice) function <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R} \rightarrow \mathbb{R}" class="latex" title="f:\mathbb{R} \rightarrow \mathbb{R}" /> is (strongly) <em>convex</em> if it satisfies one of the following three equivalent conditions:</p>



<ol><li>For every two points <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,y" class="latex" title="x,y" />, the line between <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> and <img src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(y,f(y))" class="latex" title="(y,f(y))" /> is above the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</li><li>For every point <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, the tangent line at <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> with slope <img src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x)" class="latex" title="f'(x)" /> is below the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</li><li>For every <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x)&gt;0" class="latex" title="f''(x)&gt;0" />.</li></ol>



<figure class="wp-block-image"><img src="https://i.imgur.com/0p5cY35.png" alt="" /></figure>



<p>To see that for example, 2 implies 3, we can use the contrapositive. If 3 does not hold and <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is such that <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%3C+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x) &lt; 0" class="latex" title="f''(x) &lt; 0" /> (should really assume <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%5Cleq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x) \leq 0" class="latex" title="f''(x) \leq 0" /> but we’re being rough) then by Taylor, around <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> we get</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x+%2B+%5Cdelta%29+%3D+f%28x%29+%2B+%5Cdelta+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29+%2F2+%2B+O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)" class="latex" title="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)" /></p>



<p>For <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> small enough, <img src="https://s0.wp.com/latex.php?latex=O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(\delta^3)" class="latex" title="O(\delta^3)" /> is negligible and so we see that the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> near <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> equals the tangent line <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%2B+%5Cdelta+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x) + \delta f'(x)" class="latex" title="f(x) + \delta f'(x)" /> plus a negative term, and hence it is below the line, contradicting 2.</p>



<p>To show that 2 implies 1, we can again use the contrapositive and show by a “proof by picture” that if there is some point in which <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is above the line between <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> and <img src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(y,f(y))" class="latex" title="(y,f(y))" />, then there must be a point <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in which the tangent line at <img src="https://s0.wp.com/latex.php?latex=%28z%2Cf%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(z,f(z))" class="latex" title="(z,f(z))" /> is above <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/84qFxfN.png" alt="" /></figure>



<p>Some tips on convexity:</p>



<ol><li>The function <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dx%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=x^2" class="latex" title="f(x)=x^2" /> is convex (proof: <a href="https://www.google.com/search?q=plot+x%5E2&amp;oq=plot+x%5E2">Google</a>)</li><li>If <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ek+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R}^k \rightarrow \mathbb{R}" class="latex" title="f:\mathbb{R}^k \rightarrow \mathbb{R}" /> is convex and <img src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L:\mathbb{R}^d \rightarrow \mathbb{R}^k" class="latex" title="L:\mathbb{R}^d \rightarrow \mathbb{R}^k" /> is linear then <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+f%28L%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \mapsto f(L(x))" class="latex" title="x \mapsto f(L(x))" /> is convex (lines are still lines). </li><li>If <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is convex and <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is convex then <img src="https://s0.wp.com/latex.php?latex=a%5Ccdot+f+%2B+b+%5Ccdot+g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a\cdot f + b \cdot g" class="latex" title="a\cdot f + b \cdot g" /> is convex for every positive <img src="https://s0.wp.com/latex.php?latex=a%2Cb&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a,b" class="latex" title="a,b" />.</li></ol>



<h2>Gradient descent</h2>



<p>The gradient descent algorithm minimizes a function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by starting at some point <img src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_0" class="latex" title="x_0" /> and repeating the following operation:</p>



<p><img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+%5Ccdot+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} = x_t - \eta \cdot f'(x_t)" class="latex" title="x_{t+1} = x_t - \eta \cdot f'(x_t)" /></p>



<p>for some small <img src="https://s0.wp.com/latex.php?latex=%5Ceta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta&gt;0" class="latex" title="\eta&gt;0" />.</p>



<p>By Taylor, <img src="https://s0.wp.com/latex.php?latex=f%28x%2B%5Cdelta%29+%5Capprox+f%28x%29+%2B+%5Cdelta+%5Ccdot+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2" class="latex" title="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2" />, and so setting <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%5Ccdot+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta = -\eta \cdot f'(x)" class="latex" title="\delta = -\eta \cdot f'(x)" />, we can see that</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]" class="latex" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]" /></p>



<p>Since <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x_t%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x_t)&gt;0" class="latex" title="f''(x_t)&gt;0" />, we see that as long as <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+2%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta &lt; 2/ f''(x_t)" class="latex" title="\eta &lt; 2/ f''(x_t)" /> we make progress. If we set <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Csim+const+%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta \sim const / f''(x_t)" class="latex" title="\eta \sim const / f''(x_t)" /> then we reduce in each step the value of the function by roughly <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2Ff%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2/f''(x)" class="latex" title="f'(x_t)^2/f''(x)" />.</p>



<p>In the high dimensional case, we replace <img src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x)" class="latex" title="f'(x)" /> with the gradient <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29+%3D+%28+df%28x%29%2Fdx_1+%2C+%5Cldots%2C+df%28x%29%2Fdx_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)" class="latex" title="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)" /> and <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x)" class="latex" title="f''(x)" /> with the Hessian which is the matrix <img src="https://s0.wp.com/latex.php?latex=%28df%28x%29%2Fdx_i+dx_j%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(df(x)/dx_i dx_j)_{i,j}" class="latex" title="(df(x)/dx_i dx_j)_{i,j}" />. The progress we can make is controlled by the ratio of the smallest to largest eigenvalues of the Hessian, which is one over its _condition number_.</p>



<p>In <strong>stochastic gradient descent</strong>, instead of performing the step <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} = x_t - \eta f'(x_t)" class="latex" title="x_{t+1} = x_t - \eta f'(x_t)" /> we use <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+-+%5Ceta+%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} - \eta \hat{f'}(x_t)" class="latex" title="x_{t+1} - \eta \hat{f'}(x_t)" />, where <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{f'}(x_t)" class="latex" title="\hat{f'}(x_t)" /> is a random variable satisfying:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Chat%7Bf%27%7D%28x%29+%3D+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} \hat{f'}(x) = f'(x)" class="latex" title="\mathbb{E} \hat{f'}(x) = f'(x)" /></li><li><img src="https://s0.wp.com/latex.php?latex=Var+%5Chat%7Bf%27%7D%28x%29+%3D+%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Var \hat{f'}(x) = \sigma^2" class="latex" title="Var \hat{f'}(x) = \sigma^2" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma" class="latex" title="\sigma" />.</li></ul>



<p>Let’s define <img src="https://s0.wp.com/latex.php?latex=N_t+%3D+%5Chat%7Bf%27%7D%28x_t%29-+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t = \hat{f'}(x_t)- f'(x_t)" class="latex" title="N_t = \hat{f'}(x_t)- f'(x_t)" />. Then <img src="https://s0.wp.com/latex.php?latex=N_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t" class="latex" title="N_t" /> is a mean zero and variance <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2" class="latex" title="\sigma^2" /> random variable, and let’s heuristically imagine that <img src="https://s0.wp.com/latex.php?latex=N_1%2CN_2%2C%5Cldots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_1,N_2,\ldots" class="latex" title="N_1,N_2,\ldots" /> are independent. If we plug in this into the Taylor approximation, then since <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+N_t+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} N_t = 0" class="latex" title="\mathbb{E} N_t = 0" />, only the terms with <img src="https://s0.wp.com/latex.php?latex=N_t%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t^2" class="latex" title="N_t^2" /> survive.</p>



<p>So by plugging <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%28f%27%28x%29+%2B+N_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta = -\eta (f'(x) + N_t)" class="latex" title="\delta = -\eta (f'(x) + N_t)" /> to the Taylor approximation, we get that in expectation</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2" class="latex" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2" /></p>



<p>We see that now to make progress, we need to ensure that <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> is sufficiently smaller than <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2F%28%5Csigma%5E2+f%27%27%28x_t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2/(\sigma^2 f''(x_t))" class="latex" title="f'(x_t)^2/(\sigma^2 f''(x_t))" />. We note that in the beginning, when <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2" class="latex" title="f'(x_t)^2" /> is large, we can use a larger learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" />, while when we get closer to the optimum, then we need to use a smaller learning rate.</p>



<h2>Generalization bounds</h2>



<p>The <em>supervised learning problem</em> is the task, given labeled training inputs <img src="https://s0.wp.com/latex.php?latex=S+%3D+%7B+%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S = { (x_1,y_1),\ldots,(x_n,y_n) }" class="latex" title="S = { (x_1,y_1),\ldots,(x_n,y_n) }" /> of obtaining a classifier/regressor <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that will satisfy <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%5Capprox+y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x) \approx y" class="latex" title="f(x) \approx y" /> for future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y)" class="latex" title="(x,y)" /> from the same distribution.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZVTtBmW.png" alt="" /></figure>



<p>Let’s assume that our goal is to minimize some quantity <img src="https://s0.wp.com/latex.php?latex=LOSS%28f%29%3D+%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))" class="latex" title="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}" class="latex" title="\mathcal{L}" /> is a <em>loss function</em> (that we will normalize to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[0,1]" class="latex" title="[0,1]" /> for convenience). We call the quantity <img src="https://s0.wp.com/latex.php?latex=LOSS&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="LOSS" class="latex" title="LOSS" /> the population loss (and abuse notation by denoting it as <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f)" class="latex" title="\mathcal{L}(f)" />) and the corresponding quantity over the training set <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D+%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))" class="latex" title="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))" /> the empirical loss.</p>



<p>The <strong>generalization gap</strong> is the difference <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29+-+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))" class="latex" title="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))" /> between the population and empirical losses. (We could add an absolute value though we expect that the loss over the training set would be smaller than the population loss; the population loss can be approximated by the “test loss” and so these terms are sometimes used interchangibly.)</p>



<p><strong>Why care about the generalization gap?</strong> You might argue that we only care about the population loss and not the gap between population and empirical loss. However, as mentioned before, we don’t even care about the population loss but about a more nebulous notion of “real-world performance.” We want the relations between our different abstractions to be as minimally “leaky” as possible and so bound the difference between train and test performance.</p>



<h3>Bias-variance tradeoff</h3>



<p>Suppose that our algorithm performs <em>empirical risk minimization (ERM)</em> which means that on input <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" />, we output <img src="https://s0.wp.com/latex.php?latex=f+%3D+%5Carg%5Cmin_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)" class="latex" title="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)" />. Let’s assume that we have a collection of classifiers <img src="https://s0.wp.com/latex.php?latex=%7B+f_1%2Cf_2%2C%5Cldots+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ f_1,f_2,\ldots }" class="latex" title="{ f_1,f_2,\ldots }" /> and define <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_K+%3D+%7B+f_1%2C%5Cldots%2C+f_K+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}_K = { f_1,\ldots, f_K }" class="latex" title="\mathcal{F}_K = { f_1,\ldots, f_K }" />. For every <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f)" class="latex" title="\hat{\mathcal{L}}_S(f)" /> is an estimator for <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f)" class="latex" title="\mathcal{L}(f)" /> and so we can write <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Cmathcal%7BL%7D%28f%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i" class="latex" title="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i" /> where <img src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_i" class="latex" title="N_i" /> is a random variable with mean zero and variance roughly <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/n" class="latex" title="1/n" /> (because we have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> samples).</p>



<p>The ERM algorithm outputs the <img src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i" class="latex" title="f_i" /> which minimizes <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f_i%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f_i) + N_i" class="latex" title="\mathcal{L}(f_i) + N_i" />. As <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> grows, the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bi+%5Cin+%5BK%5D%7D%5Cmathcal%7BL%7D%28f_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{i \in [K]}\mathcal{L}(f_i)" class="latex" title="\min_{i \in [K]}\mathcal{L}(f_i)" /> (which is known as the <strong>bias</strong> term) shrinks. The quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bi+%5Cin+%5BK%5D%7D+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_{i \in [K]} N_i" class="latex" title="\max_{i \in [K]} N_i" /> (which is known as the <strong>variance</strong> term) grows. When the variance term dominates the bias term, we could potentially start outputting classifiers that don’t perform better on the population. This is known as the “bias-variance tradeoff.”</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/B56EjDS.png" alt="" /></figure>



<h3>Counting generalization gap</h3>



<p>The most basic generalization gap is the following:</p>



<p><strong>Thm (counting gap):</strong> With high probability over <img src="https://s0.wp.com/latex.php?latex=S+%5Cin+%28X%2CY%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S \in (X,Y)^n" class="latex" title="S \in (X,Y)^n" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D%5Cleft%7C+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%5Cright%7C+%5Cleq+O%5Cleft%28+%5Csqrt%7B%5Ctfrac%7B%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D%7Bn%7D%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)" class="latex" title="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)" />.</p>



<p><strong>Proof:</strong> By standard bounds such as Chernoff etc.., the random variable <img src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_i" class="latex" title="N_i" /> behaves like a Normal/Gaussian of mean zero and standard deviation at most <img src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\sqrt{n}" class="latex" title="1/\sqrt{n}" />, which means that the probability that <img src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|N_i| \geq k/\sqrt{n}" class="latex" title="|N_i| \geq k/\sqrt{n}" /> is at most <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c+%5Ccdot+k%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-c \cdot k^2)" class="latex" title="\exp(-c \cdot k^2)" />. If we set <img src="https://s0.wp.com/latex.php?latex=k+%3D+10+%5Csqrt%7B10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%2Fc%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k = 10 \sqrt{10 \log |\mathcal{F}|/c}" class="latex" title="k = 10 \sqrt{10 \log |\mathcal{F}|/c}" /> then for every <img src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i" class="latex" title="f_i" />, <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B+%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D+%5D+%5Cleq+e%5E%7B-ck%5E2%7D+%3D+e%5E%7B-10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D+%3C+%7C%5Cmathcal%7BF%7D%7C%5E%7B-10%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}" class="latex" title="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}" />. Hence by the union bound, the probability that there <em>exists</em> <img src="https://s0.wp.com/latex.php?latex=f_i%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i\in \mathcal{F}" class="latex" title="f_i\in \mathcal{F}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|N_i| \geq k/\sqrt{n}" class="latex" title="|N_i| \geq k/\sqrt{n}" /> is at most <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BF%7D%7C%2F%7C%5Cmathcal%7BF%7D%7C%5E%7B10%7D+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0" class="latex" title="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0" />. QED</p>



<h3>Other generalization bounds</h3>



<p>One way to count the number of classifiers in a family <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> is by the bits to represent a member of the family– there are at most <img src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^k" class="latex" title="2^k" /> functions that can be represented using <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> bits. But this bound can be quite loose – for example, it can make a big difference if we use <img src="https://s0.wp.com/latex.php?latex=32&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="32" class="latex" title="32" /> or <img src="https://s0.wp.com/latex.php?latex=64&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="64" class="latex" title="64" /> bits to specify numbers, and some natural families (e.g., linear functions) are <em>infinite</em>. There are many bounds in the literature of the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BGeneralization+gap%7D+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Cfrac%7Bd%7D%7Bn%7D%7D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)" class="latex" title="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)" /></p>



<p>with values of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> other than <img src="https://s0.wp.com/latex.php?latex=%5Clog+%7C%5Cmathcal%7BF%7D%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log |\mathcal{F}|" class="latex" title="\log |\mathcal{F}|" />.<br />Intuitively <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> corresponds to the “capacity” of the classifier family/algorithm – the number of samples it can fit/memorize. Some examples (very roughly stated) include:</p>



<ul><li><strong>VC dimension:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the maximum number such that for every set of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> points and labels, there is a classifier in the family that fits the points to the labels. That is for every <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_d" class="latex" title="x_1,\ldots,x_d" /> and <img src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_1,\ldots,y_d" class="latex" title="y_1,\ldots,y_d" /> there is <img src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in\mathcal{F}" class="latex" title="f\in\mathcal{F}" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cforall_i+f%28x_i%29%3Dy_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall_i f(x_i)=y_i" class="latex" title="\forall_i f(x_i)=y_i" />.</li><li><strong>Rademacher Complexity:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the maximum number such that for random <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_d" class="latex" title="x_1,\ldots,x_d" /> from <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> and <img src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_1,\ldots,y_d" class="latex" title="y_1,\ldots,y_d" /> uniform (assume say over <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }" class="latex" title="{ \pm 1 }" />) with high probability there exists <img src="https://s0.wp.com/latex.php?latex=f%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in \mathcal{F}" class="latex" title="f\in \mathcal{F}" /> with <img src="https://s0.wp.com/latex.php?latex=f%28x_i%29%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_i)\approx y_i" class="latex" title="f(x_i)\approx y_i" /> for most <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />.</li><li><strong>PAC Bayes:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the mutual information between the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" /> that the learning algorithm is given as input and the classifier that it outputs. This requires some conditions on the learning algorithm and some prior distribution on the classifier. To get bounds on this quantity when the weights are continuous, we can add <em>noise</em> to them.</li><li><strong>Margin bounds:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the “effective dimensionality” as measured by some margin. For example, for random unit vectors <img src="https://s0.wp.com/latex.php?latex=w%2Cx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w,x" class="latex" title="w,x" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" title="\mathbb{R}^d" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Clangle+w%2Cx+%5Crangle%7C+%5Csim+1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\langle w,x \rangle| \sim 1/\sqrt{d}" class="latex" title="|\langle w,x \rangle| \sim 1/\sqrt{d}" />. For linear classifiers, the margin bound is the minimum <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> such that correct labels over the training set are classified with at least <img src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\sqrt{d}" class="latex" title="1/\sqrt{d}" /> margin.</li></ul>



<p>A recent empirical study of generalization bounds is <a href="https://arxiv.org/abs/1912.02178">“fantastic generalization measures and where to find them”</a>by Jiang, Neyshabur, Mobahi, Krishnan, and Bengio, and <a href="https://arxiv.org/abs/2010.11924">“In Search of Robust Measures of Generalization”</a> by Dziugaite,  Drouin, Neal, Rajkumar, Caballero, Wang, Mitliagkas, and Roy.</p>



<h2>Limitations of generalization bounds</h2>



<p>The generalization gap <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bf%3DA%28S%29%7D%5Cleft%5B+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D%28f%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]" class="latex" title="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]" /> depends on several quantities:</p>



<ul><li>The family <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> of functions.</li><li>The algorithm <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> used to map the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" /> to <img src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in\mathcal{F}" class="latex" title="f\in\mathcal{F}" />.</li><li>The distribution <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> of datapoints</li><li>The distribution <img src="https://s0.wp.com/latex.php?latex=Y%7CX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y|X" class="latex" title="Y|X" /> of labels.</li></ul>



<p>A <strong>generalization bound</strong> is an upper bound on the gap that only depends on some of these quantities. In an influential paper, <a href="https://arxiv.org/abs/1611.03530">Zhang, Bengio, Hardt, Recht, Vinyals</a> showed significant barriers to obtaining such results that are meaningful for practical deep networks. They showed that in many natural settings, we cannot get such bounds even if we allow them to be based arbitrarily on the first three factors. That is, they showed that for natural families of functions (modern deep nets), natural algorithms (gradient descent on the empirical loss), natural distributions (CIFAR 10 and ImageNet), if we replace <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" title="Y" /> by the uniform distribution, then we can get arbitrarily large generalization gap.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/QqP2qnc.png" alt="" /></figure>



<p>We can also interpolate between the Zhang et al. experiment and the plain CIFAR-10 distribution. If we consider a distribution <img src="https://s0.wp.com/latex.php?latex=%28X%2C%5Ctilde%7BY%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X,\tilde{Y})" class="latex" title="(X,\tilde{Y})" /> where we take <img src="https://s0.wp.com/latex.php?latex=%28X%2CY%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X,Y)" class="latex" title="(X,Y)" /> from CIFAR-10 with probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> we replace the <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" title="Y" /> with a random label (one of the 10 CIFAR-10 classes) then the test/population performance (fraction of correct classifications) will be at most <img src="https://s0.wp.com/latex.php?latex=%281-p%29+%2B+p%2F10&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(1-p) + p/10" class="latex" title="(1-p) + p/10" /> (not surprising), but the training/empirical accuracy will remain at roughly 100%. The left-hand side of the gif below demonstrates this (this comes from <a href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">this paper with Bansal and Kaplun</a> which shows that, as the right side demonstrates, certain self-supervised learning algorithms do not suffer from this phenomenon; here the noise level is the fraction of wrong labels so <img src="https://s0.wp.com/latex.php?latex=0.9&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0.9" class="latex" title="0.9" /> is perfect noise):</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif" alt="" /></figure>



<h2>“Double descent.”</h2>



<p>While classical learning theory predicts a “bias-variance tradeoff” whereby as we increase the model class size, we get worse and worse performance, this is not what happens in modern deep learning systems. <a href="https://arxiv.org/abs/1812.11118">Belkin, Hsu, Ma, and Mandal</a> posited that such systems undergo a “double descent” whereby performance behaves according to the classical bias/variance curve up to the point in which we achieve <img src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 0" class="latex" title="\approx 0" /> training error and then starts improving again. This <a href="https://windowsontheory.org/2019/12/05/deep-double-descent/">actually happens</a> in real deep networks.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/PpyW6HW.png" alt="" /></figure>



<p>To get some intuition for the double descent phenomenon, consider the case of fitting a univariate polynomial of degree <img src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d^0" class="latex" title="d^0" /> to <img src="https://s0.wp.com/latex.php?latex=n+%3E+d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n &gt; d^0" class="latex" title="n &gt; d^0" /> samples of the form <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%2Bnoise%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x)+noise)" class="latex" title="(x,f(x)+noise)" /> where <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is a degree <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> polynomial. When <img src="https://s0.wp.com/latex.php?latex=d%3Cd%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d&lt;d^0" class="latex" title="d&lt;d^0" /> we are “under-fitting” and will not get good performance. As <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> trends between <img src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d^0" class="latex" title="d^0" /> and <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, we fit more and more of the noise, until for <img src="https://s0.wp.com/latex.php?latex=d%3Dn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=n" class="latex" title="d=n" /> we have a perfect interpolating polynomial that will have perfect train but very poor test performance. When <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> grows beyond <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, more than one polynomial can fit the data, and (under certain conditions) SGD will select the minimal norm one, which will make the interpolation smoother and smoother and actually result in better performance.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZhPtS0Y.gif" alt="" /></figure>



<h2>Approximation and representation</h2>



<p>Consider the task of distinguishing between the speech of an adult and a child. In the time domain, this may be hard, but by switching to representation in the Fourier domain, the task becomes much easier. (See this cartoon)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8SKSB9q.png" alt="" /></figure>



<p>The Fourier transform is based on the following theorem: for every continuous <img src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:[0,1] \rightarrow \mathbb{R}" class="latex" title="f:[0,1] \rightarrow \mathbb{R}" />, we can arbitrarily well approximate <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> as a linear combination of functions of the form <img src="https://s0.wp.com/latex.php?latex=e%5E%7B2%5Cpi+i+%5Calpha+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{2\pi i \alpha x}" class="latex" title="e^{2\pi i \alpha x}" />. Another way to say it is that if we use the embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N" class="latex" title="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N" /> which maps <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into (sufficiently large) <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> coordinates of the form <img src="https://s0.wp.com/latex.php?latex=e%5E%7B2+%5Cpi+i+%5Calpha_j+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{2 \pi i \alpha_j x}" class="latex" title="e^{2 \pi i \alpha_j x}" /> then <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> becomes linear.</p>



<p>The wave functions are not the only ones that can approximate an arbitrary function. A <em>ReLU</em> is a function <img src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="r:\mathbb{R}^d \rightarrow \mathbb{R}" /> of the form <img src="https://s0.wp.com/latex.php?latex=r%28x%29+%3D+max+%5C%7B+w+%5Ccdot+x+%2B+b+%2C+0%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x) = max \{ w \cdot x + b , 0\}" class="latex" title="r(x) = max \{ w \cdot x + b , 0\}" />. We can approximate every continuous function arbitrarily well as a combination of ReLUs:</p>



<p><strong>Theorem:</strong> For every continous <img src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:[0,1]^d \rightarrow \mathbb{R}" class="latex" title="f:[0,1]^d \rightarrow \mathbb{R}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" /> there is a function <img src="https://s0.wp.com/latex.php?latex=g%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g:[0,1]^d \rightarrow \mathbb{R}" class="latex" title="g:[0,1]^d \rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is a linear combination of ReLUs and <img src="https://s0.wp.com/latex.php?latex=%5Cint+%7Cf-g%7C+%3C+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int |f-g| &lt; \epsilon" class="latex" title="\int |f-g| &lt; \epsilon" />.</p>



<p>In one dimension, this follows from the facts that:</p>



<ul><li>ReLUs can give an arbitrarily good approximation to bump functions of the form<br /><img src="https://s0.wp.com/latex.php?latex=I_%7Ba%2Cb%7D%28x%29+%3D+%5Cbegin%7Bcases%7D+1+%26+a+%5Cleq+x+%5Cleq+b+%5C%5C+0+%26+%5Ctext%7Botherwise%7D%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}" class="latex" title="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}" /></li><li>Every continuous function on a bounded domain can be arbitrarily well approximated by the sum of bump functions.</li></ul>



<p>The second fact is well known, and here is a “proof by picture” for the first one:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/sKhHEap.png" alt="" /></figure>



<p>For higher dimensions, we need to create higher dimension bump functions. For example, in two dimensions, we can create a “noisy circle” by summing over all rotations of our bump. We can then add many such circles to create a two-dimensional bump. The same construction extends to an arbitrary number of dimensions.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/V8Wm5X1.png" alt="" /></figure>



<p><strong>How many ReLUs?</strong> The above shows that a linear combination of ReLUs can approximate every function on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> variables, but how many ReLUs are needed? Every ReLU <img src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="r:\mathbb{R}^d \rightarrow \mathbb{R}" /> is specified by <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d+1" class="latex" title="d+1" /> numbers for the weights and bias. Intuitively, we could discretize each coordinate to a constant number of choices, and so there would be <img src="https://s0.wp.com/latex.php?latex=O%281%29%5Ed+%3D+2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(1)^d = 2^{O(d)}" class="latex" title="O(1)^d = 2^{O(d)}" /> choices for such ReLUs. Indeed, it can be shown that every continuous function can be approximated by a linear combination of <img src="https://s0.wp.com/latex.php?latex=2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^{O(d)}" class="latex" title="2^{O(d)}" /> ReLUs. It turns out that some functions <em>require</em> an exponential number of ReLUS.</p>



<p>The above discussion doesn’t apply just for ReLUs but virtually any non-linear function.</p>



<h3>Representation summary</h3>



<p>By embedding our input <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in \mathbb{R}^d" class="latex" title="x\in \mathbb{R}^d" /> as a vector <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x) \in \mathbb{R}^N" class="latex" title="\varphi(x) \in \mathbb{R}^N" />, we can often make many “interesting” functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> become much simpler to compute (e.g., linear). In learning, we typically search for an <em>embedding</em> or <em>representation</em> that is “good” in one or more of the following senses:</p>



<ul><li>The dimension of embedding <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> is not too large for many “interesting” functions.</li><li>Two inputs <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,y" class="latex" title="x,y" /> are “semantically similar” if and only if <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(y)" class="latex" title="\varphi(y)" /> are correlated (e.g., <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x),\varphi(y) \rangle" class="latex" title="\langle \varphi(x),\varphi(y) \rangle" /> is large).</li><li>We can efficiently compute <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> and (sometimes) can compute <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C+%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x), \varphi(y) \rangle" class="latex" title="\langle \varphi(x), \varphi(y) \rangle" /> without needing to explicitly compute <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29%2C%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x),\varphi(y)" class="latex" title="\varphi(x),\varphi(y)" />.</li><li>For “interesting” functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> can be approximated by a linear function in the embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> with “structured” coefficients (for example, sparse combination, or combination of coefficients of certain types, such as low frequency coefficients in Fourier domain)</li><li>…</li></ul>



<h2>Kernels and nearest neighbors</h2>



<p>Suppose that we have some notion <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> of “similarity” between inputs, where <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,y)" class="latex" title="K(x,y)" /> being large means that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is “close” to <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> and <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,y)" class="latex" title="K(x,y)" /> being small means that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is “far” from <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />.</p>



<p>This suggests that we can use one of the following methods approximating a function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> given inputs of the form <img src="https://s0.wp.com/latex.php?latex=%7B%28x_i%2C+y_i+%5Capprox+f%28x_i%29%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{(x_i, y_i \approx f(x_i))}" class="latex" title="{(x_i, y_i \approx f(x_i))}" />. On input <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, any of the following can be reasonable approximations to <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> depending on context:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> where <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> is the closest to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> in <img src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_1,\ldots, x_n }" class="latex" title="{ x_1,\ldots, x_n }" />. (This is known as the <em>nearest neighbor</em> algorithm.)</li><li>The mean (or other combining function) of <img src="https://s0.wp.com/latex.php?latex=y_%7Bi_1%7D%2C%5Cldots%2C+y_%7Bi_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_{i_1},\ldots, y_{i_k}" class="latex" title="y_{i_1},\ldots, y_{i_k}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B+x_%7Bi_1%7D%2C%5Cldots%2C+x_%7Bi_k%7D+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_{i_1},\ldots, x_{i_k} }" class="latex" title="{ x_{i_1},\ldots, x_{i_k} }" /> are the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> nearest inputs to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. (This is known as the <em><img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> nearest neighbor</em> algorithm.)</li><li>Some linear combination of <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> where the coefficients depend on <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cx_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,x_i)" class="latex" title="K(x,x_i)" />. (This is known as the <em>kernel</em> algorithm.)</li></ul>



<p>All of these algorithms are _non-parametric methods_ in the sense that the final regressor/classifier is specified by the full training set <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i) }_{i=1..n}" class="latex" title="{ (x_i,y_i) }_{i=1..n}" />.</p>



<p><strong>Kernel algorithms</strong> can also be described as follows. Given some embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N" class="latex" title="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{X}" class="latex" title="\mathcal{X}" /> is our input space, a Kernel regression approximates a function <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathcal{X} \rightarrow \mathbb{R}" class="latex" title="f:\mathcal{X} \rightarrow \mathbb{R}" /> by a linear function in <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" />.</p>



<p>The key observation is that to solve linear equations or least-square minimization in <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w \in \mathbb{R}^N" class="latex" title="w \in \mathbb{R}^N" /> of the form <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29+%2C+w+%5Crangle+%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x_i) , w \rangle \approx y_i" class="latex" title="\langle \varphi(x_i) , w \rangle \approx y_i" />, we don’t need to know the vectors <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x_i)" class="latex" title="\varphi(x_i)" />. Rather, it is enough to know the inner products <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29%2C+%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x_i), \varphi(x_j) \rangle" class="latex" title="\langle \varphi(x_i), \varphi(x_j) \rangle" />. In Kernel methods we are often not given the embedding explicitly (indeed <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> might even be infinite) but rather the function <img src="https://s0.wp.com/latex.php?latex=K%3A%5Cmathcal%7BX%7D+%5Ctimes+%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}" class="latex" title="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=K%28x_i%2Cx_j%29+%3D+%5Clangle+%5Cvarphi%28x_i%29%2C%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle" class="latex" title="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle" />. The only thing to verify is that <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> actually defines an inner product by checking that the matrix <img src="https://s0.wp.com/latex.php?latex=%28+K%28x_i%2Cx_j%29%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="( K(x_i,x_j))_{i,j}" class="latex" title="( K(x_i,x_j))_{i,j}" /> is positive semi-definite.</p>



<p>In general, Kernels and neural networks look quite similar – both ultimately involve composing a linear function <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> on top of a non-linear embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" />. It is not always clear cut whether an algorithm is a kernel or deep neural net method. Some characteristics of kernels are:</p>



<ul><li>The embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" /> is not learned from the data. However, if <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" /> was learned from some other data, or was inspired by representations that were learned from data, then it becomes a fuzzier distinction.</li><li>There is a “shortcut” to compute the inner product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x),\varphi(x') \rangle" class="latex" title="\langle \varphi(x),\varphi(x') \rangle" /> using significantly smaller than <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> steps.</li></ul>



<p>Generally, the distinction between a kernel and deep nets depends on the application (is it to apply some analysis such as generalization bounds for kernels? is it to use kernel methods with shortcuts for the inner product?) and is more a spectrum than a binary partition.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/jeHi32z.png" alt="" /></figure>



<h2>Conclusion</h2>



<p>The above was a very condensed and rough survey of generalization, representation, approximation, and kernel methods. All of these are covered much better in the understanding machine learning book and the upcoming Hardt and Recht book.</p>



<p>In the next lecture, we will discuss the algorithmic bias of gradient descent, including the cases of linear regression and deep linear networks. We will discuss the “simplicity bias” of SGD and what can we say about what is learned at different layers of a deep network.</p>



<p><strong>Acknowledgements:</strong>  Thanks to Manos Theodosis and Preetum Nakkiran for pointing out several typos in a previous version.</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/"><span class="datestr">at January 31, 2021 07:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/01/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/01/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-probe-unsolved-hilbert-polynomial-problem-20210114/">Hilbert’s 13th, unsolved</a> (<a href="https://mathstodon.xyz/@11011110/105569819827303922">\(\mathbb{M}\)</a>). You can solve polynomials of degree at most four using one-argument algebraic functions like \(\sqrt x\). If \(RD(n)\) denotes the number of arguments needed for degree-\(n\) polynomials, <span style="white-space: nowrap;">then \(RD(4)=1\).</span> Hilbert asked whether \(RD(7)=2\). Vladimir Arnold showed in the 1950s that you can solve all polynomials with two-variable continuous (but not algebraic) functions, but mathematicians are only now catching on that the algebraic problem is still  open. See also <a href="https://arxiv.org/abs/2001.06515">some recent bounds on \(RD\)</a>.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/joshmillard/status/1349979253937381379">Fogleworms</a> (<a href="https://mastodon.social/@joshmillard/105572806531271932">\(\mathbb{M}\)</a>), partitions of \(n\times n\) grids into \(n\)-vertex grid paths, their enumeration, and a crafty project to visualize them.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/s00283-020-10034-w">A figure with Heesch number 6: Pushing a two-decade-old boundary</a> (<a href="https://mathstodon.xyz/@11011110/105579308071467320">\(\mathbb{M}\)</a>), Bojan Bašić in the <em>Mathematical Intelligencer</em>. When a shape cannot tile the plane, its <a href="https://en.wikipedia.org/wiki/Heesch%27s_problem">Heesch number</a> measures how far you can tile before getting stuck: if you surround the shape by layers of the same shape, how many layers can you make? Casey Mann’s previous record of five looked like a row of five hexagons with extra crenellations. This one uses six hexagons, with simpler crenellations.</p>
  </li>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/computation-in-service-of-poetry/">Exponentiation by squaring, in somewhat cryptic form, in a work by Pingala from India from over 2000 years ago</a> (<a href="https://mathstodon.xyz/@11011110/105586527250804920">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>I’ve been trying to understand the structure of the Kurilpa Bridge in Brisbane, supposedly “the world’s largest tensegrity bridge” (<a href="https://mathstodon.xyz/@11011110/105592704371209563">\(\mathbb{M}\)</a>). The clearest description I’ve found is from <a href="http://tadashidesign.com/kurilpa-bridge">Tadashi Design</a>. <a href="https://en.wikipedia.org/wiki/Kurilpa_Bridge">Wikipedia</a> is more cagy, calling it a “hybrid tensegrity bridge”, as it also includes features of cable-stayed bridges where the deck hangs from cables attached to tower piers. But the Tadashi Design site shows long sections far from the piers, so it seems the tensegrity is not just for show.</p>
  </li>
  <li>
    <p><a href="https://www.nationalgeographic.com/science/2021/01/we-need-better-face-masks-and-origami-might-help/">How origami folding patterns might help in the design of better face masks</a> (<a href="https://mathstodon.xyz/@11011110/105595412289181842">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>More US universities using covid as an excuse to treat faculty badly (<a href="https://mathstodon.xyz/@11011110/105609583164670586">\(\mathbb{M}\)</a>): <a href="https://www.chronicle.com/article/kansas-regents-allow-sped-up-dismissals-of-tenured-faculty-members">the Kansas state university system guts tenure</a>, and <a href="https://www.insidehighered.com/news/2021/01/21/u-florida-asks-students-report-professors-who-arent-teaching-person">the University of Florida asks students to snitch on faculty who refuse to endanger themselves by teaching in person</a>.</p>
  </li>
  <li>
    <p><a href="https://www.departures.com/lifestyle/architecture/hayri-atak-design-sarcostyle-building-manhattan-skyline">Proposed New York waterfront tower is a handlebody of high genus</a> (<a href="https://mathstodon.xyz/@11011110/105612316794720508">\(\mathbb{M}\)</a>, <a href="https://mastodon.social/@sarielhp/105612251833052747">via</a>, <a href="https://twitter.com/MathematicsUCL/status/1353328317781467137">via2</a>).</p>
  </li>
  <li>
    <p>Although it also <a href="https://en.wikipedia.org/wiki/Book_(graph_theory)">has other names</a>, the graph \(K_{1,1,n}\)  has been called the “thagomizer graph”, and its associated graphic matroid has been called the “thagomizer matroid” (<a href="https://mathstodon.xyz/@11011110/105620864011377814">\(\mathbb{M}\)</a>). The term appears to have been introduced by Katie Gedeon in  <a href="https://arxiv.org/abs/1610.05349">arXiv:1610.05349</a> in honor of the famous Far Side cartoon, whose terminology has <a href="https://en.wikipedia.org/wiki/Thagomizer">also been adopted by some paleontologists</a>.</p>
  </li>
  <li>
    <p>This new preprint looks interesting: <a href="https://arxiv.org/abs/2101.09592">Point-hyperplane incidence geometry and the log-rank conjecture, Noah Singer and Madhu Sudan, arXiv:2101.09592</a> (<a href="https://mathstodon.xyz/@11011110/105626816229116967">\(\mathbb{M}\)</a>). In the plane, \(n\) points and \(m\) lines can only touch \(\Theta\bigl((mn)^{2/3}+m+n\bigr)\) times. In 3d, points and planes can have mn incidences but only by sharing a common line. This paper connects similar problems in high dimensions to the <a href="https://en.wikipedia.org/wiki/Log-rank_conjecture">log-rank conjecture</a>, a famous unsolved problem in communication complexity.</p>
  </li>
  <li>
    <p><a href="https://www.math.ucdavis.edu/research/seminars/?talk_id=6082">Unknot recognition in quasi-polynomial time</a> (<a href="https://mathstodon.xyz/@11011110/105630455655140054">\(\mathbb{M}\)</a>, <a href="https://www.scottaaronson.com/blog/?p=5270">via</a>). Title of talk announcement by Marc Lackenby. No details or preprint yet but judging solely from the title and non-fringe status of the author this sounds like big news.</p>
  </li>
  <li>
    <p><a href="https://mathcs.clarku.edu/~fgreen/bookreviews/51-4.pdf">Frederic Green has published another review of my book “Forbidden Configurations in Discrete Geometry” in the latest <em>SIGACT News</em></a> (<a href="https://mathstodon.xyz/@11011110/105636589457644955">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.1145/3444815.3444817">official but paywalled url</a>). Thanks to Joe O’Rourke for the heads-up: I last checked my mail at the office, where my physical copies of <em>SIGACT News</em> would go if they went anywhere, months ago, and even then it looked like magazines weren’t getting through.</p>
  </li>
  <li>
    <p>Mathematics on the cutting block at Leicester again: <a href="https://gowers.wordpress.com/2021/01/30/leicester-mathematics-under-threat-again/">Gowers</a>, <a href="https://golem.ph.utexas.edu/category/2021/01/problems_at_the_university_of.html">nCat</a>, <a href="https://www.ipetitions.com/petition/mathematics-is-not-redundant">petition</a> (<a href="https://mathstodon.xyz/@11011110/105646674167288349">\(\mathbb{M}\)</a>). The plan is to eliminate research in pure mathematics at the University of Leicester, fire eight professors, and hire three back in purely teaching positions. I’m not sure who the eight are – the <a href="https://le.ac.uk/mathematics/people/academic-and-research">staff list</a> includes some other disciplines – but Leicester mathematicians in Wikipedia include <a href="https://en.wikipedia.org/wiki/Katrin_Leschke">Katrin Leschke</a> and <a href="https://en.wikipedia.org/wiki/Sergei_Petrovskii">Sergei Petrovskii</a>.</p>
  </li>
  <li>
    <p>Two newly-listed Good Articles on Wikipedia: <a href="https://en.wikipedia.org/wiki/Curve_of_constant_width">Curve of constant width</a> and <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ronald Graham</a> (<a href="https://mathstodon.xyz/@11011110/105652461392545754">\(\mathbb{M}\)</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/01/31/linkage.html"><span class="datestr">at January 31, 2021 03:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/008">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/008">TR21-008 |  Random walks and forbidden minors III: poly(d/?)-time partition oracles for minor-free graph classes | 

	Akash Kumar, 

	C. Seshadhri, 

	Andrew Stolman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the family of bounded degree graphs in any minor-closed family (such as planar graphs). Let d be the degree bound and n be the number of vertices of such a graph. Graphs in these classes have hyperfinite decompositions, where, for a sufficiently small ? &gt; 0, one removes
?dn edges to get connected components of size independent of n. An important tool for sublinear
algorithms and property testing for such classes is the partition oracle, introduced by the seminal
work of Hassidim-Kelner-Nguyen-Onak (FOCS 2009). A partition oracle is a local procedure
that gives consistent access to a hyperfinite decomposition, without any preprocessing. Given a
query vertex v, the partition oracle outputs the component containing v in time independent of
n. All the answers are consistent with a single hyperfinite decomposition.
The partition oracle of Hassidim et al. runs in time d^poly(d/?)-per query. They pose the
open problem of whether poly(d/?)-time partition oracles exist. Levi-Ron (ICALP 2013) give
a refinement of the previous approach, to get a partition oracle that runs in time d^log(d/?)-per
query.
In this paper, we resolve this open problem and give poly(d/?)-time partition oracles for
bounded degree graphs in any minor-closed family. Unlike the previous line of work based on
combinatorial methods, we employ techniques from spectral graph theory. We build on a recent
spectral graph theoretical toolkit for minor-closed graph families, introduced by the authors to
develop efficient property testers. A consequence of our result is a poly(d/?)-query tester for
any property of minor-closed families (such as bipartite planar graphs). Our result also gives
poly(d/?)-query algorithms for additive ?n-approximations for problems such as maximum
matching, minimum vertex cover, maximum independent set, and minimum dominating set for
these graph families.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/008"><span class="datestr">at January 30, 2021 11:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5253">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5253">Once we can see them, it’s too late</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><em>[updates: <a href="https://arxiv.org/abs/2102.01522">here’s the paper</a>, and <a href="https://www.scottaaronson.com/blog/?p=5253#comment-1877426">here’s Robin’s brief response</a> to some of the comments here]</em></p>



<p>This month <a href="http://mason.gmu.edu/~rhanson/home.html">Robin Hanson</a>, the famous and <a href="https://www.scottaaronson.com/blog/?p=3766">controversy-prone</a> George Mason University economics professor who I’ve known since 2004, was visiting economists here in Austin for a few weeks.  So, while my fear of covid considerably exceeds Robin’s, I met with him a few times in the mild Texas winter in an outdoor, socially-distanced way.  It took only a few minutes for me to remember why I enjoy talking to Robin so much.</p>



<p>See, while I’d been moping around depressed about covid, the vaccine rollout, the insurrection, my inability to focus on work, and a dozen other things, Robin was bubbling with excitement about a brand-new mathematical model he was working on to understand the growth of civilizations across the universe—a model that, Robin said, explained lots of cosmic mysteries in one fell swoop and also made striking predictions.  My cloth facemask was, I confess, unable to protect me from Robin’s infectious enthusiasm.</p>



<p>As I listened, I went through the classic stages of reaction to a new Hansonian proposal: first, bemusement over the sheer weirdness of what I was being asked to entertain, as well as Robin’s failure to acknowledge that weirdness in any way whatsoever; then, confusion about the unstated steps in his radically-condensed logic; next, the raising by me of numerous objections (each of which, it turned out, Robin had already thought through at length); finally, the feeling that I <em>must have</em> seen it this way all along, because isn’t it kind of obvious?</p>



<p>Robin has been explaining his model in a <a href="https://www.overcomingbias.com/2020/12/how-far-aggressive-aliens.html">sequence</a> <a href="https://www.overcomingbias.com/2020/12/how-far-aggressive-aliens-part-2.html">of</a> <em><a href="https://www.overcomingbias.com/2020/12/the-long-term-future-of-history.html">Overcoming</a> <a href="https://www.overcomingbias.com/2021/01/try-menu-combo-filter-steps.html">Bias</a></em> <a href="https://www.overcomingbias.com/2021/01/why-we-cant-see-grabby-aliens.html">posts</a>, and <s>will apparently have a paper out about the model soon</s> <a href="https://arxiv.org/abs/2102.01522">the paper is here!</a>  In this post, I’d like to offer my own take on what Robin taught me.  Blame for anything I mangle lies with me alone.</p>



<p>To cut to the chase, Robin is trying to explain the famous <a href="https://en.wikipedia.org/wiki/Fermi_paradox">Fermi Paradox</a>: why, after 60+ years of looking, and despite the periodic excitement around <a href="https://en.wikipedia.org/wiki/Tabby%27s_Star">Tabby’s star</a> and <a href="https://en.wikipedia.org/wiki/%CA%BBOumuamua">‘Oumuamua</a> and the like, have we not seen a single undisputed sign of an extraterrestrial civilization?  Why all this nothing, even though the observable universe is vast, even though (as we now know) organic molecules and planets in Goldilocks zones are everywhere, and even though there have been billions of years for aliens someplace to get a technological head start on us, expanding across a galaxy to the point where they’re easily seen?</p>



<p>Traditional answers to this mystery include: maybe the extraterrestrials quickly annihilate themselves in nuclear wars or environmental cataclysms, just like we soon will; maybe the extraterrestrials don’t <em>want</em> to be found (whether out of self-defense or a cosmic Prime Directive); maybe they spend all their time playing video games.  Crucially, though, all answers of that sort founder against the realization that, given a million alien civilizations, each perhaps more different from the others than kangaroos are from squid, <em>it would only take one</em>, spreading across a billion light-years and transforming everything to its liking, for us to have noticed it.</p>



<p>Robin’s answer to the puzzle is as simple as it is terrifying.  Such civilizations might well exist, he says, but if so, by the time we noticed one, it would already be nearly too late.  Robin proposes, plausibly I think, that if you give a technological civilization 10 million or so years—i.e., an eyeblink on cosmological timescales—then <em>either</em></p>



<ol><li>the civilization wipes itself out, <em>or else</em></li><li>it reaches some relatively quiet steady state, <em>or else</em></li><li>if it’s serious about spreading widely, then it “maxes out” the technology with which to do so, approaching the limits set by physical law.</li></ol>



<p>In cases 1 or 2, the civilization will of course be hard for us to detect, unless it happens to be close by.  But what about case 3?  There, Robin says, the “civilization” should look from the outside like a sphere expanding at nearly the speed of light, transforming everything in its path.</p>



<p>Now think about it: when could we, on earth, detect such a sphere with our telescopes?  Only when the sphere’s thin outer shell had reached the earth—perhaps carrying radio signals from the extraterrestrials’ early history, <em>before</em> their rapid expansion started.  By that point, though, the expanding sphere itself would be nearly upon us!</p>



<p>What would happen to us once we were inside the sphere?  Who knows?  The expanding civilization might obliterate us, it might preserve us as zoo animals, it might merge us into its hive-mind, it might do something else that we can’t imagine, but in any case, <em>detecting</em> the civilization would presumably no longer be the relevant concern!</p>



<p>(Of course, one could also wonder what happens when two of these spheres collide: do they fight it out?  do they reach some agreement?  do they merge?  Whatever the answer, though, it doesn’t matter for Robin’s argument.)</p>



<p>On the view described, there’s only a tiny cosmic window in which a SETI program could be expected to succeed: namely, when the thin surface of the first of these expanding bubbles has just hit us, and when that surface hasn’t yet passed us by.  So, given our “selection bias”—meaning, the fact that we apparently <em>haven’t</em> yet been swallowed up by one of the bubbles—it’s no surprise if we don’t right now happen to find ourselves in the tiny detection window!</p>



<p>This basic proposal, it turns out, is not original to Robin.  Indeed, an <em>Overcoming Bias</em> reader named Daniel X. Varga <a href="http://disq.us/p/2eczhce">pointed out to Robin</a> that he (Daniel) <a href="https://www.scottaaronson.com/blog/?p=334#comment-10766">shared the same idea right here</a>—in a <em>Shtetl-Optimized</em> comment thread—back in 2008!  I must have read Daniel Varga’s comment then, but (embarrassingly) it didn’t make enough of an impression for me to have remembered it.  I probably thought the same as <em>you</em> probably thought while reading this post:</p>



<blockquote class="wp-block-quote"><p>“Sure, whatever.  This is an amusing speculation that could make for a fun science-fiction story.  Alas, like with virtually <em>every</em> story about extraterrestrials, there’s no good reason to favor this over a hundred other stories that a fertile imagination could just as easily spin.  Who the hell knows?”</p></blockquote>



<p>This is where Robin claims to take things further.  Robin would say that he takes them further by developing a mathematical model, and fitting the parameters of the model to the known facts of cosmic history.  Read <a href="https://www.overcomingbias.com/">Overcoming Bias</a>, or Robin’s forthcoming paper, if you want to know the details of his model.  Personally, I confess I’m less interested in those details than I am in the qualitative points, which (unless I’m mistaken) are easy enough to explain in words.</p>



<p>The key realization is this: when we contemplate the Fermi Paradox, we know more than the mere fact that we look and look and we don’t see any aliens.  There are other relevant data points to fit, having to do with the one sample of a technological civilization that we <em>do</em> have.</p>



<p>For starters, there’s the fact that life on earth has been evolving for at least ~3.5 billion years—for most of the time the earth has existed—<em>but</em> life has a mere billion more years to go, until the expanding sun boils away the oceans and makes the earth barely habitable.  In other words, at least on <em>this</em> planet, we’re already relatively close to the end.  Why should that be?</p>



<p>It’s an excellent fit, Robin says, to a model wherein there are a few incredibly difficult, improbable steps along the way to a technological civilization like ours—steps that might include the origin of life, of multicellular life, of consciousness, of language, of something else—and wherein, having achieved some step, evolution basically just does a random search until it either stumbles onto the next step or else runs out of time.</p>



<p>Of course, given that we’re here to talk about it, we necessarily find ourselves on a planet where all the steps necessary for blog-capable life happen to have succeeded.  There might be vastly more planets where evolution got stuck on some earlier step.</p>



<p>But here’s the interesting part: conditioned on all the steps having succeeded, we <em>should</em> find ourselves near the end of the useful lifetime of our planet’s star—simply because the more time is available on a given planet, the better the odds there.  I.e., look around the universe and you should find that, on <em>most</em> of the planets where evolution achieves all the steps, it nearly runs out the planet’s clock in doing so.  Also, as we look back, we should find the hard steps roughly evenly spaced out, with each one having taken a good fraction of the whole available time.  All this is an excellent match for what we see.</p>



<p>OK, but it leads to a second puzzle.  Life on earth is at least ~3.5 billion years old, while the observable universe is ~13.7 billion years old.  Forget for a moment about the oft-stressed enormity of these two timescales and concentrate on their <em>ratio</em>, which is merely ~4. <em> </em><strong>Life on earth stretches a full quarter of the way back in time to the Big Bang.</strong>  Even as an adolescent, I remember finding that striking, and not at all what I would’ve guessed <em>a priori</em>.  It seemed like obviously a clue to <em>something</em>, if I could only figure out what.</p>



<p>The puzzle is compounded once you realize that, even though the sun will boil the oceans in a billion years (and then die in a few billion more), other stars, primarily dwarf stars, will continue shining brightly for <em>trillions</em> more years.  Granted, the dwarf stars don’t seem quite as hospitable to life as sun-like stars, but they do seem <em>somewhat</em> hospitable, and there will be <em>lots</em> of them—indeed, more than of sun-like stars.  And they’ll last orders of magnitude longer.</p>



<p>To sum up, our temporal position relative to the lifetime of the sun makes it look as though life on earth was just a lucky draw from a gigantic cosmic <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</a>.  By contrast, our position relative to the lifetime of <em>all</em> the stars makes it look as though we arrived crazily, freakishly early—not at all what you’d expect under a random model.  So what gives?</p>



<p>Robin contends that <em>all</em> of these facts are explained under his bubble scenario.  If we’re to have an experience remotely like the human one, he says, then we <em>have to be</em> relatively close to the beginning of time—since hundreds of billions of years from now, the universe will likely be dominated by near-light-speed expanding spheres of intelligence, and a little upstart civilization like ours would no longer stand a chance.  I.e., even though our existence is down to some lucky accidents, and even though those same accidents probably recur throughout the cosmos, we shouldn’t yet <em>see</em> any of the other accidents, since if we did see them, it would already be nearly too late for us.</p>



<p>Robin admits that his account leaves a huge question open: namely, why should our experience have been a “merely human,” “pre-bubble” experience at all?  If you buy that these expanding bubbles are coming, it seems likely that there will be trillions of times more sentient experiences inside them than outside.  So experiences like ours would be rare and anomalous—like finding yourself at the dawn of human history, with Hammurabi et al., and realizing that almost every interesting thing that will ever happen is still to the future.  So Robin simply takes as a brute fact that our experience is “earth-like” or “human-like”; he then tries to explain the other observations from that starting point.</p>



<p>Notice that, in Robin’s scenario, the present epoch of the universe is extremely special: it’s when civilizations are just forming, when perhaps a few of them will achieve technological liftoff, but <em>before</em> one or more of the civilizations has remade the whole of creation for its own purposes.  Now is the time when the early intelligent beings like us can still look out and see quadrillions of stars shining to no apparent purpose, just <em>wasting</em> all that nuclear fuel in a near-empty cosmos, waiting for someone to come along and put the energy to good use.  In that respect, we’re sort of like the Maoris having just landed in New Zealand, or Bill Gates surveying the microcomputer software industry in 1975.  We’re ridiculously lucky.  The situation is way out of equilibrium.  The golden opportunity in front of us can’t possibly last forever.</p>



<p>If we accept the above, then a major question I had was the role of cosmology.  In 1998, astronomers discovered that the present cosmological epoch is special for a completely<em> different</em> reason than the one Robin talks about.  Namely, right now is when matter and <a href="https://en.wikipedia.org/wiki/Dark_energy">dark energy</a> contribute roughly similarly to the universe’s energy budget, with ~30% the former and ~70% the latter.  Billions of years hence, the universe will become more and more dominated by dark energy.  Our observable region will get sparser and sparser, as the dark energy pushes the galaxies further and further away from each other and from us, with more and more galaxies receding past the horizon where we could receive signals from them at the speed of light.  (Which means, in particular, that if you want to visit a galaxy a few billion light-years from here, you’d better start out while you still can!)</p>



<p>So here’s my question: is it just a coincidence that the time—right now—when the universe is “there for the taking,” potentially poised between competing spacefaring civilizations, is <em>also</em> the time when it’s poised between matter and dark energy?  Note that, in 2007, Bousso et al. tried to give a <a href="https://arxiv.org/abs/hep-th/0702115">sophisticated anthropic argument</a> for the value of the cosmological constant Λ, which measures the density of dark energy, and hence the eventual size of the observable universe.  <a href="https://www.scottaaronson.com/blog/?p=328">See here</a> for my blog post on what they did (“The array size of the universe”).  Long story short, for reasons that I explain in the post, it turns out to be essential to their anthropic explanation for Λ that civilizations flourish <em>only</em> (or mainly) in the present epoch, rather than trillions of years in the future.  If we had to count civilizations that far into the future, then the calculations would favor values of Λ much smaller than what we actually observe.  This, of course, seems to dovetail nicely with Robin’s account.</p>



<p>Let me end with some “practical” consequences of Robin’s scenario, supposing as usual that we take it seriously.  The most immediate consequence is that the prospects for <a href="https://en.wikipedia.org/wiki/Search_for_extraterrestrial_intelligence">SETI</a> are dimmer than you might’ve thought before you’d internalized all this.  (Even <em>after</em> having interalized it, I’d still like at least an order of magnitude more resources devoted to SETI than what our civilization currently spares.  Robin’s assumptions might be wrong!)</p>



<p>But a second consequence is that, if we want human-originated sentience to spread across the universe, then the sooner we get started the better!  Just like Bill Gates in 1975, we should expect that there will soon be competitors out there.  Indeed, there are likely competitors out there “already” (where “already” means, let’s say, in the rest frame of the cosmic microwave background)—it’s just that the light from them hasn’t yet reached us.  So if we want to determine our own cosmic destiny, rather than having post-singularity extraterrestrials determine it for us, then it’s way past time to get our act together as a species.  We might have only a few hundred million more years to do so.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> For more discussion of this post, see the <a href="https://www.reddit.com/r/slatestarcodex/comments/l8l4np/once_we_can_see_them_its_too_late/">SSC Reddit thread</a>.  I especially liked a <a href="https://www.reddit.com/r/slatestarcodex/comments/l8l4np/once_we_can_see_them_its_too_late/glel0lq/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">beautiful comment by “Njordsier,”</a> which fills in some important context for the arguments in this post:</p>



<blockquote class="wp-block-quote"><p>Suppose you’re an alien anthropologist that sent a probe to Earth a million years ago, and that probe can send back one high-resolution image of the Earth every hundred years. You’d barely notice humans at first, though they’re there. Then, circa 10,000 years ago (99% of the way into the stream) you begin to see plots of land turned into farms. Houses, then cities, first in a few isolated places in river valleys, then exploding across five or six continents. Walls, roads, aqueducts, castles, fortresses. Four frames before the end of the stream, the collapse of the population on two of the continents as invaders from another continent bring disease. At T-minus three frames, a sudden appearance of farmland and cities on the coasts those continents. At T-minus two frames, half the continent. At the second to last frame, a roaring interconnected network of roads, cities, farms, including skyscrapers in the cities that were just trying villas three frames ago. And in the last frame, nearly 80 percent of all wilderness converted to some kind of artifice, and the sky is streaked with the trails of flying machines all over the world.</p><p>Civilizations rose and fell, cultures evolved and clashed, and great and terrible men and women performed awesome deeds. But what the alien anthropologist sees is a consistent, rapid, exponential explosion of a species bulldozing everything in its path.</p><p>That’s what we’re doing when we talk about the far future, or about hypothetical expansionist aliens, on long time scales. We’re zooming out past the level where you can reason about individuals or cultures, but see the strokes of much longer patterns that emerge from that messy, beautiful chaos that is civilization.</p></blockquote>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Jan. 31):</span></strong> Reading the reactions here, <a href="https://news.ycombinator.com/item?id=25972111">on Hacker News</a>, and elsewhere underscored for me that a lot of people get off Robin’s train well before it’s even left the station.  Such people think of extraterrestrial civilizations as things that you either <em>find</em> or, if you haven’t found one, you just <em>speculate</em> or <em>invent stories</em> about.  They’re not even in the category of things that you have any serious hope to <em>reason</em> about.  For myself, I’d simply observe that trying to reason about matters far beyond current human experience, based on the microscopic shreds of fact available to us (e.g., about the earth’s spatial and temporal position within the universe), has led to some of our species’ embarrassing failures but also to some of its greatest triumphs.  Since even the failures tend to be relatively cheap, I feel like we ought to be “venture capitalists” about such efforts to reason beyond our station, encouraging them collegially and mocking them only gently.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5253"><span class="datestr">at January 30, 2021 08:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18041">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/">Alan Selman, 1941–2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Keeping a promise to structure the field of complexity</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/alanwhoswho/" rel="attachment wp-att-18043"><img width="160" alt="" src="https://rjlipton.files.wordpress.com/2021/01/alanwhoswho.jpg?w=160&amp;h=180" class="aligncenter wp-image-18043" height="180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">2017 <em>Who’s Who <a href="https://www.24-7pressrelease.com/press-release/439185/dr-alan-louis-selman-named-a-lifetime-achiever-by-marquis-whos-who">award</a></em></font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman, my longtime friend and colleague, passed away last Friday, from the one illness that is most cruel for someone who has excelled by brainpower and effervescent <em>joie de vivre</em> alike.</p>
<p>
Today, Dick and I join others in mourning and also in appreciation.</p>
<p>
I last saw Alan with his wife Sharon in May 2019 at the dinner for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">NP50</a> celebration of Steve Cook in Toronto. There were inklings of his affliction but nothing that kept him from engaging in dinner talk about how our field has progressed, about reunited friends, about the convivial atmosphere and culture on display during the celebration, and even my own goings-on with chess and quantum complexity and all our grown children. Alan was still going out to concerts last March until the pandemic stopped all that kind of activity. </p>
<p>
I’ve known Alan in person for almost 40 years, starting a decade before he became my Chair for 6 years and colleague for 18 years more of them before his retirement and move to New Jersey. This was all through my graduate study at Oxford. It began at ICALP 1982 in Aarhus after my first year. During a December 1983 workshop at Brooklyn College he shared a problem with me whose story I told <a href="https://rjlipton.wordpress.com/2012/07/14/it-dont-come-easy/">here</a>. He visited me in Oxford in early 1986 accompanied by his son Jeffrey, whose eulogy in this past Sunday’s burial <a href="https://www.facebook.com/162731740423576/videos/756739685268316">service</a> was especially moving. Alan gave me other helps in 1985–86 that aided my “repatriation,” so to speak. My point is that he took great interest in builders of complexity theory even as students—Uwe Schöning noted that he visited him in 1982 while he was a student as well. My 2014 <a href="https://rjlipton.wordpress.com/2014/05/13/nexp-and-the-next-generation/">post</a> on Alan’s retirement party expressed how this continued throughout. </p>
<p>
</p><p></p><h2> P NE NP </h2><p></p>
<p></p><p>
As others have remarked, Alan’s cars had a license plate that declared <b>P NE NP</b>. Over a quarter century I regularly saw it in the parking lot serving Bell Hall, and after 2012, the same lot for our department’s new digs in Davis Hall. My colleague Sargur Srihari relates seeing someone approach Alan in the lot and say, “Yes, but can you prove it?”—adding that it was “probably the only thing in complexity that Alan couldn’t do.” An irony I will say here now is that the Dean of Engineering and Applied Sciences whom Alan worked with during much of his time as Chair, Mark Karwan, <a href="https://arxiv.org/abs/1610.00353">claims</a> the <a href="https://arxiv.org/abs/1902.03549">opposite</a>. It would have been fun to see a dueling license plate saying <b>P EQ NP</b>.</p>
<p><a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/license_20210126153121_34106/" rel="attachment wp-att-18044"><img src="https://rjlipton.files.wordpress.com/2021/01/license_20210126153121_34106.jpg?w=600" alt="" class="aligncenter size-full wp-image-18044" /></a></p>
<p>
Long followers of this blog know that Dick and I have embodied this <a href="https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/">duel</a> between ourselves. Only recently has Dick <a href="https://rjlipton.wordpress.com/2017/12/08/pnp-perhaps-i-change-my-mind/">swung</a> toward <a href="https://rjlipton.wordpress.com/2020/06/16/pnp/">P <img src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;}" class="latex" title="{&lt;}" /> NP</a>, while <a href="https://emanueleviola.wordpress.com/2018/02/16/i-believe-pnp/">some</a> have <a href="https://www.informit.com/articles/article.aspx?p=2213858">moved</a> the other <a href="https://www.cs.rice.edu/~vardi/comp409/pvnp10.pdf">way</a>. </p>
<p>
But what we think everyone would agree on is that we cannot point to any definite progress in the past dozen-plus years, at least not directly on the question. This struck me particularly from the Cook workshop, and this would have been the lead theme of a post on it, had I not been sprinting toward an effective June deadline to complete a major upgrade to my statistical chess model which I <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">achieved</a> two months later. No one attested real progress in comments to our P=NP status <a href="https://rjlipton.wordpress.com/2020/01/12/our-thoughts-on-pnp/">post</a> a year ago. I wrote to Sharon two weeks ago that I wished I could tell Alan of progress but no.</p>
<p>
Where Alan, Dick, I, and many of us are completely aligned is on these two points:</p>
<ol>
<li>
The importance of unwavering focus on P versus NP and other fundamental questions about computational problems, despite our field’s lack of scientific achievement in classifying them. <p></p>
</li><li>
Emphasis on what our field has positively, scientifically, and perhaps surprisingly achieved, which is showing deep and crisp mathematical relations among thousands of diverse problems. These relations can be phrased not only in the common term <em>reductions</em> but in terms that arose in structural complexity, such as <a href="https://en.wikipedia.org/wiki/Berman-Hartmanis_conjecture">isomorphisms</a>, <a href="https://en.wikipedia.org/wiki/Truth-table_reduction">(non-)adaptivity</a>, <a href="https://en.wikipedia.org/wiki/Parsimonious_reduction">parsinomiousness</a>, and <a href="http://people.cs.uchicago.edu/~fortnow/papers/insep.pdf">(in-)separability</a>.
</li></ol>
<p>
We put in the mouth of a fictional <a href="https://rjlipton.wordpress.com/2014/05/23/stoc-1500/">STOC 1500</a> keynote the position that relations between problems should be the field’s fundamental objects. There is incredible beauty in these objects, as was the theme of Lane Hemaspaandra’s <a href="https://arxiv.org/pdf/1406.4106.pdf">tribute</a> to Alan on his retirement surveying gems of Alan’s work, and it was my elation to be captivated by them beginning in my graduate years of the 1980s.</p>
<p>
</p><p></p><h2> Elijah </h2><p></p>
<p></p><p>
Alan’s Hebrew name was <em>Eliyahu</em>, Elijah. In the Bible, Elijah is a traveling prophet: he not only goes from <a href="https://en.wikipedia.org/wiki/Sarepta">Zarephath</a> to <a href="https://en.wikipedia.org/wiki/Beersheba">Beersheba</a> but all the way south to Mount Sinai, where he is commanded to return all the way north to Damascus. The mission from Sinai is conveyed not in cloud and fire as with Moses, nor in wind or earthquake, but <a href="https://www.biblegateway.com/passage/?search=1+Kings+19:9-16&amp;version=KJV">famously</a> in a still small voice. </p>
<p>
That was the kind of voice in which Alan during his many travels spread the love of complexity theory. I was glad to experience his kindness, good crisp advice, and also the giving of freedom to pursue things with zeal—a trait that Elijah had and recognized. I was not so fast to recognize good food and fine culture. I remember once when Alan was especially happy about securing tickets to see the 86-year-old legendary jazz violinist Stéphane Grappelli perform in Buffalo and I did not know who that was. I was, however, able to reply that I’d once seen Chet Baker live while standing in a small room in London. He and Sharon and my wife Debbie and I shared other musical interests. All of what I implied about whole-person education in my <a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/">memorial</a> for Peter Neumann applies to Alan. In Bell Hall we had only a mid-size seminar room for gatherings, yet Alan right away recognized the importance of making it like Oxford’s Mathematical Institute tea room—but with wine included—in weekly Friday afternoon “TGIF” gatherings open to faculty and graduate students, for which I did the shopping those mornings at Wegmans.</p>
<p>
Elijah is also known as a keeper of promises. I won’t go into the gruesome biblical details here, but rather note that Alan shepherded a promise of a different kind: the notion of a <a href="https://en.wikipedia.org/wiki/Promise_problem">promise problem</a>, following on from his famous 1984 <a href="https://www.sciencedirect.com/science/article/pii/S001999588480056X">paper</a> with Shimon Even and Yacov Yacobi. This is a structural complexity concept whose usage has grown, besides the original technical challenges which I recounted <a href="https://rjlipton.wordpress.com/2012/07/14/it-dont-come-easy/">here</a>. I’ve sometimes felt that wider uses have run risk of getting burned by inexactitudes, such as in the notion of promise-completeness.  Alan’s exactness was one trait that made me feel at home.</p>
<p>
Elijah also founds a school <a href="https://www.biblegateway.com/passage/?search=2+Kings+2:1-9&amp;version=KJV">called</a> the “sons of the prophets,” passing his mantle to Elisha before being taken up in a chariot of fire. It is not for me to say who might pick up Alan’s mantle in structural complexity. But many in his train have said wonderful things in tribute over the weekend, and I wish to pass along some of them, with quotes from those who have given permission or have already written in public, in particular as comments to Lance Fortnow’s <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html">tribute</a> to Alan.</p>
<p>
</p><p></p><h2> Some Tributes From Colleagues </h2><p></p>
<p></p><p>
The news was passed around our department in Buffalo. Our Chair, <b>Chunming Qiao</b>, mourned the loss of a great colleague, a former Chair, and a respected scholar. <b>Sargur Srihari</b> told the parking lot story above and and noted how Alan recruited a star to the department in the person of Jin-Yi Cai. <b>Stuart Shapiro</b>, who preceded and succeeded Alan as Chair, noted Alan’s higher personal standards for research and publishing. <b>Russ Miller</b>, writing earlier this month when we had news of Alan’s being in hospice care, hailed him as “a terrific person,” meeting the standard of a “true gentleman and scholar,” and hailed his communication skills with colleagues and the university administration alike. <b>Jinhui Xu</b> expressed thanks for Alan’s thoughtfulness and role as a mentor. I can say all this from my own experience. Many of us also took part in the earlier outpouring of affection, including photos and memories of Alan’s time in Buffalo, that was communicated to Sharon. </p>
<p>
<b>Mark Karwan</b> adds, “Alan continued to win awards and accolades throughout my 12 years as Dean which ended in 2006. All of his professional accomplishments provided a great sense of pride and international recognition for UB. Alan was truly a senior leader and mentor for so many. He spoke to me as an advocate for his department and for the young rising faculty and the need to nourish and retain them. His wise counsel was always given in the most gentlemanly manner. When I picture Alan today, I see the twinkle in his eyes and his contagious joy of life. I only started working relatively recently with a colleague in our field of Operations Research on matters in complexity with implications to support P = NP. It would have been such fun to bring Alan into our conversations! We will all miss Alan and will always appreciate the great legacy he created here at UB.” </p>
<p>
<b>Jin-Yi</b>, of course, is well known to all who knew Alan in the theory community and readers of this blog. He called Alan a great structural complexity theorist, and wrote in one of many e-mails I’ve been copied on: “Alan [was] a wonderful colleague of mine, and had a lifetime contribution to complexity theory that will certainly live on. He had a dry wit, enigmatic smile, and most of all, a truly kind heart. We will always remember him in our fond memory.”</p>
<p>
<b>Atri Rudra</b> worked with Alan on many projects including organizing and securing funding for the Eastern Great Lakes (EaGL) Theory Workshops along with Bobby Kleinberg of Cornell. Here are two photos from the first one in 2008:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/cookalaneagl2008/" rel="attachment wp-att-18046"><img width="400" alt="" src="https://rjlipton.files.wordpress.com/2021/01/cookalaneagl2008.jpg?w=400&amp;h=270" class="aligncenter wp-image-18046" height="270" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite from 2008 EaGL photo <a href="https://cse.buffalo.edu/events/theory/images.html">page</a>; Kleinberg to Alan’s right and Atri (obscured) behind.</font>
</td>
</tr>
</tbody></table>
<p>
Atri writes, “I still remember being taken directly from the airport to Alan’s favorite restaurant, Trattoria Aroma, during my interview at Buffalo. I was a bit intimidated by Alan during the dinner but we bonded over the fact that we were both married to epidemiologists. After I joined Buffalo, Alan’s sage advice helped me throughout my tenure process. Alan was a giant in the department and having him in my corner did not hurt. [H]e had a wicked sense of humor as well.”</p>
<p>
<b>Mitsunori Ogihara</b> succeeded Alan as Editor-in-Chief of the <a href="https://www.springer.com/journal/224">journal</a> <em>Theory of Computer Systems</em> and blossomed from being a postdoc recruited by Alan at UB to being Chair of Computer Science at the University of Rochester in a few short years. The story of my taking Mitsu from the airport on a snowy night will be told another time. He is writing his own tribute to appear in the journal, from which I quote just a few words: “While being a scholar of uncanny vision and incredible ingenuity, Alan was a lovely human being. He was generous with his time for his students and colleagues, always willing to offer consultation and advice. The field of theoretical computer science has lost its giant. Alan is no longer here to lead us, but his legacy will live.”</p>
<p>
<b>Lane Hemaspaandra</b>, Mitsu’s colleague at UR, writes: “Very briefly put, Alan’s research handiwork and vision is ingrained in the shape of the field. I was lucky enough to through his generosity know Alan across decades: we co-edited a book, wrote two research papers together, even wrote—Alan had a real love of writing and language—a note on writing in theoretical computer science, and had a long tradition of research seminars and theory days that brought together the University at Buffalo, RIT, and University of Rochester theory groups. Alan was also my wife’s wonderful postdoctoral advisor. And, throughout all that, and coexisting with Alan’s technical artistry and amazing taste in theory research, Alan’s love of and expertise in the ‘real’ world was quiet yet luminous. Alan loved the theater, and spoke warmly of his beloved Shaw Festival at Niagara-on-the-Lake. He loved and knew food; any restaurant commended by Alan was going to be an experience.”</p>
<p>
<b>Ashish Naik</b> <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611460983996#c8100240511053878034">noted</a> that he was Alan’s first student in Buffalo: “[I] still remember going to his office to ask if he would take me as a student, somewhat intimidated because of his stature in the field. Alan he put me at ease immediately with his sense of humor (I still remember the cartoon clippings on his office door) and his generosity. Despite a busy schedule as the Chair, Alan always had time for me — introducing me to the field, sharing interesting problems and his ideas, and teaching me how to write well and asking the right questions, skills that have stayed with me forever.”</p>
<p>
<b>D. Sivakumar</b> was my student at the same time as Ashish, and began his own <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611529242421#c422919304502293363">comment</a> in Lance’s tribute by noting how Alan’s early work with Neil Jones on a logical characterization of nondeterministic exponential time was a precursor to Ron Fagin’s 1974 <a href="https://researcher.watson.ibm.com/researcher/files/us-fagin/genspec.pdf">characterization</a> of NP. He continued how at the end of his first year in Buffalo, “Alan encouraged us students to attend the Structures conference in Boston even though only one of us a theory-committed student. Alan drummed up some money to pay our registration, and we rented a car and stayed with friends in Boston. Structures’92 was magical for me—my first academic conference—everything about the atmosphere, the ideas, the people was fascinating and I decided to work in that field. All the work in my thesis (sparse sets, measure theory,…) were on topics I heard about for the first time at Structures’92—without Alan’s nudge, my life would likely have taken an entirely different trajectory. … Alan’s wit, dry humor, and his generosity are the things I’ll remember forever.”</p>
<p>
<b>Pavan Aduri</b> was also Alan’s student in the 1990s and, speaking during a <em>shiv’ah</em> service with the family on Monday (via Zoom), noted Alan’s quest for <em>simplicity</em> in complexity theory. “Whenever I started to present a proof, Alan would ask, ‘Can you simplify the proof?’ ” Now at Iowa State, he recalled that he would frequently reach out to Alan for advice on various matters, even on what would be a more appropriate title for a research proposal. “Alan was always very generous with his time and advice.” <b>Frederic Green</b> related at the <em>shiv’ah</em> how Alan helped him migrate from physics to complexity and once reviewed every single step of four pages of notes on a complicated proof to optimize it for presentation. <b>Steve Fenner</b> told of Alan’s hosting him in Buffalo right after Steve’s PhD and putting him at ease with small talk about the permanently temporary quarters in Bell Hall. <b>Steve Homer</b> and his wife told of being travel partners exploring castles and flower markets, and the long gestation of the Homer-Selman <a href="https://www.springer.com/gp/book/9781461406815">textbook</a>, <em>Computability and Complexity Theory</em>.</p>
<p>
<b>Harry Buhrman</b> writes: “Alan has been an inspiration for me from the time I started working as a PhD student in complexity theory. My first encounter was his beautiful work on reductions and P-selective sets. I jokingly used to call them P-Selman-sets. I consider myself very lucky that he came over to my PhD defense in 1993 in Amsterdam. It was a joyous gathering.” I remember the photos Alan showed of that defence as well as Edith Hemaspaandra’s: grand pageantry on a scale I did not see even at Oxford, and led by Harry’s co-advisor <b>Peter van Emde Boas</b>, whose <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611483268043#c1171975184441035945">note</a> in Lance’s tribute recalled the genesis of the “Structures” conferences in 1985–86 and the 1994 edition in Amsterdam and both hosting and visiting Alan and Sharon for dinners.</p>
<p>
My retired colleague <b>Mike Buckley</b> <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html?showComment=1611419108397#c7225101539941877122">wrote</a>, “I taught with Alan at UB, and he taught me how to teach. Students sought him out until the day he left. He put as much into the undergrads as the PhDs. His presence at faculty meetings meant that no decision was made until he had his last comment. He was a giant in the field, but also the world’s nicest man. I kept in touch and we talked stereos and jazz. I miss him already.”</p>
<p>
<b>Arnold Rosenberg</b> and <b>Paul Spirakis</b> hailed Alan as “a true pioneer and leader in the area of logic-based CS” and noted their long cooperation on the editorial board of the <em>Theory of Computing Systems</em> journal. Arnold continued: “He played vital roles in both research and education in CS. He approached every professional challenge with vision and unswerving devotion to the highest standards of our field. Our journal and our field have lost a great person.” <b>Rod Downey</b> included special thanks for Alan’s early support of parameterized complexity and the journal’s support, for instance in special issues.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Readers are welcome to place words of grace in comments; more may be added above as well. </p>
<p>
Our embraces go to Sharon, to Jeffrey, to their daughter Heather—who also works in medicine—and to all the family. </p>
<p>
[Cantor for the service was not Heather Wargo but Sandra Messinger Aguilar; some word fixes]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2021/01/29/alan-selman-1941-2021/"><span class="datestr">at January 29, 2021 10:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-8353632136896364991">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/01/one-phd-and-one-postdoc-position-at.html">One PhD and one postdoc position at Reykjavik University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div dir="ltr"><div><div style="text-align: center;"><b>Mode(l)s of Verification and Monitorability</b></div><div style="text-align: center;"><br /></div><b><div style="text-align: center;"><b>Department of Computer Science, Reykjavik University</b></div></b><div style="text-align: center;"><br /></div><b><div style="text-align: center;"><b>One PhD and one postdoc position</b></div></b><br /><br />We  invite applications for a total of two positions: one PhD position and  one postdoc position, at the Department of Computer Science of Reykjavik  University.<br /><br />The position is part of a research project funded by  the Icelandic Research Fund, under the direction of Antonis Achilleos  (Reykjavik University), Luca Aceto (Reykjavik University), and Anna  Ingolfsdottir (Reykjavik University) in cooperation with Adrian  Francalanza (University of Malta) and Karoliina Lehtinen (LIS,  Aix-Marseille).<br /><br />The project continues previous work in the  theoretical foundations of runtime verification and its overarching goal  is to better understand the properties and push the limits of  monitorability in different settings. For more information on the  project, please visit <br /><br /><a href="https://sites.google.com/view/antonisachilleos/movemnt" target="_blank">https://sites.google.com/view/antonisachilleos/movemnt</a> <br /><br />or contact Antonis Achilleos (email: <a href="mailto:antonios@ru.is" target="_blank">antonios@ru.is</a>)<br /><br />The successful candidates will benefit from, and contribute to, the<br />research environment at the Icelandic Centre of Excellence in<br />Theoretical Computer Science (ICE-TCS), with research groups on<br />concurrency, logic and semantics, algorithms, combinatorics. For<br />information about ICE-TCS and its activities, see<br /><br /><a href="http://icetcs.ru.is/" target="_blank">http://icetcs.ru.is/</a> .<br /><br />Moreover, they will cooperate with Adrian Francalanza and Karoliina </div><div>Lehtinen during the project work and will benefit from the interaction </div><div>with their research groups at the University of Malta and LIS, Aix-Marseille.<br /><br /></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Qualification requirements*</i></div></blockquote><div><br />Applicants for the postdoctoral position should have, or be about to<br />defend, a PhD degree in computer science or a closely related<br />field. Moreover, previous knowledge in logic, concurrency theory, or any </div><div>of the project's related areas, and mathematical competence are desirable.<br /><br />Applicants for the PhD fellowship should have, or be about to obtain, an </div><div>MSc degree in Computer Science, or closely related fields. Some </div><div>background in logic, concurrency theory, or some other of the project's </div><div>related areas, and mathematical competence are desirable.<br /><br /></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Remuneration*</i></div></blockquote><div><br />The PhD position provides a stipend of 383,000 ISK per month before </div><div>taxes, and the salary for the postdoc position is 460,000 ISK per month </div><div>before taxes.<br /><br /></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Start date and duration*</i></div></blockquote><div><br />The PhD position is for three years, to start as soon as possible.<br /><br />The  postdoc position is for one year, to start in July or August 2021 (the  start date is negotiable), and can be renewed for one more year, based  on mutual agreement.<br /><br /></div><blockquote style="border: medium none; margin: 0px 0px 0px 40px; padding: 0px;"><div><i>*Application details*</i></div></blockquote><div><br />Interested  applicants should send their CV, including a list of publications, in  PDF to all the addresses below, together with a statement outlining  their suitability for the project and the names of at least two  referees.<br /><br />Antonis Achilleos<br />email: <a href="mailto:antonios@ru.is" target="_blank">antonios@ru.is</a><br /><br />Luca Aceto<br />email: <a href="mailto:luca@ru.is" target="_blank">luca@ru.is</a><br /><br />Anna Ingolfsdottir<br />email: <a href="mailto:annai@ru.is" target="_blank">annai@ru.is</a><br /><br />Adrian Francalanza<br />email: <a href="mailto:adrian.francalanza@um.edu.mt" target="_blank">adrian.francalanza@um.edu.mt</a><br /><br />Karoliina Lehtinen<br />email: <a href="mailto:lehtinen@lis-lab.fr" target="_blank">lehtinen@lis-lab.fr</a><br /><br />Informal inquiries about the project and the conditions of work are very welcome.<br /><br />We  will start reviewing applications as soon as they arrive and will  continue to accept applications until each position is filled. We  strongly encourage interested applicants to send their applications as  soon as possible and no later than 20 February 2021.</div></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/01/one-phd-and-one-postdoc-position-at.html"><span class="datestr">at January 29, 2021 06:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18030">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/">Happy Un-Birthday Rich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Still going strong</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p>
Richard DeMillo just turned 74 years old the other day. Happy Birthday Rich, and many more. </p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/rich/" rel="attachment wp-att-18032"><img width="200" alt="" class="alignright wp-image-18032" src="https://rjlipton.files.wordpress.com/2021/01/rich.jpg?w=200" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Today I want to wish him also a happy <a href="https://en.wikipedia.org/wiki/Unbirthday">un-birthday</a>.</p>
<p>
Recall an un-birthday is celebrated on any day that is not your birthday. It was created by Lewis Carroll in his 1871 novel Through the Looking-Glass. </p>
<p>I aways liked the concept of un-birthdays. Probably only a mathematician like Carroll could think of this—every set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> has a complement <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbar%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bar{S}}" class="latex" title="{\bar{S}}" />.</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/un-2/" rel="attachment wp-att-18033"><img width="200" alt="" class="aligncenter  wp-image-18033" src="https://rjlipton.files.wordpress.com/2021/01/un.jpg?w=200" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p></p><h2> Quest for Correctness </h2><p></p>
<p></p><p>
Rich and I started our work together over four decades ago. A central theme of our work was <i>correctness</i>. We were concerned That programs might not work as planned. At the time it was not obvious that this was a major theme of our joint work. But looking back now I can see that it was. </p>
<p>
We wrote several papers on correctness, from various angles.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> Our work on practical approaches to program testing. The main paper was <a href="https://ieeexplore.ieee.org/document/1646911">Hints on test data selection: Help for the practicing programmer</a> by Rich DeMillo, Fred Sayward in 1978. This was part of our work on the mutation program testing <a href="https://en.wikipedia.org/wiki/Mutation_testing">method</a>. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> Our <a href="https://link.springer.com/chapter/10.1007/3-540-69053-0_4">On the importance of checking cryptographic protocols for faults</a> by Dan Boneh, Rich DeMillo in 1997. This showed that incorrect crypto systems could be attacked much easier than correct ones. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> Our <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">Social processes and proofs of theorems and programs</a> by Rich DeMillo, Alan Perlis. This was paper argued against verification technology as the main way to make programs correct. Was attacked as wrong then, and probably still viewed as wrong by many. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> Our <a href="https://apps.dtic.mil/sti/pdfs/ADA050745.pdf">Probabilistic Remark on Algebraic Program Testing</a> 1977 By Rich DeMillo. And in <a href="https://www.sciencedirect.com/science/article/abs/pii/0020019078900674">Information Processing Letters</a>. See how we <a href="https://rjlipton.wordpress.com/2009/11/30/the-curious-history-of-the-schwartz-zippel-lemma/">missed the boat</a> on this important result. </p>
<p>
</p><p></p><h2> Other Quests </h2><p></p>
<p></p><p>
Rich has done much besides his research, that covered much more than just the above papers. He has had a long successful career that has two paths. He was a leader in industry and in government. At Hewlett-Packard Company he served as the company’s first Chief Technology Officer. He also held executive positions with Telcordia Technologies (formerly known as Bell Communications Research) and the National Science Foundation.</p>
<p>
He was and still is a leader in academia. He is currently Distinguished Professor of Computing and Professor of Management at the Georgia Institute of Technology. He was the John P. Imlay Dean of Computing at Georgia Tech for six years. He is now the Chair of the new School of Cybersecurity and Privacy in the College of Computing.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
See <a href="https://rjlipton.wordpress.com/2009/02/18/insider-baseball-and-pnp/">this</a> for some earlier remarks on Rich. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/01/29/happy-un-birthday-rich/"><span class="datestr">at January 29, 2021 03:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20946">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">Possible future Polymath projects (2009, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.files.wordpress.com/2021/01/polymath.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/01/polymath.png" class="alignnone size-full wp-image-21120" height="269" /></a></p>
<p style="text-align: center;"><strong><span style="color: #ff0000;">What will be our next polymath project?</span></strong></p>
<p>A <a href="https://en.wikipedia.org/wiki/Polymath_Project">polymath project</a> (<a href="https://en.wikipedia.org/wiki/Polymath_Project">Wikipedia</a>) is a collaboration among mathematicians to solve important and difficult mathematical problems by coordinating many mathematicians to communicate with each other on finding the best route to the solution. The project began in January 2009 on Timothy Gowers’s blog when he posted a problem and asked his readers to post partial ideas and partial progress toward a solution. This experiment resulted in a new answer to a difficult problem, and since then the Polymath Project has grown to describe a particular process of using an online collaboration to solve any math problem.</p>



<p>After the success of Polymath1 and the launching of Polymath3 and Polymath4, Tim Gowers wrote a blog post <a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects">“Possible future Polymath projects”</a> for planning the next polymath project on his blog. The post mentioned 9 possible projects. (Three of them later turned  to polymath projects, and one turned into a project of a different nature.) Following the post and separate posts describing some of the proposed projects, a few polls were taken and a problem – the Erdős discrepancy problem, was selected for the next project polymath5. </p>
<p>One of our next posts will have the same title and similar purpose as Tim’s 2009 post.  I will describe several possibilities for my next polymath project. <span style="color: #ff0000;"><strong>(A quick rather vague and tentative preview can be found at the end of this post.</strong></span>) Today we go back to Tim’s 2009 post and the problems posed there.</p>
<p><strong><span style="color: #0000ff;"> Comments on the 2009 proposed projects, the new proposed projects, other proposed projects, and on the polymath endeavor, are most welcome. (At the end of the post I also mention a few “meta” questions.)</span></strong></p>
<p>Let me also mention <a href="https://polytcs.wordpress.com/" rel="home">The PolyTCS Project</a> aimed for proposing projects in theoretical computer science. There are so far three very interesting proposals there, and the first proposal is the Friedgut-Kalai <a href="https://terrytao.wordpress.com/2007/08/16/gil-kalai-the-entropyinfluence-conjecture/">Entropy/Influence conjecture</a>. For various proposals, see also <a href="http://polymathprojects.org/">the polymath blog</a> administered by Tim Gowers, Michael Nielsen, Terry Tao, and me, and <a href="https://mathoverflow.net/questions/219638/proposals-for-polymath-projects">this MO question</a>.</p>
<h2>The proposed projects in Gowers’s 2009 post and updates regarding these projects.</h2>
<p><strong>1. Littlewood’s conjecture and related problems.</strong></p>
<p>The conjecture states that if <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" title="\alpha" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\beta" class="latex" title="\beta" /> are any two real numbers, and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" style="padding: 0; border: none; vertical-align: middle; color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 12.6px; font-style: normal; font-weight: 400; letter-spacing: normal; text-align: justify; text-indent: 0; white-space: normal;" class="latex" alt="\epsilon&gt;0" title="\epsilon&gt;0" />, then there exists a positive integer <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> such that <img src="https://s0.wp.com/latex.php?latex=%7C%7C%5Calpha+n%7C%7C%5C%2C%7C%7C%5Cbeta+n%7C%7C%5Cleq%5Cepsilon%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="||\alpha n||\,||\beta n||\leq\epsilon/n" class="latex" title="||\alpha n||\,||\beta n||\leq\epsilon/n" />. Famously, Einsiedler, Katok and Lindenstrauss proved that the Hausdorff dimension of the set of counterexamples to the conjecture is zero. Gowers had ideas for an elementary approach, and his ideas  <a href="https://gowers.wordpress.com/2009/11/17/problems-related-to-littlewoods-conjecture-2/">are described in this later post.</a> This project was not launched and I am also not aware of progress related to the problem (but I am not an expert). </p>
<p><strong>2. A DHJ-related project.</strong></p>
<p>DHJ stands for “density Hales Jewett” which was the topic of polymath1. The second proposed project was to build on the success of polymath1 and at<a href="https://gowers.wordpress.com/2009/11/14/the-first-unknown-case-of-polynomial-dhj/"> a later post</a> the following problem was proposed.</p>
<p><strong>Conjecture:  </strong><em>For every <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" style="padding: 0; border: none; vertical-align: middle;" class="latex" alt="\delta&gt;0" title="\delta&gt;0" /> and every positive integer <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> there exists <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> such that if <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> is any subset of <img src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5E%7B%5Bn%5D%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[k]^{[n]^2}" class="latex" title="[k]^{[n]^2}" /> of density at least <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" />, then <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> contains a <a href="http://michaelnielsen.org/polymath1/index.php?title=Combinatorial_line">combinatorial line</a> such that the wildcard set is of the form <img src="https://s0.wp.com/latex.php?latex=X%5Ctimes+X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="X\times X" class="latex" title="X\times X" /> for some subset <img src="https://s0.wp.com/latex.php?latex=X%5Csubset%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="X\subset[n]" class="latex" title="X\subset[n]" />.</em></p>
<p>As far as I know, this conjecture is still open.</p>
<p>Both questions “what kind of forbidden patters in <img src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[k]^n" class="latex" title="[k]^n" /> force exponentially small density” and “what kind of forbidden patters in <img src="https://s0.wp.com/latex.php?latex=%5Bk%5D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[k]^n" class="latex" title="[k]^n" /> force vanishing density” are fascinating. Let me recommend again the <a href="https://gilkalai.wordpress.com/2009/05/18/the-cap-set-problem-and-frankl-rodl-theorem-c/">Frankl-Rodl theorem</a> and its proof as a role model.</p>
<p><strong>3. Four Erdős-style combinatorial problems.</strong></p>
<p><strong>3a. Erdős’s discrepancy problem</strong></p>
<p>This was the problem that was eventually chosen for <strong>polymath5</strong>. This was a very nice story. The problem was presented<a href="https://gowers.wordpress.com/2009/12/17/erdoss-discrepancy-problem/"> in this blog post</a> and selected as polymath5 after some polls among readers. The <a href="https://gowers.wordpress.com/category/polymath5/">polymath project</a> was the longest in polymath history. There were six preliminary discussion posts with more than 600 comments followed by 21 official posts EDP1-EDP21. There was some short revival of the project (EDP22-EDP27) in 2012 where I contributed three posts. Famously, in 2015 Terry Tao solved the problem. The paper <a href="https://discreteanalysisjournal.com/article/609">appeared in the journal Discrete Analysis</a>. Tao’s solution relies on some insights from the polymath project including a crucial reduction that Terry himself contributed. It also crucially relied on a (then) new theory by Kaisa Matomaki and Maksym Radziwill. The solution was triggered  by a blog comment by Uwe Stroinski who pointed to a possible connection to EDP, and a subsequent one by Kodlu who seconded Uwe’s suggestion. This is reported in <a href="https://gowers.wordpress.com/2015/09/20/edp28-problem-solved-by-terence-tao/">EDP28</a>, and here on my blog we celebrated the solution in <a href="https://gilkalai.wordpress.com/2015/10/22/edp-reflections-and-celebrations/">this post</a>.  </p>
<p><strong>3b. The Erdős-Hajnal conjecture.</strong></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> be a positive integer and let <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> be a graph. Erdös and Hajnal conjectured that there is a constant <img src="https://s0.wp.com/latex.php?latex=C%3DC%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="C=C(H)" class="latex" title="C=C(H)" /> such that if <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> is any graph with at least <img src="https://s0.wp.com/latex.php?latex=k%5EC&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^C" class="latex" title="k^C" /> vertices that does not contain any induced copy of <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" />, then either <img src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G" class="latex" title="G" /> or <img src="https://s0.wp.com/latex.php?latex=G%5Ec&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="G^c" class="latex" title="G^c" /> contain a clique of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />.</p>
<p>Tim asserted that the simplest open case is where <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> is a pentagon. This special case was recently settled by Maria Chudnovsky<span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">, </span>Alex Scott<span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">, Paul Seymour and Sophie Spirkl. They rely on recent results by </span>Janos Pach and Istvan Tomon. See <a href="https://youtu.be/nz-fnYvsuN8">this videotaped lecture</a> by Paul Seymour at IBS Discrete Mathematics Group, South Korea.</p>
<p><strong>3c. Frankl’s union-closed conjecture.</strong></p>
<p><span id="more-20946"></span>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BA%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathcal{A}" class="latex" title="\mathcal{A}" /> be a collection of sets that is closed under taking unions. Must there be an element that is contained in at least half the sets?</p>
<p>Tim wrote:<em> This is a notorious question, and possibly the least likely to yield to a Polymath approach (it feels as though there might be a burst of ideas, none of which would work, followed by disillusionment, or else, if we were very lucky, a single bright idea from one person that essentially solved the problem, but I could be wrong).</em></p>
<p><a href="https://gowers.wordpress.com/category/polymath11/"><strong>Polymath11</strong></a> on Tim Gowers’s blog (Launched January, 2016) was devoted to Frankl’s conjecture. The problem is still open.</p>
<p><strong>3d. The delta-systems problem.</strong></p>
<p>This question is due to Erdős and Rado. A <em>delta system</em> is a collection of sets <img src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cdots%2CA_m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A_1,\dots,A_m" class="latex" title="A_1,\dots,A_m" /> such that all the sets <img src="https://s0.wp.com/latex.php?latex=A_i%5Ccap+A_j&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A_i\cap A_j" class="latex" title="A_i\cap A_j" /> (with <img src="https://s0.wp.com/latex.php?latex=i%5Cne+j&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="i\ne j" class="latex" title="i\ne j" />) are equal. Equivalently, there exists some set <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> such that <img src="https://s0.wp.com/latex.php?latex=X%5Csubset+A_i&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="X\subset A_i" class="latex" title="X\subset A_i" /> for every <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />, and the sets <img src="https://s0.wp.com/latex.php?latex=A_i%5Csetminus+X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="A_i\setminus X" class="latex" title="A_i\setminus X" /> are disjoint.</p>
<p>Erdös offered 1000 dollars for a solution to the following problem: does there exist a constant <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="C" class="latex" title="C" /> such that for every <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> and every system of at least <img src="https://s0.wp.com/latex.php?latex=C%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="C^k" class="latex" title="C^k" /> sets of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />, there must exist three of them that form a delta system?</p>
<p>I devoted <a href="https://gilkalai.wordpress.com/category/polymath10/"><strong>polymath10</strong></a> to this problem. I tried to promote certain  homological approach. This has not led to progress but did lead to some interesting observations on the problem and also some refinement and better understanding of my approach.</p>
<p>While still open since 2009, there were major breakthroughs regarding the problem that we described <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">here</a> and <a href="https://gilkalai.wordpress.com/2016/05/17/polymath-10-emergency-post-5-the-erdos-szemeredi-sunflower-conjecture-is-now-proven/">here,</a> most notably the problem was <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/"><strong>nearly solved</strong> by Ryan Alweiss, Shachar Lovett, Kewen Wu,  and Jiapeng Zhang</a>.</p>
<p><strong>4. Non-mathematical projects.</strong></p>
<p><strong>4a. Developing a new type of chess-playing program.</strong></p>
<p>The precise formulation was: “how good a chess-playing program is possible if the amount of memory space allowed is very restricted, and the amount of calculation is also limited?” In <a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects/#comment-4000">a comment</a>, Tim explained that one would like to find a method of programming that applied to all games of a certain type (things like Othello, Go, etc.) and not just chess. And this could be taken as the aim. </p>
<p><a href="https://gowers.wordpress.com/2009/09/16/possible-future-polymath-projects/#comment-4006">One comment</a> mentioned the then very recent computer programs for playing Go (based on machine learning). Let me mention that deep learning led to a revolution in this area around 2015.  (And also that we had <a href="https://gilkalai.wordpress.com/2008/06/25/amir-ban-on-deep-junior/">a guest post by Amir Ban</a> on chess playing computer programs.)</p>
<p><strong>4b. The origin of life.</strong></p>
<p>This rather tentative suggestion, was to try to come up with a model that would show convincingly how life could emerge from non-life by purely naturalistic processes. <a href="https://gowers.wordpress.com/2009/11/07/polymath-and-the-origin-of-life/">It was further discussed in this post.</a> There was a lot of excitement around it but it did not lead to a polymath project, and I am not sure about related progress after 2009.</p>
<p>As an aside let me mention that Aubrey de Gray, who famously improved the lower bound on the chromatic number of the plane (that led to <a href="https://dustingmixon.wordpress.com/2018/04/14/polymath16-first-thread-simplifying-de-greys-graph/">polymath16</a>), is very famous for his works and ideas on aging. I suggested to Aubrey, that he should have a “polymath style” project on the issue of aging and his approach to the problem. However Aubrey’s response was that  various attempts to do things like that have failed over the years, across many biological fields. </p>
<p><strong>5. A tentative approach to complexity lower bounds.</strong></p>
<p>This turned into a series of posts (<a href="https://gowers.wordpress.com/2009/09/22/a-conversation-about-complexity-lower-bounds/">first one here</a>, <a href="https://gowers.wordpress.com/category/complexity/">whole “complexity category” here</a>) where Tim tried to develop several ideas related to the <img src="https://s0.wp.com/latex.php?latex=NP+%5Cne+P&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="NP \ne P" class="latex" title="NP \ne P" /> problem.</p>
<p>________</p>
<p>OK, let me now briefly and tentatively preview some suggestions for my future polymath project. </p>
<h2>Possible future Polymath projects (2021)  </h2>
<p>1. <strong><a href="https://gilkalai.wordpress.com/2021/01/10/open-problem-session-of-huji-combsem-problem-5-gil-kalai-the-3%e1%b5%88-problem/">The  3ᵈ conjecture</a> and the flag conjecture for centrally symmetric polytopes</strong>. </p>
<p><strong>2. Mathematical questions regarding social welfare functions. </strong> Here is a  <a href="https://gilkalai.wordpress.com/2009/06/02/social-choice-preview/">related 2009 post</a> and <a href="https://arxiv.org/abs/2012.10352">a survey by Elchanan Mossel</a>.</p>
<p><strong>3. Developing a theory polymath-style</strong></p>



<p><span style="color: #0000ff;">Can a polymath project be used to develop a theory rather than solving a problem? </span>This question was raised by <strong>Peter Sarnak</strong> in a <a href="https://polymathprojects.files.wordpress.com/2011/03/polymathias.jpg">2010 IAS debate about polymath projects</a>.</p>
<p><strong>3.a A theory of convex hulls of real algebraic varieties</strong>.  One project in this spirit that <a href="https://polymathprojects.org/2014/01/20/two-polymath-of-a-sort-proposed-projects/">I already proposed</a> is to try to develop polymathly a theory of convex hulls of real algebraic varieties.</p>
<p><strong>3.b  Extending algebraic shifting to a wide variety of combinatorial structures.</strong> This is a project with Hélène Barcelo from the mid 90s that at the time did not get off the ground and it could be very nice to explore it collectively.</p>
<p>What  other theories would you like to see developed openly and collectively?<span style="color: #ff0000;">*</span></p>
<p><strong>4.</strong> <strong>Touching non overlapping simplices – <a href="https://gilkalai.wordpress.com/2017/08/27/touching-simplices-and-polytopes-perles-argument/">the 2ᵈ-conjecture</a>.</strong></p>
<p><strong>5.</strong>  <strong>Erdős-style combinatorial problems.</strong> Two possibilities are</p>
<p><strong>5.a</strong> <strong>Simonyi’s conjecture;</strong> and  <strong>5.b</strong> <strong>Chvatal’s conjecture</strong>.</p>
<p>Both can be found at the end of <a href="https://gilkalai.wordpress.com/2008/09/28/extremal-combinatorics-iii-some-basic-theorems/">this post</a>. More links, <a href="https://holzman.technion.ac.il/files/2012/09/cancel.pdf">SC1</a>, <a href="https://arxiv.org/abs/1510.07597">SC2</a>, <a href="http://users.encs.concordia.ca/~chvatal/conjecture.html">CC1</a>, <a href="https://arxiv.org/abs/1608.08954">CC2</a></p>
<p><strong>6.</strong> <strong>Enumeration – weights to the rescue  </strong>(sorry for being vague)</p>
<p><strong>7.</strong> <strong>Mathematics with computers </strong>(this is even more vague for now; but see this <a href="https://mathoverflow.net/questions/12085/experimental-mathematics-leading-to-major-advances">MO question; </a>  Here I am thinking about an experimental project. Most of the other projects may have large ingredients of computer experimentation.)</p>
<p><strong>8. Is there a statistical methodology for detecting biased data selection and related statistical follies.</strong>   See <a href="https://gilkalai.wordpress.com/2020/06/26/to-cheer-you-up-in-difficult-times-6-play-rani-sharims-two-player-games-of-life-read-maya-bar-hillel-presentation-on-catching-lies-with-statistics-and-more/">this post</a> and <a href="https://gilkalai.files.wordpress.com/2020/06/ester.ppt">Maya Bar Hillel’s slides,</a> and also <a href="https://windowsontheory.org/2012/05/31/rigged-lottery-bible-codes-and-spinning-globes-what-would-kolmogorov-say/">this post by Omer Reingold</a> on Windows in Theory. </p>
<p><strong>9.</strong> <strong>A problem posed by Oded Schramm:</strong> Is there some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\epsilon &gt;0" class="latex" title="\epsilon &gt;0" /> so that for every <img src="https://s0.wp.com/latex.php?latex=n%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n&gt;1" class="latex" title="n&gt;1" /> there exist a set <img src="https://s0.wp.com/latex.php?latex=K_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="K_n" class="latex" title="K_n" /> of constant width 1 in dimension <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> whose volume satisfies <img src="https://s0.wp.com/latex.php?latex=VOL%28K_n%29+%5Cle+%281-%5Cepsilon%29%5En+V_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="VOL(K_n) \le (1-\epsilon)^n V_n" class="latex" title="VOL(K_n) \le (1-\epsilon)^n V_n" />. (<img src="https://s0.wp.com/latex.php?latex=V_n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="V_n" class="latex" title="V_n" /> is the volume of the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />-dimensional ball of width one.) See <a href="https://mathoverflow.net/questions/29000/volumes-of-sets-of-constant-width-in-high-dimensions">this MO question</a>.</p>
<h3><span style="color: #0000ff;">Meta questions</span></h3>
<p>There are many meta questions regarding polymath projects: </p>
<ol>
<li>One important one is: What is the ideal platform for a polymath project?</li>
<li>Could a polymath project be moderated by a computer?</li>
<li> What are the potential of polymath projects? The limitations? Are polymath projects a good way to do mathematics.</li>
<li>What are the incentives to participate?</li>
<li>Are polymath projects inviting in terms of diversity of participants?  </li>
</ol>
<p><span style="color: #ff0000;">*</span> I wonder if a question “What mathematical theory would you like to see developed” on Math Overflow could stay alive and whether it may lead to nice responses. (There was a nice question about <a href="https://mathoverflow.net/questions/53036/books-you-would-like-to-read-if-somebody-would-just-write-them">what book would you like to see written.</a>) </p>
<p> </p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/"><span class="datestr">at January 29, 2021 09:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5382973292753625821">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/phds-and-green-cards.html">PhDs and Green Cards</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Joe Biden's <a href="https://joebiden.com/immigration/">immigration policy</a> has some interesting policies for PhDs. </p><p></p><blockquote>Biden will exempt from any cap recent graduates of PhD programs in STEM fields in the U.S. who are poised to make some of the most important contributions to the world economy. Biden believes that foreign graduates of a U.S. doctoral program should be given a green card with their degree and that losing these highly trained workers to foreign economies is a disservice to our own economic competitiveness. </blockquote><p>Biden will submit an immigration plan to congress soon but it is not clear if the above will be in the bill, whether it will survive negotiations or even whether an immigration bill will be passed at all. </p><p>Nevertheless I worry about the unintended consequences of this policy. It will encourage students to apply and come for a PhD who have no interest in research, but want the green card. It gives too much power to professors who may abuse their students who need the PhD. Conversely it will pressure professors and thesis committees to grant PhDs because there would be a big difference between graduating with a PhD and leaving early with a Masters. By making the PhD so valuable, we may devalue it.</p><p>The solution is to give green cards to Masters students as well. We shouldn't limit talented researchers and developers who can help the United States keep its technology edge. They don't take jobs away from Americans, but instead help create new ones.</p><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/phds-and-green-cards.html"><span class="datestr">at January 28, 2021 02:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5270">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5270">Research (by others) proceeds apace</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>At age 39, I already feel more often than not like a washed-up has-been in complexity theory and quantum computing research.  It’s not intelligence that I feel like I’ve lost, so much as two other necessary ingredients: <em>burning motivation</em> and <em>time</em>.  But all is not lost: I still have students and postdocs to guide and inspire!  I still have the people who email me every day—journalists, high-school kids, colleagues—asking this and that!  Finally, I still have this blog, with which to talk about all the exciting research that <em>others</em> are doing!</p>



<p>Speaking of blogging about research: I know I ought to do more of it, so let me start right now.</p>



<ul><li>Last night, Renou et al. posted a striking paper on the arXiv entitled <a href="https://arxiv.org/pdf/2101.10873.pdf">Quantum physics needs complex numbers</a>.  One’s immediate reaction to the title might be “well duh … who ever thought it didn’t?”  (See <a href="https://www.scottaaronson.com/blog/?p=4021">this post of mine</a> for a survey of explanations for why quantum mechanics “should have” involved complex numbers.)  Renou et al., however, are interested in ruling out a subtler possibility: namely, that our universe is secretly based on a version of quantum mechanics with real amplitudes only, and that it uses extra Hilbert space dimensions that we don’t see in order to <em>simulate</em> complex quantum mechanics.  Strictly speaking, such a possibility can <em>never</em> be ruled out, any more than one can rule out the possibility that the universe is a classical computer that simulates quantum mechanics.  In the latter case, though, the whole point of Bell’s Theorem is to show that <em>if</em> the universe is secretly classical, then it also needs to be radically nonlocal (relying on faster-than-light communication to coordinate measurement outcomes).  Renou et al. claim to show something analogous about real quantum mechanics: there’s an experiment—as it happens, one involving three players and two entangled pairs—for which conventional QM predicts an outcome that can’t be explained using any variant of QM that’s both local and secretly based on real amplitudes.  Their experiment seems eminently doable, and I imagine it will be done in short order.</li></ul>



<ul><li>A bunch of people from PsiQuantum posted a <a href="https://arxiv.org/pdf/2101.09310.pdf">paper on the arXiv</a> introducing “fusion-based quantum computation” (FBQC), a variant of <a href="https://en.wikipedia.org/wiki/One-way_quantum_computer">measurement-based quantum computation</a> (MBQC) and apparently a new approach to fault-tolerance, which the authors say can handle a ~10% rate of lost photons.  PsiQuantum is the large, Palo-Alto-based startup trying to build scalable quantum computers based on photonics.  They’ve been notoriously secretive, to the point of not having a website.  I’m delighted that they’re sharing details of the sort of thing they hope to build; I hope and expect that the FBQC proposal will be evaluated by people more qualified than me.</li></ul>



<ul><li>Since this is already on social media: apparently, Marc Lackenby from Oxford will be <a href="https://www.math.ucdavis.edu/research/seminars/?talk_id=6082">giving a Zoom talk at UC Davis next week</a>, about a quasipolynomial-time algorithm to decide whether a given knot is the unknot.  A preprint doesn’t seem to be available yet, but this is a big deal if correct, on par with Babai’s quasipolynomial-time algorithm for graph isomorphism from four years ago (see <a href="https://www.scottaaronson.com/blog/?p=2521">this post</a>).  I can’t wait to see details!  (Not that I’ll understand them well.)</li></ul>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5270"><span class="datestr">at January 27, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html">Which induced-subgraph problems are easy, and which are hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest preprint, <a href="https://arxiv.org/abs/2101.09918">arXiv:2101.09918</a> with Sid Gupta and Elham Havvaei, has a mouthful of a title: “Parameterized complexity of finding subgraphs with hereditary properties on hereditary graph classes”. It’s about finding large induced subgraphs with a given property within a larger graph, such as in an earlier paper I wrote on <a href="https://11011110.github.io/blog/2014/08/26/planarization-by-vertex.html">finding large planar induced subgraphs</a>.</p>

<p>Since we’re looking for induced subgraphs, it makes sense to restrict attention to properties that behave nicely under induced subgraphs; these are called hereditary properties. And if there’s no restriction on what kind of graph the larger graph can be, then the parameterized complexity of this induced subgraph problem (parameterized by the number of vertices in the subgraph) is completely settled by a paper by Khot and Raman, “Parameterized complexity of finding subgraphs with hereditary properties” (<em>Theor. Comput. Sci.</em> 2002). The result depends only on whether the subgraphs you’re looking for include arbitrary large cliques or arbitrarily large independent sets. If both, then the answer to “is there a \(k\)-vertex induced subgraph with the property” is (for all sufficiently large inputs) yes, by <a href="https://en.wikipedia.org/wiki/Ramsey's_theorem">Ramsey’s theorem</a>, so the algorithmic problem is easy. If neither, then again Ramsey’s theorem says that (for all sufficiently large \(k\)) the answer is no, so again the problem is easy. And in all remaining cases, when the property includes large cliques but not large independent sets or vice versa, Khot and Raman show that the problem is hard for parameterized computation.</p>

<p>But what if the input is not allowed to be an arbitrary graph, but is restricted to another hereditary class of graphs? An example we consider is the problem of finding large planar induced subgraphs in <a href="https://en.wikipedia.org/wiki/Unit_disk_graph">unit disk graphs</a>. Planar graphs fall into the hard side of Khot and Raman’s dichotomy, but their hardness proof uses graphs that might not be unit disk graphs. Our results involve more case analysis based on cliques and independent sets, but for the input graph class as well as for the target subgraph class, so there are many more cases to consider. Again, Ramsey theory clears away many of them, either by saying that all sufficiently large inputs contain big subgraphs in the target class, or by limiting the number of subgraphs we need to consider to a finite set.</p>

<p>The case analysis from our paper focuses attention on one remaining case, where Ramsey-like arguments do not prevail. This is the case where the target subgraph class is one of the types that is hard for Khot and Raman’s dichotomy (such as finding induced planar subgraphs), and where the input subgraph class can contain both large cliques and large independent sets (as is true for the unit disk graphs). It would be nice to say that in this case everything is hard, providing a nice clean dichotomy, but some problems like this are not hard. For instance, when the input is a <a href="https://en.wikipedia.org/wiki/Cluster_graph">cluster graph</a>, a disjoint union of cliques, the largest induced planar subgraph is obtained by taking at most four vertices from each clique, and many other induced subgraph problems on cluster graphs are equally easy. We were at least able to find a couple of general hardness reductions that apply in many cases, including in the planar-in-unit-disk case. But the question of whether all problems in this case are either easy (fixed-parameter tractable) or hard, and if so whether there is an easy way of determining which side of the dichotomy any particular subgraph-searching problem falls into, remains open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105631932001078385">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html"><span class="datestr">at January 27, 2021 08:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/01/26/assistant-professor-tenure-track-at-university-of-victoria-apply-by-march-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/01/26/assistant-professor-tenure-track-at-university-of-victoria-apply-by-march-1-2021/">Assistant Professor, Tenure-track at University of Victoria (apply by March 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Victoria is seeking applicants for two positions at the Assistant Professor rank with an anticipated start date of July 1, 2021. We are seeking candidates in the areas of:</p>
<p>A. Equity, fairness, accountability, transparency, explainability and privacy B. Theory of Computer Science<br />
C. Machine learning and applications<br />
D. Software development</p>
<p>Website: <a href="https://www.uvic.ca/engineering/computerscience/assets/docs/employment/hiring-ad-non-preferential.pdf">https://www.uvic.ca/engineering/computerscience/assets/docs/employment/hiring-ad-non-preferential.pdf</a><br />
Email: search@csc.uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/01/26/assistant-professor-tenure-track-at-university-of-victoria-apply-by-march-1-2021/"><span class="datestr">at January 26, 2021 10:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/01/26/the-role-of-government/">The role of government</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Is there anything in this world which works well, which is not a series of time-consuming, useless steps inherited from the before-Internet era, of endless waits to speak to someone who behaves like a robot in none of the good ways and all the bad?</p>



<p>I hear <em>linear algebra</em>.</p>



<p>Yes, I think that’s as good as it gets.</p>



<p>If I had one penny for every time I said “V like Victor” over the phone… A game-changing innovation of the unfolding millennium will be a button which, upon pressure, instantly releases all your information to wh(o/at)ever needs to know it. Instead, they now do us the favor of “safeguarding our privacy” by forcing us to enter and re-enter and re-re-renter our information; us who don’t have someone who does it in our stead.</p>



<p>In yet another example of adding insult to injury, much of this is cloaked under “Consumer protection.”  While in fact its effect is precisely the opposite: it is to cut the consumer off the market.  For example, there’s a rule that you must wait three days to close on a loan.  This is supposed to help you sleep over it and think if you really want to go for it and if you can afford it.  How nice of them to help us avoid rushed decisions!  Obviously, sellers don’t want to deal with people subject to such delays.  If one really wanted to help, the rule should be that <em>everybody</em> has to wait three days to complete a transaction, regardless if it’s cash or financed, so that those for whom the transaction is more significant than drinking a tea can indeed sleep over it.</p>



<p>More absurdities.  Person A currently has loan with interest rate X.  Now interests have been set to Y &lt; X.  However, A cannot refinance their loan because A does not have enough cash reserves (or some other suspicious condition).  So A will be forbidden from refinancing and will be forced to keep their loan at old, unfavorable interest rate X &gt; Y.</p>



<p>Question: Under which interest rate is A more likely to default?</p>



<p>The only possible deduction is that the lender wants A to default.  You can’t say that by refinancing the lender will take a greater risk.  They are just using this loophole to make more money.  Wasn’t this supposed to help citizens?  This situation has another name as well.</p>



<p>Special torture awaits serial debtors.  Another loan in effect can delay processing by… <em>months</em>.  In the meanwhile, the documents for the new loan will expire, and re-expire, and re-re-expire yet again, while the hapless debtor will be forced to continuously work to keep them up to date — similar to what happens to Sisyphus or the Danaides, but the classics weren’t perverse enough to conceive the endless task of collecting documents from disconnected websites, each with its own distinctive and rapidly-mutating user interface, each requiring a new password or dual-factor authentication at each download, each in turn disappearing and being replaced by yet another system hosted by yet another party which will require yet another type of authentication, and so on forever — do you want this money or not?</p>



<p>There is a (probably well-known) quote which I read in <em>The Instinct for Cooperation : A Graphic Novel Conversation with Noam Chomsky</em> (not a terrible read — I fittingly found it in a free exchange library and returned it to another) which stuck with me.  I don’t recall it exactly (see, I shouldn’t have returned it), and I can’t find it online precisely, but it basically says that “<strong>The role of the government is to protect property from those who don’t have it.</strong>” (An online search seems to reveal that something along these lines was expressed by James Madison.)  Regardless of whether it was said or not, and regardless of whether you agree with it or not, it is quite useful to keep this quote in mind, because it explains much of the world we live in.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/01/26/the-role-of-government/"><span class="datestr">at January 26, 2021 02:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6340814883494512723">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/answers-to-prez-quiz-and-some.html">Answers to the Prez Quiz, and some interesting history</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>I posted a presidential quiz (that is, a quiz about presidents, not a quiz that is so majestic it can be called presidential) on Thursday Jan 21.</p><p>Here is the link to the post: <a href="https://blog.computationalcomplexity.org/2021/01/presidential-quiz-three-ways-to-take-it.html">The Post </a></p><p>Here is the link to the quiz:<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/prezquiz.pdf">The Prez Quiz</a></p><p> I now post the answers here: <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/prezquizsol.pdf">The Prez Quiz Answers</a></p><p><br /></p><p>Looking back at the quiz and some prior ones I realize that some questions are TRIVIA while others are not- that is- they tell us something interesting beyond the answer. Some thoughts on that: </p><p><br /></p><p>a) Asking who the oldest prez ever has to be better defined. But however you define it, the answer is Joltin Joe. I will ask it as Age-when-sworn-in, which for Joltin Joe is 78. Second is Trump who was 70 when sworn in.  INFO: Presidents seem to be getting older. Is this a trend or will 2024 and 2028 feature younger folks? I note that both VP's are in their 50's. </p><p>b) Presidential first via xkcd; <a href="https://xkcd.com/2383/">here</a></p><p>c) Between Nixon-Kennedy 1960 and Bush-Kerry 2004 EVERY election had one of he candidates being either a former or current Prez or Vice Prez.  (This was a question on the 2008 quiz, but since the streak is broken it is no longer as interesting.) INFO: It was hard to break into the prez business unless you were already somewhat known. Having said that, note that some of the non-VPs and non-Prezs DID win. </p><p>d) Between 1948 and 2008 at least one of the candidates for Prez had served in the Military. How did they do? In many cases both served so I am not going to to a chart of how well the Military ones did. (This was a question on the 2012 quiz, but since the streak is broken it is no longer interesting.) INFO: At one time a lot of people were in the military. At one time a lot of people knew someone in the military. Now it seems like the military is rather far removed from civilians. </p><p>e) Kennedy was our first Catholic Prez in 1960. Biden is our second one in 2020. My impression is that it was an issue for the Kennedy Campaign but not for the Biden campaign. Romney being a Mormon seems to have not been an issue in the General election, but some of the other candidates brought it up in the primaries. INFO: Religious bigotry within different denominations of Christianity is declining. </p><p>f) Reagan was our first divorced Prez, in 1980. Trump was our second in 2016. It don't think it was  an issue for either one. INFO: We need to couple this with Rockefellers divorce being a problem for him getting the nomination in 1964. Peoples attitude on divorce has changed A LOT. </p><p><br /></p><p>Contrast: Knowing which president had a fictional street gang named after him  does not tell us anything about History .I will remove that question when I do the quiz in 2025. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/answers-to-prez-quiz-and-some.html"><span class="datestr">at January 26, 2021 06:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html">Bracing squaregraphs (and other rhombus tilings)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A bookshelf or other structure made only of vertical and horizontal beams will easily fall over unless its joints are very strong, because axis-parallel structures are not inherently <a href="https://en.wikipedia.org/wiki/Structural_rigidity">rigid</a>. The vertical beams can tilt away from being perpendicular to the horizontals, without changing the relative spacing of the joints, and that can lead to books all over the floor. Here’s an example <a href="https://www.si.edu/newsdesk/photos/earthquake-damage-botany-library">from a 2011 earthquake at the Smithsonian</a> that didn’t get quite that far, but did damage some shelves beyond repair:</p>

<p style="text-align: center;"><img width="65%" style="border-style: solid; border-color: black;" alt="Damaged bookshelves in the Botany and Horticulture Library at the Smithsonian's National Museum of Natural History after the August 2011 Virginia earthquake, photographed by James DiLoreto, from https://www.si.edu/newsdesk/photos/earthquake-damage-botany-library" src="https://11011110.github.io/blog/assets/2021/nhb2011-01160-botany_library.jpg" /></p>

<p>Triangular bookshelves would be annoying for different reasons, but they wouldn’t collapse like this, because triangular structures are more rigid: there is only one triangle that three sides of fixed lengths can form, unlike squares and rectangles which can flex into rhombi and parallelograms without changing their side lengths. So the standard solution to the bookshelf rigidity problem is to add <a href="https://en.wikipedia.org/wiki/Cross_bracing">cross bracing</a>, diagonal beams built into shelves or other rectangular structures that make them similarly rigid. But which parts to brace? Even if every vertical and horizontal segment of a square bookshelf is allowed to flex independently of the other ones on its same line, bracing all the squares of the bookshelf would be too many. The vertical segments in each row of squares must stay parallel (in any flex small enough to avoid self-crossings), and similarly the horizontal segments in each column of squares must stay parallel. So a square grid with \(r\) rows and \(c\) columns has only \(r+c-1\) degrees of freedom in its shape (the relative angles among the verticals of each of the \(r\) rows and the horizontals of each of the \(c\) columns), far fewer than the \(rc\) braces that would be obtained by bracing all the squares.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/grid-flex.svg" alt="Degrees of freedom of a flexible square grid" /></p>

<p>A beautiful theory showing how to minimally brace any square (or rectangular) grid, connecting this <a href="https://en.wikipedia.org/wiki/Grid_bracing">grid bracing problem</a> to the connectivity of bipartite graphs, was developed in the late 1970s and early 1980s by Ethan Bolker, <a href="https://en.wikipedia.org/wiki/Henry_Crapo_(mathematician)">Henry Crapo</a>, <a href="https://en.wikipedia.org/wiki/Jenny_Baglivo">Jenny Baglivo</a>, and Jack Graver.<sup id="fnref:bc"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:bc" class="footnote">1</a></sup> <sup id="fnref:bg"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:bg" class="footnote">2</a></sup> Their idea was to represent the grid and its bracing more abstractly, as a graph, whose vertices represent the rows and columns of grid squares. A braced square can be represented as an edge in this graph:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/grid-bracing.svg" alt="Representing a braced square grid as a graph" /></p>

<p>An edge in this graph, or more generally any chain of edges, fixes the relative angles between the vertical edges in the rows that it connects, and the horizontal edges of the columns. So if the whole graph is connected, as it is in the figure, all of these angles are fixed, and the grid is made rigid. In particular, any spanning tree of the complete bipartite graph on the rows and columns (such as the spanning tree shown in the figure) provides a bracing pattern that will suffice to make the grid rigid. It is also necessary for rigidity that the bracing pattern contain a spanning tree: any set of braces including a cycle is redundant, with one of the braces removable without changing the possible motions of the braced grid, and any forest that is not a tree has too few braces to eliminate all of the degrees of freedom. So a grid is braced rigidly if and only if its graph is connected, and the minimal rigid bracings are exactly the spanning trees.</p>

<p>The same theory can be (and has been) generalized in multiple ways. A grid is “double braced” if every brace is redundant: you can remove any one brace and the whole structure will stay rigid. This is true if and only if the corresponding graph has the property that you can remove any one edge and the whole graph will remain connected, which is true exactly for the connected <a href="https://en.wikipedia.org/wiki/Bridge_(graph_theory)">bridgeless graphs</a>. A version with directed graphs applies to bracing by strings or wires, strong under tension but useless under compression. These constrain the sides of the squares from turning only in one direction: if one side turns clockwise, the other side is forced to turn clockwise, or vice versa, but not both ways. So we can represent a tension brace in a square by a directed edge. The edge is directed from the square’s row to its column when a clockwise turn of the vertical sides in the row would force a clockwise turn of the horizontal sides in the column, and in the other direction otherwise. Then, the whole structure is rigid under this sort of tension bracing if and only if the resulting directed graph is strongly connected.</p>

<p>For this theory to work, it is necessary that the braced structure consist of quadrilaterals with parallel sides, but they don’t have to be squares: the same thing works for grids of rectangles, rhombi, or parallelograms. It would work for the deformed square grid in the right of the second figure, for instance. It’s also not necessary for the quadrilaterals to be connected in the pattern of a square grid. In 2006, Ture Wester observed that the same method should work for the rhombic version of the Penrose tiling, for instance, but was very vague about boundary conditions (not distinguishing carefully between infinite tilings and finite patches of them).<sup id="fnref:w"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:w" class="footnote">3</a></sup> The correct boundary conditions are that this works for any tiling of a disk in the plane by finitely many parallel-sided quadrilaterals. For such tilings, one can group the quadrilaterals into zones, sequences of quadrilaterals connected edge-to-edge on opposite sides, so that each quadrilateral belongs to two zones (giving it a zone by itself if two opposite sides are both on the boundary of the disk). Without bracing, the shared parallel sides in each zone can turn independently of any other motion of the tiling; this is the part that requires that the tiled area be a disk rather than an annulus or more complicated shape, so that the two parts of the graph separated by each zone are disconnected from each other and can move independently. And the overall shape of the tiling is completely determined by the angles of the shared sides of all of its zones. Therefore, the number of degrees of freedom (factoring out motions in which the tiling translates without rotation, or rotates rigidly) is exactly one less than the number of zones. Any set of braced quadrilaterals can be represented as a subgraph of the intersection graph of the zones, and after the calculation of degrees of freedom, the same argument as for grids shows that the tiling is rigid if this subgraph connects all the zones, that it is double braced if this subgraph is connected and bridgeless, or that it is tension braced if the directed version of this subgraph is strongly connected.</p>

<p>So as Wester stated, this does work for rhombic Penrose tilings, as long as one considers disk-shaped patches of the tilings. Similar, it works for (simply-connected) polyominoes, or disk-shaped patches of the <a href="https://en.wikipedia.org/wiki/Rhombille_tiling">rhombille tiling</a>. It also works for <a href="https://en.wikipedia.org/wiki/Squaregraph">squaregraphs</a>, planar graphs in which all interior vertices have degree at least four and all interior faces are quadrilaterals, as long as those quadrilaterals are drawn with parallel sides. One of my old papers shows that these drawings always exist,<sup id="fnref:e"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:e" class="footnote">4</a></sup> and another shows how to optimize them to avoid sharp angles.<sup id="fnref:ew"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:ew" class="footnote">5</a></sup>. In all of these cases, the tiling can be made rigid by bracing tiles forming a spanning tree of its zones, doubly rigid by bracing tiles forming a bridgeless spanning subgraph, or rigid for tension bracing by adding braces that form a strongly connected graph.</p>

<p>However, some algorithmic aspects of this theory may depend more strongly on the fact that for square grids, the intersection graphs of their zones have a very simple structure. Square grids have complete bipartite zone intersection graphs, but more general tilings of disks by parallel-sided quadrilaterals have arbitrary <a href="https://en.wikipedia.org/wiki/Circle_graph">circle graphs</a> and squaregraphs have triangle-free circle graphs. Extending a partial bracing to a minimal rigid bracing is just a matter of extending a forest to a spanning tree, easy in all graphs, but the corresponding problem for tension bracing is more complicated. Gabow and Jordán solve it for square grids (equivalently, adding as few edges as possible to a bipartite directed graph to make it strongly connected, while respecting its given bipartition) in linear time.<sup id="fnref:gj"><a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fn:gj" class="footnote">6</a></sup> But it is not at all obvious that it’s as easy to extend a directed subgraph of a circle graph to a minimal strongly connected subgraph of the same circle graph.</p>

<div class="footnotes">
  <ol>
    <li id="fn:bc">
      <p>Bolker, Ethan D., and Crapo, Henry (1977), “How to brace a one-story building”, <em>Environment and Planning B: Planning and Design</em> 4 (2): 125–152, <a href="https://doi.org/10.1068/b040125">doi:10.1068/b040125</a>. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:bc" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:bg">
      <p>Baglivo, Jenny A., and Graver, Jack E. (1983), “3.10 Bracing structures”, <em>Incidence and Symmetry in Design and Architecture</em>, Cambridge University Press, pp. 76–87. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:bg" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:w">
      <p>Wester, Ture (2006), “<a href="http://new.math.uiuc.edu/oldnew/quasicrystals/papers/TureWester.pdf">The structural morphology of Penrose and quasicrystal patterns, part I</a>”, <em>Adaptables2006, TU/e, International Conference On Adaptable Building Structures, Eindhoven, The Netherlands, 3–5 July 2006</em>, 10-290. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:w" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:e">
      <p>Eppstein, David (2004), “Algorithms for drawing media”, <em>Proc. 12th Int. Symp. Graph Drawing</em>, Springer, LNCS 3383, pp. 173–183, <a href="https://arxiv.org/abs/cs.DS/0406020">arXiv:cs.DS/0406020</a>. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:e" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:ew">
      <p>Eppstein, David, and Wortman, Kevin (2011), “Optimal angular resolution for face-symmetric drawings”, <em>J. Graph Algorithms and Applications</em> 15 (4): 551–564, <a href="https://doi.org/10.7155/jgaa.00238">doi:10.7155/jgaa.00238</a>. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:ew" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:gj">
      <p>Gabow, Harold N., and Jordán, Tibor (2000), “How to make a square grid framework with cables rigid”, <em>SIAM Journal on Computing</em> 30 (2): 649–680, <a href="https://doi.org/10.1137/S0097539798347189">doi:10.1137/S0097539798347189</a>. <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html#fnref:gj" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105603974591107625">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/01/22/bracing-squaregraphs.html"><span class="datestr">at January 22, 2021 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/01/22/oxford-warwick-postdoc-in-complexity-theory-at-university-of-oxford-university-of-warwick-apply-by-february-28-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/01/22/oxford-warwick-postdoc-in-complexity-theory-at-university-of-oxford-university-of-warwick-apply-by-february-28-2021/">Oxford-Warwick Postdoc in Complexity Theory at University of Oxford / University of Warwick (apply by February 28, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Rahul Santhanam (Oxford) and Igor Oliveira (Warwick) invite expressions of interest for a joint postdoctoral position in computational complexity theory.<br />
Funding is available for 20 months, with the possibility of an extension. The start date is negotiable.<br />
To be considered, please send an email to Rahul Santhanam and Igor Oliveira with your CV &amp; Publications. Informal enquiries are welcome.</p>
<p>Website: <a href="https://www.dcs.warwick.ac.uk/~igorcarb/complexity-meetings.html">https://www.dcs.warwick.ac.uk/~igorcarb/complexity-meetings.html</a><br />
Email: igorcarb@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/01/22/oxford-warwick-postdoc-in-complexity-theory-at-university-of-oxford-university-of-warwick-apply-by-february-28-2021/"><span class="datestr">at January 22, 2021 07:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1995922000817154646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html">Alan Selman (1941-2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p></p><span style="text-align: center;"><table cellpadding="0" align="center" style="margin-left: auto; margin-right: auto;" cellspacing="0" class="tr-caption-container"><tbody><tr><td style="text-align: center;"><a style="margin-left: auto; margin-right: auto;" href="https://1.bp.blogspot.com/-4M0SuOy0o4s/X_y99qhI9aI/AAAAAAAB6E0/ZRefljNynqkOfgg4Kj3zCOk-dPyEp-TKACPcBGAsYHg/s2258/00000429.jpg"><img width="550" src="https://1.bp.blogspot.com/-4M0SuOy0o4s/X_y99qhI9aI/AAAAAAAB6E0/ZRefljNynqkOfgg4Kj3zCOk-dPyEp-TKACPcBGAsYHg/w400-h174/00000429.jpg" border="0" height="246" /></a></td></tr><tr><td style="text-align: center;" class="tr-caption">From a 1994 Dagstuhl Workshop. Selman is the one wearing a cap.</td></tr></tbody></table></span><div>Alan Selman, one of the early leaders in structural complexity and the co-founder and first chair of what is now the Computational Complexity Conference, passed away this morning. He was one of the true greats in our field both for his research and his service.<p></p><p>Selman was a good colleague and a friend. I hosted his sabbatical at the University of Chicago in 1998 which <a href="https://doi.org/10.1007/s00224-001-0003-0">produced a paper</a> with Selman's student and my later postdoc Pavan Aduri. In Spring of 1999 Selman ran a NSF workshop in Chicago on <a href="https://cse.buffalo.edu/~selman/report/Report.html">Challenges for Theory of Computing</a>.</p><p>In a desire to create a community of complexity theorists and foster publications of their results, Selman led the efforts to establish the Structure in Complexity Theory conference, first held in 1986 in Berkeley, California. As a student in Berkeley I attended that meeting and the next twenty-five. The conference would join the IEEE in 1987, in 1996 renamed the IEEE Conference and Computational Complexity and in 2015 drop from the IEEE to become the Computational Complexity Conference, still going strong. Alan served as the first conference chair and as PC co-chair of the first meeting. </p><p>For all his service for the field, Selman received the ACM SIGACT Distinguished Service Award in 2002.</p><p>Selman joined University at Buffalo in 1990 to serve six years as department chair and then as a professor until he retired in 2014. Lane Hemaspaandra wrote a highly recommended <a href="https://arxiv.org/abs/1406.4106">appreciation</a> for Selman and his research in honor of his retirement where he mentions some of Selman's research programs, including his seminal work on P-Selective sets, non-deterministic functions and his work with Joachim Grollman on one-way functions. Selman came down hard on anyone who got the wrong number of l's in Grollman, so we would often joking cite it as Grollman and Sellman or just Grollman et Al.</p><p>In my <a href="https://doi.org/10.1137/S0097539794268315">favorite Selman paper</a>, in work with Hemaspaandra, Ashish Naik and Mitsu Ogihara, looks at witness reduction. If there is a set A in P such that for all satisfiable formula φ there is a unique satisfying assignment a of φ such that (φ,a) is in A then the polynomial-time hierarchy collapses. The more general question of whether NP=UP implies the polynomial-time hierarchy collapses <a href="https://blog.computationalcomplexity.org/2003/12/does-npup.html">remains open</a>.</p><p>Also <a href="https://doi.org/10.1016/0022-0000(92)90023-C">Selman's paper with Steve Homer</a> giving an oracle where Σ<sub>2</sub><sup>P</sup>-complete sets were isomorphic was instrumental to my own work on the isomorphism conjecture.</p><p>Bill wanted me to mention the Selman paper that most influenced him. Selman and Theodore Baker and <a href="https://doi.org/10.1016/0304-3975(79)90043-4">gave the first oracle</a> separating the second level of the polynomial-time hierarchy in 1979. It would take <a href="https://doi.org/10.1109/SFCS.1985.49">Yao</a> and <a href="https://doi.org/10.1145/12130.12132">Håstad</a> over five more years to get the whole hierarchy infinite relative to an oracle. The Baker-Selman proof could easily extend to show that AM games did not sit in Σ<sub>2</sub><sup>P</sup>, a bit surprising as MA is in Σ<sub>2</sub><sup>P</sup>.</p><p>In addition to his research, Selman wrote an <a href="https://amzn.to/3oBKWsD">introductory theory textbook</a> with Homer as well as <a href="https://amzn.to/38zspaQ">editor</a> and <a href="https://amzn.to/3qeA8kj">co-editor</a> of two editions of Complexity Theory Retrospective.</p><p>It would be no exaggeration to say the field of computational complexity and my own research within it would have been much different without Alan.</p><p>Alan drove a car with a "P NE NP" license plate. One day someone came up to him in a parking lot and said "Yes, but can you prove it?" Possibly the one thing in complexity he couldn't do.</p></div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/alan-selman-1941-2021.html"><span class="datestr">at January 22, 2021 04:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5862964813565889129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/presidential-quiz-three-ways-to-take-it.html">Presidential Quiz:  Three ways to take it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Every four years around the time of the inaugural I post a presidential Quiz! I do that today, and I will post the answers on Monday.  I am tempted to joke that I am posting it AFTER Joltin Joe gets sworn in just in case something happens; however, I always posted it after the prez is sworn in. </p><p>The quiz is here: <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/prezquiz.pdf">quiz</a></p><p><b> GRADING CRITERIA</b></p><p>33 questions, 3 points each, and 1 free point.</p><p>If the answer is a list of L elts and you get x correct, you get x/L points. If any are wrong then 0 points.</p><p>You can take the quiz one of three ways.</p><p>1) Take it WITHOUT using the web and see how many you can get right. Take 3 hours.</p><p>WARNING- its a hard quiz so this may not be fun. </p><p>2) Take it and use the web and try to do it fast. Stop when you want. Your Score is as follows:</p><p>If  R is the number if points and  T be how many minutes you took,  your score is </p><p> (180R/T)+ 1.</p><p>If  you get all 33 right in 60 minutes then you get a 100. You could get more than 100 if you do it faster.</p><p>3) The answer key has all kinds of other information in it and is fun to read. So do not take the quiz and enjoy  reading the answers on Monday. That's what I did. </p><p><br /></p><p>I was curious if Joltin Joe would be sworn in by Joseph or Joe. I was routing for Joe so he would be the second president sworn in by his nickname (Jimmy Carter was the first--his real name is James). When I am sworn in as president I will use <i>Bill</i> and hence join the nickname-club. Lance does not have a nickname, so my veep will be sworn in by his actual name. Would we win? Is  America ready for a 2-Phd ticket? </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/presidential-quiz-three-ways-to-take-it.html"><span class="datestr">at January 22, 2021 05:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5264">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5264">Sufficiently amusing that I had no choice</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">BREAKING: President Biden signs executive order banning people from saying “Quantum computers solve problems by just trying all possible solutions in parallel” <a href="https://t.co/zeAZvmKA1T">pic.twitter.com/zeAZvmKA1T</a></p>— Olivia Lanes (@Liv_Lanes) <a href="https://twitter.com/Liv_Lanes/status/1352350146881867781?ref_src=twsrc%5Etfw">January 21, 2021</a></blockquote>
</div></figure></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5264"><span class="datestr">at January 22, 2021 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/01/21/science-advisor/">Science Advisor</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Resources for a new term from our vantage point</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/01/lander.jpg"><img width="135" alt="" src="https://rjlipton.files.wordpress.com/2021/01/lander.jpg?w=135&amp;h=157" class="alignright wp-image-18017" height="157" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Crop from Broad Institute <a href="https://www.broadinstitute.org/bios/eric-s-lander">src</a></font></td>
</tr>
</tbody>
</table>
<p></p><p>
Eric Lander has been <a href="https://buildbackbetter.gov/wp-content/uploads/2021/01/OSTP-Appointment.pdf">appointed</a> director of the US Office of Science and Technology Policy, a post newly elevated to Cabinet status. He is thus the top science <a href="https://www.sciencemag.org/news/2021/01/biden-appoints-geneticist-eric-lander-science-advisor">advisor</a> in Joe Biden’s cabinet.</p>
<p>
Today we congratulate Eric, whom we both have known personally, and survey some sources of science advice at his career’s roots.</p>
<p>
Peter Cameron, who was Eric’s doctoral advisor at Oxford University, says in his <a href="https://cameroncounts.wordpress.com">blog</a> that: </p>
<blockquote><p><b> </b> <em> I have had so many congratulatory emails that it almost seems as if I have done something good myself <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /> </em>
</p></blockquote>
<p>
I found out about Eric’s appointment while working on this post. Eric is a longtime friend, who while not close is someone that I have always been impressed with. He is a mathematician who turned molecular biologist—we interacted when I worked on <a href="https://science.sciencemag.org/content/268/5210/542">DNA computing</a>. Eric previously worked closely with Biden during the Barack Obama presidency. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<p>
<a href="https://rjlipton.wordpress.com/2021/01/21/science-advisor/landerobamabiden/" rel="attachment wp-att-18026"><img width="540" alt="" src="https://rjlipton.files.wordpress.com/2021/01/landerobamabiden.jpg?w=540&amp;h=200" class="aligncenter wp-image-18026" height="200" /></a>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Obama archives <a href="https://obamawhitehouse.archives.gov/blog/2017/01/09/celebrating-contributions-presidents-council-advisors-science-and-technology">source</a> (note Biden next to Obama at left, Lander across from Obama)</font>
</td>
</tr>
</tbody></table>
<p></p><p><br />
Ken found out first from reading his Princeton Class of 1981 news on Facebook. He says: </p>
<blockquote><p><b> </b> <em> Eric was my Resident Advisor in Foulke Hall my freshman year, 1977–78. I originally had the ground-floor double room next to his, but I swapped roommates one door further down with another lover of mathematics and Broadway musicals. His RA group of 16 in the staircase included now-Justice Elena Kagan. Eric brought all of us down for meetings in the early weeks; blessing the roommate swap and discussing compatibility aspects in general was a subject of one of them. I got to Oxford just as Eric was leaving, and my first interaction with Peter Neumann was actually Neumann saying Eric had neglected to do something and could I get him in touch before he returned to the US? Kagan also came to Oxford along with three other ’81-ers whom I associated with more. </em>
</p></blockquote>
<p>
</p><p></p><h2> A Wellspring of Advice </h2><p></p>
<p></p><p>
Eric did a swap himself his first weeks in Oxford. As Cameron relates in a followup <a href="https://cameroncounts.wordpress.com/2021/01/19/family/">post</a>:</p>
<blockquote><p><b> </b> <em> He arrived in Oxford on a Rhodes Scholarship to do research in algebraic topology. But before the term started, he had changed his mind and decided to do coding theory instead. By chance, it happened that I was the only person in Oxford at the time who claimed to know anything about coding theory. <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /> So I got to supervise Eric. </em>
</p></blockquote>
<p></p><p>
Peter had a good problem ready about how to combine two strands of applying design theory to codes.</p>
<blockquote><p><b> </b> <em> Eric, with a good background in algebraic number theory, took to it immediately. The coding theorists had taken an integer matrix and considered its row space over the finite field with <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> elements. Eric observed that, if instead you took the row space over the <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" />-adic numbers, then you could reduce it modulo arbitrary powers of <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" />, and get a chain of codes, with the property that duality reversed the order in the chain (so if the power of <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> involved was odd then the code in the middle was self-dual).</em></p><em>
</em><p><em>
Eric’s thesis took off from there, and at the end he turned it into a book with the title <em>Symmetric Designs: An Algebraic Approach</em>. </em>
</p></blockquote>
<p></p><p>
Of course, coding theory and “stringology” feed into genomics, and from there into the broad reach of science. Eric has no shortage of sources across the spectrum. But the wellspring of his career was our general area, not to mention that both of us followed him in several geographic and academic <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/2003-04/dna-computing/history.htm">respects</a>. </p>
<p>
Many of our peer bloggers step out to give general advice on science. Some conspicuously often take their advice and recommendations on natural sciences openly national. Others address it within smaller communities, but their ideas can be equally valuable. All may be worthy of consultation.</p>
<p>
</p><p></p><h2> Blogs </h2><p></p>
<p></p><p>
The one rule we’re setting out now is that in order to be advising the advisor, the blogs must stay current. There are tons of terrific blogs that have stopped publishing altogether, or whose last <em>new</em> post is many moons ago. As for those who try to be reasonably current, Ken and I know how hard it is to do. So we took a three-week horizon—that is, who has posted something this year, 2021. That said, here are some of our favorite blogs on math and CS theory—after the first they are alphabetical by writer(s).</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://terrytao.wordpress.com/tag/model-theory/">What’s New</a> Terence Tao <br />
The best math blog of all time. If you must read one, then this is the one. If you read two or more math blogs, then this is still the one. If you read two or more posts on this blog, then you qualify as a mathematician.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://www.scottaaronson.com/blog/">Shtetl Optimized</a> Scott Aaronson <br />
Wonderful blog. A brilliant combination of results, comments, and opinions. We have “encoded” his name in the previous section—see if you can spot where and how.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://francisbach.com">Machine Learning Research Blog</a> Francis Bach <br />
Focused on connections between optimization and learning. I conjecture that the key to solving some of our deep questions—P<img src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;}" class="latex" title="{&lt;}" />NP—could be resolved by machine-learning technology. I recall a year ago while talking with learning experts at Chicago that they said: We think SAT should have an algorithm that runs in <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bc%5Csqrt+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{c\sqrt n}}" class="latex" title="{2^{c\sqrt n}}" /> time for some <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" title="{c}" />.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://johncarlosbaez.wordpress.com/">Azimuth</a> John Carlos Baez <br />
Often goes into physics and policy, such as today’s third <a href="https://johncarlosbaez.wordpress.com/2021/01/21/us-environmental-policy-part-3/">post</a> in a series on environmental policy and climate change. But <a href="https://johncarlosbaez.wordpress.com/2021/01/17/categories-of-nets/">two</a> recent <a href="https://johncarlosbaez.wordpress.com/2021/01/20/categories-of-nets-part-2/">posts</a> were on Petri nets and higher abstractions of them.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" title="{\bullet}" /> <a href="https://windowsontheory.org/">Windows on Theory</a> Boaz Barak <br />
This was originally a joint blog by researchers at the Microsoft Silicon Valley Research Center before it <a href="https://rjlipton.wordpress.com/2014/09/27/microsoft-closes-svc/">closed</a> in 2014. Last week’s <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">post</a> is also on machine learning and theory.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://micromath.wordpress.com">Mathematics under the Microscope</a> Alexandre Borovik <br />
See his <a href="https://www.openbookpublishers.com/10.11647/OBP.0168.pdf">book</a> with Tony Gardiner, <em>The Essence of Mathematics</em>. The spirit of the book is seen in this quote from George Pólya: <i>It is better to solve one problem in five different ways than to solve five problems in one way.</i></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://agtb.wordpress.com">Turing’s Invisible Hand</a> Felix Brandt, Michal Feldman, Jason Hartline, Bobby Kleinberg, Kevin Leyton-Brown, Noam Nisan, Vijay Vazirani <br />
They have an annotated version of John Nash’s 1955 <a href="https://agtb.wordpress.com/2012/02/17/john-nashs-letter-to-the-nsa/">letter</a> to the NSA about the complexity of crypto, which beat Kurt Gödel’s “lost letter” by a whole year. As we <a href="https://rjlipton.wordpress.com/2012/04/01/introducing-techplex-com/">joked</a> on 4/1/12, if we had known about it, GLL would have been NLL.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://cameroncounts.wordpress.com">Peter Cameron’ Blog</a> Peter Cameron <br />
This is where I found out about the appointment of Lander to Biden’s cabinet. Peter is at St. Andrews and emeritus from Queen Mary University of London. Ken wrote about him in his <a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/">memorial</a> for Peter Neumann.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://quomodocumque.wordpress.com">Quomodocumque</a> Jordan Ellenberg <br />
He <a href="https://quomodocumque.wordpress.com/2021/01/15/am-i-supposed-to-say-something-about-the-invasion-of-the-united-states-capitol/">asks</a>: Am I Supposed To Say Something About The Invasion Of The United States Capitol? He does. We haven’t. (We are mulling a post on quantitative matters from the pandemic and election that have become political footballs.)</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://11011110.github.io/blog/">11011110</a> David Eppstein—the blog name is his initials DE in hexadecimal and his surname has a double-p.<br />
Right now he has a wonderful <a href="https://11011110.github.io/blog/2021/01/15/linkage.html">list</a> of open questions and known results. For example there is a discussion of USA flag arrangements, and also the recent claimed solution of an almost 50 year old conjecture: A proof of the Erdős-Faber-Lóvasz <a href="https://arxiv.org/abs/2101.04698">conjecture</a> by Dong Yeap Kang, Tom Kelly, Daniela Kuhn, Abhishek Methuku, Deryk Osthus.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://explainingmaths.wordpress.com">Explaining mathematics</a> Joel Feinstein <br />
He asks: When proving there exists statements, is it enough to give just one example or do you have to prove it using the definitions, and so on? <a href="https://explainingmaths.wordpress.com/2021/01/09/proving-there-exists-statements/">Read on</a> for more.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://blog.computationalcomplexity.org">Computational Complexity</a> Lance Fortnow and Bill Gasarch <br />
The CS theory blog that started it all. Continues to be one of the top blogs. Lance’s <a href="https://blog.computationalcomplexity.org/2021/01/the-ethics-board.html">post</a> on Tuesday says that “the way of most suggestions I make in my blog [is] a quick road to nowhere,” but the one in that post went somewhere.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="http://jdh.hamkins.org">logic and more</a> Joel Hamkins <br />
He discusses the math tea argument—one heard at an afternoon tea. I miss these very much, even though I do not drink tea. The argument is: There must be some real numbers that we cannot define, since there are uncountably many real numbers, but only countably many definitions. Is it correct? <a href="http://jdh.hamkins.org/definability-and-the-math-tea-argument-warsaw-22-january-2021/">Read on</a> about the talk he is giving tomorrow “in” Warsaw for an explanation.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://gilkalai.wordpress.com">Combinatorics and more</a> Gil Kalai <br />
Gil’s wonderful blog is a great place to see announcements of new results. He’s had a year-long series, “To cheer you up in difficult times”; its 18th <a href="https://gilkalai.wordpress.com/2021/01/19/what-if-they-are-all-wrong/">installment</a> links to a wonderful, thoughtful, entertaining, and provocative post by Igor Pak about conjectures. The 17th <a href="https://gilkalai.wordpress.com/2021/01/14/to-cheer-you-up-in-difficult-times-17-amazing-the-erdos-faber-lovasz-conjecture-for-large-n-was-proved-by-dong-yeap-kang-tom-kelly-daniela-kuhn-abhishek-methuku-and-deryk-osthus/">installment</a> was about the Erdős-Faber-Lóvasz news. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="http://m-phi.blogspot.com">M-Phi</a> Many authors <br />
All of the recent entries have been by Richard Pettigrew but they have a long list of previous contributors. The recent posts catch Ken’s eye because they employ the <a href="https://en.wikipedia.org/wiki/Brier_score">Brier score</a> to reason philosophically about inaccuracy. Ken has employed his group’s novel <a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/">adaptation</a> of the Brier score in chess cheating cases all through the pandemic. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://dustingmixon.wordpress.com">Short, Fat Matrices</a> Dustin Mixon <br />
He discusses a problem by Mario Krenn. The <a href="https://mariokrenn.wordpress.com/graph-theory-question/">problem</a> has consequences for quantum computation—and comes with cash prizes—one is €3,000.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://vzn1.wordpress.com/">Turing Machine</a> VZN <br />
Just nipped under with a New Year’s Day <a href="https://vzn1.wordpress.com/2021/01/01/collatz-search-for-overarching-strategy/">post</a> on the Collatz <img src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3n+1}" class="latex" title="{3n+1}" /> conjecture.  Ken used to know VZN’s full name but can’t find it now.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://noncommutativeanalysis.wordpress.com">Noncommutative Analysis</a> Orr Shalit <br />
The issues here are important to quantum computation, since for operators <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" title="{B}" />, <img src="https://s0.wp.com/latex.php?latex=%7BAB+%3D+BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{AB = BA}" class="latex" title="{AB = BA}" /> is usually not true.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://lucatrevisan.wordpress.com">in theory</a> Luca Trevisan <br />
The only theory blog—I believe—that quotes Homer Simpson: “Marge, I agree with you—in theory. In theory, communism works. In theory.” </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <a href="https://www.math.columbia.edu/~woit/wordpress/">Not Even Wrong</a> Peter Woit <br />
Mathematical physics, for the most part, growing out from his 2005 <a href="https://www.amazon.com/Not-Even-Wrong-Failure-Physical/dp/0465092764">book</a> of that title critiquing string theory.</p>
<p>
We also link to sites such as John Awbrey’s <a href="https://inquiryintoinquiry.com/about/">Inquiry Into Inquiry</a> and <a href="https://pinkiguana2.wordpress.com/news/">Pink Iguana</a> that grow in beehive style, and sites with higher-volume politics and culture content such as <a href="https://priorprobability.com/">prior probability</a> by Enrique Guerra-Pujol.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We would be grateful for suggestions of additional mathematics and computing blogs that an advisor’s staff might consult. </p>
<p>
[added Obama-era photo to intro]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/01/21/science-advisor/"><span class="datestr">at January 21, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
