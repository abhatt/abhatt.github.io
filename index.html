<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 17, 2021 02:41 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07631">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07631">Fair and Reliable Reconnections for Temporary Disruptions in Electric Distribution Networks using Submodularity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hettle:Cyrus.html">Cyrus Hettle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Swati.html">Swati Gupta</a>, Daniel Molzahn <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07631">PDF</a><br /><b>Abstract: </b>We analyze a distributed approach for automatically reconfiguring
distribution systems into an operational radial network after a fault occurs by
creating an ordering in which switches automatically close upon detection of a
downstream fault. The switches' reconnection ordering significantly impacts the
expected time to reconnect under normal disruptions and thus affects
reliability metrics such as SAIDI and CAIDI, which are the basis for
regulator-imposed financial incentives for performance.
</p>
<p>We model the problem of finding a switch reconnection ordering that minimizes
SAIDI and the expected reconnection time as Minimum Reconnection Time (MRT),
which we show is a special case of the well-known minimum linear ordering
problem from the submodular optimization literature, and in particular the Min
Sum Set Cover problem (MSSC). We prove that MRT is also NP-hard.
</p>
<p>We generalize the kernel-based rounding approaches of Bansal et al. for Min
Sum Vertex Cover to give tight approximation guarantees for MSSC on c-uniform
hypergraphs for all c. For all instances of MSSC, our methods have a strictly
better approximation ratio guarantee than the best possible methods for general
MSSC.
</p>
<p>Finally, we consider optimizing multiple metrics simultaneously using local
search methods that also reconfigure the system's base tree to ensure fairness
in service disruptions and reconnection times and reduce energy loss. We
computationally validate our approach on the NREL SMART-DS Greensboro synthetic
urban-suburban network. We evaluate the performance of our reconfiguration
methods and show significant reductions compared to single-metric-based
optimizations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07631"><span class="datestr">at April 16, 2021 11:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07582">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07582">SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanakagiri:Raghavendra.html">Raghavendra Kanakagiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kwasniewski:Grzegorz.html">Grzegorz Kwasniewski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ausavarungnirun:Rachata.html">Rachata Ausavarungnirun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ber=aacute=nek:Jakub.html">Jakub Beránek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanellopoulos:Konstantinos.html">Konstantinos Kanellopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janda:Kacper.html">Kacper Janda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vonarburg=Shmaria:Zur.html">Zur Vonarburg-Shmaria</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gianinazzi:Lukas.html">Lukas Gianinazzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stefan:Ioana.html">Ioana Stefan</a>, Juan Gómez Luna, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Copik:Marcin.html">Marcin Copik</a>, Lukas Kapp-Schwoerer, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Girolamo:Salvatore_Di.html">Salvatore Di Girolamo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Konieczny:Marek.html">Marek Konieczny</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutlu:Onur.html">Onur Mutlu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07582">PDF</a><br /><b>Abstract: </b>Simple graph algorithms such as PageRank have recently been the target of
numerous hardware accelerators. Yet, there also exist much more complex graph
mining algorithms for problems such as clustering or maximal clique listing.
These algorithms are memory-bound and thus could be accelerated by hardware
techniques such as Processing-in-Memory (PIM). However, they also come with
non-straightforward parallelism and complicated memory access patterns. In this
work, we address this with a simple yet surprisingly powerful observation:
operations on sets of vertices, such as intersection or union, form a large
part of many complex graph mining algorithms, and can offer rich and simple
parallelism at multiple levels. This observation drives our cross-layer design,
in which we (1) expose set operations using a novel programming paradigm, (2)
express and execute these operations efficiently with carefully designed
set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA
instructions. The key design idea is to alleviate the bandwidth needs of SISA
instructions by mapping set operations to two types of PIM: in-DRAM bulk
bitwise computing for bitvectors representing high-degree vertices, and
near-memory logic layers for integer arrays representing low-degree vertices.
Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned
baselines, offering more than 10x speedup over the established Bron-Kerbosch
algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric
algorithm formulations, illustrating SISA's wide applicability.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07582"><span class="datestr">at April 16, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07563">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07563">Approximate and discrete Euclidean vector bundles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scoccola:Luis.html">Luis Scoccola</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perea:Jose_A=.html">Jose A. Perea</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07563">PDF</a><br /><b>Abstract: </b>We introduce $\varepsilon$-approximate versions of the notion of Euclidean
vector bundle for $\varepsilon \geq 0$, which recover the classical notion of
Euclidean vector bundle when $\varepsilon = 0$. In particular, we study
\v{C}ech cochains with coefficients in the orthogonal group that satisfy an
approximate cocycle condition. We show that $\varepsilon$-approximate vector
bundles can be used to represent classical vector bundles when $\varepsilon &gt;
0$ is sufficiently small. We also introduce distances between approximate
vector bundles and use them to prove that sufficiently similar approximate
vector bundles represent the same classical vector bundle. This gives a way of
specifying vector bundles over finite simplicial complexes using a finite
amount of data, and also allows for some tolerance to noise when working with
vector bundles in an applied setting. As an example, we prove a reconstruction
theorem for vector bundles from finite samples. We give algorithms for the
effective computation of low-dimensional characteristic classes of vector
bundles directly from discrete and approximate representations and illustrate
the usage of these algorithms with computational examples.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07563"><span class="datestr">at April 16, 2021 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07487">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07487">Lipschitz Selectors may not Yield Competitive Algorithms for Convex Body Chasing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>C. J. Argue, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molinaro:Marco.html">Marco Molinaro</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07487">PDF</a><br /><b>Abstract: </b>The current best algorithms for convex body chasing problem in online
algorithms use the notion of the Steiner point of a convex set. In particular,
the algorithm which always moves to the Steiner point of the request set is
$O(d)$ competitive for nested convex body chasing, and this is optimal among
memoryless algorithms [Bubeck et al. 2020]. A memoryless algorithm coincides
with the notion of a selector in functional analysis. The Steiner point is
noted for being Lipschitz with respect to the Hausdorff metric, and for
achieving the minimal Lipschitz constant possible. It is natural to ask whether
every selector with this Lipschitz property yields a competitive algorithm for
nested convex body chasing. We answer this question in the negative by
exhibiting a selector which yields a non-competitive algorithm for nested
convex body chasing but is Lipschitz with respect to Hausdorff distance.
Furthermore, we show that being Lipschitz with respect to an $L_p$-type analog
to the Hausdorff distance is sufficient to guarantee competitiveness if and
only if $p=1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07487"><span class="datestr">at April 16, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07463">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07463">Single-Exponential Time 2-Approximation Algorithm for Treewidth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korhonen:Tuukka.html">Tuukka Korhonen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07463">PDF</a><br /><b>Abstract: </b>We give an algorithm, which given an $n$-vertex graph $G$ and an integer $k$,
in time $2^{O(k)} n$ either outputs a tree decomposition of $G$ of width at
most $2k + 1$ or determines that the treewidth of $G$ is larger than $k$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07463"><span class="datestr">at April 16, 2021 11:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07454">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07454">Memory Capacity of Neural Turing Machines with Matrix Representation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Animesh Renanse, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chandra:Rohitash.html">Rohitash Chandra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharma:Alok.html">Alok Sharma</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07454">PDF</a><br /><b>Abstract: </b>It is well known that recurrent neural networks (RNNs) faced limitations in
learning long-term dependencies that have been addressed by memory structures
in long short-term memory (LSTM) networks. Matrix neural networks feature
matrix representation which inherently preserves the spatial structure of data
and has the potential to provide better memory structures when compared to
canonical neural networks that use vector representation. Neural Turing
machines (NTMs) are novel RNNs that implement notion of programmable computers
with neural network controllers to feature algorithms that have copying,
sorting, and associative recall tasks. In this paper, we study the augmentation
of memory capacity with a matrix representation of RNNs and NTMs (MatNTMs). We
investigate if matrix representation has a better memory capacity than the
vector representations in conventional neural networks. We use a probabilistic
model of the memory capacity using Fisher information and investigate how the
memory capacity for matrix representation networks are limited under various
constraints, and in general, without any constraints. In the case of memory
capacity without any constraints, we found that the upper bound on memory
capacity to be $N^2$ for an $N\times N$ state matrix. The results from our
experiments using synthetic algorithmic tasks show that MatNTMs have a better
learning capacity when compared to its counterparts.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07454"><span class="datestr">at April 16, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07381">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07381">On the Assessment of Benchmark Suites for Algorithm Comparison</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mattos:David_Issa.html">David Issa Mattos</a>, Lucas Ruud, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bosch:Jan.html">Jan Bosch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olsson:Helena_Holmstr=ouml=m.html">Helena Holmström Olsson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07381">PDF</a><br /><b>Abstract: </b>Benchmark suites, i.e. a collection of benchmark functions, are widely used
in the comparison of black-box optimization algorithms. Over the years,
research has identified many desired qualities for benchmark suites, such as
diverse topology, different difficulties, scalability, representativeness of
real-world problems among others. However, while the topology characteristics
have been subjected to previous studies, there is no study that has
statistically evaluated the difficulty level of benchmark functions, how well
they discriminate optimization algorithms and how suitable is a benchmark suite
for algorithm comparison. In this paper, we propose the use of an item response
theory (IRT) model, the Bayesian two-parameter logistic model for multiple
attempts, to statistically evaluate these aspects with respect to the empirical
success rate of algorithms. With this model, we can assess the difficulty level
of each benchmark, how well they discriminate different algorithms, the ability
score of an algorithm, and how much information the benchmark suite adds in the
estimation of the ability scores. We demonstrate the use of this model in two
well-known benchmark suites, the Black-Box Optimization Benchmark (BBOB) for
continuous optimization and the Pseudo Boolean Optimization (PBO) for discrete
optimization. We found that most benchmark functions of BBOB suite have high
difficulty levels (compared to the optimization algorithms) and low
discrimination. For the PBO, most functions have good discrimination parameters
but are often considered too easy. We discuss potential uses of IRT in
benchmarking, including its use to improve the design of benchmark suites, to
measure multiple aspects of the algorithms, and to design adaptive suites.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07381"><span class="datestr">at April 16, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07293">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07293">Sized Types with Usages for Parallel Complexity of Pi-Calculus Processes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baillot:Patrick.html">Patrick Baillot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghyselen:Alexis.html">Alexis Ghyselen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Naoki.html">Naoki Kobayashi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07293">PDF</a><br /><b>Abstract: </b>We address the problem of analysing the complexity of concurrent programs
written in Pi-calculus. We are interested in parallel complexity, or span,
understood as the execution time in a model with maximal parallelism. A type
system for parallel complexity has been recently proposed by Baillot and
Ghyselen but it is too imprecise for non-linear channels and cannot analyse
some concurrent processes. Aiming for a more precise analysis, we design a type
system which builds on the concepts of sized types and usages. The new variant
of usages we define accounts for the various ways a channel is employed and
relies on time annotations to track under which conditions processes can
synchronize. We prove that a type derivation for a process provides an upper
bound on its parallel complexity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07293"><span class="datestr">at April 16, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07247">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07247">Quantum Oracle Separations from Complex but Easily Specified States</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/LaRacuente:Nicholas.html">Nicholas LaRacuente</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07247">PDF</a><br /><b>Abstract: </b>A foundational question in quantum computational complexity asks how much
more useful a quantum state can be in a given task than a comparable, classical
string. Aaronson and Kuperberg showed such a separation in the presence of a
quantum oracle, a black box unitary callable during quantum computation. Their
quantum oracle responds to a random, marked, quantum state, which is
intractable to specify classically. We constrain the marked state in ways that
make it easy to specify classically while retaining separations in task
complexity. Our method replaces query by state complexity. Furthermore,
assuming a widely believed separation between the difficulty of creating a
random, complex state and creating a specified state, we propose an
experimental demonstration of quantum witness advantage on near-term,
distributed quantum computers. Finally, using the fact that a standard,
classically defined oracle may enable a quantum algorithm to prepare an
otherwise hard state in polynomial steps, we observe quantum-classical oracle
separation in heavy output sampling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07247"><span class="datestr">at April 16, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07205">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07205">Faster Algorithms for Rooted Connectivity in Directed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chekuri:Chandra.html">Chandra Chekuri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quanrud:Kent.html">Kent Quanrud</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07205">PDF</a><br /><b>Abstract: </b>We consider the fundamental problems of determining the rooted and global
edge and vertex connectivities (and computing the corresponding cuts) in
directed graphs. For rooted (and hence also global) edge connectivity with
small integer capacities we give a new randomized Monte Carlo algorithm that
runs in time $\tilde{O}(n^2)$. For rooted edge connectivity this is the first
algorithm to improve on the $\Omega(n^3)$ time bound in the dense-graph
high-connectivity regime. Our result relies on a simple combination of sampling
coupled with sparsification that appears new, and could lead to further
tradeoffs for directed graph connectivity problems.
</p>
<p>We extend the edge connectivity ideas to rooted and global vertex
connectivity in directed graphs. We obtain a $(1 + \epsilon)$-approximation for
rooted vertex connectivity in $\tilde{O}(nW/\epsilon)$ time where $W$ is the
total vertex weight (assuming integral vertex weights); in particular this
yields an $\tilde{O}(n^2/\epsilon)$ time randomized algorithm for unweighted
graphs. This translates to a $\tilde{O}(\kappa nW)$ time exact algorithm where
$\kappa$ is the rooted connectivity. We build on this to obtain similar bounds
for global vertex connectivity.
</p>
<p>Our results complement the known results for these problems in the low
connectivity regime due to work of Gabow [9] for edge connectivity from 1991,
and the very recent work of Nanongkai et al. [24] and Forster et al. [7] for
vertex connectivity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07205"><span class="datestr">at April 16, 2021 11:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07166">On Lev Gordeev's "On P Versus NP"</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narv=aacute=ez:David.html">David Narváez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillips:Patrick.html">Patrick Phillips</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07166">PDF</a><br /><b>Abstract: </b>In the paper "On P versus NP," Lev Gordeev attempts to extend the method of
approximation, which successfully proved exponential lower bounds for monotone
circuits, to the case of De Morgan Normal (DMN) circuits. As in Razborov's
proof of exponential lower bounds for monotone circuits, Gordeev's work is
focused on the NP-complete problem CLIQUE. If successful in proving exponential
DMN circuit lower bounds for CLIQUE, Gordeev would prove that P $\neq$ NP.
However, we show that Gordeev makes a crucial mistake in Lemma 12. This mistake
comes from only approximating operations over positive circuit inputs.
Furthermore, we argue that efforts to extend the method of approximation to DMN
circuits will need to approximate negated inputs as well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07166"><span class="datestr">at April 16, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07114">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07114">A Better-Than-2 Approximation for Weighted Tree Augmentation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traub:Vera.html">Vera Traub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zenklusen:Rico.html">Rico Zenklusen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07114">PDF</a><br /><b>Abstract: </b>We present an approximation algorithm for Weighted Tree Augmentation with
approximation factor $1+\ln 2 + \varepsilon &lt; 1.7$. This is the first algorithm
beating the longstanding factor of $2$, which can be achieved through many
standard techniques.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07114"><span class="datestr">at April 16, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07097">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07097">Novel Matrix Hit and Run for Sampling Polytopes and Its GPU Implementation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Mario Vazquez Corte, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montiel:Luis_V=.html">Luis V. Montiel</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07097">PDF</a><br /><b>Abstract: </b>We propose and analyze a new Markov Chain Monte Carlo algorithm that
generates a uniform sample over full and non-full dimensional polytopes. This
algorithm, termed "Matrix Hit and Run" (MHAR), is a modification of the Hit and
Run framework. For the regime $n^{1+\frac{1}{3}} \ll m$, MHAR has a lower
asymptotic cost per sample in terms of soft-O notation ($\SO$) than do existing
sampling algorithms after a \textit{warm start}. MHAR is designed to take
advantage of matrix multiplication routines that require less computational and
memory resources. Our tests show this implementation to be substantially faster
than the \textit{hitandrun} R package, especially for higher dimensions.
Finally, we provide a python library based on Pytorch and a Colab notebook with
the implementation ready for deployment in architectures with GPU or just CPU.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07097"><span class="datestr">at April 16, 2021 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.07061">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.07061">Exact and Approximate Hierarchical Clustering Using A*</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Greenberg:Craig_S=.html">Craig S. Greenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Macaluso:Sebastian.html">Sebastian Macaluso</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Monath:Nicholas.html">Nicholas Monath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dubey:Avinava.html">Avinava Dubey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Flaherty:Patrick.html">Patrick Flaherty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zaheer:Manzil.html">Manzil Zaheer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Amr.html">Amr Ahmed</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cranmer:Kyle.html">Kyle Cranmer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McCallum:Andrew.html">Andrew McCallum</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.07061">PDF</a><br /><b>Abstract: </b>Hierarchical clustering is a critical task in numerous domains. Many
approaches are based on heuristics and the properties of the resulting
clusterings are studied post hoc. However, in several applications, there is a
natural cost function that can be used to characterize the quality of the
clustering. In those cases, hierarchical clustering can be seen as a
combinatorial optimization problem. To that end, we introduce a new approach
based on A* search. We overcome the prohibitively large search space by
combining A* with a novel \emph{trellis} data structure. This combination
results in an exact algorithm that scales beyond previous state of the art,
from a search space with $10^{12}$ trees to $10^{15}$ trees, and an approximate
algorithm that improves over baselines, even in enormous search spaces that
contain more than $10^{1000}$ trees. We empirically demonstrate that our method
achieves substantially higher quality results than baselines for a particle
physics use case and other clustering benchmarks. We describe how our method
provides significantly improved theoretical bounds on the time and space
complexity of A* for clustering.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.07061"><span class="datestr">at April 16, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/04/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/04/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Keller%27s_conjecture">Keller’s conjecture</a> (<a href="https://mathstodon.xyz/@11011110/105994491983819823">\(\mathbb{M}\)</a>), another new Good Article on Wikipedia. The conjecture was falsified in 1992 with all remaining cases solved by 2019, but the name stuck. It’s about tilings of \(n\)-space by unit cubes, and pairs of cubes that share \((n-1)\)-faces. In 2d, all squares share an edge with a neighbor, but a 3d tiling derived from <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix</a> has many cubes with no face-to-face neighbor. Up to 7d, some cubes must be face-to-face, but tilings in eight or more dimensions can have no face-to-face pair.</p>
  </li>
  <li>
    <p><a href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/">Italians and bibliometrics</a> (<a href="https://mathstodon.xyz/@11011110/105996551762053680">\(\mathbb{M}\)</a>): Luca Trevisan (a leading theorist with 7 SODA papers, 2 FOCS papers, a JACM paper and a SICOMP paper in the last four years) gets dinged for poor productivity as the Italian system only counts journal papers that do not match conference papers. The fact that these are all in top venues is irrelevant, and the conference papers count only negatively against matching journal papers. Comments discuss similar problems in other countries.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5402">What is the computational complexity of dinosaur train tracks?</a> (<a href="https://mathstodon.xyz/@11011110/106009429519024889">\(\mathbb{M}\)</a>).  Answer: not very high, because the only usable junction, a Y that remembers which way you came through it and sends you the same way if you come back through the other direction, is just not powerful enough to do much.</p>
  </li>
  <li>
    <p>Congratulations to Martín Farach-Colton, Shang-Hua Teng, and all of the other <a href="https://sinews.siam.org/Details-Page/siam-announces-class-of-2021-fellows">new SIAM Fellows</a> (<a href="https://mathstodon.xyz/@11011110/106016827432463028">\(\mathbb{M}\)</a>)!</p>
  </li>
  <li>
    <p><a href="https://writings.stephenwolfram.com/2021/03/a-little-closer-to-finding-what-became-of-moses-schonfinkel-inventor-of-combinators/">Stephen Wolfram tries to track down</a> what happened to logician <a href="https://en.wikipedia.org/wiki/Moses_Sch%C3%B6nfinkel">Moses Schönfinkel</a> (<a href="https://mathstodon.xyz/@11011110/106025064389253132">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26694685">via</a>), who worked in Göttingen from 1914 to 1924, returned to Moscow, and then “basically vanished”. Wikipedia has more detail about what happened after (mental health issues, death around 1942), but Wolfram says the evidence for all that is weak. He doesn’t make direct progress on Schönfinkel himself but does find some relatives.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2021/04/my-robot-draws-tex/">How Christian Lawson-Perfect got a pen plotter to draw mathematical notation using TeX</a> (<a href="https://mathstodon.xyz/@christianp/106030474747952758">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.improbable.com/2021/04/08/a-look-way-back-at-some-bearded-mathematicians/">When mathematicians wore geometric beards</a> (<a href="https://mathstodon.xyz/@11011110/106034106736789855">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5437">Aaronson on politicization of research prizes</a> (<a href="https://mathstodon.xyz/@11011110/106039607813461033">\(\mathbb{M}\)</a>). Jeff Ullman won the Turing Award despite deplorable (some say racist) treatment of grad applicants for the crime of being Iranian, and Oded Goldreich was blocked from the Israel Prize for anti-settlement politics. Politicization is two-edged. I’d rather see Ullman awarded for his worthy contributions, and use the opportunity to decry his abhorrent actions and statements, than subject prizes to litmus tests from all sides.</p>
  </li>
  <li>
    <p><a href="https://thatsmaths.com/2021/04/08/circles-polygons-and-the-kepler-bouwkamp-constant/">Circles, polygons and the Kepler-Bouwkamp constant</a> (<a href="https://mathstodon.xyz/@11011110/106045568373359503">\(\mathbb{M}\)</a>). On the limiting behavior of infinitely-nested shapes alternating between circles and polygons with increasing numbers of sides.</p>
  </li>
  <li>
    <p><a href="https://www.technologyreview.com/2021/04/09/1022217/facebook-ad-algorithm-sex-discrimination">Continuing gender bias in who sees job-opening ads on Facebook</a> (<a href="https://mathstodon.xyz/@11011110/106051162687419819">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26760790">via</a>): if an employer or industry has historically skewed male or female, Facebook replicates that bias, even for pairs of ads with identical qualifications. This is illegal, but Facebook appears unable to find a technical fix and unwilling to apply the obvious fix of not targeting its ads even when that targeting is illegal. As usual for Hacker News via links on topics related to social justice, don’t read the comments there.</p>
  </li>
  <li>
    <p>Amusing quote from McLarty’s 2003 “<a href="http://www.landsburg.com/grothendieck/mclarty1.pdf">Grothendieck on simplicity and generality</a>” (<a href="https://mathstodon.xyz/@11011110/106059214777848356">\(\mathbb{M}\)</a>, <a href="https://golem.ph.utexas.edu/category/2021/04/algebraic_closure.html">via</a>): “Serre created a series of concise elegant tools which Grothendieck and coworkers simplified into thousands of pages of category theory.” Nowadays I guess the people doing this sort of simplification are the ones formulating machine-verifiable proofs…</p>
  </li>
  <li>
    <p><a href="https://projects.cs.dal.ca/wads2021/wads-2021-accepted-papers/">WADS 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/106062479142848045">\(\mathbb{M}\)</a>). 
The biennial Algorithms and Data Structures Symposium is usually in Canada, and this time was supposed to be in Halifax, but is looking very likely to be completely online, this August. I have one paper on the list; I’ll write more about it later when I have a preprint version ready to share.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=S5fPwE7GQOA">A “confounding topological curiosity”</a> (<a href="https://mathstodon.xyz/@11011110/106068163014347342">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2021/04/13/heres-a-confounding-topological-curiosity.html">via</a>): a double torus with a line through one of its holes can be continuously transformed so that the line instead goes through both holes.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11818">Strange inverses in the group rings of torsion-free groups</a> (<a href="https://mathstodon.xyz/@11011110/106072203556292078">\(\mathbb{M}\)</a>, <a href="https://www.quantamagazine.org/mathematician-disproves-group-algebra-unit-conjecture-20210412/">via</a>, <a href="https://www.uni-muenster.de/MathematicsMuenster/news/artikel/2021-03-04.shtml">see also</a>). This result of Giles Gardam disproves the strongest of the three <a href="https://en.wikipedia.org/wiki/Kaplansky%27s_conjectures">Kaplansky conjectures on group rings</a>. It’s just an isolated example at this point but it does show that group rings are less well-behaved than had been hoped.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/04/15/linkage.html"><span class="datestr">at April 15, 2021 10:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/">TOC: a Personal Perspective (2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Editorial note</strong>: this post has been written in celebration of 25 years for “<a href="http://www.wisdom.weizmann.ac.il/~oded/toc-sp.html">TOC: a Scientific Perspective (1996),</a>” by <a href="http://www.wisdom.weizmann.ac.il/~oded/">Oded Goldreich</a> and <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>. In the process, I have been made aware of a Facebook discussion from a few weeks ago (which I don’t know how to link to), and to <a href="https://simons.berkeley.edu/talks/tbd-271">Avi Wenderson’s recent talk</a> that addresses this discussion (and more). I will not attribute statements from the Facebook discussion to individuals (nor will I specify its initiator), as they may not identify with the statements, when taken out of their original context or in the editorialized version here. In any case, this specific discussion is not the point. Feel free to claim ownership in comments .</p>



<p>———————————–</p>



<p>I’m sure that, like me, you read on social media: “TOC is in crisis, scratch that, it is lost! We do not have agreed-upon challenges (that are not way out of reach) and do not know how to evaluate papers. Therefore our conferences are favoring progress in techniques and favor complicated papers on obscure problems over progress on important problems (in fact, there is shortage of interesting work on relevant problems). Our flagship conferences are broken and newer conferences, that aimed to do better, have become just more of the same. This is not TCS as we remember it!”</p>



<p>There are many points with which I agree. Like others, I have been critical at times about the way we do some things, mourning papers and subfields that our conferences have missed. On several occasions, I have been pushing for change. At times I was successful but in other instances <a href="https://windowsontheory.org/2015/06/08/can-we-get-serious/">my suggestions </a>were deemed radical by the powers that be (and more modest/timid suggestions were adopted). <strong><em>But did TOC really lose its way?</em> </strong> </p>



<p>One of the commenter was saying “I have been hearing these complaints about focs and Stoc for more than ten years. They have come from powerful people that sit on committees. … So why nothing changes?“ This comment strikes a chord with me, but let me revise it and say that I have been hearing such complaints for the last 25 years (since attending my first conference), and it is almost always stated by the powerful people, those that have the responsibility to shape the field.</p>



<p>Following the continuous self-criticism, we are likely to assume that TOC is a dysfunctional field and has been so for many years. But if we look at the research achievements of TOC in the last quarter century, we must conclude that this was a glorious period. And the contributions of TCS were on different fronts. Contributions to applied CS and industry, growing contributions outside of CS as well as progress on fundamental questions within TOC. Said progress was obtained by simple papers and by complex and long papers. By papers developing new techniques, by papers making progress on known problems and by papers that introduced new problems, models or even papers that initiated new subfields. They have been made by breakthrough papers and by long sequences of modest papers. By papers in FOCS/STOC and papers in other conferences. So <strong>if TOC is in a continuous crisis, it is the most wonderful crisis possible</strong>.</p>



<p>During my studies (ages ago), I was intensely attracted to TOC. But at the same time, I felt that the field is under constant external attack. It was claimed that we are not as deep as Math and not as useful as CS. Many fewer universities than today have been hiring theoreticians. The field was grossly underfunded (still underfunded but less grossly) but still calls have been made to reduce funding to any area in which TOC is not directly serving other, more applied areas. The dissonance between my intuitive attraction and external criticism could have deterred me from TOC, but there were incredible leaders of TOC that effectively defended the field and shouted – look, something amazing is happening here. “TOC: a Scientific Perspective (1996),” by Oded Goldreich and Avi Wigderson gave me courage to continue. 25 years later, the case they once made in defense of TOC is so much easier to support (and they kept on making this case throughout the years in essays and <a href="https://www.math.ias.edu/avi/book">books</a>). <a href="https://simons.berkeley.edu/talks/tbd-271">As Avi argued</a>, TOC’s success have brought growth and diversification, influx of young talent, scientific respect, industrial respect, and societal respect. <strong>We should do better on self-respect.</strong></p>



<p>I am of course not advocating resting on our laurels’. Like in Alice’s adventures, in our fast field, it takes all the running we can do, to keep in the same place. If we want to get somewhere else, we must run at least twice as fast as that! Constructive criticism is a good thing but our tendency for alarmist/defeatist cries is not serving us well. As someone who grew (scientifically) in an atmosphere of struggle, I am grateful for the progress made in establishing our field by the generations that preceded me. We shouldn’t take for granted how easy we have it. But more importantly, confidence in our field and optimism towards our future are important for our impact on the world (I discussed one aspect of this <a href="https://theorydish.blog/2019/06/24/on-the-importance-of-disciplinary-pride-for-multidisciplinary-collaboration/">here</a>). Finally, thinking of students interested in TOC today and hearing the most powerful people in the field announcing that it is lost. I ask myself, who are the Avi and Oded that will give them the needed courage and optimism? The answer is still that their Avi and Oded are the very same Avi and Oded from my student years. But isn’t it about time that we lend a hand?</p>



<p></p></div>







<p class="date">
by Omer Reingold <a href="https://theorydish.blog/2021/04/15/toc-a-personal-perspective-2021/"><span class="datestr">at April 15, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18549">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/">Scott Wins a Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Quantum mechanics makes absolutely no sense—Roger Penrose</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/aaronsonhorgan/" rel="attachment wp-att-18565"><img width="160" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/AaronsonHorgan.jpg?resize=160%2C170&amp;ssl=1" class="alignright size-full wp-image-18565" height="170" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Scott Answers Big Questions <a href="https://blogs.scientificamerican.com/cross-check/scott-aaronson-answers-every-ridiculously-big-question-i-throw-at-him/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Scott Aaronson has just been named the 2020 ACM <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">Prize</a> in Computing for groundbreaking contributions to quantum computing, Penrose’s comment notwithstanding.  The prize <a href="https://awards.acm.org/about/2020-acm-prize">citation</a> also credits Scott’s multifaceted public outreach for making our fields accessible to many.</p>
<p>
Today Ken and I send congrats to Scott for this singular honor.</p>
<p>
The ACM Prize was founded in 2007.  It does not have the age limit of a Fields Medal but is similarly positioned.  Scott joins a impressive list of winners: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/who-2/" rel="attachment wp-att-18562"><img width="550" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/who-1.png?resize=550%2C625&amp;ssl=1" class="aligncenter wp-image-18562" height="625" /></a></p>
<p></p><p><br />
</p><h2> Not Why He Did Win? </h2><p></p>
<p>
Scott perhaps could have won for the following three accomplishments:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His wonderful <a href="https://www.scottaaronson.com/blog/">blog</a>. About the prize, he <a href="https://www.scottaaronson.com/blog/?p=5448">says</a> there: </p>
<blockquote><p><b> </b> <em> Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently. Am I in trouble? … Luckily, Dina only wanted to tell me that I’d been selected to receive the 2020 ACM Prize in Computing, a mid-career award founded in 2007 that comes with $250,000 from Infosys. Not the Turing Award but I’d happily take it! And I could even look back on 2020 fondly for something. </em>
</p></blockquote>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">book</a> on quantum: </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/book-6/" rel="attachment wp-att-18554"><img width="160" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/book-1.jpg?resize=160%2C240&amp;ssl=1" class="aligncenter wp-image-18554" height="240" /></a></p>
<p>
Democritus was known as “the laughing philosopher,” though some other depictions of the laugh range from world-weary to pained.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> His sense of <a href="https://www.scottaaronson.com/blog/?p=62">humor</a>. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/kl2/" rel="attachment wp-att-18563"><img width="600" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/kl2.png?resize=600%2C351&amp;ssl=1" class="aligncenter size-large wp-image-18563" height="351" /></a></p>
<p>
This is my returning thanks, in a way. Ken says that what impresses him is not the floor-length garment but the floor-length blackboard. Both of us hope that after handling “For All” and “Exists,” he went on to solve the problem of placing “Not” in English—for instance, “Why He Did Not Win?” is more grammatically natural but wrong.</p>
<p></p><h2> Why He Did Win? </h2><p></p>
<p>
Scott perhaps did win for the following four accomplishments:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> The theoretical foundations of the sampling-based quantum supremacy <a href="https://arxiv.org/pdf/1612.05903.pdf">experiments</a>—joint with Lijie Chen. Ken adds that this paper is also known for the “Schrödinger” and “Feynman” nomenclature for simulating quantum classically, and the idea of hybridizing those approaches.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> The algebrization <a href="https://www.scottaaronson.com/papers/alg.pdf">barrier</a> in complexity theory—joint with Avi Wigderson.  About Avi’s talk on this at my 60th birthday workshop, Ken <a href="https://blog.computationalcomplexity.org/2008/05/report-on-sym-for-liptons-60th-bday.html">wrote</a> that it “planted a Monty Python foot on further progress” on lower bounds.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" /> Limitations on quantum computers—work on the quantum lower bound for the <a href="https://www.scottaaronson.com/papers/collision.ps">collision problem</a>.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" /> The opposite of limitations—getting quantum advantage from the simplest of components in linear <a href="https://dl.acm.org/doi/10.1145/1993636.1993682">optics</a>. And <a href="https://arxiv.org/abs/1309.7460">this</a>, likewise joint with Alex Arkhipov. This approach has been followed by many, notably last <a href="https://science.sciencemag.org/content/370/6523/1460">December</a> by a large team in China.  We just <a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/">wrote</a> about envy of big experiments, but Scott has arguably done the most of anyone in our field to launch them.</p>
<p>
There are many more. But maybe the one that will get us all rich is Scott’s <a href="https://arxiv.org/abs/1110.5353">work</a> on <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>. Qubitcoin, anyone?</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
Wonderful to add Scott to the list of winners of this award. Congrats again.</p>
<p>
We also congratulate Paul Beame on the SIGACT Distinguished Service Award.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/15/scott-wins-a-prize/"><span class="datestr">at April 15, 2021 01:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3988126252359540708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html">Ordering Beauty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>First, congratulations to fellow complexity theorist and <a href="https://www.scottaaronson.com/blog/">blogger</a> Scott Aaronson for <a href="https://awards.acm.org/about/2020-acm-prize">receiving the 2020 ACM Prize in Computing</a> for "groundbreaking contributions to quantum computing". The prize is ACM's highest honor for mid-career researchers. Well deserved! </p><p>Now back to our regularly scheduled post...</p><p>Every freshman at Cornell back in 1981 had to take two seminar courses, basically one-shot courses in an usually humanities area which required no prerequisites but lots of writing. I took my first course in philosophy. The instructor, a PhD student, at one point described his research, a philosophical argument that there is an intrinsic total ordering of beauty, say that Beethoven would always sit above the Beatles, no matter the beholder. I didn't believe him then and still don't today. A few months ago the Washington Post ran a story with the same theme entitled <a href="https://www.washingtonpost.com/entertainment/maradona-messi-ronaldo-zlatan-shakespeare-beatles/2020/12/23/27654712-38a9-11eb-9276-ae0ca72729be_story.html">Maradona was great, and maybe the greatest. Can we make similar claims about artists?</a></p><p>Somehow we have this belief when it comes to conference submissions, that there is some perfect ordering of the submissions and a good PC can suss it out. That's not really how it works. Let's say a conference has an accept rate of 30%. Typically 10% of the submissions are strong and will be accepted by any committee. About half the submissions are just okay or worse and would be rejected. The other 40% of the submissions will be chosen seemingly randomly based on the tastes of the specific members of the program committee. Experiments in the NeurIPS and ESA conferences have bourn this out. </p><p>Why not make the randomness explicit instead of implicit? Have the PC divide the papers into three piles, definite accepts, definite rejects and the middle. Take the third group and randomly choose which ones to accept. It will create a more interesting program. Also randomness removes biases, randomness doesn't care about gender, race and nationality or whether the authors are at senior professors at MIT or first year grad students at Southern North Dakota. </p><p>We put far too much weight on getting accepted into a conference given the implicit randomness of a PC. If we make the randomness explicit that would devalue that weight. We would have to judge researchers on the quality of their research instead of their luck in conferences.</p><p>Given that conferences, especially the virtual ones, have no real limits on the number of papers and talks, you might say why not just accept all the papers in the middle. Works for me.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/ordering-beauty.html"><span class="datestr">at April 15, 2021 12:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/054">TR21-054 |  Encodings and the Tree Evaluation Problem | 

	Ian Mertz, 

	James  Cook</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that the Tree Evaluation Problem with alphabet size $k$ and height $h$ can be solved by branching programs of size $k^{O(h/\log h)} + 2^{O(h)}$. This answers a longstanding challenge of Cook et al. (2009) and gives the first general upper bound since the problem's inception.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/054"><span class="datestr">at April 14, 2021 07:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5448">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5448">The ACM Prize thing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Last week I got an email from Dina Katabi, my former MIT colleague, asking me to call her urgently.  <em>Am I in trouble?  For what, though??  I haven’t even worked at MIT for five years!</em></p>



<p>Luckily, Dina only wanted to tell me that I’d been <a href="https://awards.acm.org/about/2020-acm-prize">selected</a> to receive the 2020 <a href="https://en.wikipedia.org/wiki/ACM_Prize_in_Computing">ACM Prize in Computing</a>, a mid-career award founded in 2007 that comes with $250,000 from Infosys.  Not the Turing Award but I’d happily take it!  And I could even look back on 2020 fondly for something.</p>



<p>I was utterly humbled to see the <a href="https://awards.acm.org/acm-prize/award-winners">list</a> of past ACM Prize recipients, which includes amazing computer scientists I’ve been privileged to know and learn from (like Jon Kleinberg, Sanjeev Arora, and Dan Boneh) and others who I’ve admired from afar (like Daphne Koller, Jeff Dean and Sanjay Ghemawat of Google MapReduce, and David Silver of AlphaGo and AlphaZero).</p>



<p>I was even more humbled, later, to read my <a href="https://awards.acm.org/award_winners/aaronson_9555914">prize citation</a>, which focuses on four things:</p>



<ol><li>The theoretical foundations of the sampling-based quantum supremacy experiments now being carried out (and in particular, my and Alex Arkhipov’s <a href="https://www.theoryofcomputing.org/articles/v009a004/">2011 paper on BosonSampling</a>);</li><li>My and Avi Wigderson’s <a href="https://www.scottaaronson.com/papers/alg.pdf">2008 paper</a> on the algebrization barrier in complexity theory;</li><li>Work on the limitations of quantum computers (in particular, the 2002 <a href="https://www.scottaaronson.com/papers/collision.pdf">quantum lower bound for the collision problem</a>); and</li><li>Public outreach about quantum computing, including through <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">QCSD</a>, popular talks and articles, and this blog.</li></ol>



<p>I don’t know if I’m worthy of such a prize—but I know that if I am, then it’s mainly for work I did between roughly 2001 and 2012.  This honor inspires me to want to be more like I was back then, when I was driven, non-jaded, and obsessed with figuring out the contours of BQP and efficient computation in the physical universe.  It makes me want to justify the ACM’s faith in me.</p>



<p>I’m grateful to the committee and nominators, and more broadly, to the whole quantum computing and theoretical computer science communities—which I “joined” in some sense around age 16, and which were the first communities where I ever felt like I belonged.  I’m grateful to the mentors who made me what I am, especially Chris Lynch, Bart Selman, Lov Grover, Umesh Vazirani, Avi Wigderson, and (if he’ll allow me to include him) John Preskill.  I’m grateful to the slightly older quantum computer scientists who I looked up to and tried to emulate, like Dorit Aharonov, Andris Ambainis, Ronald de Wolf, and John Watrous.  I’m grateful to my wonderful colleagues at UT Austin, in the CS department and beyond.  I’m grateful to my students and postdocs, the pride of my professional life.  I’m grateful, of course, to my wife, parents, and kids.</p>



<p>By coincidence, my <a href="https://www.scottaaronson.com/blog/?p=5437">last post</a> was also about prizes to theoretical computer scientists—in that case, two prizes that attracted controversy because of the recipient’s (or would-be recipient’s) political actions or views.  It would understate matters to point out that not everyone has always agreed with everything I’ve said on this blog.  I’m <em>ridiculously</em> lucky, and I know it, that even living through this polarized and tumultuous era, I never felt forced to choose between academic success and the freedom to speak my conscience in public under my real name.  If there’s been one constant in my public stands, I’d like to think that—inspired by memories of my own years as an unknown, awkward, self-conscious teenager—it’s been my determination to nurture and protect talented young scientists, whatever they look like and wherever they come from.  And I’ve tried to live up to that ideal in real life, and I welcome anyone’s scrutiny as to how well I’ve done.</p>



<p>What should I do with the prize money? I confess that my first instinct was to donate it, in its entirety, to some suitable charity—specifically, something that would make all the strangers who’ve attacked me on Twitter, Reddit, and so forth over the years realize that I’m fundamentally a good person.  But I was talked out of this plan by my family, who pointed out that<br />(1) in all likelihood, <em>nothing</em> will make online strangers stop hating me,<br />(2) in any case this seems like a poor basis for making decisions, and<br />(3) if I really want to give others a say in what to do with the winnings, then why not everyone who’s stood by me and supported me?</p>



<p>So, beloved commenters!  Please mention your favorite charitable causes below, especially weird ones that I wouldn’t have heard of otherwise.  If I support their values, I’ll make a small donation from my prize winnings.  Or a larger donation, especially if you donate yourself and challenge me to match.  Whatever’s left after I get tired of donating will probably go to my kids’ college fund.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> And by an amusing coincidence, today is apparently <a href="https://worldquantumday.org/">“World Quantum Day”</a>!  I hope your Quantum Day is as pleasant as mine (and stable and coherent).</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5448"><span class="datestr">at April 14, 2021 03:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/053">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/053">TR21-053 |  Information in propositional proofs and algorithmic proof search | 

	Jan  Krajicek</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study from the proof complexity perspective the (informal) proof search problem:
Is there an optimal way to search for propositional proofs?
We note that for any fixed proof system there exists a time-optimal proof search algorithm. Using classical proof complexity results about reflection principles we prove that a time-optimal proof search algorithm exists w.r.t. all proof systems iff a p-optimal proof system exists.
To characterize precisely the time proof search algorithms need for individual formulas we introduce a new proof complexity measure based on algorithmic information concepts. In particular, to a proof system P we attach {\bf information-efficiency function} $i_P(\tau)$ assigning to a tautology a natural number, and we show that:
- $i_P(\tau)$ characterizes time any $P$-proof search algorithm has to use on $\tau$ and that for a fixed $P$ there is such an information-optimal algorithm,
- a proof system is information-efficiency optimal iff it is p-optimal,
- for non-automatizable systems $P$ there are formulas $\tau$ with short proofs but having large information measure $i_P(\tau)$.
We isolate and motivate the problem to establish {\em unconditional} super-logarithmic lower bounds for $i_P(\tau)$ where no super-polynomial size lower bounds are known. We also point out connections of the new measure with some topics in proof complexity other than proof search.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/053"><span class="datestr">at April 13, 2021 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18514">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/">Wobble in the Standard Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Prediction is very difficult, especially if it’s about the future—Niels Bohr</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randalluncertainty/" rel="attachment wp-att-18526"><img width="153" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallUncertainty.jpg?resize=153%2C210&amp;ssl=1" class="alignright wp-image-18526" height="210" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Boston Globe “Uncertainty” <a href="https://www.bostonglobe.com/ideas/2011/10/22/lisa-randall-physics-universe-uncertainty/BfSYjipZy7HkQPmmRs8ZRI/story.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written popular books about her work and science in general. Thus she has a handle on aspects of science that overlap my expertise—not to mention those of her sister Dana Randall, whom I have known as a colleague for many years.</p>
<p>
Today, Ken and I thought we would talk about recent developments in particle physics, and their connection to two topics dear to us.<br />
<span id="more-18514"></span></p>
<p>
Randall’s most recent popular book is <a href="https://en.wikipedia.org/wiki/Dark_Matter_and_the_Dinosaurs">Dark Matter and the Dinosaurs</a>. The idea she advances is that the periodic extinctions in Earth’s history may have been caused when the solar system passes through a plane of dark matter within our galaxy. But dark matter and also dark energy have come under <a href="https://www.sciencenews.org/article/dark-matter-mystery-deepens-demise-reported-detection">increasing</a> <a href="https://phys.org/news/2021-03-composition-percent-universe.html">recent</a> <a href="https://www.nbcnews.com/science/space/maybe-dark-matter-doesn-t-exist-after-all-new-research-n1252995">doubt</a>, even from their original <a href="https://www.newscientist.com/article/mg24632851-400-why-the-universe-i-invented-is-right-but-still-not-the-final-answer/">formulator</a>. Maybe Niels Bohr’s quote should also say: </p>
<blockquote><p><b> </b> <em> <i>Prediction is very difficult, especially if it’s about the past.</i> </em>
</p></blockquote>
<p>
Randall’s previous book, <a href="https://en.wikipedia.org/wiki/Knocking_on_Heaven's_Door_(book)">Knocking on Heaven’s Door</a>, is most relevant to this post. The 1973 Bob Dylan <a href="https://en.wikipedia.org/wiki/Knockin'_on_Heaven's_Door">song</a> title it pinches describes the feeling of doing frontier physical science. Insofar as her own work is mostly theoretical, much of it connects to feelings we have in computer science—especially complexity lower bounds where the door mostly feels slammed shut. </p>
<p>
But the book is also about the practice of experimental science—not only how to gather knowledge but when and how we can have confidence in it. Its long middle part is titled, “Machines, Measurements, and Probability.” All three elements are foremost in considering a new development that involves two measurements taken 20 years apart.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randallbooks/" rel="attachment wp-att-18528"><img width="406" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallBooks.jpg?resize=406%2C254&amp;ssl=1" class="aligncenter wp-image-18528" height="254" /></a></p>
<p></p><h2> Muons </h2><p></p>
<p>
Last Tuesday’s New York Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">highlighted</a> a potential discovery in particle physics. It was in their Tuesday science section. </p>
<p>
The result is an experimental discovery that could show that the current model of matter is wrong. </p>
<blockquote><p><b> </b> <em> “This is our Mars rover landing moment,” said Chris Polly, a physicist at the Fermi National Accelerator Laboratory, or Fermilab, in Batavia, Ill., who has been working toward this finding for most of his career. </em>
</p></blockquote>
<p>
Indeed. It is a Mars landing moment. They both involved many people, lots of exotic machinery, lots of money, many years. Say three billion dollars or so for Mars. Say nearly the same amount for muons—the annual budget for Fermilab is over one half billion dollars. It was certainly enough to reassemble and upgrade a huge accelerator ring that was <a href="https://www.bnl.gov/newsroom/news.php?a=112259">first used</a> at Brookhaven National Lab on Long Island in 2001:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/fermilabring/" rel="attachment wp-att-18539"><img width="450" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/FermilabRing.jpg?resize=450%2C301&amp;ssl=1" class="aligncenter wp-image-18539" height="301" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NY Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">src</a></font>
</td>
</tr>
</tbody></table>
<p>
The study used the muon to probe the Standard Model of physics. Muons are useful because they are charged like an electron, which helps control them in an accelerator. Yet their mass is roughly 207 times larger than an electron. The same charge helps control their motion and the large mass makes collisions more interesting. As Polly stated in Natalie Wolchover’s <a href="https://www.quantamagazine.org/muon-g-2-experiment-at-fermilab-finds-hint-of-new-particles-20210407/">story</a> for <em>Quanta</em>:</p>
<blockquote><p><b> </b> <em> “[I]f you’re looking for particles that could explain the missing mass of the universe—dark matter—or you’re looking for [supersymmetry], that’s where the muon has a unique role.” </em>
</p></blockquote>
<p></p><h2> Theory and Jealousy </h2><p></p>
<p>
Computer science theory is so different from high end physics. We are closer to the type of research that Randall does. We involve few people, no exotic machinery, and small amounts of money. Maybe the closest attribute we have to high-end research is we also take years and years.</p>
<p>
Perhaps we are also jealous of high-end physics. Not just for money, but for the ability of particle physicists to get announcements into the New York Times. Polly said the <a href="https://www.energy.gov/science/articles/first-person-science-chris-polly-muon-physics">following</a> about the ending day of the muon experiment two decades ago:</p>
<blockquote><p><b> </b> <em> When we revealed the results, people from all over the world flew in to visit the lab. These experiments take decades to build and analyze, so you don’t get to go to very many of these events. We did a little “Drumroll, please” and then had the postdoc managing the spreadsheet hit the button to show it on the projector. Lo and behold, you could see that there was still a three-sigma discrepancy! </em>
</p></blockquote>
<p>
At the time he was a graduate assistant assigned to machinery for measuring particle energies. He had fixed a problem where someone had touched a component with bare hands and thereby ruined its sheathing. All such problems were meant to be ironed-out by the drum-roll event. But all this raises two further interesting issues that connect the muon results with issues we think about in computer science. Let’s look at them next.</p>
<p></p><h2> Three to Four Sigma </h2><p></p>
<p>
In an experimental science one must be aware that results are not exact. They are samples from some random process. Flip a coin 10 times in a row. If they all come up heads what does that mean? Could be the coin is fair but this happens about one time in a thousand. Or the coin is biased. Or something else. </p>
<p>
Flip a muon many times. That is sample some muon experiment. The outcome is from a random process. Some of it comes from properties of the natural processes themselves and others from incidentals of the measurement apparatus. How do we decide if the experiment means what we think it does?</p>
<p>
The theory developed by Carl Gauss and others before and after to delineate the normal distribution was largely prompted by analysis of measurement errors <a href="https://www.maa.org/sites/default/files/images/upload_library/22/Allendoerfer/stahl96.pdf">to begin with</a>. This yields the “<a href="https://en.wikipedia.org/wiki/68-95-99.7_rule">rule of three</a>,” about the percentage of values that naturally lie within an interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively.</p>
<p>
The question is, how to assess cases where the measurement result is well outside these intervals—when can we conclude it is more than a deviation by natural chance? In social sciences a result is “significant” provided it lies outside two-sigma. In particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a discovery. No Nobel prize for less. </p>
<p>
The situation with the muons has an extra factor of repeated measurements—but there have been only two measurements so far:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/muoncharts/" rel="attachment wp-att-18531"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/MuonCharts.png?resize=550%2C247&amp;ssl=1" class="aligncenter wp-image-18531" height="247" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://news.fnal.gov/2021/04/first-results-from-fermilabs-muon-g-2-experiment-strengthen-evidence-of-new-physics/">src1</a>, <a href="https://www.sciencemag.org/news/2021/04/particle-mystery-deepens-physicists-confirm-muon-more-magnetic-predicted">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The blue line in the left figure is the original Brookhaven measurement; the red is the new one. There is also <a href="https://4gravitons.com/2021/04/09/theoretical-uncertainty-and-uncertain-theory/">theoretical uncertainty</a> in the calculation of the Standard Model prediction, and that combines with the measurement error bars to give the sigma baseline. The chart at right normalizes the deviation to parts per billion—the measurements need to be incredibly fine. This scale appears to be about 25% under the current <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sigma}" class="latex" />-scale (it shows about <img src="https://s0.wp.com/latex.php?latex=%7B2.8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.8}" class="latex" /> for Brookhaven compared to its <img src="https://s0.wp.com/latex.php?latex=%7B3.7%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3.7\sigma}" class="latex" /> after <a href="https://arxiv.org/pdf/2006.04822.pdf">revised</a> uncertainty) but it is close enough to get the picture.</p>
<p>
Although the new Fermilab result by itself deviates slightly less from the Standard Model, it corroborates the earlier measurement. It is not fully independent from it, but the combination is enough to raise the current claimed deviation to about <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> sigmas. This is well above social science level but below Nobel level. This is with respect to the probability that the effects are real. </p>
<p>
The social significance of <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> is that it is above the “<img src="https://s0.wp.com/latex.php?latex=%7B3%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3+}" class="latex" />” level where hoped-for anomalies have subsequently disappeared for reasons chalked up to natural chance. This is because physicists around the world do many a hundredfold amount of hopeful measurements. Some measurements get initial “bumps” up just because of the numbers. But <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> reduces the natural frequency under 1-in-40,000. This is why reproducing measurements is so important, why the new Fermilab team devoted all the expense and effort. A more independent measurement on other machines could give a higher boost that might get over the <img src="https://s0.wp.com/latex.php?latex=%7B5.0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{5.0}" class="latex" /> line. Time to break out the wallets and hammers?</p>
<p>
The <img src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.2}" class="latex" /> is not, however, beyond the realm of <a href="https://en.wikipedia.org/wiki/Faster-than-light_neutrino_anomaly">recent</a> <a href="https://physics.aps.org/articles/v11/s78">experience</a> with apparatus faults and modeling error. On the latter, there is <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12292">still</a> some <a href="http://resonaances.blogspot.com/2021/04/why-is-it-when-something-happens-it-is.html">doubt</a> about the theoretical prediction for the muon’s magnetic moment. In any event, the muon results are exciting but still below what is required for a true discovery. Time will tell.</p>
<p>
The second factor we draw attention to concerns the human “hoping” directly.</p>
<p></p><h2> Blinding </h2><p></p>
<p>
In any experimental science one must also be aware that people are not unbiased. Scientists have much invested in the outcome of their experiments. Think jobs, tenure even, funding, and more. So a big physics experiment like the muon one must be careful. They follow standard practice to perform <a href="https://en.wikipedia.org/wiki/blinded_experiment">blinded</a> data analysis. </p>
<p>
This surprised me. This blinding is a crypto-type protocol, which is something we computer scientists study. The muon team performed a protocol that protected against cheating. Here is how they did it:</p>
<blockquote><p><b> </b> <em> In this case, the master clock that keeps track of the muons’ wobble had been set to a rate unknown to the researchers. The figure was sealed in envelopes that were locked in the offices at Fermilab and the University of Washington in Seattle.</em></p><em>
<p>
In a ceremony on Feb. 25 that was recorded on video and watched around the world on Zoom, Dr. Polly opened the Fermilab envelope and David Hertzog from the University of Washington opened the Seattle envelope. The number inside was entered into a spreadsheet, providing a key to all the data, and the result popped out to a chorus of wows.</p>
</em><p><em>
“That really led to a really exciting moment, because nobody on the collaboration knew the answer until the same moment,” said Saskia Charity, a Fermilab postdoctoral fellow who has been working remotely from Liverpool, England, during the pandemic. </em>
</p></blockquote>
<p>
This mechanism for blinding suggests possible crypto questions. They hid the master clock rate. Can this be modeled as one of our crypto problems? Can we prove some security bounds? If they claim that hiding the rate protects against cheating then they should be able to make this claim precise. The <a href="https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves">discovery</a> of gravitational waves used a <a href="https://www.ligo.org/news/blind-injection.php">blind injection</a> scheme tailored for that experiment. How can this be generalized?</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
We have discussed two aspects that involve soft numbers rather than hard machines and hard-shelled particles. Perhaps they are interesting new problems for us? What do you think?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/"><span class="datestr">at April 12, 2021 11:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/052">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/052">TR21-052 |  Upslices, Downslices, and Secret-Sharing with Complexity of $1.5^n$ | 

	Oded Nir, 

	Benny Applebaum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A secret-sharing scheme allows to distribute a secret $s$ among $n$ parties such that only some predefined ``authorized'' sets of parties can reconstruct the secret, and all other ``unauthorized'' sets learn nothing about $s$. 
The collection of authorized/unauthorized sets can be captured by a monotone function $f:\{0,1\}^n\rightarrow \{0,1\}$. 
In this paper, we focus on monotone functions that all their min-terms are sets of size $a$, and on their duals -- monotone functions whose max-terms are of size $b$. We refer to these classes as $(a,n)$-upslices and $(b,n)$-downslices, and note that these natural families correspond to monotone $a$-regular DNFs and monotone $(n-b)$-regular CNFs. We derive the following results.

1. (General downslices) Every downslice can be realized with total share size of $1.5^{n+o(n)}&lt;2^{0.585 n}$. Since every monotone function can be cheaply decomposed into $n$ downslices, we obtain a similar result for general access structures improving the previously known $2^{0.637n+o(n)}$ complexity of Applebaum, Beimel, Nir and Peter (STOC 2020). We also achieve a minor improvement in the exponent of linear secrets sharing schemes. 

2. (Random mixture of upslices) Following Beimel and Farras (TCC 2020) who studied the complexity of random DNFs with constant-size terms, we consider the following general distribution $F$ over monotone DNFs: For each width value $a\in [n]$, uniformly sample $k_a$ monotone terms of size $a$, where $k=(k_1,\ldots,k_n)$ is an arbitrary vector of non-negative integers. We show that, except with exponentially small probability, $F$ can be realized with share size of $2^{0.5 n+o(n)}$ and
    can be linearly realized with an exponent strictly smaller than $2/3$. Our proof also provides a candidate distribution for ``exponentially-hard'' access structure. 
    
We use our results to explore connections between several seemingly unrelated questions about the complexity of secret-sharing schemes such as worst-case vs. average-case, linear vs. non-linear and primal vs. dual access structures. We prove that, in at least one of these settings, there is a significant gap in secret-sharing complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/052"><span class="datestr">at April 12, 2021 05:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8147147831052480535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html">Is the following reaction to getting the first COVID shot logical?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Alice works at a charity that puts together bag and box lunches for children.</p><p><br />They all wear masks and they are 12 feet apart and very careful, and nobody there has gotten COVID.</p><p>Then Alice gets here first COVID shot and says:</p><p><br /></p><p><i>I am not going to work for that charity until I have had my second shot and waited  4 weeks so I am immune. </i></p><p><i><br /></i></p><p>She is really scared of getting COVID NOW that  she is on the verge of being immune. </p><p><br /></p><p>Is that logical? She was not scared before. So does it make sense to be scared now? I see where she is coming from emotionally, but is there a logical argument for her viewpoint? I ask nonrhetorically.</p><p><br /></p><p>bill g. </p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html"><span class="datestr">at April 12, 2021 04:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5437">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5437">Just some prizes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://en.wikipedia.org/wiki/Oded_Goldreich">Oded Goldreich</a> is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel.  He’s best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the <a href="https://en.wikipedia.org/wiki/Hard-core_predicate">Goldreich-Levin Theorem</a> (every one-way function can be modified to have a hard-core predicate), the <a href="https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How%20To%20Construct%20Random%20Functions.pdf">Goldreich-Goldwasser-Micali Theorem</a> (every pseudorandom generator can be made into a pseudorandom function), and the <a href="https://www.cs.purdue.edu/homes/hmaji/teaching/Fall%202017/lectures/39.pdf">Goldreich-Micali-Wigderson protocol</a> for secure multi-party computation.  I first met Oded more than 20 years ago, when he lectured at a summer school at the Institute for Advanced Study in Princeton, barefoot and wearing a tank top and what looked like pajama pants.  It was a bracing introduction to complexity-theoretic cryptography.  Since then, I’ve interacted with Oded from time to time, partly around his <a href="http://www.wisdom.weizmann.ac.il/~oded/on-qc.html">firm belief</a> that quantum computing is impossible.</p>



<p>Last month a committee in Israel voted to award Goldreich the <a href="https://en.wikipedia.org/wiki/Israel_Prize">Israel Prize</a> (roughly analogous to the US National Medal of Science), for which I’d say Goldreich had been a plausible candidate for decades.  But alas, Yoav Gallant, Netanyahu’s Education Minister, then rather <a href="https://www.jpost.com/israel-news/high-court-revokes-israel-prize-in-math-to-pro-bds-professor-664538">non-gallantly blocked the award</a>, solely because he objected to Goldreich’s far-left political views (and apparently because of various statements Goldreich signed, including in support of a boycott of Ariel University, which is in the West Bank).  The case went all the way to the Israeli Supreme Court (!), which <a href="https://www.washingtonpost.com/world/middle_east/israeli-academic-wont-receive-prize-after-signing-petition/2021/04/08/d1e987ca-987b-11eb-8f0a-3384cf4fb399_story.html">ruled two days ago</a> in Gallant’s favor: he gets to “delay” the award to investigate the matter further, and in the meantime has apparently sent out invitations for an award ceremony next week that doesn’t include Goldreich.  Some are now calling for the other winners to boycott the prize in solidarity until this is righted.</p>



<p>I doubt readers of this blog need convincing that this is a travesty and an embarrassment, a <em><a href="https://en.wiktionary.org/wiki/shanda#:~:text=shanda%20(uncountable),(Jewish)%20shame%3B%20disgrace.">shanda</a></em>, for the Netanyahu government itself.  That I disagree with Goldreich’s far-left views (or <em>might</em> disagree, if I knew in any detail what they were) is totally immaterial to that judgment.  In my opinion, not even Goldreich’s belief in the impossibility of quantum computers should affect his eligibility for the prize. <img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /></p>



<p>Maybe it would be better to say that, as far as his academic colleagues in Israel and beyond are concerned, Goldreich <em>has</em> won the Israel Prize; it’s only some irrelevant external agent who’s blocking his receipt of it.  Ironically, though, among Goldreich’s many heterodox beliefs is a <a href="http://www.wisdom.weizmann.ac.il/~oded/on-awards1.html">total rejection of the value of scientific prizes</a> (although Goldreich has also said he wouldn’t refuse the Israel Prize if offered it!).</p>



<p></p><hr /><p></p>



<p>In unrelated news, the 2020 Turing Award has been given to <a href="https://en.wikipedia.org/wiki/Alfred_Aho">Al Aho</a> and <a href="https://en.wikipedia.org/wiki/Jeffrey_Ullman">Jeff Ullman</a>.  Aho and Ullman have both been celebrated leaders in CS for half a century, having laid many of the foundations of formal languages and compilers, and having coauthored one of CS’s <a href="https://www.amazon.com/Design-Analysis-Computer-Algorithms/dp/0201000296/ref=pd_lpo_14_t_1/140-9181226-0879049?_encoding=UTF8&amp;pd_rd_i=0201000296&amp;pd_rd_r=4c6cd308-4669-45ee-ad4d-292ee24e043f&amp;pd_rd_w=WxNWv&amp;pd_rd_wg=AAt3G&amp;pf_rd_p=337be819-13af-4fb9-8b3e-a5291c097ebb&amp;pf_rd_r=T95EJ78DHVE1V1G604D1&amp;psc=1&amp;refRID=T95EJ78DHVE1V1G604D1">defining textbooks</a> with <a href="https://en.wikipedia.org/wiki/John_Hopcroft">John Hopcroft</a> (who already received a different Turing Award).</p>



<p>But again there’s a controversy.  <a href="https://lobelog.com/niac-calls-out-anti-iranian-stanford-professor/">Apparently</a>, in 2011, Ullman wrote to an Iranian student who wanted to work with him, saying that as “a matter of principle,” he would not accept Iranian students until the Iranian government recognized Israel.  Maybe I should say that I, like Ullman, am both a Jew and a Zionist, but I find it hard to imagine the state of mind that would cause me to hold some hapless student responsible for the misdeeds of their birth-country’s government.  Ironically, this is a mirror-image of the <a href="https://en.wikipedia.org/wiki/Academic_boycott_of_Israel#Mona_Baker,_Miriam_Shlesinger_and_Gideon_Toury">tactics</a> that the BDS movement has wielded against Israeli academics.  Unlike Goldreich, though, Ullman seems to have gone beyond merely expressing his beliefs, actually turning them into a one-man foreign policy.</p>



<p>I’m <a href="https://www.scottaaronson.com/blog/?p=3167">proud</a> of the Iranian students I’ve mentored and hope to mentor more.  While I don’t think this issue should affect Ullman’s Turing Award (and I haven’t seen anyone claim that it should), I do think it’s appropriate to use the occasion to express our opposition to all forms of discrimination.  I fully endorse Shafi Goldwasser’s <a href="https://www.facebook.com/SimonsInstitute/">response</a> in her capacity as Director of the Simons Institute for Theory of Computing in Berkeley:</p>



<blockquote class="wp-block-quote"><p>As a senior member of the computer science community and an American-Israeli, I stand with our Iranian students and scholars and outright reject any notion by which admission, support, or promotion of individuals in academic settings should be impeded by national origin or politics. Individuals should not be conflated with the countries or institutions they come from. Statements and actions to the contrary have no place in our computer science community. Anyone experiencing such behavior will find a committed ally in me.</p></blockquote>



<p>As for Al Aho?  I knew him fifteen years ago, when he became interested in quantum computing, in part due to his then-student <a href="https://www.microsoft.com/en-us/research/people/ksvore/">Krysta Svore</a> (who’s now the head of Microsoft’s quantum computing efforts).  Al struck me as not only a famous scientist but a gentleman who radiated kindness everywhere.  I’m not aware of any controversies he’s been involved in and never heard anyone say a bad word about him.</p>



<p>Anyway, this seems like a good occasion to recognize some foundational achievements in computer science, as well as the complex human beings who produce them!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5437"><span class="datestr">at April 09, 2021 06:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/051">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/051">TR21-051 |  Binary Interactive Error Resilience Beyond $1/8$ (or why $(1/2)^3 &amp;gt; 1/8$) | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Interactive error correcting codes are codes that encode a two party communication protocol to an error-resilient protocol that succeeds even if a constant fraction of the communicated symbols are adversarially corrupted, at the cost of increasing the communication by a constant factor. What is the largest fraction of corruptions that such codes can protect against? 

If the error-resilient protocol is allowed to communicate large (constant sized) symbols, Braverman and Rao  (STOC, 2011) show that the maximum rate of corruptions that can be tolerated is $1/4$. They also give a binary interactive error correcting protocol that only communicates bits and is resilient to $1/8$ fraction of errors, but leave the optimality of this scheme as an open problem.

We answer this question in the negative, breaking the $1/8$ barrier. Specifically, we give a binary interactive error correcting scheme that is resilient to $5/39 &gt; 1/8$ fraction of adversarial errors. Our scheme builds upon a novel construction of binary list-decodable interactive codes with small list size.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/051"><span class="datestr">at April 09, 2021 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-709704913623244646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">Quantum Stories</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Scott Aaronson <a href="https://www.scottaaronson.com/blog/?p=5387">wrote last month</a> about the hype over quantum computing. I'd thought I'd drop a few stories.</p><p>I was once asked to review a grant proposal (outside the US) that claimed it would find a quantum algorithm for NP-hard problems. I wrote a scathing review but the grant was funded because I failed to prove that it was impossible. I replied that they should fund my research to teleport people from Chicago to Paris because they couldn't prove I couldn't do it. I never got a response.</p><div>I was at an NSF sponsored meeting on quantum computing. I suggested, as a complexity theorist, that we need to explore the limits of quantum computing. A senior researcher said we shouldn't mention that in the report or it might hurt our chances of funding the field if they think quantum computing might not be a complete success.</div><p>I went to a Microsoft Faculty Research Summit which had a big focus on quantum computing. I complained of the quantum computing hype. My friends in the field denied the hype. Later at the summit a research head said that Microsoft will solve world hunger with quantum computing.</p><p>I was meeting with a congressional staffer who had worked on the National Quantum Initiative which coincidentally was being announced that day. I said something about high risk, high reward. He looked shocked--nobody had told him before that quantum computing is a speculative technology.</p><p>Quantum computing has generated a large number of beautiful and challenging scientific questions. Thinking about quantum has helped generate classical complexity and algorithmic results. But quantum computing having a real-world impact in the near or mid-term is unlikely. Most scientists I know working directly in quantum research are honest about the limitations and challenges in quantum computing. But somehow that message is not often getting to the next layers up, the policy makers, the research managers, the university administrators, the media and the venture capitalists. </p><p>But who knows, maybe some quantum heuristic that doesn't need much entanglement will change the world tomorrow. I can't prove it's impossible.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html"><span class="datestr">at April 08, 2021 12:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-21129445.post-1274107059735040105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/pizza.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html">Postdoctoral openings at Amazon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists" target="_blank">Amazon Advertising opens applications for early career scientists.</a> The new program, which offers full-time two-year positions, is aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. The application deadline is May 14.</p></div>







<p class="date">
by metoo (noreply@blogger.com) <a href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html"><span class="datestr">at April 08, 2021 04:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/04/07/ripvanwinkle/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/04/07/ripvanwinkle/">Rip van Winkle's Razor, a Simple New Estimate for Adaptive Data Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><em>Can you trust a model whose designer had access to the test/holdout set?</em> This implicit question 
in <a href="https://science.sciencemag.org/content/349/6248/636.full">Dwork et al 2015</a> launched a new field, <em>adaptive data analysis</em>.
The question referred to the fact that in many scientific settings as well as modern machine learning (with its standardized datasets like CIFAR, 
ImageNet etc.) the model designer has full access to the holdout set and is free to ignore the</p>

<blockquote>
  <p>(Basic Dictum of Data Science) “Thou shalt not train on the test/holdout set.”</p>
</blockquote>

<p>Furthermore, even researchers who scrupulously follow the Basic Dictum may be unknowingly violating it when they take inspiration (and design choices) 
from published works by others who presumably published <em>only the best of the many models they evaluated on the test set.</em></p>

<p>Dwork et al. showed that if the test set has size $N$, and the designer is allowed to see the error of the first $i-1$ models on the test set before designing the $i$’th model, then a clever designer can  use so-called <a href="https://arxiv.org/pdf/1502.04585.pdf"><em>wacky boosting</em></a> (see this <a href="http://blog.mrtz.org/2015/03/09/competition.html">blog post</a>) to ensure the accuracy of the $t$’th model on the test set as high as $\Omega(\sqrt{t/N})$. In other words, the test set could become essentially useless once $t \gg N$, a 
condition that holds in ML, whereby in popular datasets (CIFAR10, CIFAR100, ImageNet etc.)  $N$ is no more than $100,000$ and the total number of models being trained 
world-wide is well in the millions if not higher (once you include hyperparameter searches).</p>

<blockquote>
  <p><strong>Meta-overfitting Error (MOE)</strong> of a model is the difference between its average error on the test data and its expected error on the full distribution.
(It is closely related to <a href="https://en.wikipedia.org/wiki/False_discovery_rate"><em>false discovery rate</em></a> in statistics.)</p>
</blockquote>

<p>This blog post concerns <a href="https://arxiv.org/pdf/2102.13189.pdf">our new paper</a>, which gives meaningful upper bounds on this sort of trouble for popular 
deep net architectures, whereas prior ideas from adaptive data analysis gave no nontrivial estimates. We call our estimate  <em>Rip van Winkle’s Razor</em> 
which combines references to <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s Razor</a> and the 
<a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">mythical person who fell asleep for  20 years</a>.</p>

<figure align="center">
<img width="50%" alt="drawing" src="http://www.offconvex.org/assets/ripvanwinkle.jpg" />
   Rip Van Winkle wakes up from 20 years of sleep, clearly needing a Razor 
</figure>

<h2 id="adaptive-data-analysis-brief-tour">Adaptive Data Analysis: Brief tour</h2>

<p>It is well-known that for a model trained <strong>without</strong> ever querying the test set, MOE scales (with high probability over choice of the test set) as $1/\sqrt{N}$ where $N$ 
is the size of the test set.  Furthermore standard concentration bounds imply that even if we train $t$ models without ever referring to the test set (in other words, 
using proper data hygiene) then the maximum meta-overfitting error among the $t$ models scales whp as $O(\sqrt{\log(t)/ N})$. The trouble pinpointed by Dwork et al. 
can happen only if models are designed adaptively, with test error of the previous models shaping the design of the next model.</p>

<p>Adaptive Data Analysis has come up with many good practices for honest researchers to mitigate such issues. For instance, Dwork et al. showed that using 
Differential Privacy on labels while evaluating models can lower MOE. Or the <a href="https://arxiv.org/pdf/1502.04585.pdf">Ladder mechanism</a> helps in Kaggle-like 
settings where the test dataset resides on a server that can choose to answers only a  selected subset of queries, which essentially takes away the MOE issue.</p>

<p>For several good practices  matching lower bounds exist showing a way to construct cheating models with MOE matching the upper bound.</p>

<p>However such recommended best practices do not help with understanding the MOE in the performance numbers of a new model  since there is no guarantee that the 
inventors never tuned models using the test set, or didn’t get inspiration from existing models that may have been designed that way.  Thus statistically 
speaking the above results still give no reason to believe that a modern deep net such as ResNet152 has low MOE.</p>

<p><a href="http://proceedings.mlr.press/v97/recht19a/recht19a.pdf">Recht et al. 2019</a> summed up the MOE issue in a catchy title: <em>Do ImageNet Classifiers Generalize to ImageNet?</em>  They tried to answer their question experimentally by creating new test sets from scratch –we discuss their results later.</p>

<h2 id="moe-bounds-and-description-length">MOE bounds and description length</h2>

<p>The starting point of our work is the following classical concentration bounds:</p>

<blockquote>
  <p><strong>Folklore Theorem</strong> With high probability over the choice of a test set of size $N$, the MOE of <em>all</em> models with description length at most $k$ bits is  $O(\sqrt{k/N})$.</p>
</blockquote>

<p>At first sight this doesn’t seem to help us because one cannot imagine modern deep nets having a short description. The most obvious description involves reporting 
values of the net parameters, which requires millions or even hundreds of millions of bits, resulting in a vacuous upper bound  on MOE.</p>

<p>Another obvious description would be the computer program used to produce the model using the (publicly available) training and validation sets. However, these 
programs usually rely on imported libraries through layers of encapsulation and so the effective program size is pretty large as well.</p>

<h2 id="rip-van-winkles-razor">Rip van Winkle’s Razor</h2>
<p>Our new upper bound involves a more careful definition of <em>Description Length</em>: it is the smallest description that allows a referee  to reproduce a model of 
similar performance using the (universally available) training and validation datasets.</p>

<p>While this phrasing may appear reminiscent of the review process for conferences and journals, there is a subtle difference  with respect to what the referee 
can or cannot be assumed to know. (Clearly, assumptions about the referee can greatly affect description length —e.g,  a referee ignorant of even basic 
calculus might need a very long explanation!)</p>

<blockquote>
  <p><strong>Informed Referee:</strong> “Knows everything that was known to humanity (e.g., about deep learning, mathematics,optimization, statistics etc.) right up to the 
moment of creation of the Test set.”</p>
</blockquote>

<blockquote>
  <p><strong>Unbiased Referee:</strong> Knows nothing discovered since the Test set was created.</p>
</blockquote>

<p>Thus <em>Description Length</em> of a model is the number of bits in the shortest description that allows an informed but unbiased referee to reproduce the claimed result.</p>

<p>Note that informed referees let descriptions get shorter. Unbiased require longer descriptions that rule out any statistical “contamination” due to any interaction whatsoever with the test set. For example, momentum techniques in optimization were 
well-studied before the creation of ImageNet test set, so informed referees can be expected to understand a line like “SGD with momentum 0.9.” But a 
line like “Use Batch Normalization” cannot be understood by unbiased referees since conceivably this technique (invented after 2012) might have
 become popular precisely because it leads to better performance on the test set of ImageNet.</p>

<p>By now it should be clear why the estimate is named after  <a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">“Rip van Winkle”</a>: the referee can be thought 
of as an infinitely well-informed researcher who went into deep sleep at the moment of creation of the test set, and has just been woken up years later 
to start refereeing the  latest papers.  Real-life journal referees who luckily did not suffer this way should try to simulate the idealized Rip van Winkle 
in their heads while perusing the description submitted by the researcher.</p>

<p>To allow as short a  description as possible the researcher is allowed to compress the description of their new deep net non-destructively using any compression  that would make sense to Rip van Winkle (e.g., <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Coding</a>). The description of the compression method itself 
is not counted towards the description length – provided the same method is used for all papers submitted to Rip van Winkle. To give an example, a 
technique appearing in a text known to Rip van Winkle could be succinctly referred to using the book’s ISBN number and page number.</p>

<h2 id="estimating-moe-of-resnet-152">Estimating MOE of ResNet-152</h2>
<p>As an illustration, here we provide a suitable description allowing  Rip van Winkle to reproduce a mainstream ImageNet model, ResNet-152, which achieves $4.49\%$ top-5 
test error.</p>

<p>The description consists of three types of expressions: English phrases, Math equations, and directed graphs. In the paper, we describe in detail how to encode 
each of them into binary strings and count their lengths.  The allowed vocabulary includes primitive concepts that were known before 2012, such 
as <em>CONV, MaxPool, ReLU, SGD</em> etc., as well as a graph-theoretic notation/shorthand  for describing net architecture. The newly introduced concepts 
including <em>Batch-Norm</em>, <em>Layer, Block</em> are defined precisely using Math, English, and other primitive concepts.</p>

<figure align="center">
<img width="80%" alt="drawing" src="http://www.offconvex.org/assets/resnet_description.png" />
  <b>Description for reproducing ResNet-152</b>
</figure>

<p>According to our estimate, the length of the above description is $1032$ bits, which translates into a upper bound on meta-overfitting error of merely $5\%$! 
This suggests the real top-5 error of the model on full distribution is at most $9.49\%$. In the paper we also provide a $980$-bit long description for 
reproducing DenseNet-264, which leads to $5.06\%$ upper bound on its meta-overfitting error.</p>

<p>Note that the number $5.06$ suggests higher precision than actually given by the method, since it is possible to quibble about the coding assumptions 
that led to it.  Perhaps others might use a more classical coding mechanism and obtain an estimate of $6\%$ or $7\%$.</p>

<p>But the important point is that unlike  existing bounds in Adaptive Data Analysis, there is <strong>no</strong> dependence on $t$, the number of models that have been tested before, and the bound is non-vacuous.</p>

<h2 id="empirical-evidence-about-lack-of-meta-overfitting">Empirical evidence about lack of meta-overfitting</h2>

<p>Our estimates indicate that the issue of meta-overfitting on ImageNet for these mainstream models is mild. The reason is that despite the vast number
 of parameters and hyper-parameters in today’s deep nets, the <em>information content</em> of these models is not high given  knowledge circa 2012.</p>

<p>Recently Recht et al. <a href="https://arxiv.org/abs/1902.10811">tried to reach an empirical upper bound on MOE</a> for
ImageNet and <a href="https://arxiv.org/abs/1806.00451">CIFAR-10</a>. They created new tests sets by carefully replicating the methodology used for constructing the original ones. They found that error of famous published models of the past seven years is as much as 10-15% higher on the new test set as compared to the original.  On the face of it, this seemed to confirm a case of bad meta-overfitting. But they  also presented evidence  that the swing in test error was due to systemic effects during test set creation. For instance, a comparable swing happens also for models that predated the creation of ImageNet (and thus were not overfitted to the ImageNet test set). 
<a href="https://proceedings.neurips.cc/paper/2019/hash/ee39e503b6bedf0c98c388b7e8589aca-Abstract.html">A followup study</a> of a hundred Kaggle competitions used fresh, 
identically distributed test sets that were available from the official competition organizers. The authors concluded that MOE does not appear to be significant in modern ML.</p>

<h2 id="conclusions">Conclusions</h2>
<p>To us the  disquieting takeaway from Recht et al.’s results was that  estimating MOE by creating a new test set is rife with systematic bias at best, and perhaps impossible, especially in datasets concerning rare or one-time phenomena (e.g., stock prices).  Thus their work still left a pressing need for effective upper bounds on  meta-overfitting error. Our Rip van Winkle’s Razor is elementary, and easily deployable by the average researcher. We hope it becomes part of the standard toolbox in Adaptive Data Analysis.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/04/07/ripvanwinkle/"><span class="datestr">at April 07, 2021 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1613">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/">Ensuring equity in high-stakes online advertising</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility down to advertisers rather than addressing it at the platform leaves manipulating a complex system to those least aware of the system’s inner workings. She then proposes a simpler, more unified solution to this problem: advertising slots should be either targetable or untargetable, and high-stakes ads should be in the untargeted segment. Finally, the post concludes with a discussion of how this segmentation need not cost these systems substantial revenue if reserve prices are used appropriately.</p>



<p></p>



<p><a href="https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420">https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420</a></p></div>







<p class="date">
by jamiemorgenstern <a href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/"><span class="datestr">at April 07, 2021 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=547">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/">TCS+ talk: Wednesday, April 14 — Andrea Lincoln, UC Berkeley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: left;">The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://sites.google.com/site/andrealiresume/home"><strong>Andrea Lincoln</strong></a> from UC Berkeley will speak about “<em>New Techniques for Proving Fine-Grained Average-Case Hardness</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In this talk I will cover a new technique for worst-case to average-case reductions. There are two primary concepts introduced in this talk: “factored” problems and a framework for worst-case to average-case fine-grained (WCtoACFG) self reductions.</p>
<p>We will define new versions of OV, kSUM and zero-k-clique that are both worst-case and average-case fine-grained hard assuming the core hypotheses of fine-grained complexity. We then use these as a basis for fine-grained hardness and average-case hardness of other problems. Our hard factored problems are also simple enough that we can reduce them to many other problems, e.g. to edit distance, k-LCS and versions of Max-Flow. We further consider counting variants of the factored problems and give WCtoACFG reductions for them for a natural distribution.</p>
<p>To show hardness for these factored problems we formalize the framework of [Boix-Adsera et al. 2019] that was used to give a WCtoACFG reduction for counting k-cliques. We define an explicit property of problems such that if a problem has that property one can use the framework on the problem to get a WCtoACFG self reduction. In total these factored problems and the framework together give tight fine-grained average-case hardness for various problems including the counting variant of regular expression matching.</p>
<p>Based on joint work with Mina Dalirrooyfard and Virginia Vassilevska Williams.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/"><span class="datestr">at April 06, 2021 08:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/06/tcs-summer-school-call-for-tas/">TCS summer school – call for TAs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TL;DR: The <a href="https://boazbk.github.io/tcs-summerschool/">summer school</a> we are organizing is looking for TAs.  Please forward this to your students as well as any departmental mailing lists.<br /></p>



<p>Are you passionate about teaching? Or about increasing diversity within TCS? If so, we need your help!</p>



<p>The committee for advancement of theoretical computer science (CATCS) is organizing an online summer course that will take place on May 31 till June 4, 2021. New horizons in theoretical computer science is a week-long online summer school which will expose undergraduates to exciting research areas in the area of theoretical computer science and its applications. The school will contain several mini-courses from top researchers in the field. We particularly encourage participants from groups that are currently under-represented in TCS. See <a href="https://boazbk.github.io/tcs-summerschool/" target="_blank" rel="noreferrer noopener">https://boazbk.github.io/tcs-summerschool/</a> for more details.</p>



<p>We are looking for TAs to help run the school.</p>



<p>TAs will have the following responsibilities:<br />        • Plan team building and ice breaking activities and social events for the summer school<br />        • Lead small groups during the week<br />        • Monitor questions in chat during lectures<br />        • Work with one of the instructors to prepare one homework<br />        • Grade homework<br />        • Provide mentorship to students<br />        • Possibly assist with reviewing applications and other technical/admin aspects of running the school</p>



<p>The time commitment will be ~20 hours during the week of May 31-June 4; ~5-10 hours prior to that week; and ~2-3 hours following that week. We are hoping to pay an amount of $500 to each TA (please note that international students will need a CPT for this).</p>



<p>To apply for a TA position, please fill in the application form at <a href="https://forms.gle/QCxLn8R81Ga4JQLH8" target="_blank" rel="noreferrer noopener">https://forms.gle/QCxLn8R81Ga4JQLH8</a>  by April 15, 2021. Please also have a faculty advisor send a short recommendation to <a href="mailto:summer-school-admin@boazbarak.org" target="_blank" rel="noreferrer noopener">summer-school-admin@boazbarak.org</a>. Please ask them to use the subject “TA recommendation for &lt;&lt;Your Name&gt;&gt;”.</p>



<p>Course organizers: Boaz Barak (Harvard), Shuchi Chawla (UT Austin), Madhur Tulsiani (TTI-Chicago)</p>



<p>Current list of confirmed instructors: Antonio Blanca (Penn State University), Ashia Wilson (MIT), Jelani Nelson (UC Berkeley), Nicole Immorlica (Microsoft Research), Yael Kalai (Microsoft research).</p>



<p>Please email  <a href="mailto:summer-school-admin@boazbarak.org" target="_blank" rel="noreferrer noopener">summer-school-admin@boazbarak.org</a> with any questions.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/06/tcs-summer-school-call-for-tas/"><span class="datestr">at April 06, 2021 05:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/04/06/ci-fellows-at-any-apply-by-may-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/04/06/ci-fellows-at-any-apply-by-may-1-2021/">CI Fellows at Any (apply by May 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>CIFellows is a general CRA/CCC program for matching postdocs with advisors, and supports 2-year postdocs. Applications will open mid-April, and are due early May. It is competitive across all areas of CS.</p>
<p>Website: <a href="https://cifellows2021.org/">https://cifellows2021.org/</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/04/06/ci-fellows-at-any-apply-by-may-1-2021/"><span class="datestr">at April 06, 2021 01:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18492">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/06/relaxing-the-primes/">Relaxing the Primes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Although the prime numbers are rigidly determined, they somehow feel like experimental data—Tim Gowers</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/06/relaxing-the-primes/sm/" rel="attachment wp-att-18494"><img width="181" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/sm.jpg?resize=181%2C102&amp;ssl=1" class="alignright wp-image-18494" height="102" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Finnish Scientific Courage Award <a href="https://www.utu.fi/en/news/news/2016-academy-of-finland-award-granted-to-kaisa-matomaki">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Kaisa Matomäki is a Finnish mathematician working in <a href="https://en.wikipedia.org/wiki/Kaisa_Matomaki">number theory</a>. She has some terrific results on prime numbers—results that have won several important prizes including the 2021 Ruth Lyttle Satter <a href="https://en.wikipedia.org/wiki/Ruth_Lyttle_Satter_Prize_in_Mathematics">Prize</a>: It is presented to a woman who has made an outstanding contribution to mathematics research. The prize <a href="https://www.ams.org/news?news_id=6455">citation</a> specifically mentions a 2015 <a href="https://arxiv.org/abs/1501.04585">paper</a> with Maksym Radziwill that contributed to Terence Tao’s resolution of the Erdős discrepancy problem—and indeed we highlighted Tao’s followup work with her and Radziwill in our 2015 <a href="https://rjlipton.wpcomstaging.com/2015/09/24/frogs-and-lily-pads-and-discrepancy/">post</a> on this.</p>
<p>
Today, Ken and I thought we would combine one of her new deep theorems with some shallow observations.<br />
<span id="more-18492"></span></p>
<p>
The set of primes is denoted by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" />, as usual. As complexity theorists we have known that for decades that the Boolean circuit complexity of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" /> is polynomial. This follows from the existence of a polynomial random algorithm for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" /> due to Robert Solovay and Volker Strassen, and then applying Len Adleman’s connection between such algorithms and Boolean circuit complexity.</p>
<p>
Matomäki views <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" /> in a different way. As an analytic number theorist she is concerned with the density and structure of primes, not so much the complexity of recognizing or generating a prime.</p>
<p>
</p><h2> Relaxations </h2><p></p>
<p>
Exact results on the set of primes are hard to come by. </p>
<p>
<b>Twin Primes</b>: We know that there are infinitely many <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bp%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p+2}" class="latex" /> are prime. But not proved.</p>
<p>
<b>Goldbach</b>: We know that every even number from <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4}" class="latex" /> onward is the sum of two primes. But not proved.</p>
<p>
<b>Density of Primes</b>: We known that the gap between consecutive primes is at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" /> But not proved.</p>
<p>
The Riemann Hypothesis attempts to say more about density of the primes than the Prime Number Theorem does. It has been proved equivalent to some statements about approximate growth rates, yet even these forms have not been touched.</p>
<p>
Instead of trying for better approximate results about primes, can we learn more by relaxing the notion of “prime” itself? For each <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k \geq 1}" class="latex" />, define <img src="https://s0.wp.com/latex.php?latex=%7BP_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_k}" class="latex" /> to be the set of numbers that are products of at most <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> primes. Then <img src="https://s0.wp.com/latex.php?latex=%7BP_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_1}" class="latex" /> is the set of primes (not the prime powers) and <img src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_2}" class="latex" /> is the set of products of two primes—which include the <a href="https://en.wikipedia.org/wiki/Blum_integer">Blum integers</a> of special interest to cryptography. Collectively the <img src="https://s0.wp.com/latex.php?latex=%7BP_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_k}" class="latex" /> sets represent different levels of being “<a href="https://en.wikipedia.org/wiki/Almost_prime">almost prime</a>.” The motivating question is:</p>
<blockquote><p><b> </b> <em> How well and in what ways does the structure of the sets of almost-primes model the structure of the primes? </em>
</p></blockquote>
<p>
In 2010, John Friedlander and Henryk Iwaniec wrote a <a href="https://www.ams.org/books/coll/057/coll057-endmatter.pdf">monograph</a> titled <em>Opera de Cribro</em>, which is Latin for “Works of the Sieve.” They proved certain results about <img src="https://s0.wp.com/latex.php?latex=%7BP_%7B13%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_{13}}" class="latex" /> and speculated whether their methods improve them to work at least for <img src="https://s0.wp.com/latex.php?latex=%7BP_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_3}" class="latex" />. Then they say, “It would be interesting to get integers with at most two prime divisors.” This is where Matomäki comes in—with a second kind of relaxing, one applying to “almost all” members of <img src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_2}" class="latex" />.</p>
<p>
</p><h2> New Result </h2><p></p>
<p>
Matomäki has a brand new <a href="https://arxiv.org/abs/2012.11565">paper</a> (revised two weeks ago from a December original) titled, “Almost primes in almost all very short intervals.” It follows on from a <a href="https://arxiv.org/abs/1510.06005">paper</a> by one of her students, Joni Teräväinen, titled “Almost all primes in almost all short intervals.” </p>
<p>
Her main result is the following theorem. </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose <img src="https://s0.wp.com/latex.php?latex=%7Bh_X+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h_X \rightarrow \infty}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X \rightarrow \infty}" class="latex" /> and put <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta_X+%3D+h%5Clog+X%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Delta_X = h\log X}" class="latex" />. Then for almost all <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%28%5Cfrac%7BX%7D%7B2%7D%2CX%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \in (\frac{X}{2},X]}" class="latex" />, the interval <img src="https://s0.wp.com/latex.php?latex=%7B%28x-%5CDelta_X%2Cx%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x-\Delta_X,x]}" class="latex" /> contains a <img src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_2}" class="latex" />. </em>
</p></blockquote>
<p>
Roger Heath-Brown had established the corresponding result for primes (that is, for <img src="https://s0.wp.com/latex.php?latex=%7BP_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P_1}" class="latex" />) but only assuming simultaneously the Riemann hypothesis and the pair correlation conjecture for the zeros of the Riemann zeta function. With no hypotheses, the presence of primes in almost all intervals is known only when the intervals have length <img src="https://s0.wp.com/latex.php?latex=%7BX%5E%7B%5COmega%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X^{\Omega(1)}}" class="latex" />—concretely, <img src="https://s0.wp.com/latex.php?latex=%7BX%5E%7B1%2F20%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X^{1/20}}" class="latex" /> is known. Thus, Matomäki has traded off the use of conjectures for the relaxed notion of prime.</p>
<p>
The “almost call” is in the sense of average density of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> whose short interval is populated. This average instead of worst case mirrors our difficulties with circuit lower bounds. We can easily show that on average Boolean function have huge circuit lower bounds. But worst case bounds for specific functions is beyond reach. Hmmm.</p>
<p>
</p><h2> Lower Bounds </h2><p></p>
<p>
We <a href="https://rjlipton.wordpress.com/2021/01/14/priming-random-restrictions/">previously</a> proved: </p>
<blockquote><p><b>Theorem 2 (Lower Bound)</b> <em> For any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" />, the depth of a DeMorgan boolean circuit for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" /> is at least <img src="https://s0.wp.com/latex.php?latex=%7B%282-%5Cepsilon%29%5Clog_2+n+%2B+O%281%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(2-\epsilon)\log_2 n + O(1)}" class="latex" />. </em>
</p></blockquote>
<p>
Recall a DeMorgan formula <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> variables <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_1,\dots,x_n}" class="latex" /> is a binary tree whose leaves are labeled with variables or their negations, and whose internal nodes are labeled with either <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvee%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\vee}" class="latex" /> for OR and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cwedge%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\wedge}" class="latex" /> for AND gates. </p>
<p>
Here is a high level view of this result. See <a href="https://rjlipton.wordpress.com/2021/01/14/priming-random-restrictions/">here</a> for details.</p>
<p>
Assume there is a DeMorgan Boolean circuit for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" /> is at most <img src="https://s0.wp.com/latex.php?latex=%7B%282-%5Cepsilon%29%5Clog_2+n+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(2-\epsilon)\log_2 n + O(1)}" class="latex" /> depth. We can use the random restriction method to set lots of the inputs to random values <img src="https://s0.wp.com/latex.php?latex=%7B0%2F1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0/1}" class="latex" />. </p>
<p>
We claim that with high probability the input looks like 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1%2C%5Cdots%2Cx_m%2C+x_%7Bm%2B1%7D%2C%5Cdots%2Cx_n.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x_1,\dots,x_m, x_{m+1},\dots,x_n. " class="latex" /></p>
<p>The right-most bits 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bm%2B1%7D%2C%5Cdots%2Cx_%7Bn%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  x_{m+1},\dots,x_{n} " class="latex" /></p>
<p>are left unset, and the other bits to the left have some bits randomly set. Moreover, the number of bits in the above right is order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{\epsilon}}" class="latex" />.</p>
<p>
Now set all the bits in the left group that are unset also to random values <img src="https://s0.wp.com/latex.php?latex=%7B0%2F1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0/1}" class="latex" />. Leave the right group unset.</p>
<p>
The point of this is that if we assume that the formula had size at most order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B2-%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{2-\epsilon}}" class="latex" />, then we have that the formula is likely to be constant. But this contradicts the density of primes and composites. </p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>
Can Matomäki’s theorem be used to get stronger lower bounds on the complexity of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{PRIMES}}" class="latex" />? Can it go beyond the quadratic limit?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/04/06/relaxing-the-primes/"><span class="datestr">at April 06, 2021 04:43 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5900">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/i-am-writing-a-book/">I am writing a book!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">After several attempts, I finally found the energy to start writing a book. It grew out of lecture notes for a graduate class I taught last semester. I make the <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">draft</a> available so that I can get feedback before a (hopefully) final effort next semester.</p>



<p class="justify-text">The goal of the book is to present old and recent results in learning theory, for the most widely-used learning architectures. This book is geared towards theory-oriented students as well as students who want to acquire a basic mathematical understanding of algorithms used throughout machine learning and associated fields that are large users of learning methods, such as computer vision or natural language processing.</p>



<p class="justify-text">A particular effort is made to prove <strong>many results from first principles</strong>, while keeping the exposition as simple as possible. This will naturally lead to a choice of key results that show-case in simple but relevant instances the important concepts in learning theory. Some general results will also be presented without proofs. Of course the concept of first principles is subjective, and a good knowledge of linear algebra, probability theory and differential calculus will be assumed.</p>



<p class="justify-text">Moreover, I will focus on the part of learning theory that does not exist outside of algorithms that can be run in practice, and thus all algorithmic frameworks described in this book are routinely used. For most learning methods, some simple <strong>illustrative experiments</strong> are presented, with the plan to have accompanying code (Matlab, Julia, and Python) so that students can see for themselves that the algorithms are simple and effective in synthetic experiments.</p>



<p class="justify-text">This is <em>not</em> an introductory textbook on machine learning. There are already several good ones in several languages [1, 2]. Many topics are not covered, and many more are not covered in much depth. There are many good textbooks on learning theory that go deeper [3, 4, 5].</p>



<p class="justify-text">The choice of topics is arbitrary (and thus personal). Many important algorithmic frameworks are forgotten (e.g.,  reinforcement learning, unsupervised learning, etc.). Suggestions of extra themes are welcome! A few additional chapters are currently being written such as: ensemble learning, bandit optimization, probabilistic methods, structured prediction.</p>



<h2>Help wanted!</h2>



<p class="justify-text">This is still work in progress. In particular, there are still a lot of typos, probably some mistakes, and almost surely places where more details are needed; readers are most welcome to report them to me (and then get credit for it). Also, the bibliography is currently quite short and would benefit from some expansion (all suggestions welcome, in particular for giving proper credit).</p>



<p class="justify-text">Moreover, I am convinced that simpler mathematical arguments are possible in many places in the book. If you are aware of elegant and simple ideas that I have overlooked, please let me know.</p>



<h2>References</h2>



<p class="justify-text">[1] Chloé-Agathe Azencott. Introduction au Machine Learning. Dunod, 2019.<br />[2] Ethem Alpaydin. Introduction to Machine Learning. MIT Press, 2020.<br />[3] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2018.<br />[4] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.<br />[5] Andreas Christmann and Ingo Steinwart. Support Vector Machines. Springer, 2008.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/i-am-writing-a-book/"><span class="datestr">at April 05, 2021 08:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1500">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1500">News for March 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A somewhat quieter month by recent standards. <s>Three</s> Two papers: graph property testing and quantum distribution testing. <em>(Ed: The distribution testing paper was a revision of a paper we already covered in Sept 2020.)</em></p>



<p><strong>Robust Self-Ordering versus Local Self-Ordering</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/034">ECCC</a>). In Nov 2020, we covered a <a href="https://eccc.weizmann.ac.il/report/2020/160/">paper</a> that uses a tool called <em>self-ordered graphs, </em>that transferred bit string function lower bounds to graph property testing. Consider a labeled graph. A graph is self-ordered if its automorphism group only contains the identity element (it has no non-trivial isomorphisms). A graph is robustly self-ordered, if every permutation of the vertices leads to a (labeled) graph that is sufficiently “far” according to edit distance. Given a self-ordered graph \(G\), a local self-ordering procedure is the following. Given access to a copy \(G’\) of \(G\) and a vertex \(v \in V(G’)\), this procedure determines the (unique) vertex in \(V(G)\) that corresponds to \(v\) with sublinear queries to \(G\). In other words, it can locally “label” the graph. Intuitively, one would think that more robustly self-ordered graphs will be easier to locally label. This paper studies the relation between robust and local self-ordering. Curiously, this paper refutes the above intuition for bounded-degree graphs, and (weakly) confirms it for dense graphs. Roughly speaking, there are bounded degree graphs that are highly robustly self-ordered, for which any local self-ordering procedure requires \(\omega(\sqrt{n})\) queries. Moreover, there are bounded degree graphs with \(O(\log n)\)-query local self-ordering procedures, yet are not robustly self-ordered even for weak parameters. For dense graphs, the existence of fast non-adaptive local self-ordering procedures implies robust self-ordering.</p>



<p><strong>Testing identity of collections of quantum states: sample complexity analysis</strong> by Marco Fanizza, Raffaele Salvia, and Vittorio Giovannetti (<a href="https://arxiv.org/abs/2103.14511">arXiv</a>). This paper takes identity testing to the quantum setting. One should think of a \(d\)-dimensional quantum state as a \(d \times d\) density matrix (with some special properties). To learn the state entirely up to error \(\varepsilon\) would require \(O(\varepsilon^{-2} d^2)\) samples/measurements. A recent result of <a href="https://arxiv.org/pdf/1708.06002.pdf">Badescu-O’Donnell-Wright</a> proves that identity testing to a known state can be done significantly faster using \(O(\varepsilon^{-2} d)\) measurements. This paper takes this result a step further by consider a set of \(N\) quantum states. A “sample” is like a classical sample, where one gets a sample from a distribution of quantum states. The YES (“uniform”) case is when all the states are identical. The NO (“far from uniform”) case is when they are “far” from being the same state. This paper proves that \(O(\varepsilon^{-2}\sqrt{N}d)\) samples suffices for distinguishing these cases.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1500"><span class="datestr">at April 05, 2021 04:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7501643626120846375">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/do-any-np-reductions-use-deep.html">Do any NP-reductions use deep mathematics? Non-rhetically</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
BILL: Lets say we didn't know that FACTORING NPC --&gt; NP=coNP.<div>then what direction would we think Factoring in P or NPC? </div><div><br />STUDENT: In P. After all, Number Theory is a deep subject and I can imagine some deep Theorem in it leading to FACTORING in P. </div><div><br /></div><div>BILL: That cuts both ways. I can imagine some deep theorem in NT being the key to showing </div><div><br /></div><div>FACTORING poly-reduces to  SAT</div><div><br /></div><div>STUDENT: </div><div>Deep mathematics HAS been used for algorithms. Factoring algs is one example, The graph minor theorem yielding MANY problems in P is another. Can you give me an example where deep mathematics has been used for an NPC reduction?</div><div><br /></div><div>BILL: Oh. Gee. I do not know of any. </div><div><br /></div><div>STUDENT: If only you had some mechanism to ask the theory community. Maybe you could call it a web log, or weblog.</div><div><br /></div><div>BILL: If only...</div><div><br />QUESTIONS</div><div>1) Are there any NPC reductions that use deep math? (I realize that the phrase `deep math' is not well defined,)</div><div><br /></div><div>2) Are there other reductions that use deep math?</div><div><br /></div><div>3) If P NE NP then: </div><div>For all epsilon there is no approx-alg for MAX3SAT which yields  \ge  (7/8 + epsilon)OPT</div><div><br /></div><div>For all delta  there is no approx-alg for CLIQ which yields &gt; n^{-delta} OPT</div><div><br /></div><div>No approx-alg for SET COVER which yields \ge (ln n + o(1))OPT. </div><div><br /></div><div>All of these proofs use  the PCP machinery or something similar. My impression is that the original PCP theorem, while long, hard, and impressive, did not use deep math. I have a vague memory of some paper or PhD thesis stating that the ONLY theorem needed was that a poly of degree d over a finite field has \le d roots. </div><div><br /></div><div>However to get the optimal lower bounds seemed to use some deep math. But I am more ASKING than telling. </div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/do-any-np-reductions-use-deep.html"><span class="datestr">at April 05, 2021 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5402">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5402">The Computational Expressiveness of a Model Train Set: A Paperlet</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (April 5, 2021):</span></strong> So it turns out that Adam Chalcraft and Michael Greene already <a href="http://www.monochrom.at/turingtrainterminal/Chalcraft.pdf">proved the essential result of this post back in 1994</a> (hat tip to commenter Dylan).  Not terribly surprising in retrospect!</p>



<p></p><hr /><p></p>



<p>My son Daniel had his fourth birthday a couple weeks ago.  For a present, he got an electric train set.  (For completeness—and since the details of the train set will be rather important to the post—it’s called <a href="https://www.amazon.com/Dinosaur-Flexible-Tracks-Create-Children/dp/B07ZDLXSXK">“WESPREX Create a Dinosaur Track”</a>, but this is not an ad and I’m not getting a kickback for it.)</p>



<figure class="wp-block-video"><video src="https://www.scottaaronson.com/yjunctions.MOV"></video></figure>



<p>As you can see, the main feature of this set is a Y-shaped junction, which has a flap that can control which direction the train goes.  The logic is as follows:</p>



<ul><li>If the train is coming up from the “bottom” of the Y, then it continues to either the left arm or the right arm, depending on where the flap is.  It leaves the flap as it was.</li></ul>



<ul><li>If the train is coming down the left or right arms of the Y, then it continues to the bottom of the Y, <em>pushing the flap out of its way if it’s in the way</em>.  (Thus, if the train were ever to return to this Y-junction coming up from the bottom, not having passed the junction in the interim, it would necessarily go to the same arm, left or right, that it came down from.)</li></ul>



<p>The train set also comes with bridges and tunnels; thus, there’s no restriction of planarity.  Finally, the train set comes with little gadgets that can reverse the train’s direction, sending it back in the direction that it came from:</p>



<figure class="wp-block-video"><video src="https://www.scottaaronson.com/train1.MOV"></video></figure>



<p>These gadgets don’t seem particularly important, though, since we could always replace them if we wanted by a Y-junction together with a loop.</p>



<p>Notice that, at each Y-junction, the position of the flap stores one bit of internal state, and that the train can both “read” and “write” these bits as it moves around.  Thus, a question naturally arises: can this train set do any nontrivial computations?  If there are <em>n</em> Y-junctions, then can it cycle through exp(<em>n</em>) different states?  Could it even solve <strong>PSPACE</strong>-complete problems, if we let it run for exponential time?  (For a very different example of a model-train-like system that, as it turns out, <em>is</em> able to express <strong>PSPACE</strong>-complete problems, see <a href="https://arxiv.org/abs/1905.00518">this recent paper</a> by Erik Demaine et al.)</p>



<p>Whatever the answers regarding Daniel’s train set, I knew immediately on watching the thing go that I’d have to write a “paperlet” on the problem and publish it on my blog (no, I don’t inflict such things on journals!).  Today’s post constitutes my third “paperlet,” on the general theme of a discrete dynamical system that someone showed me in real life (e.g. in a children’s toy or in biology) having more structure and regularity than one might naïvely expect.  My first such paperlet, from 2014, was <a href="https://www.scottaaronson.com/blog/?p=1902">on a 1960s toy called the Digi-Comp II</a>; my second, from 2016, was <a href="https://www.scottaaronson.com/blog/?p=2862">on DNA strings acted on by recombinase</a> (OK, that one <em>was</em> associated with a <a href="https://science.sciencemag.org/content/353/6297/aad8559.full?ijkey=wzroPPh1eIu9k&amp;keytype=ref&amp;siteid=sci">paper in <em>Science</em></a>, but my combinatorial analysis wasn’t the main point of the paper).</p>



<p>Anyway, after spending an enjoyable evening on the problem of Daniel’s train set, I was able to prove that, alas, the possible behaviors are quite limited (I classified them all), falling far short of computational universality.</p>



<p>If you feel like I’m wasting your time with trivialities (or if you simply enjoy puzzles), then before you read any further, I encourage you to stop and try to prove this for yourself!</p>



<p>Back yet?  OK then…</p>



<p></p><hr /><p></p>



<p><strong>Theorem:</strong> Assume a finite amount of train track.  Then after a linear amount of time, the train will necessarily enter a “boring infinite loop”—i.e., an attractor state in which at most two of the flaps keep getting toggled, and the rest of the flaps are fixed in place.  In more detail, the attractor must take one of four forms:</p>



<p>I. a line (with reversing gadgets on both ends),<br />II. a simple cycle,<br />III. a “lollipop” (with one reversing gadget and one flap that keeps getting toggled), or<br />IV. a “dumbbell” (with two flaps that keep getting toggled).</p>



<p>In more detail still, there are seven possible topologically distinct trajectories for the train, as shown in the figure below.</p>



<figure class="wp-block-image size-large"><a href="https://www.scottaaronson.com/trajectories.png"><img src="https://www.scottaaronson.com/trajectories.png" alt="" /></a></figure>



<p>Here the red paths represent the attractors, where the train loops around and around for an unlimited amount of time, while the blue paths represent “runways” where the train spends a limited amount of time on its way into the attractor.  Every degree-3 vertex is assumed to have a Y-junction, while every degree-1 vertex is assumed to have a reversing gadget, unless (in IIb) the train starts at that vertex and never returns to it.</p>



<p>The proof of the theorem rests on two simple observations.</p>



<p><strong>Observation 1:</strong> While the Y-junctions correspond to vertices of degree 3, there are no vertices of degree 4 or higher.  This means that, if the train ever revisits a vertex <em>v</em> (other than the start vertex) for a second time, then there must be some edge <em>e</em> incident to <em>v</em> that it also traverses for a second time immediately afterward.</p>



<p><strong>Observation 2:</strong> Suppose the train traverses some edge <em>e</em>, then goes around a simple cycle (meaning, one where no edges or vertices are reused), and then traverses <em>e</em> again, <em>going in the same direction as the first time</em>.  Then from that point forward, the train will just continue around the same simple cycle forever.</p>



<p>The proof of Observation 2 is simply that, if there were any flap that might be in the train’s way as it continued around the simple cycle, then the train would already have pushed it out of the way its <em>first</em> time around the cycle, and nothing that happened thereafter could possibly change the flap’s position.</p>



<p>Using the two observations above, let’s now prove the theorem.  Let the train start where it will, and follow it as it traces out a path.  Since the graph is finite, at some point some already-traversed edge must be traversed a second time.  Let <em>e</em> be the first such edge.  By Observation 1, this will also be the first time the train’s path intersects itself at all.  There are then three cases:</p>



<p><strong>Case 1:</strong> The train traverses <em>e</em> in the same direction as it did the first time.  By Observation 2, the train is now stuck in a simple cycle forever after.  So the only question is what the train could’ve done <em>before</em> entering the simple cycle.  We claim that at most, it could’ve traversed a simple path.  For otherwise, we’d contradict the assumption that <em>e</em> was the first edge that the train visited twice on its journey.  So the trajectory must have type IIa, IIb, or IIc in the figure.</p>



<p><strong>Case 2:</strong> Immediately after traversing e, the train hits a reversing gadget and traverses <em>e</em> again the other way.  In this case, the train will clearly retrace its entire path and then continue past its starting point; the question is what happens next.  If it hits another reversing gadget, then the trajectory will have type I in the figure.  If it enters a simple cycle and stays in it, then the trajectory will have type IIb in the figure.  If, finally, it makes a simple cycle and then <em>exits</em> the cycle, then the trajectory will have type III in the figure.  In this last case, the train’s trajectory will form a “lollipop” shape.  Note that there must be a Y-junction where the “stick” of the lollipop meets the “candy” (i.e., the simple cycle), with the base of the Y aligned with the stick (since otherwise the train would’ve continued around and around the candy).  From this, we deduce that every time the train goes around the candy, it does so in a different orientation (clockwise or counterclockwise) than the time before; and that the train toggles the Y-junction’s flap every time it exits the candy (although not when it enters the candy).</p>



<p><strong>Case 3:</strong> At some point after traversing <em>e</em> in the forward direction (but not <em>immediately</em> after), the train traverses <em>e</em> in the reverse direction.  In this case, the broad picture is analogous to Case 2.  So far, the train has made a lollipop with a Y-junction connecting the stick to the candy (i.e. cycle), the base of the Y aligned with the stick, and <em>e</em> at the very top of the stick.  The question is what happens next.  If the train next hits a reversing gadget, the trajectory will have type III in the figure.  If it enters a new simple cycle, disjoint from the first cycle, and never leaves it, the trajectory will have type IId in the figure.  If it enters a new simple cycle, disjoint from the first cycle, and <em>does</em> leave it, then the trajectory now has a “dumbbell” pattern, type IV in the figure (also shown in the first video).  There’s only one other situation to worry about: namely, that the train makes a new cycle that <em>intersects</em> the first cycle, forming a “theta” (θ) shaped trajectory.  In this case, there must be a Y-junction at the point where the new cycle bumps into the old cycle.  Now, if the base of the Y isn’t part of the old cycle, then the train never could’ve made it all the way around the old cycle in the first place (it would’ve exited the old cycle at this Y-junction), contradiction.  If the base of the Y <em>is</em> part of the old cycle, then the flap must have been initially set to let the train make it all the way around the old cycle; when the train then reenters the old cycle, the flap must be moved so that the train will never make it all the way around the old cycle again.  So now the train is stuck in a new simple cycle (sharing some edges with the old cycle), and the trajectory has type IIc in the figure.</p>



<p>This completes the proof of the theorem.</p>



<p></p><hr /><p></p>



<p>We might wonder: <em>why</em> isn’t this model train set capable of universal computation, of AND, OR, and NOT gates—or at any rate, of <em>some</em> computation more interesting than repeatedly toggling one or two flaps?  My answer might sound tautological: it’s simply that the logic of the Y-junctions is too limited.  Yes, the flaps can get pushed out of the way—that’s a “bit flip”—but every time such a flip happens, it helps to set up a “groove” in which the train just wants to continue around and around forever, not flipping any additional bits, with only the minor complications of the lollipop and dumbbell structures to deal with.  Even though my proof of the theorem might’ve seemed like a tedious case analysis, it had this as its unifying message.</p>



<p>It’s interesting to think about what gadgets would need to be added to the train set to <em>make</em> it computationally universal, or at least expressively richer—able, as <a href="https://www.scottaaronson.com/blog/?p=1902">turned out</a> to be the case for the Digi-Comp II, to express some nontrivial complexity class falling short of <strong>P</strong>.  So for example, what if we had degree-4 vertices, with little turnstile gadgets?  Or multiple trains, which could be synchronized to the millisecond to control how they interacted with each other via the flaps, or which could even crash into each other?  I look forward to reading your ideas in the comment section!</p>



<p>For the truth is this: quantum complexity classes, BosonSampling, closed timelike curves, circuit complexity in black holes and AdS/CFT, etc. etc.—all these topics are great, but the same models and problems do get stale after a while.  I aspire for my research agenda to chug forward, full steam ahead, into new computational domains.</p>



<p>PS. Happy Easter to those who celebrate!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5402"><span class="datestr">at April 04, 2021 06:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/050">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/050">TR21-050 |  Linear Threshold Secret-Sharing with Binary Reconstruction | 

	Marshall Ball, 

	Alper Cakan, 

	Tal Malkin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Motivated in part by applications in lattice-based cryptography, we initiate the study of the size of linear threshold (`$t$-out-of-$n$') secret-sharing where the linear reconstruction function is restricted to coefficients in $\{0,1\}$. We prove upper and lower bounds on the share size of such schemes. One ramification of our results is that a natural variant of Shamir's classic scheme [Comm. of ACM, 1979], where bit-decomposition is applied to each share, is optimal for when the underlying field has characteristic 2. Another ramification is that schemes obtained from some monotone formulae are optimal for certain threshold values when the field's characteristic is any constant. We prove our results by defining and investigating an equivalent variant of Karchmer and Wigderson's Monotone Span Programs [CCC, 1993]. 

We also study the complexity such schemes with the additional requirement that the joint distribution of the shares of any unauthorized set of parties is not only independent of the secret, but also uniformly distributed. This property is critical for security of certain applications in lattice-based cryptography. We show that any such scheme must use $\Omega(n\log n)$ field elements, regardless of the field. Moreover, for any field this is tight up to constant factors for the special cases where any $t=n-1$ parties can reconstruct, as well as for any threshold when the field characteristic is 2.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/050"><span class="datestr">at April 04, 2021 06:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Natural Language Processing (guest lecture by Sasha Rush)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by Benjamin Basseri and <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://rush-nlp.com/">Alexander (Sasha) Rush</a> is a professor at Cornell working in in Deep Learning / NLP. He applies machine learning to problems of text generation, summarizing long documents, and interactions between character and word-based models. Sasha is previously at Harvard, where he taught an awesome NLP class, and we are excited to have him as our guest! (Note: some of the figures in this lecture are taken from other papers or presentations.)</p>



<p>The first half of the talk will focus on how NLP works and what makes it interesting: a bird’s-eye view of the field. The second half of the talk will focus on current research.</p>



<h2>Basics of NLP</h2>



<p>Textual data has many different challenges that differ from computer vision (CV), since it is a human phenomenon. There are methods that work in computer vision / other ML models that just don’t work for NLP (e.g. GANs). As effective methods were found for computer vision around 2009-2014, we thought that these methods would also work well for NLP. While this was sometimes the case, it has not been true in general.</p>



<p>What are the difficulties of working with natural language? Language works at different scales:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">word &lt; phrase &lt; sentence &lt; document &lt; ...
</pre></div>


<p>Here are examples of structure at each level:</p>



<ol><li>Zipf’s Law: The frequency of any word is inversely proportional to its popularity rank.</li><li>Given the last <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> symbols, it is often possible to predict the next one (The Shannon Game).</li><li>Linguists have found many rules about syntax and semantics of a language.</li><li>In a document, we have lots of discourse between different sentences. For example, “it” or other pronouns are context dependent.</li></ol>



<p>In NLP, we will talk about the <em>syntax</em> and <em>semantics</em> of a document. The syntax refers to how words can fit together, and semantics refers to the meaning of these words.</p>



<h2>Language Modeling</h2>



<p>There are many different NLP tasks such as sentiment analysis, question answering, named entity recognition, and translation. However, recent research shows that these tasks are often related to language modeling.</p>



<p>Language modeling, as explained in Shannon 1948, aims to answer the following question: <em>Think of language as a stochastic process producing symbols. Given the last <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> symbols, can we predict the next one?</em></p>



<p>This question is challenging as is. Consider the following example:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">A reading lamp on the desk shed glow on polished ___
</pre></div>


<p>There are many options: marble/desk/stone/engraving/etc., and it is already difficult to give a probability here. In general, language is hard to model because the next word can be connected to words from a long time ago.</p>



<p>Shannon proposes variants of Markov models to perform this prediction, based on the last couple characters or the context in general.</p>



<p>Since local context matters most, we assume that only the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> most recent words matter. Then, we get the model<br /><img src="https://s0.wp.com/latex.php?latex=p%28w_%7Bt%2B1%7D%7Cw_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%3D%5Cfrac%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%2Cw_%7Bt%2B1%7D%29%7D%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(w_{t+1}|w_{t-n},\ldots,w_t)=\frac{p(w_{t-n},\ldots,w_t,w_{t+1})}{p(w_{t-n},\ldots,w_t)}." class="latex" /></p>



<h3>Measuring Performance</h3>



<p>As <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">we have seen in the generative models lecture</a>, we can use cross entropy as a loss function for density estimation models. Given model density distribution <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> and true distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />, the cross entropy (which equals negative expected log-likelihood) is defined as follows:</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28p%2C+q%29+%3D+-+E_p+%5Clog+q%28w_t+%7C+w_1%2C+%5Cldots+%2C+w_%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p, q) = - E_p \log q(w_t | w_1, \ldots , w_{t-1})" class="latex" /></p>



<p>In NLP we tend to use the metric “perplexity”, which is the exponentiated negative cross entropy:</p>



<p><img src="https://s0.wp.com/latex.php?latex=ppl%28p%2C+q%29+%3D+%5Cexp+-H%28p%2C+q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="ppl(p, q) = \exp -H(p, q)." class="latex" /></p>



<p>This corresponds to the equivalent vocabulary size of a uniformly distributed model. Lower perplexity means our model was closer to the underlying text distribution. As an example, the perplexity of the perfect dice-roll model would be 6.</p>



<p>Why do we care about perplexity anyway?</p>



<ol><li>With a good model we can determine the natural perplexity of a language, which is interesting.</li><li>Many NLP questions are language modeling with conditioning. Speech recognition is language modeling conditioned on some sound signal, and translation is language modeling conditioned on text from another language.</li><li>More importantly, we have found recent applications in <em>transfer learning</em>. That is, a language model can be trained on some (small) input data for a specific task. Then, such a model becomes effective at the given task!</li></ol>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/model_task.png"><img src="https://windowsontheory.files.wordpress.com/2021/04/model_task.png?w=1024" alt="" class="wp-image-8081" /></a></figure>



<p>A few years ago, the best perplexity on WSJ text was 150. Nowadays, it is about 20! To understand how we got here, we look at modern language modeling.</p>



<h2>Predicting the next word</h2>



<p>We start with the model</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28w_t+%7C+w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+softmax%28%5Cmathbf%7BW%7D%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(w_t | w_{1:t-1}; \theta) = softmax(\mathbf{W}\phi(w_{1:t-1}; \theta))." class="latex" /></p>



<p>(The softmax function maps a vector <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bv%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{v}" class="latex" /> into the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bp%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{p}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+%5Cexp%28v_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_i \propto \exp(v_i)" class="latex" />. That is, <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp%28v_i%29%2F%5Csum_j+%5Cexp%28v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_i = \exp(v_i)/\sum_j \exp(v_j)" class="latex" />. Note that this is a Boltzman distribution of the type we saw in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">statistical physics and variational algorithms</a> lecture)</p>



<p>We call <strong>W</strong> the output word embeddings, <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> is some neural basis (e.g. <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> are all but the final layers of a neural net with weights <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\theta" class="latex" />, and <strong>W</strong> is the final layer). However, this means to use softmax to predict requires computing softmax over every word in your vocabulary (tens or hundreds of thousands). This was often infeasible until GPU computing became available.</p>



<p>As an aside, why not predict characters instead of words? The advantage is that there are much fewer characters than words. However, computation with characters is slower. Empirically, character-based models tend to perform worse than word-based. However, character-based models can handle words outside the vocabulary.</p>



<p><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-pair encoding</a> offers a bridge between character and word models. This greedily builds up new tokens as repetitive patterns are found in the original text.</p>



<p>In the last decade NLP has seen a few dominant architectures, all using SGD but with varying bases. First, we must cast the words as one-hot vectors, then embed them into vector space:<br /><img src="https://s0.wp.com/latex.php?latex=x_t+%3D+Vw_t%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t = Vw_t," class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_t" class="latex" /> is the one-hot encoded vector and <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V" class="latex" /> is the learned embedding transformation.</p>



<h3>NNLM</h3>



<p>NNLM (Neural Network Language Model) is like a CNN. The model predicts on possibly multiple NN transformations:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-k-1%7D+%5Coplus+%5Cldots+%5Coplus+x_%7Bt-1%7D%5D%5D%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-k-1} \oplus \ldots \oplus x_{t-1}]])," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Coplus&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\oplus" class="latex" /> denotes concatenation, <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="U" class="latex" /> is some convolutional filter and <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma" class="latex" /> is the activation function. This has the benefit of learning fast. The matrices it learns also transfer well.</p>



<p>As an example, GloVe is a NNLM-inspired model. It stores the words in 300-dimensional space. When we project the some words to 2-dimensions using PCA, we find semantic information in the language model.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/wesRAtM.png" alt="Language structure" /></figure>



<h3>Recurrent Models</h3>



<p>A recurrent model uses a fixed set of previous words to predict the next word. A recurrent network uses all previous words:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-1%7D%5Coplus+%5Cphi%28w_%7B1%3At-2%7D%3B%5Ctheta%29%5D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-1}\oplus \phi(w_{1:t-2};\theta)])." class="latex" /></p>



<p>Previous information is ‘summarized’ in the <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> on the right, so this model uses finite memory. Below is an illustration of the recurrent neural network.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/rnn.png"><img src="https://windowsontheory.files.wordpress.com/2021/04/rnn.png?w=1024" alt="" class="wp-image-8080" /></a></figure>



<p>Since the recurrent model uses the full context, it is a more plausible model for how we really process language. Furthermore, the introduction of RNN saw drastically improved performance. In the graph below, the items in the chart are performances from previous NNLMs. The recurrent network performance is “off the chart”.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/v5rgNeQ.png" alt="RNN Performance" /></figure>



<p>However, the model grows with sequence length. This requires gradient flow to backpropagate over arbitrarily long sequences, and often required baroque network designs to facilitate longer sequences while avoiding exploding/vanishing gradients.</p>



<h3>Attentional Models</h3>



<p>To understand modern NLP we must look at <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a>. For a set of vectors <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />, keys <img src="https://s0.wp.com/latex.php?latex=k_1%2C+%5Cldots%2C+k_T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k_1, \ldots, k_T" class="latex" /> and a query <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />, we define attention as</p>



<p><img src="https://s0.wp.com/latex.php?latex=Att%28q%2C+k%2C+v%29+%3D+%5Csum_t+%5Calpha_t+v_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Att(q, k, v) = \sum_t \alpha_t v_t" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+softmax%28q%5Ccdot+k_1+%5Coplus+%5Cldots+%5Coplus+q+%5Ccdot+k_T%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = softmax(q\cdot k_1 \oplus \ldots \oplus q \cdot k_T)" class="latex" />.</p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" /> can be considered a probability density of the model’s ‘memory’ of the sequence. The model decides which words are important by combining <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />.</p>



<p>The <em>attentional model</em> can be fully autoregressive (use all previously seen words), and the query <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> can be learned or be specific to an input. In general, we have:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5BAtt%28q%2C+x_%7B1%3At-1%7D%2C+x_%7B1%3At-1%7D%29%5D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[Att(q, x_{1:t-1}, x_{1:t-1})])" class="latex" /></p>



<p>Since we condense all previous information into the attention mechanism, it is simpler to backpropagate.<br />In particular attention enables looking at information from a large context without paying for this in the depth of the network and hence in the depth of back-propagation you need to cover. (Boaz’s note: With my crypto background, attention reminds me of the design of <a href="https://en.wikipedia.org/wiki/Block_cipher">block ciphers</a>, which use linear operations to mix between far away parts of the inputs, and then apply non-liearity locally to each small parts.)</p>



<p>Note that attention is defined with respect to a set of vectors. There is no idea of positional information in the attentional model. How do we encode positional information for the model? One way to do this is using <em>sinusoidal encoding</em> in the keys. We store the word <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Ccos%28%5Cpi+n%2Fk%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cos(\pi n/k)" class="latex" /> for some period <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" />. Notice that if we choose many different periods, then the cosine ways will almost never meet at the same point. As a result, only recent points will have high dot products between the different cosine values.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/kAnYjAP.png" alt="Sinusoidal" /></figure>



<h3>Transformers</h3>



<p>A transformer is a stacked attention model. Computation in one layer becomes query, keys and values for the next layer. This is a multiheaded attention model. We learn <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> projections for each query/key (generally between 8 and 32) then do softmax across these projections:<br /><img src="https://s0.wp.com/latex.php?latex=%5Calpha%5Eh+%3D+softmax%28%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_1%29%5Coplus+%5Cldots+%5Coplus+%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_T%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha^h = softmax((W^{(h)}q)\cdot(V^{(h)}k_1)\oplus \ldots \oplus (W^{(h)}q)\cdot(V^{(h)}k_T))." class="latex" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/uUp2elD.png" alt="Transformer Architecture" /></figure>



<p>These heads can be computed in parallel and can be implemented with batch matrix multiplication. As a result, transformers can be massively scaled and are extremely efficient in modern hardware. This has led these models to be very dominant in the field. Here are some example models:</p>



<ol><li>GPT-1,2,3 are able to generate text that is quite convincing to a human. They also handle the syntax and semantics of a language quite well.</li><li>BERT is a transformer-based model that examines text both forwards and backwards in making its predictions. It works well with transfer fining tuning: train on a large data set, then take the feature representations and train a task on top of the learned representation.</li></ol>



<h2>Scaling</h2>



<p>In recent years we have had larger and larger models, from GPT1’s 110 million to GPT3’s 175 billion.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8drDknV.png" alt="Scaling Up" /></figure>



<p>On these massive scales, scaling has become a very interesting issue: how do we process more samples? How do we run distributed computation? How much autoregressive input should each model looks at? (Boaz’s note: To get a sense of how big these models are, GPT-3 was trained on about <a href="https://lambdalabs.com/blog/demystifying-gpt-3/">1000B tokens</a>. The total <a href="https://www.prb.org/howmanypeoplehaveeverlivedonearth/">number of people that ever lived</a> is about 100B, and only about half since the invention of the printing press, so this is arguably a non-negligible fraction of all text produced in human history.)</p>



<p>For a model like BERT, most of the cost still comes from feed-forward network — mostly matrix multiplications. These are tasks we are familiar with and can scale up.</p>



<p>One question is to have long-range attentional lookup, which is important for language modeling. For now, models often look at most 512 words back. Can we do longer range lookups? One approach to this is kernel feature attention.</p>



<h3>Kernel Feature Attention</h3>



<p>Recall that we have <img src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cmathrm%7Bsoftmax%7D%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha=\mathrm{softmax}(q\cdot k_i)" class="latex" />. Can we approximate this with some kernel <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" />? The main approach is that <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5Cpropto+%5Cexp%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha\propto \exp(q\cdot k_i)" class="latex" />, which we approximate with the kernel <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28v_1%5Ccdot+v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(v_1\cdot v_2)" class="latex" />. There is a rich literature on approximating <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" /> where<br /><img src="https://s0.wp.com/latex.php?latex=K%28v_1%2C+v_2%29+%5Capprox+%5Cphi%28v_1%29%5Ccdot%5Cphi%28v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(v_1, v_2) \approx \phi(v_1)\cdot\phi(v_2)" class="latex" /></p>



<p>for some transfomration <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" />. Then, we can try to approximate <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" /> with linear features.</p>



<p>Practically, transformers do well but are slower. For longer texts, we have faster models that do slightly worse. A recent model called <a href="https://arxiv.org/abs/2009.14794">performer</a> is such an example.<br /><img src="https://i.imgur.com/S3vs2o5.png" alt="LRA Performance" /></p>



<h3>Scaling Down</h3>



<p>Ultimately, we want to make models run on “non Google scale” hardware once it has been trained to a specific task. This can often require scaling down.</p>



<p>One approach is to prune weights according to their magnitude. However, since models are often overparameterized and weights do not move much, the weights that get pruned according to this method are usually the weights that were simply initialized closest to 0. In the diagram below, we can consider only leaving the orange weights and cutting out the gray.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/MqY5jK2.png" alt="Pruning" /></figure>



<p>Another approach is to mask out the weights that are unnecessary for a specific task, if you’re trying to ship a model for a specific task.</p>



<h2>Research Questions</h2>



<p>Two major lines of research dominate NLP today:</p>



<ol><li>Attention / Transformer architecture,</li><li>Pretraining with language modeling as a deep learning paradigm.</li></ol>



<p>We are also in a space race to produce efficient models with more parameters, given how much scaling has been effective.</p>



<p>The paper <a href="https://arxiv.org/abs/2010.00854">Which BERT</a> classifies modern questions in NLP into the following categories:</p>



<ol><li><strong>Tasks:</strong> Is (masked) language modeling the best task for pretraining? Language modeling emphasizes local information. We could imagine doing other types of denoising. See also <a href="https://openai.com/blog/dall-e/">DALL-E</a>.</li><li><strong>Efficiency:</strong> We see that much fewer parameters are needed in practice after pruning. Does pruning lose something? Pruned models tend to do well on in-sample data, but out-of-sample data tends to make the pruned model do worse.</li><li><strong>Data:</strong> How does data used in pretraining impact task accuracy? How does the data of pretraining impact task bias?</li><li><strong>Interpretability:</strong> What does BERT know, and how can we determine this? Does interpretability need to come at a cost to performance?</li><li><strong>Multilinguality:</strong> Many languages don’t have the same amount of data as English. What methods apply when we have less data?</li></ol>



<h2>Q&amp;A</h2>



<p>We have many questions asked during and after lecture. Here are some of the questions.</p>



<ol><li><strong>Q:</strong> Should we say GANs fail at NLP or that other generative models are more advanced in NLP than in CV? <strong>A:</strong> One argument is that language is a human generated system, there are some inherent structures that help with generation. We can do language in left-to-right, but in CV this would be a lot more difficult. At the same time, this can change in the future!</li><li><strong>Q:</strong> Why are computer vision and NLP somewhat close to each other? <strong>A:</strong> classically, they are both perception-style tasks under AI. Also, around 2014 we had lots of ideas that come from porting CV ideas into NLP, and recently we have seen NLP ideas ported to CV.</li><li><strong>Q:</strong> Since languages have unique grammars, is NLP better at some languages? Do we have to translate language to an “NLP-effective” language and back? <strong>A:</strong> In the past, some languages are better. Ex: we used to struggle with Japanese to other languages but do well with English to other languages. However, modern models are <em>extremely</em> data driven, so we have needed much less hardcoding.</li><li><strong>Q:</strong> Have we done any scatter plot of the form (data available for language X, performance on X) to see if performance is just a function of available data? <strong>A:</strong> Not right now, but these plots can potentially be really cool! Multilinguality is a broad area of research in general.</li><li><strong>Q:</strong> What are some NLP techniques for low-resource languages? <strong>A:</strong> Bridging is commonly used. Iterative models (translate and translate back with some consistency) is also used to augment the data.</li><li><strong>Q:</strong> Do you think old-school parsers will make a comeback? <strong>A:</strong> Unlikely to deploy parsers, but the techniques of parsing is interesting.</li><li><strong>Q:</strong> Given the large number of possible “correct” answers, has there been work on which “contexts” are most informative? <strong>A:</strong> Best context is the closest context, which is expected. The other words matter but matter a lot less.</li><li><strong>Q:</strong> Is there any model that captures the global structure first (e.g. an outline) before writing sequentially, like humans do when they write longer texts? <strong>A:</strong> Currently no. Should be possible, but we do not have data about the global structure of writing.</li><li><strong>Q:</strong> Why is our goal density estimation? <strong>A:</strong> It is useful because it tells us how “surprising” the next word is. This is also related to why a language feels “fast” when you first learn it: because you are not familiar with the words, you cannot anticipate the next words.</li><li><strong>Q:</strong> Why is lower perplexity better? <strong>A:</strong> Recall from past talk that lower cross-entropy means less distance between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />, and intuitive you have more “certainty”.</li><li><strong>Q:</strong> Is the reason LM so important because evaluations are syntax-focused? <strong>A:</strong> Evaluations are actually more semantically focused, but syntax and semantics are quite connected.</li><li><strong>Q:</strong> Do we see attentional models in CV? <strong>A:</strong> Yes, we have seen more use of transformer in CV. In a standard model we use data only recently, and here we get to work with data across space and time. As such, we will need to encode time positionally.</li><li><strong>Q:</strong> Why is attention generalized convolution? <strong>A:</strong> If you have attention with all mass in the local area, that’s probably like convolution.</li><li><strong>Q:</strong> How do we compare heads with depth? e.g. Is having 5 heads better than 5x depth? <strong>A:</strong> when we use heads we add a lot less parameters. As such, we can parallelize heads and increase performance.</li><li><strong>Q:</strong> Do you think transformers are the be-all end-all of NLP models? <strong>A:</strong> Maybe. To dethrone transformers, you have to both show similar work on small-scale and show that it can be scaled easily.</li><li><strong>Q:</strong> How does simplicity bias affect these transferrable models? <strong>A:</strong> surprising and we are not sure. In CV we found that the models quickly notice peculiarities in the data (e.g. how mechanical turks are grouped), but the models do work.</li><li><strong>Q:</strong> We get bigger models every year and better performance. Will this end soon? <strong>A:</strong> Probably not, as it seems like having more parameters helps it recognize some additional features.</li><li><strong>Q:</strong> If we prune models to the same size, will they have the same performance? <strong>A:</strong> for small models we can seem to prune them, but for the bigger models it is hard to run them in academica given the computational resource constraints.</li><li><strong>Q:</strong> When we try to remember something from a long time ago we would look up a textbook / etc. Have we had similar approaches in practice? <strong>A:</strong> transformer training is static at first, and tasks happen later. So, we have to decide how to throw away information before we train on the tasks.</li><li><strong>Q:</strong> Are better evaluation metrics an important direction for future research? <strong>A:</strong> Yes — this has been the case for the past few years in academia.</li><li><strong>Q:</strong> What is a benchmark/task where you think current models show deep lack of capability? <strong>A:</strong> During generation, models don’t seem to distinguish between information that makes it “sound good” and factually correct information.</li></ol></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/"><span class="datestr">at April 03, 2021 02:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
