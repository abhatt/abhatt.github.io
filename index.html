<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at February 12, 2021 05:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=838">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/02/11/submitting-to-icalp-2021/">Submitting to ICALP 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am taking advantage of the pandemic to participate in conferences where it’s usually hard for me to participate because they require intercontinental travel.  I have a paper in CSR 2021, and now am submitting a paper to ICALP 2021 (I count submission as participation, even in case the paper gets rejected).  The latter requires submissions to be formatted in a specific way, <a href="https://emanueleviola.wordpress.com/tag/utopia-tcs/">a topic discussed at length on this blog</a>.</p>



<p>Begin 12:48</p>



<p>Download the LIPICs package.</p>



<p>Try to compile their sample paper.</p>



<p>Get error message:  ! LaTeX Error: File `l3backend-dvips.def’ not found.</p>



<p>Google solution.  Says to install packages.</p>



<p>Unfortunately, I am using windows but I only have LyX, and the solution expects MixTeX.</p>



<p>Google how to install packages in LyX.</p>



<p>Can’t find anything simple.</p>



<p>Create a new document on overleaf.</p>



<p>Copy all the LIPIcs files there.</p>



<p>Try to compile their sample.</p>



<p>It works!</p>



<p>Paste my latex.</p>



<p>Usual avalanche of problems to be fixed at the speed of light.</p>



<p>Add dummy section “Introduction” which wasn’t in my paper, otherwise theorem numbers look weird.</p>



<p>Numbers still look weird.  Something’s wrong with theorem statements.</p>



<p>Replace {thm} with {theorem}</p>



<p>Looks better.  Still some wrong stuff all around, however.</p>



<p>No it wasn’t that.  Remove the dummy section.  It seems their “paragraph” environment puts strange numbers like 0.0.0.1</p>



<p>Replace \paragaph with \paragaph* everywhere</p>



<p>Actually, looks weird the way they put the ack– put back the dummy Introduction section.</p>



<p>Check page limit: <strong>no more than 12 pages, excluding references</strong></p>



<p>I’m a little over.  Does this really matter?  Apparently, <a href="https://emanueleviola.wordpress.com/2014/09/30/eliminate-all-formatting-requirements-survival-tip/">it does!</a>  Move last proof to the appendix.  Actually, last proof is kind of short, I should move the penultimate proof. Update paper organization (next time I shouldn’t put it).</p>



<p>Final look.  Fix a few indentations.</p>



<p>OK, time to actually submit.  Go to the easychair website.  They want me to re-enter all the information!?  Why, after forcing me to enter title, keywords, etc. in <em>their </em>format, are they asking me to do this again?  Can’t we just send the .tex file and extract it from there?</p>



<p>Oh come one, it’s just a few seconds of copy-paste.</p>



<p>OK, done, paper submitted.</p>



<p>End: 3:05</p>



<p>Well, next time it will be easier.  Perhaps <em>easier</em>, but not <em>easy</em> because as the reader knows there will be another missing package, another incompatible system, etc.  And of course, if the paper is rejected, then I won’t even save the time to convert it into camera-ready format.  On the other hand, the benefit is non-existent.  It would be better for everyone if in order to submit a paper you have to complete a random 1-hour task on Amazon mechanical Turk and donate the profit to charity.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/02/11/submitting-to-icalp-2021/"><span class="datestr">at February 11, 2021 01:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/010">TR21-010 |  Cryptographic Hardness under Projections for Time-Bounded Kolmogorov Complexity | 

	Eric Allender, 

	John Gouwar, 

	Shuichi Hirahara, 

	Caleb Robelle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A version of time-bounded Kolmogorov complexity, denoted KT, has received attention in the past several years, due to its close connection to circuit complexity and to the Minimum Circuit Size Problem MCSP. Essentially all results about the complexity of MCSP hold also for MKTP (the problem of computing the KT complexity of a string). Both MKTP and MCSP are hard for SZK (Statistical Zero Knowledge) under BPP-Turing reductions; neither is known to be NP-complete. Recently, some hardness results for MKTP were proved that are not (yet) known to hold for MCSP. In particular, MKTP is hard for DET (a subclass of P) under nonuniform NC^0 m-reductions.

In this paper, we improve this, to show that MKTP is hard for the (apparently larger) class NISZK_L under not only NC^0 m-reductions but even under projections. Also MKTP is hard for NISZK under P/poly m-reductions. Here, NISZK is the class of problems with non-interactive zero-knowledge proofs, and NISZK_L is the non-interactive version of the class SZK_L that was studied by Dvir et al.

As an application, we provide several improved worst-case to average-case reductions to problems in NP.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/010"><span class="datestr">at February 11, 2021 12:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05632">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05632">Hitting Sets and Reconstruction for Dense Orbits in $\text{VP}_e$ and $\Sigma\Pi\Sigma$ Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medini:Dori.html">Dori Medini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shpilka:Amir.html">Amir Shpilka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05632">PDF</a><br /><b>Abstract: </b>In this paper we study polynomials in $\text{VP}_e$ (polynomial-sized
formulas) and in $\Sigma\Pi\Sigma$ (polynomial-size depth-$3$ circuits) whose
orbits, under the action of the affine group
$\text{GL}_n^{\text{aff}}(\mathbb{F})$, are $\mathit{dense}$ in their ambient
class. We construct hitting sets and interpolating sets for these orbits as
well as give reconstruction algorithms.
</p>
<p>As $\text{VP}=\text{VNC}^2$, our results for $\text{VP}_e$ translate
immediately to $\text{VP}$ with a quasipolynomial blow up in parameters.
</p>
<p>If any of our hitting or interpolating sets could be made $\mathit{robust}$
then this would immediately yield a hitting set for the superclass in which the
relevant class is dense, and as a consequence also a lower bound for the
superclass. Unfortunately, we also prove that the kind of constructions that we
have found (which are defined in terms of $k$-independent polynomial maps) do
not necessarily yield robust hitting sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05632"><span class="datestr">at February 11, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05629">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05629">Agnostic Proper Learning of Halfspaces under Gaussian Marginals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kontonis:Vasilis.html">Vasilis Kontonis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzamos:Christos.html">Christos Tzamos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zarifis:Nikos.html">Nikos Zarifis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05629">PDF</a><br /><b>Abstract: </b>We study the problem of agnostically learning halfspaces under the Gaussian
distribution. Our main result is the {\em first proper} learning algorithm for
this problem whose sample complexity and computational complexity qualitatively
match those of the best known improper agnostic learner. Building on this
result, we also obtain the first proper polynomial-time approximation scheme
(PTAS) for agnostically learning homogeneous halfspaces. Our techniques
naturally extend to agnostically learning linear models with respect to other
non-linear activations, yielding in particular the first proper agnostic
algorithm for ReLU regression.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05629"><span class="datestr">at February 11, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05579">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05579">All instantiations of the greedy algorithm for the shortest superstring problem are equivalent</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolaev:Maksim.html">Maksim Nikolaev</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05579">PDF</a><br /><b>Abstract: </b>In the Shortest Common Superstring problem (SCS), one needs to find the
shortest superstring for a set of strings. While SCS is NP-hard and
MAX-SNP-hard, the Greedy Algorithm "choose two strings with the largest
overlap; merge them; repeat" achieves a constant factor approximation that is
known to be at most 3.5 and conjectured to be equal to 2. The Greedy Algorithm
is not deterministic, so its instantiations with different tie-breaking rules
may have different approximation factors. In this paper, we show that it is not
the case: all factors are equal. To prove this, we show how to transform a set
of strings so that all overlaps are different whereas their ratios stay roughly
the same.
</p>
<p>We also reveal connections between the original version of SCS and the
following one: find a~superstring minimizing the number of occurrences of a
given symbol. It turns out that the latter problem is equivalent to the
original one.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05579"><span class="datestr">at February 11, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05566">Layer VQE: A Variational Approach for Combinatorial Optimization on Noisy Quantum Computers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Xiaoyuan.html">Xiaoyuan Liu</a>, Anthony Angone, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shaydulin:Ruslan.html">Ruslan Shaydulin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Safro:Ilya.html">Ilya Safro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alexeev:Yuri.html">Yuri Alexeev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cincio:Lukasz.html">Lukasz Cincio</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05566">PDF</a><br /><b>Abstract: </b>Combinatorial optimization on near-term quantum devices is a promising path
to demonstrating quantum advantage. However, the capabilities of these devices
are constrained by high noise levels and limited error mitigation. In this
paper, we propose an iterative Layer VQE (L-VQE) approach, inspired by the
Variational Quantum Eigensolver (VQE). We present a large-scale numerical
study, simulating circuits with up to 40 qubits and 352 parameters, that
demonstrates the potential of the proposed approach. We evaluate quantum
optimization heuristics on the problem of detecting multiple communities in
networks, for which we introduce a novel qubit-frugal formulation. We
numerically compare L-VQE with QAOA and demonstrate that QAOA achieves lower
approximation ratios while requiring significantly deeper circuits. We show
that L-VQE is more robust to sampling noise and has a higher chance of finding
the solution as compared with standard VQE approaches. Our simulation results
show that L-VQE performs well under realistic hardware noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05566"><span class="datestr">at February 11, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05548">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05548">Breaking the Quadratic Barrier for Matroid Intersection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blikstad:Joakim.html">Joakim Blikstad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mukhopadhyay:Sagnik.html">Sagnik Mukhopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nanongkai:Danupon.html">Danupon Nanongkai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05548">PDF</a><br /><b>Abstract: </b>The matroid intersection problem is a fundamental problem that has been
extensively studied for half a century. In the classic version of this problem,
we are given two matroids $\mathcal{M}_1 = (V, \mathcal{I}_1)$ and
$\mathcal{M}_2 = (V, \mathcal{I}_2)$ on a comment ground set $V$ of $n$
elements, and then we have to find the largest common independent set $S \in
\mathcal{I}_1 \cap \mathcal{I}_2$ by making independence oracle queries of the
form "Is $S \in \mathcal{I}_1$?" or "Is $S \in \mathcal{I}_2$?" for $S
\subseteq V$. The goal is to minimize the number of queries.
</p>
<p>Beating the existing $\tilde O(n^2)$ bound, known as the quadratic barrier,
is an open problem that captures the limits of techniques from two lines of
work. The first one is the classic Cunningham's algorithm [SICOMP 1986], whose
$\tilde O(n^2)$-query implementations were shown by CLS+ [FOCS 2019] and Nguyen
[2019]. The other one is the general cutting plane method of Lee, Sidford, and
Wong [FOCS 2015]. The only progress towards breaking the quadratic barrier
requires either approximation algorithms or a more powerful rank oracle query
[CLS+ FOCS 2019]. No exact algorithm with $o(n^2)$ independence queries was
known.
</p>
<p>In this work, we break the quadratic barrier with a randomized algorithm
guaranteeing $\tilde O(n^{9/5})$ independence queries with high probability,
and a deterministic algorithm guaranteeing $\tilde O(n^{11/6})$ independence
queries. Our key insight is simple and fast algorithms to solve a graph
reachability problem that arose in the standard augmenting path framework
[Edmonds 1968]. Combining this with previous exact and approximation algorithms
leads to our results.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05548"><span class="datestr">at February 11, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05536">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05536">Slicing the hypercube is not easy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yehuda:Gal.html">Gal Yehuda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yehudayoff:Amir.html">Amir Yehudayoff</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05536">PDF</a><br /><b>Abstract: </b>We prove that at least $\Omega(n^{0.51})$ hyperplanes are needed to slice all
edges of the $n$-dimensional hypercube. We provide a couple of applications:
lower bounds on the computational complexity of parity, and a lower bound on
the cover number of the hypercube by skew hyperplanes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05536"><span class="datestr">at February 11, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05347">Simple and Near-Optimal MAP Inference for Nonsymmetric DPPs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anari:Nima.html">Nima Anari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vuong:Thuy=Duong.html">Thuy-Duong Vuong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05347">PDF</a><br /><b>Abstract: </b>Determinantal point processes (DPPs) are widely popular probabilistic models
used in machine learning to capture diversity in random subsets of items. While
traditional DPPs are defined by a symmetric kernel matrix, recent work has
shown a significant increase in the modeling power and applicability of models
defined by nonsymmetric kernels, where the model can capture interactions that
go beyond diversity. We study the problem of maximum a posteriori (MAP)
inference for determinantal point processes defined by a nonsymmetric positive
semidefinite matrix (NDPPs), where the goal is to find the maximum $k\times k$
principal minor of the kernel matrix $L$. We obtain the first multiplicative
approximation guarantee for this problem using local search, a method that has
been previously applied to symmetric DPPs. Our approximation factor of
$k^{O(k)}$ is nearly tight, and we show theoretically and experimentally that
it compares favorably to the state-of-the-art methods for this problem that are
based on greedy maximization. The main new insight enabling our improved
approximation factor is that we allow local search to update up to two elements
of the solution in each iteration, and we show this is necessary to have any
multiplicative approximation guarantee.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05347"><span class="datestr">at February 11, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05301">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05301">Parallel Minimum Cuts in $O(m \log^2(n))$ Work and Low Depth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anderson:Daniel.html">Daniel Anderson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blelloch:Guy_E=.html">Guy E. Blelloch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05301">PDF</a><br /><b>Abstract: </b>We present an $O(m \log^2(n))$ work, $O(\text{polylog}(n))$ depth parallel
algorithm for minimum cut. This algorithm matches the work of a recent
sequential algorithm by Gawrychowski, Mozes, and Weimann [ICALP'20, (2020),
57:1-57:15], and improves on the previously best known parallel algorithm by
Geissmann and Gianinazzi [SPAA'18, (2018), pp. 1-11] which performs $O(m
\log^4(n))$ work in $O(\text{polylog}(n))$ depth.
</p>
<p>Our algorithm makes use of three components that might be of independent
interest. Firstly, we design a parallel data structure for dynamic trees that
solves mixed batches of queries and weight updates in low depth. It generalizes
and improves the work bounds of a previous data structure of Geissmann and
Gianinazzi and is work efficient with respect to the best sequential algorithm.
Secondly, we design a parallel algorithm for approximate minimum cut that
improves on previous results by Karger and Motwani. We use this algorithm to
give a work-efficient procedure to produce a tree packing, as in Karger's
sequential algorithm for minimum cuts. Lastly, we design a work-efficient
parallel algorithm for solving the minimum $2$-respecting cut problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05301"><span class="datestr">at February 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05209">A Low Degree Learning Algorithm for Quantum Data via Quantum Fourier</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heidari:Mohsen.html">Mohsen Heidari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szpankowski:Wojciech.html">Wojciech Szpankowski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05209">PDF</a><br /><b>Abstract: </b>Advances in quantum information processing compel us to explore learning from
quantum data. We consider a classical-quantum learning problem in which the
samples are quantum states with classical labels and the predictors are quantum
measurements. To study this problem, we introduce a quantum counterpart of PAC
framework. We argue that the major difficulties arising from the quantum nature
of the problem are the compatibility of the measurements and the no-cloning
principle. With that in mind, we establish bounds on the quantum sample
complexity for a family of quantum concept classes called concentrated
measurements. Using a quantum Fourier expansion on qubits, we propose a quantum
low-degree learning algorithm which is a quantum counterpart of (Linial et al.,
1993).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05209"><span class="datestr">at February 11, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05174">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05174">On the Hardness of PAC-learning stabilizer States with Noise</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gollakota:Aravind.html">Aravind Gollakota</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liang:Daniel.html">Daniel Liang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05174">PDF</a><br /><b>Abstract: </b>We consider the problem of learning stabilizer states with noise in the
Probably Approximately Correct (PAC) framework of Aaronson (2007) for learning
quantum states. In the noiseless setting, an algorithm for this problem was
recently given by Rocchetto (2018), but the noisy case was left open. Motivated
by approaches to noise tolerance from classical learning theory, we introduce
the Statistical Query (SQ) model for PAC-learning quantum states, and prove
that algorithms in this model are indeed resilient to common forms of noise,
including classification and depolarizing noise. We prove an exponential lower
bound on learning stabilizer states in the SQ model. Even outside the SQ model,
we prove that learning stabilizer states with noise is in general as hard as
Learning Parity with Noise (LPN) using classical examples. Our results position
the problem of learning stabilizer states as a natural quantum analogue of the
classical problem of learning parities: easy in the noiseless setting, but
seemingly intractable even with simple forms of noise.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05174"><span class="datestr">at February 11, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05168">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05168">Deterministic Tree Embeddings with Copies for Algorithms Against Adaptive Adversaries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haeupler:Bernhard.html">Bernhard Haeupler</a>, D Ellis Hershkowitz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05168">PDF</a><br /><b>Abstract: </b>Embeddings of graphs into distributions of trees that preserve distances in
expectation are a cornerstone of many optimization algorithms. Unfortunately,
online or dynamic algorithms which use these embeddings seem inherently
randomized and ill-suited against adaptive adversaries.
</p>
<p>In this paper we provide a new tree embedding which addresses these issues by
deterministically embedding a graph into a single tree containing $O(\log n)$
copies of each vertex while preserving the connectivity structure of every
subgraph and $O(\log^2 n)$-approximating the cost of every subgraph.
</p>
<p>Using this embedding we obtain several new algorithmic results: We reduce an
open question of Alon et al. [SODA 2004] -- the existence of a deterministic
poly-log-competitive algorithm for online group Steiner tree on a general graph
-- to its tree case. We give a poly-log-competitive deterministic algorithm for
a closely related problem -- online partial group Steiner tree -- which,
roughly, is a bicriteria version of online group Steiner tree. Lastly, we give
the first poly-log approximations for demand-robust Steiner forest, group
Steiner tree and group Steiner forest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05168"><span class="datestr">at February 11, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.05077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.05077">The Multiplicative Version of Azuma's Inequality, with an Application to Contention Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qi:Qi.html">Qi Qi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.05077">PDF</a><br /><b>Abstract: </b>Azuma's inequality is a tool for proving concentration bounds on random
variables. The inequality can be thought of as a natural generalization of
additive Chernoff bounds. On the other hand, the analogous generalization of
multiplicative Chernoff bounds has, to our knowledge, never been explicitly
formulated.
</p>
<p>We formulate a multiplicative-error version of Azuma's inequality. We then
show how to apply this new inequality in order to greatly simplify (and
correct) the analysis of contention delays in multithreaded systems managed by
randomized work stealing.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.05077"><span class="datestr">at February 11, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1318">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2021/02/10/sigact-award-deadlines-for-2021/">SIGACT Award deadlines for 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>From the SIGACT executive committee:</p>



<p>The deadlines to submit nominations for the Gödel Prize and the SIGACT Distinguished Service Award are coming soon. Calls for nominations for both awards can be found at the links below.</p>



<ul><li><a href="https://www.sigact.org/prizes/g%C3%B6del/g%C3%B6del_call21.pdf">Gödel Prize</a>: deadline <strong>February 28</strong>, 2021.</li><li><a href="https://sigact.org/prizes/service.html">SIGACT Distinguished Service Award</a>: deadline <strong>March 8</strong>, 2021.</li></ul></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2021/02/10/sigact-award-deadlines-for-2021/"><span class="datestr">at February 10, 2021 08:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://bit-player.org/?p=2296">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hayes.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://bit-player.org/2021/foldable-words">Foldable Words</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://bit-player.org" title="bit-player">bit-player</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Packing up the household for a recent move, I was delving into shoeboxes, photo albums, and file folders that had not been opened in decades. One of my discoveries, found in an envelope at the back of a file drawer, was the paper sleeve from a drinking straw, imprinted with a saccharine message:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2021/01/Its-a-Pleasure-to-Serve-You-recto.jpg" height="82" width="640" alt="Drinking-straw wrapper inscribed “It’s A Pleasure To Serve You”" border="0" class="centered" /></p>
<p>This flimsy slip of paper seems like an odd scrap to preserve for the ages, but when I pulled it out of the envelope, I knew instantly where it came from and why I had saved it.</p>
<p>The year was 1967. I was 17 then; I’m 71 now. Transposing those two digits takes just a flick of the fingertips. I can blithely skip back and forth from one prime number to the other. But the span of lived time between 1967 and 2021 is a chasm I cannot so easily leap across. At 17 I was in a great hurry to grow up, but I couldn’t see as far as 71; I didn’t even try. Going the other way—revisiting the mental and emotional life of an adolescent boy—is also a journey deep into alien territory. But the straw wrapper helps—it’s a Proustian <em>aide memoire</em>.</p>
<p>In the spring of 1967 I had a girlfriend, Lynn. After school we would meet at the Maple Diner, where the booths had red leatherette upholstery and formica tabletops with a boomerang motif. We’d order two Cokes and a plate of french fries to share. The waitress liked us; she’d make sure we had a full bottle of ketchup. I mention the ketchup because it was a token of our progress toward intimacy. On our first dates Lynn had put only a dainty dab on her fries, but by April we were comfortable enough to reveal our true appetites.</p>
<p>One afternoon I noticed she was fiddling intently with the wrapper from her straw, folding and refolding. I had no idea what she was up to. A teeny paper airplane she would sail over my head? When she finished, she pushed her creation across the table:</p>
<p><img src="http://bit-player.org/wp-content/uploads/2021/01/Its-a-Pleasure-to-Serve-You-folded.jpg" height="" width="250" alt="“It’s a Pleasure to Serve You” folded to read “I love You”" border="0" class="centered" /></p>
<p>What a wallop there was in that little wad of paper. At that point in our romance, the words had not yet been spoken aloud. </p>
<p>How did I respond to Lynn’s folded declaration? I can’t remember; the words are lost. But evidently I got through that awkward moment without doing any permanent damage. A year later Lynn and I were married.</p>
<p>Today, at 71, with the preserved artifact in front of me, my chief regret is that I failed to take up the challenge implicit in the word game Lynn had invented. Why didn’t I craft a reply by folding my own straw wrapper? There are quite a few messages I could have extracted by strategic deletions from “It’s a pleasure to serve you.”</p>
<div style="padding: 1em;">
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsap</span><strong>l</strong><span style="color: #ccc;">easuret</span><strong>o</strong><span style="color: #ccc;">ser</span><strong>veyou</strong>   ==&gt;   I love you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsa</span><strong>pleas</strong><span style="color: #ccc;">ur</span><strong>e</strong><span style="color: #ccc;">toserve</span><strong>you</strong>   ==&gt;   I please you.</pre>
<pre style="padding: 0.5em;">          <strong>it</strong><span style="color: #ccc;">sapl</span><strong>eas</strong><span style="color: #ccc;">ur</span><strong>e</strong><span style="color: #ccc;">toserve</span><strong>you</strong>   ==&gt;   I tease you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsa</span><strong>pleasure</strong><span style="color: #ccc;">toserve</span><strong>you</strong>   ==&gt;   I pleasure you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsa</span><strong>p</strong><span style="color: #ccc;">l</span><strong>e</strong><span style="color: #ccc;">a</span><strong>s</strong><span style="color: #ccc;">ure</span><strong>t</strong><span style="color: #ccc;">os</span><strong>er</strong><span style="color: #ccc;">ve</span><strong>you</strong>   ==&gt;   I pester you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsa</span><strong>p</strong><span style="color: #ccc;">l</span><strong>e</strong><span style="color: #ccc;">asur</span><strong>e</strong><span style="color: #ccc;">toser</span><strong>veyou</strong>   ==&gt;   I peeve you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">t</span><strong>sa</strong><span style="color: #ccc;">p</span><strong>l</strong><span style="color: #ccc;">eas</span><strong>u</strong><span style="color: #ccc;">re</span><strong>t</strong><span style="color: #ccc;">os</span><strong>e</strong><span style="color: #ccc;">rve</span><strong>you</strong>   ==&gt;   I salute you.</pre>
<pre style="padding: 0.5em;">          <strong>i</strong><span style="color: #ccc;">tsap</span><strong>lea</strong><span style="color: #ccc;">suretoser</span><strong>veyou</strong>   ==&gt;   I leave you.</pre>
</div>
<p class="indent">Not all of those statements would have been suited to the occasion of our rendezvous at the Maple Diner, but over the course of our years together—17 years, as it turned out—there came a moment for each of them.</p>
<hr />
<p>How many words can we form by making folds in the straw-paper slogan? I could not have answered that question in 1967. I couldn’t have even asked it. But times change. Enumerating all the foldable messages now strikes me as an obvious thing to do when presented with the straw wrapper. Furthermore, I have the computational means to do it—although the project was not quite as easy as I expected.</p>
<p>A first step is to be explicit about the rules of the game. We are given a source text, in this case “It’s a pleasure to serve you.” Let us ignore the spaces between words as well as all punctuation and capitalization; in this way we arrive at the normalized text “itsapleasuretoserveyou”. A word is <em>foldable</em> if all of its letters appear in the normalized text in the correct order (though not necessarily consecutively). The folding operation amounts to an editing process in which our only permitted act is deletion of letters; we are not allowed to insert, substitute, or permute. If two or more foldable words are to be combined to make a phrase or sentence, they must follow one another in the correct order without overlaps.</p>
<p>So much for foldability. Next comes the fraught question: What is a word? Linguists and lexicographers offer many subtly divergent opinions on this point, but for present purposes a very simple definition will suffice: A finite sequence of characters drawn from the 26-letter English alphabet is a word if it can legally be played in a game of Scrabble. I have been working with a word list from the 2015 edition of <a href="https://en.wikipedia.org/wiki/Collins_Scrabble_Words">Collins Scrabble Words</a>, which has about 270,000 entries. (There are a number of alternative lists, which I discuss in an appendix at the end of this article.)</p>
<p>Scrabble words range in length from 2 to 15 letters. The upper limit—determined by the size of the game board—is not much of a concern. You’re unlikely to meet a straw-paper text that folds to yield words longer than <em>sesquipedalian</em>. The absence of 1-letter words is more troubling, but the remedy is easy: I simply added the words <em>a</em>, <em>I</em>, and <em>O</em> to my copy of the Scrabble list.</p>
<p>My first computational experiments with foldable words searched for examples at random. Writing a program for random sampling is often easier than taking an exact census of a population, and the sample offers a quick glimpse of typical results. The following Python procedure generates random foldable sequences of letters drawn from a given source text, then returns those sequences that are found in the Scrabble word list. (The parameter <em>k</em> is the length of the words to be generated, and <em>reps</em> specifies the number of random trials.)</p>
<pre class="language-python"><code>def randomFoldableWords(text, lexicon, k, reps):
    normtext = normalize(text)
    n = len(normtext)
    findings = []
    for i in range(reps):
        indices = random.sample(range(n), k)
        indices.sort()
        letters = ""
        for idx in indices:
            letters += normtext[idx]
        if letters in lexicon:
            findings.append(letters)
    return findings</code></pre>
<p>Here are the six-letter foldable words found by invoking the program as follows: <code class="language-python">randomFoldableWords(scrabblewords, 6, 10000)</code>. </p>
<blockquote><p>please, plater, searer, saeter, parter, sleety, sleeve, parser, purvey, laster, islets, taster, tester, slarts, paseos, tapers, saeter, eatery, salute, tsetse, setose, salues, sparer</p></blockquote>
<p>Note that the word saeter (you could look it up—I had to) appears twice in this list. The frequency of such repetitions can yield an estimate of the total population size. A variant of the <a href="http://www.pitt.edu/~yuc2/cr/history.htm">mark-and-recapture</a> method, well-known in wildlife ecology, led me to an estimate of 92 six-letter foldable Scrabble words in the straw-wrapper slogan. The actual number turns out to be 106.</p>
<p>Samples and estimates are helpful, but they leave me wondering, What am I missing? What strange and beautiful word has failed to turn up in any of the samples, like the big fish that never takes the bait? I had to have an exhaustive list.</p>
<hr />
<p>In many word games, the tool of choice for computer-aided playing (or cheating) is the regular expression, or regex. A regex is a pattern defining a set of strings, or character sequences; from a collection of strings, a regex search will pick out those that match the pattern. For example, the regular expression <code>^.*love.*$</code> selects from the Scrabble word list all words that have the letter sequence <em>love</em> somewhere within them. There are 137 such words, including some that I would not have thought of, such as <em>rollover</em> and <em>slovenly</em>. The regex <code>^.*l.*o.*v.*e.*$</code> finds all words in which <em>l, o, v,</em> and <em>e</em> appear in sequence, whether of not they are adjacent. The set has 267 members, including such secret-lover gems as <em>bloviate</em>, <em>electropositive</em>, and <em>leftovers</em>.</p>
<p>A solution to the foldable words problem could surely be crafted with regular expressions, but I am not a regex wizard. In search of a more muggles-friendly strategy, my first thought was to extend the idea behind the random-sampling procedure. Instead of selecting foldable sequences at random, I’d generate all of them, and check each one against the word list.</p>
<p>The procedure below generates all three-letter strings that can be folded from the given text, and returns the subset of those strings that appear in the Scrabble word list:</p>
<pre><code class="language-python">def foldableStrings3(lexicon, text):
    normtext = normalize(text)
    n = len(normtext)
    words = []
    for i in range(0, n-2):
        for j in range(i+1, n-1):
            for k in range(j+1, n):
                s = normtext[i] + normtext[j] + normtext[k]
                if s in lexicon:
                    words.append(s)
    return(words)</code></pre>
<p>At the heart of the procedure are three nested loops that methodically step through all the foldable combinations: For any initial letter <code>text[i]</code> we can choose any following letter <code>text[j]</code> with<code> j &gt; i</code>; likewise <code>text[j]</code> can be followed by any <code>text[k]</code> with <code>k &gt; j</code>. This scheme works perfectly well, finding 348 instances of three-letter words. I speak of “instances” because some words appear in the list more than once; for example, <em>pee</em> can be formed in three ways. If we count only unique words, there are 137.</p>
<p>Following this model, we could write a separate routine for each word length from 1 to 15 letters, but that looks like a dreary and repetitious task. Nobody wants to write a procedure with loops nested 15 deep. An alternative is to write a meta-procedure, which would generate the appropriate procedure for each word length. I made a start on that exercise in advanced loopology, but before I got very far I realized there’s an easier way. I was wondering: In a text of <em>n</em> letters, how many foldable substrings exist—whether or not they are recognizable words? There are several ways of answering this question, but to me the most illuminating argument comes from an inclusion/​exclusion principle. Consider the first letter of the text, which in our case is the letter <em>I</em>. In the set of all foldable strings, half include this letter and half exclude it. The same is true of the second letter, and the third, and so on. Thus each letter added to the text doubles the number of foldable strings, which means the total number of strings is simply \(2^n\). (Included in this count is the empty string, made up of no letters.)</p>
<p>This observation suggests a simple algorithm for generating all the foldable strings in any <em>n</em>-letter text. Just count from \(0\) to \(2^{n} - 1\), and for each value along the way line up the binary representation of the number with the letters of the text. Then select those letters that correspond to a <code>1</code> bit, like so:</p>
<div style="padding: 1em;">
<pre style="padding: 0.5em;">                    <span style="color: #ccc;">itsa</span><strong>p</strong><span style="color: #ccc;">leasu</span><strong>re</strong><span style="color: #ccc;">to</span><strong>serve</strong><span style="color: #ccc;">you</span>
                    0000100000110011111000</pre>
</div>
<p>And so we see that the word <code>preserve</code> corresponds to the binary representation of the number <code>134392</code>.</p>
<p>Counting is something that computers are good at, so a word-search procedure based on this principle is straightforward:</p>
<pre><code class="language-python">def foldablesByCounting(lexicon, text):
    normtext = normalize(text)
    n = len(normtext)
    words = []
    for i in range(2**n - 1):
        charSeq = ''
        positions = positionsOf1Bits(i, n)
        for p in positions:
            charSeq += normtext[p]
        if charSeq in lexicon:
            words.append(charSeq)
    return(words)</code></pre>
<p>The outer loop (variable <code>i</code>) counts from \(0\) to \(2^{n} - 1\); for each of these numbers the inner loop (variable <code>p</code>) picks out the letters corresponding to 1 bits. The program produces the output expected. Unfortunately, it does so very slowly. For every character added to the text, running time roughly doubles. I haven’t the patience to plod through the \(2^22\) patterns in “itsapleasuretoserveyou”; estimates based on shorter phrases suggest the running time would be more than three hours.</p>
<hr />
<p>In the middle of the night I realized my approach to this problem was totally backwards. Instead of blindly generating all possible character strings and filtering out the few genuine words, I could march through the list of Scrabble words and test each of them to see if it’s foldable. At worst I would have to try some 270,000 words. I could speed things up even more by making a preliminary pass through the Scrabble list, discarding all words that include characters not present in the normalized text. For the text “It’s a pleasure to serve you,” the character set has just 12 members: <code>aeiloprstuvy</code>. Allowing only words formed from these letters slashes the Scrabble list down to a length of 12,816.</p>
<p>To make this algorithm work, we need a procedure to report whether or not a word can be formed by folding the given text. The simplest approach is to slide the candidate word along the text, looking for a match for each character in turn:</p>
<div style="padding: 0.5em;">
<pre>                    taste
                    <span style="color: #ccc;">itsapleasuretoserveyou</span>

                     <strong>t</strong>aste
                    <span style="color: #ccc;">i</span><strong>t</strong><span style="color: #ccc;">sapleasuretoserveyou</span>

                     <strong>t a</strong>ste
                    <span style="color: #ccc;">i</span><strong>t</strong><span style="color: #ccc;">s</span><strong>a</strong><span style="color: #ccc;">pleasuretoserveyou</span>

                     <strong>t a    s</strong>te
                    <span style="color: #ccc;">i</span><strong>t</strong><span style="color: #ccc;">s</span><strong>a</strong><span style="color: #ccc;">plea</span><strong>s</strong><span style="color: #ccc;">uretoserveyou</span>

                     <strong>t a    s   t</strong>e
                    <span style="color: #ccc;">i</span><strong>t</strong><span style="color: #ccc;">s</span><strong>a</strong><span style="color: #ccc;">plea</span><strong>s</strong><span style="color: #ccc;">ure</span><strong>t</strong><span style="color: #ccc;">oserveyou</span>

                     <strong>t a    s   t  e</strong>
                    <span style="color: #ccc;">i</span><strong>t</strong><span style="color: #ccc;">s</span><strong>a</strong><span style="color: #ccc;">plea</span><strong>s</strong><span style="color: #ccc;">ure</span><strong>t</strong><span style="color: #ccc;">os</span><strong>e</strong><span style="color: #ccc;">rveyou</span>
</pre>
</div>
<p>If every letter of the word finds a mate in the text, the word is foldable, as in the case of <code>taste</code>, shown above. But an attempt to match <code>tastes</code> would fall off the end of the text looking for a second <code>s</code>, which does not exist.</p>
<p>The following code implements this idea:</p>
<pre><code class="language-python">def wordIsFoldable(word, text):
    normtext = normalize(text)
    t = 0                      # pointer to positions in normtext
    w = 0                      # pointer to positions in word
    while t &lt; len(normtext):
        if word[w] == normtext[t]:  # matching chars in word and text
            w += 1                  # move to next char in word
        if w == len(word):          # matched all chars in word
            return(True)            # so: thumbs up
        t += 1                 # move to next char in text
    return(False)              # fell off the end: thumbs down</code></pre>
<p>All we need to do now is embed this procedure in a loop that steps through all the candidate Scrabble words, collecting those for which <code>wordIsFoldable</code> returns <code>True</code>. </p>
<p>There’s still some waste motion here, since we are searching letter-by-letter through the same text, and repeating the same searches thousands of times. The source code (available on GitHub as a <a href="https://github.com/bit-player/foldable-words">Jupyter notebook</a>) explains some further speedups. But even the simple version shown here runs in less than two tenths of a second, so there’s not much point in optimizing.</p>
<p>I can now report that there are 778 unique foldable Scrabble words in “It’s a pleasure to serve you” (including the three one-letter words I added to the list). Words that can be formed in multiple ways bring the total count to 899.</p>
<p>And so we come to the tah-dah! moment—the unveiling of the complete list. I have organized the words into groups based on each word’s starting position within the text. (By Python convention, the positions are numbered from 0 through \(n-1\).) Within each group, the words are sorted according to the position of their last character; that position is given in the subscript following the word. For example, <em>tapestry</em> is in Group 1 because it begins at position 1 in the text (the <em>t</em> in <em>It’s</em>), and it carries the subscript 19 because it ends at position 19 (the <em>y</em> in <em>you</em>). </p>
<p>This arrangement of the words is meant to aid in contructing multiword phrases. If a word ends at position \(m\), the next word in the phrase must come from a group numbered \(m+1\) or greater. </p>
<p><br /></p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 0:</b> i<sub>0</sub> it<sub>1</sub> is<sub>2</sub> its<sub>2</sub> ita<sub>3</sub> isle<sub>6</sub> ilea<sub>7</sub> isles<sub>8</sub> itas<sub>8</sub> ire<sub>11</sub> issue<sub>11</sub> iure<sub>11</sub> islet<sub>12</sub> io<sub>13</sub> iso<sub>13</sub> ileus<sub>14</sub> ios<sub>14</sub> ires<sub>14</sub> islets<sub>14</sub> isos<sub>14</sub> issues<sub>14</sub> issuer<sub>16</sub> ivy<sub>19</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 1:</b> ta<sub>3</sub> tap<sub>4</sub> tae<sub>6</sub> tale<sub>6</sub> tape<sub>6</sub> te<sub>6</sub> tala<sub>7</sub> talea<sub>7</sub> tapa<sub>7</sub> tea<sub>7</sub> taes<sub>8</sub> talas<sub>8</sub> tales<sub>8</sub> tapas<sub>8</sub> tapes<sub>8</sub> taps<sub>8</sub> tas<sub>8</sub> teas<sub>8</sub> tes<sub>8</sub> tapu<sub>9</sub> tau<sub>9</sub> talar<sub>10</sub> taler<sub>10</sub> taper<sub>10</sub> tar<sub>10</sub> tear<sub>10</sub> tsar<sub>10</sub> taleae<sub>11</sub> tare<sub>11</sub> tease<sub>11</sub> tee<sub>11</sub> tapet<sub>12</sub> tart<sub>12</sub> tat<sub>12</sub> taut<sub>12</sub> teat<sub>12</sub> test<sub>12</sub> tet<sub>12</sub> tret<sub>12</sub> tut<sub>12</sub> tao<sub>13</sub> taro<sub>13</sub> to<sub>13</sub> talars<sub>14</sub> talers<sub>14</sub> talus<sub>14</sub> taos<sub>14</sub> tapers<sub>14</sub> tapets<sub>14</sub> tapus<sub>14</sub> tares<sub>14</sub> taros<sub>14</sub> tars<sub>14</sub> tarts<sub>14</sub> tass<sub>14</sub> tats<sub>14</sub> taus<sub>14</sub> tauts<sub>14</sub> tears<sub>14</sub> teases<sub>14</sub> teats<sub>14</sub> tees<sub>14</sub> teres<sub>14</sub> terts<sub>14</sub> tests<sub>14</sub> tets<sub>14</sub> tres<sub>14</sub> trets<sub>14</sub> tsars<sub>14</sub> tuts<sub>14</sub> tasse<sub>15</sub> taste<sub>15</sub> tate<sub>15</sub> terete<sub>15</sub> terse<sub>15</sub> teste<sub>15</sub> tete<sub>15</sub> toe<sub>15</sub> tose<sub>15</sub> tree<sub>15</sub> tsetse<sub>15</sub> taperer<sub>16</sub> tapster<sub>16</sub> tarter<sub>16</sub> taser<sub>16</sub> taster<sub>16</sub> tater<sub>16</sub> tauter<sub>16</sub> tearer<sub>16</sub> teaser<sub>16</sub> teer<sub>16</sub> teeter<sub>16</sub> terser<sub>16</sub> tester<sub>16</sub> tor<sub>16</sub> tutor<sub>16</sub> tav<sub>17</sub> tarre<sub>18</sub> testee<sub>18</sub> tore<sub>18</sub> trove<sub>18</sub> tutee<sub>18</sub> tapestry<sub>19</sub> tapstry<sub>19</sub> tarry<sub>19</sub> tarty<sub>19</sub> tasty<sub>19</sub> tay<sub>19</sub> teary<sub>19</sub> terry<sub>19</sub> testy<sub>19</sub> toey<sub>19</sub> tory<sub>19</sub> toy<sub>19</sub> trey<sub>19</sub> troy<sub>19</sub> try<sub>19</sub> too<sub>20</sub> toro<sub>20</sub> toyo<sub>20</sub> tatou<sub>21</sub> tatu<sub>21</sub> tutu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 2:</b> sap<sub>4</sub> sal<sub>5</sub> sae<sub>6</sub> sale<sub>6</sub> sea<sub>7</sub> spa<sub>7</sub> sales<sub>8</sub> sals<sub>8</sub> saps<sub>8</sub> seas<sub>8</sub> spas<sub>8</sub> sau<sub>9</sub> sar<sub>10</sub> sear<sub>10</sub> ser<sub>10</sub> slur<sub>10</sub> spar<sub>10</sub> spear<sub>10</sub> spur<sub>10</sub> sur<sub>10</sub> salse<sub>11</sub> salue<sub>11</sub> seare<sub>11</sub> sease<sub>11</sub> seasure<sub>11</sub> see<sub>11</sub> sere<sub>11</sub> sese<sub>11</sub> slae<sub>11</sub> slee<sub>11</sub> slue<sub>11</sub> spae<sub>11</sub> spare<sub>11</sub> spue<sub>11</sub> sue<sub>11</sub> sure<sub>11</sub> salet<sub>12</sub> salt<sub>12</sub> sat<sub>12</sub> saut<sub>12</sub> seat<sub>12</sub> set<sub>12</sub> slart<sub>12</sub> slat<sub>12</sub> sleet<sub>12</sub> slut<sub>12</sub> spart<sub>12</sub> spat<sub>12</sub> speat<sub>12</sub> spet<sub>12</sub> splat<sub>12</sub> spurt<sub>12</sub> st<sub>12</sub> suet<sub>12</sub> salto<sub>13</sub> so<sub>13</sub> salets<sub>14</sub> salses<sub>14</sub> saltos<sub>14</sub> salts<sub>14</sub> salues<sub>14</sub> sapless<sub>14</sub> saros<sub>14</sub> sars<sub>14</sub> sass<sub>14</sub> sauts<sub>14</sub> sears<sub>14</sub> seases<sub>14</sub> seasures<sub>14</sub> seats<sub>14</sub> sees<sub>14</sub> seres<sub>14</sub> sers<sub>14</sub> sess<sub>14</sub> sets<sub>14</sub> slaes<sub>14</sub> slarts<sub>14</sub> slats<sub>14</sub> sleets<sub>14</sub> slues<sub>14</sub> slurs<sub>14</sub> sluts<sub>14</sub> sos<sub>14</sub> spaes<sub>14</sub> spares<sub>14</sub> spars<sub>14</sub> sparts<sub>14</sub> spats<sub>14</sub> spears<sub>14</sub> speats<sub>14</sub> speos<sub>14</sub> spets<sub>14</sub> splats<sub>14</sub> spues<sub>14</sub> spurs<sub>14</sub> spurts<sub>14</sub> sues<sub>14</sub> suets<sub>14</sub> sures<sub>14</sub> sus<sub>14</sub> salute<sub>15</sub> saree<sub>15</sub> sasse<sub>15</sub> sate<sub>15</sub> saute<sub>15</sub> setose<sub>15</sub> slate<sub>15</sub> sloe<sub>15</sub> sluse<sub>15</sub> sparse<sub>15</sub> spate<sub>15</sub> sperse<sub>15</sub> spree<sub>15</sub> saeter<sub>16</sub> salter<sub>16</sub> saluter<sub>16</sub> sapor<sub>16</sub> sartor<sub>16</sub> saser<sub>16</sub> searer<sub>16</sub> seater<sub>16</sub> seer<sub>16</sub> serer<sub>16</sub> serr<sub>16</sub> slater<sub>16</sub> sleer<sub>16</sub> spaer<sub>16</sub> sparer<sub>16</sub> sparser<sub>16</sub> spearer<sub>16</sub> speer<sub>16</sub> spuer<sub>16</sub> spurter<sub>16</sub> suer<sub>16</sub> surer<sub>16</sub> sutor<sub>16</sub> sav<sub>17</sub> sov<sub>17</sub> salve<sub>18</sub> save<sub>18</sub> serre<sub>18</sub> serve<sub>18</sub> slave<sub>18</sub> sleave<sub>18</sub> sleeve<sub>18</sub> slove<sub>18</sub> sore<sub>18</sub> sparre<sub>18</sub> sperre<sub>18</sub> splore<sub>18</sub> spore<sub>18</sub> stere<sub>18</sub> sterve<sub>18</sub> store<sub>18</sub> stove<sub>18</sub> salary<sub>19</sub> salty<sub>19</sub> sassy<sub>19</sub> saury<sub>19</sub> savey<sub>19</sub> say<sub>19</sub> serry<sub>19</sub> sesey<sub>19</sub> sey<sub>19</sub> slatey<sub>19</sub> slaty<sub>19</sub> slavey<sub>19</sub> slay<sub>19</sub> sleety<sub>19</sub> sley<sub>19</sub> slurry<sub>19</sub> sly<sub>19</sub> soy<sub>19</sub> sparry<sub>19</sub> spay<sub>19</sub> speary<sub>19</sub> splay<sub>19</sub> spry<sub>19</sub> spurrey<sub>19</sub> spurry<sub>19</sub> spy<sub>19</sub> stey<sub>19</sub> storey<sub>19</sub> story<sub>19</sub> sty<sub>19</sub> suety<sub>19</sub> surety<sub>19</sub> surrey<sub>19</sub> survey<sub>19</sub> salvo<sub>20</sub> servo<sub>20</sub> stereo<sub>20</sub> sou<sub>21</sub> susu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 3:</b> a<sub>3</sub> al<sub>5</sub> ae<sub>6</sub> ale<sub>6</sub> ape<sub>6</sub> aa<sub>7</sub> ala<sub>7</sub> aas<sub>8</sub> alas<sub>8</sub> ales<sub>8</sub> als<sub>8</sub> apes<sub>8</sub> as<sub>8</sub> alu<sub>9</sub> alar<sub>10</sub> aper<sub>10</sub> ar<sub>10</sub> alae<sub>11</sub> alee<sub>11</sub> alure<sub>11</sub> apse<sub>11</sub> are<sub>11</sub> aue<sub>11</sub> alert<sub>12</sub> alt<sub>12</sub> apart<sub>12</sub> apert<sub>12</sub> apt<sub>12</sub> aret<sub>12</sub> art<sub>12</sub> at<sub>12</sub> aero<sub>13</sub> also<sub>13</sub> alto<sub>13</sub> apo<sub>13</sub> apso<sub>13</sub> auto<sub>13</sub> aeros<sub>14</sub> alerts<sub>14</sub> altos<sub>14</sub> alts<sub>14</sub> alures<sub>14</sub> alus<sub>14</sub> apers<sub>14</sub> apos<sub>14</sub> apres<sub>14</sub> apses<sub>14</sub> apsos<sub>14</sub> apts<sub>14</sub> ares<sub>14</sub> arets<sub>14</sub> ars<sub>14</sub> arts<sub>14</sub> ass<sub>14</sub> ats<sub>14</sub> aures<sub>14</sub> autos<sub>14</sub> alate<sub>15</sub> aloe<sub>15</sub> arete<sub>15</sub> arose<sub>15</sub> arse<sub>15</sub> ate<sub>15</sub> alastor<sub>16</sub> alerter<sub>16</sub> alter<sub>16</sub> apter<sub>16</sub> aster<sub>16</sub> arere<sub>18</sub> ave<sub>18</sub> aery<sub>19</sub> alary<sub>19</sub> alay<sub>19</sub> aleatory<sub>19</sub> apay<sub>19</sub> apery<sub>19</sub> arsey<sub>19</sub> arsy<sub>19</sub> artery<sub>19</sub> artsy<sub>19</sub> arty<sub>19</sub> ary<sub>19</sub> ay<sub>19</sub> aloo<sub>20</sub> arvo<sub>20</sub> avo<sub>20</sub> ayu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 4:</b> pe<sub>6</sub> pa<sub>7</sub> pea<sub>7</sub> plea<sub>7</sub> pas<sub>8</sub> peas<sub>8</sub> pes<sub>8</sub> pleas<sub>8</sub> plu<sub>9</sub> par<sub>10</sub> pear<sub>10</sub> per<sub>10</sub> pur<sub>10</sub> pare<sub>11</sub> pase<sub>11</sub> peare<sub>11</sub> pease<sub>11</sub> pee<sub>11</sub> pere<sub>11</sub> please<sub>11</sub> pleasure<sub>11</sub> plue<sub>11</sub> pre<sub>11</sub> pure<sub>11</sub> part<sub>12</sub> past<sub>12</sub> pat<sub>12</sub> peart<sub>12</sub> peat<sub>12</sub> pert<sub>12</sub> pest<sub>12</sub> pet<sub>12</sub> plast<sub>12</sub> plat<sub>12</sub> pleat<sub>12</sub> pst<sub>12</sub> put<sub>12</sub> pareo<sub>13</sub> paseo<sub>13</sub> peso<sub>13</sub> pesto<sub>13</sub> po<sub>13</sub> pro<sub>13</sub> pareos<sub>14</sub> pares<sub>14</sub> pars<sub>14</sub> parts<sub>14</sub> paseos<sub>14</sub> pases<sub>14</sub> pass<sub>14</sub> pasts<sub>14</sub> pats<sub>14</sub> peares<sub>14</sub> pears<sub>14</sub> peases<sub>14</sub> peats<sub>14</sub> pees<sub>14</sub> peres<sub>14</sub> perts<sub>14</sub> pesos<sub>14</sub> pestos<sub>14</sub> pests<sub>14</sub> pets<sub>14</sub> plats<sub>14</sub> pleases<sub>14</sub> pleasures<sub>14</sub> pleats<sub>14</sub> plues<sub>14</sub> plus<sub>14</sub> pos<sub>14</sub> pros<sub>14</sub> pures<sub>14</sub> purs<sub>14</sub> pus<sub>14</sub> puts<sub>14</sub> parse<sub>15</sub> passe<sub>15</sub> paste<sub>15</sub> pate<sub>15</sub> pause<sub>15</sub> perse<sub>15</sub> plaste<sub>15</sub> plate<sub>15</sub> pose<sub>15</sub> pree<sub>15</sub> prese<sub>15</sub> prose<sub>15</sub> puree<sub>15</sub> purse<sub>15</sub> parer<sub>16</sub> parr<sub>16</sub> parser<sub>16</sub> parter<sub>16</sub> passer<sub>16</sub> paster<sub>16</sub> pastor<sub>16</sub> pater<sub>16</sub> pauser<sub>16</sub> pearter<sub>16</sub> peer<sub>16</sub> perter<sub>16</sub> pester<sub>16</sub> peter<sub>16</sub> plaster<sub>16</sub> plater<sub>16</sub> pleaser<sub>16</sub> pleasurer<sub>16</sub> pleater<sub>16</sub> poser<sub>16</sub> pretor<sub>16</sub> proser<sub>16</sub> puer<sub>16</sub> purer<sub>16</sub> purr<sub>16</sub> purser<sub>16</sub> parev<sub>17</sub> pav<sub>17</sub> perv<sub>17</sub> pareve<sub>18</sub> parore<sub>18</sub> parve<sub>18</sub> passee<sub>18</sub> pave<sub>18</sub> peeve<sub>18</sub> perve<sub>18</sub> petre<sub>18</sub> pore<sub>18</sub> preeve<sub>18</sub> preserve<sub>18</sub> preve<sub>18</sub> prore<sub>18</sub> prove<sub>18</sub> parry<sub>19</sub> party<sub>19</sub> pastry<sub>19</sub> pasty<sub>19</sub> patsy<sub>19</sub> paty<sub>19</sub> pay<sub>19</sub> peatery<sub>19</sub> peaty<sub>19</sub> peavey<sub>19</sub> peavy<sub>19</sub> peeoy<sub>19</sub> peery<sub>19</sub> perry<sub>19</sub> pervy<sub>19</sub> pesty<sub>19</sub> plastery<sub>19</sub> platy<sub>19</sub> play<sub>19</sub> ploy<sub>19</sub> plurry<sub>19</sub> ply<sub>19</sub> pory<sub>19</sub> posey<sub>19</sub> posy<sub>19</sub> prey<sub>19</sub> prosy<sub>19</sub> pry<sub>19</sub> pursy<sub>19</sub> purty<sub>19</sub> purvey<sub>19</sub> puy<sub>19</sub> parvo<sub>20</sub> poo<sub>20</sub> proo<sub>20</sub> proso<sub>20</sub> pareu<sub>21</sub> patu<sub>21</sub> poyou<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 5:</b> la<sub>7</sub> lea<sub>7</sub> las<sub>8</sub> leas<sub>8</sub> les<sub>8</sub> leu<sub>9</sub> lar<sub>10</sub> lear<sub>10</sub> lur<sub>10</sub> lare<sub>11</sub> lase<sub>11</sub> leare<sub>11</sub> lease<sub>11</sub> leasure<sub>11</sub> lee<sub>11</sub> lere<sub>11</sub> lure<sub>11</sub> last<sub>12</sub> lat<sub>12</sub> least<sub>12</sub> leat<sub>12</sub> leet<sub>12</sub> lest<sub>12</sub> let<sub>12</sub> lo<sub>13</sub> lares<sub>14</sub> lars<sub>14</sub> lases<sub>14</sub> lass<sub>14</sub> lasts<sub>14</sub> lats<sub>14</sub> leares<sub>14</sub> lears<sub>14</sub> leases<sub>14</sub> leasts<sub>14</sub> leasures<sub>14</sub> leats<sub>14</sub> lees<sub>14</sub> leets<sub>14</sub> leres<sub>14</sub> leses<sub>14</sub> less<sub>14</sub> lests<sub>14</sub> lets<sub>14</sub> los<sub>14</sub> lues<sub>14</sub> lures<sub>14</sub> lurs<sub>14</sub> laree<sub>15</sub> late<sub>15</sub> leese<sub>15</sub> lose<sub>15</sub> lute<sub>15</sub> laer<sub>16</sub> laser<sub>16</sub> laster<sub>16</sub> later<sub>16</sub> leaser<sub>16</sub> leer<sub>16</sub> lesser<sub>16</sub> lor<sub>16</sub> loser<sub>16</sub> lurer<sub>16</sub> luser<sub>16</sub> luter<sub>16</sub> lav<sub>17</sub> lev<sub>17</sub> luv<sub>17</sub> lave<sub>18</sub> leave<sub>18</sub> lessee<sub>18</sub> leve<sub>18</sub> lore<sub>18</sub> love<sub>18</sub> lurve<sub>18</sub> lay<sub>19</sub> leary<sub>19</sub> leavy<sub>19</sub> leery<sub>19</sub> levy<sub>19</sub> ley<sub>19</sub> lory<sub>19</sub> lovey<sub>19</sub> loy<sub>19</sub> lurry<sub>19</sub> laevo<sub>20</sub> lasso<sub>20</sub> levo<sub>20</sub> loo<sub>20</sub> lassu<sub>21</sub> latu<sub>21</sub> lou<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 6:</b> ea<sub>7</sub> eas<sub>8</sub> es<sub>8</sub> eau<sub>9</sub> ear<sub>10</sub> er<sub>10</sub> ease<sub>11</sub> ee<sub>11</sub> ere<sub>11</sub> east<sub>12</sub> eat<sub>12</sub> est<sub>12</sub> et<sub>12</sub> euro<sub>13</sub> ears<sub>14</sub> eases<sub>14</sub> easts<sub>14</sub> eats<sub>14</sub> eaus<sub>14</sub> eres<sub>14</sub> eros<sub>14</sub> ers<sub>14</sub> eses<sub>14</sub> ess<sub>14</sub> ests<sub>14</sub> euros<sub>14</sub> erose<sub>15</sub> esse<sub>15</sub> easer<sub>16</sub> easter<sub>16</sub> eater<sub>16</sub> err<sub>16</sub> ester<sub>16</sub> erev<sub>17</sub> eave<sub>18</sub> eve<sub>18</sub> easy<sub>19</sub> eatery<sub>19</sub> eery<sub>19</sub> estro<sub>20</sub> evo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 7:</b> a<sub>7</sub> as<sub>8</sub> ar<sub>10</sub> ae<sub>11</sub> are<sub>11</sub> aue<sub>11</sub> aret<sub>12</sub> art<sub>12</sub> at<sub>12</sub> auto<sub>13</sub> ares<sub>14</sub> arets<sub>14</sub> ars<sub>14</sub> arts<sub>14</sub> ass<sub>14</sub> ats<sub>14</sub> aures<sub>14</sub> autos<sub>14</sub> arete<sub>15</sub> arose<sub>15</sub> arse<sub>15</sub> ate<sub>15</sub> aster<sub>16</sub> arere<sub>18</sub> ave<sub>18</sub> aery<sub>19</sub> arsey<sub>19</sub> arsy<sub>19</sub> artery<sub>19</sub> artsy<sub>19</sub> arty<sub>19</sub> ary<sub>19</sub> ay<sub>19</sub> aero<sub>20</sub> arvo<sub>20</sub> avo<sub>20</sub> ayu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 8:</b> sur<sub>10</sub> sue<sub>11</sub> sure<sub>11</sub> set<sub>12</sub> st<sub>12</sub> suet<sub>12</sub> so<sub>13</sub> sets<sub>14</sub> sos<sub>14</sub> sues<sub>14</sub> suets<sub>14</sub> sures<sub>14</sub> sus<sub>14</sub> see<sub>15</sub> sese<sub>15</sub> setose<sub>15</sub> seer<sub>16</sub> ser<sub>16</sub> suer<sub>16</sub> surer<sub>16</sub> sutor<sub>16</sub> sov<sub>17</sub> sere<sub>18</sub> serve<sub>18</sub> sore<sub>18</sub> stere<sub>18</sub> sterve<sub>18</sub> store<sub>18</sub> stove<sub>18</sub> sesey<sub>19</sub> sey<sub>19</sub> soy<sub>19</sub> stey<sub>19</sub> storey<sub>19</sub> story<sub>19</sub> sty<sub>19</sub> suety<sub>19</sub> surety<sub>19</sub> surrey<sub>19</sub> survey<sub>19</sub> servo<sub>20</sub> stereo<sub>20</sub> sou<sub>21</sub> susu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 9:</b> ur<sub>10</sub> ure<sub>11</sub> ut<sub>12</sub> ures<sub>14</sub> us<sub>14</sub> uts<sub>14</sub> use<sub>15</sub> ute<sub>15</sub> ureter<sub>16</sub> user<sub>16</sub> uey<sub>19</sub> utu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 10:</b> re<sub>11</sub> ret<sub>12</sub> reo<sub>13</sub> reos<sub>14</sub> res<sub>14</sub> rets<sub>14</sub> ree<sub>15</sub> rete<sub>15</sub> roe<sub>15</sub> rose<sub>15</sub> rev<sub>17</sub> reeve<sub>18</sub> resee<sub>18</sub> reserve<sub>18</sub> retore<sub>18</sub> rore<sub>18</sub> rove<sub>18</sub> retry<sub>19</sub> rory<sub>19</sub> rosery<sub>19</sub> rosy<sub>19</sub> retro<sub>20</sub> roo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 11:</b> et<sub>12</sub> es<sub>14</sub> ee<sub>15</sub> er<sub>16</sub> ere<sub>18</sub> eve<sub>18</sub> eery<sub>19</sub> evo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 12:</b> to<sub>13</sub> te<sub>15</sub> toe<sub>15</sub> tose<sub>15</sub> tor<sub>16</sub> tee<sub>18</sub> tore<sub>18</sub> toey<sub>19</sub> tory<sub>19</sub> toy<sub>19</sub> trey<sub>19</sub> try<sub>19</sub> too<sub>20</sub> toro<sub>20</sub> toyo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 13:</b> o<sub>13</sub> os<sub>14</sub> oe<sub>15</sub> ose<sub>15</sub> or<sub>16</sub> ore<sub>18</sub> oy<sub>19</sub> oo<sub>20</sub> ou<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 14:</b> ser<sub>16</sub> see<sub>18</sub> sere<sub>18</sub> serve<sub>18</sub> sey<sub>19</sub> servo<sub>20</sub> so<sub>20</sub> sou<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 15:</b> er<sub>16</sub> ee<sub>18</sub> ere<sub>18</sub> eve<sub>18</sub> evo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 16:</b> re<sub>18</sub> reo<sub>20</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 17:</b> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 18:</b> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 19:</b> yo<sub>20</sub> you<sub>21</sub> yu<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 20:</b> o<sub>20</sub> ou<sub>21</sub> </p>
<p style="margin-bottom: 6pt;" class="undent"><b>Group 21:</b> </p>
<hr />
<p>Naturally, I’ve tried out the code on a few other well-known phrases. </p>
<p>If Lynn and I had met at a different dining establishment, she might have found a straw with the statement, “It takes two hands to handle a Whopper.” There’s quite a diverse assortment of possible messages lurking in this text, with 1,154 unique foldable words and almost 2,000 word instances. Perhaps she would have chosen the upbeat “Inhale hope.” Or, in a darker mood, “I taste woe.”</p>
<p>If we had been folding dollar bills instead of straw wrappers, “In God We Trust” might have become the forward-looking proclamation, “I go west!” Horace Greeley’s marching order on the same theme, “Go west, young man,” gives us the enigmatic “O, wet yoga!” or, perhaps more aptly, “Gunman.”</p>
<p>Jumping forward from 1967 to 2021—from the Summer of Love to the Winter of COVID—I can turn “Wear a mask. Wash your hands.” into the plaintive, “We ask: Why us?” With “Maintain social distance,” the best I can do is “A nasal dance” or “A sad stance.”</p>
<p>And then there’s “Make America Great Again.” It yields “Meme rage.” Also “Make me ragtag.”</p>
<hr />
<h4>Appendix: The Word-List Problem.</h4>
<p>In a project like this one, you might think that getting a suitable list of English words would be the easy part. In fact it seems to be the main trouble spot.</p>
<p>The Scrabble lexicon I’ve been relying on derives from a word list known as SOWPODS, compiled by two associations of Scrabble players starting in the 1980s. Current editions of the list are distributed  by a commercial publisher, Collins Dictionaries. If I understand correctly, all versions of the list are subject to copyright (see <a href="https://boardgames.stackexchange.com/questions/38366/latest-collins-scrabble-words-list-in-text-file">discussion on Stack Exchange</a>) and cannot legally be distributed without permission. But no one seems to be much bothered by that fact. Copies of the lists in plain-text format, with one word per line, are easy to find on the internet—and not just on dodgy sites that specialize in pirated material.</p>
<p>There are alternative lists without legal encumbrances. Indeed, there’s a good chance you already have one such list pre-installed on your computer. A file called <code>words</code> is included in most distributions of the Unix operating system, including MacOS; my copy of the file lives in <code>usr/share/dict/words</code>. If you don’t have or can’t find the Unix <code>words</code> file, I suggest downloading the <a href="http://www.nltk.org/nltk_data/">Natural Language Toolkit</a>, a suite of data files and Python programs that includes a lexicon almost identical to Unix words, as well as many other linguistic resources.</p>
<p>The Scrabble list has one big advantage over <code>words</code>: It includes plurals and inflected forms of verbs—not just <em>test</em> but also <em>tests</em>, <em>tested</em>, and <em>testing</em>. [Bad example; see comments below.] The <code>words</code> file is more like a list of dictionary head words, with only the stem form explicitly included. On the other hand, <code>words</code> has an abundance of names and other proper nouns, as well as abbreviations, which are excluded from the Scrabble list since they are not legal plays in the board game.</p>
<p>How about combining the two word lists? Their union has just under 400,000 entries—quite a large lexicon. Using this augmented list for the analysis of “It’s a pleasure to serve you,” my program finds an additional 219 foldable words, beyond the 778 found with the Scrabble list alone. Here they are:</p>
<blockquote><p>aaru aer aerose aes alares alaster alea alerse aleut alo alose alur aly ao apa apar aperu apus aro arry aru ase asor asse ast astor atry aueto aurore aus ausu aute e eastre eer erse esere estre eu ey iao ie ila islay ist isuret itala itea iter ito iyo l laet lao larry larve lastre lasty latro laur leo ler lester lete leto loro lu lue luo lut luteo lutose ly oer ory ovey p parsee parto passo pastose pato pau paut pavo pavy peasy perty peru pess peste pete peto petr plass platery pluto poe poy presee pretry pu purre purry puru r reve ro roer roey roy s sa saa salar salat salay saltee saltery salvy sao sapa saple sapo sare sart saur sauty sauve se seary seave seavy seesee sero sert sesuto sla slare slav slete sloo sluer soe sory soso spary spass spave spleet splet splurt spor spret sprose sput ssu stero steve stre strey stu sueve suto sutu suu t taa taar tal talao talose taluto tapeats tapete taplet tapuyo tarr tarse tartro tarve tasser tasu taur tave tavy teaer teaey teart teasy teaty teave teet teety tereu tess testor toru torve tosy tou treey tsere tst tu tue tur turr turse tute tutory u uro urs uru usee v vu y</p></blockquote>
<p>Many of the proper nouns in this list are present in the vocabulary of most English speakers: <em>Aleut, Peru, Pluto, Slav</em>; the same is true of personal names such as <em>Larry, Leo, Stu, Tess</em>. But the rest of the words are very unlikely to turn up in the smalltalk of teenage sweethearts. Indeed, the list is full of letter sequences I simply don’t recognize as English words. Please define <em>isuret, ovey, spleet,</em> or <em>sput</em>.</p>
<p>There are even bigger word lists out there. In 2006 Google extracted <a href="https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html">13.5 million unique English words</a> from public web pages. (The sheer number implies a very liberal definition of <em>English</em> and <em>word</em>.) A good place to start exploring this archive is <a href="https://norvig.com/ngrams/">Peter Norvig’s website</a>, which offers a file with the 333,333 most frequent words from the corpus. The list begins as you might expect: <em>the, of, and, to, a, in, for</em>…; but the weirdness creeps in early. The single letters <em>c, e, s,</em> and <em>x</em> are all listed among the 100 most common “words,” and the rest of the alphabet turns up soon after. By the time we get to the end of the file, it’s mostly typos <em>(mepquest, halloweeb, scholarhips)</em>, run-together words <em>(dietsdontwork, weightlossdrugs)</em>, and hundreds of letter strings that have some phonetic or orthographic resemblance to <em>Google</em> or <em>Yahoo!</em> or both <em>(hoogol, googgl, yahhol, gofool, yogol)</em>. (I suspect that much of this rubbish was scraped not from the visible text of web pages but from metadata stuffed into headers for purposes of search-engine optimization.)</p>
<p>Applying the Google list to the search for foldable words more than doubles the volume of results, but it contributes almost nothing to the stock of words that might form interesting messages. I found 1,543 new words, beyond those that are also present in the union of the Scrabble and Unix lists. In alphabetical order, the additions begin: <em>aae, aao, aaos, aar, aare, aaro, aars, aart, aarts, aase, aass, aast, aasu, aat, aats, aatsr, aau, aaus, aav, aave, aay, aea, aeae….</em> I’m not going to be folding up any straw wrappers with those words for my sweetheart.</p>
<p>What we really need, I begin to think, is not a longer word list but a shorter and more discriminating one.</p></div>







<p class="date">
by Brian Hayes <a href="http://bit-player.org/2021/foldable-words"><span class="datestr">at February 09, 2021 08:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=810">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/02/09/et-al-ii/">Et Al. II</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>From <a href="https://emanueleviola.wordpress.com/2018/01/16/et-al/">Thoughts, <time datetime="2018-01-16T16:48:38-05:00">January 16, 2018</time>:</a></strong></p>



<p>The et al. citation style favors scholars whose last name comes early in the dictionary. For example, other things equal, a last name like Aaron would circulate a lot more than Zuck. This problem is compounded by the existence of highly-cited papers which deviate from alphabetical ordering of authors. They carry the message: order matters, and some of you can’t use this trick, vae victis!</p>



<p>My suggestion is to avoid et al. and instead spell out every name (as in Aaron and Zuck) or every initial (as in AZ). It isn’t perfect, but improvements like randomly permuting the order still aren’t easy to implement. The suggestion actually cannot be implemented in journals like computational complexity which punish the authors into using an idiosyncratic style which has et al. But it doesn’t matter too much; nobody reads papers in those formats anyway, as we discussed <a href="https://emanueleviola.wordpress.com/tag/utopia/">several times</a>.</p>



<p></p>



<p></p>



<p></p>



<p><strong>From the <a href="http://acm-stoc.org/stoc2021/stoc-cfp.html">STOC 2021 call for papers</a></strong>:</p>



<p></p>



<p>Authors are asked to avoid “et al.” in citations in favor of an equal mention of all authors’ surnames (unless the number of authors is very large, and if it is large, consider just using \cite{} with no “et al.”). When not listing authors’ names, citations should preferably include the first letters of the authors’ surnames (or at least the first three followed by a +, and possibly the year of publication). If using BibTeX, this can be accomplished by using \bibliographystyle{alpha}.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/02/09/et-al-ii/"><span class="datestr">at February 09, 2021 06:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-9121434197807183435">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/whence-do-research-collaborations-in.html">Whence do research collaborations (in TCS) arise?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>About ten days ago, I gave a <a href="http://icetcs.ru.is/slides/intro2icetcs2021.pdf" target="_blank">talk</a> to my colleagues at the <a href="https://en.ru.is/st/dcs/" target="_blank">Department of Computer Science at Reykjavik University</a>, introducing my personal (and admittedly very biased) view of the past, present and future of <a href="http://icetcs.ru.is/" target="_blank">ICE-TCS</a>. </p><p>After my presenta­tion, a colleague asked me how she could engage mathematicians and theoretical computer scientists in joint research. I gave her an answer off the top of my head, but it was clear that she was unconvinc­ed and felt that I was avoid­ing answering her question. (For the record, I basically told her that she should knock on our door, discuss with us the problems she was interested in solving and hope that they are of interest to us. I feel that many research collaborations arise from serendipity and that there is no recipe that is guaranteed to work.) </p><p>The thought that she felt that I might have dodged her question prompted me to look back at my own research collabo­rations and how they came about. The rest of this post is the result of that quick-and-dirty reflection. Let me state right away that my list isn't meant to be exhausti­ve and that I won't mention many of the collaborations in which I have been lucky to be involved and that I have played a crucial role in shaping my academic development.  </p><p><b>Reading papers.</b> One of my long-term research collaborations arose from reading a paper written by a colleague. His paper prompted my companion and me to ask ourselves whether we could prove a similar result to the one our colleague had shown in a different setting. We succeeded and sent him our paper. Subsequently, we invited him to visit us in Aalborg. That visit marked the start of a collaboration and friendship that has lasted for over 20 years. <br /><br /><b>Approaching a colleague via email for help in solving a problem.</b> At some point, my companion and I were thinking about a research problem that had frustrated us for a while. I remembered reading a number of papers by a colleague on related topics, so I wrote to him, describing the problem, our attempts at solving it and where we had hit a brick wall. I asked him whether he would be interested in working with us on solving it. He did and that was one of the lucky breaks I have had in my research career. Once more, that collaboration offer via email led to mutual visits, other joint papers and, IMHO even more importantly, a long-term friendship that extended beyond work. <br /> </p><p><b>Available funding and building on one's mistakes.</b> One day in 2009, an email in my mailbox alerted me to the availability of substantial funding for research collabo­ration between universities in country X and those locat­ed in Norway, Iceland and Lichtenstein. This opportunity was enticing, as I had never visited country X, so I asked myself: "Is there anyone there we might conceivably work with?'' Mulling over that question, I recalled that a colleague from country X had spotted an imprecision in a paper I had coauthored. </p><p>I wrote to him, we applied for that funding jointly and got it. That successful grant application provided the funds for many research visits involving several people in our research groups. Those visits resulted in joint papers, another successful grant application and a number of friendships. </p><p><b>Coffee breaks at conferences.</b> I have at least two exhibits under this heading. The first belongs to a previous geologic­al era (1991). I was attend­ing a conference at CMU and asked a colleague what he was working on. He told me<br />about a problem he was tackling, which I knew was also on the radar of a fellow researcher and on which I had started working independently. Eventually, after some email exchanges, that chat over coffee turned into a three-way collaboration that, thanks to my coauthors, produced one of my best papers. <br /><br />Fast forward to 2017 and I'm in Rome to deliver an invited talk at a small conference. During the coffee break follow­ing my presentation, I was approached by a young research­er, with whom I had a number of pleasant conversations during the conference. Some time later, she sent me a draft paper dealing with a topic related to the content of my invited talk. I invited her to visit our research group in Reykjavik and to join the team working on a research project for which we had funding at the time. Those coffee-break conversations led to a collaboration and friendship that I hope will last for a long time. Meeting that colleague has been another of my lucky breaks. <br /> </p><p><b>Reading groups.</b> Last, but by no means least, let me mention that my first research collaboration that did not involve my thesis supervisors arose when I read Gordon Plotkin's famous "<a href="http://homepages.inf.ed.ac.uk/gdp/publications/Domains_a4.ps">Pisa Notes (On Domain Theory)</a>" with a fellow PhD student. Reading that work led to our first joint paper in 1991 and a companionship that has lasted to this day. I heard <a href="https://www.youtube.com/watch?v=KYfmXpLCiy4&amp;list=PLEyo0HIOhDCSR8os82H_2p5M0dE70K6KL&amp;index=2" target="_blank">Orna Kupferman</a> give the following, tongue-in-cheek advice to young researchers: "Write papers with your twin-sister!" Mine might be: "Write papers with your companion in life!" </p><p>Let me conclude by saying that serendipity and an actual friendship that extends beyond the confines of scientific work were the key aspects in my most pleasant and enduring collaborations. I apologise to the colleagues from whom I have learnt much over the years (former students and postdocs, as well as others) who were the prime movers in research collaborations I did not mention in this post. </p><p> I guess that this note provides much more information than my colleague was intending to receive, but I thought I should put it out for the benefit of the young researchers at Reykjavik University and at the Gran Sasso Science Institute, and of any reader I might have. </p><p>How did your research collaborations arise? If you have anything to add to what I wrote above, and I am sure you do, add your contributions as comments to this post. <br /><br /><br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/whence-do-research-collaborations-in.html"><span class="datestr">at February 08, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/">To cheer you up in difficult times 20: Ben Green presents super-polynomial lower bounds for off-diagonal van der Waerden numbers W(3,k)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">What will be the next polymath project? click here for our post about it. </a></p>
<h2 class="title mathjax"><a href="https://arxiv.org/abs/2102.01543">New lower bounds for van der Waerden numbers</a> by Ben Green</h2>
<p><span style="color: #0000ff;"><strong>Abstract:</strong> We show that there is a red-blue colouring of <em>[N]</em> with no blue 3-term arithmetic progression and no red arithmetic progression of length <img src="https://s0.wp.com/latex.php?latex=e%5E%7BC%28%5Clog+N%29%5E%7B3%2F4%7D%28%5Clog+%5Clog+N%29%5E%7B1%2F4%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="e^{C(\log N)^{3/4}(\log \log N)^{1/4}}." class="latex" title="e^{C(\log N)^{3/4}(\log \log N)^{1/4}}." /> Consequently, the two-colour van der Waerden number w(3,k) is bounded below by <img src="https://s0.wp.com/latex.php?latex=k%5E%7Bb%28k%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^{b(k)}" class="latex" title="k^{b(k)}" />, where <img src="https://s0.wp.com/latex.php?latex=b%28k%29%3Dc%28%5Cfrac%7B%5Clog+k%7D%7B%5Clog+%5Clog+k%7D%29%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="b(k)=c(\frac{\log k}{\log \log k})^{1/3}" class="latex" title="b(k)=c(\frac{\log k}{\log \log k})^{1/3}" />. Previously it had been speculated, supported by data, that <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29%3DO%28k%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k)=O(k^2)" class="latex" title="w(3,k)=O(k^2)" />.</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/gsm.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/gsm.png?w=640&amp;h=180" class="alignnone size-full wp-image-21182" height="180" /></a></p>
<p><span style="color: #ff0000;">The left side of the picture shows the world record holders for W(3,k). On the left Ben Green (LB) and in the centre Tomasz Schoen (UB). The pictures on the right shows protective mittens for people who make bold mathematical conjectures (<a href="https://gilkalai.wordpress.com/2021/01/19/what-if-they-are-all-wrong/">see Igor Pak’s post</a>) </span></p>
<p>The two-colour van der Waerden number <img src="https://s0.wp.com/latex.php?latex=w%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(m,k)" class="latex" title="w(m,k)" /> is the smallest  N such that however [N] = {1, . . . , N} is coloured blue and red, there is either a blue m-term arithmetic progression or a red k-term arithmetic progression. The celebrated theorem of van der Waerden implies that <img src="https://s0.wp.com/latex.php?latex=w%28m%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(m, k)" class="latex" title="w(m, k)" /> is finite.</p>
<p>The van der Waerden number <img src="https://s0.wp.com/latex.php?latex=w%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(m,k)" class="latex" title="w(m,k)" /> is analogous to the Ramsey number <img src="https://s0.wp.com/latex.php?latex=R%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="R(m,k)" class="latex" title="R(m,k)" />. Finding the behaviors of <img src="https://s0.wp.com/latex.php?latex=R%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="R(m,k)" class="latex" title="R(m,k)" /> is an important problem in Ramsey theory and much attention is given to <img src="https://s0.wp.com/latex.php?latex=R%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="R(k,k)" class="latex" title="R(k,k)" /> and of <img src="https://s0.wp.com/latex.php?latex=R%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="R(3,k)" class="latex" title="R(3,k)" />. Similarly, understanding the values of <img src="https://s0.wp.com/latex.php?latex=W%28m%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W(m,k)" class="latex" title="W(m,k)" />, and especially of <img src="https://s0.wp.com/latex.php?latex=W%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W(k,k)" class="latex" title="W(k,k)" /> and <img src="https://s0.wp.com/latex.php?latex=W%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="W(3,k)" class="latex" title="W(3,k)" />  are also  central problems in Ramsey theory. A big difference between van der Waerden number and Ramsey numbers is that there are density theorems for the existence of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />-terms arithmetic progressions. (Roth’s theorem for <img src="https://s0.wp.com/latex.php?latex=k%3D3&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k=3" class="latex" title="k=3" /> and Szemeredi’s theorem for general <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />.) There are several important methods to derive those density theorems (including Fourier methods, ergodic methods, and Szemeredi-type regularity) and these methods, as far as I know, do not apply for ordinary Ramsey numbers. (But correct me if I am wrong here, and if I am right and you have some insights as to why ergodic methods or Fourier methods do not apply to “ordinary” Ramsey, please share.)</p>
<p>Green’s paper  studies the values of <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k)" class="latex" title="w(3,k)" />. The best known upper bound is of Tomasz Schoen, <img src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29%3Ce%5E%7Bk%5E%7B1-c%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3, k)&lt;e^{k^{1-c}}" class="latex" title="w(3, k)&lt;e^{k^{1-c}}" /> for some constant <img src="https://s0.wp.com/latex.php?latex=c+%3E+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="c &gt; 0." class="latex" title="c &gt; 0." /> The best known lower bound until the new paper, was by Li and Shu: <img src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29+%5Cgg+%28k%2F+log+k%29%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3, k) \gg (k/ log k)^2" class="latex" title="w(3, k) \gg (k/ log k)^2" />. (This result, as the earlier bound by Robertson, used a probabilistic argument and relied on Lovasz’s local lemma.)</p>
<p>Several people conjectured, also based on empirical data, that <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29+%3D+O%28k%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k) = O(k^2)" class="latex" title="w(3,k) = O(k^2)" /> but now Green proved a super-polynomial lower bound! This is amazing! Congratulations, Ben!</p>
<p>It is largely conjectured that the Behrend-type bounds give the correct quantitative behaviour for Roth’s theorem (and Szemeredi theorem). In rough terms what we see from Green’s example is that this might be true also for van der Waerden numbers.</p>
<p>The proof is rather involved and long, so, naturally, there is  little I can say about it, which only slightly exceeds the little I actually know about it. The overview and other fragments of the paper I looked at are very illuminating. Here are a few things that caught my eyes.</p>
<p>1) A word about Tomasz Schoen’s upper bound and important paper: <a href="https://arxiv.org/abs/2006.02877">A subexponential upper bound for van der Waerden numbers W(3,k).</a>  Among other things Schoen’s proof relies on a lemma developed by Schoen for improved Roth bound. This relies on a structure theory of Bateman and Katz.  The paper gives a nice description of the state of the art regarding the diagonal values <img src="https://s0.wp.com/latex.php?latex=w%28k%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(k,k)" class="latex" title="w(k,k)" />. (Schoen’s upper bound on <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k)" class="latex" title="w(3,k)" /> follows also from the <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">more recent bound for Roth’s theorem</a> by Bloom and Sisask.)</p>
<p>2) Among the people that speculated that <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k)" class="latex" title="w(3,k)" /> behaves like <img src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^2" class="latex" title="k^2" /> is Ben Green himself. This is recorded in reference [9] of the paper. However, Green’s first reaction to this possibility was that it must be false. But he realized that some ideas for showing that it is false are themselves false.</p>
<p>3) Reference [9] in the paper is: <strong>B. J. Green, 100 open problems, manuscript, available on request</strong>. If you are curious about the list, request it!</p>
<p>4) New lower bounds in Ramsey theory are nor frequent. Thirteen years ago I described <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/">Elkin’s improvement</a> to Behrend’s bound and a few days ago I mentioned <a href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/">Linial and Shraibman’s new lower bounds</a> for the corner problem.  Green’s study started by looking at complements of 3-AP free sets. An example by Green and Julia Wolf (that followed Elkin’s result) turned out to be important for reaching some parts of Green’s strategy.</p>
<p>5) In some sense, something about the <img src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^2" class="latex" title="k^2" /> prediction is not entirely lost. The construction gives a sort of a multi-scale behaviour where in the <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" />th scale the example’s cardinality is <img src="https://s0.wp.com/latex.php?latex=k%5Er&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^r" class="latex" title="k^r" />.  (So all the empirical data comes from the <img src="https://s0.wp.com/latex.php?latex=r%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r=2" class="latex" title="r=2" /> regime.) Ben Green boldly suggests that the true values of <img src="https://s0.wp.com/latex.php?latex=w%283%2Ck%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3,k)" class="latex" title="w(3,k)" /> might exhibit such multi-scale behaviour. He conjectures that the true value of <img src="https://s0.wp.com/latex.php?latex=w%283%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w(3, k)" class="latex" title="w(3, k)" /> is quasi-polynomial, namely it lies somewhere in between the bound given by his construction and  something like <img src="https://s0.wp.com/latex.php?latex=k%5E%7Bc%5Clog+k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="k^{c\log k}" class="latex" title="k^{c\log k}" /> (which is Behrend-bound behaviour on the nose) .</p>
<p>6) Until Ben’s list of 100 problems becomes available to you, you may find interest in Francis Su’s <a href="https://www.francissu.com/post/100-questions-about-mathematics">100 questions about mathematics for discussion and reflection</a>.</p>
<p>7) In connection with Linial and Shraibman’s new lower bounds for the corner problem, let me mention that the best upper bound is by I.D. Shkredov. The paper is:  On a two-dimensional analog of Szemeredi’s Theorem in Abelian groups, Izvestiya of Russian Academy of Sciences, 73 (2009), 455–505.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/"><span class="datestr">at February 08, 2021 05:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8474138395715737050">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">The Victoria Delfino Problems: an example of math problems named after a non-mathematician</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> If you Google <b>Victoria Delfino </b>you will find that she is a real estate agent in LA (well, one of the Victoria Delfino's you find is such).  After this blog is posted you may well get this post on the first Google page. </p><p>If you Google <b>Victoria Delfino Problems</b> you will find a paper:</p><p><a href="https://andrescaicedo.files.wordpress.com/2008/04/vdp-finalversion-withreferences.pdf">The fourteen Victoria Delfino Problems and their Status in the year 2015</a></p><p>(ADDED LATER: a comment pointed me to an updated version, so  you can see that- I got to a pay wall.) </p><p>How did a real estate agent get honored by having 14 problems in descriptive set theory named after her?</p><p>Possibilities before I tell you which one.</p><p>1) Real estate is her day job. Her hobby is Descriptive Set Theory. Recall that Fermat was a lawyer (or something like that- see <a href="https://en.wikipedia.org/wiki/Pierre_de_Fermat">his Wikipedia page</a>) so perhaps she is similar. Doubtful- I think math is too hard for that now.  Or at least descriptive  set theory is too hard for that now. </p><p>2) She just happened to remark one day,<i> Gee, I wonder if</i></p><p><i> ZFC + SEP(Sigma_3^1) + #   implies DET(Delta_2^1).</i> </p><p>Its just the kind of thing someone might just say. That was problem 4 of the 14. </p><p>3) There are two Victoria Delfino's- one is a realtor, one is a mathematician. While plausible, that would not be worth blogging about. </p><p>4) And now the truth: Victoria was the realtor who helped Moschovakis (a descriptive set theorist who I will henceforth describe as M) buy his house. When Tony Martin (another Desc. Set Theorist) moved to UCLA, M referred him to Victoria and she did indeed help Tony find a house. Victoria gave M a large commission which he tried to turn down. She did not want it returned, so M used the money to fund five problems. Later problems were added, but for no money. The article <i>The Fourteen... </i>linked to above has the full story. It also has the curious line: </p><p><i>Contrary to popular belief, no monetary prize is attached to further problems. </i></p><p>I didn't think any of this was so well known as to have popular believes. </p><p>ANYWAY, this is an example of a math problem named after a non-math person. Are there others? Will the name stick? Probably not- already 12 of the 14 are solved. I have noted in a prior blog (<a href="https://blog.computationalcomplexity.org/2009/08/how-much-credit-should-conjecturer-get_14.html">here</a>) once a conjecture gets proven, the one who made the conjecture gets forgotten. Or in this case the person who the conjectures is named after. </p><p>So are there other open problems in math named after non-math people? How about Theorems?</p><p>Near Misses: </p><p>Pythagoras: Not clear what he had to do with the theorem that bears his name. </p><p>L'hopital's Rule: the story could be a blog in itself, and in fact it is! Not mind, but someone else: <a href="https://andrescaicedo.wordpress.com/2013/11/05/credit/">here</a>. However L'hopital was a mathematician. </p><p>Sheldon's conjecture (see <a href="https://blog.computationalcomplexity.org/2019/10/the-sheldon-conjecture-too-late-for.html">here</a>) was named after a FICTIONAL physicist. Note that Sheldon inspired the conjecture but did not make it. It has been solved. </p><p>The Governor's  Theorem (see <a href="https://blog.computationalcomplexity.org/2013/08/how-much-trig-does-your-governor-know.html">here</a>) was named because Jeb Bush was asked for the angles of a 3-4-5 right triangle (not a fair question). </p><p>The Monty Hall Paradox.</p><p>SO- are there Open Problems, Theorems, Lemmas, any math concepts, named after non-math people? I really mean non-STEM people. If a Physicist or an Engineer or a Chemist or Biologist or...  has their name on something, that would not really be what I want.</p><p>(ADDED LATER - someone emailed me two oddly-named math things:</p><p>Belphegor's prime, <span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">see </span><a href="https://en.wikipedia.org/wiki/Belphegor%27s_prime">here</a></p><p>Morrie's law- odd since Morrie is the FIRST name of who the name is honoring, see <a href="https://en.wikipedia.org/wiki/Morrie%27s_law">here</a> </p><p>)</p><p><br /></p><p>Are there any other open problems in descriptive set theory  named after realtors?</p><p><i><br /></i></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html"><span class="datestr">at February 08, 2021 04:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1475">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1475">News for January 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The first month of 2021 has brought with it 5 papers, covering graph testing, Boolean function testing, and distribution testing — as well as database theory. Let’s dive into it.</p>



<p><strong>Random walks and forbidden minors III: \(\mathrm{poly}(d/\varepsilon)\)-time partition oracles for minor-free graph classes</strong>, by Akash Kumar, C. Seshadhri, and Andrew Stolman (<a href="https://arxiv.org/abs/2102.00556">arXiv</a>). Minor-closed bounded-degree graphs have a very nice property: denoting by \(d\) the degree bound and \(n\) the number of edges, it is always possible to partition any such graph into components of <em>constant</em> size, \(O_\varepsilon(1)\), just by removing a linear number of edges, merely \(\varepsilon dn\) (\(\varepsilon\) being a small parameter). This is a crucial result in many graph property testing algorithms, those which rely on something called a “partition oracle”: loosely speaking, a routine which makes “few” queries to the graph, and is able to indicate which component of an underlying partition any given vertex belongs to. But what is “few” here? The first partition oracles made \(d^{\mathrm{poly}(d,1/\varepsilon)}\) queries to the graph to answer any such request. This later got significantly improved to \(d^{\mathrm{log}(d/\varepsilon)}\). Using spectral graph theory techniques previously developed by the authors (hence the “III” in the title), this work settles the question, achieving partition oracles which as good as it gets: making only \(\mathrm{poly}(d,1/\varepsilon)\) queries to the graph! This in turns has immediate consequences for graph property testing, which the paper details.</p>



<p>And since we are on the topic of oracles… more exciting news on that front:</p>



<p><strong>Spectral Clustering Oracles in Sublinear Time</strong>, by Grzegorz Gluch, Michael Kapralov, Silvio Lattanzi, Aida Mousavifar, Christian Sohler (<a href="https://arxiv.org/abs/2101.05549">arXiv</a>). Given a graph \(G\) and an integer \(k\), one often want to partition the graph into components \(C_1,C_2,\dots,C_k\), such that each \(C_i\) is well-connected and has few edges to the other \(C_j\)’s. But can we get <em>ultra</em> efficient algorithms for such spectral clusterings? Specifically, can we design oracles for them: sublinear-time algorithms which provide implicit access to an underlying “good” spectral clustering \(\hat{C}_1,\hat{C}_2,\dots,\hat{C}_k\), by returning on any query vertex \(v\) the index of the cluster \(\hat{C}_i\) to which \(v\) belongs? This paper introduces the question, and answers it in the affirmative: in more detail, it provides a spectral clustering oracle which, for any \(\varepsilon&gt;0\), has preprocessing time \(2^{\mathrm{poly}(k/\varepsilon)}n^{1/2+O(\varepsilon)}\), query time \(n^{1/2+O(\varepsilon)}\), and space \(n^{1/2+O(\varepsilon)}\); and provides access to a clustering with relative error \(O(\varepsilon\log k)\) per cluster. The paper also allows tradeoffs between query time and space, and discusses applications to the Local Computation Algorithms (LCA) model.</p>



<p>Next stop, distribution testing…</p>



<p><strong>The Sample Complexity of Robust Covariance Testing</strong>, by Ilias Diakonikolas and Daniel M. Kane (<a href="https://arxiv.org/abs/2012.15802">arXiv</a>). Suppose you have i.i.d. samples from some high-dimensional Gaussian \(\mathcal{N}(0,\Sigma)\) in \(d\) dimensions, and want to test whether the unknown covariance matrix \(\Sigma\) is the identity, versus \(\varepsilon\)-far from it (in Frobenius norm). Good news: we know how to do that, and \(\Theta(d/\varepsilon^2)\) samples are necessary and sufficient. (To <em>learn</em> \(\Sigma\), that’d be \(\Theta(d^2/\varepsilon^2)\).) Bad news: you don’t have i.i.d. samples from some high-dimensional Gaussian \(\mathcal{N}(0,\Sigma)\); what you have is i.i.d. samples from a noisy version of it, \((1-\alpha)\mathcal{N}(0,\Sigma) + \alpha B\), where \(B\) is an arbitrary “bad” distribution (not necessarily Gaussian itself). You still want to test whether the covariance \(\Sigma\) is the identity, but now you have that extra \(\alpha\) fraction of noisy samples, and you need to do that testing robustly… The good news is, you can still do that by learning the covariance matrix \(\Sigma\), robustly, with \(O(d^2/\varepsilon^2)\) samples. The bad news is the main result of this paper: that’s also the best you can do. That is, \(\Omega(d^2)\) samples are necessary: if you have to be robust to noise, testing is no longer easier than learning….</p>



<p>Onto Boolean functions!</p>



<p><strong>Junta Distance Approximation with Sub-Exponential Queries</strong>, by Vishnu Iyer, Avishay Tal, and Michael Whitmeyer (<a href="https://eccc.weizmann.ac.il/report/2021/004/">ECCC</a>). If you follow this blog, you may have seen over the past couple years a flurry of results about <em>tolerant junta testing</em>: “given query access to some Boolean function \(f\colon\{-1,1\}^n\to\{-1,1\}\), how close is \(f\) to only depending on \(k \ll n\) of its variables?”<br />This paper contains several results on this problem, including an improved bicriteria tolerant testing algorithm: an efficient algorithm to distinguish between \(\varepsilon\)-close to \(k\)-junta and \(1.1\varepsilon\)-far from \(k’\)-junta making \(\mathrm{poly}(k,1/\varepsilon)\) queries (and \(k’ = O(k/\varepsilon^2)\)). But the main result of the paper, and the one giving it its name, is for the non-relaxed version where \(k’=k\): while all previous works had a query complexity \(2^{O(k)}\), here the authors show how to break that exponential barrier, giving a fully tolerant testing algorithm with query complexity \(2^{\tilde{O}(\sqrt{k})}\)!</p>



<p>And finally, a foray into database theory:</p>



<p><strong>Towards Approximate Query Enumeration with Sublinear Preprocessing Time</strong>, by  Isolde Adler and Polly Fahey (<a href="https://arxiv.org/abs/2101.06240">arXiv</a>). In this paper, the authors are concerned with the task of (approximate) query enumeration on databases, aiming for ultra efficient (i.e., sublinear-time) algorithms. Leveraging techniques from property testing (specifically, in the bounded-degree graph model), they show the following:<br /><em>On input databases of bounded degree and bounded tree-width, every (fixed) first-order definable query can be enumerated approximately in time linear in the output size, after only a sublinear-time preprocessing phase</em>.</p>



<p>That’s all for this month! If you noticed a paper we missed, please let us know in the comments.</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1475"><span class="datestr">at February 07, 2021 10:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=83">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/02/05/thursday-feb-18-costis-daskalakis-from-mit/">Thursday Feb 18 — Costis Daskalakis from MIT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Welcome to the Spring 2021 edition of Foundations of Data Science Virtual Talks. </p>



<p>Our first talk for the season will take place on <strong>Thursday, Feb 18</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Costis Daskalakis</strong> from <strong>MIT</strong> will speak about “<strong>Equilibrium Computation and the Foundations of Deep Learning</strong>.”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: Deep Learning has recently yielded important advances in single-agent learning challenges, much of that progress being fueled by the empirical success of gradient descent and its variants in computing local optima of non-convex optimization problems. In multi-agent learning applications, the role of single-objective optimization is played by equilibrium computation, yet our understanding of its complexity in settings that are relevant for Deep Learning remains sparse. In this talk we focus on min-max optimization of nonconvex-nonconcave objectives, which has found applications in GANs, and other adversarial learning problems. Here, not only are there no known gradient-descent based methods converging to even local and approximate min-max equilibria, but the computational complexity of identifying them remains poorly understood. We show that finding approximate local min-max equilibria of Lipschitz and smooth objectives requires a number of queries to the function and its gradient that is exponential in the relevant parameters, in sharp contrast to the polynomial number of queries required to find approximate local minima of non convex objectives. Our oracle lower bound is a byproduct of a complexity-theoretic result showing that finding approximate local min-max equilibria is computationally equivalent to finding Brouwer fixed points, and Nash equilibria in non zero-sum games, and thus PPAD-complete.</p>



<p>Minimal complexity theory knowledge will be assumed in the talk. Joint work with Stratis Skoulakis and Manolis Zampetakis.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/02/05/thursday-feb-18-costis-daskalakis-from-mit/"><span class="datestr">at February 05, 2021 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-7077954644615536585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/support-research-in-foundations-of.html">Support research in the Foundations of Computing at the University of Leicester!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In an ideal world, university administrators would support the work of the top-class academics employed by their institution, especially if they attract students, have a high research standing within their communities and bring in substantial funding from competitive research funds. After all, to quote <a href="https://en.wikipedia.org/wiki/Isidor_Isaac_Rabi" target="_blank">Isidor Isaac Rabi</a>, "<a href="https://academicanchor.wordpress.com/2012/08/09/dwight-eisenhower-and-university-faculty/" target="_blank">the faculty are the university</a>" and the most valuable currency for an academic institution is reputation. </p><p>Unfortunately, university administrations the world over repeatedly surprise me by making structural changes that affect some of their very best academics and actually <i>reduce</i> the reputation of their institutions in the eyes of the community at large. </p><p>The latest example comes from the University of Leicester, where, as stated <a href="https://www.ipetitions.com/petition/foco-is-not-redundant" target="_blank">here</a>, </p><p style="margin-left: 40px; text-align: left;">[the] University VC proposes to merge Informatics and Mathematics into a  combined school focussed exclusively on AI, data science, computational  modelling and "digitalisation". This includes the proposal to cease  research in Foundations of Computer Science (FoCo) where research is  "highly theoretical and not directly linked with applications",  retaining staff only if the research they have published in the past (!)  aligns well with the new desired focus on foundations of AI,  computational modelling, data science and digitalisation. Staff have  been given no opportunity to alter their research to fit with the  proposed new direction. The plan is <u>to make redundant (in the middle of a pandemic) all (up to 10) staff in foundations of computer science</u> whose past research is deemed not to be a good enough fit with the new strategic priorities. </p><p>See also <a href="https://www.uculeicester.org.uk/ucu/first-statement-on-threatened-compulsory-redundancies/" target="_blank">this statement</a> by the University and College Union of the University of Leicester. </p><p>I might be biased, but I find it inconceivable that one can think of building a world-class research programme in AI, data science and computational modelling without building on existing strengths in the Foundations of Computer Science and Mathematics. What my crystal ball tells me is that the strong Leicester academics who might be affected by the planned restructuring will find positions elsewhere and that the University of Leicester is shooting itself in the foot. Which high-profile academic would be enticed to join a university that has shown so little consideration for its existing areas of strength and where one's job might be in danger when the buzzwords du jour change, as they undoubtedly will? </p><p>I encourage you to sign the <a href="https://www.ipetitions.com/petition/foco-is-not-redundant" target="_blank">petition</a> in support of our Leicester colleagues. Kudos to <a href="https://en.wikipedia.org/wiki/Isobel_Armstrong" target="_blank">Isobel Armstrong</a>, FBA, for <a href="https://twitter.com/leicesterucu/status/1355277601980489729" target="_blank">returning her honorary doctorate</a> to the University of Leicester upon hearing of their plans!<br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/support-research-in-foundations-of.html"><span class="datestr">at February 05, 2021 11:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=528">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/02/04/tcs-talk-wednesday-february-17-william-hoza-ut-austin/">TCS+ talk: Wednesday, February 17 — William Hoza, UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Welcome back for a new season of TCS+! Our talks are back to a fortnightly schedule, with an exciting slate of speakers ahead of us.</p>
<p>The first TCS+ talk will take place in two weeks, Wednesday, February 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>William Hoza</strong> from UT Austin will speak about “<em>Fooling Constant-Depth Threshold Circuits</em>” (abstract below). You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The talk will be recorded and posted on our website and <a href="https://www.youtube.com/user/TCSplusSeminars/videos">YouTube channel</a> afterwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We present the first non-trivial pseudorandom generator (PRG) for linear threshold (LTF) circuits of arbitrary constant depth and super-linear size. This PRG fools circuits with depth <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> and <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1+%2B+%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n^{1 + \delta}" class="latex" title="n^{1 + \delta}" /> wires, where <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+%5Cexp%28-O%28d%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\delta = \exp(-O(d))" class="latex" title="\delta = \exp(-O(d))" />, using seed length <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B1+-+%5Cdelta%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="O(n^{1 - \delta})" class="latex" title="O(n^{1 - \delta})" /> and with error <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-n%5E%7B%5Cdelta%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\exp(-n^{\delta})" class="latex" title="\exp(-n^{\delta})" />. This tightly matches the best known lower bounds for this circuit class. As a consequence of our result, all the known hardness for LTF circuits has now effectively been translated into pseudorandomness. This brings the extensive effort in the last decade to construct PRGs and deterministic circuit-analysis algorithms for this class to the point where any subsequent improvement would yield breakthrough lower bounds.</p>
<p>A key ingredient in our construction is a pseudorandom restriction procedure that has tiny failure probability, but simplifies the function to a non-natural “hybrid computational model” that combines decision trees and LTFs. As part of our proof we also construct an “extremely low-error” PRG for the class of functions computable by an arbitrary function of s linear threshold functions that can handle even the extreme setting of parameters <img src="https://s0.wp.com/latex.php?latex=s+%3D+n%2F%5Cmathrm%7Bpolylog%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="s = n/\mathrm{polylog}(n)" class="latex" title="s = n/\mathrm{polylog}(n)" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+%5Cexp%28-n%2F%5Cmathrm%7Bpolylog%7D%28n%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\epsilon = \exp(-n/\mathrm{polylog}(n))" class="latex" title="\epsilon = \exp(-n/\mathrm{polylog}(n))" />.</p>
<p>Joint work with Pooya Hatami, Avishay Tal, and Roei Tell.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/02/04/tcs-talk-wednesday-february-17-william-hoza-ut-austin/"><span class="datestr">at February 05, 2021 03:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/04/graph-products/">Graph Products</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>The power of definitions and notations</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/ikronec001p1/" rel="attachment wp-att-18075"><img width="120" alt="" class="alignright  wp-image-18075" src="https://rjlipton.files.wordpress.com/2021/02/kro.jpg?w=120" /></a></p>
<p>
Leopold Kronecker was one of the great mathematicians of the <a href="https://en.wikipedia.org/wiki/Leopold_Kronecker">19th century</a>. He thought about foundational issues deeply. We highlighted him <a href="https://rjlipton.wordpress.com/2012/08/31/high-school-trig-is-powerful/">before</a>—well not deeply.</p>
<p>
Today I thought we would talk about some core math ideas arising out of Kronecker’s work.</p>
<p>
Kronecker’s angle as an early leader in the modern <a href="https://en.wikipedia.org/wiki/Foundations_of_mathematics">foundations</a> of mathematics was on which aspects are helpful in concrete analysis. He no doubt would be comfortable with complexity theory, with our interest in not just existence proofs, but in concrete algorithms for the construction of objects. He famously said:</p>
<blockquote><p><b> </b> <em> <i>God made the integers, all else is the work of man.</i> </em>
</p></blockquote>
<p></p><p>
His was a philosophy that would agree with our view of complexity theory. Maybe he would say now:</p>
<blockquote><p><b> </b> <em> <i>God made the binary strings, all else is the work of people.</i> </em>
</p></blockquote>
<p>
</p><p></p><h2> Operations </h2><p></p>
<p></p><p>
In any part of mathematics we are often interested in operations that take objects and make new objects. These operations are important as they allow us to build new interesting objects. </p>
<ul>
<li>
In strings we can take two and concatenate them to make a new one. <p></p>
</li><li>
In matrices we can add or multiply them. <p></p>
</li><li>
More relevant to today’s topic we can take two matrices and make the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker </a> product: <p></p>
<p align="center"> <a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/prod/" rel="attachment wp-att-18079"><img src="https://rjlipton.files.wordpress.com/2021/02/prod.png?w=600" alt="" class="aligncenter size-full wp-image-18079" /></a> </p>
</li><li>
In graph theory we can take two graphs <img src="https://s0.wp.com/latex.php?latex=%7BG_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_1}" class="latex" title="{G_1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_2}" class="latex" title="{G_2}" /> and make a new one <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" />.
</li></ul>
<p>
There are a wealth of such operations on graphs. One is called Cartesian product, one the strong product, one the direct product. A trouble, in my opinion, is that the names and the notation for these operations is not uniform. There are alternative names for almost all the major operations: For example,</p>
<blockquote><p><b> </b> <em> The Cartesian product of graphs is sometimes called the box product of graphs—see <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">here</a>. The notation that <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Ctimes+H%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G \times H}" class="latex" title="{G \times H}" /> has often been used for Cartesian products of graphs, but is now more commonly used for another construction known as the tensor product of graphs. </em>
</p></blockquote>
<p></p><p>
Confusing, no? </p>
<p>
Ken notes that it gets even more confusing when one teaches the Cartesian product construction of finite automata. Each automaton has a state graph. In general the state graph is directed and the edges are labeled by characters, but one can make cases where the graph is undirected and there is only one character. The state graph of the product machine is in general <em>not</em> the Cartesian product of the graphs of the individual machines. What product is it? Let’s look at graph operations.</p>
<p>
</p><p></p><h2> Towards a Uniform Definition </h2><p></p>
<p></p><p>
Let us give a uniform definition of three basic graph <a href="https://en.wikipedia.org/wiki/Graph_product">products</a>. Let <img src="https://s0.wp.com/latex.php?latex=%7BG_1%2C%5Cdots%2CG_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_1,\dots,G_m}" class="latex" title="{G_1,\dots,G_m}" /> be undirected graphs. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H+%3D+%5Csquare%28d%2CG_1%2C%5Cdots%2CG_m%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  H = \square(d,G_1,\dots,G_m) " class="latex" title="\displaystyle  H = \square(d,G_1,\dots,G_m) " /></p>
<p>is the graph with vertices <img src="https://s0.wp.com/latex.php?latex=%7B%28x_1%2C%5Cdots%2Cx_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x_1,\dots,x_m)}" class="latex" title="{(x_1,\dots,x_m)}" /> so that each <img src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_k}" class="latex" title="{x_k}" /> is a vertex in <img src="https://s0.wp.com/latex.php?latex=%7BG_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_k}" class="latex" title="{G_k}" />, and the vertices <img src="https://s0.wp.com/latex.php?latex=%7B%28x_1%2C%5Cdots%2Cx_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(x_1,\dots,x_m)}" class="latex" title="{(x_1,\dots,x_m)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%28y_1%2C%5Cdots%2Cy_m%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(y_1,\dots,y_m)}" class="latex" title="{(y_1,\dots,y_m)}" /> are connected provided for exactly <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" /> indices <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" title="{k}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_k}" class="latex" title="{x_k}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y_k}" class="latex" title="{y_k}" /> are an edge in <img src="https://s0.wp.com/latex.php?latex=%7BG_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G_k}" class="latex" title="{G_k}" /> and for the rest <img src="https://s0.wp.com/latex.php?latex=%7Bx_k%3Dy_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_k=y_k}" class="latex" title="{x_k=y_k}" />.</p>
<ul>
<li>
The <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">Cartesian</a> product requires exactly one edge: 	<p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csquare%281%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \square(1,G_1,\dots,G_m). " class="latex" title="\displaystyle  \square(1,G_1,\dots,G_m). " /></p>
<p>It is usually written as 	 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Csquare%5Ccdots+%5Csquare+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  G_1 \square\cdots \square G_m." class="latex" title="\displaystyle  G_1 \square\cdots \square G_m." /></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Strong_product_of_graphs">strong</a> product requires at least one edge: 	<p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbigcup_%7Bd+%5Cge+1%7D%5C+%5Csquare%28d%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \bigcup_{d \ge 1}\ \square(d,G_1,\dots,G_m). " class="latex" title="\displaystyle  \bigcup_{d \ge 1}\ \square(d,G_1,\dots,G_m). " /></p>
<p>It is usually written as 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Cfbox%7Bx%7D+%5Ccdots+%5Cfbox%7Bx%7D+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  G_1 \fbox{x} \cdots \fbox{x} G_m." class="latex" title="\displaystyle  G_1 \fbox{x} \cdots \fbox{x} G_m." /></p>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor</a> product requires all edges: 	<p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csquare%28m%2CG_1%2C%5Cdots%2CG_m%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \square(m,G_1,\dots,G_m). " class="latex" title="\displaystyle  \square(m,G_1,\dots,G_m). " /></p>
<p>It is usually written as 	 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G_1+%5Ctimes+%5Ccdots+%5Ctimes+G_m.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  G_1 \times \cdots \times G_m." class="latex" title="\displaystyle  G_1 \times \cdots \times G_m." /></p>
</li></ul>
<p>
Here is a comparison of four graph products.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/prod2/" rel="attachment wp-att-18076"><img src="https://rjlipton.files.wordpress.com/2021/02/prod2.png?w=600" alt="" class="aligncenter size-full wp-image-18076" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The answer to Ken’s question is that you get the tensor product, which is Kronecker’s product on the adjacency matrices of the graphs. This is because each step of the product requires an action by each constituent machine.</p>
<p>
Of course then we get other types of products for other values of <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" />. Are any of these interesting? We get intermediate notions up to Kronecker’s product. Can the be put to any natural use?</p>
<p>
</p><p></p><h2> Coloring Products—Conjecture </h2><p></p>
<p></p><p>
An application of graph products is that they yield some quite compelling conjectures. The <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">conjecture</a> due to Stephen Hedetniemi in 1966 is one example. This states that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cchi+%28G+%5Ctimes+H%29+%3D+min%28+%5Cchi+%28G%29%2C+%5Cchi+%28H%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \chi (G \times H) = min( \chi (G), \chi (H)). " class="latex" title="\displaystyle  \chi (G \times H) = min( \chi (G), \chi (H)). " /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi(G)}" class="latex" title="{\chi(G)}" /> is the number of colors needed to color <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />. The conjecture is false–see <a href="https://arxiv.org/abs/1905.02167">Counterexamples to Hedetniemi’s conjecture</a> by Yaroslav Shitov. Also see Gil Kalai’s <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">post</a> about this news.</p>
<p>
</p><p></p><h2> Coloring Products—Proofs </h2><p></p>
<p></p><p>
A potential application of graph products is to the famous Four Color Theorem (4CT)—see <a href="https://en.wikipedia.org/wiki/Four_color_theorem">here</a>. In a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.4392&amp;rep=rep1&amp;type=pdf">paper</a> of mine with Atish Das Sarma, Amita Gajewar, and Danupon Nanongkai we show:</p>
<blockquote><p><b>Theorem 1 (An Approximate Restatement of the Four-Color Theorem)</b> <em> Suppose every two-edge connected, cubic, planar graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" /> can be edge 3-colored with <img src="https://s0.wp.com/latex.php?latex=%7Bo%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{o(n)}" class="latex" title="{o(n)}" /> errors. Then the Four Color Theorem is true. </em>
</p></blockquote>
<p></p><p>
The proof is simple. We assume that some graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" /> is a counterexample to the 4CT. Then form a kind of product <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />. To be honest we did not see the proof exactly as this, but is essentially what we did. The <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> is a huge product of many of the copies of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />, with some small modifications. Then we show if we could almost four color <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> then we would be able to exactly color <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Meanwhile, the paper showing that Hedetniemi’s conjecture is false contains an important lesson for mathematicians, Noga Alon says, </p>
<blockquote><p><b> </b> <em> <i>Sometimes, the reason that a conjecture is very hard to prove is simply that it is false.</i> </em>
</p></blockquote>
<p></p><p><br />
[restored missing line in second sentence, other fixes]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/04/graph-products/"><span class="datestr">at February 04, 2021 10:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2021/02/03/focs-2021/">FOCS 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A (very) preliminary call for papers for FOCS 2021 now available: </p>



<p><a href="https://t.co/1SAg6jInlH?amp=1" target="_blank" rel="noreferrer noopener">https://cs.yale.edu/homes/vishnoi/focs-2021-cfp.html…</a> </p>



<figure class="wp-block-image"><img src="https://pbs.twimg.com/media/EtQ2VzPXIAIOFiE?format=jpg&amp;name=large" alt="Image" /></figure>



<p>This year,  the submission deadline has been delayed to early June. This is in order to maximize the chances of a physical conference (sometime in early 2022). </p>



<p></p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2021/02/03/focs-2021/"><span class="datestr">at February 03, 2021 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/">Postdoc in Dynamic Graph Algorithms at Technical University of Denmark, Copenhagen, Denmark (apply by March 21, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Together, we will study different hypotheses, problems, and ideas concerning dynamic graph algorithms, in the pursuit of new, efficient algorithms.<br />
Here, the fun challenge is to find just the right partial answers to maintain as the graph changes, and often, the road to efficient dynamic algorithms goes via new graph theoretic insights.<br />
Contact: Eva Rotenberg.</p>
<p>Website: <a href="https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21">https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions/job?id=9903f461-7515-4f33-84bc-843f749b3d21</a><br />
Email: erot@dtu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/03/postdoc-in-dynamic-graph-algorithms-at-technical-university-of-denmark-copenhagen-denmark-apply-by-march-21-2021/"><span class="datestr">at February 03, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7991231367461036724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html">A Blood Donation Puzzle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the US you can donate whole blood every eight weeks. Suppose Elvira does exactly that. Will she hit every date of the year? For example, if Elvira gave blood today, will she in some future year give blood on the 4th of July? Can we figure it out without having to rely on a computer simulation or even a calculator?</p><p>Let's make the assumptions that the blood center is open every day and that Elvira gives blood exactly every 56 days for eternity. </p><p>A year has 365 days which is relatively prime to 56=2<sup>3</sup>*7 since 365 mod 2 =1 and 365 mod 7 = 1. By the <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a> her next 365 blood donations will be on 365 distinct dates. If Elvira started giving blood at age 17, she will have hit every date at age 73.</p><p>That was easy but wrong. We have to account for those pesky leap years.</p><p>In a four year span, there will be one leap day. The total days in four years (using modular arithmetic) will still be odd and 5 mod 7, so still relatively prime to 56. So Elvira will donate on every day on the calendar exactly four times, except February 29th which she will hit once, over a period 56*4=224 years. </p><p>Alas not quite. Years ending 00 are not leap years, unless the year is divisible by 400. 2000 was a leap year but 2100 won't be. Any stretch of 224 years will hit at least one of those 00 non-leap years.</p><p>In 400 years, there will be 97 leap years. Since a regular year is 1 mod 7 days and a leap year is 2 mod 7 days, 400 years will be 497 mod 7 days. Since 497=71*7, 400 years has a multiple of 7 days. Every 400 years we have exactly the same calendar. February 3, 2421 is also a Wednesday.</p><p>The cycle of blood donations will repeat every 3200 years, the number of years in the least common multiple of 56 and the odd multiple of seven number of days in 400 years. But we can no longer directly apply the Chinese remainder theorem and argue that every day of the year will be hit. In those 3200 years Elvira will have over 20,000 blood donations. If the dates were chosen randomly the expected number to hit all dates would be 2372 by <a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem">coupon collector</a>. So one would expect Elvira would hit every day, but that's not a proof.</p><p>So I had to dust off my Python skills and do the computer simulation after all. No matter what day Elvira starts donating she will eventually hit every date of the year. If Elvira starts donating today, she would give blood on the 4th of July for the first time in 2035 and <a href="https://docs.google.com/document/d/10aTFv5UUWfsrkKmVeud_r2VObfS1_J1tJflMeWld2w4/edit?usp=sharing">hit all dates</a> on January 8, 2087 after 431 donations. The longest sequence is 3235 donations starting April 25, 2140 and hitting all dates on February 29, 2636.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/a-blood-donation-puzzle.html"><span class="datestr">at February 03, 2021 01:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/">To cheer you up in difficult times 19: Nati Linial and Adi Shraibman construct larger corner-free sets from better numbers-on-the-forehead protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">What will be the next polymath project? click here for our previous post. </a></p>
<h2>Number on the forehead, communication complexity, and additive combinatorics</h2>
<p class="title mathjax"><a href="https://arxiv.org/abs/2102.00421">Larger Corner-Free Sets from Better NOF Exactly-<span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">N</span></span></span></span> Protocols</a>, by Nati Linial and Adi Shraibman</p>
<p><span style="color: #0000ff;"><strong>Abstract:</strong> A subset of the integer planar grid <em><span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6">[</span><span class="mi" id="MathJax-Span-7">N</span><span class="mo" id="MathJax-Span-8">]</span><span class="mo" id="MathJax-Span-9">×</span><span class="mo" id="MathJax-Span-10">[</span><span class="mi" id="MathJax-Span-11">N</span><span class="mo" id="MathJax-Span-12">]</span></span></span></span></em> is called <strong>corner-free</strong> if it contains no triple of the form <em><span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mo" id="MathJax-Span-15">(</span><span class="mi" id="MathJax-Span-16">x</span><span class="mo" id="MathJax-Span-17">,</span><span class="mi" id="MathJax-Span-18">y</span><span class="mo" id="MathJax-Span-19">)</span><span class="mo" id="MathJax-Span-20">,</span><span class="mo" id="MathJax-Span-21">(</span><span class="mi" id="MathJax-Span-22">x</span><span class="mo" id="MathJax-Span-23">+</span><span class="mi" id="MathJax-Span-24">δ</span><span class="mo" id="MathJax-Span-25">,</span><span class="mi" id="MathJax-Span-26">y</span><span class="mo" id="MathJax-Span-27">)</span><span class="mo" id="MathJax-Span-28">,</span><span class="mo" id="MathJax-Span-29">(</span><span class="mi" id="MathJax-Span-30">x</span><span class="mo" id="MathJax-Span-31">,</span><span class="mi" id="MathJax-Span-32">y</span><span class="mo" id="MathJax-Span-33">+</span><span class="mi" id="MathJax-Span-34">δ</span><span class="mo" id="MathJax-Span-35">)</span></span></span></span></em>. It is known that such a set has a vanishingly small density, but how large this density can be remains unknown. The best previous construction was based on Behrend’s large subset of <em><span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-36"><span class="mrow" id="MathJax-Span-37"><span class="mo" id="MathJax-Span-38">[</span><span class="mi" id="MathJax-Span-39">N</span><span class="mo" id="MathJax-Span-40">]</span></span></span></span></em> with no <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-41"><span class="mrow" id="MathJax-Span-42"><span class="mn" id="MathJax-Span-43">3</span></span></span></span>-term arithmetic progression. Here we provide the first substantial improvement to this lower bound in decades. Our approach to the problem is based on the theory of communication complexity.</span></p>
<p><span style="color: #0000ff;">In the <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-44"><span class="mrow" id="MathJax-Span-45"><span class="mn" id="MathJax-Span-46">3</span></span></span></span>-players exactly-<span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-47"><span class="mrow" id="MathJax-Span-48"><span class="mi" id="MathJax-Span-49">N</span></span></span></span> problem the players need to decide whether <em><span class="MathJax" id="MathJax-Element-8-Frame"><span class="math" id="MathJax-Span-50"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52">x</span><span class="mo" id="MathJax-Span-53">+</span><span class="mi" id="MathJax-Span-54">y</span><span class="mo" id="MathJax-Span-55">+</span><span class="mi" id="MathJax-Span-56">z</span><span class="mo" id="MathJax-Span-57">=</span><span class="mi" id="MathJax-Span-58">N</span></span></span></span> </em>for inputs <em><span class="MathJax" id="MathJax-Element-9-Frame"><span class="math" id="MathJax-Span-59"><span class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61">x</span><span class="mo" id="MathJax-Span-62">,</span><span class="mi" id="MathJax-Span-63">y</span><span class="mo" id="MathJax-Span-64">,</span><span class="mi" id="MathJax-Span-65">z</span></span></span></span> </em>and fixed <em><span class="MathJax" id="MathJax-Element-10-Frame"><span class="math" id="MathJax-Span-66"><span class="mrow" id="MathJax-Span-67"><span class="mi" id="MathJax-Span-68">N</span></span></span></span></em>. This is the first problem considered in the multiplayer Number On the Forehead (NOF) model. Despite the basic nature of this problem, no progress has been made on it throughout the years. Only recently have explicit protocols been found for the first time, yet no improvement in complexity has been achieved to date. The present paper offers the first improved protocol for the exactly-<em><span class="MathJax" id="MathJax-Element-11-Frame"><span class="math" id="MathJax-Span-69"><span class="mrow" id="MathJax-Span-70"><span class="mi" id="MathJax-Span-71">N</span></span></span></span></em> problem. This is also the first significant example where algorithmic ideas in communication complexity bear fruit in additive combinatorics.</span></p>
<p>This is remarkable for various reasons. On the additive combinatorics side, improved constructions are rare. For example, <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/">we reported</a> here in 2008 Elkin’s (small) improvements of Behrend’s bound. For the corner-free problem the paper of Nati and Adi goes beyond the Behrend’s (and Elkin’s) constructions. On the communication complexity side this is significant progress on a classical 1983 problem of Chandra, Furst and Lipton. The connection that goes from improved result on communication complexity to additive combinatorics is exciting — certainly a <a href="https://gilkalai.wordpress.com/2013/11/01/natifest-is-coming/">new frontier for Nati and Adi</a>. On the blogging side, I cannot compete with the beautifully written introduction. <a href="https://arxiv.org/abs/2102.00421">Click here to read the paper</a>.</p>
<p><strong>Remark 1:</strong> The number of the forehead problem is  related to Levine’s hat problem that we discussed in <a href="https://gilkalai.wordpress.com/2020/12/12/open-problem-session-of-huji-combsem-problem-3-ehud-friedgut-independent-sets-and-lionel-levins-infamous-hat-problem/">this post</a>.</p>
<p><strong>Remark 2: </strong>Ryan Alweiss just told me about Ben Green’s new paper <a href="https://arxiv.org/abs/2102.01543">New lower bounds for van der Waerden numbers.</a> It gives a construction of a red blue colouring of {1,2,…,N} with no 3 term blue or a k-term red arithmetic progression, where N is super-polynomial! Stay tune for a fuller report.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/02/03/to-cheer-you-up-in-difficult-times-19-nati-linial-and-adi-shraibman-construct-larger-corner-free-sets-from-better-numbers-on-the-forehead-protocols/"><span class="datestr">at February 03, 2021 11:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-1310669999043503597">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2021/02/forc-2021-call-for-papers.html">FORC 2021 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Reminder to anyone who has forgotten about FORC 2021 --- its a very nice venue --- and also a nice place to highlight recent work that is published or submitted elsewhere, via the non-archival track.</p><p><br /></p><p><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Symposium on Foundations of Responsible Computing (FORC) 2021 Call for Papers - Deadline February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The second annual Symposium on Foundations of Responsible Computing (FORC) is planned to be held on June 9-11, 2021, *online*. FORC is a forum for mathematically rigorous research in computation and society writ large.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics, and other relevant analytical fields to problems of pressing and anticipated societal concern.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Topics that fall in scope include, but are not restricted to, formal approaches to privacy, including differential privacy; theoretical approaches to fairness in machine learning, including the investigation of definitions, algorithms and lower bounds, tradeoffs, and economic incentives; computational and mathematical social choice (including apportionment and redistricting); theoretical foundations of sustainability; mechanism design for social good; mathematical approaches to bridging computer science, law and ethics; and theory related to modeling and mitigating the spread of epidemics. The Program Committee also warmly welcomes mathematically rigorous work on societal problems that have not traditionally received attention in the theoretical computer science literature. Whatever the topic, submitted papers should communicate their contributions towards responsible computing, broadly construed.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">The symposium itself will feature a mixture of talks by authors of accepted papers and invited talks. At least one author of each accepted paper should be present at the symposium to present the work (with an option for virtual attendance, as needed).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Dual Submission Policy. Authors must indicate at the time of submission whether they are submitting to the archival-option track or the non-archival track.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the non-archival track, it is permitted to submit papers that have appeared in a peer-reviewed conference or journal since the last FORC. It is also permitted to simultaneously or subsequently submit substantially similar work to another conference or to a journal. Accepted papers in the non-archival track will receive talks at the symposium and will appear as one-page abstracts on the symposium website. They will not appear in the proceedings.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* For submissions to the archival-option track, papers that are substantially similar to papers that have been previously published, accepted for publication, or submitted in parallel to other peer-reviewed conferences with proceedings may not be submitted. Also, submissions that are substantially similar to papers that are already published in a journal at the time of submission may not be submitted to the archival-option track. Accepted papers in the archival-option track will receive talks at the symposium. Authors of papers accepted to the archival-option track will be given the option to choose whether to convert to a one-page abstract (which will not appear in the proceedings) or publish a 10-page version of their paper in the proceedings. The proceedings of FORC 2021 will be published by LIPIcs.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Authors are also responsible for ensuring that submitting to FORC would not be in violation of other journals’ or conferences’ submission policies.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">PC members and reviewers will be aware during the review process of whether papers have been submitted as archival-option or non-archival. The PC reserves the right to hold non-archival papers to a different standard than archival-option papers.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission Instructions.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors should upload a PDF of the paper here: </span><a style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" href="https://easychair.org/conferences/?conf=forc2021" target="_blank">https://easychair.org/conferences/?conf=forc2021</a><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* A footnote on the title of the paper should indicate whether the paper is a submission to the archival-option track or the non-archival track. Submissions to the non-archival track should also indicate in this footnote any archival venues (conferences or journals) at which the paper has appeared, a link to the publication, and the date on which it was published.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* The font size should be at least 11 point and the format should be single-column.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Author names and affiliations should appear at the top of the paper (reviewing for FORC is single, not double blind).</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Beyond these, there are no formatting or length requirements, but reviewers will only be asked to read the first 10 pages of the submission. It is the authors’ responsibility that the main results of the paper and their significance be clearly stated within the first 10 pages. For both the archival-option track and the non-archival track, submissions should include proofs of all central claims, and the committee will put a premium on writing that conveys clearly and in the simplest possible way what the paper is accomplishing.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">* Authors are free to post their submissions on arXiv or other online repositories.</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">All questions about submissions should be emailed to the PC chair, Katrina Ligett, at </span><a style="background-color: white; color: #1155cc; font-family: Arial, Helvetica, sans-serif; font-size: small;" href="mailto:katrina@cs.huji.ac.il" target="_blank">katrina@cs.huji.ac.il</a><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC Steering Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Avrim Blum</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Cynthia Dwork      </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Shafi Goldwasser  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Sampath Kannan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jon Kleinberg</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Toni Pitassi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Omer Reingold</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Guy Rothblum  </span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salvatore Ruggieri</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adrian Weller</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">FORC 2021 Program Committee</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Borja Balle</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Raef Bassily</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Mark Bun</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Elisa Celis</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Aloni Cohen</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Moon Duchin</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Vitaly Feldman</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kira Goldner</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Krishna Gummadi</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Swati Gupta</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Gautam Kamath</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Michael Kearns</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Scott Kominers</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Himabindu Lakkaraju</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Katrina Ligett (chair)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Jamie Morgenstern</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Seth Neel</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kobbi Nissim</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Adam Smith</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Kunal Talwar</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Salil Vadhan</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Important Dates</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Submission deadline: February 15, 2021 AOE (anywhere on Earth)</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Author notification: March 31, 2021</span><br style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;" /><span style="background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small;">Conference: June 9-11, 2021</span></p></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2021/02/forc-2021-call-for-papers.html"><span class="datestr">at February 03, 2021 02:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/">Postdoc at PUC Chile &amp; Millennium Institute for Foundational Research on Data (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IMFD Chile, <a href="http://www.imfd.cl" rel="nofollow">http://www.imfd.cl</a> offers an open position for a postdoc to advance the understanding of theoretical aspects of neural networks.<br />
IMFD is a joint initiative held by several universities in Chile. It is a vibrant and truly interdisciplinary environment, which gathers together over 40 researchers and more than 100 students working on theoretical and applied aspects of data science.</p>
<p>Website: <a href="https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing">https://docs.google.com/document/d/1PyHp-MRAPWg_0aeinpDGmzJGZbwMtsqC6BqE_4T3KFc/edit?usp=sharing</a><br />
Email: pbarcelo@uc.cl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/03/postdoc-at-puc-chile-millennium-institute-for-foundational-research-on-data-apply-by-march-31-2021/"><span class="datestr">at February 03, 2021 01:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://kamathematics.wordpress.com/?p=212">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kamath.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/">Learning Theory Alliance and Mentoring Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://www.cs.utexas.edu/~surbhi/">Surbhi Goel</a>, <a href="https://people.eecs.berkeley.edu/~nika/">Nika Haghtalab</a>, and <a href="https://vitercik.github.io/">Ellen Vitercik</a> are the organizers of an excellent new initiative called the <a href="https://www.let-all.com/">Learning Theory Alliance</a>. They have the following inspiring mission statement:</p>



<p><em>Our mission is to develop a strong, supportive learning theory community and ensure its healthy growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers.</em></p>



<p>Their first event is a mentoring workshop, to be held at ALT 2021. I’ll be helping out by mentoring the creation of some written ALT highlights. Read on for more details from the organizers.</p>



<hr class="wp-block-separator" />



<p>We are pleased to announce the first<strong> <a href="https://www.let-all.com/alt.html">Learning Theory Mentorship Workshop</a></strong> in collaboration with the <a href="http://algorithmiclearningtheory.org/alt2021/">Conference on Algorithmic Learning Theory (ALT) 2021</a> to be held virtually on <strong>March 4-5, 2021</strong>. The workshop will focus on building technical and networking skills while giving participants an opportunity to interact with fellow researchers in the field. </p>



<p>The workshop is intended for upper-level undergraduate and all-level graduate students as well as postdoctoral researchers who are excited about the possibility of learning theory research. No prior research experience in the field is expected.</p>



<p>We have several planned events including:</p>



<ul><li><strong>How-to talks </strong>which will provide general advice about giving talks, structuring papers, writing reviews, networking, and attending conferences.</li><li>A small group discussion <strong>dissecting a short talk</strong> with feedback from a senior researcher.</li><li>An informal and interactive<strong> “Ask Me Anything” </strong>sessionwith a senior member of the learning theory community.</li><li><strong>General audience talks</strong> about recent learning theory research which will be accessible to new researchers.</li><li><strong>Social events </strong>such as board games.</li></ul>



<p>Our lineup includes Jacob Abernethy, Kamalika Chaudhuri, Nadav Cohen, Rafael Frongillo, Shafi Goldwasser, Zhiyi Huang, Robert Kleinberg, Pravesh Kothari, Po-Ling Loh, Lester Mackey, Jamie Morgenstern, Praneeth Netrapalli, Vatsal Sharan and Mary Wootters.</p>



<p>Together with Gautam Kamath, we will also organize a written account of ALT titled <strong>“ALT Highlights”</strong> which will summarize the research presented at ALT. We will assist students and postdocs to set up interviews with presenters and keynote speakers as part of the highlights.</p>



<p>A short application<a href="https://forms.gle/v8b8aeJMgWxJ1Bbx9" target="_blank" rel="noreferrer noopener"> form</a> is required to participate with an <strong>application deadline of Friday, Feb. 19, 2021</strong>. Students with backgrounds that are underrepresented or underserved in related fields are especially encouraged to apply. <strong>We will be accommodating all time zones.</strong> More information can be found on the event’s website:<a href="http://let-all.com/alt.html" target="_blank" rel="noreferrer noopener"> http://let-all.com/alt.html</a>.</p>



<p>This workshop is part of our broader community building initiative called the Learning Theory Alliance (advised by Peter Bartlett, Avrim Blum, Stefanie Jegelka, Po-Ling Loh and Jenn Wortman Vaughan). Check out <a href="http://let-all.com/" target="_blank" rel="noreferrer noopener">http://let-all.com/</a> for more details and to sign up to volunteer.</p>



<p>Best,<br />Surbhi Goel, Nika Haghtalab and Ellen Vitercik</p></div>







<p class="date">
by Gautam <a href="https://kamathematics.wordpress.com/2021/02/02/learning-theory-alliance-and-mentoring-workshop/"><span class="datestr">at February 02, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/009">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/009">TR21-009 |  One-way Functions and Partial MCSP | 

	Eric Allender, 

	Mahdi Cheraghchi, 

	Dimitrios Myrisiotis, 

	Harsha Tirumala, 

	Ilya Volkovich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
One-way functions (OWFs) are central objects of study in cryptography and computational complexity theory. In a seminal work, Liu and Pass (FOCS 2020) proved that the average-case hardness of computing time-bounded Kolmogorov complexity is equivalent to the existence of OWFs. It remained an open problem to establish such an equivalence for the average-case hardness of some NP-complete problem. In this paper, we make progress on this question by studying a polynomially-sparse variant of Partial Minimum Circuit Size Problem (Partial MCSP), which we call Sparse Partial MCSP, as follows.

1. First, we prove that if Sparse Partial MCSP is zero-error average-case hard on a polynomial fraction of its instances, then there exist OWFs.
2. Then, we observe that Sparse Partial MCSP is NP-complete under polynomial-time deterministic reductions. That is, there are NP-complete problems whose average-case hardness implies the existence of OWFs.
3. Finally, we prove that the existence of OWFs implies the nontrivial zero-error average-case hardness of Sparse Partial MCSP.

Thus the existence of OWFs is inextricably linked to the average-case hardness of this NP-complete problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/009"><span class="datestr">at February 01, 2021 09:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5433">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/self-concordant-analysis-newton/">Going beyond least-squares – I : self-concordant analysis of Newton method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Least-squares is a workhorse of optimization, machine learning, statistics, signal processing, and many other scientific fields. I find it particularly appealing (too much, according to some of my students and colleagues…), because all algorithms, such as stochastic gradient [<a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">1</a>], and analyses, such as for kernel ridge regression [<a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">2</a>], are much simpler and rely on reasonably simple linear algebra.</p>



<p class="justify-text">Despite the unique appeal of least-squares, most interesting optimization or machine problems go beyond quadratic functions. While there are many algorithms and analyses dedicated to more general situations, it is tempting to use least-squares for analysis or algorithms by considering the functions at hand to be approximately quadratic.</p>



<p class="justify-text">Using bounds on the third-order derivatives and <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansions</a> are the usual ways to go, but they are not ideal for sharp non-asymptotic results where the deviation from quadratic functions has to be precisely quantified. In many situations, more finer structures can be used. In a series of posts, I will describe <em>self-concordance</em> properties, that relate the third order derivatives to second order ones.</p>



<p class="justify-text">There are many great books that cover this topic where the material below is taken from [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>, <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">6</a>].</p>



<h2>Self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ You may wonder why the power \(3/2\) or why the constant \(2\). The constant is just a convention (multiplying the function \(f\) by \(c\) would replace \(2\) by \(2/\sqrt{c}\)), while the power \(3/2\) is fundamental, as it makes the definition “affine-invariant”, that is, if \(f\) is self-concordant, so is \(y \mapsto f(ay)\) for any \(a \in \mathbb{R}\).</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), this has to be true along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h,h]= \sum_{i,j,k=1}^d h_i h_j h_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the symmetric third-order tensor and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the second-order one, then $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h,h]| \leqslant 2 f^{\prime\prime}(x)^{3/2}[h,h].$$</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are self-concordant, then so is \(f+g\) (but not their average). Moreover, if \(f\) is self-concordant, so is \(y\mapsto f(Ay)\) for any matrix \(A\). The property is also preserved by Fenchel conjugation. Classical examples are all linear and quadratic functions, the negative logarithm, the negative log-determinant, or the negative logarithm of quadratic functions. The three previous examples are particularly important because they are “barrier functions”, with non-full domains, and are instrumental to interior-point methods (see below).</p>



<p class="justify-text"><strong>Properties in one dimension.</strong>  A nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( f^{\prime\prime}(x)^{-1/2} \big) \big| = \big| \frac{1}{2} f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-3/2} \big| \leqslant 1,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ – x \leqslant f^{\prime\prime}(x)^{-1/2} \, – f^{\prime\prime}(0)^{-1/2} \leqslant x,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3} \frac{f^{\prime\prime}(0)}{\big(1 + x f^{\prime\prime}(0)^{1/2}\big)^2} \leqslant f^{\prime\prime}(x) \leqslant \frac{f^{\prime\prime}(0)}{\big(1 – x f^{\prime\prime}(0)^{1/2}\big)^2}.$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$-f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1+x f^{\prime\prime}(0)^{1/2}} \leqslant f^\prime(x)-f^\prime(0) \leqslant -f^{\prime\prime}(0)^{1/2} + \frac{f^{\prime\prime}(0)^{1/2}}{1-x f^{\prime\prime}(0)^{1/2}},$$ and  $$ \tag{4} \rho \big( – f^{\prime\prime}(0)^{1/2} x \big) \leqslant f(x) \ – f(0) \ – f^\prime(0) x \leqslant   \rho \big( f^{\prime\prime}(0)^{1/2} x \big),$$ with \(\displaystyle \rho(u) =\  – \log(1-u) \ – u \sim \frac{u^2}{2} \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. This upper-bound is valid as long as \(\delta = f^{\prime\prime}(0)^{1/2} x \in [0,1]\), while the lower-bound on \(f\) is always true. The function \(\rho\) is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="303" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/rho.png" class="wp-image-5631" height="233" /></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(h \in \mathbb{R}^d\) such that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\), we have upper and lower bounds for the Hessian, the gradient and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm (see detailed proofs at the end of the post) $$\tag{5}(1-\delta)^2 f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq  \frac{1}{(1-\delta)^2}  f^{\prime \prime}(x),$$ $$ \tag{6}\big\| f^{\prime\prime}(x)^{-1/2} \big(f^\prime(x+\Delta)-f^\prime(x) -f^{\prime \prime}(x)\Delta \big) \big\|  \leqslant \frac{\delta^2}{1-\delta},$$ and $$\tag{7} \rho(-\delta) \leqslant f(x+\Delta)\ -f(x) \ – f^\prime(x)^\top \Delta \leqslant \rho(\delta).$$ A nice consequence is that if \(\delta &lt; 1\), then \(x+\Delta \in C\), that is, we get “for free” a feasible point. Moreover, these approximations are “second-order tight” at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact.</p>



<p class="justify-text"><strong>Dikin ellipsoid.</strong> The condition that \(\delta^2 = \Delta^\top f^{\prime\prime}(x) \Delta &lt; 1\) defines an ellipsoid around \(x\) which is always strictly inside the domain of \(f\) (see example in the plot below). The results above essentially state that when inside the Dikin ellipsoid, the locally quadratic approximation can be used. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="388" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/dykin.gif" class="wp-image-5642" height="307" />Dikin ellipsoids for the function \(f(x) = \sum_{i=1}^k \log (b_i – a_i^\top x)\), which is self-concordant with a domain which is a polytope.</figure></div>



<p class="justify-text">In this post, I will focus primarily on the use self-concordant functions in optimization through the analysis of Newton method.</p>



<h2>Why should you care about Newton method?</h2>



<ol class="justify-text"><li>For fun: Newton method is one of the classics of optimization!</li><li>For high precisions: as we will see, it is quadratically convergent and attains machine precision after solving a few linear systems.</li><li>Even in high dimensions where the linear system can be expensive, Newton method may still be the method of choice for severely ill-conditioned problems where even accelerated first-order methods are too slow to obtain low precision solutions.</li><li>It sometimes comes for free in situations where gradients are expensive to evaluate compared to \(d\).</li></ol>



<h2>Classical analysis of Newton method</h2>



<p class="justify-text">Given a function \(f: \mathbb{R}^d \to \mathbb{R}\), Newton method is an iterative optimization algorithm consisting in locally approximating the function \(f\) around the iterate \(x_{t}\) by a second-order Taylor expansion $$f(x_t) + f^\prime(x_t)^\top(x-x_t) + \frac{1}{2} (x-x_t)^\top f^{\prime \prime}(x_t) ( x – x_t),$$ whose minimum can be found in closed form as $$\tag{8} x_{t+1} = x_t \ – f^{\prime \prime}(x_t)^{-1} f^{\prime}(x_t).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="323" alt="" src="https://francisbach.com/wp-content/uploads/2021/02/approx_taylor.gif" class="wp-image-5705" height="275" />Quadratic approximation and Newton step (in green) for varying starting points (in red). When the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer.</figure></div>



<p class="justify-text">Newton method is classically analyzed for three times differentiable convex functions with bounded Hessians and third-order derivatives. The method is only locally convergent, that is, far away from the global minimizer \(x_\ast\) (even for very regular convex function), the method may diverge. In the non-convex setting, this leads to nice <a href="https://en.wikipedia.org/wiki/Newton_fractal">fractal plots</a>, but even for convex functions, the method can be unstable (see plot above).</p>



<p class="justify-text">Locally, it is quadratically convergent, that is, there exists \(c&gt;0\), such that if \(\| x_t \ – x_\ast\| \leqslant c\), then \(\| x_{t+1} \ – x_\ast \| /c  \leqslant \big( \|x_t\  – x_\ast\| / c \big)^2\). Roughly, the number of significant digits doubles at every iteration.</p>



<p class="justify-text">This leads to $$ \| x_{t} \ – x_\ast \| \leqslant c \big( \|x_{t_0} \ – x_\ast\| / c \big)^{2^{t-t_0}} ,$$ for \(t \geqslant t_0\) and \(t_0\) an index for which \(\| x_{t_0} \ – x_\ast\| \leqslant c\). It is less than \(\varepsilon\), as soon as \(2^{t-t_0} \log ( c / \| x_{t_0} – x_\ast \| ) \geqslant \log ( c / \varepsilon)\), that is, $$ t \geqslant t_0 + \frac{ \log \log ( c / \varepsilon)}{ \log 2} \  –  \frac{\log \log ( c / \| x_{t_0} – x_\ast \| )}{\log 2}.$$</p>



<p class="justify-text">That is, once we enter the quadratic phase, we obtain a number of iterations in \( \log \log ( 1/ \varepsilon)\), that is, only very few iterations. For example, for \(\varepsilon = 10^{-16}\), \(\log \log ( 1/ \varepsilon) \leqslant 4\).</p>



<p class="justify-text">Two major issues may be solved elegantly using self-concordant analysis: </p>



<ol class="justify-text"><li>Dealing with the two phases of Newton method, the quadratically convergent final phase, as well as the initial phase.</li><li>Obtaining convergence rates which are affine-invariant, that is, minimizing \(f(x)\) of \(f(Ax+b)\) for \(A\) an invertible matrix should lead to exactly the same convergence rate (this is not the case for the classical analysis, where for example the constant \(c\) depends on non affine-invariant quantities).</li></ol>



<h2>Self-concordant analysis of Newton method</h2>



<p class="justify-text">Consider \(f: \mathcal{C} \to \mathbb{R}\) which is self-concordant. Since Newton method in Eq. (8) is not globally convergent, we need to study a version where the Newton step is performed partially. There are several possible strategies. Here I present the so-called “damped Newton” iteration and thus study the iteration $$ x^+ = x\  – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x),$$ where we define the “Newton decrement” \(\lambda(x)\) at \(x \in C\), as $$ \lambda^2 = \lambda(x)^2 =f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x) = \|f^{\prime \prime}(x)^{-1/2} f^\prime(x) \|^2.$$</p>



<p class="justify-text">The Newton decrement is a key quantity in the analysis of Newton method, as \(\frac{1}{2} \lambda(x)^2\) is exactly the decrease in the quadratic approximation obtained by a full Newton step. Moreover, </p>



<ol class="justify-text"><li>If \(\lambda(x) &lt; 1\), then \(x^+\) is in the Dikin ellipsoid where we can expect the local quadratic approximation to be relevant.</li><li>If \(\lambda(x) &lt; 1\), then one can show that \(f(x) \ – f(x_\ast) \leqslant \rho( \lambda(x)) \sim \frac{1}{2} \lambda(x)^2\) when \(\lambda(x)\) is close to zero, that is, the Newton decrement provides an upper bound on the distance to optimum.</li></ol>



<p class="justify-text">The update corresponds to \(\Delta = \ – \frac{1}{1+\lambda(x)} f^{\prime \prime}(x)^{-1} f^\prime(x)\), and \(\delta  = \frac{\lambda(x)}{1+\lambda(x)}  \in [0,1]\), thus \(x^+\) is automatically feasible (which is important for constrained case, see below).</p>



<p class="justify-text">Moreover, using Eq. (7), we get $$f(x^+)-f(x) \leqslant \ – \frac{f^\prime(x)^\top f^{\prime \prime}(x)^{-1} f^\prime(x)}{1+\lambda(x)} + \rho \Big( \frac{\lambda(x)}{1+\lambda(x)} \Big) = \log (1+ \lambda(x)) \ – \lambda(x).$$ This immediately leads to a fixed decrease of \(\frac{1}{4} \, – \log \frac{5}{4} \geqslant 0.0268\) if \(\lambda(x) \geqslant \frac{1}{4}\). </p>



<p class="justify-text">We can now compute the Newton decrement at \(x^+\), to see how it decreases, by first bounding $$\lambda(x^+) =\|f^{\prime \prime}(x^+)^{-1/2} f^\prime(x^+) \| \leqslant \frac{1}{1-\delta} \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \|,$$ using Eq. (5). We then have, using Eq. (6): $$ \|f^{\prime \prime}(x)^{-1/2} f^\prime(x^+) \| \leqslant \big\| f^{\prime \prime}(x)^{-1/2} \big( f^\prime(x) + f^{\prime\prime}(x) \Delta  \big) \big\| + \frac{\delta^2}{ 1-\delta } \leqslant \frac{\lambda(x)^2}{1+\lambda(x)} + \frac{\delta^2}{ 1-\delta  }.$$ This exactly leads to $$\lambda(x^+) \leqslant 2  \lambda(x)^2, $$ which leads to quadratic convergence if \(\lambda(x)\) is small enough.</p>



<p class="justify-text">We can then divide the analysis in two phases: before \(\lambda(x) \leqslant 1/4\) and after. The first integer \(t_0\) such that \(\lambda(x) \leqslant 1/4\) is less than \(\frac{ f(x_0) – f(x_\ast)}{0.0268} \leqslant 38 [ f(x_0) – f(x_\ast) ]\). Then, for the second phase, \(2\lambda(x_t) \leqslant (1/2)^{2^{t-t_0}}\). Given that for \(\lambda \leqslant 1/4\), \(\rho(\lambda) \leqslant 2\lambda\), we reach precision \(\varepsilon\) as soon as \(2^{t-t_0} \log 2 \geqslant \log \frac{1}{\varepsilon}\), that is, \(t \geqslant t_0 + \frac{1}{\log 2} \log \log \frac{1}{\varepsilon} -1\). This leads to number of iterations to reach a precision \(\varepsilon\)  which is less than $$38[ f(x_0) \ – f(x_\ast)]  +2 \log \log \frac{1}{\varepsilon}.$$</p>



<h2>Interior point methods</h2>



<p class="justify-text">Self-concordant functions are also key in the analysis of interior point methods. Consider a function \(f\) defined on \(\mathbb{R}^d\) and the constrained optimization problem $$\min_{ x \in C} f(x),$$ where \(C\) is a convex set. Barrier methods are appending a so-called “barrier function” \(g(x)\) to the objective function. A function \(g\) is a barrier function if \(g\) is convex and with domain containing the relative interior of \(C\), with gradients that explode when reaching the boundary of \(C\). We then solve instead $$\tag{9} \min_{x \in \mathbb{R}^d}  \varepsilon^{-1} f(x) +  g(x), $$ where \(\varepsilon &gt; 0\). Typically, the minimizer \(x_\varepsilon\) is in the relative interior of \(C\) (hence the name interior point method), and, when \(\varepsilon\) tends to zero, \(x_\varepsilon\) tends to the minimizer of \(f\) on \(C\).</p>



<p class="justify-text">When both the original function \(f\) and the barrier function \(g\) are self-concordant, the (damped) Newton method is particularly useful as it ensures feasibility of the iterates. Moreover, the interplay between the progressive reduction of \(\varepsilon\) towards zero and the approximate resolution of Eq. (9) can be completely characterized (see [<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">3</a>, <a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">4</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">5</a>]). This applies directly to linear programming, second-order cone programming and semidefinite programming.</p>



<h2>Applications in machine learning</h2>



<p class="justify-text">If you have reached this point, you are probably a big fan of self-concordance. While this property is crucial in optimization, is it really relevant for machine learning or statistics? The sad truth is that most of the non-quadratic functions within machine learning are <em>not</em> self-concordant in the sense of Eq. (1) or Eq. (2). In particular, log-sum-exp functions, such that the logistic loss \(f(t) = \log( 1 + \exp(-t) )\),  do satisfy a relationship between third and second-order derivatives, but of the form $$| f^{\prime \prime \prime}(t)| \leqslant f^{\prime \prime }(t),$$ without the power \(3/2\). This seemingly small difference leads to several variations [7] which will the topic of next month blog post. Meanwhile, it is worth mentioning two applications in machine learning of classical self-concordance.</p>



<p class="justify-text"><strong>Maximum likelihood estimation for covariance matrices.</strong> Beyond its use in interior point methods, self-concordant functions arise naturally when estimating the covariance matrix using maximum likelihood estimation with a Gaussian model. Indeed, the negative log-likelihood can be written as $$ – \log p (x| \mu ,\Sigma) =\frac{d}{2} \log(2\pi) +  \frac{1}{2} \log \det \Sigma + \frac{1}{2} ( x -\mu)^\top \Sigma^{-1} ( x – \mu),$$  which leads to a negative log-determinant of the inverse \(\Sigma^{-1}\) of the covariance matrix \(\Sigma\), which is a self-concordant function, on which the guarantees discussed above apply.</p>



<p class="justify-text"><strong>Self-concordant losses.</strong> One can also design losses which are self-concordant. For example, a self-concordant version of the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> is $$f(t) = \sqrt{1+t^2} \ – 1 \ –  \log \frac{ \sqrt{1+t^2} +1 }{2} .$$ It can be seen as the Fenchel-conjugate of \(– \log(1-t^2)\), and the proximity with a quadratic problem can be leveraged to obtain generalization performances using this loss function which are essentially the same as for the square loss. It can also be used for binary classification (see [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">8</a>] for details, and the two nice <a href="https://ostrodmit.github.io/blog/2018/11/12/self-concordance-part-1/">blog posts</a> of Dmitrii Ostrovskii).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="359" alt="" src="https://francisbach.com/wp-content/uploads/2021/01/loss_selfc.png" class="wp-image-5654" height="251" /></figure></div>



<p class="justify-text"><strong>One-step-estimation.</strong> Given the focus of this post on Newton method, I cannot resist mentioning a great technique coming from statistics that relies on a single Newton step. We consider a classical empirical risk minimization problem (statisticians would call it an M-estimation problem), with empirical risk \(\displaystyle \widehat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i) )\). Given an estimator \(\hat{\theta}\), obtained by any means, then if \(\hat{\theta}\) is \(\frac{1}{\sqrt{n}}\) away from the optimal parameter (with optimal performance on unseen data), then one Newton step on the function \(\widehat{R}\) started from \(\hat{\theta}\) will lead to an estimator achieving asymptotically the usual <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao</a> lower bound. In a nutshell, a single Newton step on the empirical risk transforms a good estimator into a very good estimator. See [9, Section 5.7] for more details.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Adrien Taylor and Dmitrii Ostrovskii for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] F. Bach and E. Moulines. <a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate</a> \(O(1/n)\). Advances in Neural Information Processing Systems (NIPS), 2013.<br />[2] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/article/10.1007/s10208-006-0196-8">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7(3):331-368, 2007.<br />[3] Yurii Nesterov, and Arkadii Nemirovskii. <em><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791">Interior</a><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611970791.bm">-Point Polynomial Algorithms in Convex Programming</a></em>, SIAM, 1994.<br />[4] Arkadii Nemirovski. <em><a href="https://www2.isye.gatech.edu/~nemirovs/LecIPM.pdf">Interior Point Polynomial Time Methods in Convex Programming</a></em>. Lecture notes, 1996.<br />[5] Yurii Nesterov. <em><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-91578-4.pdf">Lectures on Convex Optimization</a></em> (Vol. 137). Springer, 2018.<br />[6] Stephen P. Boyd, and Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br />[7] Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1271941980">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010<br />[8] Dmitrii Ostrovskii, and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1609902192">Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br />[9] Aad W. Van der Vaart. <em><a href="http://Aad W. Van der Vaart. Asymptotic Statistics">Asymptotic Statistics</a></em>, volume 3. Cambridge University Press, 2000.</p>



<h2>Detailed proofs for self-concordance properties</h2>



<p class="justify-text">We first show Eq. (7), by considering the function \(a(t) = f(x+t\Delta)\), which is a one-dimensional self-concordant function, for which \(a^\prime(t) = \Delta^\top f^\prime(x+t\Delta)\), and \(a^{\prime\prime}(t) = \Delta^\top f^{\prime\prime}(x+t\Delta) \Delta\). Then \(a^{\prime\prime}(0) = \delta^2\), and Eq. (4) for \(x=1\) and \(a\) exactly leads to Eq. (7).</p>



<p class="justify-text">In order to show Eq. (5), we consider \(h \in \mathbb{R}^d\), and the function \(b(t) = h^\top f^{\prime\prime}(x+t\Delta) h\). We have, \(b'(t) = f^{\prime\prime\prime}(x+t\Delta)[h,h,\Delta]\), which can be bounded using Eq. (2) as $$ |b'(t) | \leqslant 2 f^{\prime\prime}(x+t\Delta)[h,h] f^{\prime\prime}(x+t\Delta)[\Delta,\Delta]^{1/2} = 2 b(t) a^{\prime \prime}(t)^{1/2} \leqslant 2 b(t) \frac{\delta}{1-t\delta}, $$ using Eq. (3). This implies that for \(\delta t \in [0,1)\): $$\frac{d}{dt} \big[ (1-\delta t)^2 b(t) \big] = -2\delta (1-\delta t) b(t) + (1-\delta t)^2 b'(t) \leqslant 0, $$  which implies \(b(t) \leqslant \frac{b(0)}{(1-\delta t)^2} = \frac{h^\top f^{\prime\prime}(x) h}{(1-\delta t)^2}\), which leads to the right-hand side of Eq. (5) since this is true for all \(h \in \mathbb{R}^d\). The left-hand side is proved similarly.</p>



<p class="justify-text">In order to show Eq. (6), we consider the function \(g(t) = h^\top f^\prime(x+t\Delta)\), for which, \(g^\prime(t) = h^\top f^{\prime\prime}(x+t\Delta) \Delta\), and \(g^{\prime\prime}(t) =   f^{\prime\prime\prime}(x+t\Delta) [h,\Delta,\Delta]\), which satisfies: $$|g^{\prime\prime}(t)| \leqslant 2 f^{\prime\prime}(x+t\Delta)[\Delta,\Delta] f^{\prime\prime}(x+t\Delta)[h,h]^{1/2} \leqslant 2 b(t)^{1/2} a^{\prime \prime}(t). $$ This leads to $$ |g^{\prime\prime}(t)| \leqslant 2 \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{(1-\delta t)^3}.$$ We can then integrate twice, using \(g(0) = h^\top f^\prime(x)\) and \(g^\prime(0) = h^\top f^{\prime\prime}(x) \Delta\), to get: $$h^\top \big(f^\prime(x+t\Delta)-f^\prime(x) \ – f^{\prime \prime}(x)\Delta \big) \leqslant    \big( h^\top f^{\prime\prime}(x) h\big)^{1/2} \frac{\delta^2}{1-\delta},$$ which leads to Eq. (5) after maximizing with respect to \(h\).</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/self-concordant-analysis-newton/"><span class="datestr">at February 01, 2021 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6639458808942063282">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/two-phd-positions-at-department-of.html">Two PhD positions at the Department of Computer Science, Reykjavik University: Model-driven SE for blockchain and smart contracts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">The<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Software and Emerging Technology Lab at the Department of Computer Science,</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">Reykjavik University</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">, is looking for two PhD candidates to work on an ongoing research project on the application of Model Driven Software Engineering principles, methodologies, technologies and abstractions to Blockchain and Smart Contracts. While both positions require strong software development skills and familiarity with the<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">model-driven software engineering</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span>approach, the first position will focus on domain analysis and code generation, while the second position is concerned with contract safety and validity and requires knowledge in model<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">verification</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"> and validation. The project is based in Iceland. It will be directed by Mohammad Hamdaqa in collaboration with Luca Aceto and Gísli Hjálmtýsson and in close collaboration with Polytechnique Montréal in Canada. The positions</span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;"><span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">are fully funded and<span> </span></span></span><span style="font-size: x-small;"><span style="font-size: 9pt;">include full tuition waiver and a salary in accordance with the Icelandic<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">Research Fund guidelines. Particularly the funding is covering full tuition as well as a stipend of 383,000 ISK</span></span><span style="font-size: x-small;"><span style="font-size: 9pt;"><span> </span>per month before taxes for a minimum of three years. </span></span></span></div><div align="justify"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">If you are interested to apply, please send the documents below to<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">the following email addresses:<span> </span></span></span><a href="mailto:mhamdaqa@ru.is" target="_blank" rel="noopener noreferrer"><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">mhamdaqa@ru.is</span></span></a><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">; </span></span><a href="mailto:luca@ru.is" target="_blank" rel="noopener noreferrer">luca@ru.is</a>; <a href="mailto:gisli@ru.is" target="_blank" rel="noopener noreferrer">gisli@ru.is</a></span></div><ul><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of your CV and research interests</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A copy of all your transcripts</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A sample publication</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">A maximum of one page research statement of your plans for research in your PhD.</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">Your intended starting date / and if you need a visa</span></span></span></li><li style="background-color: white;"><span style="background-color: white;"><span style="font-size: x-small;"><span style="font-size: 9pt;">For more information about the position and the research topics, do not hesitate to send your enquiries<span> </span></span></span><span style="font-size: x-small;"><span lang="en-CA" style="font-size: 9pt;">to any of the project collaborators.</span></span></span></li></ul><div><span style="background-color: white;">Mohammad Hamdaqa  (<a href="https://en.ru.is/cress/" target="_blank" rel="noopener noreferrer">https://en.ru.is/cress/</a>)<br /><a href="mailto:mhamdaqa@ru.is" target="_blank" rel="noopener noreferrer">mhamdaqa@ru.is</a></span></div><div><span style="background-color: white;">Luca Aceto (<a href="http://icetcs.ru.is/" target="_blank" rel="noopener noreferrer">http://icetcs.ru.is/</a>)<br /><a href="mailto:luca@ru.is" target="_blank" rel="noopener noreferrer">email: luca@ru.is</a><br /><br />Gísli Hjálmtýsson (https://en.ru.is/fintech/)<br />email: gisli@ru.is</span></div><div><br /></div><div>Informal inquiries about the project and the conditions of work are very welcome. We will start reviewing applications as soon as they arrive and will continue to accept applications until each position is filled. We strongly encourage interested applicants to send their applications as soon as possible and no later than 28 February 2021.</div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/two-phd-positions-at-department-of.html"><span class="datestr">at February 01, 2021 09:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/">Postdoc at Georgia Institute of Technology (apply by February 25, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms and Randomness Center (ARC) at Georgia Tech is recruiting postdocs for a 1-year position with the possibility of extension by another year starting August 1, 2021. Area of expertise include algorithms, discrete mathematics, optimization, theoretical machine learning. Candidates with a PhD in Computer Science, Math, Operations Research are encouraged to apply by February 25.</p>
<p>Website: <a href="http://arc.gatech.edu/postdoc21">http://arc.gatech.edu/postdoc21</a><br />
Email: lisa.cox@isye.gatech.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/01/postdoc-at-georgia-institute-of-technology-apply-by-february-25-2021/"><span class="datestr">at February 01, 2021 12:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8809394001786387491">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html">Grading policies during Covid-No easy answers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Because of COVID  (my spellecheck says covid and Covid are not works, but COVID is) various schools have done various things to make school less traumatic. Students already have problems, either getting COVID or having their friends got family get it (I've had four relatives get it, and one died) . Some do not adjust to learning online.  Some do not have good computer connection to learn on line. So what is a good policy? Here are some things I have either seen schools do or heard that they might do.</p><p><br /></p><p>1) Be more generous with Tuition-Refunds if a student has to withdraw. </p><p>2) Be more generous with Housing-Refunds if a students comes to campus thinking it will be courses on campus and there are no courses on campus. Or if a student has to withdraw. </p><p>3) Make the deadlines for dropping-without-a-W, or taking-it-pass-fail, later in the semester. </p><p>4) Tell the teachers to `just teach them the bare min they need for the next course.'</p><p>5) Allow students to take courses P/F in their major and still allow them to count, so a student might get a D in Discrete Math and be able to go on in the major. </p><p>6) How far to extend deadlines? How is this: extend deadline to make it P/F until the last day of classes (but before the final) and then after the final is given, the school changes its mind and says - OH, you can change to P/F now if you want to.</p><p>7) Allow either an absolute number (say 7) or a fraction (say 1/3) of the courses to be changed to P/F by the last day of class.</p><p>8) Combine 6 or 7 with saying NO- a D is an F for a P/F course. Perhaps only if its in the major, but that maybe hard to work out. since majors can change. Some schools do A-B-C-NO CREDIT, where the NC grade does not go into the GPA.</p><p>9) Give standard letter grades and tell the students to tough it out. Recall the following inspirational quotes</p><p>When the going gets tough, the tough go shopping</p><p>When the going gets tough, the tough take a nap</p><p>If at first you don't succeed, quit. Why make a damn fool of yourself. </p><p>If at first you don't succeed, then skydiving is not for you. </p><p>10) Decide later in the term what to do depending on who yells the loudest. </p><p>11) Any combination of the above that makes sense, and even some that don't. </p><p><br /></p><p>On the one hand, there are students who are going through very hard times because of covid and should be given a break. On the other hand, we want to give people a good education and give grades that are meaningful (the logic of how to give grades in normal times is another issue for another blog post). </p><p>What is your school doing? Is it working? What does it mean to be working?</p><p>The problems I am talking about are first-world problems or even champagne-problems. I know there are people who have far worse problems then getting a bad grade or dropping courses.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/01/grading-policies-during-covid-no-easy.html"><span class="datestr">at January 31, 2021 09:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7972">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through classical statistical learning theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">ML theory with bad drawings</a> <strong>Next post:</strong> TBD,  see also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5c6a9e86-bca7-42df-a04a-acc200ed2c2d">Lecture video</a> (starts in slide 2 since I hit record button 30 seconds too late – sorry!)</p>



<p>These are rough notes for the first lecture in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">my advanced topics in machine learning seminar</a>. See the <a href="https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/">previous post</a> for the introduction.</p>



<p>This lecture’s focus was on <strong>“classical” learing theory</strong>. The distinction between “classical learning” and “deep learning” is semantic/philosophical, and doesn’t matter much for this seminar. I personally view this difference as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/dzDbio7.png" alt="" /></figure>



<p>That is, deep learning is a framework that allows you to translate more resources (data and computation) into bettter performance. “Classical” methods often have a “threshold effect” where a certain amount of data and computation is needed, and more would not really help. For example, in parametric methods there will typically be a sharp threshold for the amount of data required for saturating the potential performance. Even in non-parametric models such as nearest neighbors or kernel methods, the computational cost is fixed for a fixed amount of data, and there is no way to profitably trade more computation for better performance.</p>



<p>In contrast, for deep learning, we often can get better performance using the same data by using bigger models or more computation. For example, I doubt this <a href="http://karpathy.github.io/2019/04/25/recipe/">story of Andrej Karpathy</a> could have happened with a non deep-learning method:</p>



<p><em>“One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).”</em></p>



<h2>Leaky pipelines</h2>



<p>We can view machine learning (deep or not) as a series of “leaky pipelines”:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/0dpRqYa.png" alt="" /></figure>



<p>We want to create an adaptive system that performs well in the wild, but to do so, we:</p>



<ol><li>Set up a benchmark of a test distribution, so we have some way to compare different systems.</li><li>We typically can’t optimize directly on the benchmark, both because losses like accuracy are not differentiable and because we don’t have access to an unbounded number of samples from the distribution. (Though there are exceptions, such as when optimizing for playing video games.) Hence we set up the task of optimizing some proxy loss function <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}" class="latex" title="\mathcal{L}" /> on some finite samples of training data.</li><li>We then run an optimization algorithm whose ostensible goal is to find the <img src="https://s0.wp.com/latex.php?latex=f+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \in \mathcal{F}" class="latex" title="f \in \mathcal{F}" /> that minimizes the loss function over the training data. (<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> is a set of models, sometimes known as <em>architecture</em>, and sometimes we also add other restrictions such norms of weights, which is known as <em>regularization</em>)</li></ol>



<p>All these steps are typically “leaky.” Test performance on benchmarks is not the same as real-world performance. Minimizing the loss over the training set is not the same as test performance. Moreover, we typically can’t solve the loss minimization task optimally, and there isn’t a unique minimizer, so the choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> depends on the algorithm.</p>



<p>Much of machine learning theory is about obtaining guarantees bounding the “leakiness” of the various steps. These are often easier to do in “classical” contexts of statistical learning theory than for deep learning. In this lecture, we will make a short blitz through classical learning theory. This material is covered in several sources, including the excellent book <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">understanding machine learning</a> and the upcoming Hardt-Recht text.</p>



<p>We will be very rough, using proofs by picture and making some simplifications (e.g., working in one dimension, assuming functions are always differentiable, etc.)</p>



<h2>Convexity</h2>



<p>A (nice) function <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R} \rightarrow \mathbb{R}" class="latex" title="f:\mathbb{R} \rightarrow \mathbb{R}" /> is (strongly) <em>convex</em> if it satisfies one of the following three equivalent conditions:</p>



<ol><li>For every two points <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,y" class="latex" title="x,y" />, the line between <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> and <img src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(y,f(y))" class="latex" title="(y,f(y))" /> is above the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</li><li>For every point <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, the tangent line at <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> with slope <img src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x)" class="latex" title="f'(x)" /> is below the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</li><li>For every <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x)&gt;0" class="latex" title="f''(x)&gt;0" />.</li></ol>



<figure class="wp-block-image"><img src="https://i.imgur.com/0p5cY35.png" alt="" /></figure>



<p>To see that for example, 2 implies 3, we can use the contrapositive. If 3 does not hold and <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is such that <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%3C+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x) &lt; 0" class="latex" title="f''(x) &lt; 0" /> (should really assume <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29+%5Cleq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x) \leq 0" class="latex" title="f''(x) \leq 0" /> but we’re being rough) then by Taylor, around <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> we get</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x+%2B+%5Cdelta%29+%3D+f%28x%29+%2B+%5Cdelta+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29+%2F2+%2B+O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)" class="latex" title="f(x + \delta) = f(x) + \delta f'(x) + \delta^2 f''(x) /2 + O(\delta^3)" /></p>



<p>For <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> small enough, <img src="https://s0.wp.com/latex.php?latex=O%28%5Cdelta%5E3%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(\delta^3)" class="latex" title="O(\delta^3)" /> is negligible and so we see that the curve of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> near <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> equals the tangent line <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%2B+%5Cdelta+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x) + \delta f'(x)" class="latex" title="f(x) + \delta f'(x)" /> plus a negative term, and hence it is below the line, contradicting 2.</p>



<p>To show that 2 implies 1, we can again use the contrapositive and show by a “proof by picture” that if there is some point in which <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is above the line between <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x))" class="latex" title="(x,f(x))" /> and <img src="https://s0.wp.com/latex.php?latex=%28y%2Cf%28y%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(y,f(y))" class="latex" title="(y,f(y))" />, then there must be a point <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in which the tangent line at <img src="https://s0.wp.com/latex.php?latex=%28z%2Cf%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(z,f(z))" class="latex" title="(z,f(z))" /> is above <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/84qFxfN.png" alt="" /></figure>



<p>Some tips on convexity:</p>



<ol><li>The function <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dx%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=x^2" class="latex" title="f(x)=x^2" /> is convex (proof: <a href="https://www.google.com/search?q=plot+x%5E2&amp;oq=plot+x%5E2">Google</a>)</li><li>If <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ek+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R}^k \rightarrow \mathbb{R}" class="latex" title="f:\mathbb{R}^k \rightarrow \mathbb{R}" /> is convex and <img src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L:\mathbb{R}^d \rightarrow \mathbb{R}^k" class="latex" title="L:\mathbb{R}^d \rightarrow \mathbb{R}^k" /> is linear then <img src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+f%28L%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \mapsto f(L(x))" class="latex" title="x \mapsto f(L(x))" /> is convex (lines are still lines). </li><li>If <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is convex and <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is convex then <img src="https://s0.wp.com/latex.php?latex=a%5Ccdot+f+%2B+b+%5Ccdot+g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a\cdot f + b \cdot g" class="latex" title="a\cdot f + b \cdot g" /> is convex for every positive <img src="https://s0.wp.com/latex.php?latex=a%2Cb&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a,b" class="latex" title="a,b" />.</li></ol>



<h2>Gradient descent</h2>



<p>The gradient descent algorithm minimizes a function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by starting at some point <img src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_0" class="latex" title="x_0" /> and repeating the following operation:</p>



<p><img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+%5Ccdot+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} = x_t - \eta \cdot f'(x_t)" class="latex" title="x_{t+1} = x_t - \eta \cdot f'(x_t)" /></p>



<p>for some small <img src="https://s0.wp.com/latex.php?latex=%5Ceta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta&gt;0" class="latex" title="\eta&gt;0" />.</p>



<p>By Taylor, <img src="https://s0.wp.com/latex.php?latex=f%28x%2B%5Cdelta%29+%5Capprox+f%28x%29+%2B+%5Cdelta+%5Ccdot+f%27%28x%29+%2B+%5Cdelta%5E2+f%27%27%28x%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2" class="latex" title="f(x+\delta) \approx f(x) + \delta \cdot f'(x) + \delta^2 f''(x)/2" />, and so setting <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%5Ccdot+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta = -\eta \cdot f'(x)" class="latex" title="\delta = -\eta \cdot f'(x)" />, we can see that</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]" class="latex" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right]" /></p>



<p>Since <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x_t%29%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x_t)&gt;0" class="latex" title="f''(x_t)&gt;0" />, we see that as long as <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+2%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta &lt; 2/ f''(x_t)" class="latex" title="\eta &lt; 2/ f''(x_t)" /> we make progress. If we set <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Csim+const+%2F+f%27%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta \sim const / f''(x_t)" class="latex" title="\eta \sim const / f''(x_t)" /> then we reduce in each step the value of the function by roughly <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2Ff%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2/f''(x)" class="latex" title="f'(x_t)^2/f''(x)" />.</p>



<p>In the high dimensional case, we replace <img src="https://s0.wp.com/latex.php?latex=f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x)" class="latex" title="f'(x)" /> with the gradient <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29+%3D+%28+df%28x%29%2Fdx_1+%2C+%5Cldots%2C+df%28x%29%2Fdx_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)" class="latex" title="\nabla f(x) = ( df(x)/dx_1 , \ldots, df(x)/dx_d)" /> and <img src="https://s0.wp.com/latex.php?latex=f%27%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f''(x)" class="latex" title="f''(x)" /> with the Hessian which is the matrix <img src="https://s0.wp.com/latex.php?latex=%28df%28x%29%2Fdx_i+dx_j%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(df(x)/dx_i dx_j)_{i,j}" class="latex" title="(df(x)/dx_i dx_j)_{i,j}" />. The progress we can make is controlled by the ratio of the smallest to largest eigenvalues of the Hessian, which is one over its _condition number_.</p>



<p>In <strong>stochastic gradient descent</strong>, instead of performing the step <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+x_t+-+%5Ceta+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} = x_t - \eta f'(x_t)" class="latex" title="x_{t+1} = x_t - \eta f'(x_t)" /> we use <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+-+%5Ceta+%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} - \eta \hat{f'}(x_t)" class="latex" title="x_{t+1} - \eta \hat{f'}(x_t)" />, where <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%27%7D%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{f'}(x_t)" class="latex" title="\hat{f'}(x_t)" /> is a random variable satisfying:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Chat%7Bf%27%7D%28x%29+%3D+f%27%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} \hat{f'}(x) = f'(x)" class="latex" title="\mathbb{E} \hat{f'}(x) = f'(x)" /></li><li><img src="https://s0.wp.com/latex.php?latex=Var+%5Chat%7Bf%27%7D%28x%29+%3D+%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Var \hat{f'}(x) = \sigma^2" class="latex" title="Var \hat{f'}(x) = \sigma^2" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma" class="latex" title="\sigma" />.</li></ul>



<p>Let’s define <img src="https://s0.wp.com/latex.php?latex=N_t+%3D+%5Chat%7Bf%27%7D%28x_t%29-+f%27%28x_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t = \hat{f'}(x_t)- f'(x_t)" class="latex" title="N_t = \hat{f'}(x_t)- f'(x_t)" />. Then <img src="https://s0.wp.com/latex.php?latex=N_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t" class="latex" title="N_t" /> is a mean zero and variance <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma^2" class="latex" title="\sigma^2" /> random variable, and let’s heuristically imagine that <img src="https://s0.wp.com/latex.php?latex=N_1%2CN_2%2C%5Cldots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_1,N_2,\ldots" class="latex" title="N_1,N_2,\ldots" /> are independent. If we plug in this into the Taylor approximation, then since <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+N_t+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} N_t = 0" class="latex" title="\mathbb{E} N_t = 0" />, only the terms with <img src="https://s0.wp.com/latex.php?latex=N_t%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_t^2" class="latex" title="N_t^2" /> survive.</p>



<p>So by plugging <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3D+-%5Ceta+%28f%27%28x%29+%2B+N_t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta = -\eta (f'(x) + N_t)" class="latex" title="\delta = -\eta (f'(x) + N_t)" /> to the Taylor approximation, we get that in expectation</p>



<p><img src="https://s0.wp.com/latex.php?latex=f%28x_%7Bt%2B1%7D%29+-+f%28x_t%29+%5Capprox+-%5Ceta+f%27%28x_t%29%5E2+%2B+%5Ceta%5E2+f%27%28x_t%29%5E2+f%27%27%28x_t%29%2F2+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2+%3D+-%5Ceta+f%27%28x_t%29%5E2+%5Cleft%5B+1+-+%5Ceta+f%27%27%28x_t%29%2F2+%5Cright%5D+%2B+%5Ceta%5E2+%5Csigma%5E2+f%27%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2" class="latex" title="f(x_{t+1}) - f(x_t) \approx -\eta f'(x_t)^2 + \eta^2 f'(x_t)^2 f''(x_t)/2 + \eta^2 \sigma^2 f''(x_t)^2 = -\eta f'(x_t)^2 \left[ 1 - \eta f''(x_t)/2 \right] + \eta^2 \sigma^2 f''(x_t)^2" /></p>



<p>We see that now to make progress, we need to ensure that <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> is sufficiently smaller than <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2%2F%28%5Csigma%5E2+f%27%27%28x_t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2/(\sigma^2 f''(x_t))" class="latex" title="f'(x_t)^2/(\sigma^2 f''(x_t))" />. We note that in the beginning, when <img src="https://s0.wp.com/latex.php?latex=f%27%28x_t%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f'(x_t)^2" class="latex" title="f'(x_t)^2" /> is large, we can use a larger learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" />, while when we get closer to the optimum, then we need to use a smaller learning rate.</p>



<h2>Generalization bounds</h2>



<p>The <em>supervised learning problem</em> is the task, given labeled training inputs <img src="https://s0.wp.com/latex.php?latex=S+%3D+%7B+%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S = { (x_1,y_1),\ldots,(x_n,y_n) }" class="latex" title="S = { (x_1,y_1),\ldots,(x_n,y_n) }" /> of obtaining a classifier/regressor <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that will satisfy <img src="https://s0.wp.com/latex.php?latex=f%28x%29+%5Capprox+y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x) \approx y" class="latex" title="f(x) \approx y" /> for future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y)" class="latex" title="(x,y)" /> from the same distribution.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZVTtBmW.png" alt="" /></figure>



<p>Let’s assume that our goal is to minimize some quantity <img src="https://s0.wp.com/latex.php?latex=LOSS%28f%29%3D+%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))" class="latex" title="LOSS(f)= \mathbb{E}{x,y} \mathcal{L}(y,f(x))" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}" class="latex" title="\mathcal{L}" /> is a <em>loss function</em> (that we will normalize to <img src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[0,1]" class="latex" title="[0,1]" /> for convenience). We call the quantity <img src="https://s0.wp.com/latex.php?latex=LOSS&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="LOSS" class="latex" title="LOSS" /> the population loss (and abuse notation by denoting it as <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f)" class="latex" title="\mathcal{L}(f)" />) and the corresponding quantity over the training set <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D+%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))" class="latex" title="\hat{\mathcal{L}}_S(f) = \tfrac{1}{n}\sum_{i=1..n} \mathcal{L}(y_i,f(x_i))" /> the empirical loss.</p>



<p>The <strong>generalization gap</strong> is the difference <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%2Cy%7D+%5Cmathcal%7BL%7D%28y%2Cf%28x%29%29+-+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1..n%7D%5Cmathcal%7BL%7D%28y_i%2Cf%28x_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))" class="latex" title="\mathbb{E}{x,y} \mathcal{L}(y,f(x)) - \tfrac{1}{n}\sum_{i=1..n}\mathcal{L}(y_i,f(x_i))" /> between the population and empirical losses. (We could add an absolute value though we expect that the loss over the training set would be smaller than the population loss; the population loss can be approximated by the “test loss” and so these terms are sometimes used interchangibly.)</p>



<p><strong>Why care about the generalization gap?</strong> You might argue that we only care about the population loss and not the gap between population and empirical loss. However, as mentioned before, we don’t even care about the population loss but about a more nebulous notion of “real-world performance.” We want the relations between our different abstractions to be as minimally “leaky” as possible and so bound the difference between train and test performance.</p>



<h3>Bias-variance tradeoff</h3>



<p>Suppose that our algorithm performs <em>empirical risk minimization (ERM)</em> which means that on input <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" />, we output <img src="https://s0.wp.com/latex.php?latex=f+%3D+%5Carg%5Cmin_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)" class="latex" title="f = \arg\min_{f \in \mathcal{F}} \hat{\mathcal{L}}_S(f)" />. Let’s assume that we have a collection of classifiers <img src="https://s0.wp.com/latex.php?latex=%7B+f_1%2Cf_2%2C%5Cldots+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ f_1,f_2,\ldots }" class="latex" title="{ f_1,f_2,\ldots }" /> and define <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D_K+%3D+%7B+f_1%2C%5Cldots%2C+f_K+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}_K = { f_1,\ldots, f_K }" class="latex" title="\mathcal{F}_K = { f_1,\ldots, f_K }" />. For every <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f)" class="latex" title="\hat{\mathcal{L}}_S(f)" /> is an estimator for <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f)" class="latex" title="\mathcal{L}(f)" /> and so we can write <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%3D+%5Cmathcal%7BL%7D%28f%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i" class="latex" title="\hat{\mathcal{L}}_S(f) = \mathcal{L}(f) + N_i" /> where <img src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_i" class="latex" title="N_i" /> is a random variable with mean zero and variance roughly <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/n" class="latex" title="1/n" /> (because we have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> samples).</p>



<p>The ERM algorithm outputs the <img src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i" class="latex" title="f_i" /> which minimizes <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28f_i%29+%2B+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(f_i) + N_i" class="latex" title="\mathcal{L}(f_i) + N_i" />. As <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> grows, the quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bi+%5Cin+%5BK%5D%7D%5Cmathcal%7BL%7D%28f_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{i \in [K]}\mathcal{L}(f_i)" class="latex" title="\min_{i \in [K]}\mathcal{L}(f_i)" /> (which is known as the <strong>bias</strong> term) shrinks. The quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bi+%5Cin+%5BK%5D%7D+N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_{i \in [K]} N_i" class="latex" title="\max_{i \in [K]} N_i" /> (which is known as the <strong>variance</strong> term) grows. When the variance term dominates the bias term, we could potentially start outputting classifiers that don’t perform better on the population. This is known as the “bias-variance tradeoff.”</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/B56EjDS.png" alt="" /></figure>



<h3>Counting generalization gap</h3>



<p>The most basic generalization gap is the following:</p>



<p><strong>Thm (counting gap):</strong> With high probability over <img src="https://s0.wp.com/latex.php?latex=S+%5Cin+%28X%2CY%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S \in (X,Y)^n" class="latex" title="S \in (X,Y)^n" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D%5Cleft%7C+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D_S%28f%29+%5Cright%7C+%5Cleq+O%5Cleft%28+%5Csqrt%7B%5Ctfrac%7B%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D%7Bn%7D%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)" class="latex" title="\max_{f \in \mathcal{F}}\left| \mathcal{L}(f) - \hat{\mathcal{L}}_S(f) \right| \leq O\left( \sqrt{\tfrac{\log |\mathcal{F}|}{n}}\right)" />.</p>



<p><strong>Proof:</strong> By standard bounds such as Chernoff etc.., the random variable <img src="https://s0.wp.com/latex.php?latex=N_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N_i" class="latex" title="N_i" /> behaves like a Normal/Gaussian of mean zero and standard deviation at most <img src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\sqrt{n}" class="latex" title="1/\sqrt{n}" />, which means that the probability that <img src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|N_i| \geq k/\sqrt{n}" class="latex" title="|N_i| \geq k/\sqrt{n}" /> is at most <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c+%5Ccdot+k%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-c \cdot k^2)" class="latex" title="\exp(-c \cdot k^2)" />. If we set <img src="https://s0.wp.com/latex.php?latex=k+%3D+10+%5Csqrt%7B10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%2Fc%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k = 10 \sqrt{10 \log |\mathcal{F}|/c}" class="latex" title="k = 10 \sqrt{10 \log |\mathcal{F}|/c}" /> then for every <img src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i" class="latex" title="f_i" />, <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B+%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D+%5D+%5Cleq+e%5E%7B-ck%5E2%7D+%3D+e%5E%7B-10+%5Clog+%7C%5Cmathcal%7BF%7D%7C%7D+%3C+%7C%5Cmathcal%7BF%7D%7C%5E%7B-10%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}" class="latex" title="\Pr[ |N_i| \geq k/\sqrt{n} ] \leq e^{-ck^2} = e^{-10 \log |\mathcal{F}|} &lt; |\mathcal{F}|^{-10}" />. Hence by the union bound, the probability that there <em>exists</em> <img src="https://s0.wp.com/latex.php?latex=f_i%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_i\in \mathcal{F}" class="latex" title="f_i\in \mathcal{F}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7CN_i%7C+%5Cgeq+k%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|N_i| \geq k/\sqrt{n}" class="latex" title="|N_i| \geq k/\sqrt{n}" /> is at most <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BF%7D%7C%2F%7C%5Cmathcal%7BF%7D%7C%5E%7B10%7D+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0" class="latex" title="|\mathcal{F}|/|\mathcal{F}|^{10} \rightarrow 0" />. QED</p>



<h3>Other generalization bounds</h3>



<p>One way to count the number of classifiers in a family <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> is by the bits to represent a member of the family– there are at most <img src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^k" class="latex" title="2^k" /> functions that can be represented using <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> bits. But this bound can be quite loose – for example, it can make a big difference if we use <img src="https://s0.wp.com/latex.php?latex=32&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="32" class="latex" title="32" /> or <img src="https://s0.wp.com/latex.php?latex=64&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="64" class="latex" title="64" /> bits to specify numbers, and some natural families (e.g., linear functions) are <em>infinite</em>. There are many bounds in the literature of the form</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BGeneralization+gap%7D+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Cfrac%7Bd%7D%7Bn%7D%7D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)" class="latex" title="\text{Generalization gap} \leq O\left(\sqrt{\frac{d}{n}} \right)" /></p>



<p>with values of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> other than <img src="https://s0.wp.com/latex.php?latex=%5Clog+%7C%5Cmathcal%7BF%7D%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log |\mathcal{F}|" class="latex" title="\log |\mathcal{F}|" />.<br />Intuitively <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> corresponds to the “capacity” of the classifier family/algorithm – the number of samples it can fit/memorize. Some examples (very roughly stated) include:</p>



<ul><li><strong>VC dimension:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the maximum number such that for every set of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> points and labels, there is a classifier in the family that fits the points to the labels. That is for every <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_d" class="latex" title="x_1,\ldots,x_d" /> and <img src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_1,\ldots,y_d" class="latex" title="y_1,\ldots,y_d" /> there is <img src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in\mathcal{F}" class="latex" title="f\in\mathcal{F}" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cforall_i+f%28x_i%29%3Dy_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall_i f(x_i)=y_i" class="latex" title="\forall_i f(x_i)=y_i" />.</li><li><strong>Rademacher Complexity:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the maximum number such that for random <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_d" class="latex" title="x_1,\ldots,x_d" /> from <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> and <img src="https://s0.wp.com/latex.php?latex=y_1%2C%5Cldots%2Cy_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_1,\ldots,y_d" class="latex" title="y_1,\ldots,y_d" /> uniform (assume say over <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }" class="latex" title="{ \pm 1 }" />) with high probability there exists <img src="https://s0.wp.com/latex.php?latex=f%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in \mathcal{F}" class="latex" title="f\in \mathcal{F}" /> with <img src="https://s0.wp.com/latex.php?latex=f%28x_i%29%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x_i)\approx y_i" class="latex" title="f(x_i)\approx y_i" /> for most <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />.</li><li><strong>PAC Bayes:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the mutual information between the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" /> that the learning algorithm is given as input and the classifier that it outputs. This requires some conditions on the learning algorithm and some prior distribution on the classifier. To get bounds on this quantity when the weights are continuous, we can add <em>noise</em> to them.</li><li><strong>Margin bounds:</strong> <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the “effective dimensionality” as measured by some margin. For example, for random unit vectors <img src="https://s0.wp.com/latex.php?latex=w%2Cx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w,x" class="latex" title="w,x" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" title="\mathbb{R}^d" />, <img src="https://s0.wp.com/latex.php?latex=%7C%5Clangle+w%2Cx+%5Crangle%7C+%5Csim+1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\langle w,x \rangle| \sim 1/\sqrt{d}" class="latex" title="|\langle w,x \rangle| \sim 1/\sqrt{d}" />. For linear classifiers, the margin bound is the minimum <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> such that correct labels over the training set are classified with at least <img src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\sqrt{d}" class="latex" title="1/\sqrt{d}" /> margin.</li></ul>



<p>A recent empirical study of generalization bounds is <a href="https://arxiv.org/abs/1912.02178">“fantastic generalization measures and where to find them”</a>by Jiang, Neyshabur, Mobahi, Krishnan, and Bengio, and <a href="https://arxiv.org/abs/2010.11924">“In Search of Robust Measures of Generalization”</a> by Dziugaite,  Drouin, Neal, Rajkumar, Caballero, Wang, Mitliagkas, and Roy.</p>



<h2>Limitations of generalization bounds</h2>



<p>The generalization gap <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bf%3DA%28S%29%7D%5Cleft%5B+%5Cmathcal%7BL%7D%28f%29+-+%5Chat%7B%5Cmathcal%7BL%7D%7D%28f%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]" class="latex" title="\mathbb{E}_{f=A(S)}\left[ \mathcal{L}(f) - \hat{\mathcal{L}}(f) \right]" /> depends on several quantities:</p>



<ul><li>The family <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{F}" class="latex" title="\mathcal{F}" /> of functions.</li><li>The algorithm <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> used to map the training set <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" title="S" /> to <img src="https://s0.wp.com/latex.php?latex=f%5Cin%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\in\mathcal{F}" class="latex" title="f\in\mathcal{F}" />.</li><li>The distribution <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> of datapoints</li><li>The distribution <img src="https://s0.wp.com/latex.php?latex=Y%7CX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y|X" class="latex" title="Y|X" /> of labels.</li></ul>



<p>A <strong>generalization bound</strong> is an upper bound on the gap that only depends on some of these quantities. In an influential paper, <a href="https://arxiv.org/abs/1611.03530">Zhang, Bengio, Hardt, Recht, Vinyals</a> showed significant barriers to obtaining such results that are meaningful for practical deep networks. They showed that in many natural settings, we cannot get such bounds even if we allow them to be based arbitrarily on the first three factors. That is, they showed that for natural families of functions (modern deep nets), natural algorithms (gradient descent on the empirical loss), natural distributions (CIFAR 10 and ImageNet), if we replace <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" title="Y" /> by the uniform distribution, then we can get arbitrarily large generalization gap.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/QqP2qnc.png" alt="" /></figure>



<p>We can also interpolate between the Zhang et al. experiment and the plain CIFAR-10 distribution. If we consider a distribution <img src="https://s0.wp.com/latex.php?latex=%28X%2C%5Ctilde%7BY%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X,\tilde{Y})" class="latex" title="(X,\tilde{Y})" /> where we take <img src="https://s0.wp.com/latex.php?latex=%28X%2CY%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X,Y)" class="latex" title="(X,Y)" /> from CIFAR-10 with probability <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> we replace the <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" title="Y" /> with a random label (one of the 10 CIFAR-10 classes) then the test/population performance (fraction of correct classifications) will be at most <img src="https://s0.wp.com/latex.php?latex=%281-p%29+%2B+p%2F10&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(1-p) + p/10" class="latex" title="(1-p) + p/10" /> (not surprising), but the training/empirical accuracy will remain at roughly 100%. The left-hand side of the gif below demonstrates this (this comes from <a href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">this paper with Bansal and Kaplun</a> which shows that, as the right side demonstrates, certain self-supervised learning algorithms do not suffer from this phenomenon; here the noise level is the fraction of wrong labels so <img src="https://s0.wp.com/latex.php?latex=0.9&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0.9" class="latex" title="0.9" /> is perfect noise):</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif" alt="" /></figure>



<h2>“Double descent.”</h2>



<p>While classical learning theory predicts a “bias-variance tradeoff” whereby as we increase the model class size, we get worse and worse performance, this is not what happens in modern deep learning systems. <a href="https://arxiv.org/abs/1812.11118">Belkin, Hsu, Ma, and Mandal</a> posited that such systems undergo a “double descent” whereby performance behaves according to the classical bias/variance curve up to the point in which we achieve <img src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 0" class="latex" title="\approx 0" /> training error and then starts improving again. This <a href="https://windowsontheory.org/2019/12/05/deep-double-descent/">actually happens</a> in real deep networks.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/PpyW6HW.png" alt="" /></figure>



<p>To get some intuition for the double descent phenomenon, consider the case of fitting a univariate polynomial of degree <img src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d^0" class="latex" title="d^0" /> to <img src="https://s0.wp.com/latex.php?latex=n+%3E+d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n &gt; d^0" class="latex" title="n &gt; d^0" /> samples of the form <img src="https://s0.wp.com/latex.php?latex=%28x%2Cf%28x%29%2Bnoise%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,f(x)+noise)" class="latex" title="(x,f(x)+noise)" /> where <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> is a degree <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> polynomial. When <img src="https://s0.wp.com/latex.php?latex=d%3Cd%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d&lt;d^0" class="latex" title="d&lt;d^0" /> we are “under-fitting” and will not get good performance. As <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> trends between <img src="https://s0.wp.com/latex.php?latex=d%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d^0" class="latex" title="d^0" /> and <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, we fit more and more of the noise, until for <img src="https://s0.wp.com/latex.php?latex=d%3Dn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=n" class="latex" title="d=n" /> we have a perfect interpolating polynomial that will have perfect train but very poor test performance. When <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> grows beyond <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, more than one polynomial can fit the data, and (under certain conditions) SGD will select the minimal norm one, which will make the interpolation smoother and smoother and actually result in better performance.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZhPtS0Y.gif" alt="" /></figure>



<h2>Approximation and representation</h2>



<p>Consider the task of distinguishing between the speech of an adult and a child. In the time domain, this may be hard, but by switching to representation in the Fourier domain, the task becomes much easier. (See this cartoon)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8SKSB9q.png" alt="" /></figure>



<p>The Fourier transform is based on the following theorem: for every continuous <img src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:[0,1] \rightarrow \mathbb{R}" class="latex" title="f:[0,1] \rightarrow \mathbb{R}" />, we can arbitrarily well approximate <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> as a linear combination of functions of the form <img src="https://s0.wp.com/latex.php?latex=e%5E%7B2%5Cpi+i+%5Calpha+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{2\pi i \alpha x}" class="latex" title="e^{2\pi i \alpha x}" />. Another way to say it is that if we use the embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N" class="latex" title="\varphi:\mathbb{R} \rightarrow \mathbb{R}^N" /> which maps <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into (sufficiently large) <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> coordinates of the form <img src="https://s0.wp.com/latex.php?latex=e%5E%7B2+%5Cpi+i+%5Calpha_j+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{2 \pi i \alpha_j x}" class="latex" title="e^{2 \pi i \alpha_j x}" /> then <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> becomes linear.</p>



<p>The wave functions are not the only ones that can approximate an arbitrary function. A <em>ReLU</em> is a function <img src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="r:\mathbb{R}^d \rightarrow \mathbb{R}" /> of the form <img src="https://s0.wp.com/latex.php?latex=r%28x%29+%3D+max+%5C%7B+w+%5Ccdot+x+%2B+b+%2C+0%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x) = max \{ w \cdot x + b , 0\}" class="latex" title="r(x) = max \{ w \cdot x + b , 0\}" />. We can approximate every continuous function arbitrarily well as a combination of ReLUs:</p>



<p><strong>Theorem:</strong> For every continous <img src="https://s0.wp.com/latex.php?latex=f%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:[0,1]^d \rightarrow \mathbb{R}" class="latex" title="f:[0,1]^d \rightarrow \mathbb{R}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" /> there is a function <img src="https://s0.wp.com/latex.php?latex=g%3A%5B0%2C1%5D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g:[0,1]^d \rightarrow \mathbb{R}" class="latex" title="g:[0,1]^d \rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is a linear combination of ReLUs and <img src="https://s0.wp.com/latex.php?latex=%5Cint+%7Cf-g%7C+%3C+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\int |f-g| &lt; \epsilon" class="latex" title="\int |f-g| &lt; \epsilon" />.</p>



<p>In one dimension, this follows from the facts that:</p>



<ul><li>ReLUs can give an arbitrarily good approximation to bump functions of the form<br /><img src="https://s0.wp.com/latex.php?latex=I_%7Ba%2Cb%7D%28x%29+%3D+%5Cbegin%7Bcases%7D+1+%26+a+%5Cleq+x+%5Cleq+b+%5C%5C+0+%26+%5Ctext%7Botherwise%7D%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}" class="latex" title="I_{a,b}(x) = \begin{cases} 1 &amp; a \leq x \leq b \\ 0 &amp; \text{otherwise}\end{cases}" /></li><li>Every continuous function on a bounded domain can be arbitrarily well approximated by the sum of bump functions.</li></ul>



<p>The second fact is well known, and here is a “proof by picture” for the first one:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/sKhHEap.png" alt="" /></figure>



<p>For higher dimensions, we need to create higher dimension bump functions. For example, in two dimensions, we can create a “noisy circle” by summing over all rotations of our bump. We can then add many such circles to create a two-dimensional bump. The same construction extends to an arbitrary number of dimensions.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/V8Wm5X1.png" alt="" /></figure>



<p><strong>How many ReLUs?</strong> The above shows that a linear combination of ReLUs can approximate every function on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> variables, but how many ReLUs are needed? Every ReLU <img src="https://s0.wp.com/latex.php?latex=r%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="r:\mathbb{R}^d \rightarrow \mathbb{R}" /> is specified by <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d+1" class="latex" title="d+1" /> numbers for the weights and bias. Intuitively, we could discretize each coordinate to a constant number of choices, and so there would be <img src="https://s0.wp.com/latex.php?latex=O%281%29%5Ed+%3D+2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O(1)^d = 2^{O(d)}" class="latex" title="O(1)^d = 2^{O(d)}" /> choices for such ReLUs. Indeed, it can be shown that every continuous function can be approximated by a linear combination of <img src="https://s0.wp.com/latex.php?latex=2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^{O(d)}" class="latex" title="2^{O(d)}" /> ReLUs. It turns out that some functions <em>require</em> an exponential number of ReLUS.</p>



<p>The above discussion doesn’t apply just for ReLUs but virtually any non-linear function.</p>



<h3>Representation summary</h3>



<p>By embedding our input <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in \mathbb{R}^d" class="latex" title="x\in \mathbb{R}^d" /> as a vector <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x) \in \mathbb{R}^N" class="latex" title="\varphi(x) \in \mathbb{R}^N" />, we can often make many “interesting” functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> become much simpler to compute (e.g., linear). In learning, we typically search for an <em>embedding</em> or <em>representation</em> that is “good” in one or more of the following senses:</p>



<ul><li>The dimension of embedding <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> is not too large for many “interesting” functions.</li><li>Two inputs <img src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,y" class="latex" title="x,y" /> are “semantically similar” if and only if <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(y)" class="latex" title="\varphi(y)" /> are correlated (e.g., <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x),\varphi(y) \rangle" class="latex" title="\langle \varphi(x),\varphi(y) \rangle" /> is large).</li><li>We can efficiently compute <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> and (sometimes) can compute <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C+%5Cvarphi%28y%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x), \varphi(y) \rangle" class="latex" title="\langle \varphi(x), \varphi(y) \rangle" /> without needing to explicitly compute <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29%2C%5Cvarphi%28y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x),\varphi(y)" class="latex" title="\varphi(x),\varphi(y)" />.</li><li>For “interesting” functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> can be approximated by a linear function in the embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" /> with “structured” coefficients (for example, sparse combination, or combination of coefficients of certain types, such as low frequency coefficients in Fourier domain)</li><li>…</li></ul>



<h2>Kernels and nearest neighbors</h2>



<p>Suppose that we have some notion <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> of “similarity” between inputs, where <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,y)" class="latex" title="K(x,y)" /> being large means that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is “close” to <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> and <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,y)" class="latex" title="K(x,y)" /> being small means that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is “far” from <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />.</p>



<p>This suggests that we can use one of the following methods approximating a function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> given inputs of the form <img src="https://s0.wp.com/latex.php?latex=%7B%28x_i%2C+y_i+%5Capprox+f%28x_i%29%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{(x_i, y_i \approx f(x_i))}" class="latex" title="{(x_i, y_i \approx f(x_i))}" />. On input <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, any of the following can be reasonable approximations to <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> depending on context:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> where <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> is the closest to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> in <img src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_1,\ldots, x_n }" class="latex" title="{ x_1,\ldots, x_n }" />. (This is known as the <em>nearest neighbor</em> algorithm.)</li><li>The mean (or other combining function) of <img src="https://s0.wp.com/latex.php?latex=y_%7Bi_1%7D%2C%5Cldots%2C+y_%7Bi_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_{i_1},\ldots, y_{i_k}" class="latex" title="y_{i_1},\ldots, y_{i_k}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B+x_%7Bi_1%7D%2C%5Cldots%2C+x_%7Bi_k%7D+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_{i_1},\ldots, x_{i_k} }" class="latex" title="{ x_{i_1},\ldots, x_{i_k} }" /> are the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> nearest inputs to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. (This is known as the <em><img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> nearest neighbor</em> algorithm.)</li><li>Some linear combination of <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> where the coefficients depend on <img src="https://s0.wp.com/latex.php?latex=K%28x%2Cx_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x,x_i)" class="latex" title="K(x,x_i)" />. (This is known as the <em>kernel</em> algorithm.)</li></ul>



<p>All of these algorithms are _non-parametric methods_ in the sense that the final regressor/classifier is specified by the full training set <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i) }_{i=1..n}" class="latex" title="{ (x_i,y_i) }_{i=1..n}" />.</p>



<p><strong>Kernel algorithms</strong> can also be described as follows. Given some embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N" class="latex" title="\varphi:\mathcal{X} \rightarrow \mathbb{R}^N" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{X}" class="latex" title="\mathcal{X}" /> is our input space, a Kernel regression approximates a function <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathcal{X} \rightarrow \mathbb{R}" class="latex" title="f:\mathcal{X} \rightarrow \mathbb{R}" /> by a linear function in <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x)" class="latex" title="\varphi(x)" />.</p>



<p>The key observation is that to solve linear equations or least-square minimization in <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w \in \mathbb{R}^N" class="latex" title="w \in \mathbb{R}^N" /> of the form <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29+%2C+w+%5Crangle+%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x_i) , w \rangle \approx y_i" class="latex" title="\langle \varphi(x_i) , w \rangle \approx y_i" />, we don’t need to know the vectors <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi%28x_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi(x_i)" class="latex" title="\varphi(x_i)" />. Rather, it is enough to know the inner products <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x_i%29%2C+%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x_i), \varphi(x_j) \rangle" class="latex" title="\langle \varphi(x_i), \varphi(x_j) \rangle" />. In Kernel methods we are often not given the embedding explicitly (indeed <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> might even be infinite) but rather the function <img src="https://s0.wp.com/latex.php?latex=K%3A%5Cmathcal%7BX%7D+%5Ctimes+%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}" class="latex" title="K:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=K%28x_i%2Cx_j%29+%3D+%5Clangle+%5Cvarphi%28x_i%29%2C%5Cvarphi%28x_j%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle" class="latex" title="K(x_i,x_j) = \langle \varphi(x_i),\varphi(x_j) \rangle" />. The only thing to verify is that <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" title="K" /> actually defines an inner product by checking that the matrix <img src="https://s0.wp.com/latex.php?latex=%28+K%28x_i%2Cx_j%29%29_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="( K(x_i,x_j))_{i,j}" class="latex" title="( K(x_i,x_j))_{i,j}" /> is positive semi-definite.</p>



<p>In general, Kernels and neural networks look quite similar – both ultimately involve composing a linear function <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> on top of a non-linear embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" />. It is not always clear cut whether an algorithm is a kernel or deep neural net method. Some characteristics of kernels are:</p>



<ul><li>The embedding <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" /> is not learned from the data. However, if <img src="https://s0.wp.com/latex.php?latex=%5Cvarphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\varphi" class="latex" title="\varphi" /> was learned from some other data, or was inspired by representations that were learned from data, then it becomes a fuzzier distinction.</li><li>There is a “shortcut” to compute the inner product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cvarphi%28x%29%2C%5Cvarphi%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \varphi(x),\varphi(x') \rangle" class="latex" title="\langle \varphi(x),\varphi(x') \rangle" /> using significantly smaller than <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" title="N" /> steps.</li></ul>



<p>Generally, the distinction between a kernel and deep nets depends on the application (is it to apply some analysis such as generalization bounds for kernels? is it to use kernel methods with shortcuts for the inner product?) and is more a spectrum than a binary partition.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/jeHi32z.png" alt="" /></figure>



<h2>Conclusion</h2>



<p>The above was a very condensed and rough survey of generalization, representation, approximation, and kernel methods. All of these are covered much better in the understanding machine learning book and the upcoming Hardt and Recht book.</p>



<p>In the next lecture, we will discuss the algorithmic bias of gradient descent, including the cases of linear regression and deep linear networks. We will discuss the “simplicity bias” of SGD and what can we say about what is learned at different layers of a deep network.</p>



<p><strong>Acknowledgements:</strong>  Thanks to Manos Theodosis and Preetum Nakkiran for pointing out several typos in a previous version.</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/"><span class="datestr">at January 31, 2021 07:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/01/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/01/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-probe-unsolved-hilbert-polynomial-problem-20210114/">Hilbert’s 13th, unsolved</a> (<a href="https://mathstodon.xyz/@11011110/105569819827303922">\(\mathbb{M}\)</a>). You can solve polynomials of degree at most four using one-argument algebraic functions like \(\sqrt x\). If \(RD(n)\) denotes the number of arguments needed for degree-\(n\) polynomials, <span style="white-space: nowrap;">then \(RD(4)=1\).</span> Hilbert asked whether \(RD(7)=2\). Vladimir Arnold showed in the 1950s that you can solve all polynomials with two-variable continuous (but not algebraic) functions, but mathematicians are only now catching on that the algebraic problem is still  open. See also <a href="https://arxiv.org/abs/2001.06515">some recent bounds on \(RD\)</a>.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/joshmillard/status/1349979253937381379">Fogleworms</a> (<a href="https://mastodon.social/@joshmillard/105572806531271932">\(\mathbb{M}\)</a>), partitions of \(n\times n\) grids into \(n\)-vertex grid paths, their enumeration, and a crafty project to visualize them.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/s00283-020-10034-w">A figure with Heesch number 6: Pushing a two-decade-old boundary</a> (<a href="https://mathstodon.xyz/@11011110/105579308071467320">\(\mathbb{M}\)</a>), Bojan Bašić in the <em>Mathematical Intelligencer</em>. When a shape cannot tile the plane, its <a href="https://en.wikipedia.org/wiki/Heesch%27s_problem">Heesch number</a> measures how far you can tile before getting stuck: if you surround the shape by layers of the same shape, how many layers can you make? Casey Mann’s previous record of five looked like a row of five hexagons with extra crenellations. This one uses six hexagons, with simpler crenellations.</p>
  </li>
  <li>
    <p><a href="https://blogs.scientificamerican.com/roots-of-unity/computation-in-service-of-poetry/">Exponentiation by squaring, in somewhat cryptic form, in a work by Pingala from India from over 2000 years ago</a> (<a href="https://mathstodon.xyz/@11011110/105586527250804920">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>I’ve been trying to understand the structure of the Kurilpa Bridge in Brisbane, supposedly “the world’s largest tensegrity bridge” (<a href="https://mathstodon.xyz/@11011110/105592704371209563">\(\mathbb{M}\)</a>). The clearest description I’ve found is from <a href="http://tadashidesign.com/kurilpa-bridge">Tadashi Design</a>. <a href="https://en.wikipedia.org/wiki/Kurilpa_Bridge">Wikipedia</a> is more cagy, calling it a “hybrid tensegrity bridge”, as it also includes features of cable-stayed bridges where the deck hangs from cables attached to tower piers. But the Tadashi Design site shows long sections far from the piers, so it seems the tensegrity is not just for show.</p>
  </li>
  <li>
    <p><a href="https://www.nationalgeographic.com/science/2021/01/we-need-better-face-masks-and-origami-might-help/">How origami folding patterns might help in the design of better face masks</a> (<a href="https://mathstodon.xyz/@11011110/105595412289181842">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>More US universities using covid as an excuse to treat faculty badly (<a href="https://mathstodon.xyz/@11011110/105609583164670586">\(\mathbb{M}\)</a>): <a href="https://www.chronicle.com/article/kansas-regents-allow-sped-up-dismissals-of-tenured-faculty-members">the Kansas state university system guts tenure</a>, and <a href="https://www.insidehighered.com/news/2021/01/21/u-florida-asks-students-report-professors-who-arent-teaching-person">the University of Florida asks students to snitch on faculty who refuse to endanger themselves by teaching in person</a>.</p>
  </li>
  <li>
    <p><a href="https://www.departures.com/lifestyle/architecture/hayri-atak-design-sarcostyle-building-manhattan-skyline">Proposed New York waterfront tower is a handlebody of high genus</a> (<a href="https://mathstodon.xyz/@11011110/105612316794720508">\(\mathbb{M}\)</a>, <a href="https://mastodon.social/@sarielhp/105612251833052747">via</a>, <a href="https://twitter.com/MathematicsUCL/status/1353328317781467137">via2</a>).</p>
  </li>
  <li>
    <p>Although it also <a href="https://en.wikipedia.org/wiki/Book_(graph_theory)">has other names</a>, the graph \(K_{1,1,n}\)  has been called the “thagomizer graph”, and its associated graphic matroid has been called the “thagomizer matroid” (<a href="https://mathstodon.xyz/@11011110/105620864011377814">\(\mathbb{M}\)</a>). The term appears to have been introduced by Katie Gedeon in  <a href="https://arxiv.org/abs/1610.05349">arXiv:1610.05349</a> in honor of the famous Far Side cartoon, whose terminology has <a href="https://en.wikipedia.org/wiki/Thagomizer">also been adopted by some paleontologists</a>.</p>
  </li>
  <li>
    <p>This new preprint looks interesting: <a href="https://arxiv.org/abs/2101.09592">Point-hyperplane incidence geometry and the log-rank conjecture, Noah Singer and Madhu Sudan, arXiv:2101.09592</a> (<a href="https://mathstodon.xyz/@11011110/105626816229116967">\(\mathbb{M}\)</a>). In the plane, \(n\) points and \(m\) lines can only touch \(\Theta\bigl((mn)^{2/3}+m+n\bigr)\) times. In 3d, points and planes can have mn incidences but only by sharing a common line. This paper connects similar problems in high dimensions to the <a href="https://en.wikipedia.org/wiki/Log-rank_conjecture">log-rank conjecture</a>, a famous unsolved problem in communication complexity.</p>
  </li>
  <li>
    <p><a href="https://www.math.ucdavis.edu/research/seminars/?talk_id=6082">Unknot recognition in quasi-polynomial time</a> (<a href="https://mathstodon.xyz/@11011110/105630455655140054">\(\mathbb{M}\)</a>, <a href="https://www.scottaaronson.com/blog/?p=5270">via</a>). Title of talk announcement by Marc Lackenby. No details or preprint yet but judging solely from the title and non-fringe status of the author this sounds like big news.</p>
  </li>
  <li>
    <p><a href="https://mathcs.clarku.edu/~fgreen/bookreviews/51-4.pdf">Frederic Green has published another review of my book “Forbidden Configurations in Discrete Geometry” in the latest <em>SIGACT News</em></a> (<a href="https://mathstodon.xyz/@11011110/105636589457644955">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.1145/3444815.3444817">official but paywalled url</a>). Thanks to Joe O’Rourke for the heads-up: I last checked my mail at the office, where my physical copies of <em>SIGACT News</em> would go if they went anywhere, months ago, and even then it looked like magazines weren’t getting through.</p>
  </li>
  <li>
    <p>Mathematics on the cutting block at Leicester again: <a href="https://gowers.wordpress.com/2021/01/30/leicester-mathematics-under-threat-again/">Gowers</a>, <a href="https://golem.ph.utexas.edu/category/2021/01/problems_at_the_university_of.html">nCat</a>, <a href="https://www.ipetitions.com/petition/mathematics-is-not-redundant">petition</a> (<a href="https://mathstodon.xyz/@11011110/105646674167288349">\(\mathbb{M}\)</a>). The plan is to eliminate research in pure mathematics at the University of Leicester, fire eight professors, and hire three back in purely teaching positions. I’m not sure who the eight are – the <a href="https://le.ac.uk/mathematics/people/academic-and-research">staff list</a> includes some other disciplines – but Leicester mathematicians in Wikipedia include <a href="https://en.wikipedia.org/wiki/Katrin_Leschke">Katrin Leschke</a> and <a href="https://en.wikipedia.org/wiki/Sergei_Petrovskii">Sergei Petrovskii</a>.</p>
  </li>
  <li>
    <p>Two newly-listed Good Articles on Wikipedia: <a href="https://en.wikipedia.org/wiki/Curve_of_constant_width">Curve of constant width</a> and <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ronald Graham</a> (<a href="https://mathstodon.xyz/@11011110/105652461392545754">\(\mathbb{M}\)</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/01/31/linkage.html"><span class="datestr">at January 31, 2021 03:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/008">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/008">TR21-008 |  Random walks and forbidden minors III: poly(d/?)-time partition oracles for minor-free graph classes | 

	Akash Kumar, 

	C. Seshadhri, 

	Andrew Stolman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the family of bounded degree graphs in any minor-closed family (such as planar graphs). Let d be the degree bound and n be the number of vertices of such a graph. Graphs in these classes have hyperfinite decompositions, where, for a sufficiently small ? &gt; 0, one removes
?dn edges to get connected components of size independent of n. An important tool for sublinear
algorithms and property testing for such classes is the partition oracle, introduced by the seminal
work of Hassidim-Kelner-Nguyen-Onak (FOCS 2009). A partition oracle is a local procedure
that gives consistent access to a hyperfinite decomposition, without any preprocessing. Given a
query vertex v, the partition oracle outputs the component containing v in time independent of
n. All the answers are consistent with a single hyperfinite decomposition.
The partition oracle of Hassidim et al. runs in time d^poly(d/?)-per query. They pose the
open problem of whether poly(d/?)-time partition oracles exist. Levi-Ron (ICALP 2013) give
a refinement of the previous approach, to get a partition oracle that runs in time d^log(d/?)-per
query.
In this paper, we resolve this open problem and give poly(d/?)-time partition oracles for
bounded degree graphs in any minor-closed family. Unlike the previous line of work based on
combinatorial methods, we employ techniques from spectral graph theory. We build on a recent
spectral graph theoretical toolkit for minor-closed graph families, introduced by the authors to
develop efficient property testers. A consequence of our result is a poly(d/?)-query tester for
any property of minor-closed families (such as bipartite planar graphs). Our result also gives
poly(d/?)-query algorithms for additive ?n-approximations for problems such as maximum
matching, minimum vertex cover, maximum independent set, and minimum dominating set for
these graph families.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/008"><span class="datestr">at January 30, 2021 11:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
