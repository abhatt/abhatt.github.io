<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at February 27, 2021 02:40 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/">PhD position at University of Amsterdam (apply by March 18, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity.</p>
<p>Website: <a href="https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html">https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html</a><br />
Email: f.speelman@uva.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/"><span class="datestr">at February 26, 2021 10:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/">Faculty at KREA University, India (apply by May 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br />
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br />
Email: sias.chair_sciences@krea.edu.in</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/"><span class="datestr">at February 26, 2021 08:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.13098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.13098">Toward Instance-Optimal State Certification With Incoherent Measurements</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, Ryan O'Donnell <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.13098">PDF</a><br /><b>Abstract: </b>We revisit the basic problem of quantum state certification: given copies of
unknown mixed state $\rho\in\mathbb{C}^{d\times d}$ and the description of a
mixed state $\sigma$, decide whether $\sigma = \rho$ or $\|\sigma -
\rho\|_{\mathsf{tr}} \ge \epsilon$. When $\sigma$ is maximally mixed, this is
mixedness testing, and it is known that $\Omega(d^{\Theta(1)}/\epsilon^2)$
copies are necessary, where the exact exponent depends on the type of
measurements the learner can make [OW15, BCL20], and in many of these settings
there is a matching upper bound [OW15, BOW19, BCL20].
</p>
<p>Can one avoid this $d^{\Theta(1)}$ dependence for certain kinds of mixed
states $\sigma$, e.g. ones which are approximately low rank? More ambitiously,
does there exist a simple functional $f:\mathbb{C}^{d\times
d}\to\mathbb{R}_{\ge 0}$ for which one can show that
$\Theta(f(\sigma)/\epsilon^2)$ copies are necessary and sufficient for state
certification with respect to any $\sigma$? Such instance-optimal bounds are
known in the context of classical distribution testing, e.g. [VV17].
</p>
<p>Here we give the first bounds of this nature for the quantum setting, showing
(up to log factors) that the copy complexity for state certification using
nonadaptive incoherent measurements is essentially given by the copy complexity
for mixedness testing times the fidelity between $\sigma$ and the maximally
mixed state. Surprisingly, our bound differs substantially from instance
optimal bounds for the classical problem, demonstrating a qualitative
difference between the two settings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.13098"><span class="datestr">at February 26, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.13095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.13095">Subcubic Certificates for CFL Reachability</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chistikov:Dmitry.html">Dmitry Chistikov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Majumdar:Rupak.html">Rupak Majumdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schepper:Philipp.html">Philipp Schepper</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.13095">PDF</a><br /><b>Abstract: </b>Many problems in interprocedural program analysis can be modeled as the
context-free language (CFL) reachability problem on graphs and can be solved in
cubic time. Despite years of efforts, there are no known truly sub-cubic
algorithms for this problem. We study the related certification task: given an
instance of CFL reachability, are there small and efficiently checkable
certificates for the existence and for the non-existence of a path? We show
that, in both scenarios, there exist succinct certificates ($O(n^2)$ in the
size of the problem) and these certificates can be checked in subcubic (matrix
multiplication) time. The certificates are based on grammar-based compression
of paths (for positive instances) and on invariants represented as matrix
constraints (for negative instances). Thus, CFL reachability lies in
nondeterministic and co-nondeterministic subcubic time.
</p>
<p>A natural question is whether faster algorithms for CFL reachability will
lead to faster algorithms for combinatorial problems such as Boolean
satisfiability (SAT). As a consequence of our certification results, we show
that there cannot be a fine-grained reduction from SAT to CFL reachability for
a conditional lower bound stronger than $n^\omega$, unless the nondeterministic
strong exponential time hypothesis (NSETH) fails.
</p>
<p>Our results extend to related subcubic equivalent problems: pushdown
reachability and two-way nondeterministic pushdown automata (2NPDA) language
recognition. For example, we describe succinct certificates for pushdown
non-reachability (inductive invariants) and observe that they can be checked in
matrix multiplication time. We also extract a new hardest 2NPDA language,
capturing the "hard core" of all these problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.13095"><span class="datestr">at February 26, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.13068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.13068">Truncated Log-concave Sampling with Reflective Hamiltonian Monte Carlo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalkis:Apostolos.html">Apostolos Chalkis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fisikopoulos:Vissarion.html">Vissarion Fisikopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papachristou:Marios.html">Marios Papachristou</a>, Elias Tsigaridas <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.13068">PDF</a><br /><b>Abstract: </b>We introduce Reflective Hamiltonian Monte Carlo (ReHMC), an HMC-based
algorithm, to sample from a log-concave distribution restricted to a convex
polytope. We prove that, starting from a warm start, it mixes in $\widetilde
O(\kappa d^2 \ell^2 \log (1 / \varepsilon))$ steps for a well-rounded polytope,
ignoring logarithmic factors where $\kappa$ is the condition number of the
negative log-density, $d$ is the dimension, $\ell$ is an upper bound on the
number of reflections, and $\varepsilon$ is the accuracy parameter. We also
developed an open source implementation of ReHMC and we performed an
experimental study on various high-dimensional data-sets. Experiments suggest
that ReHMC outperfroms Hit-and-Run and Coordinate-Hit-and-Run regarding the
time it needs to produce an independent sample.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.13068"><span class="datestr">at February 26, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12975">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12975">The Power of $D$-hops in Matching Power-Law Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Liren.html">Liren Yu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiaming.html">Jiaming Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Xiaojun.html">Xiaojun Lin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12975">PDF</a><br /><b>Abstract: </b>This paper studies seeded graph matching for power-law graphs. Assume that
two edge-correlated graphs are independently edge-sampled from a common parent
graph with a power-law degree distribution. A set of correctly matched
vertex-pairs is chosen at random and revealed as initial seeds. Our goal is to
use the seeds to recover the remaining latent vertex correspondence between the
two graphs. Departing from the existing approaches that focus on the use of
high-degree seeds in $1$-hop neighborhoods, we develop an efficient algorithm
that exploits the low-degree seeds in suitably-defined $D$-hop neighborhoods.
Specifically, we first match a set of vertex-pairs with appropriate degrees
(which we refer to as the first slice) based on the number of low-degree seeds
in their $D$-hop neighborhoods. This significantly reduces the number of
initial seeds needed to trigger a cascading process to match the rest of the
graphs. Under the Chung-Lu random graph model with $n$ vertices, max degree
$\Theta(\sqrt{n})$, and the power-law exponent $2&lt;\beta&lt;3$, we show that as
soon as $D&gt; \frac{4-\beta}{3-\beta}$, by optimally choosing the first slice,
with high probability our algorithm can correctly match a constant fraction of
the true pairs without any error, provided with only $\Omega((\log
n)^{4-\beta})$ initial seeds. Our result achieves an exponential reduction in
the seed size requirement, as the best previously known result requires
$n^{1/2+\epsilon}$ seeds (for any small constant $\epsilon&gt;0$). Performance
evaluation with synthetic and real data further corroborates the improved
performance of our algorithm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12975"><span class="datestr">at February 26, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12926">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12926">Persistent Homology and Graphs Representation Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajij:Mustafa.html">Mustafa Hajij</a>, Ghaza Zamzmi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Xuanting.html">Xuanting Cai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12926">PDF</a><br /><b>Abstract: </b>This article aims to study the topological invariant properties encoded in
node graph representational embeddings by utilizing tools available in
persistent homology. Specifically, given a node embedding representation
algorithm, we consider the case when these embeddings are real-valued. By
viewing these embeddings as scalar functions on a domain of interest, we can
utilize the tools available in persistent homology to study the topological
information encoded in these representations. Our construction effectively
defines a unique persistence-based graph descriptor, on both the graph and node
levels, for every node representation algorithm. To demonstrate the
effectiveness of the proposed method, we study the topological descriptors
induced by DeepWalk, Node2Vec and Diff2Vec.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12926"><span class="datestr">at February 26, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12886">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12886">Generalized Parametric Path Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Prerona.html">Prerona Chatterjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gajjar:Kshitij.html">Kshitij Gajjar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Radhakrishnan:Jaikumar.html">Jaikumar Radhakrishnan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Varma:Girish.html">Girish Varma</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12886">PDF</a><br /><b>Abstract: </b>Parametric path problems arise independently in diverse domains, ranging from
transportation to finance, where they are studied under various assumptions. We
formulate a general path problem with relaxed assumptions, and describe how
this formulation is applicable in these domains.
</p>
<p>We study the complexity of the general problem, and a variant of it where
preprocessing is allowed. We show that when the parametric weights are linear
functions, algorithms remain tractable even under our relaxed assumptions.
Furthermore, we show that if the weights are allowed to be non-linear, the
problem becomes NP-hard. We also study the mutli-dimensional version of the
problem where the weight functions are parameterized by multiple parameters. We
show that even with two parameters, the problem is NP-hard.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12886"><span class="datestr">at February 26, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12879">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12879">A Faster Tight Approximation for Submodular Maximization Subject to a Knapsack Constraint</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulik:Ariel.html">Ariel Kulik</a>, Roy Schwartz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shachnai:Hadas.html">Hadas Shachnai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12879">PDF</a><br /><b>Abstract: </b>The problem of maximizing a monotone submodular function subject to a
knapsack constraint admits a tight $(1-e^{-1})$-approximation: exhaustively
enumerate over all subsets of size at most three and extend each using the
greedy heuristic [Sviridenko, 2004]. We prove it suffices to enumerate only
over all subsets of size at most two and still retain a tight
$(1-e^{-1})$-approximation. This improves the running time from $O(n^5)$ to
$O(n^4)$ queries. The result is achieved via a refined analysis of the greedy
heuristic.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12879"><span class="datestr">at February 26, 2021 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12872">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12872">Digital almost nets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bukh:Boris.html">Boris Bukh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Ting=Wei.html">Ting-Wei Chao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12872">PDF</a><br /><b>Abstract: </b>Digital nets (in base $2$) are the subsets of $[0,1]^d$ that contain the
expected number of points in every not-too-small dyadic box. We construct sets
that contain almost the expected number of points in every such box, but which
are exponentially smaller than the digital nets. We also establish a lower
bound on the size of such almost nets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12872"><span class="datestr">at February 26, 2021 11:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12842">Coalgebra Encoding for Efficient Minimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deifel:Hans=Peter.html">Hans-Peter Deifel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milius:Stefan.html">Stefan Milius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wi=szlig=mann:Thorsten.html">Thorsten Wißmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12842">PDF</a><br /><b>Abstract: </b>Recently, we have developed an efficient generic partition refinement
algorithm, which computes behavioural equivalence on a state-based system given
as an encoded coalgebra, and implemented it in the tool CoPaR. Here we extend
this to a fully fledged minimization algorithm and tool by integrating two new
aspects: (1) the computation of the transition structure on the minimized state
set, and (2) the computation of the reachable part of the given system. In our
generic coalgebraic setting these two aspects turn out to be surprisingly
non-trivial requiring us to extend the previous theory. In particular, we
identify a sufficient condition on encodings of coalgebras, and we show how to
augment the existing interface, which encapsulates computations that are
specific for the coalgebraic type functor, to make the above extensions
possible. Both extensions have linear run time. Surprisingly, all information
necessary for computing the reachable part of a coalgebra is already present in
the data structures that we previously developed only for the computation of
behavioural equivalence.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12842"><span class="datestr">at February 26, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12824">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12824">A Linear Time Algorithm for Constructing Hierarchical Overlap Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Park:Sangsoo.html">Sangsoo Park</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Park:Sung_Gwan.html">Sung Gwan Park</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cazaux:Bastien.html">Bastien Cazaux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Park:Kunsoo.html">Kunsoo Park</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rivals:Eric.html">Eric Rivals</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12824">PDF</a><br /><b>Abstract: </b>The hierarchical overlap graph (HOG) is a graph that encodes overlaps from a
given set P of n strings, as the overlap graph does. A best known algorithm
constructs HOG in O(||P|| log n) time and O(||P||) space, where ||P|| is the
sum of lengths of the strings in P. In this paper we present a new algorithm to
construct HOG in O(||P||) time and space. Hence, the construction time and
space of HOG are better than those of the overlap graph, which are O(||P|| +
n^2).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12824"><span class="datestr">at February 26, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12822">Algorithms and Complexity on Indexing Founder Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Equi:Massimo.html">Massimo Equi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Norri:Tuukka.html">Tuukka Norri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alanko:Jarno.html">Jarno Alanko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cazaux:Bastien.html">Bastien Cazaux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tomescu:Alexandru_I=.html">Alexandru I. Tomescu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=auml=kinen:Veli.html">Veli Mäkinen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12822">PDF</a><br /><b>Abstract: </b>We introduce a compact pangenome representation based on an optimal
segmentation concept that aims to reconstruct founder sequences from a multiple
sequence alignment (MSA). Such founder sequences have the feature that each row
of the MSA is a recombination of the founders. Several linear time dynamic
programming algorithms have been previously devised to optimize segmentations
that induce founder blocks that then can be concatenated into a set of founder
sequences. All possible concatenation orders can be expressed as a founder
graph. We observe a key property of such graphs: if the node labels (founder
segments) do not repeat in the paths of the graph, such graphs can be indexed
for efficient string matching. We call such graphs repeat-free founder graphs
when constructed from a gapless MSA and repeat-free elastic founder graphs when
constructed from a general MSA with gaps. We give a linear time algorithm and a
parameterized near linear time algorithm to construct a repeat-free founder
graph and a repeat-free elastic founder graph, respectively. We derive a
tailored succinct index structure to support queries of arbitrary length in the
paths of a repeat-free (elastic) founder graph. In addition, we show how to
turn a repeat-free (elastic) founder graph into a Wheeler graph in polynomial
time. Furthermore, we show that a property such as repeat-freeness is essential
for indexability. In particular, we show that unless the Strong Exponential
Time Hypothesis (SETH) fails, one cannot build an index on an elastic founder
graph in polynomial time to support fast queries.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12822"><span class="datestr">at February 26, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12772">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12772">A Comprehensive Survey on the Multiple Travelling Salesman Problem: Applications, Approaches and Taxonomy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheikhrouhou:Omar.html">Omar Cheikhrouhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khoufi:Ines.html">Ines Khoufi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12772">PDF</a><br /><b>Abstract: </b>The Multiple Travelling Salesman Problem (MTSP) is among the most interesting
combinatorial optimization problems because it is widely adopted in real-life
applications, including robotics, transportation, networking, etc. Although the
importance of this optimization problem, there is no survey dedicated to
reviewing recent MTSP contributions. In this paper, we aim to fill this gap by
providing a comprehensive review of existing studies on MTSP. In this survey,
we focus on MTSP's recent contributions to both classical vehicles/robots and
unmanned aerial vehicles. We highlight the approaches applied to solve the MTSP
as well as its application domains. We analyze the MTSP variants and propose a
taxonomy and a classification of recent studies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12772"><span class="datestr">at February 26, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12646">Spanning Tree Constrained Determinantal Point Processes are Hard to (Approximately) Evaluate</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matsuoka:Tatsuya.html">Tatsuya Matsuoka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ohsaka:Naoto.html">Naoto Ohsaka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12646">PDF</a><br /><b>Abstract: </b>We consider determinantal point processes (DPPs) constrained by spanning
trees. Given a graph $G=(V,E)$ and a positive semi-definite matrix $\mathbf{A}$
indexed by $E$, a spanning-tree DPP defines a distribution such that we draw
$S\subseteq E$ with probability proportional to $\det(\mathbf{A}_S)$ only if
$S$ induces a spanning tree. We prove $\sharp\textsf{P}$-hardness of computing
the normalizing constant for spanning-tree DPPs and provide an
approximation-preserving reduction from the mixed discriminant, for which FPRAS
is not known. We show similar results for DPPs constrained by forests.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12646"><span class="datestr">at February 26, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12610">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12610">Approximate Privacy-Preserving Neighbourhood Estimations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Alvaro Garcia-Recuero <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12610">PDF</a><br /><b>Abstract: </b>Anonymous social networks present a number of new and challenging problems
for existing Social Network Analysis techniques. Traditionally, existing
methods for analysing graph structure, such as community detection, required
global knowledge of the graph structure. That implies that a centralised entity
must be given access to the edge list of each node in the graph. This is
impossible for anonymous social networks and other settings where privacy is
valued by its participants. In addition, using their graph structure inputs for
learning tasks defeats the purpose of anonymity. In this work, we hypothesise
that one can re-purpose the use of the HyperANF a.k.a HyperBall algorithm --
intended for approximate diameter estimation -- to the task of
privacy-preserving community detection for friend recommending systems that
learn from an anonymous representation of the social network graph structure
with limited privacy impacts. This is possible because the core data structure
maintained by HyperBall is a HyperLogLog with a counter of the number of
reachable neighbours from a given node. Exchanging this data structure in
future decentralised learning deployments gives away no information about the
neighbours of the node and therefore does preserve the privacy of the graph
structure.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12610"><span class="datestr">at February 26, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12604">Random Graphs with Prescribed $K$-Core Sequences: A New Null Model for Network Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koevering:Katherine_Van.html">Katherine Van Koevering</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Benson:Austin_R=.html">Austin R. Benson</a>, Jon Kleinberg <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12604">PDF</a><br /><b>Abstract: </b>In the analysis of large-scale network data, a fundamental operation is the
comparison of observed phenomena to the predictions provided by null models:
when we find an interesting structure in a family of real networks, it is
important to ask whether this structure is also likely to arise in random
networks with similar characteristics to the real ones. A long-standing
challenge in network analysis has been the relative scarcity of reasonable null
models for networks; arguably the most common such model has been the
configuration model, which starts with a graph $G$ and produces a random graph
with the same node degrees as $G$. This leads to a very weak form of null
model, since fixing the node degrees does not preserve many of the crucial
properties of the network, including the structure of its subgraphs.
</p>
<p>Guided by this challenge, we propose a new family of network null models that
operate on the $k$-core decomposition. For a graph $G$, the $k$-core is its
maximal subgraph of minimum degree $k$; and the core number of a node $v$ in
$G$ is the largest $k$ such that $v$ belongs to the $k$-core of $G$. We provide
the first efficient sampling algorithm to solve the following basic
combinatorial problem: given a graph $G$, produce a random graph sampled nearly
uniformly from among all graphs with the same sequence of core numbers as $G$.
This opens the opportunity to compare observed networks $G$ with random graphs
that exhibit the same core numbers, a comparison that preserves aspects of the
structure of $G$ that are not captured by more local measures like the degree
sequence. We illustrate the power of this core-based null model on some
fundamental tasks in network analysis, including the enumeration of networks
motifs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12604"><span class="datestr">at February 26, 2021 10:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12589">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12589">A New Algorithm for Euclidean Shortest Paths in the Plane</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Haitao.html">Haitao Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12589">PDF</a><br /><b>Abstract: </b>Given a set of pairwise disjoint polygonal obstacles in the plane, finding an
obstacle-avoiding Euclidean shortest path between two points is a classical
problem in computational geometry and has been studied extensively. Previously,
Hershberger and Suri [SIAM J. Comput. 1999] gave an algorithm of $O(n\log n)$
time and $O(n\log n)$ space, where $n$ is the total number of vertices of all
obstacles. Recently, by modifying Hershberger and Suri's algorithm, Wang [SODA
2021] reduced the space to $O(n)$ while the runtime of the algorithm is still
$O(n\log n)$. In this paper, we present a new algorithm of $O(n+h\log h)$ time
and $O(n)$ space, provided that a triangulation of the free space is given,
where $h$ is the number of obstacles. The algorithm, which improves the
previous work when $h=o(n)$, is optimal in both time and space as
$\Omega(n+h\log h)$ is a lower bound on the runtime. Our algorithm builds a
shortest path map for a source point $s$, so that given any query point $t$,
the shortest path length from $s$ to $t$ can be computed in $O(\log n)$ time
and a shortest $s$-$t$ path can be produced in additional time linear in the
number of edges of the path.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12589"><span class="datestr">at February 26, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.12531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.12531">SALSA: Self-Adjusting Lean Streaming Analytics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basat:Ran_Ben.html">Ran Ben Basat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Einziger:Gil.html">Gil Einziger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vargaftik:Shay.html">Shay Vargaftik</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.12531">PDF</a><br /><b>Abstract: </b>Counters are the fundamental building block of many data sketching schemes,
which hash items to a small number of counters and account for collisions to
provide good approximations for frequencies and other measures. Most existing
methods rely on fixed-size counters, which may be wasteful in terms of space,
as counters must be large enough to eliminate any risk of overflow. Instead,
some solutions use small, fixed-size counters that may overflow into secondary
structures.
</p>
<p>This paper takes a different approach. We propose a simple and general method
called SALSA for dynamic re-sizing of counters and show its effectiveness.
SALSA starts with small counters, and overflowing counters simply merge with
their neighbors. SALSA can thereby allow more counters for a given space,
expanding them as necessary to represent large numbers. Our evaluation
demonstrates that, at the cost of a small overhead for its merging logic, SALSA
significantly improves the accuracy of popular schemes (such as Count-Min
Sketch and Count Sketch) over a variety of tasks. Our code is released as
open-source [1].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.12531"><span class="datestr">at February 26, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/">Tenure Track (Open Rank) at University of Illinois, Urbana-Champaign (apply by June 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas.</p>
<p>Website: <a href="https://cs.illinois.edu/about/positions/faculty-positions">https://cs.illinois.edu/about/positions/faculty-positions</a><br />
Email: chekuri@illinois.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/"><span class="datestr">at February 25, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2202248828009562800">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html">Complexity is the Enemy of Speed</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The title of this post came from an <a href="https://www.wsj.com/articles/connecticuts-covid-vaccine-lesson-11614124012">opinion piece</a> in the Wall Street Journal yesterday on vaccine distribution. Many attempts to get the vaccines to the right groups first have slowed down distribution and sometime even caused <a href="https://www.nbcnews.com/news/us-news/thousands-covid-19-vaccines-wind-garbage-because-fed-state-guidelines-n1254364">vaccines to go to waste</a>. Rules to help spread vaccines across minority groups often backfire. Often when some rules lead to inequity, we try to fix it with more rules when we need less much less. Attempts to distribute vaccines to multiple medical and pharmacy sites have made it difficult to get appointments even if you are eligible.</p><p>Randomness is the simplest way to fairness. The movie Contagion got it right, just choose birthdays by picking balls from a bin to distribute the vaccine. Then people can just show up at a few chosen sites with proof of birthday. No need to sign up.</p><p>You could argue to add back conditions like age, medical conditions, jobs but that just leads you down the same problematic path. The fastest way to get past this pandemic is to get vaccines into arms. Trust the randomness.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html"><span class="datestr">at February 25, 2021 02:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/">Faculty at KREA University, India (apply by May 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br />
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br />
Email: sias.chair_sciences@krea.edu.in</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/"><span class="datestr">at February 25, 2021 09:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=532">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/">TCS+ talk: Wednesday, March 3 — Steve Hanneke, TTIC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Steve Hanneke</strong> from TTIC will speak about “<em>A Theory of Universal Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> aftwerwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.</p>
<p>In this work, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this work is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.</p>
<p>Joint work with Olivier Bousquet, Shay Moran, Ramon van Handel, and Amir Yehudayoff.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/"><span class="datestr">at February 25, 2021 02:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8008">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Unsupervised Learning and generative models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do neural networks learn and when do they learn it</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=70cafab0-bdea-412b-a353-acc90173fd61">video</a></p>



<p>In this lecture, we move from the world of supervised learning to unsupervised learning, with a focus on generative models. We will</p>



<ul><li>Introduce unsupervised learning and the relevant notations.</li><li>Discuss various approaches for generative models, such as PCA, VAE, Flow Models, and GAN.</li><li>Discuss theoretical and practical results we currently have for these approaches.</li></ul>



<h2>Setup for Unsupervised Learning</h2>



<p>In <em>supervised learning</em>, we have data <img src="https://s0.wp.com/latex.php?latex=x_i%5Csim+p%5Csubset+%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i\sim p\subset \mathbb R^d" class="latex" title="x_i\sim p\subset \mathbb R^d" /> and we want to understand the distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. For example,</p>



<ol><li><em>Probability estimation:</em> Given <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, can we compute/approximate <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" title="p(x)" /> (the probability that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is output under <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />)?</li><li><em>Generation:</em> Can we sample from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, or from a “nearby” distribution?</li><li><em>Encoding:</em> Can we find a representation <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathrm%7BSupport%7D%28p%29+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" class="latex" title="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" /> such that for <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim p" class="latex" title="x \sim p" />, <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> makes it easy to answer semantic questions on <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />? And such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28x%29+%2C+E%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle E(x) , E(x') \rangle" class="latex" title="\langle E(x) , E(x') \rangle" /> corresponds to “semantic similarity” of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" />?</li><li><em>Prediction:</em> We would like to be able to predict (for example) the second half of <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim p" class="latex" title="x \sim p" /> from the first half. More generally, we want to solve the <em>conditional generation</em> task, where given some function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> (e.g., the projection to the first half) and some value <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, we can sample from the conditional probability distribution <img src="https://s0.wp.com/latex.php?latex=p%7Cf%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p|f(x)=y" class="latex" title="p|f(x)=y" />.</li></ol>



<p>Our “dream” is to solve all of those by the following setup:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/EPyXsSW.png" alt="" /></figure>



<p>There is an “encoder” <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> that maps <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into a representation <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in the latent space, and then a “decoder” <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> that can transform such a representation back into <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. We would like it to be the case that:</p>



<ol><li><em>Generation:</em> For <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" />, the induced distribution <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> is “nice” and efficiently sampleable (e.g., the standard normal <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^r" class="latex" title="\mathbb{R}^r" />) such that we can (approximately) sample from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> by sampling <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> and outputting <img src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)" class="latex" title="D(z)" />.</li><li><em>Density estimation:</em> We would like to be able to evaluate the probability that <img src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)=x" class="latex" title="D(z)=x" />. For example, if <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> is the inverse of <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" />, and <img src="https://s0.wp.com/latex.php?latex=z+%5Csim+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z \sim N(0,I)" class="latex" title="z \sim N(0,I)" /> we could do so by computing <img src="https://s0.wp.com/latex.php?latex=%7C+E%28x%29+%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| E(x) |" class="latex" title="| E(x) |" />.</li><li><em>Semantic representation:</em> We would like the latent representation <img src="https://s0.wp.com/latex.php?latex=E%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(z)" class="latex" title="E(z)" /> to map <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into meaningful latent space. Ideally, linear directions in this space will correspond to semantic attributes.</li><li><em>Conditional sampling:</em> We would like to be able to do conditional generation, and in particular for some functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and values <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, be able to sample from the set of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" />‘s such that <img src="https://s0.wp.com/latex.php?latex=f%28E%28z%29%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(E(z))=y" class="latex" title="f(E(z))=y" /></li></ol>



<p>Ideally, if we could map images to the latent variables used to generate them and vice versa (as in the cartoon from the last lecture), then we could achieve these goals:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/3X4aqfl.png" alt="" /></figure>



<p>At the moment, we do not have a single system that can solve all these problems for a natural domain such as images or language, but we have several approaches that achieve part of the dream.</p>



<p><strong>Digressions.</strong> Before discussing concrete models, we make three digressions. One will be non-technical, and the other three technical. The three technical digressions are the following:</p>



<ol><li>If we have multiple objectives, we want a way to interpolate between them.</li><li>To measure how good our models are, we have to measure distances between statistical distributions.</li><li>Once we come up with generating models, we would <em>metrics</em> for measuring how good they are.</li></ol>



<h2>Non-technical digression: Is deep learning a cargo cult science? (spoiler: no)</h2>



<p>In an <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">influential essay</a>, Richard Feynman coined the term “cargo cult science” for the activities that have superficial similarities to science but do not follow the scientific method. Some of the tools we use in machine learning look suspiciously close to “cargo cult science.” We use the tools of classical learning, but in a setting in which they were not designed to work in and on which we have no guarantees that they will work. For example, we run (stochastic) gradient descent – an algorithm designed to minimize a convex function – to minimize convex loss. We also write use <em>empirical risk minimization</em> – minimizing loss on our training set – in a setting where we have no guarantee that it will not lead to “overfitting.”</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/tBw6UsX.png" alt="" /></figure>



<figure class="wp-block-image"><img src="https://i.imgur.com/s5G6xfj.png" alt="" /></figure>



<p>And yet, unlike the original cargo cults, in deep learning, “the planes do land”, or at least they often do. When we use a tool <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> in a situation <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> that it was not designed to work in, it can play out in one (or mixture) of the following scenarios:</p>



<ul><li><strong>Murphy’s Law:</strong> “Anything that can go wrong will go wrong.” As computer scientists, we are used to this scenario. The natural state of our systems is that they have bugs and errors. There is a reason why software engineering talks about “contracts”, “invariants”, preconditions” and “postconditions”: typically, if we try to use a component <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> in a situation that it wasn’t designed for, it will not turn out well. This is doubly the case in security and cryptography, where people have learned the hard way time and again that Murphy’s law holds sway.</li><li><strong>“Marley’s Law”:</strong> “Every little thing gonna be alright”. In machine learning, we sometimes see the opposite phenomenon- we use algorithms outside the conditions under which they have been analysed or designed to work in, but they still produce good results. Part of it could be because ML algorithms are already robust to certain errors in their inputs, and their output was only guaranteed to be approximately correct in the first place.</li></ul>



<p>Murphy’s law does occasionally pop up, even in machine learning. We will see examples of both phenomena in this lecture.</p>



<h2>Technical digression 1: Optimization with Multiple Objectives</h2>



<p>During machine learning, we often have multiple objectives to optimize. For example, we may want both an efficient encoder and an effective decoder, but there is a tradeoff between them.</p>



<p>Suppose we have 2 loss functions <img src="https://s0.wp.com/latex.php?latex=L_1%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1(w)" class="latex" title="L_1(w)" /> and <img src="https://s0.wp.com/latex.php?latex=L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_2(w)" class="latex" title="L_2(w)" />, but there can be a trade off between them. The <em>pareto curve</em> is the set <img src="https://s0.wp.com/latex.php?latex=P%3D%7B%28a%2Cb%29%3A+%5Cforall+w%5Cin+W%2C+L_1%28w%29%5Cge+a%5Cvee+L_2%28w%29%5Cge+b.%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" class="latex" title="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/QbPRQtR.jpg" alt="Pareto curve for 2 loss functions" /></figure>



<p>If a model is above the curve, it is not optimal. If it is below the curve, the model is infeasible.</p>



<p>When the set <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> is convex, we can reach any point on the curve <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> by minimizing <img src="https://s0.wp.com/latex.php?latex=L_1%28w%29%2B%5Clambda+L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1(w)+\lambda L_2(w)" class="latex" title="L_1(w)+\lambda L_2(w)" />. The proof is by the picture above: for any point <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" /> on the curve, there is a tangent line at <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" /> that is strictly below the curve. If <img src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a+\lambda b" class="latex" title="a+\lambda b" /> is the normal vector for this line, then the global minimum of <img src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a+\lambda b" class="latex" title="a+\lambda b" /> on the feasible set will be <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" />.<br />This motivates the common practice of minimizing two introducing a hyperparameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" title="\lambda" /> to aggregate two objectives into one.</p>



<p>When <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> is not convex, it may well be that:</p>



<ul><li>Some points on <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> are not minima of <img src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1 + \lambda L_2" class="latex" title="L_1 + \lambda L_2" /></li><li><img src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1 + \lambda L_2" class="latex" title="L_1 + \lambda L_2" /> might have multiple minima</li><li>Depending on the path one takes, it is possible to get “stuck” in a point that is <em>not</em> a global minima</li></ul>



<p>The following figure demonstrates all three possibilities</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Rjg4iZU.png" alt="" /></figure>



<p>Par for the course, this does not stop people in machine learning from using this approach to minimize different objectives, and often “Marley’s Law” holds, and this works fine. But this is not always the case. A <a href="https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/">nice blog post by Degrave and Kurshonova</a> discusses this issue and why sometimes we do in fact, see “Murphy’s law” when we combine objectives. They also detail some other approaches for combining objectives, but there is no single way that will work in all cases.</p>



<p>Figure from Degrave-Kurshonova demonstrating where the algorithm could reach in the non-convex case depending on initialization and <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" title="\lambda" />:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/4VZZaRR.gif" alt="" /></figure>



<h2>Technical digression 2: Distances between probability measures</h2>



<p>Suppose we have two distributions <img src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p,q" class="latex" title="p,q" /> over <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" />. There are two common ways of measuring the distances between them.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/5JTzg6D.png" alt="" /></figure>



<p>The <em>Total Variance (TV)</em> (also known as statistical distance) between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is equal to</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5Cfrac12+%5Csum_%7Bx%5Cin+D%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmax_%7Bf%3AD%5Cto+%7B0%2C1%7D%7D+%7C+%5Cmathbb%7BE%7D_p%28f%29-%5Cmathbb%7BE%7D_q%28f%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." class="latex" title="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." /></p>



<p>The second equality can be proved by constructing <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that outputs 1 on <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> where <img src="https://s0.wp.com/latex.php?latex=p%28x%29-q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)-q(x)" class="latex" title="p(x)-q(x)" /> and vice versa. The <img src="https://s0.wp.com/latex.php?latex=%5Cmax_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_f" class="latex" title="\max_f" /> definition has a crypto-flavored interpretation: For any adversary <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, the TV measures the advantage they can have over half of determining whether <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" /> or <img src="https://s0.wp.com/latex.php?latex=x%5Csim+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim q" class="latex" title="x\sim q" />.</p>



<p>Second, the <em>Kullback–Leibler (KL) Divergence</em> between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is equal to</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%2Fq%28x%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." class="latex" title="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." /></p>



<p>(The total variation distance is symmetric, in the sense that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5CDelta_%7BTV%7D%28q%2Cp%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" class="latex" title="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" />, but the KL divergence is not. Both have the property that they are non-negative and equal to zero if and only if <img src="https://s0.wp.com/latex.php?latex=p%3Dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=q" class="latex" title="p=q" />.)</p>



<p>Unlike the total variation distance, which is bounded between <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> and <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" />, the KL divergence can be arbitrarily large and even infinite (though it can be shown using the concavity of log that it is always non-negative). To interpret the KL divergence, it is helpful to separate between the case that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)" class="latex" title="\Delta_{KL}(p||q)" /> is close to zero and the case where it is a large number. If <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \delta" class="latex" title="\Delta_{KL}(p||q) \approx \delta" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cll+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta \ll 1" class="latex" title="\delta \ll 1" />, then we would need about <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\delta" class="latex" title="1/\delta" /> samples to distinguish between samples of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and samples of <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. In particular, suppose that we get <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> and we want to distinguish between the case that we they were independently sampled from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and the case that they were independently sampled from <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. A natural (and as it turns out, optimal) approach is to use a <em>likelihood ratio test</em> where we decide the samples came from <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> if <img src="https://s0.wp.com/latex.php?latex=%5CPr_p%5Bx_1%2C%5Cldots%2Cx_n%5D%2F%5CPr_q%5Bx_1%2C%5Cldots%2Cx_n%5D%3ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" class="latex" title="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" />. For example, if we set <img src="https://s0.wp.com/latex.php?latex=T%3D20&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T=20" class="latex" title="T=20" /> then this approach will guarantee that our “false positive rate” (announcing that samples came from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> when they really came from <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />) will be most <img src="https://s0.wp.com/latex.php?latex=1%2F20%3D5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/20=5\%" class="latex" title="1/20=5\%" />. Taking logs and using the fact that the probability of these independent samples is the product of probabilities, this amounts to testing whether <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Cleft%28%5Ctfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%5Cright%29+%5Cgeq+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" class="latex" title="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" />. When samples come from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, the expectation of the righthand side is <img src="https://s0.wp.com/latex.php?latex=n%5Ccdot+%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\cdot \Delta_{KL}(p||q)" class="latex" title="n\cdot \Delta_{KL}(p||q)" />, so we see that to ensure <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> is larger than <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> we need the number samples to be at least <img src="https://s0.wp.com/latex.php?latex=1%2F%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\Delta_{KL}(p||q)" class="latex" title="1/\Delta_{KL}(p||q)" /> (and as it turns out, this will do).</p>



<p>When the <img src="https://s0.wp.com/latex.php?latex=KL&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="KL" class="latex" title="KL" /> divergence is a large number <img src="https://s0.wp.com/latex.php?latex=k%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k&gt;1" class="latex" title="k&gt;1" />, we can think of it as the number of bits of “surprise” in <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> as opposed to <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. For example, in the common case where <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is obtained by conditioning <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> on some event <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />, <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)" class="latex" title="\Delta_{KL}(p||q)" /> will typically be <img src="https://s0.wp.com/latex.php?latex=%5Clog+1%2F%5CPr%5BA%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log 1/\Pr[A]" class="latex" title="\log 1/\Pr[A]" /> (some fine print applies). In general, if <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is obtained from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> by revealing <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> bits of information (i.e., by conditioning on a random variable whose mutual information with <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> is <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />) then <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)=k" class="latex" title="\Delta_{KL}(p||q)=k" />.</p>



<p><strong>Generalizations:</strong> The total variation distance is a special case of metrics of the form <img src="https://s0.wp.com/latex.php?latex=%5CDelta%28p%2Cq%29+%3D+%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%7C%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D+f%28x%29+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" class="latex" title="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" />. These are known as <a href="https://arxiv.org/abs/0901.2698">integral probability metrics</a> and include examples such as the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. KL divergence is a special case of divergence measures known as <a href="https://en.wikipedia.org/wiki/F-divergence"><img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />-divergence</a>, which are measures of the form <img src="https://s0.wp.com/latex.php?latex=%5CDelta_f%28p%7C%7Cq%29%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%5Cleft%28%5Ctfrac%7Bp%28x%29%7D%7Bq%28x%29%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" class="latex" title="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" />. The KL divergence is obtained by setting <img src="https://s0.wp.com/latex.php?latex=f%28t%29+%3D+t+%5Clog+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(t) = t \log t" class="latex" title="f(t) = t \log t" />. (In fact even the TV distance is a special case of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> divergence by setting <img src="https://s0.wp.com/latex.php?latex=f%28t%29%3D%7Ct-1%7C%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(t)=|t-1|/2" class="latex" title="f(t)=|t-1|/2" />.)</p>



<p><strong>Normal distributions:</strong> It is a useful exercise to calculate the TV and KL distances for normal random variables. If <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,1)" class="latex" title="p=N(0,1)" /> and <img src="https://s0.wp.com/latex.php?latex=q%3DN%28-%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(-\epsilon,1)" class="latex" title="q=N(-\epsilon,1)" />, then since most probability mass in the regime where <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+%281%5Cpm+%5Cepsilon%29+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \approx (1\pm \epsilon) q(x)" class="latex" title="p(x) \approx (1\pm \epsilon) q(x)" />, <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q) \approx \epsilon" class="latex" title="\Delta_{TV}(p,q) \approx \epsilon" /> (i.e., up to some multiplicative constant). For KL divergence, if we selected <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> from a normal between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> then with probability about half we’ll have <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+q%28x%29%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \approx q(x)(1+\epsilon)" class="latex" title="p(x) \approx q(x)(1+\epsilon)" /> and with probability about half we will have <img src="https://s0.wp.com/latex.php?latex=p%28q%29+%5Capprox+q%28x%29%281-%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(q) \approx q(x)(1-\epsilon)" class="latex" title="p(q) \approx q(x)(1-\epsilon)" />. By selecting <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" />, we increase probability of the former to <img src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 1/2+\epsilon" class="latex" title="\approx 1/2+\epsilon" /> and the decrease the probability of the latter to <img src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 1/2 - \epsilon" class="latex" title="\approx 1/2 - \epsilon" />. So we have <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" title="\epsilon" /> bias towards <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />‘s where <img src="https://s0.wp.com/latex.php?latex=p%28x%29%2Fq%28x%29+%5Capprox+1%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)/q(x) \approx 1+\epsilon" class="latex" title="p(x)/q(x) \approx 1+\epsilon" />, or <img src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29%2Fq%28x%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log p(x)/q(x) \approx \epsilon" class="latex" title="\log p(x)/q(x) \approx \epsilon" />. Hence <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \epsilon^2" class="latex" title="\Delta_{KL}(p||q) \approx \epsilon^2" />. The above generalizes to higher dimensions. If <img src="https://s0.wp.com/latex.php?latex=p%3D+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p= N(0,I)" class="latex" title="p= N(0,I)" /> is a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />-variate normal, and <img src="https://s0.wp.com/latex.php?latex=q%3DN%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(\mu,I)" class="latex" title="q=N(\mu,I)" /> for <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in \mathbb{R}^d" class="latex" title="\mu \in \mathbb{R}^d" />, then (for small <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mu|" class="latex" title="|\mu|" />) <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q) \approx |\mu|" class="latex" title="\Delta_{TV}(p,q) \approx |\mu|" /> while <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%5Capprox+%7C%5Cmu%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)\approx |\mu|^2" class="latex" title="\Delta_{KL}(p||q)\approx |\mu|^2" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/w3aXr99.png" alt="" /></figure>



<p>If <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,1)" class="latex" title="p=N(0,1)" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is a “narrow normal” of the form <img src="https://s0.wp.com/latex.php?latex=q%3DN%280%2C%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(0,\epsilon^2)" class="latex" title="q=N(0,\epsilon^2)" /> then their TV distance is close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> while <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+1%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx 1/\epsilon^2" class="latex" title="\Delta_{KL}(p||q) \approx 1/\epsilon^2" />. In the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> dimensional case, if <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,I)" class="latex" title="p=N(0,I)" /> and <img src="https://s0.wp.com/latex.php?latex=q%3DN%280%2CV%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(0,V)" class="latex" title="q=N(0,V)" /> for some covariance matrix <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V" class="latex" title="V" />, then <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cmathrm%7BTr%7D%28V%5E%7B-1%7D%29+-+d+%2B+%5Cln+%5Cdet+V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" class="latex" title="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" />. The two last terms are often less significant. For example if <img src="https://s0.wp.com/latex.php?latex=V+%3D+%5Cepsilon%5E2+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V = \epsilon^2 I" class="latex" title="V = \epsilon^2 I" /> then <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+d%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta_{KL}(p||q) \approx d/\epsilon^2" class="latex" title="\delta_{KL}(p||q) \approx d/\epsilon^2" />.</p>



<h2>Technical digression 3: benchmarking generative models</h2>



<p>Given a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> of natural data and a purported generative model <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />, how do we measure the quality of <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />?</p>



<p>A natural measure is the KL divergence <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||g)" class="latex" title="\Delta_{KL}(p||g)" /> but it can be hard to evaluate, since it involves the term <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" title="p(x)" /> which we cannot evaluate. However, we can rewrite the KL divergence as <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%29+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+q%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" class="latex" title="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" />. The term <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%5Csim+p%7D+%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}{x\sim p} \log p(x)" class="latex" title="\mathbb{E}{x\sim p} \log p(x)" /> is equal to <img src="https://s0.wp.com/latex.php?latex=-H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-H(p)" class="latex" title="-H(p)" /> where <img src="https://s0.wp.com/latex.php?latex=H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p)" class="latex" title="H(p)" /> is the <em>entropy</em> of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. The term <img src="https://s0.wp.com/latex.php?latex=-%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\mathbb{E}_{x \sim p} \log q(x)" class="latex" title="-\mathbb{E}_{x \sim p} \log q(x)" /> is known as the <em>cross entropy </em>of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. Note that the cross-entropy of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is simply the expectation of the negative log likelihood of <img src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x)" class="latex" title="g(x)" /> for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> sampled from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />.</p>



<p>When <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> is fixed, minimizing <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||g)" class="latex" title="\Delta_{KL}(p||g)" /> corresponds to minimizing the cross entropy <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,g)" class="latex" title="H(p,g)" /> or equivalently, maximizing the log likelihood. This is useful since often is the case that we can sample elements from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> (e.g., natural images) but can only evaluate the probability function for <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />. Hence a common metric in such cases is minimizing the cross-entropy / negative log likelihood <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29%3D+-%5Cmathbb%7BE%7D_%7Bx+sim+p%7D+%5Clog+g%28x%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+%281%2Fg%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" class="latex" title="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" />. For images, a common metric is “bits per pixel” which simply equals <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29%2Fd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,q)/d" class="latex" title="H(p,q)/d" /> where <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the length of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. Another metric (often used in natural language processing) is perplexity, which interchanges the expectation and the logarithm. The logarithm of the perplexity of <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is <img src="https://s0.wp.com/latex.php?latex=-+%5Ctfrac%7B1%7D%7Bd%7D%5Clog+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" class="latex" title="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" /> where <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the length of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> (e.g., in tokens). Another way to write this is that log of the perplexity is the average of <img src="https://s0.wp.com/latex.php?latex=%5Clog+g%28x_i%7Cx%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log g(x_i|x{&lt;i})" class="latex" title="\log g(x_i|x{&lt;i})" /> where <img src="https://s0.wp.com/latex.php?latex=g%28x_i%7Cx_%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x_i|x_{&lt;i})" class="latex" title="g(x_i|x_{&lt;i})" /> is the probability of <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> under <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> conditioned on the first <img src="https://s0.wp.com/latex.php?latex=i-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i-1" class="latex" title="i-1" /> parts of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />.</p>



<p><strong>Memorization for log-likelihood.</strong> The issue of “overfitting” is even more problematic for generative models than for classifiers. Given samples <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> and enough parameters, we can easily come up with a model corresponding to the uniform distribution <img src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_1,\ldots, x_n }" class="latex" title="{ x_1,\ldots, x_n }" />. This is obviously a useless model that will never generate new examples. However, this model will not only get a large log likelihood value on the training set, in fact, it will get <em>even better log likelihood</em> than the true distribution! For example, any reasonable natural distribution on images would have at least tens of millions, if not billions or trillions of potential images. In contrast, a typical training set might have fewer than 1M samples. Hence, unlike in the classification setting, for generation, the “overfitting” model will not only match but can, in fact, beat the ground truth. (This is reminiscent of the following quote from <a href="https://etc.usf.edu/lit2go/86/peter-pan/1602/chapter-12-the-children-are-carried-off/">Peter and Wendy</a>: <em>“Not a sound is to be heard, save when they give vent to a wonderful imitation of the lonely call of the coyote. The cry is answered by other braves; and some of them do it even better than the coyotes, who are not very good at it.”</em>)</p>



<p>If we cannot compute the density function, then benchmarking becomes more difficult. What often happens in practice is an “I know it when I see it” approach. The paper includes a few pictures generated by the model, and if the pictures look realistic, we think it is a good model. However, this can be deceiving. After all, we are feeding in good pictures into the model, so generating a good photo may not be particularly hard (e.g. the model might memorize some good pictures and use those as outputs).</p>



<p>There is another metric called the <em>inception score</em>, which loosely corresponds to how similar the “inception” neural network finds the GAN model to ImageNet (in the sense that inception thinks it covers many of the ImageNet classes and that produces images on which inception has high confidence)  but it too has its problems. <a href="https://arxiv.org/pdf/1905.10887.pdf">Ravuri-Vinyalis 2019</a> used a GAN model with a good inception score used its outputs to train a different model on ImageNet. Despite the high inception score (which should have indicated that the GANs output are as good as ImageNets) the accuracy when training on the GAN output dropped from the original value of <img src="https://s0.wp.com/latex.php?latex=74%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="74\%" class="latex" title="74\%" /> to as low as <img src="https://s0.wp.com/latex.php?latex=5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="5\%" class="latex" title="5\%" />!  (Even in the best case, accuracy dropped by at least 30 points.) Compare this with the  11-14% drop when we train on ImageNet and test on <a href="https://arxiv.org/abs/1902.10811">ImageNet v2</a>.</p>



<p>This figure from <a href="https://arxiv.org/abs/1701.00160">Goodfellow’s tutorial</a> describes generative models where we know and don’t know how to compute the density function:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/3hVJBPl.png" alt="" /></figure>



<h1>Auto Encoder / Decoder</h1>



<p>We now shift our attention to the encoder/decoder architecture mentioned above.</p>



<p>Recall that we want to understand <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, generate new elements <img src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^*" class="latex" title="x^*" />, and find a good representation of the elements. Our dream is to solve all of the issues with auto encoder/decoder, whose setup is as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/udIY089.png" alt="Setup for Auto Encoder/Decoder" /></figure>



<p>That is, we want <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" />, <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" /> such that</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=D%28E%28x%29%29+%5Capprox+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(E(x)) \approx x" class="latex" title="D(E(x)) \approx x" /></li><li>The representation <img src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E,D" class="latex" title="E,D" /> enables us to solve tasks such as generation, classification, etc..</li></ul>



<p>To each the first point, we can aim to minimize <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_i ||x_i - D(E(x_i))||^2" class="latex" title="\sum_i ||x_i - D(E(x_i))||^2" />. However, we can of course, make this loss zero by letting <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> be the identity function. Much of the framework of generative models can be considered as placing some restrictions on the “communication channel” that rule out this trivial approach, with the hope that would require the encoder and decoder to “intelligently” correspond to the structure of the natural data.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/XW6cDE7.png" alt="" /></figure>



<h2>Auto Encoders: noiseless short <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /></h2>



<p>A natural idea is to simply restrict the dimension of the latent space to be small (<img src="https://s0.wp.com/latex.php?latex=r+%5Cll+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r \ll d" class="latex" title="r \ll d" />). In principle, the optimal compression scheme for a probability distribution will require knowing the distribution. Moreover, the optimal compression will maximize the entropy of the latent data <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" />. Since the maximum entropy distribution is uniform (in the discrete case), we could easily sample from it. (In the continuous setting, the standard normal distribution plays the role of the uniform distribution.)</p>



<p>For starter, consider the case of picking <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> to be small and minimizing <img src="https://s0.wp.com/latex.php?latex=%5Csum+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum ||x_i - D(E(x_i))||^2" class="latex" title="\sum ||x_i - D(E(x_i))||^2" /> for <em>linear</em> <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" />, <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" />. Since <img src="https://s0.wp.com/latex.php?latex=DE&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="DE" class="latex" title="DE" /> is a rank <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> matrix, we can write this as finding a rank <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> matrix <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> that minimizes <img src="https://s0.wp.com/latex.php?latex=%7C+%28I-L%29X%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| (I-L)X|^2" class="latex" title="| (I-L)X|^2" /> where <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X = (x_1,\ldots,x_n)" class="latex" title="X = (x_1,\ldots,x_n)" /> is our input data. It can be shown that <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> that would minimize this will be the projection to the top <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> eigenvectors of <img src="https://s0.wp.com/latex.php?latex=XX%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="XX^\top" class="latex" title="XX^\top" /> which exactly corresponds to <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>



<p>In the nonlinear case, we can obtain better compression. However, we do not achieve our other goals:</p>



<ul><li>It is not the case that we can generate realistic data by sampling uniform/normal <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> and output <img src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)" class="latex" title="D(z)" /></li><li>It is not the case that semantic similarity between <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" /> corresponds to large dot product between <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> and <img src="https://s0.wp.com/latex.php?latex=E%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x')" class="latex" title="E(x')" />.</li></ul>



<p>It seems that model just rediscovers a compression algorithm like JPEG. We do not expect the JPEG encoding of an image to be semantically informative, and JPEG decoding of a random file will not be a good way to generate realistic images. It turns out that sometimes “Murphy’s law” does hold and if it’s possible to minimize the loss in a not very useful way then that will indeed be the case.</p>



<h2>Variational Auto Encoder (VAE)</h2>



<p>We now discuss <em>variational auto encoders</em> (VAEs). We can think of these as generalization auto-encoders to the case where the channel has some Gaussian noise. We will describe VAEs in two nearly equivalent ways:</p>



<ul><li>We can think of VAEs as trying to optimize two objectives: both the auto-encoder objective of minimizing <img src="https://s0.wp.com/latex.php?latex=%7C+D%28E%28x%29%29-x%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| D(E(x))-x|^2" class="latex" title="| D(E(x))-x|^2" /> and another objective of minimizing the KL divergence between <img src="https://s0.wp.com/latex.php?latex=D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(x)" class="latex" title="D(x)" /> and the standard normal distribution <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" />.</li><li>We can think of VAEs as trying to maximize a proxy for the log-likelihood. This proxy is a quantity known as the “Evidence Lower Bound (ELBO)” which we can evaluate using <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> and <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and is always smaller or equal to the log-likelihood.</li></ul>



<p>We start with the first description. One view of VAEs is that we search for a pair <img src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E,D" class="latex" title="E,D" /> of encoder and decoder that are aimed at minimizing the following two objectives:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%7C+x+-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| x - D(E(x))|^2" class="latex" title="| x - D(E(x))|^2" /> (standard AE objective)</li><li><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28+E%28x%29+%7C%7C+N%280%2CI%29+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}( E(x) || N(0,I) )" class="latex" title="\Delta_{KL}( E(x) || N(0,I) )" /> (distance of latent from the standard normal)</li></ul>



<p>To make the second term a function of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, we consider <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> as a probability distribution with respect to a <em>fixed</em> <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. To ensure this makes sense, we need to make <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> <em>randomized</em>. A randomized Neural network has “sampling neurons” that take no input, have parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmu%2C%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu,\sigma" class="latex" title="\mu,\sigma" /> and produce an element <img src="https://s0.wp.com/latex.php?latex=v+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v \sim N(\mu,\sigma^2)" class="latex" title="v \sim N(\mu,\sigma^2)" />. We can train such a network by fixing a random <img src="https://s0.wp.com/latex.php?latex=t+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \sim N(0,1)" class="latex" title="t \sim N(0,1)" /> and defining the neuron to simply output <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%2B+%5Csigma+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu + \sigma t" class="latex" title="\mu + \sigma t" />.</p>



<p><strong>ELBO derivation:</strong> Another view of VAEs is that they aim at maximizing a term known as the evidence lower bound or ELBO. We start by deriving this bound. Let <img src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z=N(0,I)" class="latex" title="Z=N(0,I)" /> be the standard normal distribution over the latent space. Define <img src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x" class="latex" title="p_x" /> to be the distribution of <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> conditioned on <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> decoding to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> (i.e., <img src="https://s0.wp.com/latex.php?latex=Z%3D+z%5Csim+Z%7CD%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z= z\sim Z|D(z)=x" class="latex" title="Z= z\sim Z|D(z)=x" />, and define <img src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x" class="latex" title="q_x" /> be the distribution <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" />. Since <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28q_x%7C%7Cp_x%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(q_x||p_x) \geq 0" class="latex" title="\Delta_{KL}(q_x||p_x) \geq 0" />, we know that</p>



<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+p_x%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" class="latex" title="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" /></p>



<p>By the definition of <img src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x" class="latex" title="p_x" />, <img src="https://s0.wp.com/latex.php?latex=p_x%28z%29+%3D+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2F+%5CPr%5BD%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" class="latex" title="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" />. Hence we can derive that</p>



<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29+-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" class="latex" title="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" /><br />(since <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr[ D(Z)=x]" class="latex" title="\Pr[ D(Z)=x]" /> depends only on <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, given that <img src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z=N(0,I)" class="latex" title="Z=N(0,I)" />.)</p>



<p>Rearranging, we see that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" class="latex" title="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" /></p>



<p>or in other words, we have the following theorem:</p>



<p><strong>Theorem (ELBO):</strong> For every (possibly randomized) maps <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathcal{X} \rightarrow \mathcal{Z}" class="latex" title="E:\mathcal{X} \rightarrow \mathcal{Z}" /> and <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathcal%7BZ%7D+%5Crightarrow+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathcal{Z} \rightarrow \mathcal{X}" class="latex" title="D:\mathcal{Z} \rightarrow \mathcal{X}" />, distribution <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{Z}" class="latex" title="\mathcal{Z}" /> and <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in \mathcal{X}" class="latex" title="x\in \mathcal{X}" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5CPr_%7Bz+%5Csim+E%28x%29%2C+z%27++%5Csim+Z%7D%5B+D%28z%29+%3D+x+%5Cwedge+z%3Dz%27+%5D+%2B+H%28E%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" class="latex" title="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" /></p>



<p>The left-hand side of this inequality is simply the log-likelihood of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. The right-hand side (which, as the inequality shows, is always smaller or equal to it) is known as the <em>evidence lower bound</em> or ELBO. We can think of VAEs as trying to maximize the ELBO.</p>



<p>The reason that the two views are roughly equivalent is the follows:</p>



<ul><li>The first term of the ELBO, known as the <em>reconstruction term</em>, is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" class="latex" title="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" /> if we assume some normal noise, then the probabiility taht <img src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)=x" class="latex" title="D(z)=x" /> will be proportional to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%7Cx-D%28z%29%7C%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-|x-D(z)|^2)" class="latex" title="\exp(-|x-D(z)|^2)" /> since for <img src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x" class="latex" title="q_x" />, <img src="https://s0.wp.com/latex.php?latex=z%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z=E(x)" class="latex" title="z=E(x)" /> we get that <img src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%5Capprox+-%7C+x-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" class="latex" title="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" /> and hence maximizing this term corresponds to minimizing the square distance.</li><li>The second term of the ELBO, known as the <em>divergence term</em>, is <img src="https://s0.wp.com/latex.php?latex=H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(q_x)" class="latex" title="H(q_x)" /> which is roughly equal to <img src="https://s0.wp.com/latex.php?latex=r+-%5CDelta_%7BKL%7D%28q_x%7C%7CN%280%2CI%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r -\Delta_{KL}(q_x||N(0,I))" class="latex" title="r -\Delta_{KL}(q_x||N(0,I))" />, where <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> is the dimension of the latent space. Hence maximizing this term corresponds to minimizing the KL divergence between <img src="https://s0.wp.com/latex.php?latex=q_x%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x=E(x)" class="latex" title="q_x=E(x)" /> and the standard normal distribution.</li></ul>



<p>How well does VAE work? First of all, we can actually generate images using them. We also find that similar inputs will have similar encodings, which is good. However, sometimes VAEs can still “cheat” (as in auto encoders). There is a risk that the learned model will split <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> to two parts of the form <img src="https://s0.wp.com/latex.php?latex=%28N%280%2CI%29%2C+JPEG%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(N(0,I), JPEG(x))" class="latex" title="(N(0,I), JPEG(x))" />. The first part of the data is there to minimize divergence, while the second part is there for reconstruction. Such a model is similarly uninformative.</p>



<p>However, VAEs have found practical success. For example, <a href="https://arxiv.org/pdf/1610.00291.pdf">Hou et. al 2016</a> used VAE to create an encoding where two dimensions seem to correspond to “sunglasses” and “blondness”, as illustrated below. We do note that “sunglasses” and “blondness” are somewhere between “semantic” and “syntactic” attributes. They do correspond to relatively local changes in “pixel space”.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/O48nnWB.jpg" alt="VAE Example 1" /></figure>



<p>The picture can be blurry because of the noise we injected to make <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> random. However, recent models have used new techniques (e.g. <a href="https://arxiv.org/abs/1906.00446">vector quantized VAE</a> and <a href="https://arxiv.org/abs/2007.03898">hierarchical VAE</a>) to resolve the blurriness and significantly improve on state of art.</p>



<h2>Flow Models</h2>



<p>In a flow model, we flip the order of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> and <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and set <img src="https://s0.wp.com/latex.php?latex=E%3DD%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E=D^{-1}" class="latex" title="E=D^{-1}" /> (so <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> must be invertible). The input <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> to <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> will come from the standard normal distribution <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" />. The idea is that we obtain <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> by a composition of simple invertible functions. We use the fact that if we can compute the density function of a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" title="\mathbb{R}^d" /> and <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" class="latex" title="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" /> is invertible and differentiable, then we can compute the density function of <img src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\circ p" class="latex" title="f\circ p" /> (i.e., the distribution obtained by sampling <img src="https://s0.wp.com/latex.php?latex=w+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w \sim p" class="latex" title="w \sim p" /> and outputting <img src="https://s0.wp.com/latex.php?latex=f%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(w)" class="latex" title="f(w)" />). To see why this is the case, consider the setting when <img src="https://s0.wp.com/latex.php?latex=d%3D2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=2" class="latex" title="d=2" /> and a small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Ctimes+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta \times \delta" class="latex" title="\delta \times \delta" /> rectangle <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />. If <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> is small enough, <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> will be roughly linear and hence will map <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> into a parallelogram <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" />. Shifting the <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> coordinate by <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> corresponds to shifting the output of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by the vector <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdx%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" class="latex" title="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" /> and shifting the <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> coordinate by <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> corresponds to shifting the output of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by the vector <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdy%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdy%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" class="latex" title="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" />. For every <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z \in B" class="latex" title="z \in B" />, the density of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> under <img src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\circ p" class="latex" title="f\circ p" /> will be proportional to the density of <img src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f^{-1}(z)" class="latex" title="f^{-1}(z)" /> with the proportionality fector being <img src="https://s0.wp.com/latex.php?latex=vol%28A%29%2Fvol%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="vol(A)/vol(B)" class="latex" title="vol(A)/vol(B)" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/pet8tBU.png" alt="" /></figure>



<p>Overall we the density of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> under <img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \circ p" class="latex" title="f \circ p" /> will equal <img src="https://s0.wp.com/latex.php?latex=p%28f%5E%7B-1%7D%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(f^{-1}(z))" class="latex" title="p(f^{-1}(z))" /> times the inverse determinant of the <em>Jacobian</em> of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> at the point <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ItOPqSX.png" alt="" /></figure>



<p>There are different ways to compose together simple reversible functions to compute a complex one. Indeed, this issue also arises in cryptography and quantum computing (e.g., the <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Fiestel cipher</a>). Using similar ideas, it is not hard to show that any probability distribution can be approximated by a (sufficiently big) combination of simple reversible functions.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/AlYrWJx.png" alt="" /></figure>



<p>In practice, we have some recent succcessful flow models. A few examples of these models are in the lecture slides.</p>



<h1>Giving up on the dream</h1>



<p>In section 2, we had a dream of doing both representation and generation at once. So far, we have not been able to find success with these models. What if we do each goal separately?</p>



<p>The tasks of representation becomes self-supervised learning with approaches such SIMCLR. The task of generation can be solved by GANs. Both areas have had recent success.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/D0CpobJ.jpg" alt="Model after we separate E and D" /></figure>



<p>Open-AI <a href="https://openai.com/blog/clip">CLIP</a> and <a href="https://openai.com/blog/dall-e/">DALL-E</a> is a pair of models that perform each part of these tasks well, and suggest an approach to merge them.<br />CLIP does representation for both texts and images where the two encoders are aligned, i.e. <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28%5Ctext%7B%27cat%27%7D%29%2C+E%28%5Ctext%7Bimg+of+cat%29%7D%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" class="latex" title="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" /> is large. DALL-E, given some text, generates an image corresponding to the text. Below are images generated by DALL-E when asked for an armchair in the shape of an avocado.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZcsHXKE.png" alt="DALL-E Example" /></figure>



<h2>Contrastive learning</h2>



<p>The general approach used in CLIP is called contrastive learning.</p>



<p>Suppose we have some representation function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and inputs <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" /> which represent similar objects. Let <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D%3Df%28u_i%5Ccdot+v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,j}=f(u_i\cdot v_j)" class="latex" title="M_{i,j}=f(u_i\cdot v_j)" />, then we want <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,j}" class="latex" title="M_{i,j}" /> to be large when <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />, but small when <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />. So, let the loss function be <img src="https://s0.wp.com/latex.php?latex=L%28M%29%3D%5Csum+M_%7Bi%2Ci%7D+%2F+%5Csum_%7Bi%5Cneq+j%7D+M_%7Bi%2Cj%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." class="latex" title="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." /> How do we create similar <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" />? In SIMCLR, <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" /> are augmentations of the same image <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" />. In CLIP, <img src="https://s0.wp.com/latex.php?latex=%28u_i%2Cv_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(u_i,v_i)" class="latex" title="(u_i,v_i)" /> is an image and a text that describes it.</p>



<p>CLIPs representation space does seem to have nice properties such as correspondence between semantic attributes and linear directions, which enables doing some “semantic linear algebra” on representations: (see this based on <a href="https://github.com/haltakov/natural-language-image-search">Vladimir Hatlakov’s code</a> – in the snippet below <code>tenc</code> maps text to its encoding/representation and <code>get_img</code> finds nearest image to representation in a the unsplash dataset):</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-6.png"><img src="https://windowsontheory.files.wordpress.com/2021/02/image-6.png?w=854" alt="" class="wp-image-8017" /></a></figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-7.png"><img src="https://windowsontheory.files.wordpress.com/2021/02/image-7.png?w=444" alt="" class="wp-image-8019" /></a></figure>



<h2>GANs</h2>



<p>The theory of GANs is currently not well-developed. As an objective, we want images that “look real” (which is not well defined), and we have no posterior distribution. If we just define the distribution based on real images, our GAN might memorize the photos to beat us.</p>



<p>However, we know that Neural Networks are good at discriminating real vs. fake images. So, we add in a discriminator <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and define the loss function <img src="https://s0.wp.com/latex.php?latex=L%28D%29+%3D+%5Cmax_%7Bf%3A%5Cmathbb+R%5Ed%5Cto+%5Cmathbb+R%7D+%7C%5Cmathbb%7BE%7D_%7B%5Chat+x%5Csim+D%28z%29%7Df%28%5Chat+x%29-%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7Df%28x%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." class="latex" title="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." /></p>



<p>The generator model and discriminator model form a 2-player game, which are often harder to train and very delicate. We typically train by changing a player’s action to the best response. However, we need to be careful if the two players have very different skill levels. They may be stuck in a setting where no change of strategies will make much difference, since the stronger player always dominates the weaker one. In particular in GANs we need to ensure that the generator is not cheating by using a degenerate distribution that still succeeds with respect to the discriminator.</p>



<p>If a 2-player model makes training more difficult, why do we use it? If we fix the discriminator, then the generator can find a picture that the discriminator thinks is real and only output that one, obtaining low loss. As a result, the discriminator needs to update along with the generator. This example also highlights that the discriminator’s job is often harder. To fix this, we have to somehow require the generator to give us good entropy.</p>



<p>Finally, how good are GANs in practice? Recently, we have had GANs that make great images as well as audios. For example, modern deepfake techniques often use GANs in their architecture. However, it is still unclear how rich the images are.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/"><span class="datestr">at February 24, 2021 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18187">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/">A Quiz of Quotes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantormathquote/" rel="attachment wp-att-18201"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2021/02/cantormathquote.jpg?w=150&amp;h=150" class="alignright wp-image-18201" height="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">MathQuotes <a href="https://www.facebook.com/mathsqoutes/posts/the-mathematician-does-not-study-pure-mathematics-because-it-is-useful-he-studie/144012037239958/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
George Cantor has been featured <a href="https://rjlipton.wordpress.com/2014/07/31/the-cantor-bernstein-schroder-theorem/">here</a> and <a href="https://rjlipton.wordpress.com/2009/04/18/cantors-non-diagonal-proof/">here</a> and <a href="https://rjlipton.wordpress.com/2012/09/04/thinking-out-of-the-notation-box/">here</a> before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was born on March 3rd in 1845.</p>
<p>
Today we thought it might be fun to have a quiz on math quotes.</p>
<p>
Wait. Cantor did not invent quotation marks, nor is he known for many quotes. He does of course have many famous results, and they will live forever. But his results were subject to immediate horrible criticism and therefore memorable quotes. </p>
<p>Leopold Kronecker was a particular source of barbs.  For example: “What good is your beautiful proof on the transcendence of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" title="{\pi}" />? Why investigate such problems, given that irrational numbers do not even exist?” </p>
<p>
As a complexity theorist I must say that Kronecker has a point when he also said: </p>
<blockquote><p><b> </b> <em> “Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any degree of accuracy.” </em>
</p></blockquote>
<p>David Hilbert defended Cantor and said: “No one shall expel us from the paradise that Cantor has created.”</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantor/" rel="attachment wp-att-18190"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/02/cantor.png?w=600&amp;h=205" class="aligncenter wp-image-18190" height="205" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BBVA Open Mind <a href="https://www.bbvaopenmind.com/en/science/mathematics/georg-cantor-the-man-who-discovered-different-infinities/">src</a>
</font></td>
</tr>
</tbody></table>
<p></p><h2> Quotes Quiz </h2><p></p>
<p></p><p>
On to the quiz. Each quote is followed by two possible authors in alphabetical order. You should pick the one you think is correct. The players are: </p>
<blockquote><p><b> </b> <em> 1. Douglas Adams  2. Bernard Baruch  3. Eric Temple Bell  4. Raoul Bott<br />
5. Paul Erdős  6. Richard Hamming  7. Godfrey Hardy  8. David Hilbert<br />
9. Admiral Grace Hooper  10. Alan Kay  11. Donald Knuth  12. John von Neumann<br />
13. Alan Perlis  14. Henri Poincaré  15. Srinivasa Ramanujan  16. Marcus du Sautoy<br />
17. Raymond Smullyan  18. Alan Turing  19. Moshe Vardi  20. Andrew Wiles<br />
</em>
</p></blockquote>
<p>
</p><ol>
<p></p><li>
Those who can imagine anything, can create the impossible.<br />
—Kay <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Turing<p></p>
<p></p></li><li>
I really didn’t foresee the Internet. But then, neither did the computer industry. Not that that tells us very much of course–the computer industry didn’t even foresee that the century was going to end.<br />
— Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Knuth<p></p>
<p></p></li><li>
One man’s constant is another man’s variable.<br />
—Perlis <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> du Sautoy<p></p>
<p></p></li><li>
The most damaging phrase in the language is: “It’s always been done that way.”<br />
—-Hopper <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Perlis<p></p>
<p></p></li><li>
The best way to predict the future is to invent it.<br />
—Kay <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Turing<p></p>
<p></p></li><li>
The purpose of computing is insight, not numbers.<br />
Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Hamming<p></p>
<p></p></li><li>
Beware of bugs in the above code; I have only proved it correct, not tried it.<br />
—Knuth <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Vardi<p></p>
<p></p></li><li>
No, it is a very interesting number, it is the smallest number expressible as a sum of two cubes in two different ways.<br />
—Bell <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Ramanujan <p></p>
<p></p></li><li>
Beauty is the first test: there is no permanent place in the world for ugly mathematics. <br />
—Erdős <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Hardy<p></p>
<p></p></li><li>
Mathematics is the art of giving the same name to different things. <br />
—Hooper <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Poincaré <p></p>
<p></p></li><li>
There’s no sense in being precise when you don’t even know what you’re talking about.<br />
—Bott <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> von Neumann<p></p>
<p></p></li><li>
I hope we’ll be able to solve these problems before we leave. <br />
—Erdős <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Perlis<p></p>
<p></p></li><li>
Some people are always critical of vague statements. I tend rather to be critical of precise statements; they are the only ones which can correctly be labeled ‘wrong’. <br />
—Knuth <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Smullyan<p></p>
<p></p></li><li>
Everything that humans can do a machine can do. <br />
—Perlis <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Vardi<p></p>
<p></p></li><li>
“Obvious” is the most dangerous word in mathematics.<br />
— Bell <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Hooper<p></p>
<p></p></li><li>
Just because we can’t find a solution, it doesn’t mean there isn’t one.<br />
— Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Wiles<p></p>
<p></p></li><li>
Mathematics is a place where you can do things which you can’t do in the real world.<br />
— du Sautoy <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Turing<p></p>
<p></p></li><li>
Millions saw the apple fall, but Newton asked why.<br />
— Baruch <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Hopper<p></p>
<p></p></li><li>
The definition of a good mathematical problem is the mathematics it generates rather than the problem itself.<br />
— Hilbert <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Wiles<p></p>
<p></p></li><li>
There are two ways to do great mathematics. The first is to be smarter than everybody else. The second way is to be stupider than everybody else – but persistent.<br />
— Bott <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" title="{||}" /> Knuth<p></p>
</li></ol>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
“I always have a quotation for everything—it saves original thinking.”<br />
—Dorothy Sayers</p>
<p>
Here are the answers:</p>
<p><a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/ans/" rel="attachment wp-att-18194"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/02/ans.png?w=600&amp;h=702" class="alignright size-full wp-image-18194" height="702" /></a></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/"><span class="datestr">at February 24, 2021 05:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/027">TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/027"><span class="datestr">at February 24, 2021 02:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5350">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5350">Stop emailing my utexas address</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve this problem.  I can once again easily read emails sent to aaronson@cs.utexas.edu … well, at least for now!  I’m now checking about aaronson@utexas.edu.  Again, though, <strong>scott@scottaaronson.com to be safe</strong>.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5350"><span class="datestr">at February 23, 2021 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/026">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/026">TR21-026 |  Conditional Dichotomy of Boolean Ordered Promise CSPs | 

	Joshua Brakensiek, 

	Venkatesan Guruswami, 

	Sai Sandeep</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Promise Constraint Satisfaction Problems (PCSPs) are a generalization of Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a weak form and given a CSP instance, the objective is to distinguish if the strong form can be satisfied vs. even the weak form cannot be satisfied. Since their formal introduction by Austrin, Guruswami, and Håstad, there has been a flurry of works on PCSPs, including recent breakthroughs in approximate graph coloring. The key tool in studying PCSPs is the algebraic framework developed in the context of CSPs where the closure properties of the satisfying solutions known as *polymorphisms* are analyzed. 
    
    The polymorphisms of PCSPs are significantly richer than CSPs---this is illustrated by the fact that even in the Boolean case, we still do not know if there exists a dichotomy result for PCSPs analogous to Schaefer's dichotomy result for CSPs. In this paper, we study a special case of Boolean PCSPs, namely Boolean *Ordered* PCSPs where the Boolean PCSPs have the predicate $x \leq y$. In the algebraic framework, this is the special case of Boolean PCSPs when the polymorphisms are *monotone* functions. We prove that Boolean Ordered PCSPs exhibit a computational dichotomy assuming the Rich $2$-to-$1$ Conjecture due to Braverman, Khot, and Minzer, which is a perfect completeness surrogate of the Unique Games Conjecture. 
    
    In particular, assuming the Rich $2$-to-$1$ Conjecture, we prove that a Boolean Ordered PCSP can be solved in polynomial time if for every $\epsilon &gt;0$, it has polymorphisms where each coordinate has *Shapley value* at most $\epsilon$, else it is NP-hard. The algorithmic part of our dichotomy result is based on a structural lemma showing that Boolean monotone functions with each coordinate having low Shapley value have arbitrarily large threshold functions as minors. The hardness part proceeds by showing that the Shapley value is consistent under a uniformly random $2$-to-$1$ minor. As a structural result of independent interest, we construct an example to show that the Shapley value can be inconsistent under an adversarial $2$-to-$1$ minor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/026"><span class="datestr">at February 23, 2021 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6398537358110172858">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html">Good Names and Bad Names of Game Shows and theorems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In my post on Alex Trebek, see <a href="https://blog.computationalcomplexity.org/2020/11/alex-trebekwhat-is-todays-post-about.html">here</a>, I noted that <i>Jeopardy!</i> is not a good name for the game show since it doesn't tell you much about the show. Perhaps <i>Answers and Questions </i>is a better name.</p><p>The following game shows have names that tell you something about the game and hence have better names: </p><p>Wheel of Fortune, The Price is Right, Lets make a Deal, Beautiful women have suitcases full of money (the original name for Deal-No Deal), Win Ben Stein's Money, Beat the Geeks. </p><p>In Math we often name a concept  after a person. While this may be a good way to honor someone, the name does not tell us much about the concept and it leads to statements like:</p><p><br /></p><p><i>A Calabi-Yau manifold is a compact complex Kahler manifold with a trivial first Chern class. </i></p><p><i>A Kahler manifold is a Hermitian manifold for which the Hermitian form is closed.</i></p><p><i>A Hermitian manifold is the complex analog of the Riemann manifold. </i></p><p>(These examples are from an article I will point to later---I do not understand <i>any </i>of these terms, though I once knew what a <i>Riemann manifold</i> was. I heard the term <i>Kahler Manifold </i>in the song <a href="https://www.youtube.com/watch?v=2rjbtsX7twc">Bohemian Gravity</a>.  It's at about the 4 minute 30 second place.) </p><p>While I am amused by the name <i>Victoria Delfino Problems</i> (probably the only realtor who has problems in math named after her, see my post <a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">here</a>) it's not a descriptive way to name open problems in descriptive set theory. </p><p><br /></p><p>Sometimes  a name becomes SO connected to a concept that it IS descriptive, e.g.:</p><p><i>The first proof of VDW's theorem yields ACKERMAN-LIKE bounds. </i></p><p>but you cannot count on that happening AND it is only descriptive to people already somewhat in the field. </p><p><br /></p><p>What to do? <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">This</a> article makes the  ballian point that we should   STOP DOING THIS and that the person who first proves the theorem should name it in a way that tells you something about the concept. I would agree. But this can still be hard to really do.</p><p><br /></p><p>In my book on Muffin Mathematics (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>) I have a sequence of methods called</p><p>Floor Ceiling, Half, Mid, Interval, Easy-Buddy-Match, Hard-Buddy-Match, Gap, Train. </p><p>There was one more method that I didn't quite name, but I used the phrase `Scott Muffin Problem' to honors Scott Huddleton who came up with the method, in my description of it. </p><p>All but the last concept were given ballian names.  Even so, you would need to read the book to see why the names make sense. Still, that would be easier than trying to figure out what a Calabi-Yau manifold is. </p><p><br /></p><p></p><br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html"><span class="datestr">at February 23, 2021 05:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/">Riemann Hypothesis—Why So Hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/voroninsteklov/" rel="attachment wp-att-18177"><img width="142" alt="" src="https://rjlipton.files.wordpress.com/2021/02/voroninsteklov.jpg?w=142&amp;h=190" class="alignright wp-image-18177" height="190" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Steklov Institute memorial <a href="http://www.mi-ras.ru/index.php?c=inmemoria&amp;l=1">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his amazing 1975 result about the Riemann zeta function <a href="https://rjlipton.wordpress.com/2012/12/04/the-amazing-zeta-code/">here</a>. Others call the result the amazing theorem. I (Dick) am getting old—I almost forgot that we did a <a href="https://rjlipton.wordpress.com/2016/12/20/hunting-complexity-in-zeta/">post</a> on his theorem again over four years ago. </p>
<p>
Today I thought we would recall his theorem, sketch why the theorem is true, and then discuss some extensions.<br />
<span id="more-18152"></span></p>
<p>
Starting with Alan Turing we have been interested in universal objects. Turing famously <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">proved</a> that there are universal machines: these can simulate any other machine on any input. Martin Davis has an entire <a href="https://www.amazon.com/Universal-Computer-Road-Leibniz-Turing/dp/0393047857">book</a> on this subject. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/davis/" rel="attachment wp-att-18156"><img width="235" alt="" src="https://rjlipton.files.wordpress.com/2021/02/davis.jpg?w=235&amp;h=375" class="aligncenter wp-image-18156" height="375" /></a>
</td>
</tr>
</tbody></table>
<p>
Universal objects are basic to complexity theory. Besides Turing’s notion, a universal property is key to the definition of NP-complete. A set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> in NP is NP-complete provided all other sets in NP can be reduced to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> in polynomial time. Michael Nielsen once began a <a href="https://www.quantamagazine.org/the-physical-origin-of-universal-computing-20151027/">discussion</a> of universality in this amusing fashion:</p>
<blockquote><p><b> </b> <em> Imagine you’re shopping for a new car, and the salesperson says, “Did you know, this car doesn’t just drive on the road.” “Oh?” you reply. “Yeah, you can also use it to do other things. For instance, it folds up to make a pretty good bicycle. And it folds out to make a first-rate airplane. Oh, and when submerged it works as a submarine. And it’s a spaceship too!” </em>
</p></blockquote>
<p></p><h2> Voronin’s Insight </h2><p></p>
<p>
In 1975 Voronin had the brilliant insight that the Riemann zeta <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function has an interesting universality property.  Roughly speaking, it says that a wide class of analytic functions can be approximated by shifts <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%2Bit%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s+it)}" class="latex" title="{\zeta(s+it)}" /> with real <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" />. Recall 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " class="latex" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt;1}" class="latex" title="{\Re(s) &gt;1}" />, and it has an analytic extension for all other values but <img src="https://s0.wp.com/latex.php?latex=%7Bs%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s=1}" class="latex" title="{s=1}" />.</p>
<p>
The intense interest in the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function started in 1859 with Bernhard Riemann’s breakthrough <a href="https://en.wikipedia.org/wiki/On_the_Number_of_Primes_Less_Than_a_Given_Magnitude">article</a>. This was the first statement of what we call the Riemann Hypothesis (RH).</p>
<p>
In over a century of research on RH before Voronin’s theorem, many identities, many results, many theorems were proved about the zeta function. But none saw that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function was universal before Voronin. Given the zeta function’s importance in understanding the structure of prime numbers this seems to be surprising. </p>
<p>
Before we define the universal property I thought it might be useful to state a related <a href="https://arxiv.org/pdf/1305.3933.pdf">property</a> that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function has:</p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> is a polynomial so that for all <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" title="{s}" />, 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%5Cleft%28%5Czeta%28s%29%2C+%5Czeta%7B%27%7D%28s%29%2C%5Cdots%2C%5Czeta%5E%7B%28m%29%7D%28s%29+%5Cright%29+%3D+0.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " class="latex" title="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " /></p>
</em><p><em>Then <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> is identically zero. </em>
</p></blockquote>
<p>Since <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" title="{s}" /> is a single variable, this says that <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> and its derivatives <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta'(s)}" class="latex" title="{\zeta'(s)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%27%28s%29+%5Cdots+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta''(s) \dots }" class="latex" title="{\zeta''(s) \dots }" /> do not satisfy any polynomial relationship. This means intuitively that <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> must be <a href="https://en.wikipedia.org/wiki/Hypertranscendental_function">hypertranscendental</a>. Let’s now make this formal.</p>
<p></p><h2> Voronin’s Theorem </h2><p></p>
<p>
Here is his <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">theorem</a>: </p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7B0%3Cr%3C1%2F4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0&lt;r&lt;1/4}" class="latex" title="{0&lt;r&lt;1/4}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> be an analytic function that never is zero for <img src="https://s0.wp.com/latex.php?latex=%7B%7Cs%7C+%5Cle+r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|s| \le r}" class="latex" title="{|s| \le r}" />. Then for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> there is a real <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" /> so that 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7B%5Cleft+%7C+s+%5Cright+%7C+%5Cleq+r%7D+%5Cleft+%7C+%5Czeta%28s+%2B+%5Cfrac%7B3%7D%7B4%7D+%2B+i+t%29+-+f%28s%29+%5Cright+%7C+%3C+%5Cepsilon.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " class="latex" title="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
See the <a href="https://www.researchgate.net/profile/Renata-Macaitiene/publication/321139128_Zeros_of_the_Riemann_zeta-function_and_its_universality/links/5c7f7b5092851c695058d6fe/Zeros-of-the-Riemann-zeta-function-and-its-universality.pdf">paper</a> “Zeroes of the Riemann zeta-function and its universality,” by Ramunas Garunkstis, Antanas Laurincikas, and Renata Macaitiene, for a detailed modern discussion of his theorem. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/thm/" rel="attachment wp-att-18158"><img width="320" alt="" src="https://rjlipton.files.wordpress.com/2021/02/thm.png?w=320&amp;h=275" class="aligncenter wp-image-18158" height="275" /></a>
</td>
</tr>
</tbody></table>
<p></p><p><br />
Note that the theorem is not constructive. However, the values of <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" /> that work have a positive density—there are lots of them. Also note the restriction that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> is never zero is critical. Otherwise one would be able to show that the Riemann Hypothesis is false. In 2003, Garunkstis et al. did prove a constructive version, in a <a href="https://www.jstor.org/stable/43736941?seq=1">paper</a> titled, “Effective Uniform Approximation By The Riemann Zeta-Function.”</p>
<p></p><h2> Voronin’s Proof </h2><p></p>
<p>
The key insight is to combine two properties of the zeta <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function: The usual definition with the Euler product. Recall the Riemann zeta-function has an Euler product expression 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Cprod_p+%5Cfrac%7B1%7D%7B1-p%5E%7B-s%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " class="latex" title="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> runs over prime numbers. This is valid only in the region <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt; 1}" class="latex" title="{\Re(s) &gt; 1}" />, but it makes sense in a approximate sense in the critical strip: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F2+%3C+%5CRe%28s%29+%3C+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " class="latex" title="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " /></p>
<p>Then take logarithms and since <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log(p)}" class="latex" title="{\log(p)}" /> are linearly independent over <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q}" class="latex" title="{Q}" />, we can apply the Kronecker approximation theorem to obtain that any target function <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> can be approximated by the above finite truncation. This is the basic structure of the <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">proof</a>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Voronin’s insight was immediately interesting to number theorists. Many found new methods for proving universality and for extending it to other functions. Some methods work for all zeta-functions defined by Euler products. See this <a href="https://arxiv.org/pdf/1407.4216.pdf">survey</a> by Kohji Matsumoto and a recent <a href="https://www.semanticscholar.org/paper/Quantized-Number-Theory%2C-Fractal-Strings-and-the-to-Herichi-Lapidus/37dbcf9c28c316b8cfcfe74394f3a1f2d709235d">paper</a><br />
by Hafedh Herichi and Michel Lapidus, the latter titled “Quantized Number Theory, Fractal Strings and the Riemann Hypothesis: From Spectral Operators to Phase Transitions and Universality.”</p>
<p>
Perhaps the most interesting question is: </p>
<p><i>Can universality be used to finally unravel the RH?</i> </p>
<p>See Paul Gauthier’s 2014 IAS <a href="http://www.math.kent.edu/~zvavitch/informal/Informal_Analysis_Seminar/Slides,_April_2014_files/IAS2014_Gauthier_1.pdf">talk</a>, “Universality and the Riemann Hypothesis,” for some ideas.</p>
<p>
[fixed missing line at end]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/"><span class="datestr">at February 21, 2021 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-1990979806491986425">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html">Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel's memory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://www.strath.ac.uk/staff/kitaevsergeydr/" target="_blank">Sergey Kitaev</a> just shared with me <a href="http://ecajournal.haifa.ac.il/Volume2021/ECA2021_S1H2.pdf" target="_blank">an article</a> he wrote with Anthony Mendes in <a href="https://senate.universityofcalifornia.edu/in-memoriam/files/jeffrey-b-remmel.html" target="_blank">Jeff Remmel's memory</a>. Jeff Remmel was a distinguished mathematician with a very successful career in both logic and combinatorics. </p><p>The short biography at the start of the article paints a vivid picture of Jeff Remmel's  personality, and will be of interest and inspiration to many readers. His hiring as "an Assistant Professor in the Department of Mathematics at UC San Diego at age 25, without officially finishing his Ph.D. and without having published a single paper" was, in Jeff Remmel's own words, a "fluke that will never happen again."<br /></p><p>I had the pleasure of making Jeff Remmel's acquaintance when he visited Sergey in Reykjavik and thoroughly enjoyed talking to him about a variety of subjects. He was truly a larger-than-life academic. <br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html"><span class="datestr">at February 21, 2021 11:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/025">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/025">TR21-025 |  Improved Maximally Recoverable LRCs using Skew Polynomials | 

	Sivakanth Gopi, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over $\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$ local groups each of size $r$. Each local group satisfies `$a$' local parity checks to recover from `$a$' erasures in that local group and there are further $h$ global parity checks to provide fault tolerance from more global erasure patterns. Such an LRC is Maximally Recoverable (MR), if it offers the best blend of locality and global erasure resilience---namely it can correct all erasure patterns whose recovery is information-theoretically feasible given the locality structure (these are precisely patterns with up to `$a$' erasures in each local group and an additional $h$ erasures anywhere in the codeword).

Random constructions can easily show the existence of MR LRCs over very large fields, but a major algebraic challenge is to construct MR LRCs, or even show their existence, over smaller fields, as well as understand inherent lower bounds on their field size. We give an explicit construction of $(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by $\left(O\left(\max\{r,n/r\}\right)\right)^{\min\{h,r-a\}}$. This improves upon known constructions in many relevant parameter ranges. Moreover, it matches the lower bound from Gopi et al. (2020) in an interesting range of parameters where $r=\Theta(\sqrt{n})$, $r-a=\Theta(\sqrt{n})$ and $h$ is a fixed constant with $h\le a+2$, achieving the optimal field size of $\Theta_{h}(n^{h/2}).$

Our construction is based on the theory of skew polynomials.  We believe skew polynomials should have further applications in coding and complexity theory; as a small illustration we show how to capture algebraic results underlying list decoding folded Reed-Solomon and multiplicity codes in a unified way within this theory.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/025"><span class="datestr">at February 21, 2021 10:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/024">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/024">TR21-024 |  A Majority Lemma for Randomised Query Complexity | 

	Mika Göös, 

	Gilbert Maystre</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that computing the majority of $n$ copies of a boolean function $g$ has randomised query complexity $\mathrm{R}(\mathrm{Maj} \circ g^n) = \Theta(n\cdot \bar{\mathrm{R}}_{1/n}(g))$. In fact, we show that to obtain a similar result for any composed function $f\circ g^n$, it suffices to prove a sufficiently strong form of the result only in the special case $g=\mathrm{GapOr}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/024"><span class="datestr">at February 21, 2021 10:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/023">TR21-023 |  $3.1n - o(n)$ Circuit Lower Bounds for Explicit Functions | 

	Tianqi Yang, 

	Jiatu Li</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proving circuit lower bounds has been an important but extremely hard problem for decades. Although one may show that almost every function $f:\mathbb{F}_2^n\to\mathbb{F}_2$ requires circuit of size $\Omega(2^n/n)$ by a simple counting argument, it remains unknown whether there is an explicit function (for example, a function in $NP$) not computable by circuits of size $10n$. In fact, a $3n-o(n)$ explicit lower bound by Blum (TCS, 1984) was unbeaten for over 30 years until a recent breakthrough by Find et al. (FOCS, 2016), which proved a $(3+\frac{1}{86})n-o(n)$ lower bound for affine dispersers, a class of functions known to be constructible in $P$.

In this paper, we prove a stronger lower bound $3.1n - o(n)$ for affine dispersers. To get this result, we strengthen the gate elimination approach for $(3+\frac{1}{86})n$ lower bound, by a more sophisticated case analysis that significantly decreases the number of bottleneck structures introduced during the elimination procedure. Intuitively, our improvement relies on three observations: adjacent bottleneck structures becomes less troubled; the gates eliminated are usually connected; and the hardest cases during gate elimination have nice local properties to prevent the introduction of new bottleneck structures.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/023"><span class="datestr">at February 21, 2021 06:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/022">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/022">TR21-022 |  Depth lower bounds in Stabbing Planes for combinatorial principles | 

	Stefan Dantchev, 

	Nicola  Galesi, 

	Abdul Ghani, 

	Barnaby Martin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove logarithmic depth lower bounds in Stabbing Planes for the classes of  combinatorial principles known as  the Pigeonhole principle and the Tseitin contradictions. The depth lower bounds are new, obtained by giving almost linear length lower bounds which do not depend on the bit-size of the inequalities and in the case of  the Pigeonhole principle are tight. 

The technique known so far to prove depth lower bounds for Stabbing Planes is a generalization of that used for the Cutting Planes proof system.  In this work  we  introduce two  new approaches to prove length/depth lower bounds in Stabbing Planes: one relying on Sperner's Theorem which works for the Pigeonhole principle and Tseitin contradictions over the complete graph; a second proving the lower bound for Tseitin contradictions over a grid graph, which uses a result on essential  coverings of the boolean cube by linear polynomials, which in turn relies on Alon's combinatorial Nullenstellensatz</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/022"><span class="datestr">at February 20, 2021 08:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/021">TR21-021 |  Average-Case Perfect Matching Lower Bounds from Hardness of Tseitin Formulas | 

	Kilian Risse, 

	Per Austrin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the complexity of proving that a sparse random regular graph on an odd number of vertices does not have a perfect matching, and related problems involving each vertex being matched some pre-specified number of times. We show that this requires proofs of degree $\Omega(n/\log n)$ in the Polynomial Calculus (over fields of characteristic $\ne 2$) and Sum-of-Squares proof systems, and exponential size in the bounded-depth Frege proof system. This resolves a question by Razborov asking whether the Lovász-Schrijver proof system requires $n^\delta$ rounds to refute these formulas for some $\delta &gt; 0$. The results are obtained by a worst-case to average-case reduction of these formulas relying on a topological embedding theorem which may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/021"><span class="datestr">at February 20, 2021 06:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/020">TR21-020 |  Error Reduction For Weighted PRGs Against Read Once Branching Programs | 

	Gil Cohen, 

	Dean Doron, 

	Amnon Ta-Shma, 

	Ori Sberlo, 

	Oren Renard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Weighted pseudorandom generators (WPRGs), introduced by Braverman, Cohen and Garg [BCG20], is a generalization of pseudorandom generators (PRGs) in which arbitrary real weights are considered rather than a probability mass. Braverman et al. constructed WPRGs against read once branching programs (ROBPs) with near-optimal dependence on the error parameter. Chattopadhyay and Liao [CL20] somewhat simplified the technically involved BCG construction, also obtaining some improvement in parameters.

In this work we devise an error reduction procedure for PRGs against ROBPs. More precisely, our procedure transforms any PRG against length n width w ROBP with error 1/poly(n) having seed length s to a WPRG with seed length s + O(log(w/?)loglog(1/?)). By instantiating our procedure with Nisan’s PRG [Nis92] we obtain a WPRG with seed length O(log(n)log(nw) + log(w/?)loglog(1/?)). This improves upon [BCG20] and is incomparable with [CL20].

Our construction is significantly simpler on the technical side and is conceptually cleaner. Another advantage of our construction is its low space complexity O(log nw)+ poly(loglog(1/?)) which is logarithmic in n for interesting values of the error parameter ?. Previous constructions (like [BCG20, CL20]) specify the seed length but not the space complexity, though it is plausible they can also achieve such (or close) space complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/020"><span class="datestr">at February 20, 2021 06:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/019">TR21-019 |  Pseudodistributions That Beat All Pseudorandom Generators | 

	Edward Pyne, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent paper of Braverman, Cohen, and Garg (STOC 2018) introduced the concept of a pseudorandom pseudodistribution generator (PRPG), which amounts to a pseudorandom generator (PRG) whose outputs are accompanied with real coefficients that scale the acceptance probabilities of any potential distinguisher. They gave an explicit construction of PRPGs for ordered branching programs whose seed length has a better dependence on the error parameter $\epsilon$ than the classic PRG construction of Nisan (STOC 1990 and Combinatorica 1992). 
    
    In this work, we give an explicit construction of PRPGs that achieve parameters that are impossible to achieve by a PRG.  In particular, we construct a PRPG for ordered permutation branching programs of unbounded width with a single accept state that has seed length $\tilde{O}(\log^{3/2} n)$ for error parameter $\epsilon=1/\text{poly}(n)$, where $n$ is the input length.  In contrast, recent work of Hoza et al. (ITCS 2021) shows that any PRG for this model requires seed length $\Omega(\log^2 n)$ to achieve error $\epsilon=1/\text{poly}(n)$.
    
    As a corollary, we obtain explicit PRPGs with seed length $\tilde{O}(\log^{3/2} n)$ and error $\epsilon=1/\text{poly}(n)$ for ordered permutation branching programs of width $w=\text{poly}(n)$ with an arbitrary number of accept states.  Previously, seed length $o(\log^2 n)$ was only known when both the width and the reciprocal of the error are subpolynomial, i.e. $w=n^{o(1)}$ and $\epsilon=1/n^{o(1)}$ (Braverman, Rao, Raz, Yehudayoff, FOCS 2010 and SICOMP 2014).
    
    The starting point for our results are the recent space-efficient algorithms for estimating random-walk probabilities in directed graphs by Ahmadenijad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020), which are based on spectral graph theory and space-efficient Laplacian solvers.  We interpret these algorithms as giving PRPGs with large seed length, which we then derandomize to obtain our results.  We also note that this approach gives a simpler proof of the original result of Braverman, Cohen, and Garg, as independently discovered by Cohen, Doron, Renard, Sberlo, and Ta-Shma (personal communication, January 2021).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/019"><span class="datestr">at February 20, 2021 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/018">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/018">TR21-018 |  Monotone Branching Programs: Pseudorandomness and Circuit Complexity | 

	Dean Doron, 

	Raghu Meka, 

	Omer Reingold, 

	Avishay Tal, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study monotone branching programs, wherein the states at each time step can be ordered so that edges with the same labels never cross each other. Equivalently, for each fixed input, the transition functions are a monotone function of the state. 

We prove that constant-width monotone branching programs of polynomial size are equivalent in power to $AC^{0}$ circuits. This complements the celebrated theorem of Barrington, which states that constant-width branching programs, without the monotonicity constraint, are equivalent in power to $NC^{1}$ circuits.

Next we turn to read-once monotone branching programs of constant width, which we note are strictly more powerful than read-once $AC^0$.  Our main result is an explicit pseudorandom generator that $\varepsilon$-fools length $n$ programs with seed length $\widetilde{O}(\log(n/\varepsilon))$. This extends the families of constant-width read-once branching programs for which we have an explicit pseudorandom generator with near-logarithmic seed length. 

Our pseudorandom generator construction follows Ajtai and Wigderson's approach of iterated pseudorandom restrictions [AW89,GMRTV12]. We give a randomness-efficient width-reduction process which allows us to simplify the branching program after only $O(\log\log n)$ independent applications of the Forbes--Kelley pseudorandom restrictions [FK18].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/018"><span class="datestr">at February 20, 2021 06:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html">Loops, degrees, and matchings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A student in my graph algorithms class asked how <a href="https://11011110.github.io/blog/2021/02/19/Loop (graph theory)">self-loops</a> in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</p>

<p>However, this turns out not to make much difference to many matching problems, because the following simple transformation turns a problem with self-loops (allowed in matchings in this way) into a problem with no self-loops (so it doesn’t matter whether they are allowed or not). Simply form a <a href="https://en.wikipedia.org/wiki/Covering_graph">double cover</a>\(^*\) of the given graph (let’s call it the “loopless double cover”) by making two copies of the graph and replacing all corresponding pairs of loops by simple edges from one copy to the other. In weighted matching problems, give the replacement edges for the loops the sum of the weights of the two loops they replace; all other edges keep their original weights.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/loopless-double-cover.svg" alt="The loopless double cover of a graph and of one of its loopy matchings" /></p>

<p>Then (unlike the <a href="https://en.wikipedia.org/wiki/Bipartite_double_cover">bipartite double cover</a>, which also eliminates loops) the cardinality or optimal weight of a matching in the loopy graph can be read off from the corresponding solution in its loopless double cover. Any matching of the original loopy graph can be translated into a matching of the loopless cover by applying the same loopless cover translation to the matching instead of to the whole graph; this doubles the total weight of the matching and the total number of matched vertices. And among matchings on the loopless cover, when trying to optimize weight or matched vertices, it is never helpful to match the two copies differently, so there is an optimal solution that can be translated back to the original graph without changing its optimality.</p>

<p>This doesn’t quite work for the problem of finding a matching that maximizes the total number of matched edges, rather than the total number of matched vertices. These two problems are the same in simple graphs, but different in loopy graphs. However, in a loopy graph, if you are trying to maximize matched edges, you might as well include all loops in the matching, and then search for a maximum matching of the simple graph induced by the remaining unmatched vertices. Again, in this case, you don’t get a problem that requires any new algorithms to solve it.</p>

<p>In the case of my student, I only provided the conventional answer, because really all they wanted to know was whether these issues affected how they answered one of the homework questions, and the answer was that the question didn’t involve and didn’t need loops. However it seems that the more-complicated answer is that even if you allow loops to count only one unit towards degree, and to be included in matchings, they don’t change the matching problem much.</p>

<p>\(^*\) This is only actually a covering graph under the convention that the degree of a loop is one. For the usual degree-2 convention for loops, you would need to replace each loop by a pair of parallel edges, forming a multigraph, to preserve the degrees of the vertices.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105762400402127534">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html"><span class="datestr">at February 19, 2021 06:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
