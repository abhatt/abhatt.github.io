<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 02, 2020 05:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12872">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12872">Dynamical perturbation theory for eigenvalue problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Maseim Kenmoe, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smerlak:Matteo.html">Matteo Smerlak</a>, Anton Zadorin <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12872">PDF</a><br /><b>Abstract: </b>Many problems in physics, chemistry and other fields are perturbative in
nature, i.e. differ only slightly from related problems with known solutions.
Prominent among these is the eigenvalue perturbation problem, wherein one seeks
the eigenvectors and eigenvalues of a matrix with small off-diagonal elements.
Here we introduce a novel iterative algorithm to compute these eigenpairs based
on a reformulation of the eigenvalue problem as an algebraic equation in
complex projective space. We show from explicit and numerical examples that our
algorithm outperforms the usual Rayleigh-Schr\"odinger expansion on three
counts. First, since it is not defined as a power series, its domain of
convergence is not a priori confined to a disk in the complex plane; we find
that it indeed usually extends beyond the standard perturbative radius of
convergence. Second, it converges at a faster slower rate than the
Rayleigh-Schr\"odinger expansion, i.e. fewer iterations are required to reach a
given precision. Third, the (time- and space-) algorithmic complexity of each
iteration does not increase with the order of the approximation, allowing for
higher precision computations. Because this complexity is merely that of matrix
multiplication, our dynamical scheme also scales better with the size of the
matrix than general-purpose eigenvalue routines such as the shifted QR or
divide-and-conquer algorithms. Whether they are dense, sparse, symmetric or
unsymmetric, we confirm that dynamical diagonalization quickly outpaces LAPACK
drivers as the size of matrices grows; for the computation of just the dominant
eigenvector, our method converges order of magnitudes faster than the Arnoldi
algorithm implemented in ARPACK.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12872"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12856">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12856">Two Player Hidden Pointer Chasing and Multi-Pass Lower Bounds in Turnstile Streams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehrotra:Anay.html">Anay Mehrotra</a>, Vibhor Porwal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tewari:Raghunath.html">Raghunath Tewari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12856">PDF</a><br /><b>Abstract: </b>(Assadi, Chen, and Khanna, 2019) define a 4-player hidden-pointer-chasing
($\mathsf{HPC}^4$), and using it, give strong multi-pass lower bounds for graph
problems in the streaming model of computation and a lower bound on the query
complexity of sub-modular minimization. We present a two-player version
($\mathsf{HPC}^2$) of $\mathsf{HPC}^4$ that has matching communication
complexity to $\mathsf{HPC}^4$. Our formulation allows us to lower bound its
communication complexity with a simple direct-sum argument. Using this lower
bound on the communication complexity of $\mathsf{HPC}^2$, we retain the
streaming and query complexity lower bounds by (Assadi, Chen, and Khanna,
2019).
</p>
<p>Further, by giving reductions from $\mathsf{HPC}^2$, we prove new multi-pass
space lower bounds for graph problems in turnstile streams. In particular, we
show that any algorithm which computes the exact weight of the maximum weighted
matching in an $n$-vertex graph requires $\tilde{O}(n^{2})$ space unless it
makes $\omega(\log n)$ passes over the turnstile stream, and that any algorithm
which computes the minimum $s\text{-}t$ distance in an $n$-vertex graph
requires $n^{2-o(1)}$ space unless it makes $n^{\Omega(1)}$ passes over the
turnstile stream. Our reductions can be modified to use $\mathsf{HPC}^4$ as
well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12856"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12814">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12814">Estimating the entropy of shallow circuit outputs is hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gheorghiu:Alexandru.html">Alexandru Gheorghiu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoban:Matty_J=.html">Matty J. Hoban</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12814">PDF</a><br /><b>Abstract: </b>The decision problem version of estimating the Shannon entropy is the Entropy
Difference problem (ED): given descriptions of two circuits, determine which
circuit produces more entropy in its output when acting on a uniformly random
input. The analogous problem with quantum circuits (QED) is to determine which
circuit produces the state with greater von Neumann entropy, when acting on a
fixed input state and after tracing out part of the output. Based on plausible
complexity-theoretic assumptions, both of these problems are believed to be
intractable for polynomial-time quantum computation. In this paper, we
investigate the hardness of these problems in the case where the input circuits
have logarithmic and constant depth, respectively. We show that, relative to an
oracle, these problems cannot be as hard as their counterparts with
polynomial-size circuits. Furthermore, we show that if a certain type of
reduction from QED to the log-depth version exists, it implies that any
polynomial-time quantum computation can be performed in log depth. While this
suggests that having shallow circuits makes entropy estimation easier, we give
indication that the problem remains intractable for polynomial-time quantum
computation by proving a reduction from Learning-With-Errors (LWE) to
constant-depth ED. We then consider a potential application of our results to
quantum gravity research. First, we introduce a Hamiltonian version of QED
where one is given two local Hamiltonians and asked to estimate the
entanglement entropy difference in their ground states. We show that this
problem is at least as hard as the circuit version and then discuss a potential
experiment that would make use of the AdS/CFT correspondence to solve LWE
efficiently. We conjecture that unless the AdS/CFT bulk to boundary map is
exponentially complex, this experiment would violate the intractability
assumption of LWE.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12814"><span class="datestr">at March 02, 2020 02:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12771">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12771">Tuning as convex optimisation: a polynomial tuner for multi-parametric combinatorial samplers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bendkowski:Maciej.html">Maciej Bendkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodini:Olivier.html">Olivier Bodini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dovgal:Sergey.html">Sergey Dovgal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12771">PDF</a><br /><b>Abstract: </b>Combinatorial samplers are algorithmic schemes devised for the approximate-
and exact-size generation of large random combinatorial structures, such as
context-free words, various tree-like data structures, maps, tilings, or even
RNA sequences. In their multi-parametric variants, combinatorial samplers are
adapted to combinatorial specifications with additional parameters, allowing
for a more flexible control over the output profile of parametrised
combinatorial patterns. One can control, for instance, the number of leaves,
profile of node degrees in trees or the number of certain sub-patterns in
generated strings. However, such a flexible control requires an additional and
nontrivial tuning procedure.
</p>
<p>Using techniques of convex optimisation, we present an efficient polynomial
tuning algorithm for multi-parametric combinatorial specifications. For a given
combinatorial system of description length $L$ with $d$ tuning parameters and
target size parameter value $n$, our algorithm runs in time $O(d^{3.5} L \log
n)$. We demonstrate the effectiveness of our method on a series of practical
examples, including rational, algebraic, and so-called P\'olya specifications.
We show how our method can be adapted to a broad range of less typical
combinatorial constructions, including symmetric polynomials, labelled sets and
cycles with cardinality lower bounds, simple increasing trees or substitutions.
Finally, we discuss some practical aspects of our prototype tuner
implementation and provide its benchmark results.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12771"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12706">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12706">An optimal algorithm for Bisection for bounded-treewidth graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sone:Taiga.html">Taiga Sone</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12706">PDF</a><br /><b>Abstract: </b>The maximum/minimum bisection problems are, given an edge-weighted graph, to
find a bipartition of the vertex set into two sets whose sizes differ by at
most one, such that the total weight of edges between the two sets is
maximized/minimized. Although these two problems are known to be NP-hard, there
is an efficient algorithm for bounded-treewidth graphs. In particular, Jansen
et al. (SIAM J. Comput. 2005) gave an $O(2^tn^3)$-time algorithm when given a
tree decomposition of width $t$ of the input graph, where $n$ is the number of
vertices of the input graph. Eiben et al. (ESA 2019) improved the running time
to $O(8^tt^5n^2\log n)$. Moreover, they showed that there is no
$O(n^{2-\varepsilon})$-time algorithm for trees under some reasonable
complexity assumption.
</p>
<p>In this paper, we show an $O(2^t(tn)^2)$-time algorithm for both problems,
which is asymptotically tight to their conditional lower bound. We also show
that the exponential dependency of the treewidth is asymptotically optimal
under the Strong Exponential Time Hypothesis. Moreover, we discuss the
(in)tractability of both problems with respect to special graph classes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12706"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12694">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12694">Edge-Disjoint Branchings in Temporal Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Campos:Victor.html">Victor Campos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lopes:Raul.html">Raul Lopes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marino:Andrea.html">Andrea Marino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Ana.html">Ana Silva</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12694">PDF</a><br /><b>Abstract: </b>A temporal digraph ${\cal G}$ is a triple $(G, \gamma, \lambda)$ where $G$ is
a digraph, $\gamma$ is a function on $V(G)$ that tells us the timestamps when a
vertex is active, and $\lambda$ is a function on $E(G)$ that tells for each $uv
\in E(G)$ when $u$ and $v$ are linked. Given a static digraph $G$, and a subset
$R\subseteq V(G)$, a spanning branching with root $R$ is a subdigraph of $G$
that has exactly one path from $R$ to each $v\in V(G)$. In this paper, we
consider the temporal version of Edmonds' classical result about the problem of
finding $k$ edge-disjoint spanning branchings respectively rooted at given
$R_1,\cdots,R_k$. We introduce and investigate different definitions of
spanning branchings, and of edge-disjointness in the context of temporal
graphs. A branching ${\cal B}$ is vertex-spanning if the root is able to reach
each vertex $v$ of $G$ at some time where $v$ is active, while it is
temporal-spanning if $v$ can be reached from the root at every time where $v$
is active. On the other hand, two branchings ${\cal B}_1$ and ${\cal B}_2$ are
edge-disjoint if they do not use the same edge of $G$, and are
temporal-edge-disjoint if they can use the same edge of $G$ but at different
times. This lead us to four definitions of disjoint spanning branchings and we
prove that, unlike the static case, only one of these can be computed in
polynomial time, namely the temporal-edge-disjoint temporal-spanning branchings
problem, while the other versions are $\mathsf{NP}$-complete, even under very
strict assumptions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12694"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12662">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12662">Fast Indexes for Gapped Pattern Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/C=aacute=ceres:Manuel.html">Manuel Cáceres</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Puglisi:Simon_J=.html">Simon J. Puglisi</a>, Bella Zhukova University of Chile, University of Helsinki) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12662">PDF</a><br /><b>Abstract: </b>We describe indexes for searching large data sets for variable-length-gapped
(VLG) patterns. VLG patterns are composed of two or more subpatterns, between
each adjacent pair of which is a gap-constraint specifying upper and lower
bounds on the distance allowed between subpatterns. VLG patterns have numerous
applications in computational biology (motif search), information retrieval
(e.g., for language models, snippet generation, machine translation) and
capture a useful subclass of the regular expressions commonly used in practice
for searching source code. Our best approach provides search speeds several
times faster than prior art across a broad range of patterns and texts.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12662"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12538">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12538">Explainable $k$-Means and $k$-Medians Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dasgupta:Sanjoy.html">Sanjoy Dasgupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frost:Nave.html">Nave Frost</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Michal.html">Michal Moshkovitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rashtchian:Cyrus.html">Cyrus Rashtchian</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12538">PDF</a><br /><b>Abstract: </b>Clustering is a popular form of unsupervised learning for geometric data.
Unfortunately, many clustering algorithms lead to cluster assignments that are
hard to explain, partially because they depend on all the features of the data
in a complicated way. To improve interpretability, we consider using a small
decision tree to partition a data set into clusters, so that clusters can be
characterized in a straightforward manner. We study this problem from a
theoretical viewpoint, measuring cluster quality by the $k$-means and
$k$-medians objectives: Must there exist a tree-induced clustering whose cost
is comparable to that of the best unconstrained clustering, and if so, how can
it be found? In terms of negative results, we show, first, that popular
top-down decision tree algorithms may lead to clusterings with arbitrarily
large cost, and second, that any tree-induced clustering must in general incur
an $\Omega(\log k)$ approximation factor compared to the optimal clustering. On
the positive side, we design an efficient algorithm that produces explainable
clusters using a tree with $k$ leaves. For two means/medians, we show that a
single threshold cut suffices to achieve a constant factor approximation, and
we give nearly-matching lower bounds. For general $k \geq 2$, our algorithm is
an $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$
approximation to the optimal $k$-means. Prior to our work, no algorithms were
known with provable guarantees independent of dimension and input size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12538"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12354">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12354">A Data Dependent Algorithm for Querying Earth Mover's Distance with Low Doubling Dimension</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Tan.html">Tan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Mingyue.html">Mingyue Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Fan.html">Fan Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12354">PDF</a><br /><b>Abstract: </b>In this paper, we consider the following query problem: given two weighted
point sets $A$ and $B$ in the Euclidean space $\mathbb{R}^d$, we want to
quickly determine that whether their earth mover's distance (EMD) is larger or
smaller than a pre-specified threshold $T\geq 0$. The problem finds a number of
important applications in the fields of machine learning and data mining. In
particular, we assume that the dimensionality $d$ is not fixed and the sizes
$|A|$ and $|B|$ are large. Therefore, most of existing EMD algorithms are not
quite efficient to solve this problem. Here, we consider the problem under the
assumption that $A$ and $B$ have low doubling dimension, which is common for
high-dimensional data in real world. Inspired by the geometric method {\em net
tree}, we propose a novel "data-dependent" algorithm to solve this problem
efficiently. We also study the performance of our method on real datasets, and
the experimental results suggest that our method can save a large amount of
running time comparing with existing EMD algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12354"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11933">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11933">On Metric DBSCAN with Low Doubling Dimension</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Fan.html">Fan Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11933">PDF</a><br /><b>Abstract: </b>The density based clustering method {\em Density-Based Spatial Clustering of
Applications with Noise (DBSCAN)} is a popular method for outlier recognition
and has received tremendous attention from many different areas. A major issue
of the original DBSCAN is that the time complexity could be as large as
quadratic. Most of existing DBSCAN algorithms focus on developing efficient
index structures to speed up the procedure in low-dimensional Euclidean space.
However, the research of DBSCAN in high-dimensional Euclidean space or general
metric space is still quite limited, to the best of our knowledge. In this
paper, we consider the metric DBSCAN problem under the assumption that the
inliers (excluding the outliers) have a low doubling dimension. We apply a
novel randomized $k$-center clustering idea to reduce the complexity of range
query, which is the most time consuming step in the whole DBSCAN procedure. Our
proposed algorithms do not need to build any complicated data structures and
are easy to be implemented in practice. The experimental results show that our
algorithms can significantly outperform the existing DBSCAN algorithms in terms
of running time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11933"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11923">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11923">The Effectiveness of Johnson-Lindenstrauss Transform for High Dimensional Optimization with Outliers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a>, Ruizhe Qin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jiawei.html">Jiawei Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11923">PDF</a><br /><b>Abstract: </b>{\em Johnson-Lindenstrauss (JL) Transform} is one of the most popular methods
for dimension reduction. In this paper, we study the effectiveness of JL
transform for solving the high dimensional optimization problems with outliers.
We focus on two fundamental optimization problems: {\em $k$-center clustering
with outliers} and {\em SVM with outliers}. In general, the time complexity for
dealing with outliers in high dimensional space could be very large. Based on
some novel insights in geometry, we prove that the complexities of these two
problems can be significantly reduced through JL transform. In the experiments,
we compare JL transform with several other well known dimension reduction
methods, and study their performances on synthetic and real datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11923"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11912">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11912">Max-Affine Spline Insights into Deep Generative Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balestriero:Randall.html">Randall Balestriero</a>, Sebastien Paris, Richard Baraniuk <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11912">PDF</a><br /><b>Abstract: </b>We connect a large class of Generative Deep Networks (GDNs) with spline
operators in order to derive their properties, limitations, and new
opportunities. By characterizing the latent space partition, dimension and
angularity of the generated manifold, we relate the manifold dimension and
approximation error to the sample size. The manifold-per-region affine subspace
defines a local coordinate basis; we provide necessary and sufficient
conditions relating those basis vectors with disentanglement. We also derive
the output probability density mapped onto the generated manifold in terms of
the latent space density, which enables the computation of key statistics such
as its Shannon entropy. This finding also enables the computation of the GDN
likelihood, which provides a new mechanism for model comparison as well as
providing a quality measure for (generated) samples under the learned
distribution. We demonstrate how low entropy and/or multimodal distributions
are not naturally modeled by DGNs and are a cause of training instabilities.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11912"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11904">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11904">Layered Sampling for Robust Optimization Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Zixiu.html">Zixiu Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11904">PDF</a><br /><b>Abstract: </b>In real world, our datasets often contain outliers. Moreover, the outliers
can seriously affect the final machine learning result. Most existing
algorithms for handling outliers take high time complexities (e.g. quadratic or
cubic complexity). {\em Coreset} is a popular approach for compressing data so
as to speed up the optimization algorithms. However, the current coreset
methods cannot be easily extended to handle the case with outliers. In this
paper, we propose a new variant of coreset technique, {\em layered sampling},
to deal with two fundamental robust optimization problems: {\em
$k$-median/means clustering with outliers} and {\em linear regression with
outliers}. This new coreset method is in particular suitable to speed up the
iterative algorithms (which often improve the solution within a local range)
for those robust optimization problems. Moreover, our method is easy to be
implemented in practice. We expect that our framework of layered sampling will
be applicable to other robust optimization problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11904"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11830">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11830">Polynomial algorithms for p-dispersion problems in a 2d Pareto Front</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dupin:Nicolas.html">Nicolas Dupin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11830">PDF</a><br /><b>Abstract: </b>Having many best compromise solutions for bi-objective optimization problems,
this paper studies p-dispersion problems to select $p\geqslant 2$
representative points in the Pareto Front(PF). Four standard variants of
p-dispersion are considered. A novel variant, denoted Max-Sum-Neighbor
p-dispersion, is introduced for the specific case of a 2d PF. Firstly, it is
proven that $2$-dispersion and $3$-dispersion problems are solvable in $O(n)$
time in a 2d PF. Secondly, dynamic programming algorithms are designed for
three p-dispersion variants, proving polynomial complexities in a 2d PF. The
Max-Min p-dispersion problem is proven solvable in $O(pn\log n)$ time and
$O(n)$ memory space. The Max-Sum-Min p-dispersion problem is proven solvable in
$O(pn^3)$ time and $O(pn^2)$ space. The Max-Sum-Neighbor p-dispersion problem
is proven solvable in $O(pn^2)$ time and $O(pn)$ space. Complexity results and
parallelization issues are discussed in regards to practical implementation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11830"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11818">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11818">Finding large matchings in 1-planar graphs of minimum degree 3</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Therese Biedl, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klute:Fabian.html">Fabian Klute</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11818">PDF</a><br /><b>Abstract: </b>A matching is a set of edges without common endpoint. It was recently shown
that every 1-planar graph (i.e., a graph that can be drawn in the plane with at
most one crossing per edge) that has minimum degree 3 has a matching of size at
least $\frac{n+12}{7}$, and this is tight for some graphs. The proof did not
come with an algorithm to find the matching more efficiently than a
general-purpose maximum-matching algorithm. In this paper, we give such an
algorithm. More generally, we show that any matching that has no augmenting
paths of length 9 or less has size at least $\frac{n+12}{7}$ in a 1-planar
graph with minimum degree 3.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11818"><span class="datestr">at March 02, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1902.05357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1902.05357">Estimating the Circuit Deobfuscating Runtime based on Graph Deep Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zhiqian.html">Zhiqian Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolhe:Gaurav.html">Gaurav Kolhe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rafatirad:Setareh.html">Setareh Rafatirad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=:Sai_Manoj_P=.html">Sai Manoj P. D.</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Homayoun:Houman.html">Houman Homayoun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Liang.html">Liang Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Chang=Tien.html">Chang-Tien Lu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1902.05357">PDF</a><br /><b>Abstract: </b>Circuit obfuscation is a recently proposed defense mechanism to protect
digital integrated circuits (ICs) from reverse engineering by using camouflaged
gates i.e., logic gates whose functionality cannot be precisely determined by
the attacker. There have been effective schemes such as satisfiability-checking
(SAT)-based attacks that can potentially decrypt obfuscated circuits, called
deobfuscation. Deobfuscation runtime could have a large span ranging from few
milliseconds to thousands of years or more, depending on the number and layouts
of the ICs and camouflaged gates. And hence accurately pre-estimating the
deobfuscation runtime is highly crucial for the defenders to maximize it and
optimize their defense. However, estimating the deobfuscation runtime is a
challenging task due to 1) the complexity and heterogeneity of graph-structured
circuit, 2) the unknown and sophisticated mechanisms of the attackers for
deobfuscation. To address the above mentioned challenges, this work proposes
the first machine-learning framework that predicts the deobfuscation runtime
based on graph deep learning techniques. Specifically, we design a new model,
ICNet with new input and convolution layers to characterize and extract graph
frequencies from ICs, which are then integrated by heterogeneous deep
fully-connected layers to obtain final output. ICNet is an end-to-end framework
which can automatically extract the determinant features for deobfuscation
runtime. Extensive experiments demonstrate its effectiveness and efficiency.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1902.05357"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2386">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/richardson-extrapolation/">On the unreasonable effectiveness of Richardson extrapolation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month, I will follow up on <a href="https://francisbach.com/acceleration-without-pain/">last month’s blog post</a>, and describe classical techniques from numerical analysis that aim at accelerating the convergence of a vector sequence to its limit, by only combining elements of the sequence, and without the detailed knowledge of the iterative process that has led to this sequence. </p>



<p class="justify-text">Last month, I focused on sequences that converge to their limit exponentially fast (which is referred to as <em>linear</em> convergence), and I described <a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">Aitken’s \(\Delta^2\) method</a>, the <a href="https://en.wikipedia.org/wiki/Shanks_transformation">Shanks transformation</a>, Anderson acceleration and its <a href="https://arxiv.org/pdf/1606.04133">regularized version</a>. These methods are called “non-linear” acceleration techniques, because, although they combine linearly iterates as \(c_0 x_k + c_1 x_{k+1} + \cdots + c_m x_{k+m}\), the scalar weights in the linear combination depend non-linearly on \(x_k,\dots,x_{k+m}\).</p>



<p class="justify-text">In this post, I will focus on sequences that converge sublinearly, that is, with a difference to their limit that goes to zero as an inverse power of \(k\), typically in \(O(1/k)\). </p>



<h2>Richardson extrapolation</h2>



<p class="justify-text">We consider a sequence \((x_k)_{k \geq 0} \in \mathbb{R}^d\), with an asymptotic expansion of the form $$ x_k = x_\ast + \frac{1}{k}\Delta + O\Big(\frac{1}{k^2}\Big), $$ where \(x_\ast \in \mathbb{R}^d\) is the limit of \((x_k)_k\) and \(\Delta\) a vector in \(\mathbb{R}^d\).</p>



<p class="justify-text">The idea behind <a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">Richardson extrapolation</a> [<a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">1</a>] is to combine linearly two iterates taken at two different values of \(k\) so that the zero-th order term \(x_\ast\) is left unchanged, but the first order term in \(1/k\) cancels out. For \(k\) even, we can consider $$  2 x_k – x_{k/2} =  2 \Big( x_\ast + \frac{1}{k} \Delta  +O\Big(\frac{1}{k^2}\Big)  \Big) \, – \Big( x_\ast +  \frac{2}{k} \Delta  + O\Big(\frac{1}{k^2}\Big) \Big)  =  x_\ast +O\Big(\frac{1}{k^2}\Big).$$</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><a href="https://arxiv.org/pdf/1707.06386"><img src="https://francisbach.com/wp-content/uploads/2020/02/reg_k-1024x454.png" alt="" width="404" class="wp-image-2421" height="179" /></a>Illustration of Richardson extrapolation. Iterates (in black) with their first-order expansions (in red). The deviations (represented by circles) are of order \(O(1/k^2)\). Adapted from [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="https://arxiv.org/pdf/2002.02835">2</a>].  </figure></div>



<p class="justify-text">The key benefit of Richardson extrapolation is that we only need to know that the leading term in the asymptotic expansion is proportional to \(1/k\), <em>without the need to know the vector \(\Delta\)</em>. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/richardson_2d.gif" alt="" width="332" class="wp-image-2481" height="280" />Richardson extrapolation in two dimensions. The sequence is of the form \(x_k = \frac{1}{k} \Delta_1 + \frac{(-1)^k}{k^2} \Delta_2\). The extrapolated sequence \(2 x_k – x_{k/2}\) is only plotted for \(k\) even.</figure></div>



<p class="justify-text">In this post, following [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], I will explore situations where Richardson extrapolation can be useful within machine learning. I identified three situations where Richardson extrapolation can be useful (there are probably more):</p>



<ol class="justify-text"><li>Iterates of an optimization algorithms \((x_k)_{k \geq 0}\), and the extrapolation is \( 2x_k – x_{k/2}.\)</li><li>Extrapolation on the step-size of stochastic gradient descent, where we will combine iterates obtained from two different values of the step-size.</li><li>Extrapolation on a regularization parameter.</li></ol>



<p class="justify-text">As we will show, extrapolation techniques come with no significant loss in performance, but in several situations strong gains. It is thus “<a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences">unreasonably effective</a>“.</p>



<h2>Application to optimization algorithms</h2>



<p class="justify-text">We consider an iterate \(x_k\) of an iterative optimization algorithm which is minimizing a function \(f\), thus converging to a global minimizer \(x_\ast\) of \(f\). Then so is \(x_{k/2}\), and thus also $$  x_k^{(1)} = 2x_k – x_{k/2}.$$ Therefore, performance is never significantly deteriorated (the risk is essentially to lose half of the iterations). The potential gains depend on the way \(x_k\) converges to \(x_\ast\). The existence of a convergence rate of the form \(f(x_k) -f(x_\ast) = O(1/k)\) or \(O(1/k^2)\) is not enough, as Richardson extrapolation requires a specific direction of asymptotic convergence. As illustrated below, some algorithms are oscillating around their solutions, while some converge with a specific direction. Only the latter ones can be accelerated with Richardson extrapolation, while the former ones are good candidates for <a href="https://francisbach.com/acceleration-without-pain/">Anderson acceleration</a>.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/nonoscillating_oscillating-1024x350.png" alt="" width="476" class="wp-image-2395" height="162" /> Left: Oscillating convergence, where Richardson extrapolation does not lead to any gain. Right: non-oscillating  convergence, with a main direction \(\Delta\) (in red dotted), where Richardson extrapolation can be beneficial if the oscillations orthogonal to the direction \(\Delta\) are negligible compared to convergence along the direction \(\Delta\). </figure></div>



<p class="justify-text"><strong>Averaged gradient descent.</strong> We consider the usual gradient descent algorithm $$x_k = x_{k-1} – \gamma f'(x_{k-1}),$$ where \(\gamma &gt; 0 \) is a step-size, with Polyak-Ruppert averaging [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>]: $$ y_k = \frac{1}{k} \sum_{i=0}^{k-1} x_i.$$ Averaging is key to robustness to potential noise in the gradients [<a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">4</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>]. However it comes with the unintended consequence of losing the exponential forgetting of initial conditions for strongly-convex problems [<a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">6</a>].</p>



<p class="justify-text">A common way to restore exponential convergence (up to the noise level in the stochastic case) is to consider “tail-averaging”, that is, to replace \(y_k\) by the average of only the latest \(k/2\) iterates [<a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">7</a>]. As shown below for \(k\) even, this corresponds exactly to Richardson extrapolation on the sequence \((y_k)_k\): $$ \frac{2}{k} \sum_{i=k/2}^{k-1} x_i = \frac{2}{k} \sum_{i=0}^{k-1} x_i – \frac{2}{k} \sum_{i=0}^{k/2-1} x_i = 2 y_k – y_{k/2}. $$</p>



<p class="justify-text">With basic  assumptions on \(f\), it is shown in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>] that for locally strongly-convex problems: $$y_k = x_\ast + \frac{1}{k} \Delta + O(\rho^k), $$ where  \(\displaystyle \Delta = \sum_{i=0}^\infty (x_i – x_\ast)\) and \(\rho \in (0,1)\) depends on the condition number of \(f”(x_\ast)\). This is illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/averaged_gradient.png" alt="" width="342" class="wp-image-2507" height="250" />Averaged gradient descent on a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> problem in dimension \(d=400\), and with \(n=4000\) observations. For the regular averaged recursion, the line in the log-log plot has slope \(-2\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].</figure></div>



<p class="justify-text">We can make the following observations:</p>



<ul class="justify-text"><li>Before Richardson extrapolation, the asymptotic convergence rate after averaging is of order \(O(1/k^2)\), which is better than the usual \(O(1/k)\) upper-bound for the rate of gradient descent, but with a stronger assumption that in fact leads to exponential convergence before averaging.</li><li>While \(\Delta\) has a simple expression, it cannot be computed in practice (but Richardson extrapolation does not need to know it).</li><li>Richardson extrapolation leads to an exponentially convergent algorithm from an algorithm converging asymptotically in \(O(1/k^2)\).</li></ul>



<p class="justify-text"><strong>Accelerated gradient descent.</strong> Above, we considered averaged gradient descent, which is asymptotically converging as \(O(1/k^2)\), and on which Richardson extrapolation could be used with strong gains. Is it possible also for the accelerated gradient descent method [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">8</a>], which has a (non-asymptotic) convergence rate of \(O(1/k^2)\) for convex functions?</p>



<p class="justify-text">It turns out that the behavior of the iterates of accelerated gradient descent is exactly of the form depicted in the left plot of the figure above: that is, the iterates \(x_k\) oscillate around the optimum [<a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">9</a>, <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">10</a>], and Richardson extrapolation is of no help, but is not degrading performance too much. See below for an illustration. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/accelerated_gradient.png" alt="" width="332" class="wp-image-2509" height="243" />Accelerated gradient descent on a quadratic optimization problem in dimension \(d=1000\). See experimental details in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>].  </figure></div>



<p class="justify-text"><strong>Other algorithms.</strong> It is tempting to test it on other optimization algorithms. For example, as explained in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>], Richardson extrapolation can be used to the <a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe</a> algorithm, where sometimes it helps, sometimes it doesn’t. Others could be tried.</p>



<h2>Extrapolation on the step-size of stochastic gradient descent</h2>



<p class="justify-text">While above we have focused on Richardson extrapolation applied to the number of iterations of an iterative algorithm, it is most often used in integration methods (for computing integrals or solving ordinary differential equations), and then often referred to as <a href="https://en.wikipedia.org/wiki/Romberg%27s_method">Romberg-Richardson extrapolation</a>. Within machine learning, in a similar spirit, this can be applied to the step-size of stochastic gradient descent [<a href="https://arxiv.org/pdf/1707.06386">3</a>, <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">11</a>], which I now describe.</p>



<p class="justify-text">We consider the minimization of a function \(F(x)\) defined on \(\mathbb{R}^d\), which can be written as an expectation as $$F(x) = \mathbb{E}_{z} f(x,z).$$ We assume that we have access to \(n\) independent and identically distributed observations (i.i.d.) \(z_1,\dots,z_n\). This is a typical scenario in machine learning, where \(f(x,z)\) represents the loss for the predictor parameterized by \(x\) on the observation \(z\). </p>



<p class="justify-text">The stochastic gradient method is particularly well adapted, and we consider here a single pass, as $$x_i= x_{i-1} – \gamma f'(x_{i-1},z_i),$$ where the gradient is taken with respect to the first variable, for \(i = 1,\dots,n\). It is known that with a constant step-size, when \(n\) tends to infinity, \(x_n\) will <em>not</em> converge to the minimizer \(x_\ast\) of \(F\), as the algorithm always moves [<a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">16</a>], as illustrated below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/logistic_2d-1.gif" alt="" width="403" class="wp-image-2488" height="279" />Stochastic gradient descent on a logistic regression problem: (blue) without averaging, (red) with averaging.</figure></div>



<p class="justify-text">One way to damp the oscillations is to consider averaging, that is, $$ y_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i$$ (we consider uniform averaging for simplicity). For least-squares regression, this leads to a converging algorithm [<a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">12</a>] with attractive properties for ill-conditioned problems (see also <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/">January’s blog post</a>). However, for general loss functions, it is shown in [<a href="https://arxiv.org/pdf/1707.06386">3</a>] that \(y_n\) converges to some \(y^{(\gamma)} \neq x_\ast\). There is a bias due to a step-size \(\gamma\) that does not go to zero. In order to apply Richardson extrapolation, together with Aymeric Dieuleveut and Alain Durmus [<a href="https://arxiv.org/pdf/1707.06386">3</a>], we showed that $$ y^{(\gamma)} = x_\ast + \gamma \Delta + O(\gamma^2),$$ for some \(\Delta \in \mathbb{R}^d\) with some complex expression. Thus, we have $$2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2),$$ thus gaining one order. If we consider the iterate \(y_n^{(\gamma)}\) and \(y_n^{(2 \gamma)}\) associated to the two step-sizes \(\gamma\) and \(2 \gamma\), the linear combination $$2 y_n^{(\gamma)} – y_n^{(2\gamma)} $$ has an improved behavior as it tends to \(2 y^{(\gamma)} – y^{(2\gamma)} = x_\ast + O(\gamma^2)\): it remains not convergent, but get to way smaller values. See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/SGD_logistic-1.png" alt="" width="359" class="wp-image-2525" height="274" />Averaged stochastic gradient descent on a logistic regression problem in dimension 20.</figure></div>



<p class="justify-text"><strong>Higher-order extrapolation.</strong> If we can accelerate a sequence by extrapolation, why not extrapolate the extrapolated sequence? This is possible if we have an higher-order expansion of the form $$ y^{(\gamma)} = \theta_\ast + \gamma \Delta_1 + \gamma^2 \Delta_2 + O(\gamma^3),$$ for some (typically unknown) vectors \(\Delta_1\) and \(\Delta_2\). Then, the sharp reader can check that $$3 y_n^{(\gamma)} – 3 y_n^{(2\gamma)} +  y_n^{(3\gamma)}, $$ will lead to cancellation of the first two orders \(\gamma\) and \(\gamma^2\). This is illustrated above for SGD.</p>



<p class="justify-text">Then, why not extrapolate the extrapolation of the extrapolated sequence? One can check that $$4 y_n^{(\gamma)} – 6 y_n^{(2\gamma)} + 4  y_n^{(3\gamma)}  -y_n^{(4\gamma)}, $$ will lead to cancellation of the first three orders of an expansion of \(y^{(\gamma)}\). The <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> aficionados have already noticed the pattern there, and checked that $$ \sum_{i=1}^{m+1} (-1)^{i-1} { m+1 \choose i} y_n^{(i\gamma)}$$ will lead to cancellations of the first \(m\) orders.</p>



<p class="justify-text">Then, why not go on forever? First because \(m+1\) recursions have to be run in parallel, and second, because the constant in front of the term in \(\gamma^{m+1}\) typically explodes, a phenomenon common to many expansion methods.</p>



<h2>Extrapolation on a regularization parameter</h2>



<p class="justify-text">We now explore the application of Richardson extrapolation to regularization methods. In a nutshell, regularization allows to make an estimation problem more stable (less subject to variations for statistical problems) or the algorithm faster (for optimization problems). However, regularization adds a bias that needs to be removed. In this section, we apply Richardson extrapolation to the regularization parameter to reduce this bias. I will only present an application to smoothing for non-smooth optimization (see an application to  ridge regression in [<a href="https://hal.archives-ouvertes.fr/hal-02470950/document">2</a>]).</p>



<p class="justify-text"><strong>Non-smooth optimization problems</strong>. We consider the minimization of a convex function of the form \(f = h + g\), where \(h\) is smooth and \(g\) is non-smooth. These optimization problems are ubiquitous in machine learning and signal processing, where the lack of smoothness can come from (a) non-smooth losses such as max-margin losses used in support vector machines and more generally structured output classification [<a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">13</a>], and (b) sparsity-inducing regularizers (see, e.g., [<a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">14</a>] and references therein). While many algorithms can be used to deal with this non-smoothness, we consider a classical smoothing technique below.</p>



<p class="justify-text"><strong>Nesterov smoothing</strong>. In this section, we consider the smoothing approach of Nesterov [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>] where the non-smooth term is “smoothed” into \(g_\lambda\), where \(\lambda\) is a regularization parameter, and accelerated gradient descent is used to minimize \(h+g_\lambda\). </p>



<p class="justify-text">A typical way of smoothing the function \(g\) is to add \(\lambda\) times a strongly convex regularizer (such as the squared Euclidean norm) to the Fenchel conjugate of \(g\); this leads to a function \(g_\lambda\) which has a smoothness constant (defined as the maximum of the largest eigenvalues of all Hessians) proportional to \(1/\lambda\), with a uniform error of \(O(\lambda)\) between \(g\) and \(g_\lambda\). Given that accelerated gradient descent leads to an iterate with excess function values proportional to \(1/(\lambda k^2)\) after \(k\) iterations, with the choice of \(\lambda \propto 1/k\), this leads to an excess in function values proportional to \(1/k\), which improves on the subgradient method which converges in \(O(1/\sqrt{k})\). Note that the amount of regularization depends on the number of iterations, so that this smoothing method is not “anytime”.</p>



<p class="justify-text"><strong>Richardson extrapolation.</strong> If we denote by \(x_\lambda\) the minimizer of \(h+g_\lambda\) and \( x_\ast\) the global minimizer of \( f=h+g\), if we can show that \( x_\lambda = x_\ast + \lambda \Delta + O(\lambda^2)\), then \( x^{(1)}_\lambda = 2 x_\lambda – x_{2\lambda} = O(\lambda^2)\) and we can expand \( f(x_\lambda^{(1)})  = f(x_\ast)  + O(\lambda^2)\), which is better than the \(O(\lambda)\) approximation without extrapolation. </p>



<p class="justify-text">Then, given a number of iterations \(k\), with \( \lambda \propto k^{-2/3}\), to balance the two terms \( 1/(\lambda k^2)\) and \( \lambda^2\),  we get an overall convergence rate for the non-smooth problem of \( k^{-4/3}\). </p>



<p class="justify-text"><strong>\(m\)-step Richardson extrapolation</strong>. Like above for the step-size, we can also consider \(m\)-step Richardson extrapolation \(x_{\lambda}^{(m)}\), which leads to a bias proportional to \(\lambda^{m+1}\). Thus, if we consider \(\lambda \propto 1/k^{2/(m+2)}\), to balance the terms \(1/(\lambda k^2)\) and \(\lambda^{m+1}\), we get an error for the non-smooth problem of \(1/k^{2(m+1)/(m+2)}\), which can get arbitrarily close to \(1/k^2\) when \(m\) gets large. The downsides (like for the extrapolation on the step-size above) are that (a) the constants in front of the asymptotic equivalent may blow up (a classical problem in high-order expansions), and (b) \(m\)-step extrapolation requires running the algorithm \(m\) times (this can be down in parallel). In the experiment below, 3-step extrapolation already brings in most of the benefits.</p>



<p class="justify-text">In order to experimentally study the benefits of extrapolation, for the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a> optimization problem, and for a series of regularization parameters equal to \(2^{i}\) for \(i\) between \(-18\) and \(1\) (sampled every \(1/5\)), we run accelerated gradient descent on \(h+g_\lambda\) and we plot the value of \(f(x)-f(x_\ast)\) for the various estimates, where for each number of iterations, we minimize over the regularization parameter. This is an oracle version of varying \(\lambda\) as a function of the number of iterations. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img src="https://francisbach.com/wp-content/uploads/2020/02/smoothing.png" alt="" width="351" class="wp-image-2530" height="255" />Excess function values as a function of the number of iterations, <em>taking into account that \(m\)-step Richardson extrapolation requires \(m\)-times more iterations</em>. There is indeed a strong improvement approaching the rate \(1/k^2\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">These last two blog posts were dedicated to acceleration techniques coming from numerical analysis. They are cheap to implement, typically do not interfere with the underlying algorithm, and when used in the appropriate situation, can bring in significant speed-ups.</p>



<p class="justify-text">Next month, I will most probably host an invited post by my colleague <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a>, who will explain how machines can <s>replace</s> help researchers that prove bounds on optimization algorithms.</p>



<h2>References</h2>



<p class="justify-text">[1] Lewis Fry Richardson. <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1911.0009">The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam</a>. <em>Philosophical Transactions of the Royal Society of London, Series A</em>, 210(459-470):307–357, 1911.<br />[2] Francis Bach. <a href="https://arxiv.org/pdf/2002.02835">On the Effectiveness of Richardson Extrapolation in Machine Learning</a>. Technical report, arXiv:2002.02835, 2020.<br />[3] Aymeric Dieuleveut, Alain Durmus, Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains</a>. To appear in <em>The Annals of Statistics</em>, 2019.<br />[4] Boris T. Polyak,  Anatoli B. Juditsky. <a href="https://epubs.siam.org/doi/pdf/10.1137/0330046">Acceleration of stochastic approximation by averaging</a>. <em>SIAM journal on control and optimization</em> 30(4):838-855, 1992.<br />[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>. </em><a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br />[6] Francis Bach, Eric Moulines. <a href="https://papers.nips.cc/paper/4316-non-asymptotic-analysis-of-stochastic-approximation-algorithms-for-machine-learning.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. <em>Advances in Neural Information Processing Systems</em>, 2011.<br />[7] Prateek Jain, Praneeth Netrapalli, Sham Kakade, Rahul Kidambi, Aaron Sidford. <a href="http://jmlr.org/papers/volume18/16-595/16-595.pdf">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</a>. <em>The Journal of Machine Learning Research</em>, 18(1), 8258-8299, 2017.<br />[8] Yurii E. Nesterov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=dan&amp;paperid=46009&amp;what=fullt&amp;option_lang=eng">A method of solving a convex programming problem with convergence rate \(O(1/k^2)\)</a>, <em>Doklady Akademii Nauk SSSR</em>, 269(3):543–547, 1983.<br />[9] Weijie Su, Stephen Boyd, and Emmanuel J. Candes. <a href="http://jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1):5312-5354, 2016.<br />[10] Nicolas Flammarion, and Francis Bach. <a href="http://proceedings.mlr.press/v40/Flammarion15.pdf">From Averaging to Acceleration, There is Only a Step-size</a>. <em>Proceedings of the International Conference on Learning Theory (COLT)</em>, 2015. <br />[11] Alain Durmus, Umut Simsekli, Eric Moulines, Roland Badeau, and Gaël Richard. <a href="http://papers.nips.cc/paper/6514-stochastic-gradient-richardson-romberg-markov-chain-monte-carlo.pdf">Stochastic gradient Richardson-Romberg Markov chain Monte Carlo</a>. In <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2016.<br />[12] Francis Bach and Eric Moulines. <a href="https://papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-approximation-with-convergence-rate-o1n.pdf">Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)</a>. <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2013.<br />[13] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. <a href="https://icml.cc/Conferences/2005/proceedings/papers/113_StructuredPrediction_TaskarEtAl.pdf">Learning structured prediction models: A large margin approach</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2005.<br />[14] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. <a href="https://www.di.ens.fr/~fbach/bach_jenatton_mairal_obozinski_FOT.pdf">Optimization with sparsity-inducing penalties</a>. Foundations and Trends in Machine Learning, 4(1):1–106, 2012<br />[15] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming , 103(1):127–152, 2005.<br />[16] Georg Ch. Pflug. <a href="https://epubs.siam.org/doi/pdf/10.1137/0324039">Stochastic minimization with constant step-size: asymptotic laws</a>. <em>SIAM Journal on Control and Optimization</em>, (24)4:655-666, 1986.</p>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/richardson-extrapolation/"><span class="datestr">at March 01, 2020 12:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/">postdoc at UC San Diego (apply by March 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UCSD has several Postdoctoral Fellowships, aimed at preparing outstanding researchers for academic and leadership careers in Data Science. Areas of interest include both theoretical and practical aspects of machine learning, statistics, algorithms, and their applications. Applications will be reviewed until positions are filled.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br />
Email: shachar.lovett@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/03/01/postdoc-at-uc-san-diego-apply-by-march-1-2020/"><span class="datestr">at March 01, 2020 01:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12321">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12321">PAPRIKA: Private Online False Discovery Rate Control</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Wanrong.html">Wanrong Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cummings:Rachel.html">Rachel Cummings</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12321">PDF</a><br /><b>Abstract: </b>In hypothesis testing, a false discovery occurs when a hypothesis is
incorrectly rejected due to noise in the sample. When adaptively testing
multiple hypotheses, the probability of a false discovery increases as more
tests are performed. Thus the problem of False Discovery Rate (FDR) control is
to find a procedure for testing multiple hypotheses that accounts for this
effect in determining the set of hypotheses to reject. The goal is to minimize
the number (or fraction) of false discoveries, while maintaining a high true
positive rate (i.e., correct discoveries).
</p>
<p>In this work, we study False Discovery Rate (FDR) control in multiple
hypothesis testing under the constraint of differential privacy for the sample.
Unlike previous work in this direction, we focus on the online setting, meaning
that a decision about each hypothesis must be made immediately after the test
is performed, rather than waiting for the output of all tests as in the offline
setting. We provide new private algorithms based on state-of-the-art results in
non-private online FDR control. Our algorithms have strong provable guarantees
for privacy and statistical performance as measured by FDR and power. We also
provide experimental results to demonstrate the efficacy of our algorithms in a
variety of data environments.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12321"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12159">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12159">Random-Order Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12159">PDF</a><br /><b>Abstract: </b>This chapter introduces the \emph{random-order model} in online algorithms.
In this model, the input is chosen by an adversary, then randomly permuted
before being presented to the algorithm. This reshuffling often weakens the
power of the adversary and allows for improved algorithmic guarantees. We show
such improvements for two broad classes of problems: packing problems where we
must pick a constrained set of items to maximize total value, and covering
problems where we must satisfy given requirements at minimum total cost. We
also discuss how random-order model relates to other stochastic models used for
non-worst-case competitive analysis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12159"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12119">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12119">Tree Polymatrix Games are PPAD-hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deligkas:Argyrios.html">Argyrios Deligkas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fearnley:John.html">John Fearnley</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Savani:Rahul.html">Rahul Savani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12119">PDF</a><br /><b>Abstract: </b>We prove that it is PPAD-hard to compute a Nash equilibrium in a tree
polymatrix game with twenty actions per player. This is the first PPAD hardness
result for a game with a constant number of actions per player where the
interaction graph is acyclic. Along the way we show PPAD-hardness for finding
an $\epsilon$-fixed point of a 2D LinearFIXP instance, when $\epsilon$ is any
constant less than $(\sqrt{2} - 1)/2 \approx 0.2071$. This lifts the hardness
regime from polynomially small approximations in $k$-dimensions to constant
approximations in two-dimensions, and our constant is substantial when compared
to the trivial upper bound of $0.5$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12119"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.12034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.12034">The Complexity of Contracts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duetting:Paul.html">Paul Duetting</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Talgam=Cohen:Inbal.html">Inbal Talgam-Cohen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.12034">PDF</a><br /><b>Abstract: </b>We initiate the study of computing (near-)optimal contracts in succinctly
representable principal-agent settings. Here optimality means maximizing the
principal's expected payoff over all incentive-compatible contracts---known in
economics as "second-best" solutions. We also study a natural relaxation to
approximately incentive-compatible contracts.
</p>
<p>We focus on principal-agent settings with succinctly described (and
exponentially large) outcome spaces. We show that the computational complexity
of computing a near-optimal contract depends fundamentally on the number of
agent actions. For settings with a constant number of actions, we present a
fully polynomial-time approximation scheme (FPTAS) for the separation oracle of
the dual of the problem of minimizing the principal's payment to the agent, and
use this subroutine to efficiently compute a delta-incentive-compatible
(delta-IC) contract whose expected payoff matches or surpasses that of the
optimal IC contract.
</p>
<p>With an arbitrary number of actions, we prove that the problem is hard to
approximate within any constant c. This inapproximability result holds even for
delta-IC contracts where delta is a sufficiently rapidly-decaying function of
c. On the positive side, we show that simple linear delta-IC contracts with
constant delta are sufficient to achieve a constant-factor approximation of the
"first-best" (full-welfare-extracting) solution, and that such a contract can
be computed in polynomial time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.12034"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11880">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11880">Stochastic Matching with Few Queries: $(1-\varepsilon)$ Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behnezhad:Soheil.html">Soheil Behnezhad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Derakhshan:Mahsa.html">Mahsa Derakhshan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11880">PDF</a><br /><b>Abstract: </b>Suppose that we are given an arbitrary graph $G=(V, E)$ and know that each
edge in $E$ is going to be realized independently with some probability $p$.
The goal in the stochastic matching problem is to pick a sparse subgraph $Q$ of
$G$ such that the realized edges in $Q$, in expectation, include a matching
that is approximately as large as the maximum matching among the realized edges
of $G$. The maximum degree of $Q$ can depend on $p$, but not on the size of
$G$.
</p>
<p>This problem has been subject to extensive studies over the years and the
approximation factor has been improved from $0.5$ to $0.5001$ to $0.6568$ and
eventually to $2/3$. In this work, we analyze a natural sampling-based
algorithm and show that it can obtain all the way up to $(1-\epsilon)$
approximation, for any constant $\epsilon &gt; 0$.
</p>
<p>A key and of possible independent interest component of our analysis is an
algorithm that constructs a matching on a stochastic graph, which among some
other important properties, guarantees that each vertex is matched
independently from the vertices that are sufficiently far. This allows us to
bypass a previously known barrier towards achieving $(1-\epsilon)$
approximation based on existence of dense Ruzsa-Szemer\'edi graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11880"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2002.11795">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2002.11795">Quantum Distributed Complexity of Set Disjointness on a Line</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Frederic Magniez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayak:Ashwin.html">Ashwin Nayak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2002.11795">PDF</a><br /><b>Abstract: </b>Given $x,y\in\{0,1\}^n$, Set Disjointness consists in deciding whether
$x_i=y_i=1$ for some index $i \in [n]$. We study the problem of computing this
function in a distributed computing scenario in which the inputs $x$ and $y$
are given to the processors at the two extremities of a path of length $d$. Set
Disjointness on a Line was introduced by Le Gall and Magniez (PODC 2018) for
proving lower bounds on the quantum distributed complexity of computing the
diameter of an arbitrary network in the CONGEST model.
</p>
<p>In this work, we prove an unconditional lower bound of
$\widetilde{\Omega}(\sqrt[3]{n d^2}+\sqrt{n} )$ rounds for Set Disjointness on
a Line. This is the first non-trivial lower bound when there is no restriction
on the memory used by the processors. The result gives us a new lower bound of
$\widetilde{\Omega} (\sqrt[3]{n\delta^2}+\sqrt{n} )$ on the number of rounds
required for computing the diameter $\delta$ of any $n$-node network with
quantum messages of size $O(\log n)$ in the CONGEST model.
</p>
<p>We draw a connection between the distributed computing scenario above and a
new model of query complexity. In this model, an algorithm computing a
bi-variate function $f$ has access to the inputs $x$ and $y$ through two
separate oracles $O_x$ and $O_y$, respectively. The restriction is that the
algorithm is required to alternately make $d$ queries to $O_x$ and $d$ queries
to $O_y$. The technique we use for deriving the round lower bound for Set
Disjointness on a Line also applies to the number of rounds in this query
model. We provide an algorithm for Set Disjointness in this query model with
round complexity that matches the round lower bound stated above, up to a
polylogarithmic factor. In this sense, the round lower bound we show for Set
Disjointness on a Line is optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2002.11795"><span class="datestr">at March 01, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/02/29/leap-day-linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/02/29/leap-day-linkage.html">Leap day linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://mathstodon.xyz/@jeffgerickson/103671508553971700">Jeff Erickson links</a> one of the first non-trivial <a href="https://en.wikipedia.org/wiki/Straight_skeleton">straight skeletons</a> (as a roof model), from <em>Kotirte Ebenen (Kotirte Projektionen) und deren Anwendung</em> by Gustav A. von Peschka (1877).</p>
  </li>
  <li>
    <p><a href="https://math.unice.fr/~indira/Mygifs.html">Indira Lara Chatterji has some nice open-licensed animated gifs of concepts in low-dimensional geometry and topology</a> (<a href="https://mathstodon.xyz/@11011110/103684262226520136"></a>).</p>
  </li>
  <li>
    <p><a href="https://harvardmagazine.com/2012/11/absolutely-beautiful">Geometric art of Morton C. Bradley, Jr.</a> (<a href="https://mathstodon.xyz/@11011110/103695425477296068"></a>, <a href="https://www.georgehart.com/rp/MortonBradley/Morton-Bradley.html">see also</a>). As <a href="https://joelcooper.wordpress.com/2012/06/21/geometric-art-of-morton-c-bradley-jr/">Joel Cooper writes</a>, “I was fascinated first by the complex yet harmonious geometry of these forms, but it is really the use of color that makes these sculptures so transcendent. These sculptures are really exercises in color theory in three dimensions.”</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2020/02/01/subliminal-graph-duals/">Subliminal graph duals</a> and <a href="https://rjlipton.wordpress.com/2020/02/11/using-negative-nodes-to-count/">using negative nodes to count</a> (<a href="https://mathstodon.xyz/@11011110/103701104439329493"></a>). These posts abstract a graph to a 2-polymatroid, the function that maps a subset of edges to the number of vertices it touches. This resembles matroid rank, but a single edge has rank 2. This leads to a notion of a dual, which usually does not come from a graph, with an interesting operation of “exploding” edges dual to edge deletion.</p>
  </li>
  <li>
    <p>The latest <em>Notices</em> has profiles of mathematicians <a href="http://www.ams.org/journals/notices/202003/rnoti-p327.pdf">Fan Chung</a>, <a href="http://www.ams.org/journals/notices/202003/rnoti-p345.pdf">Olga Taussky-Todd</a>, and <a href="http://www.ams.org/journals/notices/202003/rnoti-p368.pdf">Dorothy Hoover</a> (<a href="https://mathstodon.xyz/@mathcination/103690730215482537"></a>).</p>
  </li>
  <li>
    <p>My graph algorithm lectures on stable matching have shifted their terminology and metaphors to stable matching from stable marriage (outdated, sexist, heteronormative, potentially offensive, and not a good fit for the applications) but so far Wikipedia hasn’t. Maybe the ngram below explains why: there’s a base level of colloquial usage of “stable marriage” masking the popularity of “matching” in technical usage. Google Scholar shows a clearer picture: 3940 hits for “stable matching” since 2016, 2710 for “stable marriage”. (<a href="https://mathstodon.xyz/@11011110/103717212351898100"></a>).</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/stable-marriage-vs-matching.png" alt="Stable matching vs stable marriage Google ngram" /></p>
  </li>
  <li>
    <p><a href="https://divisbyzero.com/2019/05/01/braidtiles/">Braid tiles</a> (<a href="https://mathstodon.xyz/@11011110/103723728540456919"></a>). Print your own onto cardstock, cut out, and rearrange, for all your hands-on braid-group experimental-mathematics needs.</p>
  </li>
  <li>
    <p><a href="https://gendergapinscience.files.wordpress.com/2020/02/final_report_20200204-1.pdf">Final report of the international project “A Global Approach to the Gender Gap in Mathematical, Computing, and Natural Sciences: How to Measure It, How to Reduce It?”</a> (<a href="https://mathstodon.xyz/@11011110/103729947676143732"></a>, <a href="https://euro-math-soc.eu/news/20/02/26/preliminary-report-project-global-approach-gender-gap-mathematical-computing-and">via</a>), including recommendations for instructors and organizations.</p>
  </li>
  <li>
    <p><a href="https://www.vice.com/en_us/article/4agamm/the-worlds-second-largest-wikipedia-is-written-almost-entirely-by-one-bot">The world’s second largest Wikipedia is written almost entirely by one bot</a> (<a href="https://mathstodon.xyz/@11011110/103735065395757430"></a>, <a href="https://news.ycombinator.com/item?id=22403626">via</a>). The language is Cebuano, from the Philippines. Consensus on the English Wikipedia is that automatic translation and automatic content generation are not good enough, but the different languages of Wikipedia are run independently and have different standards from each other.</p>
  </li>
  <li>
    <p>You can encode complex numbers as pairs of real numbers, their real and imaginary parts, and perform complex arithmetic using only real-number operations. But can you go the other way, and encode real numbers somehow using complex arithmetic? <a href="http://jdh.hamkins.org/the-real-numbers-are-not-interpretable-in-the-complex-field/">Joel David Hamkins gives a simple proof of the folklore result that the answer is no</a> (<a href="https://mathstodon.xyz/@11011110/103738094179310308"></a>). The short reason why not: there are too few equivalence classes of k-tuples of them to use as representatives of the reals.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/351743/440">Polyhedra that can pack 3-space only in a non-vertex-to-vertex fashion</a> (<a href="https://mathstodon.xyz/@11011110/103744463137230104"></a>). The tiling by congruent convex polyhedra in Michael Korn’s answer is very pretty, but a proof that it is the only possible tiling by these polyhedra is still lacking (although it appears very likely to be true).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/02/29/leap-day-linkage.html"><span class="datestr">at February 29, 2020 02:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4626">Freeman Dyson and Boris Tsirelson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Today, as the world braces for the possibility of losing millions of lives to the new coronavirus—to the <em>hunger for pangolin meat</em>, of all things (combined with the evisceration of competent public health agencies like the CDC)—we also mourn the loss of two incredibly special lives, those of Freeman Dyson (age 96) and Boris Tsirelson (age 69).</p>



<p>Freeman Dyson was sufficiently legendary, both within and beyond the worlds of math and physics, that there’s very little I can add to <a href="https://www.nytimes.com/2020/02/28/science/freeman-dyson-dead.html">what’s been said</a>.  It seemed like he was immortal, although I’d heard from mutual friends that his health was failing over the past year.  When I spent a year as a postdoc at the Institute for Advanced Study, in 2004-5, I often sat across from Dyson in the common room, while he drank tea and read the news.  That I never once struck up a conversation with him is a regret that I’ll now carry with me forever.</p>



<p>My only exchange with Dyson came when he gave a lecture at UC Berkeley, about how life might persist infinitely far into the future, even after the last stars had burnt out, by feeding off steadily dimishing negentropy flows in the nearly-thermal radiation.  During the Q&amp;A, I challenged Dyson that his proposal seemed to assume an analog model of computation.  But, I asked, once we took on board the quantum-gravity insights of Jacob Bekenstein and others, suggesting that nature behaves like a (quantum) digital computer at the Planck scale, with at most ~10<sup>43</sup> operations per second and ~10<sup>69</sup> qubits per square meter and so forth, wasn’t this sort of proposal ruled out?  “I’m not going to argue with you,” was Dyson’s response.  Yes, he’d assumed an analog computational model; if computation was digital then that surely changed the picture.</p>



<p>Sometimes—and not just with his climate skepticism, but also (e.g.) with his idea that general relativity and quantum mechanics <em>didn’t need to be reconciled</em>, that it was totally fine for the deepest layer of reality to be a patchwork of inconsistent theories—Dyson’s views struck me as not merely contrarian but as a high-level form of trolling.  Even so, Dyson’s book <em><a href="https://www.amazon.com/Disturbing-Universe-Sloan-Foundation-Science/dp/0465016774">Disturbing the Universe</a></em> had had a major impact on me as a teenager, for the sparkling prose as much as for the ideas.</p>



<p>With Dyson’s passing, the scientific world has lost one of its last direct links to a heroic era, of Einstein and Oppenheimer and von Neumann and a young Richard Feynman, when theoretical physics stood at the helm of civilization like never before or since.  Dyson, who apparently remained not only lucid but <em>mathematically powerful</em> (!) well into his last year, clearly remembered when the Golden Age of science fiction looked like simply sober forecasting; when the smartest young people, rather than denouncing each other on Twitter, dreamed of scouting the solar system in thermonuclear-explosion-powered spacecraft and seriously worked to make that happen.</p>



<p>Boris Tsirelson (<a href="http://www.math.tau.ac.il/~tsirel/">homepage</a>, <a href="https://en.wikipedia.org/wiki/Boris_Tsirelson">Wikipedia</a>), who emigrated from the Soviet Union and then worked at Tel Aviv University (where my wife Dana attended his math lectures), wasn’t nearly as well known as Dyson to the wider world, but was equally beloved within the quantum computing and information community.  <a href="https://en.wikipedia.org/wiki/Tsirelson%27s_bound">Tsirelson’s bound</a>, which he proved in the 1980s, showed that even quantum mechanics could only violate the Bell inequality by so much and by no more, could only let Alice and Bob win the <a href="https://www.cs.cmu.edu/~odonnell/quantum18/lecture07.pdf">CHSH game</a> with probability cos<sup>2</sup>(π/8).  This seminal result anticipated many of the questions that would only be asked decades later with the rise of quantum information.  Tsirelson’s investigations of quantum nonlocality also led him to pose the famous <a href="https://arxiv.org/abs/0812.4305">Tsirelson’s problem</a>: loosely speaking, can all sets of quantum correlations that can arise from an infinite amount of entanglement, be arbitrarily well approximated using <em>finite</em> amounts of entanglement?  The spectacular answer—no—was only announced one month ago, as a corollary of the <a href="https://www.scottaaronson.com/blog/?p=4512">MIP*=RE breakthrough</a>, something that Tsirelson happily lived to see although I don’t know what his reaction was (<font color="red"><strong>update:</strong></font> I’m told that he indeed learned of it in his final weeks, and was happy about it).  Sadly, for some reason, I never met Tsirelson in person, although I did have lively email exchanges with him 10-15 years ago about his problem and other topics.  This <a href="https://www.iqoqi-vienna.at/en/blog/article/boris-tsirelson/?fbclid=IwAR1PrVvK0u5XmnFLLoPMzMN3x9rY1WIdp1wrYZ_yYPlqSGRpXDkollYTCR0">amusing interview</a> with Tsirelson gives some sense for his personality (hat tip to Gil Kalai, who knew Tsirelson well).</p>



<p>Please share any memories of Dyson or Tsirelson in the comments section.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4626"><span class="datestr">at February 29, 2020 01:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4315">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/02/29/lies-damn-lies-and-covid-19/">Lies, damn lies, and covid-19</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In the past two weeks, in Italy, we have been drowning in information about the novel coronavirus infection, but the statistics that have been circulating were lacking proper context and interpretation. Is covid-19 just a stronger form of the flu or is it a threat to the world economy? Yes.</p>
<p>Now that the first community transmissions are happening in my adopted home in the San Francisco Bay Area, I would like to relay to my American readers what I learned from the Italian experience.</p>
<p><span id="more-4315"></span></p>
<p>The most quoted statistics concerned the mortality rate, which has been around 2-2.5% worldwide but only about 0.8% outside of Wuhan, while the flu has a mortality rate around 0.1% and measles around 0.2%. Furthermore, there are no reports of children and infants having died (<a href="http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51">source</a>) or even having been in critical conditions. This means that it is an infection only somewhat more serious than the flu and that, in particular, parents should not be concerned about their small children.</p>
<p>The other important statistics, however, is the number of infected people that require intensive care. Some source give it at 5%, while the number circulating in Italy is 10%. </p>
<p>Now, developed countries have of the order of an ICU bed per 10,000 people: the European average is one bed per 9,000 people, Italy has one per 8,000, Germany has one per 3,400 (<a href="https://link.springer.com/article/10.1007/s00134-012-2627-8">source</a>) and the United States has one per 4,000 (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4351597/">source</a>). </p>
<p>ICU beds tend to have high occupancy, and they are used for victims of stroke and heart attacks, patients recovering from difficult surgeries and so on.</p>
<p>This means that if a person in 1,000 is sick with covid-19 at a given time, the critical cases might overwhelm the capacity of intensive care units. So far, this has happened only in Wuhan, at which point the Chinese government started implementing increasingly strong measures, leading to the lockdown of Wuhan and other places in Hubei province. A ratio of 0.1% of infected people at a regional level has not occurred anywhere outside Hubei province. Even in Lombardy we have “only” about 500 cases out of 10 million people. Two nights ago, however, in the town of Lodi (which has about 50,000 residents) 17 people where brought to the hospital in critical condition from the nearby “red zone”, and many of them had to be routed to other cities because Lodi could not deal with them.</p>
<p>It has been estimated that a covid-19 infected person infects on average another 2-2.5 other people (the “R0” parameter of the disease), typically over a week or less, leading to a rather fast exponential growth. On the Diamond Princess, about 20% of passengers and crew were infected. The only way to reduce the R0 below 1, which would make the epidemic die down, or at least to a value not much bigger than 1 (which would make it grow more slowly) is to reduce person-to-person contact. This is why clusters are locked down, and in places too big to lock down there are measures to reduce such contact, such as closing schools, shutting down sports events, conferences, fairs, concerts etc., and encouraging people to work from home.</p>
<p>The latter “lockdown-lite” measures have stopped the growth of infections in cities like Shanghai, Hong Kong and Singapore, where there were relatively large clusters of cases (much more in Shanghai than in Hong Kong and Singapore). We haven’t seen the number of cases in Korea and Italy leveling off, but this is because testing tends to discover infections that happened one or two weeks prior to the test, so the effect of any measure is only seen in the test numbers a couple of weeks or more after they are implemented.</p>
<p>Meanwhile, Italy is building field hospitals in parking lots, using technology tested during earthquakes, to add hospital bed capacity to the system.</p>
<p>Finally, the measures introduced a week ago are starting to show dramatic economic consequences. The government first emphasized the gravity of the situation, in order to be seen as taking strong actions in a grave moment, and then realized it had caused a PR disaster and now is trying to sound a more optimistic note and to walk back some of the measures that are having the most negative economic consequences. Other governments are unlikely to repeat the same errors, so one should expect official government communication to be biased on the side of not creating alarm.</p>
<p>So here are the lessons for America, in brief:</p>
<ul>
<li>Don’t worry about your young children
</li>
<li>If the government follows public health best practices, prepare to see school closures and cancelations of big events wherever there are signs of community transmission</li>
<li>With containment efforts similar to Hong Kong’s and Singapore’s (and Italy’s, Japan’s and Korea’s), we might see as little as one infection per 10,000-100,000 people, meaning nearly zero risk on an individual basis and no major strain on hospitals, but with significant effects on daily life and on the economy
</li><li>There will be political pressure to send out optimistic messages (see how the White House is asking to pre-approve all communication out of the CDC and the NIH) and to avoid measures that could hurt the economy</li>
<li>If the containment efforts are too light, and infections reach even a person in 1,000 on a regional level, the individual risk is still extremely small, but the health care system will be unable to deal with the critically ill. Note that <em>this hasn’t happened anywhere outside of Wuhan</em> and whatever directives come from the top, it’s hard to imagine that local officials would let it happen in their jurisdiction</li>
<li>Try not to have a stroke, a heart attack, or serious surgery in the next few months</li>
</ul></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/02/29/lies-damn-lies-and-covid-19/"><span class="datestr">at February 29, 2020 12:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16744">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/02/28/reductions-and-jokes/">Reductions and Jokes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Plus a teaching idea that’s no joke?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/02/postcropped.png"><img src="https://rjlipton.files.wordpress.com/2020/02/postcropped.png?w=600" alt="" class="alignright size-full wp-image-16746" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Maths History <a href="http://mathshistory.st-andrews.ac.uk/PictDisplay/Post.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Emil Post was the first to use the formal notion of reduction between problems. We discussed Post’s wonderful work and its relevance to complexity earlier <a href="https://rjlipton.wordpress.com/2010/04/19/a-post-on-post/">here</a>.</p>
<p>
Today Ken and I want to discuss the notion of reduction, and also perhaps some jokes for your amusement.</p>
<p>
Reductions are used throughout mathematics. We regard Post’s definition from 1944 as quintessential. His are called <a href="https://en.wikipedia.org/wiki/Many-one_reduction">many-one</a> reductions. Here is the definition in a functional style that we’ve tried to promote in other cases: A problem <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> reduces to a problem <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> if there is a computable function <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> such that for any instance <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> of problem <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%28x%29+%3D+B%28f%28x%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  A(x) = B(f(x)). " class="latex" title="\displaystyle  A(x) = B(f(x)). " /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> are languages then this gives the familiar condition <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+A+%5Ciff+f%28x%29+%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in A \iff f(x) \in B}" class="latex" title="{x \in A \iff f(x) \in B}" />. But the idea can be more general: <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> can be functions.</p>
<p>
Alan Turing had earlier defined an even more general kind of reduction, in which given <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> one computes multiple <img src="https://s0.wp.com/latex.php?latex=%7By_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_i}" class="latex" title="{y_i}" />, gets the answer <img src="https://s0.wp.com/latex.php?latex=%7BB%28y_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(y_i)}" class="latex" title="{B(y_i)}" /> for each of them, and finally pieces together the answers to obtain <img src="https://s0.wp.com/latex.php?latex=%7BA%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A(x)}" class="latex" title="{A(x)}" />. But this feels more like “expanding” than “reducing.” We want it to be: one <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, one <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. That was Post’s notion.</p>
<p>
</p><p></p><h2> Dating Reductions and Jokes </h2><p></p>
<p></p><p>
The idea of reduction is simple: When confronted with some problem, be lazy: Instead of solving the problem, show that it can be changed and then solved by some previously known method. That is, show that your problem is reduced to someone’s already solved problem. Be lazy.</p>
<p>
In this form, we can think of two mathematical examples that are much older than Post’s:</p>
<ul>
<li>
<em>Logarithms</em>. Multiplying two numbers <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> with many digits or decimals can be cumbersome. If you can compute <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> and its inverse such that <p></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a%5Ccdot+b+%3D+f%5E%7B-1%7D%28f%28a%29+%2B+f%28b%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a\cdot b = f^{-1}(f(a) + f(b)), " class="latex" title="\displaystyle  a\cdot b = f^{-1}(f(a) + f(b)), " /></p>
<p>then your multiplication is reduced to a much easier addition. John Napier gave <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> the name “logarithm” in 1614 and soon William Oughtred built a physical device to automate this reduction. If you are over 40, perhaps you have seen <a href="https://en.wikipedia.org/wiki/Slide_rule">one</a>. </p>
</li><li>
<em>Integration</em>. Do you have a hard integral <img src="https://s0.wp.com/latex.php?latex=%7B%5Cint+A%28x%29+dx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\int A(x) dx}" class="latex" title="{\int A(x) dx}" />? Often tricks of defining <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+f%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y = f(x)}" class="latex" title="{y = f(x)}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bdy+%3D+f%27%28x%29+dx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{dy = f'(x) dx}" class="latex" title="{dy = f'(x) dx}" />, and/or integrating by parts, yields an integral <img src="https://s0.wp.com/latex.php?latex=%7B%5Cint+B%28y%29+dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\int B(y) dy}" class="latex" title="{\int B(y) dy}" /> that is easier to solve, and whose solution completes your answer.
</li></ul>
<p>
The latter example is in Wikipedia’s article on <a href="https://en.wikipedia.org/wiki/Reduction_(mathematics)">reduction</a>. The former, however, has the signature idea of reduction <em>between</em> problems, more than massaging instances of the same problem. What we really want to know is:</p>
<blockquote><p><b> </b> <em> When was the process of transforming between problems first known by the term <b>reduction</b>? </em>
</p></blockquote>
<p></p><p>
We wonder if tracing an old kind of joke can help. The point is that reductions are a neat source of jokes. Here is a classic one, taken from a big <a href="https://www.math.utah.edu/~cherk/mathjokes.html">list</a> of math jokes. The list gives a second form of the joke based on reductions and there are many others. More on the jokes later.</p>
<blockquote><p><b> </b> <em> A physicist and a mathematician are sitting in a faculty lounge. Suddenly, the coffee machine catches on fire. The physicist grabs a waste basket, empties the basket, leaps towards the sink, fills the basket with water, and puts out the fire. As this coffee machine has done this before they agree to keep a waste basket next to the coffee machine filled with water.</em></p><em>
</em><p><em>
The next day, the same two are sitting in the same lounge. Again, the coffee machine catches on fire. This time, the mathematician stands up, grabs the waste basket that is filled with water. Empties it, places some trash in the basket, and hands it to the physicist. Thus reducing the problem to a previously solved one. </em>
</p></blockquote>
<p></p><p>
Instead of putting out a fire, the following <a href="https://thumbs.gfycat.com/DifficultVapidAmericanredsquirrel-size_restricted.gif?fbclid=IwAR2AXbtag_WFTP9bmipr4JOhvViHAQbvEgE8h1oCdG_71IttR28EgcSTqhg">video</a> is about retrieving a shoe that is floating away.</p>
<p>
</p><p></p><h2> Another Example </h2><p></p>
<p></p><p>
Here is an example of reductions that are not so silly and a little less simple.</p>
<p>
Imagine that Alice and Bob are at it again. Bob wants to be able to multiply integers fast and he plans on building a hardware system that stores the answers in a table. Then his hardware system will be able to compute the product of two integers by just looking up the answers. Okay, there are really better ways to do this, but just play along for the moment. </p>
<p><a href="https://rjlipton.files.wordpress.com/2020/02/tables2.jpg"><img src="https://rjlipton.files.wordpress.com/2020/02/tables2.jpg?w=200&amp;h=200" alt="" width="200" class="aligncenter wp-image-16747" height="200" /></a></p>
<p>
Bob’s table is big and he is troubled. The above table has <img src="https://s0.wp.com/latex.php?latex=%7B100%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{100}" class="latex" title="{100}" /> entries just to multiply numbers less than <img src="https://s0.wp.com/latex.php?latex=%7B10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{10}" class="latex" title="{10}" />. Clearly for a more extensive table the cost grows fast. He asks his friend Alice for some help. She says:”Just store the diagonal values and I can show you how to handle the general case.” Here is her old trick. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a+%5Ctimes+b+%3D+%5Cfrac%7B%5Cleft%28%5Cleft%28a+%2B+b%5Cright%29%5E%7B2%7D+-+a%5E%7B2%7D+-+b%5E%7B2%7D%5Cright%29%7D%7B2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a \times b = \frac{\left(\left(a + b\right)^{2} - a^{2} - b^{2}\right)}{2}. " class="latex" title="\displaystyle  a \times b = \frac{\left(\left(a + b\right)^{2} - a^{2} - b^{2}\right)}{2}. " /></p>
<p>Using this allows Bob to just store the diagonal of the multiplication table, and forget all the rest. It is a powerful reduction that shows:</p>
<blockquote><p><b> </b> <em> One can reduce integer multiplication to addition and taking the square of a number. </em>
</p></blockquote>
<p>
For example, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++37+%5Ctimes+15+%26%3D%26+%28+52%5E%7B2%7D+-+37%5E%7B2%7D+-+15%5E%7B2%7D+%29%2F2+%5C%5C++++++++++++++%26%3D%26+%282704+-+1369+-+225%29%2F2+%5C%5C++++++++++++%26%3D%26+555.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}        37 \times 15 &amp;=&amp; ( 52^{2} - 37^{2} - 15^{2} )/2 \\              &amp;=&amp; (2704 - 1369 - 225)/2 \\            &amp;=&amp; 555. \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}        37 \times 15 &amp;=&amp; ( 52^{2} - 37^{2} - 15^{2} )/2 \\              &amp;=&amp; (2704 - 1369 - 225)/2 \\            &amp;=&amp; 555. \end{array} " /></p>
<p>
</p><p></p><h2> Complexity Reductions: No Joke? </h2><p></p>
<p></p><p>
Wikipedia actually gives the above as the first example in its <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">article</a> on the <em>complexity</em> kind of reduction. These kind of reductions not only define NP hardness and completeness, they are really needed to understand what the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P=NP}}" class="latex" title="{\mathsf{P=NP}}" /> problem is really about. </p>
<p>
For example, once one accepts that a language <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> can be represented by a uniform family of Boolean circuits <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" />, one for each length <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> of instances <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" />, the reduction to SAT is quickly defined: The <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" /> have auxiliary inputs <img src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_q%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_1,\dots,y_q}" class="latex" title="{y_1,\dots,y_q}" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q}" class="latex" title="{q}" /> is polynomial in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, such that there is a <img src="https://s0.wp.com/latex.php?latex=%7By+%5Cin+%5C%7B0%2C1%5C%7D%5Eq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y \in \{0,1\}^q}" class="latex" title="{y \in \{0,1\}^q}" /> making <img src="https://s0.wp.com/latex.php?latex=%7BC_n%28x%2Cy%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n(x,y) = 1}" class="latex" title="{C_n(x,y) = 1}" /> if and only if <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in A}" class="latex" title="{x \in A}" />. The circuit <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" /> can consist of binary NAND gates <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />, each with input wires <img src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u,v}" class="latex" title="{u,v}" /> (which may be inputs <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i}" class="latex" title="{x_i}" /> or <img src="https://s0.wp.com/latex.php?latex=%7By_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_j}" class="latex" title="{y_j}" />) and one or more output wires <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> (which may be the overall output <img src="https://s0.wp.com/latex.php?latex=%7Bw_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_0}" class="latex" title="{w_0}" />). The reduction <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> first constructs the Boolean formula </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi_n+%3D+%28w_0%29+%5Cwedge+%5Cbigwedge_%7Bg%7D+%28u+%5Cvee+w%29+%5Cwedge+%28v+%5Cvee+w%29+%5Cwedge+%28%5Cbar%7Bu%7D+%5Cvee+%5Cbar%7Bv%7D+%5Cvee+%5Cbar%7Bw%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \phi_n = (w_0) \wedge \bigwedge_{g} (u \vee w) \wedge (v \vee w) \wedge (\bar{u} \vee \bar{v} \vee \bar{w}). " class="latex" title="\displaystyle  \phi_n = (w_0) \wedge \bigwedge_{g} (u \vee w) \wedge (v \vee w) \wedge (\bar{u} \vee \bar{v} \vee \bar{w}). " /></p>
<p>To apply <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> to a given <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, simply substitute the bits of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for the variables <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i}" class="latex" title="{x_i}" /> and simplify to make the formula <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_x+%3D+f%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi_x = f(x)}" class="latex" title="{\phi_x = f(x)}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi_x}" class="latex" title="{\phi_x}" /> is satisfiable if and only if <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in A}" class="latex" title="{x \in A}" />. The reduction not only proves the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-completeness of SAT (indeed, 3SAT) instantly, it conveys the character both of SAT and what <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" /> is all about.</p>
<p>
This leads us to wonder something about teaching complexity theory. Maybe <b>reductions</b>, not <em>languages</em>, should be the principal objects of study. </p>
<p>
This may seem like a joke but there are benefits. The reductions are composable in ways that languages are not. They carry the source and target problems with them, at least in the form of the function’s arguments and values. Emphasizing reductions highlights the greatest success of complexity theory to date, which is proving relations between problems, rather than its failure to classify languages via lower bounds.</p>
<p>
</p><p></p><h2> More Jokes </h2><p></p>
<p></p><p>
Here are a selection from a big <a href="https://www.math.utah.edu/~cherk/mathjokes.html">list</a> of math jokes that speak most to computing, plus one from <a href="https://www.quora.com/What-are-some-great-Computer-Science-jokes">here</a>. We have embellished a few of them: </p>
<ul>
<li>
A theory professor is one who talks in someone else’s sleep. <p></p>
</li><li>
Two is the oddest prime of all, because it’s the only one that’s even. <p></p>
</li><li>
A student comes to the department with a shiny new coffee cup, the sort of which you get when having won something. They explain: I won it in my Programming Class contest. They asked what <img src="https://s0.wp.com/latex.php?latex=%7B7+%2B+7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{7 + 7}" class="latex" title="{7 + 7}" /> is. I said <img src="https://s0.wp.com/latex.php?latex=%7B12%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{12}" class="latex" title="{12}" /> and got 3rd place. <p></p>
</li><li>
There are 10 types of people in the world, those who see it in binary and those who don’t. <p></p>
</li><li>
An engineer thinks that his equations are an approximation to reality. A physicist thinks reality is an approximation to his equations. A mathematician doesn’t care. A computer scientist makes reality equal the equations. <p></p>
</li><li>
There is no logical foundation for mathematical proof, and Gödel proved it! <p></p>
</li><li>
Mathematics is like checkers in being suitable for the young, not too difficult, amusing, and<br />
<i>without peril to the state</i>. [Plato wrote that 2,400 years ago; they did have <a href="http://www.checkerslounge.com/ancient-checkers.html">checkers</a>.]<br />
Coding, on the other hand…<p></p>
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How should the concept of <em>reduction</em> be taught and emphasized? Can you trace the age of the classic reduction joke?</p>
<p>
There is a setting for multiplication where our Alice and Bob reduction example <em>fails</em>. Two of our latter list of jokes might suggest it to you. What is the issue? </p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2020/02/28/reductions-and-jokes/"><span class="datestr">at February 28, 2020 05:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=19402">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/02/28/remarkable-new-stochastic-methods-in-abf-ronen-eldan-and-renan-gross-found-a-new-proof-for-kkl-and-settled-a-conjecture-by-talagrand/">Remarkable New Stochastic Methods in ABF: Ronen Eldan and Renan Gross Found a New Proof for KKL and Settled a Conjecture by Talagrand</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/rr.png"><img src="https://gilkalai.files.wordpress.com/2019/11/rr.png?w=300&amp;h=282" alt="" width="300" class="alignnone size-medium wp-image-18565" height="282" /></a></p>
<p><span style="color: #ff0000;">The main conjecture from Talagrand’s paper on boundaries and influences was settled by Ronen Eldan and Renan Gross. Their paper introduces a new powerful method to the field of analysis of Boolean functions (ABF).</span></p>
<p>This post is devoted to a new breakthrough paper by Ronen Eldan and Renan Gross, <a href="https://arxiv.org/abs/1909.12067">Concentration on the Boolean hypercube via pathwise stochastic analysis.</a></p>
<p>The paper introduces remarkable new techniques based on stochastic calculus, (that bypass the use of hypercontractivity), that allow to settles several open problems on analysis of Boolean functions. (And on the way, to give a very different proof for KKL.)</p>
<p>Renan Gross recently wrote an excellent high-level explanation of the paper in his  blog: <a href="https://sarcasticresonance.wordpress.com/2020/02/06/new-paper-on-arxiv-concentration-on-the-boolean-hypercube-via-pathwise-stochastic-analysis/">New paper on arXiv: Concentration on the Boolean hypercube via pathwise stochastic analysis.</a> It is accompanied by a <a href="https://sarcasticresonance.wordpress.com/2020/02/05/catastrophic-cubic-crash-course">brief introduction to Boolean analysis, featuring the required Boolean material</a>:</p>
<p> </p>
<p>Before we move on, a trivia question:</p>
<h3><strong>Trivia question:</strong> Who invented the term hypercontractivity?</h3>
<p>(For a hint, see at the end of the post.)</p>
<p>Back to the paper by Ronen Eldan and Renan Gross. Perhaps the best way to explain what was achieved in this paper is to put one after the other the abstracts of the first version followed by the abstract of the second paper.</p>
<p> </p>
<h2>Ronen and Renan’s paper. Version 1, 26 Sept 2019.</h2>
<p><span style="color: #800080;">The title for version 1 was: Stability of Talagrand’s influence inequality</span></p>
<p><span style="color: #800080;">And here is the abstract: </span></p>
<p>We strengthen several classical inequalities concerning the influences of a Boolean function, showing that near-maximizers must have large vertex boundaries. An inequality due to Talagrand states that for a Boolean function</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281%29%7E%7E%7E%7E%7E%5Cmathrm%7Bvar%7D%5Cleft%28f%5Cright%29%5Cleq+C%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%5Cmathrm%7BInf%7D_%7Bi%7D%5Cleft%28f%5Cright%29%7D%7B1%2B%5Clog%5Cleft%281%2F%5Cmathrm%7BInf%7D_%7Bi%7D%5Cleft%28f%5Cright%29%5Cright%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1)~~~~~\mathrm{var}\left(f\right)\leq C\sum_{i=1}^{n}\frac{\mathrm{Inf}_{i}\left(f\right)}{1+\log\left(1/\mathrm{Inf}_{i}\left(f\right)\right)}" class="latex" title="(1)~~~~~\mathrm{var}\left(f\right)\leq C\sum_{i=1}^{n}\frac{\mathrm{Inf}_{i}\left(f\right)}{1+\log\left(1/\mathrm{Inf}_{i}\left(f\right)\right)}" /></p>
<p>where  <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BInf%7D_%7Bi%7D%5Cleft%28f%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathrm{Inf}_{i}\left(f\right)" class="latex" title="\mathrm{Inf}_{i}\left(f\right)" /> denote the influence of the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th coordinate. We give a lower bound for the size of the vertex boundary of functions saturating this inequality. As a corollary, we show that for sets that satisfy the edge-isoperimetric inequality or the Kahn-Kalai-Linial (KKL) inequality up to a constant, a constant proportion of the mass is in the inner vertex boundary. Our proofs rely on new techniques, based on stochastic calculus, and bypass the use of hypercontractivity common to previous proofs.</p>
<p><span style="color: #800080;">Let me say a few words about it. KKL theorem asserts that the maximal influence of a balanced Boolean function is at least <img src="https://s0.wp.com/latex.php?latex=%5COmega+%28%5Clog+n%2Fn%29+var+%28f%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\Omega (\log n/n) var (f)" class="latex" title="\Omega (\log n/n) var (f)" />. The famous tribe example of Ben-Or and Linial shows that this is tight.  The KKL paper gave some statements about other norms of the influence vector and Talagrand’s inequality stated above (from the paper “On Russo’s 0-1 law”) gives a sharp description of the influence vectors. Ronen and Renan found a new method that gave new proofs for KKL’s theorem and the stronger Talagrand’s inequality. This new methods led to new stability result.</span></p>
<p>Now, lets move to version 2.</p>
<h2>Ronen and Renan’s paper: Version 2: 12 Nov 2019,</h2>
<p>Ronen Eldan and Renan Gross, <a href="https://arxiv.org/abs/1909.12067">Concentration on the Boolean hypercube via pathwise stochastic analysis.</a></p>
<p>Abstract: We develop a new technique for proving concentration inequalities which relate between the variance and influences of Boolean functions. Using this technique, we</p>
<p><strong>1.</strong> Settle a conjecture of Talagrand [Tal97] proving that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%282%29%7E%7E%7E%7E%7E%5Cint_%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bn%7D%7D%5Csqrt%7Bh_%7Bf%7D%5Cleft%28x%5Cright%29%7Dd%5Cmu%5Cgeq+C%5Ccdot%5Cmathrm%7Bvar%7D%5Cleft%28f%5Cright%29%5Ccdot%5Cleft%28%5Clog%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum%5Cmathrm%7BInf%7D_%7Bi%7D%5E%7B2%7D%5Cleft%28f%5Cright%29%7D%5Cright%29%5Cright%29%5E%7B1%2F2%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(2)~~~~~\int_{\left\{ -1,1\right\} ^{n}}\sqrt{h_{f}\left(x\right)}d\mu\geq C\cdot\mathrm{var}\left(f\right)\cdot\left(\log\left(\frac{1}{\sum\mathrm{Inf}_{i}^{2}\left(f\right)}\right)\right)^{1/2}," class="latex" title="(2)~~~~~\int_{\left\{ -1,1\right\} ^{n}}\sqrt{h_{f}\left(x\right)}d\mu\geq C\cdot\mathrm{var}\left(f\right)\cdot\left(\log\left(\frac{1}{\sum\mathrm{Inf}_{i}^{2}\left(f\right)}\right)\right)^{1/2}," /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=h_f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="h_f(x)" class="latex" title="h_f(x)" /> is the number of edges at <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />  along which <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /> changes its value, and <img src="https://s0.wp.com/latex.php?latex=Inf_i%28f%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="Inf_i(f)" class="latex" title="Inf_i(f)" /> is the influence of the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="i" class="latex" title="i" />-th coordinate.</p>
<p><strong>2.</strong> Strengthen several classical inequalities concerning the influences of a Boolean function, showing that near-maximizers must have large vertex boundaries. An inequality due to Talagrand states that for a Boolean function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="f" class="latex" title="f" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bvar%7D%5Cleft%28f%5Cright%29%5Cleq+C%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%5Cmathrm%7BInf%7D_%7Bi%7D%5Cleft%28f%5Cright%29%7D%7B1%2B%5Clog%5Cleft%281%2F%5Cmathrm%7BInf%7D_%7Bi%7D%5Cleft%28f%5Cright%29%5Cright%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mathrm{var}\left(f\right)\leq C\sum_{i=1}^{n}\frac{\mathrm{Inf}_{i}\left(f\right)}{1+\log\left(1/\mathrm{Inf}_{i}\left(f\right)\right)}" class="latex" title="\mathrm{var}\left(f\right)\leq C\sum_{i=1}^{n}\frac{\mathrm{Inf}_{i}\left(f\right)}{1+\log\left(1/\mathrm{Inf}_{i}\left(f\right)\right)}" /></p>
<p>We give a lower bound for the size of the vertex boundary of functions saturating this inequality. As a corollary, we show that for sets that satisfy the edge-isoperimetric inequality or the Kahn-Kalai-Linial inequality up to a constant, a constant proportion of the mass is in the inner vertex boundary.</p>
<p><strong>3.</strong> Improve a quantitative relation between influences and noise stability given by Keller and Kindler.</p>
<p>Our proofs rely on techniques based on stochastic calculus, and bypass the use of hypercontractivity common to previous proofs.</p>
<p><span style="color: #800080;">Let me explain in a few words the main inequality that verified a conjecture by Talagrand. For other advances look at the paper itself.</span></p>
<p><span style="color: #800080;">The famous Margulis-Talagrand inequality asserts that</span></p>
<p><img src="https://s0.wp.com/latex.php?latex=%283%29%7E%7E%7E%7E%7E%5Cint_%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bn%7D%7D%5Csqrt%7Bh_%7Bf%7D%5Cleft%28x%5Cright%29%7Dd%5Cmu%5Cgeq+C%5Ccdot%5Cmathrm%7Bvar%7D%28f%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(3)~~~~~\int_{\left\{ -1,1\right\} ^{n}}\sqrt{h_{f}\left(x\right)}d\mu\geq C\cdot\mathrm{var}(f)." class="latex" title="(3)~~~~~\int_{\left\{ -1,1\right\} ^{n}}\sqrt{h_{f}\left(x\right)}d\mu\geq C\cdot\mathrm{var}(f)." /></p>
<p><span style="color: #800080;">Let me say a little more. Talagrand’s proved in  his paper on Influence and boundary asserts that the right hand is a constant only if <img src="https://s0.wp.com/latex.php?latex=II%28f%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="II(f)" class="latex" title="II(f)" /> – the sum of squares of the influences is a constant. He conjectured that we can actually add the square root of <img src="https://s0.wp.com/latex.php?latex=%5Clog+II%28f%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\log II(f)" class="latex" title="\log II(f)" /> to the RHS.</span></p>
<p>A few remarks:</p>
<ol>
<li> For more background see my old post “<a href="https://gilkalai.wordpress.com/2008/05/26/natis-influence/">Nati’s influence</a>“.</li>
<li>Let me mention that Talagrand’s influence inequality (Equation (1) above) is sharp up to a multiplicative constant. This is shown in a 2015 paper: <a href="https://arxiv.org/abs/1506.06325">On the Converse of Talagrand’s Influence Inequality</a> by Saleet Klein, Amit Levi, Muli Safra, Clara Shikhelman, and Yinon Spinka.</li>
<li> Finding the best constant in KKL’s theorem is a well known challenge. So is the question of understanding balanced Boolean functions on n variables where the maximum influence is <img src="https://s0.wp.com/latex.php?latex=C+%5Clog+n%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="C \log n/n" class="latex" title="C \log n/n" />. Ehud Friedgit in his paper:<a href="http://www.ma.huji.ac.il/~ehudf/docs/kkl.ps"> Influences in Product Spaces, KKL and BKKKL Revisited</a>, have made important conjectures for such Boolean functions.</li>
<li> I expect that the new method will have many applications. Ronen Eldan in a paper <a href="https://arxiv.org/abs/1912.11641">Second-order bounds on correlations between increasing families</a>, found applicaions to correlation inequalities which I will discuss at some other time.</li>
<li> See the last part of <a href="https://gilkalai.wordpress.com/2019/09/22/jeff-kahn-and-jinyoung-park-maximal-independent-sets-and-a-new-isoperimetric-inequality-for-the-hamming-cube/">this post</a> for a description of Talagrand’s various relevant papers.</li>
<li>Renan Gross is building the <a href="https://booleanzoo.weizmann.ac.il/index.php/Main_Page">BooleanZoo</a> in a similar spirit to the famous complexityZoo.</li>
<li>I was in the process of writing a post about progress on ABS, mentioning twelve or so papers, apologizing for not mentioning others, promising to write in details about some of the papers, and making further apologetic remarks. But as this large post does not advance let me start with some of the promises.</li>
</ol>
<p><img src="https://gilkalai.files.wordpress.com/2020/02/talabb.png?w=640" alt="talabb" class="alignnone size-full wp-image-19444" /></p>
<h3>Hint to the trivia question:</h3>
<p><span id="more-19402"></span></p>
<p>The <strong>very same person</strong> also coined the terms: Berry’s phase, TKN^2 integers, almost Matthieu, HVZ, Birman-Schwinger, infrared bounds, diamagnetic inequality, Verblunsky coefficient, Maryland model, ultracontractivity, …</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/02/28/remarkable-new-stochastic-methods-in-abf-ronen-eldan-and-renan-gross-found-a-new-proof-for-kkl-and-settled-a-conjecture-by-talagrand/"><span class="datestr">at February 28, 2020 02:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=26">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/">Friday, February 28 — Jon Kleinberg from Cornell University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The first Foundations of Data Science virtual talk will take place this coming Friday, February 28th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC). <strong>Jon Kleinberg</strong> from Cornell University will speak about “<em>Fairness and Bias in Algorithmic Decision-Making</em>”.</p>



<p><strong>Abstract</strong>: As data science has broadened its scope in recent years, a number of domains have applied computational methods for classification and prediction to evaluate individuals in high-stakes settings. These developments have led to an active line of recent discussion in the public sphere about the consequences of algorithmic prediction for notions of fairness and equity. In part, this discussion has involved a basic tension between competing notions of what it means for such classifications to be fair to different groups. We consider several of the key fairness conditions that lie at the heart of these debates, and in particular how these properties operate when the goal is to rank-order a set of applicants by some criterion of interest, and then to select the top-ranking applicants. The talk will be based on joint work with Sendhil Mullainathan and Manish Raghavan.</p>



<p><a href="https://sites.google.com/view/dstheory">Link to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>. </p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/02/27/friday-february-28-jon-kleinberg-from-cornell-university/"><span class="datestr">at February 27, 2020 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/028">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/028">TR20-028 |  A Super-Quadratic Lower Bound for Depth Four Arithmetic Circuits | 

	Nikhil Gupta, 

	Chandan Saha, 

	Bhargav Thankey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show an $\widetilde{\Omega}(n^{2.5})$ lower bound for general depth four arithmetic circuits computing an explicit $n$-variate degree $\Theta(n)$ multilinear polynomial over any field of characteristic zero. To our knowledge, and as stated in the survey by Shpilka and Yehudayoff (FnT-TCS, 2010), no super-quadratic lower bound was known for depth four circuits over fields of characteristic other than 2 before this work. The previous best lower bound is $\widetilde{\Omega}(n^{1.5})$ shown in the work of Sharma (Master's Thesis, 2017), which is a slight quantitative improvement over the roughly $\Omega(n^{1.33})$ bound obtained by invoking the super-linear lower bound for constant depth circuits in the works of Raz (STOC, 2008) and Shoup and Smolensky (FOCS, 1991).

Our lower bound proof follows the approach of the almost cubic lower bound for depth three circuits in the work by Kayal, Saha and Tavenas (ICALP, 2016) by replacing the shifted partials measure with a suitable variant of the projected shifted partials measure, but it differs from their proof at a crucial step - namely, the way "heavy" product gates are handled. Loosely speaking, a heavy product gate has a relatively high fan-in. Product gates of a depth three circuit compute products of affine forms, and so, it is easy to prune $\Theta(n)$ many heavy product gates by projecting the circuit to a low-dimensional affine subspace as shown in the works by Kayal, Saha and Tavenas (ICALP, 2016) and Shpilka and Wigderson (CCC, 1999). However, in a depth four circuit, the second (from the top) layer of product gates compute products of polynomials having arbitrary degree, and hence it was not clear how to prune such heavy product gates from the circuit. We show that heavy product gates can also be eliminated from a depth four circuit by projecting the circuit to a low-dimensional affine subspace, unless the heavy gates together account for $\widetilde{\Omega}(n^{2.5})$ size. This part of our argument is inspired by a well-known greedy approximation algorithm for the weighted set-cover problem.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/028"><span class="datestr">at February 27, 2020 03:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/027">TR20-027 |  The Power of Many Samples in Query Complexity | 

	Mika Göös, 

	Andrew Bassilakis, 

	Andrew Drucker, 

	Li-Yang Tan, 

	Lunjia Hu, 

	Weiyun Ma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The randomized query complexity $R(f)$ of a boolean function $f\colon\{0,1\}^n\to\{0,1\}$ is famously characterized (via Yao's minimax) by the least number of queries needed to distinguish a distribution $D_0$ over $0$-inputs from a distribution $D_1$ over $1$-inputs, maximized over all pairs $(D_0,D_1)$. We ask: Does this task become easier if we allow query access to infinitely many samples from either $D_0$ or $D_1$? We show the answer is no: There exists a hard pair $(D_0,D_1)$ such that distinguishing $D_0^\infty$ from $D_1^\infty$ requires $\Theta(R(f))$ many queries. As an application, we show that for any composed function $f\circ g$ we have $R(f\circ g) \geq \Omega(\mathrm{fbs}(f)R(g))$ where $\mathrm{fbs}$ denotes fractional block sensitivity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/027"><span class="datestr">at February 26, 2020 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4301">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/02/26/i-live-in-milan-ama/">This year, for Lent, Milan gave up nightlife</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Greetings from Milan, in Italy’s “yellow zone” of areas bordering clusters of coronavirus infections. This week, all schools, universities, museums, theaters  are closed, bars have to close by 6pm, and fairs and conferences are being postponed. The “red zone” of small towns with the clusters of infections is on lockdown.</p>
<p>Milan has been unseasonably warm and sunny in the past few days, and walking through the city, with very light car and pedestrian traffic, has been lovely. This is apparently what the city usually looks like in August, but without the heat and humidity.<br />
<span id="more-4301"></span></p>
<p>Italians have the well deserved fame of being very dramatic people, but we have been taking the inconvenience in stride and with a mix of stoicism and good humor. Maybe I am wrong, but, apart from New York City, if a large American city had a cluster of infections in the suburbs, and it were similarly shut down, we would see riots and looting on the first night.</p>
<p>Over the weekend, the lockdown of the small towns was announced before any announcement of what to do about Milan, and the news showed long lines of people in those small towns waiting to buy groceries in the few open supermarkets. This induced some panic-buying on Sunday and Monday in Milan.</p>
<p>While people in Hong Kong and Singapore panic-bought toilet paper, Italians have mostly been buying pasta. Except, that is, <i>penne lisce</i>, like this Twitter user noted in a broadly shared picture. (Short cuts of pasta, like penne, are usually ribbed, to better catch sauce. Smooth short pasta like penne lisce makes no sense)</p>
<blockquote class="twitter-tweet">
<p lang="it" dir="ltr">Continuo a guardare questa foto fatta prima al supermercato e penso al fatto che il grande sconfitto da questo virus sono le penne lisce che agli italiani fanno cagare pure quando sono presi dal panico e si preparano all’apocalisse. <a href="https://t.co/Lq9Y06jdho">pic.twitter.com/Lq9Y06jdho</a></p>
<p>— 𝚍𝚒𝚘𝚍𝚎𝚐𝚕𝚒𝚣𝚒𝚕𝚕𝚊 <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f926-200d-2642-fe0f.png" alt="🤦‍♂️" style="height: 1em;" class="wp-smiley" /> (@diodeglizilla) <a href="https://twitter.com/diodeglizilla/status/1231680287815426049?ref_src=twsrc%5Etfw">February 23, 2020</a></p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/02/erfosjlw4ampqsc.jpeg?w=584" alt="ERfOsJlW4AMPQsc" class="alignnone size-full wp-image-4305" /></p>
</blockquote>
<p>By Sunday night, the government decided that Lombardy and Veneto would be subject to a series of measures with the goal of reducing person-to-person contact, such as the closures mentioned above of schools, universities, museums etc. The meme below is titled “Here in Milan we are going too far”.</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/02/download.jpeg?w=584" alt="download" class="alignnone size-full wp-image-4306" /></p>
<p>So far, there has been no secondary infection in Rome. (The text says “This is why the coronavirus has not yet reached Rome”.)</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/02/87384731_1300944790095464_2136089705354100736_n.png?w=638&amp;h=391" alt="87384731_1300944790095464_2136089705354100736_n" width="638" height="391" /></p>
<p>(Image credit: <a href="https://www.facebook.com/lepiubellefrasidiosho">facebook.com/lepiubellefrasidiosho</a>)</p>
<p>Most annoyingly, every other newspaper headline or tv news chyron has been of the form “X in the time of coronavirus”. I can’t take it any more. This is the Autumn of journalism, a foretold death of creativity. Journalists are lost in the labyrinth of cliches. I wish all those headline writers one hundred years of solitude. The real news is the kidnapping of new ideas. Anyways, I think I am going to live to tell the tale.</p>
<p>The silver linings are that I have not been paying attention to the Democratic primaries and that I have plenty of free time to restart my planned series of posts on applications of online convex optimization to computational complexity. The next post will be on Impagliazzo’s hard-core lemma, a milestone in the theory of average-case complexity and pseudorandomness.</p>
<p>I am happy to answer any questions in the comments.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/02/26/i-live-in-milan-ama/"><span class="datestr">at February 26, 2020 04:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-02-26-selfish-mining/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-02-26-selfish-mining/">Blockchain Selfish Mining</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proof of Work (PoW) Blockchains implement a form of State Machine Replication (SMR). Unlike classical SMR protocols, they are open, i.e., anyone can join the system, and the system incentivizes participants, called miners, to follow the protocol. Therefore, unlike classical SMR protocols, reasoning about blockchain security relies not only on...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-02-26-selfish-mining/"><span class="datestr">at February 26, 2020 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/026">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/026">TR20-026 |  Spectral Sparsification via Bounded-Independence Sampling | 

	Dean Doron, 

	Jack Murtagh, 

	Salil Vadhan, 

	David Zuckerman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a deterministic, nearly logarithmic-space algorithm for mild spectral sparsification of undirected graphs. Given a weighted, undirected graph $G$ on $n$ vertices described by a binary string of length $N$, an integer $k\leq \log n$ and an error parameter $\varepsilon &gt; 0$, our algorithm runs in space $\tilde{O}(k\log (N\cdot w_{\mathrm{max}}/w_{\mathrm{min}}))$ where $w_{\mathrm{max}}$ and $w_{\mathrm{min}}$ are the maximum and minimum edge weights in $G$, and produces a weighted graph $H$ with $\tilde{O}(n^{1+2/k}/\varepsilon^2)$ expected edges that spectrally approximates $G$, in the sense of Spielmen and Teng [ST04], up to an error of $\varepsilon$.

Our algorithm is based on a new bounded-independence analysis of Spielman and Srivastava's effective resistance based edge sampling algorithm [SS08] and uses results from recent work on space-bounded Laplacian solvers [MRSV17].  In particular, we demonstrate an inherent tradeoff (via upper and lower bounds) between the amount of (bounded) independence used in the edge sampling algorithm, denoted by $k$ above, and the resulting sparsity that can be achieved.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/026"><span class="datestr">at February 26, 2020 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/02/24/should-we-teach-coding-in-high-school/">Should We Teach Coding in High School?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
Robert Sedgewick and Larry Cuban faced off today in the Wall Street Journal (WSJ) on the issue: </p>
<p><a href="https://rjlipton.wordpress.com/2020/02/24/should-we-teach-coding-in-high-school/unknown-1-5/" rel="attachment wp-att-16723"><img width="150" alt="" class="alignright  wp-image-16723" src="https://rjlipton.files.wordpress.com/2020/02/unknown-1-1.jpeg?w=150" /></a><br />
<a href="https://rjlipton.wordpress.com/2020/02/24/should-we-teach-coding-in-high-school/unknown-136/" rel="attachment wp-att-16722"><img width="150" alt="" class="alignright wp-image-16722" src="https://rjlipton.files.wordpress.com/2020/02/unknown-3.jpeg?w=150" /></a></p>
<blockquote><p><b> </b> <em> 	Should everyone be taught coding in high school? </em>
</p></blockquote>
<p></p><p>
Today we will discuss their recent <a href="https://www.wsj.com/articles/should-all-children-learn-to-code-by-the-end-of-high-school-11582513441">comments</a> on this issue.<br />
<span id="more-16720"></span></p>
<p>
Bob is a long-time friend of mine, so I want to say that that up front. He is a professor of computer science at Princeton. Cuban is an emeritus professor of education at Stanford. Note, I do not know Cuban but will call him Larry—I hope that is fine. Besides what they say in the article, Larry wrote a <a href="https://larrycuban.wordpress.com/2017/07/11/coding-the-new-vocationalism-part-1/">three</a>–<a href="https://larrycuban.wordpress.com/2017/07/14/coding-the-new-vocationalism-part-2/">part</a> <a href="https://larrycuban.wordpress.com/2017/07/17/coding-the-new-vocationalism-part-3/">series</a> in 2017 on his own education blog, while Bob was <a href="https://www.washingtonpost.com/news/the-switch/wp/2013/12/11/president-obama-talks-about-teaching-everyone-to-code-this-professor-does-it/">associated</a> with a 2013 White House-led <a href="https://www.wired.com/2013/12/obama-code/">initiative</a> on coding in schools.</p>
<p>
The WSJ article consists of ten short paragraphs by Bob followed by eleven from Larry. This is sequential structure—like with statements <img src="https://s0.wp.com/latex.php?latex=%7BS_1%3B+S_2%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1; S_2;}" class="latex" title="{S_1; S_2;}" />—or like having candidate town-halls for consectuive hours on CNN and such. What we’d like to see is a debate—like having parallel processes that must sync and communicate. Below we imagine one based on statements in the article.</p>
<p>
</p><p></p><h2> A Conversation </h2><p></p>
<p></p><p>
The following is a paraphrase that tries to re-structure some of the article in debate format.</p>
<p></p><p>
<font color="#0044cc"><br />
<em>Bob:</em> <font color="#000000"> Teaching students to code will help them understand logical thinking and foster creativity.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Larry:</i> <font color="#000000"> You could say the same for teaching writing, math, history, and many other subjects. There is no research that shows that coding is better than other topics in this regard.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Bob:</i> <font color="#000000"> I am not aware of any research that shows that each topic that is taught now is better than coding.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Larry:</i> <font color="#000000"> Yes that is true, but consider how durable core education has been for our society. A century ago, industrial groups pushed the federal government to require vocational training in schools for particular industrial and agricultural skills and to establish separate vocational schools. Those undermined the broader goals of social development and civic engagement.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Bob:</i> <font color="#000000"> Technology is basic to much of society’s issues. Perhaps the Iowa caucus fiasco could have been avoided if they had a better understanding of computing.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Larry:</i> <font color="#000000"> I think the main argument for coding is being pushed by technology CEOs. They need more coders. The educational system should not just do what they need. Do you not agree Bob?</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Bob:</i> <font color="#000000">If we teach coding it seems that it may help in lessening economic and gender based gaps. In summary, in the last millennium, education was based on reading, writing, and arithmetic. Perhaps we should now switch to reading, writing, and computing. Coding includes arithmetic and a whole lot more.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Larry:</i> <font color="#000000"> I agree education should help students achieve their potential. I just do not see that coding will do this. And further, data and projections from the U.S. Bureau of Labor Statistics show only an 11 percent increase in IT jobs, from 4.5 to 5 million, between 2018 and 2028. That’s going out a whole decade and still IT will only be about 3 percent of all jobs. Health care will <em>grow</em> in that time by as many jobs as IT has total.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Bob:</i> <font color="#000000"> Those percentages hide much of the benefit. Only a fraction of the thousands I have taught—in person and online—work in tech companies. The rest have gone into a broad variety of careers. Coding literacy is becoming a necessity in health care, social assistance, business services, construction, entertainment, manufacturing, and even politics. </font></font></p><font color="#0044cc"><font color="#000000">
<p>
<font color="#0044cc"><br />
<i>Larry:</i> <font color="#000000"> But what would you cut to make room? Foreign language? History? Arts or music? Or decrease other aspects of math and science? Curricula are already crowded with required courses and frequent testing.</font></font></p><font color="#0044cc"><font color="#000000">
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What should the role of coding be in our society? Who is right?</p>
<p></p><p><br />
[some word and grammar tweaks]</p></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/02/24/should-we-teach-coding-in-high-school/"><span class="datestr">at February 24, 2020 09:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/025">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/025">TR20-025 |  Efficient Isolation of Perfect Matching in O(log n) Genus Bipartite Graphs | 

	Chetan Gupta, 

	Vimal Raj Sharma, 

	Raghunath Tewari</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that given an embedding of an O(log n) genus bipartite graph, one can construct an edge weight function in logarithmic space, with respect to which the minimum weight perfect matching in the graph is unique, if one exists. 

As a consequence, we obtain that deciding whether the graph has a perfect matching or not is in SPL. In 1999, Reinhardt, Allender and Zhou proved that if one can construct a polynomially bounded weight function for a graph in logspace such that it isolates a minimum weight perfect matching in the graph, then the perfect matching problem can be solved in SPL. In this paper, we give a deterministic logspace construction of such a weight function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/025"><span class="datestr">at February 24, 2020 10:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/024">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/024">TR20-024 |  Randomized and Symmetric Catalytic Computation | 

	Samir Datta, 

	Chetan Gupta, 

	Rahul Jain, 

	Vimal Raj Sharma, 

	Raghunath Tewari</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A catalytic Turing machine is a model of computation that is created by equipping a Turing machine with an additional auxiliary tape which is initially filled with arbitrary content; the machine can read or write on auxiliary tape during the computation but when it halts auxiliary tape’s initial content must be restored. In this paper, we study the power of catalytic Turing machines with O(log n)-sized clean tape and a polynomial-sized auxiliary tape.

We introduce the notion of randomized catalytic Turing machine and show that the resulting complexity class CBPL is contained in the class ZPP. We also introduce the notion of symmetricity in the context of catalytic computation and prove that, under a widely believed assumption, in the logspace setting the power of a randomized catalytic Turing machine and a symmetric catalytic Turing machine is equal to a deterministic catalytic Turing machine which runs in polynomial time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/024"><span class="datestr">at February 24, 2020 10:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/">Postdoc at University of Chicago (apply by March 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Econometrics and Statistics group at the Booth School of Business of the University of Chicago invites applications for a postdoctoral researcher working with Prof. Bryon Aragam. Potential candidates should have a background in statistics and machine learning, for example nonconvex optimization, nonparametric statistics, and/or learning theory.</p>
<p>Website: <a href="https://uchicago.wd5.myworkdayjobs.com/External/job/Hyde-Park-Campus/Principal-Researcher_JR07406">https://uchicago.wd5.myworkdayjobs.com/External/job/Hyde-Park-Campus/Principal-Researcher_JR07406</a><br />
Email: bryon@chicagobooth.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/02/23/postdoc-at-university-of-chicago-apply-by-march-15-2020/"><span class="datestr">at February 23, 2020 06:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/023">TR20-023 |  Non-Malleability against Polynomial Tampering | 

	Eshan Chattopadhyay, 

	Jyun-Jie Liao, 

	Marshall Ball, 

	Tal Malkin, 

	Li-Yang Tan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present the first explicit construction of a  non-malleable code that can handle tampering functions that are bounded-degree polynomials. 

Prior to our work, this was only known  for  degree-1 polynomials (affine tampering functions), due to  Chattopadhyay and Li (STOC 2017). As a direct corollary, we obtain an explicit non-malleable code that is secure against tampering by bounded-size arithmetic circuits.

We show applications of our non-malleable code in constructing   non-malleable secret sharing schemes that are robust against bounded-degree polynomial tampering. In fact our result is stronger: we can handle adversaries that can adaptively choose the polynomial tampering function based on initial leakage of a bounded number of shares.

Our results are derived from explicit constructions of  seedless non-malleable extractors that can handle bounded-degree polynomial tampering functions. Prior to our work, no such result was known even for degree-2 (quadratic) polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/023"><span class="datestr">at February 23, 2020 05:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
