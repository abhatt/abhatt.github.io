<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 08, 2021 09:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5539">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5539">More quantum computing popularization!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>I now have a feature article up at <em>Quanta</em> magazine, entitled <a href="https://www.quantamagazine.org/why-is-quantum-computing-so-hard-to-explain-20210608/">“What Makes Quantum Computing So Hard To Explain?”</a>  I.e., why do journalists, investors, etc. so consistently get central points wrong, even after the subject has been in public consciousness for more than 25 years?  Perhaps unsurprisingly, I found it hard to discuss that meta-level question, as <em>Quanta</em>‘s editors asked me to do, without also engaging in the object-level task of actually explaining QC.  For regular <em>Shtetl-Optimized</em> readers, there will be nothing new here, but I’m happy with how the piece turned out.</p>



<p>Accompanying the <em>Quanta</em> piece is a <a href="https://www.youtube.com/watch?v=jHoEjvuPoB8&amp;t=6s">10-minute YouTube explainer on quantum computing</a>, which (besides snazzy graphics) features interviews with me, John Preskill, and Dorit Aharonov.</p>



<p>On a different note, my colleague <a href="https://www.markwilde.com/">Mark Wilde</a> has recorded a <a href="https://soundcloud.com/mark-m-wilde/quantum-computer">punk-rock song about BosonSampling</a>.  I can honestly report that it’s some of the finest boson-themed music I’ve heard in years.  It includes the following lyrics:</p>



<blockquote class="wp-block-quote"><p>Quantum computer, Ain’t no loser<br />Quantum computer, Quantum computer</p><p>People out on the streets<br />They don’t know what it is<br />They think it finds the cliques<br />Or finds graph colorings<br />But it don’t solve anything<br />Said it don’t solve anything<br />Bosonic slot machine<br />My lil’ photonic dream</p></blockquote>



<p>Speaking of BosonSampling, A. S. Popova and A. N. Rubtsov, of the Skolkovo Institute in Moscow, have a new preprint entitled <a href="https://arxiv.org/abs/2106.01445">Cracking the Quantum Advantage threshold for Gaussian Boson Sampling</a>.  In it, they claim to give an efficient classical algorithm to simulate noisy GBS experiments, like the <a href="https://www.scottaaronson.com/blog/?p=5159">one six months ago</a> from USTC in China.  I’m still unsure how well this scales from 30-40 photons up to 50-70 photons; which imperfections of the USTC experiment are primarily being taken advantage of (photon losses?); and how this relates to the earlier proposed classical algorithms for simulating noisy BosonSampling, like the one by <a href="https://arxiv.org/abs/1409.3093">Kalai and Kindler</a>.  Anyone with any insight is welcome to share!</p>



<p>OK, one last announcement: the Simons Institute for the Theory of Computing, in Berkeley, has a new online lecture series called <a href="https://simons.berkeley.edu/news/institute-launches-breakthroughs-lecture-series">“Breakthroughs,”</a> which many readers of this blog might want to check out.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5539"><span class="datestr">at June 08, 2021 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-7554871212591440842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/06/machlne-learning-for-algorithms.html">Machine Learning for Algorithms Workshop (July 13-14)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>We're having an online workshop on "Machine Learning for Algorithms" on July 13-14, with a great group of speakers.  Announcement below, link at <a href="https://fodsi.us/ml4a.html">https://fodsi.us/ml4a.html</a>, free registration (but please register in advance)!</p><div style="font-size: small;" class="gmail_default">In recent years there has been increasing interest in using machine learning to improve the performance of classical algorithms in computer science, by fine-tuning their behavior to adapt to the properties of the input distribution. This "data-driven" or "learning-based" approach to algorithm design has the potential to significantly improve the efficiency of some of the most widely used algorithms. For example, they have been used to design better data structures, online algorithms, streaming and sketching algorithms, market mechanisms and algorithms for combinatorial optimization, similarity search and inverse problems.  This virtual workshop will feature talks from experts at the forefront of this exciting area.<br /><br />The workshop is organized by Foundations of Data Science Institute (FODSI), a project supported by the NSF TRIPODS program (see fodsi.us). To attend, please register at    <br /> <br /><a href="https://fodsi.us/ml4a.html">https://fodsi.us/ml4a.html</a>  </div></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/06/machlne-learning-for-algorithms.html"><span class="datestr">at June 08, 2021 07:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/078">TR21-078 |  A direct product theorem for quantum communication complexity with applications to device-independent QKD | 

	Rahul  Jain, 

	Srijita Kundu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a direct product theorem for the entanglement-assisted interactive quantum communication complexity of an $l$-player predicate $V$. In particular we show that for a distribution $p$ that is product across the input sets of the $l$ players, the success probability of any entanglement-assisted quantum communication protocol for computing $n$ copies of $V$, whose communication is $o(\log(\mathrm{eff}^*(V,p))\cdot n)$, goes down exponentially in $n$. Here $\mathrm{eff}^*(V, p)$ is a distributional version of the quantum efficiency or partition bound introduced by Laplante, Lerays and Roland (2014), which is a lower bound on the distributional quantum communication complexity of computing a single copy of $V$ with respect to $p$.
  As an application of our result, we show that it is possible to do device-independent quantum key distribution (DIQKD) without the assumption that devices do not leak any information after inputs are provided to them. We analyze the DIQKD protocol given by Jain, Miller and Shi (2017), and show that when the protocol is carried out with devices that are compatible with $n$ copies of the Magic Square game, it is possible to extract $\Omega(n)$ bits of key from it, even in the presence of $O(n)$ bits of leakage. Our security proof is parallel, i.e., the honest parties can enter all their inputs into their devices at once, and works for a leakage model that is arbitrarily interactive, i.e., the devices of the honest parties Alice and Bob can exchange information with each other and with the eavesdropper Eve in any number of rounds, as long as the total number of bits or qubits communicated is bounded.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/078"><span class="datestr">at June 08, 2021 12:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/3db-light/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/3db-light/">3DB: A Framework for Debugging Vision Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%; margin-bottom: 0;" href="https://arxiv.org/abs/2106.03805" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%; margin-bottom: 0;" href="https://github.com/3db/3db/" class="bbutton">
<i class="fab fa-github"></i>
   Project Repo
</a>
<a style="float: left; width: 45%;" href="https://3db.github.io/3db/usage/quickstart.html" class="bbutton">
<i class="fas fa-file-alt"></i>
   Documentation and Guides
</a> 
<a style="float: left; width: 45%;" href="https://github.com/3db/blog_demo/" class="bbutton">
<i class="fab fa-github"></i>
   Blog Demo
</a> 
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, in collaboration with Microsoft Research, we introduce
3DB: an extendable, unified framework for debugging and analyzing vision models
using photorealistic simulation. We’re releasing 3DB as a package, accompanied
by extensive API documentation, guides, and demos.</i></p>

<p><em>Note: You are now viewing the Javascript-free/lightweight version of this
post—to see the full version (with interactive plots, diagrams, and models!),
click <a href="https://gradientscience.org/3db/">here</a></em></p>

<p>Identifying failure modes and biases in vision models is a rapidly
emerging challenge in machine learning. In high-stakes
applications, simply deploying models and collecting failures that arise in
the wild is often difficult, expensive, and irresponsible. To this end, a
recent line of work in vision focuses on identifying model failure
modes via in-depth analyses of <a href="https://arxiv.org/abs/1712.02779">image transformations</a> and 
<a href="https://arxiv.org/abs/1903.12261">corruptions</a>, <a href="https://objectnet.dev">object orientations</a>,
<a href="https://gradientscience.org/background/">backgrounds</a>, or <a href="https://arxiv.org/abs/1811.12231v2">shape-texture conflicts</a>. These studies 
(and other similarly important ones) reveal a variety of patterns of
performance degradation in vision models. Still, performing each such study
requires time, developing (often complex)
toolingFor <a href="https://gradientscience.org/background/">our study of image backgrounds</a>, for example, we used a
combination of bounding boxes and classical computer vision tools to crop out
image backgrounds. We then had to manually filter out the images for which
the tools failed. Even for the images where the toolkit succeeded, there
remained inevitable cropping artifacts., and a willingness to settle for less than perfect simulations of each
potential failure mode. Our question is: can we support reliable discovery of model failures in a systematic,
automated, and unified way?</p>

<h2 id="3db-a-rendering-based-debugging-platform">3DB: A Rendering-based Debugging Platform</h2>

<p><img src="https://gradientscience.org/assets/3db/3db_headline.png" alt="A sampling of the analyses enabled by 3DB." class="bigimg" /></p>
<div style="margin-top: 0;" class="caption">
A sampling of the analyses enabled by 3DB.
</div>

<p>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, we try to make progress on this question and propose
3DB, a platform for automatically identifying and analyzing the failure modes
of computer vision models using 3D rendering. 3DB aims to allow users to go
from a testable, robustness-based hypothesis to concrete, photorealistic
experimental evidence with minimal time and effort.</p>

<p>The platform revolves around the modular workflow pictured below. First,
users specify a set of 3D objects and environments, as well as a set of 3D
(or 2D) transformations called controls that determine the space of
admissible object-environment configurations. 3DB then renders a myriad of
admissible scenes and feeds them through the user’s computer vision model of
choice. The user can finally stratify, aggregate, or otherwise analyze the
results either by reading the outputted JSON, or through the pre-packaged
dashboard.</p>

<p><img src="https://gradientscience.org/assets/3db/workflow.png" alt="An illustration of the 3DB workflow." class="bigimg" /></p>

<p>3DB easily adapts to a variety of use cases: in particular, users can
modify and swap out any part of this pipeline (e.g., the renderer, the
logger, the model type, or the controls) for their own custom-written
components, without needing to modify any of the 3DB codebase. We’ve compiled <a href="https://3db.github.io/3db/">guides</a>,
extensive <a href="https://3db.github.io/3db/api_doc.html">API documentation</a>, and a
<a href="https://github.com/3db/demo">full demo</a> showing how 3DB streamlines model debugging.</p>

<p><strong>In fact, this blog post will double as another demo! We’ll present the (short) code
necessary to reproduce every plot in the post below using 3DB. You can download 
the aggregated code for this blog post <a href="https://github.com/3db/blog_demo">here</a>.</strong></p>

<p><em>To set up, follow the steps below—then, in the remainder of this post, press
“Show/hide code and instructions” to see the steps necessary to reproduce each
experiment below.</em></p>
<div class="code-container">
  <input id="general-tab1" type="radio" checked="" name="general-tab" class="tab1" />
  <label for="general-tab1"><i class="fa fa-gear"></i>  Setup</label>
  <input id="general-tab2" type="radio" name="general-tab" class="tab2" />
  <label for="general-tab2"><i class="fa fa-code"></i>  Config (base.yaml)</label>
  <div class="line"></div>
  <div class="content-container">
<div class="content c1">

<ol class="instructions">
<li>Clone the  <a href="https://github.com/3db/blog_demo">blog demo <i class="fab fa-github"></i></a> repo</li>
<li>Run <pre>cd blog_demo</pre>, then <pre>bash setup.sh</pre> (assumes
<pre>unzip</pre> is installed) to download a large Blender environment, then
<pre>cd ../</pre></li>
<li>Install 3DB: <pre>curl -L https://git.io/Js8eT | bash /dev/stdin threedb</pre></li>
<li>Run <pre>conda activate threedb</pre></li>
<li>Our experiments below will need a <pre>BLENDER_DATA</pre> folder that contains two
subfolders: <pre>blender_models/</pre> containing 3D models (<pre>.blend</pre> files with a single
object whose name matches the filename), and <pre>blender_environments/</pre>
containing environments. We will provide you with these later</li> 
<li>Separately, make a file called <pre>base.yaml</pre> and paste in the configuration from the next pane.</li>
</ol>

</div>
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models'
  label_map: 'blog_demo/resources/imagenet_mapping.json'
  class: 'resnet18'
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.classification'
  args:
    classmap_path: 'blog_demo/resources/ycb_to_IN.json'
    topk: 1
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
policy:
  module: "threedb.policies.random_search"
  samples: 5
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"

</code></pre></div>
 </div>
</div>

<h2 id="using-3db">Using 3DB</h2>

<p>Prior works have already used 3D rendering (to great effect) to study biases
of machine learning models, including pose and context-based biases. Our goal
is not to propose a specific 3D-rendering based analysis, but
rather to provide an easy-to-use, highly extendable framework that unifies
prior analyses (both 3D and 2D) while enabling users to (a) conduct a host of
new analyses with the same ease and with realistic results; and (b)
effortlessly <em>compose</em> different factors of variation to understand their
interplay.</p>

<p>We’ll dedicate the rest of this post to illustrating how one
might actually use 3DB in practice, focusing on a single example 3D
model3DB works with any 3D model, and we refer the reader to 
<a href="https://arxiv.org/abs/2106.03805">our paper</a> for more examples and details.:</p>

<p style="text-align: center;">
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_1.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_2.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_3.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_4.png" style="display: inline; width: 18%;" />
<img src="https://gradientscience.org/assets/3db/plain_mug/plain_mug_5.png" style="display: inline; width: 18%;" />
</p>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    The 3D mug model that we'll be using throughout this post.
</div>

<p>In what follows, we will walk through example applications of 3DB to discover biases of
ML models (some previously documented, others not). For the sake of brevity,
we’ll highlight just a few of these (re-)discoveries—to see more, check out
the <a href="https://arxiv.org/abs/2106.03805">paper</a>. We’ll then demonstrate that the discoveries of 3DB
transfer pretty reliably to the real world!</p>

<p>Our experiments will all operate on an ImageNet-pretrained
ResNet-18The classifier has a ~70% validation-set
accuracy. that has 42% accuracy on images from the
“coffee mug” ImageNet subclass. While we only study classification in this blog post,
3DB also supports object detection and can be easily extended to support other
image-based tasks, such as semantic segmentation, too.</p>

<h2 id="image-background-sensitivity">Image Background Sensitivity</h2>

<p>In one of our <a href="https://gradientscience.org/background/">previous posts</a>,
we continued a long line of prior work (see, e.g.,
<a href="https://hal.archives-ouvertes.fr/hal-00171412/file/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">here</a>, <a href="https://arxiv.org/abs/1611.06596">here</a>, <a href="https://arxiv.org/abs/1911.08731">here</a>, etc.) showing that models can be
over-reliant on image backgrounds, and demonstrated that they are easily
broken by adversarially chosen backgrounds. To accomplish this, our prior
analysis used classical computer vision tools to separate foregrounds from
backgrounds, then pasted foregrounds from one image onto backgrounds from
another. This process was slow and required extensive quality control to
ensure that backgrounds and foregrounds were being extracted properly—and
even when they were, a few artifacts remained:</p>

<div style="margin-bottom: 15px; overflow: hidden; text-align: center;">
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/1.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/2.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/3.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/4.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/5.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/6.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
</div>

<p>3DB lets us reproduce these findings effortlessly and without introducing
such artifacts. To demonstrate this, we use 3DB to render our mug 3D model on
hundreds of HDRI backgrounds, resulting in images such as:</p>

<div style="margin-bottom: 15px; overflow: hidden;">
    
        <img src="https://gradientscience.org/assets/3db/background-renders/1.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/2.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/3.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/4.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/5.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/6.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/7.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/8.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/9.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/10.png" style="float: right; width: 20%;" />
    
</div>

<p>We then analyze the performance of a pretrained ResNet-18Recall that this model
obtains 42% accuracy on the corresponding "coffee mug" ImageNet class subset.on these images. We
find that the performance of the classifier varies significantly across
backgrounds, and that accuracy correlates with a crude measure of
“background simplicity” (the JPEG compressed size of the image—with smaller size corresponding to being more simple).</p>

<p><img src="https://gradientscience.org/assets/3db/mug_background_complexity.png" alt="Simplicity versus model accuracy for HDRI backgrounds" /></p>

<div class="caption">
A graph plotting average accuracy of our pre-trained ImageNet model (y-axis) on
images rendered by 3DB while varying the complexity of the rendering backgrounds
(x-axis). 
</div>

<p><strong>A note on compositionality</strong>: An important part of 3DB that we don’t discuss here is compositionality, i.e., the ability to put together multiple controls and study their joint effect. For example, in our paper we studied how a model’s prediction vary with various zoom levels and backgrounds of an object. We found that the optimal zoom level varies a lot by background.</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="bg-tab1" type="radio" checked="" name="bg-tab" class="tab1" />
    <label for="bg-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="bg-tab2" type="radio" name="bg-tab" class="tab2" />
    <label for="bg-tab2"><i class="fa fa-code"></i>  Config (backgrounds.yaml)</label>
    <input id="bg-tab3" type="radio" name="bg-tab" class="tab3" />
    <label for="bg-tab3"><i class="fa fa-search"></i>  Analysis (analyze_bgs.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="bash">
# $BLENDER_DATA/blender_environments` contains several backgrounds and
# $BLENDER_DATA/blender_models contains the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/backgrounds
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# (Optional) Download additional backgrounds you want---e.g., from
# https://hdrihaven.com/hdris/ (both `.hdr` and `.blend` files work) and put
# them in BLENDER_DATA/blender_environments. 
wget https://hdrihaven.com/hdris/PATH/TO/HDRI \
    -O $BLENDER_DATA/blender_environments

# Direct results
export RESULTS_FOLDER='results_backgrounds'

# Run 3DB (with the YAML file from the next pane saved as `backgrounds.yaml`):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA backgrounds.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (third pane)
python analyze_bgs.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_backgrounds/details.log').readlines()
class_map = json.load(open('results_backgrounds/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('environment').agg(accuracy=('is_correct', 'mean'),
        most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
</div>
</div>
</details>

<h2 id="texture-bias">Texture Bias</h2>
<p>Another <a href="https://arxiv.org/abs/1811.12231v2">recent study</a> of neural network biases showed that in
contrast to humans, convolutional neural networks (CNNs) rely more on texture
to recognize objects than on shape. The example below typifies this
phenomenon—a cat with an elephant texture is recognized as a cat by humans,
but as an elephant by CNNs:</p>

<p><img src="https://gradientscience.org/assets/3db/cue_conflict.png" alt="Cue-conflict images introduced by Geirhos et al." /></p>
<div class="caption">
An example of the cue-conflict images introduced by Geirhos et al. Combining the
elephant texture with the cat shape results in a mixed image on which CNNs
consistently predict with the texture signal.
</div>

<p>This example and others like it (dubbed ‘cue-conflict’ images) provide a
striking illustration of the contrast between human and CNN-based
classification mechanisms. Still, just as in the case of image backgrounds,
creating such images typically necessitates time, technical skill,
quality control, and/or introduction of unwanted artifacts (for example, in the above figure,
ideally we would modify only the texture of the cat without altering the background).</p>

<p>However, using 3DB we can easily collect photorealistic empirical evidence of
texture bias. Without modifying the internal 3DB codebase at all,
one can write a <a href="https://github.com/3db/3db/blob/main/threedb/controls/blender/material.py">custom control</a> that modifies the texture of
objects in the scene while keeping the rest intact. With this custom control
in placeIn fact, the texture-swapping control for this experiment is now 
pre-packaged with 3DB, since we already wrote it ourselves!, one can simply 
randomize the texture of the mug across various
backgrounds, poses and camera parameters before stratifying results:</p>

<p><img src="https://gradientscience.org/assets/3db/texture_swap_histograms.png" alt="Chungus" /></p>

<p>The performance of the pretrained model on mugs (and other objects)
deteriorates severely upon replacing the mug’s texture with a “wrong” one,
providing clear corroborating evidence of the texture bias! We noticed in our
experiments that for some textures (e.g., zebra), the coffee mug was
consistently misclassified as the corresponding animal, whereas for others
(e.g., crocodile), the mug is misclassified as either a related class (e.g.,
turtle or other reptile), or as an unrelated object (e.g., a trash can).</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="texture-tab1" type="radio" checked="" name="texture-tab" class="tab1" />
    <label for="texture-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="texture-tab2" type="radio" name="texture-tab" class="tab2" />
    <label for="texture-tab2"><i class="fa fa-code"></i>  Config (texture_swaps.yaml)</label>
    <input id="texture-tab3" type="radio" name="texture-tab" class="tab3" />
    <label for="texture-tab3"><i class="fa fa-search"></i>  Analysis (analyze_ts.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1">
<pre class="wrapped"><code class="bash">
# ${BLENDER_DATA}/blender_environments contains several backgrounds,
# ${BLENDER_DATA}/blender_models contain the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/texture_swaps

# List the materials that we will use for this post:
ls blog_demo/data/texture_swaps/blender_control_material
# You can also make or download blender materials corresponding 
# to other textures you want to test, and add them to that folder 

# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

export RESULTS_FOLDER=results_texture

# Run 3DB (with the YAML file from the next pane saved as texture_swaps.yaml):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA texture_swaps.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (copy from third pane)
python analyze_ts.py

</code></pre>
</div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.position"
    offset_x: 0.
    offset_y: 0.5
    offset_z: 0.
  - module: "threedb.controls.blender.pin_to_ground"
    z_ground: 0.25
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: [0., 1.]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.material"
    replacement_material: ["cow.blend", "elephant.blend", "zebra.blend", "crocodile.blend", "keep_original"]
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_texture/details.log').readlines()
class_map = json.load(open('results_texture/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df = df.drop('render_args', axis=1).join(pd.DataFrame(df.render_args.values.tolist()))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('MaterialControl.replacement_material').agg(acc=('is_correct', 'mean'),
      most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
    </div>
</div>
</details>

<h2 id="part-of-object-attribution">Part-of-Object Attribution</h2>

<p>Beyond general hypotheses about model biases, 3DB allows us to test vision
systems on a more fine-grained level. In the case of our running mug example,
for instance, we can use the platform to understand which specific parts of
its 3D mesh correlate with classifier accuracy. Specifically, below we
generate (and classify) scenes with random mug positions, rotations, and
backgrounds. Since 3DB stores texture-coordinate information for each
rendering, we can reconstruct a three-dimensional heatmap that encodes, for
each point on the surface of the mug, the classifier’s accuracy conditioned
on that point being visible:</p>

<p style="text-align: center;">
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_1.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_2.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_3.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_4.png" style="width: 18%; display: inline;" />
    <img src="https://gradientscience.org/assets/3db/heatmap_mug/heatmap_mug_5.png" style="width: 18%; display: inline;" />
</p>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    A part-of-object attribution heatmap for our mug 3D model. <span style="color: red;">Red</span> pixels indicate areas whose visibility most
    improves accuracy, whereas <span style="color: blue;">blue</span> areas'
    visibility correlates with incorrect classifications.
</div>

<p>A number of phenomena stand out from this heatmap, including:</p>

<ol>
  <li>The classifier is worse when the side of the mug opposite the handle is seen.</li>
  <li>The classifier is more accurate when the bottom rim is visible.</li>
  <li>The classifier performs worst when the inside of the mug is visible.</li>
</ol>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="pobj-tab1" type="radio" checked="" name="pobj-tab" class="tab1" />
    <label for="pobj-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="pobj-tab2" type="radio" name="pobj-tab" class="tab2" />
    <label for="pobj-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <input id="pobj-tab3" type="radio" name="pobj-tab" class="tab3" />
    <label for="pobj-tab3"><i class="fa fa-search"></i>  Analysis (analyze_po.py)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# point BLENDER_DATA to the environments and models for this experiment
export BLENDER_DATA=$(realpath blog_demo)/data/part_of_object
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# Optionally: download additional backgrounds (`.hdr` or `.blend`) e.g.,
wget URL -O $BLENDER_DATA/blender_environments/new_env.hdr

# Point results folder to where you want output written
export RESULTS_FOLDER='results_part_of_object'

# Run 3DB (with the YAML file from the next pane):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA part_of_object.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Run `part_of_object.py` (third pane) to generate the heat map of the mug.
python po_analysis.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_uv: True
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: 1.
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"
  - module: "threedb.controls.blender.background"
    H: 1.
    S: 0.
    V: 1.
</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json
from PIL import Image

DIR = 'results_part_of_object'
log_lines = open(f'{DIR}/details.log').readlines()
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))

# From class index to class name (for readability)
class_map = json.load(open(f'{DIR}/class_maps.json'))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])

# We'll be a little lenient here to get a more interesting heatmap
df['is_correct'] = df['prediction'].isin(['cup', 'coffee mug'])

uv_num_correct = np.zeros((256, 256))
uv_num_visible = np.zeros((256, 256))
for imid in df["id"].unique().tolist():
    is_correct = float(df.set_index('id').loc[imid]['is_correct'])
    vis_coords_im = Image.open(f'{DIR}/images/{imid}_uv.png')
    vis_coords = np.array(vis_coords_im).reshape(-1, 3)
    # R and G channels encode texture coordinates (x, y), 
    # B channel is 255 for object and 0 for background
    # So we will filter by B then only look at R and G.
    vis_coords = vis_coords[vis_coords[:,2] &gt; 0][:,:2]

    uv_num_visible[vis_coords[:,0], vis_coords[:,1]] += 1.
    uv_num_correct[vis_coords[:,0], vis_coords[:,1]] += is_correct

# Accuracy = # correct / # visible
uv_accuracy = uv_num_correct / (uv_num_visible + 1e-4)

# Saves a black-and-white heatmap
Image.fromarray((255 * uv_accuracy).astype('uint8'))

</code></pre></div>
</div>
</div>
</details>
<p><br /></p>

<p>Now that we have hypotheses regarding model performance, we can test them! Inspecting
the ImageNet validation set, we found that our classifier indeed (a) struggles on coffee mugs when the
handle is not showing (providing a feasible explanation for (1), since the side
opposite the handle is only visible when the handle itself isn’t), and (b)
performs worse at higher camera angles (providing a plausible explanation for
(2)). We want to focus, however, on the third phenomenon,
i.e., that the classifier performs quite poorly whenever the inside of the
mug is visible. Why could this be the case? We can use 3DB to gain insight into the
phenomenon. Specifically, we want to test the following hypothesis: when
classifying mugs, does our ImageNet model rely on the exact liquid inside the
cup?</p>

<p>We investigate this hypothesis by writing a custom control that fills
our mug with various liquids (more precisely, a parameterized mixture of
water, milk, and coffee):</p>

<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_samples.png" alt="Examples of the mugs rendered in this experiment" /></p>
<div class="caption">
Our running example mug, filled with different parameterized liquids: 100% water
(top left), 100% coffee (top right), 100% milk (bottom right), and a coffee-milk
mixture (bottom left)
</div>

<p>In contrast to the last experiment (where we varied the orientation of the
mug), we render scenes containing the mug in a fixed set of poses that reveal
the contents—just as in the last experiment, however, we still vary
background and mug location. We visualize the results below—each cell in
the heatmap corresponds to a fixed mixture of coffee, water, and milk (i.e.,
the labeled corners are 100% coffee, 100% milk, and 100% water, and the other
cells are linear interpolations of these ratios) and the color of the cell
encodes the relative accuracy of the classifier when the mug is filled with
that liquid:</p>



<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_simplex.png" alt="" /></p>

<div class="caption">
Measuring the relative effect of the liquid mixture in the mug on model
predictions. Each cell represents a specific liquid mixture, and the color of
the cell represents the tendency of the model (averaged over random viewpoints
and relative to the other cells) to predict "cup"/"pill bottle," "bucket," or
"mug."
</div>

<p>It turns out that mug content indeed highly impacts classification:
our model is much less likely to correctly classify a mug that doesn’t contain coffee!
This is just one example of how 3DB can help in proving or disproving hypotheses
about model behavior.</p>

<h2 id="from-simulation-to-reality">From Simulation to Reality</h2>

<p>So far, we’ve used 3DB to discover ML models’ various failure modes and biases
via photorealistic rendering. To what extent though do the insights
gleaned from simulated 3DB experiments actually “transfer” to the physical
world?</p>

<p>To test such transferability, we began by creating a 3D model of a physical room we
had access to. We also collected eight different 3D models with closely matching
physical world counterparts—including the mug analyzed above. Next, we used 3DB to find correctly and incorrectly classified
configurations (pose, orientation, location) of these eight objects inside that
room. Finally, we replicated these poses (to
the best of our abilities) in the physical room, and took photos with a
cellphone camera:</p>

<p><img src="https://gradientscience.org/assets/3db/real_life_exp_samples.png" alt="Samples from our physical-world experiment." class="bigimg" /></p>

<div class="caption">
Examples of simulated scenes (top) and their re-created counterparts (bottom)
from our physical-world experiment.
</div>

<p>We classified these photos with the same vision model as before and measured
how often the simulated classifier correctness matched correctness on the
real photographs. We observed an ~85% match! So the failure
modes identified by 3DB are not merely simulation artifacts, and can indeed arise in
the real world.</p>

<h2 id="conclusion">Conclusion</h2>

<p>3DB is a flexible, easy-to-use, and extensible framework for
identifying model failure modes, uncovering biases, and testing fine-grained
hypotheses about model behavior. We hope it will prove to be a useful tool
for debugging vision models.</p>
<h2 id="bonus-object-detection-web-dashboard-and-more">Bonus: Object Detection, Web Dashboard, and more!</h2>

<p>We’ll wrap up by highlighting some additional capabilities of 3DB that we didn’t
get to demonstrate in this blog post:</p>

<h3 id="3dboard-a-web-interface-for-exploring-results">3DBoard: a web interface for exploring results</h3>

<p>In all of the code examples above, we showed how to analyze the results of a 3DB
experiment by loading the output into a <code class="language-plaintext highlighter-rouge">pandas</code> dataframe. For additional
convenience, however, 3DB also comes with a web-based dashboard for exploring
experimental results. The following command suffices to visualize the texture swaps experiment from earlier:</p>

<pre style="padding: 0; margin: 0;"><code class="bash">
python -m threedboard results_texture/ --port 3000

</code>
</pre>

<p>Navigating to <code class="language-plaintext highlighter-rouge">YOUR_IP:3000</code> should lead you to a page that looks like this:</p>

<p><img src="https://gradientscience.org/assets/3db/dashboard_screenshot.png" alt="Dashboard screenshot" /></p>

<h3 id="object-detection-and-other-tasks">Object detection and other tasks</h3>

<p>In this blog post, we focused on using 3DB to analyze image classification
models. However, the library also supports object detection out-of-the-box, and
can easily be extended to support a variety of image-based tasks (e.g.,
segmentation or regression-based tasks). For example, below we provide a simple
end-to-end object detection example:</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="objdet-tab1" type="radio" checked="" name="objdet-tab" class="tab1" />
    <label for="objdet-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="objdet-tab2" type="radio" name="objdet-tab" class="tab2" />
    <label for="objdet-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# The object detection example is separate from the rest of the blog demo, so
# run the following in a separate repo:
git clone https://github.com/3db/object_detection_demo

# The repo has a data/ folder containing the Blender model (a banana) and some
# HDRI backgrounds, a classmap.json file mapping the UID of the model to a COCO
# class, and the detection.yaml file from the next pane.
cd object_detection_demo/

export BLENDER_DATA=data/
export RESULTS_FOLDER=results/

# Run 3DB
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp; 
threedb_master $BLENDER_DATA detection.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

</code></pre></div> 
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models.detection'
  class: 'retinanet_resnet50_fpn'
  label_map: './resources/coco_mapping.json'
  normalization:
    mean: [0., 0., 0.]
    std: [1., 1., 1.]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.detection'
  args:
    iou_threshold: 0.5
    nms_threshold: 0.1
    max_num_boxes: 10
    classmap_path: 'classmap.json'
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_segmentation: true
policy:
  module: "threedb.policies.random_search"
  samples: 2
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
</div>
</div>
</details></div>







<p class="date">
<a href="https://gradientscience.org/3db-light/"><span class="datestr">at June 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/3db/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/3db/">3DB: A Framework for Debugging Vision Models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%; margin-bottom: 0;" href="https://arxiv.org/abs/2106.03805" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%; margin-bottom: 0;" href="https://github.com/3db/3db/" class="bbutton">
<i class="fab fa-github"></i>
   Project Repo
</a>
<a style="float: left; width: 45%;" href="https://3db.github.io/3db/usage/quickstart.html" class="bbutton">
<i class="fas fa-file-alt"></i>
   Documentation and Guides
</a> 
<a style="float: left; width: 45%;" href="https://github.com/3db/blog_demo/" class="bbutton">
<i class="fab fa-github"></i>
   Blog Demo
</a> 
<br /></p>

<p><i>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, in collaboration with Microsoft Research, we introduce
3DB: an extendable, unified framework for debugging and analyzing vision models
using photorealistic simulation. We’re releasing 3DB as a package, accompanied
by extensive API documentation, guides, and demos.</i></p>

<p><em>Note: this post contains some interactive plots and 3D models that use
JavaScript: click <a href="https://gradientscience.org/3db-light/">here</a> for a JS-free version of this post.</em></p>

<p>Identifying failure modes and biases in vision models is a rapidly
emerging challenge in machine learning. In high-stakes
applications, simply deploying models and collecting failures that arise in
the wild is often difficult, expensive, and irresponsible. To this end, a
recent line of work in vision focuses on identifying model failure
modes via in-depth analyses of <a href="https://arxiv.org/abs/1712.02779">image transformations</a> and 
<a href="https://arxiv.org/abs/1903.12261">corruptions</a>, <a href="https://objectnet.dev">object orientations</a>,
<a href="https://gradientscience.org/background/">backgrounds</a>, or <a href="https://arxiv.org/abs/1811.12231v2">shape-texture conflicts</a>. These studies 
(and other similarly important ones) reveal a variety of patterns of
performance degradation in vision models. Still, performing each such study
requires time, developing (often complex)
toolingFor <a href="https://gradientscience.org/background/">our study of image backgrounds</a>, for example, we used a
combination of bounding boxes and classical computer vision tools to crop out
image backgrounds. We then had to manually filter out the images for which
the tools failed. Even for the images where the toolkit succeeded, there
remained inevitable cropping artifacts., and a willingness to settle for less than perfect simulations of each
potential failure mode. Our question is: can we support reliable discovery of model failures in a systematic,
automated, and unified way?</p>

<h2 id="3db-a-rendering-based-debugging-platform">3DB: A Rendering-based Debugging Platform</h2>

<p><img src="https://gradientscience.org/assets/3db/3db_headline.png" alt="A sampling of the analyses enabled by 3DB." class="bigimg" /></p>
<div style="margin-top: 0;" class="caption">
A sampling of the analyses enabled by 3DB.
</div>

<p>In our <a href="https://arxiv.org/abs/2106.03805">latest paper</a>, we try to make progress on this question and propose
3DB, a platform for automatically identifying and analyzing the failure modes
of computer vision models using 3D rendering. 3DB aims to allow users to go
from a testable, robustness-based hypothesis to concrete, photorealistic
experimental evidence with minimal time and effort.</p>

<p>The platform revolves around the modular workflow pictured below. First,
users specify a set of 3D objects and environments, as well as a set of 3D
(or 2D) transformations called controls that determine the space of
admissible object-environment configurations. 3DB then renders a myriad of
admissible scenes and feeds them through the user’s computer vision model of
choice. The user can finally stratify, aggregate, or otherwise analyze the
results either by reading the outputted JSON, or through the pre-packaged
dashboard.</p>

<p><img src="https://gradientscience.org/assets/3db/workflow.png" alt="An illustration of the 3DB workflow." class="bigimg" /></p>

<p>3DB easily adapts to a variety of use cases: in particular, users can
modify and swap out any part of this pipeline (e.g., the renderer, the
logger, the model type, or the controls) for their own custom-written
components, without needing to modify any of the 3DB codebase. We’ve compiled <a href="https://3db.github.io/3db/">guides</a>,
extensive <a href="https://3db.github.io/3db/api_doc.html">API documentation</a>, and a
<a href="https://github.com/3db/demo">full demo</a> showing how 3DB streamlines model debugging.</p>

<p><strong>In fact, this blog post will double as another demo! We’ll present the (short) code
necessary to reproduce every plot in the post below using 3DB. You can download 
the aggregated code for this blog post <a href="https://github.com/3db/blog_demo">here</a>.</strong></p>

<p><em>To set up, follow the steps below—then, in the remainder of this post, press
“Show/hide code and instructions” to see the steps necessary to reproduce each
experiment below.</em></p>
<div class="code-container">
  <input id="general-tab1" type="radio" checked="" name="general-tab" class="tab1" />
  <label for="general-tab1"><i class="fa fa-gear"></i>  Setup</label>
  <input id="general-tab2" type="radio" name="general-tab" class="tab2" />
  <label for="general-tab2"><i class="fa fa-code"></i>  Config (base.yaml)</label>
  <div class="line"></div>
  <div class="content-container">
<div class="content c1">

<ol class="instructions">
<li>Clone the  <a href="https://github.com/3db/blog_demo">blog demo <i class="fab fa-github"></i></a> repo</li>
<li>Run <pre>cd blog_demo</pre>, then <pre>bash setup.sh</pre> (assumes
<pre>unzip</pre> is installed) to download a large Blender environment, then
<pre>cd ../</pre></li>
<li>Install 3DB: <pre>curl -L https://git.io/Js8eT | bash /dev/stdin threedb</pre></li>
<li>Run <pre>conda activate threedb</pre></li>
<li>Our experiments below will need a <pre>BLENDER_DATA</pre> folder that contains two
subfolders: <pre>blender_models/</pre> containing 3D models (<pre>.blend</pre> files with a single
object whose name matches the filename), and <pre>blender_environments/</pre>
containing environments. We will provide you with these later</li> 
<li>Separately, make a file called <pre>base.yaml</pre> and paste in the configuration from the next pane.</li>
</ol>

</div>
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models'
  label_map: 'blog_demo/resources/imagenet_mapping.json'
  class: 'resnet18'
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.classification'
  args:
    classmap_path: 'blog_demo/resources/ycb_to_IN.json'
    topk: 1
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
policy:
  module: "threedb.policies.random_search"
  samples: 5
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"

</code></pre></div>
 </div>
</div>

<h2 id="using-3db">Using 3DB</h2>

<p>Prior works have already used 3D rendering (to great effect) to study biases
of machine learning models, including pose and context-based biases. Our goal
is not to propose a specific 3D-rendering based analysis, but
rather to provide an easy-to-use, highly extendable framework that unifies
prior analyses (both 3D and 2D) while enabling users to (a) conduct a host of
new analyses with the same ease and with realistic results; and (b)
effortlessly <em>compose</em> different factors of variation to understand their
interplay.</p>

<p>We’ll dedicate the rest of this post to illustrating how one
might actually use 3DB in practice, focusing on a single example 3D
model3DB works with any 3D model, and we refer the reader to 
<a href="https://arxiv.org/abs/2106.03805">our paper</a> for more examples and details.:</p>





<div style="text-align: center;" id="mug-blocker">
    
        
            
        
        
    
</div>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    The 3D mug model that we'll be using throughout this post. <strong>
    Click to <a id="mug-interact">enable 
    interactivity</a>.</strong>
</div>

<p>In what follows, we will walk through example applications of 3DB to discover biases of
ML models (some previously documented, others not). For the sake of brevity,
we’ll highlight just a few of these (re-)discoveries—to see more, check out
the <a href="https://arxiv.org/abs/2106.03805">paper</a>. We’ll then demonstrate that the discoveries of 3DB
transfer pretty reliably to the real world!</p>

<p>Our experiments will all operate on an ImageNet-pretrained
ResNet-18The classifier has a ~70% validation-set
accuracy. that has 42% accuracy on images from the
“coffee mug” ImageNet subclass. While we only study classification in this blog post,
3DB also supports object detection and can be easily extended to support other
image-based tasks, such as semantic segmentation, too.</p>

<h2 id="image-background-sensitivity">Image Background Sensitivity</h2>

<p>In one of our <a href="https://gradientscience.org/background/">previous posts</a>,
we continued a long line of prior work (see, e.g.,
<a href="https://hal.archives-ouvertes.fr/hal-00171412/file/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">here</a>, <a href="https://arxiv.org/abs/1611.06596">here</a>, <a href="https://arxiv.org/abs/1911.08731">here</a>, etc.) showing that models can be
over-reliant on image backgrounds, and demonstrated that they are easily
broken by adversarially chosen backgrounds. To accomplish this, our prior
analysis used classical computer vision tools to separate foregrounds from
backgrounds, then pasted foregrounds from one image onto backgrounds from
another. This process was slow and required extensive quality control to
ensure that backgrounds and foregrounds were being extracted properly—and
even when they were, a few artifacts remained:</p>

<div style="margin-bottom: 15px; overflow: hidden; text-align: center;">
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/1.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/2.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/3.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/4.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/5.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
        <img src="https://gradientscience.org/assets/3db/backgrounds-manual/6.JPEG" style="width: 15%; margin: 2px; display: inline;" />
    
</div>

<p>3DB lets us reproduce these findings effortlessly and without introducing
such artifacts. To demonstrate this, we use 3DB to render our mug 3D model on
hundreds of HDRI backgrounds, resulting in images such as:</p>

<div style="margin-bottom: 15px; overflow: hidden;">
    
        <img src="https://gradientscience.org/assets/3db/background-renders/1.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/2.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/3.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/4.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/5.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/6.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/7.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/8.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/9.png" style="float: right; width: 20%;" />
    
        <img src="https://gradientscience.org/assets/3db/background-renders/10.png" style="float: right; width: 20%;" />
    
</div>

<p>We then analyze the performance of a pretrained ResNet-18Recall that this model
obtains 42% accuracy on the corresponding "coffee mug" ImageNet class subset.on these images. We
find that the performance of the classifier varies significantly across
backgrounds, and that accuracy correlates with a crude measure of
“background simplicity” (the JPEG compressed size of the image—with smaller size corresponding to being more simple).</p>

<div>
  <canvas id="myChart"></canvas>
</div>

<div class="caption">
A graph plotting average accuracy of our pre-trained ImageNet model (y-axis) on
images rendered by 3DB while varying the complexity of the rendering backgrounds
(x-axis). 
</div>

<p><strong>A note on compositionality</strong>: An important part of 3DB that we don’t discuss here is compositionality, i.e., the ability to put together multiple controls and study their joint effect. For example, in our paper we studied how a model’s prediction vary with various zoom levels and backgrounds of an object. We found that the optimal zoom level varies a lot by background.</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="bg-tab1" type="radio" checked="" name="bg-tab" class="tab1" />
    <label for="bg-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="bg-tab2" type="radio" name="bg-tab" class="tab2" />
    <label for="bg-tab2"><i class="fa fa-code"></i>  Config (backgrounds.yaml)</label>
    <input id="bg-tab3" type="radio" name="bg-tab" class="tab3" />
    <label for="bg-tab3"><i class="fa fa-search"></i>  Analysis (analyze_bgs.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="bash">
# $BLENDER_DATA/blender_environments` contains several backgrounds and
# $BLENDER_DATA/blender_models contains the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/backgrounds
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# (Optional) Download additional backgrounds you want---e.g., from
# https://hdrihaven.com/hdris/ (both `.hdr` and `.blend` files work) and put
# them in BLENDER_DATA/blender_environments. 
wget https://hdrihaven.com/hdris/PATH/TO/HDRI \
    -O $BLENDER_DATA/blender_environments

# Direct results
export RESULTS_FOLDER='results_backgrounds'

# Run 3DB (with the YAML file from the next pane saved as `backgrounds.yaml`):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA backgrounds.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (third pane)
python analyze_bgs.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_backgrounds/details.log').readlines()
class_map = json.load(open('results_backgrounds/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('environment').agg(accuracy=('is_correct', 'mean'),
        most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
</div>
</div>
</details>

<h2 id="texture-bias">Texture Bias</h2>
<p>Another <a href="https://arxiv.org/abs/1811.12231v2">recent study</a> of neural network biases showed that in
contrast to humans, convolutional neural networks (CNNs) rely more on texture
to recognize objects than on shape. The example below typifies this
phenomenon—a cat with an elephant texture is recognized as a cat by humans,
but as an elephant by CNNs:</p>

<p><img src="https://gradientscience.org/assets/3db/cue_conflict.png" alt="Cue-conflict images introduced by Geirhos et al." /></p>
<div class="caption">
An example of the cue-conflict images introduced by Geirhos et al. Combining the
elephant texture with the cat shape results in a mixed image on which CNNs
consistently predict with the texture signal.
</div>

<p>This example and others like it (dubbed ‘cue-conflict’ images) provide a
striking illustration of the contrast between human and CNN-based
classification mechanisms. Still, just as in the case of image backgrounds,
creating such images typically necessitates time, technical skill,
quality control, and/or introduction of unwanted artifacts (for example, in the above figure,
ideally we would modify only the texture of the cat without altering the background).</p>

<p>However, using 3DB we can easily collect photorealistic empirical evidence of
texture bias. Without modifying the internal 3DB codebase at all,
one can write a <a href="https://github.com/3db/3db/blob/main/threedb/controls/blender/material.py">custom control</a> that modifies the texture of
objects in the scene while keeping the rest intact. With this custom control
in placeIn fact, the texture-swapping control for this experiment is now 
pre-packaged with 3DB, since we already wrote it ourselves!, one can simply 
randomize the texture of the mug across various
backgrounds, poses and camera parameters before stratifying results:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <div style="width: 32%; margin-right: 2%;" class="image-container">
            <h4 style="text-align: center;">Average accuracy (compare to 42% on ImageNet val set)</h4>
            <canvas id="gen1"></canvas>
        </div>
        <div style="width: 66%;" class="image-container" id="gen2">
            <img class="example-texture" id="gen2-0" />
            <img class="example-texture" id="gen2-1" />
            <img class="example-texture" id="gen2-2" />
            <img class="example-texture" id="gen2-3" />
            <img class="example-texture" id="gen2-4" />
            <img class="example-texture" id="gen2-5" />
            <img class="example-texture" id="gen2-6" />
            <img class="example-texture" id="gen2-7" />
        </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="caption">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>The performance of the pretrained model on mugs (and other objects)
deteriorates severely upon replacing the mug’s texture with a “wrong” one,
providing clear corroborating evidence of the texture bias! We noticed in our
experiments that for some textures (e.g., zebra), the coffee mug was
consistently misclassified as the corresponding animal, whereas for others
(e.g., crocodile), the mug is misclassified as either a related class (e.g.,
turtle or other reptile), or as an unrelated object (e.g., a trash can).</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="texture-tab1" type="radio" checked="" name="texture-tab" class="tab1" />
    <label for="texture-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="texture-tab2" type="radio" name="texture-tab" class="tab2" />
    <label for="texture-tab2"><i class="fa fa-code"></i>  Config (texture_swaps.yaml)</label>
    <input id="texture-tab3" type="radio" name="texture-tab" class="tab3" />
    <label for="texture-tab3"><i class="fa fa-search"></i>  Analysis (analyze_ts.py)</label>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1">
<pre class="wrapped"><code class="bash">
# ${BLENDER_DATA}/blender_environments contains several backgrounds,
# ${BLENDER_DATA}/blender_models contain the 3D model of a mug.
export BLENDER_DATA=$(realpath blog_demo)/data/texture_swaps

# List the materials that we will use for this post:
ls blog_demo/data/texture_swaps/blender_control_material
# You can also make or download blender materials corresponding 
# to other textures you want to test, and add them to that folder 

# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

export RESULTS_FOLDER=results_texture

# Run 3DB (with the YAML file from the next pane saved as texture_swaps.yaml):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA texture_swaps.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Finally, run analysis using pandas (copy from third pane)
python analyze_ts.py

</code></pre>
</div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.position"
    offset_x: 0.
    offset_y: 0.5
    offset_z: 0.
  - module: "threedb.controls.blender.pin_to_ground"
    z_ground: 0.25
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: [0., 1.]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.material"
    replacement_material: ["cow.blend", "elephant.blend", "zebra.blend", "crocodile.blend", "keep_original"]
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json

log_lines = open('results_texture/details.log').readlines()
class_map = json.load(open('results_texture/class_maps.json'))
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))
df = df.drop('render_args', axis=1).join(pd.DataFrame(df.render_args.values.tolist()))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])
df['is_correct'] = (df['is_correct'] == 'True')

res = df.groupby('MaterialControl.replacement_material').agg(acc=('is_correct', 'mean'),
      most_frequent_prediction=('prediction', lambda x: x.mode()))
print(res)

</code></pre></div>
    </div>
</div>
</details>

<h2 id="part-of-object-attribution">Part-of-Object Attribution</h2>

<p>Beyond general hypotheses about model biases, 3DB allows us to test vision
systems on a more fine-grained level. In the case of our running mug example,
for instance, we can use the platform to understand which specific parts of
its 3D mesh correlate with classifier accuracy. Specifically, below we
generate (and classify) scenes with random mug positions, rotations, and
backgrounds. Since 3DB stores texture-coordinate information for each
rendering, we can reconstruct a three-dimensional heatmap that encodes, for
each point on the surface of the mug, the classifier’s accuracy conditioned
on that point being visible:</p>

<div style="text-align: center;" id="heatmap-blocker">
    
        
            
        
        
        
    
</div>
<div style="margin-top: 0px; margin-bottom: 15px;" class="caption">
    A part-of-object attribution heatmap for our mug 3D model. <span style="color: red;">Red</span> pixels indicate areas whose visibility most
    improves accuracy, whereas <span style="color: blue;">blue</span> areas'
    visibility correlates with incorrect classifications. <strong>Click here to <a id="heatmap-interact">enable
    interactivity</a>.</strong>
</div>

<p>A number of phenomena stand out from this heatmap, including:</p>

<ol>
  <li>The classifier is worse when the side of the mug opposite the handle is seen.</li>
  <li>The classifier is more accurate when the bottom rim is visible.</li>
  <li>The classifier performs worst when the inside of the mug is visible.</li>
</ol>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="pobj-tab1" type="radio" checked="" name="pobj-tab" class="tab1" />
    <label for="pobj-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="pobj-tab2" type="radio" name="pobj-tab" class="tab2" />
    <label for="pobj-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <input id="pobj-tab3" type="radio" name="pobj-tab" class="tab3" />
    <label for="pobj-tab3"><i class="fa fa-search"></i>  Analysis (analyze_po.py)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# point BLENDER_DATA to the environments and models for this experiment
export BLENDER_DATA=$(realpath blog_demo)/data/part_of_object
# if you want to use the pre-written material in blog_demo, uncomment:
# cd blog_demo

# Optionally: download additional backgrounds (`.hdr` or `.blend`) e.g.,
wget URL -O $BLENDER_DATA/blender_environments/new_env.hdr

# Point results folder to where you want output written
export RESULTS_FOLDER='results_part_of_object'

# Run 3DB (with the YAML file from the next pane):
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp;
threedb_master $BLENDER_DATA part_of_object.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

# Run `part_of_object.py` (third pane) to generate the heat map of the mug.
python po_analysis.py

</code></pre></div>
<div class="content c2"><pre><code class="YAML">
base_config: "base.yaml"
policy:
  module: "threedb.policies.random_search"
  samples: 20
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_uv: True
controls:
  - module: "threedb.controls.blender.orientation"
    rotation_x: -1.57
    rotation_y: 0.
    rotation_z: [-3.14, 3.14]
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    view_point_x: 1.
    view_point_y: 1.
    view_point_z: 1.
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"
  - module: "threedb.controls.blender.background"
    H: 1.
    S: 0.
    V: 1.
</code></pre></div>
<div class="content c3"><pre><code class="python">
import pandas as pd
import numpy as np
import json
from PIL import Image

DIR = 'results_part_of_object'
log_lines = open(f'{DIR}/details.log').readlines()
df = pd.DataFrame.from_records(list(map(json.loads, log_lines)))

# From class index to class name (for readability)
class_map = json.load(open(f'{DIR}/class_maps.json'))
df['prediction'] = df['prediction'].apply(lambda x: class_map[x[0]])

# We'll be a little lenient here to get a more interesting heatmap
df['is_correct'] = df['prediction'].isin(['cup', 'coffee mug'])

uv_num_correct = np.zeros((256, 256))
uv_num_visible = np.zeros((256, 256))
for imid in df["id"].unique().tolist():
    is_correct = float(df.set_index('id').loc[imid]['is_correct'])
    vis_coords_im = Image.open(f'{DIR}/images/{imid}_uv.png')
    vis_coords = np.array(vis_coords_im).reshape(-1, 3)
    # R and G channels encode texture coordinates (x, y), 
    # B channel is 255 for object and 0 for background
    # So we will filter by B then only look at R and G.
    vis_coords = vis_coords[vis_coords[:,2] &gt; 0][:,:2]

    uv_num_visible[vis_coords[:,0], vis_coords[:,1]] += 1.
    uv_num_correct[vis_coords[:,0], vis_coords[:,1]] += is_correct

# Accuracy = # correct / # visible
uv_accuracy = uv_num_correct / (uv_num_visible + 1e-4)

# Saves a black-and-white heatmap
Image.fromarray((255 * uv_accuracy).astype('uint8'))

</code></pre></div>
</div>
</div>
</details>
<p><br /></p>

<p>Now that we have hypotheses regarding model performance, we can test them! Inspecting
the ImageNet validation set, we found that our classifier indeed (a) struggles on coffee mugs when the
handle is not showing (providing a feasible explanation for (1), since the side
opposite the handle is only visible when the handle itself isn’t), and (b)
performs worse at higher camera angles (providing a plausible explanation for
(2)). We want to focus, however, on the third phenomenon,
i.e., that the classifier performs quite poorly whenever the inside of the
mug is visible. Why could this be the case? We can use 3DB to gain insight into the
phenomenon. Specifically, we want to test the following hypothesis: when
classifying mugs, does our ImageNet model rely on the exact liquid inside the
cup?</p>

<p>We investigate this hypothesis by writing a custom control that fills
our mug with various liquids (more precisely, a parameterized mixture of
water, milk, and coffee):</p>

<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_samples.png" alt="Examples of the mugs rendered in this experiment" /></p>
<div class="caption">
Our running example mug, filled with different parameterized liquids: 100% water
(top left), 100% coffee (top right), 100% milk (bottom right), and a coffee-milk
mixture (bottom left)
</div>

<p>In contrast to the last experiment (where we varied the orientation of the
mug), we render scenes containing the mug in a fixed set of poses that reveal
the contents—just as in the last experiment, however, we still vary
background and mug location. We visualize the results below—each cell in
the heatmap corresponds to a fixed mixture of coffee, water, and milk (i.e.,
the labeled corners are 100% coffee, 100% milk, and 100% water, and the other
cells are linear interpolations of these ratios) and the color of the cell
encodes the relative accuracy of the classifier when the mug is filled with
that liquid:</p>



<p><img src="https://gradientscience.org/assets/3db/mug_liquid_experiment_simplex.png" alt="" /></p>

<div class="caption">
Measuring the relative effect of the liquid mixture in the mug on model
predictions. Each cell represents a specific liquid mixture, and the color of
the cell represents the tendency of the model (averaged over random viewpoints
and relative to the other cells) to predict "cup"/"pill bottle," "bucket," or
"mug."
</div>

<p>It turns out that mug content indeed highly impacts classification:
our model is much less likely to correctly classify a mug that doesn’t contain coffee!
This is just one example of how 3DB can help in proving or disproving hypotheses
about model behavior.</p>

<h2 id="from-simulation-to-reality">From Simulation to Reality</h2>

<p>So far, we’ve used 3DB to discover ML models’ various failure modes and biases
via photorealistic rendering. To what extent though do the insights
gleaned from simulated 3DB experiments actually “transfer” to the physical
world?</p>

<p>To test such transferability, we began by creating a 3D model of a physical room we
had access to. We also collected eight different 3D models with closely matching
physical world counterparts—including the mug analyzed above. Next, we used 3DB to find correctly and incorrectly classified
configurations (pose, orientation, location) of these eight objects inside that
room. Finally, we replicated these poses (to
the best of our abilities) in the physical room, and took photos with a
cellphone camera:</p>

<p><img src="https://gradientscience.org/assets/3db/real_life_exp_samples.png" alt="Samples from our physical-world experiment." class="bigimg" /></p>

<div class="caption">
Examples of simulated scenes (top) and their re-created counterparts (bottom)
from our physical-world experiment.
</div>

<p>We classified these photos with the same vision model as before and measured
how often the simulated classifier correctness matched correctness on the
real photographs. We observed an ~85% match! So the failure
modes identified by 3DB are not merely simulation artifacts, and can indeed arise in
the real world.</p>

<h2 id="conclusion">Conclusion</h2>

<p>3DB is a flexible, easy-to-use, and extensible framework for
identifying model failure modes, uncovering biases, and testing fine-grained
hypotheses about model behavior. We hope it will prove to be a useful tool
for debugging vision models.</p>
<h2 id="bonus-object-detection-web-dashboard-and-more">Bonus: Object Detection, Web Dashboard, and more!</h2>

<p>We’ll wrap up by highlighting some additional capabilities of 3DB that we didn’t
get to demonstrate in this blog post:</p>

<h3 id="3dboard-a-web-interface-for-exploring-results">3DBoard: a web interface for exploring results</h3>

<p>In all of the code examples above, we showed how to analyze the results of a 3DB
experiment by loading the output into a <code class="language-plaintext highlighter-rouge">pandas</code> dataframe. For additional
convenience, however, 3DB also comes with a web-based dashboard for exploring
experimental results. The following command suffices to visualize the texture swaps experiment from earlier:</p>

<pre style="padding: 0; margin: 0;"><code class="bash">
python -m threedboard results_texture/ --port 3000

</code>
</pre>

<p>Navigating to <code class="language-plaintext highlighter-rouge">YOUR_IP:3000</code> should lead you to a page that looks like this:</p>

<p><img src="https://gradientscience.org/assets/3db/dashboard_screenshot.png" alt="Dashboard screenshot" /></p>

<h3 id="object-detection-and-other-tasks">Object detection and other tasks</h3>

<p>In this blog post, we focused on using 3DB to analyze image classification
models. However, the library also supports object detection out-of-the-box, and
can easily be extended to support a variety of image-based tasks (e.g.,
segmentation or regression-based tasks). For example, below we provide a simple
end-to-end object detection example:</p>

<details>
<strong>Show/hide code and instructions</strong>
<div class="code-container">
    <input id="objdet-tab1" type="radio" checked="" name="objdet-tab" class="tab1" />
    <label for="objdet-tab1"><i class="fa fa-gear"></i>  Setup</label>
    <input id="objdet-tab2" type="radio" name="objdet-tab" class="tab2" />
    <label for="objdet-tab2"><i class="fa fa-code"></i>  Config (part_of_object.yaml)</label>
    <div class="line"></div>
    <div class="line"></div>
    <div class="content-container">
<div class="content c1"><pre class="wrapped"><code class="makefile">
# The object detection example is separate from the rest of the blog demo, so
# run the following in a separate repo:
git clone https://github.com/3db/object_detection_demo

# The repo has a data/ folder containing the Blender model (a banana) and some
# HDRI backgrounds, a classmap.json file mapping the UID of the model to a COCO
# class, and the detection.yaml file from the next pane.
cd object_detection_demo/

export BLENDER_DATA=data/
export RESULTS_FOLDER=results/

# Run 3DB
threedb_workers 1 $BLENDER_DATA 5555 &gt; client.log &amp; 
threedb_master $BLENDER_DATA detection.yaml $RESULTS_FOLDER 5555

# Analyze results in the dashboard
python -m threedboard $RESULTS_FOLDER --port 3000
# Navigate to localhost:3000 to view the results!

</code></pre></div> 
<div class="content c2"><pre><code class="YAML">
inference:
  module: 'torchvision.models.detection'
  class: 'retinanet_resnet50_fpn'
  label_map: './resources/coco_mapping.json'
  normalization:
    mean: [0., 0., 0.]
    std: [1., 1., 1.]
  resolution: [224, 224]
  args:
    pretrained: True
evaluation:
  module: 'threedb.evaluators.detection'
  args:
    iou_threshold: 0.5
    nms_threshold: 0.1
    max_num_boxes: 10
    classmap_path: 'classmap.json'
render_args:
  engine: 'threedb.rendering.render_blender'
  resolution: 256
  samples: 16
  with_segmentation: true
policy:
  module: "threedb.policies.random_search"
  samples: 2
logging:
  logger_modules:
    - "threedb.result_logging.image_logger"
    - "threedb.result_logging.json_logger"
controls:
  - module: "threedb.controls.blender.orientation"
  - module: "threedb.controls.blender.camera"
    zoom_factor: [0.7, 1.3]
    aperture: 8.
    focal_length: 50.
  - module: "threedb.controls.blender.denoiser"

</code></pre></div>
</div>
</div>
</details></div>







<p class="date">
<a href="https://gradientscience.org/3db/"><span class="datestr">at June 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8127">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/06/07/new-seminar-series-in-simons-institute/">New seminar series in Simons Institute</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons institute started a <a href="https://simons.berkeley.edu/news/institute-launches-breakthroughs-lecture-series">new virtual seminar</a> series highlighting recent advances in theoretical computer science. The first two talks in the series will be:</p>



<ul><li>June 16th 10am-11am PDT (1pm-2pm EDT). Virginia Vassilevska Williams on a  <a href="https://simons.berkeley.edu/events/breakthroughs-refined-laser-method-and-faster-matrix-multiplication">Refined Laser Method and Faster Matrix Multiplication</a></li><li>August 5 10am-11am PDT (1pm-2pm EDT) Yuansi Chen on <a href="https://simons.berkeley.edu/events/breakthroughs-almost-constant-lower-bound-isoperimetric-coefficient-kls-conjecture-0">An Almost Constant Lower Bound of the Isoperimetric Coefficient in the KLS Conjecture</a></li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/06/07/new-seminar-series-in-simons-institute/"><span class="datestr">at June 07, 2021 06:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/icml2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/icml2021/">Conference Digest - ICML 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://icml.cc/Conferences/2021">ICML 2021</a>, one of the biggest conferences in machine learning, naturally has a ton of interesting sounding papers on the topic of differential privacy.
We went through this year’s <a href="https://icml.cc/Conferences/2021/AcceptedPapersInitial">accepted papers</a> and aggregated all the relevant papers we could find.
In addition, this year features three workshops on the topic of privacy, as well as a tutorial.
As always, please inform us if we overlooked any papers on differential privacy.</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li>
    <p><a href="http://federated-learning.org/fl-icml-2021/">Federated Learning for User Privacy and Data Confidentiality</a></p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/ml4data">Machine Learning for Data: Automated Creation, Privacy, Bias</a></p>
  </li>
  <li>
    <p><a href="https://tpdp.journalprivacyconfidentiality.org/2021/">Theory and Practice of Differential Privacy</a></p>
  </li>
</ul>

<h2 id="tutorial">Tutorial</h2>

<ul>
  <li><a href="https://icml.cc/Conferences/2021/Schedule?showEvent=10839">Privacy in Learning: Basics and the Interplay</a><br />
<a href="https://www.microsoft.com/en-us/research/people/huzhang/">Huishuai Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2009.02668">A Framework for Private Matrix Analysis in Sliding Window Model</a><br />
<a href="https://sites.google.com/view/jalajupadhyay/home">Jalaj Upadhyay</a>, <a href="https://www.fujitsu.com/us/about/businesspolicy/tech/rd/research-staff/sarvagya.html">Sarvagya Upadhyay</a></p>
  </li>
  <li>
    <p>Accuracy, Interpretability, and Differential Privacy via Explainable Boosting<br />
<a href="https://scholar.google.com/citations?user=HmxjgMAAAAAJ">Harsha Nori</a>, <a href="https://www.microsoft.com/en-us/research/people/rcaruana/">Rich Caruana</a>, <a href="https://sites.google.com/view/zhiqi-bu">Zhiqi Bu</a>, <a href="https://heyyjudes.github.io/">Judy Hanwen Shen</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a></p>
  </li>
  <li>
    <p>Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message<br />
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, <a href="https://rasmuspagh.net/">Rasmus Pagh</a>, <a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2011.00467">Differentially Private Bayesian Inference for Generalized Linear Models</a><br />
<a href="https://warwick.ac.uk/fac/sci/dcs/people/u1554597">Tejas Kulkarni</a>, <a href="https://users.aalto.fi/~jalkoj1/">Joonas Jälkö</a>, <a href="https://scholar.google.com/citations?user=Y_EvCPAAAAAJ">Antti Koskela</a>, <a href="https://people.aalto.fi/samuel.kaski">Samuel Kaski</a>, <a href="https://www.cs.helsinki.fi/u/ahonkela/">Antti Honkela</a></p>
  </li>
  <li>
    <p>Differentially-Private Clustering of Easy Instances<br />
<a href="http://www.cohenwang.com/edith/">Edith Cohen</a>, <a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a>, <a href="https://www.linkedin.com/in/eliad-tsfadia-21482b96/">Eliad Tsfadia</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08885">Differentially Private Correlation Clustering</a><br />
<a href="https://cs-people.bu.edu/mbun/">Mark Bun</a>, <a href="https://elias.ba30.eu/">Marek Elias</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.13287">Differentially Private Densest Subgraph Detection</a><br />
<a href="https://biocomplexity.virginia.edu/person/dung-nguyen">Dung Nguyen</a>, <a href="https://engineering.virginia.edu/faculty/anil-vullikanti">Anil Vullikanti</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08244">Differentially Private Quantiles</a><br />
<a href="http://jgillenw.com/">Jennifer Gillenwater</a>, <a href="https://www.majos.net/">Matthew Joseph</a>, <a href="https://www.alexkulesza.com/">Alex Kulesza</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.06641">Differentially Private Query Release Through Adaptive Projection</a><br />
<a href="https://sergulaydore.github.io/">Sergul Aydore</a>, <a href="https://wibrown.github.io/">William Brown</a>, <a href="https://www.cis.upenn.edu/~mkearns/">Michael Kearns</a>, <a href="http://www-cs-students.stanford.edu/~kngk/">Krishnaram Kenthapadi</a>, <a href="https://www.lucamel.is/">Luca Melis</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, <a href="https://ankitsiva.xyz/">Ankit Siva</a></p>
  </li>
  <li>
    <p>Differentially Private Sliced Wasserstein Distance<br />
<a href="http://asi.insa-rouen.fr/enseignants/~arakoto/">Alain Rakotomamonjy</a>, <a href="https://pageperso.lif.univ-mrs.fr/~liva.ralaivola/doku.php">Liva Ralaivola</a></p>
  </li>
  <li>
    <p>Large Scale Private Learning via Low-rank Reparametrization<br />
<a href="https://scholar.google.com/citations?user=FcRGdiwAAAAJ">Da Yu</a>, <a href="https://www.microsoft.com/en-us/research/people/huzhang/">Huishuai Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chen</a>, Jian Yin, <a href="https://www.microsoft.com/en-us/research/people/tyliu/">Tie-Yan Liu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.08598">Leveraging Public Data for Practical Private Query Release</a><br />
<a href="https://www.linkedin.com/in/terrance-liu-26796974/">Terrance Liu</a>, <a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://zstevenwu.com/">Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.09734">Locally Private k-Means in One Round</a><br />
Alisa Chang, <a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.12099">Lossless Compression of Efficient Private Local Randomizers</a><br />
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.08233">Oneshot Differentially Private Top-k Selection</a><br />
<a href="https://lsa.umich.edu/stats/people/phd-students/qiaogang.html">Gang Qiao</a>, <a href="http://www-stat.wharton.upenn.edu/~suw/">Weijie Su</a>, <a href="https://research.google/people/LiZhang/">Li Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.12321">PAPRIKA: Private Online False Discovery Rate Control</a><br />
<a href="https://wanrongz.github.io/">Wanrong Zhang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.00039">Practical and Private (Deep) Learning without Sampling or Shuffling</a><br />
<a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://research.google/people/author35837/">Brendan McMahan</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="http://www.omthakkar.com/">Om Thakkar</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a>, <a href="https://research.google/people/106689/">Zheng Xu</a></p>
  </li>
  <li>
    <p>Private Adaptive Gradient Methods for Convex Optimization<br />
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="https://web.stanford.edu/~jduchi/">John Duchi</a>, <a href="https://afallah.lids.mit.edu/">Alireza Fallah</a>, <a href="https://scholar.google.com/citations?user=_JXjrEp9FhYC">Omid Javidbakht</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p>Private Alternating Least Squares: (Nearly) Optimal Privacy/Utility Trade-off for Matrix Completion<br />
Steve Chien, <a href="https://www.prateekjain.org/">Prateek Jain</a>, <a href="http://walid.krichene.net/">Walid Krichene</a>, <a href="https://scholar.google.com/citations?user=yR-ugIoAAAAJ">Steffen Rendle</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a>, <a href="https://research.google/people/LiZhang/">Li Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.01516">Private Stochastic Convex Optimization: Optimal Rates in L1 Geometry</a><br />
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06387">The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation</a><br />
<a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://kenziyuliu.github.io/">Ziyu Liu</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/icml2021/"><span class="datestr">at June 07, 2021 04:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=857">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/">Data-structure lower bounds without encoding arguments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>                        </p>
<p style="text-align: justify;">I have recently posted the paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#Xviola-rank-samp">Vio21</a>]</span> (<a href="https://eccc.weizmann.ac.il/report/2021/073/">download</a>) which does something that I have been trying to do for a long time, more than ten years, on and off. Consider the basic data-structure problem of storing <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m" class="latex" /> bits of data <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bm%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="x\in \{0,1\}^{m}" class="latex" /> into <img src="https://s0.wp.com/latex.php?latex=m%2Br&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="m+r" class="latex" /> bits so that the <em>prefix-sum queries</em></p>
<div style="text-align: center;"> <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+%7B%5Ctext+%7B%5Ctextsc+%7BRank%7D%7D%7D%28i%29%3A%3D%5Csum+_%7Bj%5Cle+i%7Dx_%7Bj%7D+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\begin{aligned} \mathbb {\text {\textsc {Rank}}}(i):=\sum _{j\le i}x_{j} \end{aligned}" class="latex" /></div>
<p style="text-align: justify;">   can be computed by probing <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> <em>cells</em> (or <em>words</em>) of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w" class="latex" /> bits each. (You can think <img src="https://s0.wp.com/latex.php?latex=w%3D%5Clog+m&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="w=\log m" class="latex" /> throughout this post.) The paper <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> with Pǎtraşcu shows that <img src="https://s0.wp.com/latex.php?latex=r%5Cge+m%2Fw%5E%7BO%28q%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r\ge m/w^{O(q)}" class="latex" />, and this was recently shown to be tight by Yu <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/conf/stoc/Yu19">Yu19</a>]</span> (building on the breakthrough data structure <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascu08Succincter">Pǎt08</a>]</span> which motivated the lower bound and is not far from it).</p>
<p style="text-align: justify;">   As is common in data-structure lower bounds, the proof in <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> is an <em>encoding argument</em>. In the recently posted paper, an alternative proof is presented which avoids the encoding argument and is perhaps more in line with other proofs in complexity lower bounds. Of course, <em>everything</em> is an encoding argument, and <em>nothing </em>is an encoding argument, and this post won’t draw a line.</p>
<p style="text-align: justify;">   The new proof establishes an <em>intrinsic property</em> of efficient data structures, whereas typical proofs including <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XPatrascuV10">PV10</a>]</span> are somewhat tailored to the problem at hand. The property is called the <em>separator</em> and is a main technical contribution of the work. At the high level the separator shows that in any efficient data structure you can restrict the input space a little so that many queries are nearly <em>pairwise independent</em>.</p>
<p style="text-align: justify;">   Also, the new proof rules out a stronger object: a <em>sampler</em> (<a href="https://emanueleviola.wordpress.com/2014/11/09/is-nature-a-low-complexity-sampler/">see previous post here</a> on sampling lower bounds). Specifically, the distribution Rank<img src="https://s0.wp.com/latex.php?latex=%28U%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="(U)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="U" class="latex" /> is the uniform distribution cannot be sampled, not even slightly close, by an efficient cell-probe algorithm. This implies the data-structure result, and it can be informally interpreted as saying that the “reason” why the lower bound holds is not that the data is compressed, but rather that one can’t generate the type of dependencies occurring in Rank via an efficient cell-probe algorithm, regardless of what the input is.</p>
<p style="text-align: justify;">   Building on this machinery, one can prove several results about sampling, like showing that cell-probe samplers are strictly weaker than AC0 samplers. While doing this, it occurred to me that one gets a corollary for data structures which I had not seen in the literature. The corollary is a <em>probe hierarchy</em>, showing that some problem can be solved with zero redundancy (<img src="https://s0.wp.com/latex.php?latex=r%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r=0" class="latex" />) with <img src="https://s0.wp.com/latex.php?latex=O%28q%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="O(q)" class="latex" /> probes, while it requires almost linear <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> probes. For example I don’t know of a result yielding this for small <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q" class="latex" /> such as <img src="https://s0.wp.com/latex.php?latex=q%3DO%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="q=O(1)" class="latex" />; I would appreciate a reference. (As mentioned in the                                                                                                                                                        paper, the sampling viewpoint is not essential and just like for Rank one can prove the data-structure corollaries directly. Personally, and obviously, I find the sampling viewpoint useful.)</p>
<p style="text-align: justify;">   One of my favorite open problems in the area still is: can a uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Bm%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="[m]" class="latex" /> be approximately sampled by an efficient cell-probe algorithm? I can’t even rule out samplers making <em>two </em>probes!</p>
<h3 class="likesectionHead"><a id="x1-1000"></a>References</h3>
<p style="text-align: justify;">
</p><div class="thebibliography">
<p class="bibitem"><span class="biblabel">  [Pǎt08]<span class="bibsp">   </span></span><a id="XPatrascu08Succincter"></a>Mihai  Pǎtraşcu.      Succincter.      In  49th  IEEE  Symp. on         Foundations of Computer Science (FOCS). IEEE, 2008.</p>
<p class="bibitem"><span class="biblabel">  [PV10]<span class="bibsp">   </span></span><a id="XPatrascuV10"></a>Mihai Pǎtraşcu and Emanuele Viola.  Cell-probe lower bounds         for succinct partial sums.  In 21th ACM-SIAM Symp. on Discrete         Algorithms (SODA), pages 117–122, 2010.</p>
<p class="bibitem"><span class="biblabel">  [Vio21]<span class="bibsp">   </span></span><a id="Xviola-rank-samp"></a>Emanuele       Viola.                    Lower       bounds       for         samplers and data structures via the cell-probe separator. Available         at <a href="http://www.ccs.neu.edu/home/viola/" rel="nofollow">http://www.ccs.neu.edu/home/viola/</a>, 2021.</p>
<p class="bibitem"><span class="biblabel">  [Yu19] <span class="bibsp">   </span></span><a id="XDBLP:conf/stoc/Yu19"></a>Huacheng  Yu.     Optimal  succinct  rank  data  structure  via         approximate nonnegative tensor decomposition.  In Moses Charikar         and Edith Cohen, editors, ACM Symp. on the Theory of Computing         (STOC), pages 955–966. ACM, 2019.</p>
</div></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/06/07/data-structure-lower-bounds-without-encoding-arguments/"><span class="datestr">at June 07, 2021 03:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18838">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/">Happy Mom’s Day</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>You know, I loved math. My mom was a math teacher—Joan Cusack</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/kfrlmothers/" rel="attachment wp-att-18865"><img width="212" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/KFRLmothers.jpg?resize=212%2C139&amp;ssl=1" class="alignright size-full wp-image-18865" height="139" /></a></p>
<p>
Mary Kay Farley, my dear wife’s mom, and Dorothy Lipton, my mom, have unfortunately both passed away. Kathryn and I miss them greatly. Both women shared keen mathematical skills, a fascination with the game of baseball and a commitment to living a well-ordered life.  </p>
<p>
Today is <b>not</b> Mother’s Day. We still hope all mothers everywhere are enjoying their day.</p>
<p>
We will take this time to thank all of you out there. We missed doing so last month, but the pandemic has distended time anyway. What we are hearing now are stories of mothers and children and grandchildren finally being able to think of seeing each other in person rather than via video.</p>
<p>
</p><p></p><h2> Another Kind of Parentage </h2><p></p>
<p></p><p>
This has blended with musings on our recent <a href="https://rjlipton.wpcomstaging.com/2021/03/26/congrats-avi-and-laci-on-the-abel-prize/">post</a> in which I (Dick) noted that Dorit Aharonov is an academic grandchild of mine, in that Avi Wigderson co-supervised her doctoral thesis and I supervised Avi’s. </p>
<p>
Years ago we featured on Father’s Day a <a href="https://rjlipton.wpcomstaging.com/2011/06/19/who's-your-doktorvater/">post</a> with the title “Who’s Your <em>Doktorvater</em>?”—which was a play on the <a href="https://bosoxinjection.com/2014/09/24/ten-years-gone-pedro-martinez-calls-yankees-daddy/">expression</a> “who’s your daddy?” Now it is high time to note that there are many “doctor mothers”—as Dorit has herself <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=102209">become</a>. </p>
<p>
One difference from human genealogy is that most often there is only one “doctor parent.” My <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=86340&amp;fChrono=1">advisor</a>, David Parnas, has two: Alan Perlis and Everard Williams. From Perlis it is a straight shot back to Siméon Poisson, whose 1800 dissertation was co-advised by Joseph Lagrange and Pierre Laplace. For Lagrange there is a strange <a href="https://www.mathgenealogy.org/id.php?id=17864">note</a> of Leonhard Euler as a virtual advisor, but the real one is Giovanni Beccaria—who has no listed parent. Going through Laplace also dead-ends. But selecting Euler includes a chain that ends in the 1100s with Sharaf al-Dīn al-Ṭūsī, who <a href="https://en.wikipedia.org/wiki/Sharaf_al-Din_al-Tusi#Mathematics">improved</a> the complexity of approximately solving cubic equations.</p>
<p>
I appear not to have any female ancestors in my doctoral genealogy. I have two female PhD graduates, one of whom is a <em>Doktormutter</em>. Ken’s first female doctoral student, co-advised, had a successful thesis defense last week; he has another nearing the ABD stage. But I have known quite a few other “doctor mothers” personally. Today, Ken and I thought to recognize them.</p>
<p>
</p><p></p><h2> Some Doctor Moms I Know </h2><p></p>
<p></p><p>
Here are some that I have had the honor to know. They are in a certain order—do you see what it is? I give only the surname on purpose—click the second name and note its URL for a singular reflection of this.</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/Monica_S._Lam">Lam</a>: <a href="https://mathgenealogy.org/id.php?id=50307">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Blum/">Blum</a>: <a href="http://www.cs.cmu.edu/~lblum/PAPERS/lblumShortVita.pdf">co-advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Mary_Shaw_(computer_scientist)">Shaw</a>: <a href="https://www.mathgenealogy.org/id.php?id=50083">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Chung/">Chung</a>: <a href="https://www.mathgenealogy.org/id.php?id=23154">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Maria_Klawe">Klawe</a>: <a href="https://www.mathgenealogy.org/id.php?id=43243">advisee</a> <p></p>
</li><li>
<a href="https://people.csail.mit.edu/lynch/">Lynch</a>: <a href="https://www.mathgenealogy.org/id.php?id=81227">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Ruzena_Bajcsy">Bajcsy</a>: <a href="https://www.mathgenealogy.org/id.php?id=39957">advisees</a> <p></p>
</li><li>
<a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/graham-s.html">Graham</a>: <a href="https://www.mathgenealogy.org/id.php?id=22787">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Barbara_Liskov">Liskov</a>: <a href="https://www.mathgenealogy.org/id.php?id=61932">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Eva_Tardos">Tardos</a>: <a href="https://www.mathgenealogy.org/id.php?id=39422">advisees</a> <p></p>
</li><li>
<a href="https://annacgilbert.github.io/">Gilbert</a>: <a href="https://www.mathgenealogy.org/id.php?id=24150">advisees</a> <p></p>
</li><li>
<a href="https://people.math.gatech.edu/~randall/">Randall</a>: <a href="https://www.mathgenealogy.org/id.php?id=81757">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Rasiowa/">Rasiowa</a>: <a href="https://www.mathgenealogy.org/id.php?id=22601">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Sheila_Greibach">Greibach</a>: <a href="https://www.mathgenealogy.org/id.php?id=25274">advisees</a> <p></p>
</li><li>
<a href="https://en.wikipedia.org/wiki/Shafi_Goldwasser">Goldwasser</a>: <a href="https://www.mathgenealogy.org/id.php?id=35879">advisees</a> <p></p>
</li><li>
<a href="https://mathshistory.st-andrews.ac.uk/Biographies/Daubechies/">Daubechies</a>: <a href="https://www.mathgenealogy.org/id.php?id=44561">advisees</a>
</li></ul>
<p>
The last gives us an all-female tree, not just one branch, of people we know. Besides Anna Gilbert, another of Ingrid Daubechies’s students, who herself has <a href="https://www.mathgenealogy.org/id.php?id=92059">advisees</a>, is Cynthia Rudin of Duke, whom Ken knew and taught while she was an undergraduate at Buffalo.</p>
<p>
There are others I could mention who went into research labs where there are different relationships besides PhD advising. They include Irene Greif, Tal Rabin, Lynn Conway, and Jean Sammet. I could include Jamie Morgenstern, whom we recently <a href="https://rjlipton.wpcomstaging.com/2021/03/10/making-algorithms-fair/">featured</a> and who his <a href="https://jamiemorgenstern.com/">advising</a> her first students at the University of Washington—do they have to be “born” yet to count you as a <em>Doktormutter</em>? I’ve left others out—apologies for that—but the ones I’ve listed make a nice <img src="https://s0.wp.com/latex.php?latex=%7B4+%5Ctimes+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4 \times 4}" class="latex" /> collage:</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/comoms2/" rel="attachment wp-att-18841"><img width="450" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/comoms2.png?resize=450%2C486&amp;ssl=1" class="aligncenter size-full wp-image-18841" height="486" /></a></p>
<p>
</p><p></p><h2> My Upbringing </h2><p></p>
<p></p><p>
Of these, the one with the most formative impact on me was Helena Rasiowa. I learned advanced logic from her when I was an undergraduate. </p>
<p>
Here is a tribute to <a href="http://comet.lehman.cuny.edu/fitting/bookspapers/pdf/papers/Rasiowa.pdf">her</a> by Melvin Fitting: </p>
<blockquote><p><b> </b> <em> I once heard Dana Scott criticize her <a href="https://www.amazon.com/Mathematics-Metamathematics-Helena-SIKORSKI-RASIOWA/dp/B005JGKZXW">book</a>, <em>The Mathematics of Metamathematics</em> with Roman Sikorski, because, while it took an algebraic approach to logic, it did not carry the work further and consider set theory. If it had, then <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">forcing</a> would have been discovered years earlier than it was. This is not, at heart, a criticism, but a tribute. The building of mathematics always goes on. Foundations, firmly laid, enable later construction, and the foundations laid by that book were powerfully firm. </em>
</p></blockquote>
<p>
Ken also points to logic as a formative influence—though from men at Oxford.  Both of us were attracted to Gödel-type undecidability issues in complexity theory in the early 1980s.  The nexus of logic and algebra has been important to us in different ways, Ken more with finite automata and descriptive complexity.  Courses in logic gave both of us a habit of framing problems along formal lines.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Which “doctor mothers” have you known or been influenced by?</p>
<p></p><p><br />
[Added and re-formatted photos at top]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/06/07/happy-moms-day/"><span class="datestr">at June 07, 2021 04:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/">Workshop on Machine Learning for Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 13-14, 2021 Foundations of Data Science Institute (FODSI) https://fodsi.us/ml4a.html In recent years there has been increasing interest in using machine learning to improve the performance of classical algorithms in computer science, by fine-tuning their behavior to adapt to the properties of the input distribution. This “data-driven” or “learning-based” approach to algorithm design has the … <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">Workshop on Machine Learning for Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/06/07/workshop-on-machine-learning-for-algorithms/"><span class="datestr">at June 07, 2021 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-562506050674619397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/when-do-you-use-et-al-as-opposed-to.html">When do you use et al. as opposed to listing out the authors? First names? Middle initials? Jr?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> If I was refering to the paper with bibtex entry: </p><p><br /></p><p>@misc{BCDDL-2018,</p><p>  author    = {Jeffrey Bosboom and</p><p>               Spencer Congero and</p><p>               Erik D. Demaine and</p><p>               Martin L. Demaine and</p><p>               Jayson Lynch},</p><p>  title     = {Losing at Checkers is Hard},</p><p>  year      = {2018},</p><p>  note      = {\newline\url{http://arxiv.org/abs/1806.05657}},</p><p>}</p><p>(The paper is <a href="http://arxiv.org/abs/1806.05657">here</a>.)</p><p>I would write </p><p><i>Bosboom et al.~\cite{BCDDL-2018} proved that trying to lose at checkers (perhaps you are playing a child and want to keep up their self-esteem, or a Wookie and don't want your arms to be torn off your shoulders   (see <a href="https://www.starwars.com/video/let-the-wookiee-win">here</a>),  or a Wookie child) is hard. </i></p><p><br /></p><p>Why did I use<i> et al. </i>?<i> </i>Because it would be a pain to write out all of those names. </p><p>How many names does it take to make you write <i>et al. </i>? Are there exceptions? </p><p>I have not seen any discussion of this point on the web. So here are my rules of thumb and some questions.(CORRECTION- a commenter points out that this IS discussed on the web. Even so, you can comment on my thoughts or give your thoughts or whatever you like.) </p><p>1) If  there are 3 or more people than use et al. Exception: If the three are VERY WELL KNOWN as a triple. E.g., <i>the double play was Tinkers to Evers to Chance. </i>Well, that would not really come up since in baseball nobody ever says <i>the double play was Tinkers et al.</i>  More relevant examples:</p><p>Aho, Hopcroft, and Ullman</p><p>Cormen, Leiserson, and Rivest, also known as CLR</p><p>The more recent edition is</p><p>Cormen, Leiserson, Rivest, and Stein. I have heard CLRS but I don't know if people WRITE all four names. </p><p>Lenstra-Lenstra-Lovasz also usually mentions all three. </p><p>2) If there is one name do not use <i>et al</i>.  unless that one person has a multiple personality disorder.</p><p>3) If there are 2 people it can be tricky and perhaps unfair. If the second one has a long name then I am tempted to use<i> et al.</i> For example</p><p>Lewis and Papadimitriou (If I mispelled Christos's name-- well- that's  the point!- to avoid spelling errors I want to use <i>et al.</i> )</p><p>Lampropoulos and Paraskevopoulou (the first one is UMCP new faculty!). After typing in the first name I would not be in the mood to type in the second. </p><p>Similar if there are lots of accents in the name making it hard to type in LaTeX (though I have macros for some people like Erdos who come up a lot) then I might use<i> et al. </i></p><p>(ADDED LATER- some of the commenters object to my `rule' of leaving out the last name if its complicated. That is not really my rule- the point of this post was to get a discussion going about the issue, which I am happy to say has happened.) </p><p>----------</p><p>There are other issues along these lines: when to include the first name (when there is more than one person with that last name, e.g. Ryan Williams and Virginia  Vassilevska Williams), when to use middle initials (in the rare case where there is someone with the same first and last name- Michael  J. Fox added the J and uses it since there was an actor named Michael Fox.)</p><p>I will soon give a quote from a math paper that amused me, but first some context.  The problem of determining if a poly in n variables over Z has an integer solution is called E(n). By the solution to Hilbert's 10th problem we know that there exists n such that E(n) is undecidable. E(9) is undecidable, but the status of E(8) is unknown (as of May 2021) and has been since the early 1980's. </p><p>Here is the quote (from <a href="http://maths.nju.edu.cn/~zwsun/14z.pdf">here</a>).</p><p><i>Can we replace 9 by a smaller number? It is believed so. In fact, A. Baker, Matiyasevich and J.Robinson  even conjectured that E(3) is undecidable over N.</i></p><p>Note that Baker and Robinson get their first initial but Matiyasevich does not.</p><p>I suspect that they use J. Robinson since there is another mathematician with last name Robinson: Rafael Robinson who was Julia's Robinson's husband (to my younger readers--- there was a time when a women who got married took her husband's last name). There is at least one other Math-Robinson: Robert W Robinson. I do not think he is closely related. </p><p>Baker: I do not know of another mathematician named Baker. I tried Google, but the Bakers  I found were   not in the right time frame. I also kept finding hits to an anecdote about Poincare and a man whose profession was a baker (see <a href="https://gilkalai.wordpress.com/2019/10/13/the-story-of-poincare-and-his-friend-the-baker/">here</a> though its later in that blog post). However, I suspect there was another mathematician named Baker which is why the author uses the first initial.  Its possible the author did not want to confuse Alan  Baker with Theodore Baker, one of the authors of Baker-Gill-Solovay that showed there were oracles that made P = NP and others that made P NE NP.  But somehow, that just doesn't seem right to me. I suspect there is only one mathematician with last name Matijsavic. </p><p>Thomas Alva Edison named his son Thomas Alva Edison Jr.  This was a bad idea but not for reasons of authorship, see <a href="http://edisontinfoil.com/taejr/edisonjr.htm">here</a>.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/when-do-you-use-et-al-as-opposed-to.html"><span class="datestr">at June 07, 2021 03:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02619">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02619">Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allen=Zhu:Zeyuan.html">Zeyuan Allen-Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuanzhi.html">Yuanzhi Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02619">PDF</a><br /><b>Abstract: </b>Generative adversarial networks (GANs) are among the most successful models
for learning high-complexity, real-world distributions. However, in theory, due
to the highly non-convex, non-concave landscape of the minmax training
objective, GAN remains one of the least understood deep learning models. In
this work, we formally study how GANs can efficiently learn certain
hierarchically generated distributions that are close to the distribution of
images in practice. We prove that when a distribution has a structure that we
refer to as Forward Super-Resolution, then simply training generative
adversarial networks using gradient descent ascent (GDA) can indeed learn this
distribution efficiently, both in terms of sample and time complexities. We
also provide concrete empirical evidence that not only our assumption "forward
super-resolution" is very natural in practice, but also the underlying learning
mechanisms that we study in this paper (to allow us efficiently train GAN via
GDA in theory) simulates the actual learning process of GANs in practice on
real-world problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02619"><span class="datestr">at June 07, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02538">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02538">Bottleneck Profiles and Discrete Prokhorov Metrics for Persistence Diagrams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Paweł Dłotko, Niklas Hellmer <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02538">PDF</a><br /><b>Abstract: </b>In topological data analysis (TDA), persistence diagrams have been a
succesful tool. To compare them, Wasserstein and Bottleneck distances are
commonly used. We address the shortcomings of these metrics and show a way to
investigate them in a systematic way by introducing bottleneck profiles. This
leads to a notion of discrete Prokhorov metrics for persistence diagrams as a
generalization of the Bottleneck distance. They satisfy a stability result and
bounds with respect to Wasserstein metrics. We provide algorithms to compute
the newly introduced quantities and end with an discussion about experiments.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02538"><span class="datestr">at June 07, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02397">On Classifying Continuous Constraint Satisfaction problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a>, Reinier F. Schmiermann <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02397">PDF</a><br /><b>Abstract: </b>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.
</p>
<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.
</p>
<p>We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02397"><span class="datestr">at June 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02353">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02353">Spectral Hypergraph Sparsifiers of Nearly Linear Size</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapralov:Michael.html">Michael Kapralov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tardos:Jakab.html">Jakab Tardos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02353">PDF</a><br /><b>Abstract: </b>Graph sparsification has been studied extensively over the past two decades,
culminating in spectral sparsifiers of optimal size (up to constant factors).
Spectral hypergraph sparsification is a natural analogue of this problem, for
which optimal bounds on the sparsifier size are not known, mainly because the
hypergraph Laplacian is non-linear, and thus lacks the linear-algebraic
structure and tools that have been so effective for graphs.
</p>
<p>Our main contribution is the first algorithm for constructing
$\epsilon$-spectral sparsifiers for hypergraphs with $O^*(n)$ hyperedges, where
$O^*$ suppresses $(\epsilon^{-1} \log n)^{O(1)}$ factors. This bound is
independent of the rank $r$ (maximum cardinality of a hyperedge), and is
essentially best possible due to a recent bit complexity lower bound of
$\Omega(nr)$ for hypergraph sparsification.
</p>
<p>This result is obtained by introducing two new tools. First, we give a new
proof of spectral concentration bounds for sparsifiers of graphs; it avoids
linear-algebraic methods, replacing e.g.~the usual application of the matrix
Bernstein inequality and therefore applies to the (non-linear) hypergraph
setting. To achieve the result, we design a new sequence of
hypergraph-dependent $\epsilon$-nets on the unit sphere in $\mathbb{R}^n$.
Second, we extend the weight assignment technique of Chen, Khanna and Nagda
[FOCS'20] to the spectral sparsification setting. Surprisingly, the number of
spanning trees after the weight assignment can serve as a potential function
guiding the reweighting process in the spectral setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02353"><span class="datestr">at June 07, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02350">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02350">Parallel and External-Memory Construction of Minimal Perfect Hash Functions with PTHash</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pibiri:Giulio_Ermanno.html">Giulio Ermanno Pibiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trani:Roberto.html">Roberto Trani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02350">PDF</a><br /><b>Abstract: </b>A minimal perfect hash function $f$ for a set $S$ of $n$ keys is a bijective
function of the form $f : S \rightarrow \{0,\ldots,n-1\}$. These functions are
important for many practical applications in computing, such as search engines,
computer networks, and databases. Several algorithms have been proposed to
build minimal perfect hash functions that: scale well to large sets, retain
fast evaluation time, and take very little space, e.g., 2 - 3 bits/key. PTHash
is one such algorithm, achieving very fast evaluation in compressed space,
typically several times faster than other techniques. In this work, we propose
a new construction algorithm for PTHash enabling: (1) multi-threading, to
either build functions more quickly or more space-efficiently, and (2)
external-memory processing to scale to inputs much larger than the available
internal memory. Only few other algorithms in the literature share these
features, despite of their big practical impact. We conduct an extensive
experimental assessment on large real-world string collections and show that,
with respect to other techniques, PTHash is competitive in construction time
and space consumption, but retains 2 - 6$\times$ better lookup time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02350"><span class="datestr">at June 07, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02335">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02335">Covering Polygons is Even Harder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abrahamsen:Mikkel.html">Mikkel Abrahamsen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02335">PDF</a><br /><b>Abstract: </b>In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon
$\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex
polygons whose union is $\mathcal P$. It is known that MCC is
$\mathsf{NP}$-hard [Culberson &amp; Reckhow: Covering polygons is hard, FOCS
1988/Journal of Algorithms 1994] and in $\exists\mathbb{R}$ [O'Rourke: The
complexity of computing minimum convex covers for polygons, Allerton 1982]. We
prove that MCC is $\exists\mathbb{R}$-hard, and the problem is thus
$\exists\mathbb{R}$-complete. In other words, the problem is equivalent to
deciding whether a system of polynomial equations and inequalities with integer
coefficients has a real solution.
</p>
<p>If a cover for our constructed polygon exists, then so does a cover
consisting entirely of triangles. As a byproduct, we therefore also establish
that it is $\exists\mathbb{R}$-complete to decide whether $k$ triangles cover a
given polygon.
</p>
<p>The issue that it was not known if finding a minimum cover is in
$\mathsf{NP}$ has repeatedly been raised in the literature, and it was
mentioned as a "long-standing open question" already in 2001 [Eidenbenz &amp;
Widmayer: An approximation algorithm for minimum convex cover with logarithmic
performance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that
assuming the widespread belief that $\mathsf{NP}\neq\exists\mathbb{R}$, the
problem is not in $\mathsf{NP}$.
</p>
<p>An implication of the result is that many natural approaches to finding small
covers are bound to give suboptimal solutions in some cases, since irrational
coordinates of arbitrarily high algebraic degree can be needed for the corners
of the pieces in an optimal solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02335"><span class="datestr">at June 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02233">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02233">A Nearly Optimal All-Pairs Min-Cuts Algorithm in Simple Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02233">PDF</a><br /><b>Abstract: </b>We give an $n^{2+o(1)}$-time algorithm for finding $s$-$t$ min-cuts for all
pairs of vertices $s$ and $t$ in a simple, undirected graph on $n$ vertices. We
do so by constructing a Gomory-Hu tree (or cut equivalent tree) in the same
running time, thereby improving on the recent bound of $\tilde{O}(n^{2.5})$ by
Abboud et al. (FOCS 2021). Our running time is nearly optimal as a function of
$n$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02233"><span class="datestr">at June 07, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02212">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02212">Fuzzy Clustering with Similarity Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huleihel:Wasim.html">Wasim Huleihel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mazumdar:Arya.html">Arya Mazumdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pal:Soumyabrata.html">Soumyabrata Pal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02212">PDF</a><br /><b>Abstract: </b>The fuzzy or soft $k$-means objective is a popular generalization of the
well-known $k$-means problem, extending the clustering capability of the
$k$-means to datasets that are uncertain, vague, and otherwise hard to cluster.
In this paper, we propose a semi-supervised active clustering framework, where
the learner is allowed to interact with an oracle (domain expert), asking for
the similarity between a certain set of chosen items. We study the query and
computational complexities of clustering in this framework. We prove that
having a few of such similarity queries enables one to get a polynomial-time
approximation algorithm to an otherwise conjecturally NP-hard problem. In
particular, we provide probabilistic algorithms for fuzzy clustering in this
setting that asks $O(\mathsf{poly}(k)\log n)$ similarity queries and run with
polynomial-time-complexity, where $n$ is the number of items. The fuzzy
$k$-means objective is nonconvex, with $k$-means as a special case, and is
equivalent to some other generic nonconvex problem such as non-negative matrix
factorization. The ubiquitous Lloyd-type algorithms (or,
expectation-maximization algorithm) can get stuck at a local minima. Our
results show that by making few similarity queries, the problem becomes easier
to solve. Finally, we test our algorithms over real-world datasets, showing
their effectiveness in real-world applications.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02212"><span class="datestr">at June 07, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02149">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02149">Optimal Pricing Schemes for an Impatient Buyer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deng:Yuan.html">Yuan Deng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Jieming.html">Jieming Mao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sivan:Balasubramanian.html">Balasubramanian Sivan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Kangning.html">Kangning Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02149">PDF</a><br /><b>Abstract: </b>A patient seller aims to sell a good to an impatient buyer (i.e., one who
discounts utility over time). The buyer will remain in the market for a period
of time $T$, and her private value is drawn from a publicly known distribution.
What is the revenue-optimal pricing-curve (sequence of (price, time) pairs) for
the seller? Is randomization of help here? Is the revenue-optimal pricing-curve
computable in polynomial time? We answer these questions in this paper. We give
an efficient algorithm for computing the revenue-optimal pricing curve. We show
that pricing curves, that post a price at each point of time and let the buyer
pick her utility maximizing time to buy, are revenue-optimal among a much
broader class of sequential lottery mechanisms: namely, mechanisms that allow
the seller to post a menu of lotteries at each point of time cannot get any
higher revenue than pricing curves. We also show that the even broader class of
mechanisms that allow the menu of lotteries to be adaptively set, can earn
strictly higher revenue than that of pricing curves, and the revenue gap can be
as big as the support size of the buyer's value distribution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02149"><span class="datestr">at June 07, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02129">The Algorithmic Phase Transition of Random $k$-SAT for Low Degree Polynomials</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bresler:Guy.html">Guy Bresler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Brice.html">Brice Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02129">PDF</a><br /><b>Abstract: </b>Let $\Phi$ be a uniformly random $k$-SAT formula with $n$ variables and $m$
clauses. We study the algorithmic task of finding a satisfying assignment of
$\Phi$. It is known that a satisfying assignment exists with high probability
at clause density $m/n &lt; 2^k \log 2 - \frac12 (\log 2 + 1) + o_k(1)$, while the
best polynomial-time algorithm known, the Fix algorithm of Coja-Oghlan, finds a
satisfying assignment at the much lower clause density $(1 - o_k(1)) 2^k \log k
/ k$. This prompts the question: is it possible to efficiently find a
satisfying assignment at higher clause densities?
</p>
<p>To understand the algorithmic threshold of random $k$-SAT, we study low
degree polynomial algorithms, which are a powerful class of algorithms
including Fix, Survey Propagation guided decimation, and paradigms such as
message passing and local graph algorithms. We show that low degree polynomial
algorithms can find a satisfying assignment at clause density $(1 - o_k(1)) 2^k
\log k / k$, matching Fix, and not at clause density $(1 + o_k(1)) \kappa^* 2^k
\log k / k$, where $\kappa^* \approx 4.911$. This shows the first sharp (up to
constant factor) computational phase transition of random $k$-SAT for a class
of algorithms. Our proof establishes and leverages a new many-way overlap gap
property tailored to random $k$-SAT.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02129"><span class="datestr">at June 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02120">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02120">Approximation Algorithms for Min-Distance Problems in DAGs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalirrooyfard:Mina.html">Mina Dalirrooyfard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaufmann:Jenny.html">Jenny Kaufmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02120">PDF</a><br /><b>Abstract: </b>The min-distance between two nodes $u, v$ is defined as the minimum of the
distance from $v$ to $u$ or from $u$ to $v$, and is a natural distance metric
in DAGs. As with the standard distance problems, the Strong Exponential Time
Hypothesis [Impagliazzo-Paturi-Zane 2001, Calabro-Impagliazzo-Paturi 2009]
leaves little hope for computing min-distance problems faster than computing
All Pairs Shortest Paths, which can be solved in $\tilde{O}(mn)$ time. So it is
natural to resort to approximation algorithms in $\tilde{O}(mn^{1-\epsilon})$
time for some positive $\epsilon$.
</p>
<p>Abboud, Vassilevska W., and Wang [SODA 2016] first studied min-distance
problems achieving constant factor approximation algorithms on DAGs, obtaining
a $3$-approximation algorithm for min-radius on DAGs which works in
$\tilde{O}(m\sqrt{n})$ time, and showing that any $(2-\delta)$-approximation
requires $n^{2-o(1)}$ time for any $\delta&gt;0$, under the Hitting Set
Conjecture. We close the gap, obtaining a $2$-approximation algorithm which
runs in $\tilde{O}(m\sqrt{n})$ time. As the lower bound of Abboud et al only
works for sparse DAGs, we further show that our algorithm is conditionally
tight for dense DAGs using a reduction from Boolean matrix multiplication.
Moreover, Abboud et al obtained a linear time $2$-approximation algorithm for
min-diameter along with a lower bound stating that any
$(3/2-\delta)$-approximation algorithm for sparse DAGs requires $n^{2-o(1)}$
time under SETH. We close this gap for dense DAGs by obtaining a
$3/2$-approximation algorithm which works in $O(n^{2.350})$ time and showing
that the approximation factor is unlikely to be improved within $O(n^{\omega -
o(1)})$ time under the high dimensional Orthogonal Vectors Conjecture, where
$\omega$ is the matrix multiplication exponent.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02120"><span class="datestr">at June 07, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02114">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02114">Winning the War by (Strategically) Losing Battles: Settling the Complexity of Grundy-Values in Undirected Geography</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burke:Kyle.html">Kyle Burke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferland:Matthew.html">Matthew Ferland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teng:Shanghua.html">Shanghua Teng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02114">PDF</a><br /><b>Abstract: </b>We settle two long-standing complexity-theoretical questions-open since 1981
and 1993-in combinatorial game theory (CGT).
</p>
<p>We prove that the Grundy value (a.k.a. nim-value, or nimber) of Undirected
Geography is PSPACE-complete to compute. This exhibits a stark contrast with a
result from 1993 that Undirected Geography is polynomial-time solvable. By
distilling to a simple reduction, our proof further establishes a dichotomy
theorem, providing a "phase transition to intractability" in Grundy-value
computation, sharply characterized by a maximum degree of four: The Grundy
value of Undirected Geography over any degree-three graph is polynomial-time
computable, but over degree-four graphs-even when planar and bipartite-is
PSPACE-hard. Additionally, we show, for the first time, how to construct
Undirected Geography instances with Grundy value $\ast n$ and size polynomial
in n.
</p>
<p>We strengthen a result from 1981 showing that sums of tractable partisan
games are PSPACE-complete in two fundamental ways. First, since Undirected
Geography is an impartial ruleset, we extend the hardness of sums to impartial
games, a strict subset of partisan. Second, the 1981 construction is not built
from a natural ruleset, instead using a long sum of tailored short-depth game
positions. We use the sum of two Undirected Geography positions to create our
hard instances. Our result also has computational implications to
Sprague-Grundy Theory (1930s) which shows that the Grundy value of the
disjunctive sum of any two impartial games can be computed-in polynomial
time-from their Grundy values. In contrast, we prove that assuming PSPACE
$\neq$ P, there is no general polynomial-time method to summarize two
polynomial-time solvable impartial games to efficiently solve their disjunctive
sum.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02114"><span class="datestr">at June 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02113">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02113">Oblivious Stacking and MAX $k$-CUT for Circle Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olsen:Martin.html">Martin Olsen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02113">PDF</a><br /><b>Abstract: </b>Stacking is an important process within logistics. Some notable examples of
items to be stacked are steel bars or steel plates in a steel yard or
containers in a container terminal or on a ship. We say that two items are
conflicting if their storage time intervals overlap in which case one of the
items needs to be rehandled if the items are stored at the same LIFO storage
location. We consider the problem of stacking items using $k$ LIFO locations
with a minimum number of conflicts between items sharing a location. We present
an extremely simple online stacking algorithm that is oblivious to the storage
time intervals and storage locations of all other items when it picks a storage
location for an item. The risk of assigning the same storage location to two
conflicting items is proved to be of the order $1/k^2$ under mild assumptions
on the distribution of the storage time intervals for the items. Intuitively,
it seems natural to pick a storage location uniformly at random in the
oblivious setting implying a risk of $1/k$ so the risk for our algorithm is
surprisingly low. Our results can also be expressed within the context of the
MAX $k$-CUT problem for circle graphs. The results indicate that circle graphs
on average have relatively big $k$-cuts compared to the total number of edges.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02113"><span class="datestr">at June 07, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2106.02066">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2106.02066">Local Problems on Trees from the Perspectives of Distributed Algorithms, Finitary Factors, and Descriptive Combinatorics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brandt:Sebastian.html">Sebastian Brandt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chang:Yi=Jun.html">Yi-Jun Chang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Greb=iacute=k:Jan.html">Jan Grebík</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grunau:Christoph.html">Christoph Grunau</a>, Václav Rozhoň, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidny=aacute=nszky:Zolt=aacute=n.html">Zoltán Vidnyánszky</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2106.02066">PDF</a><br /><b>Abstract: </b>We study connections between distributed local algorithms, finitary factors
of iid processes, and descriptive combinatorics in the context of regular
trees.
</p>
<p>We extend the Borel determinacy technique of Marks coming from descriptive
combinatorics and adapt it to the area of distributed computing. Using this
technique, we prove deterministic distributed $\Omega(\log n)$-round lower
bounds for problems from a natural class of homomorphism problems.
Interestingly, these lower bounds seem beyond the current reach of the powerful
round elimination technique responsible for all substantial locality lower
bounds of the last years. Our key technical ingredient is a novel ID graph
technique that we expect to be of independent interest.
</p>
<p>We prove that a local problem admits a Baire measurable coloring if and only
if it admits a local algorithm with local complexity $O(\log n)$, extending the
classification of Baire measurable colorings of Bernshteyn. A key ingredient of
the proof is a new and simple characterization of local problems that can be
solved in $O(\log n)$ rounds. We complement this result by showing separations
between complexity classes from distributed computing, finitary factors, and
descriptive combinatorics. Most notably, the class of problems that allow a
distributed algorithm with sublogarithmic randomized local complexity is
incomparable with the class of problems with a Borel solution.
</p>
<p>We hope that our treatment will help to view all three perspectives as part
of a common theory of locality, in which we follow the insightful paper of
[Bernshteyn -- arXiv <a href="http://export.arxiv.org/abs/2004.04905">2004.04905</a>].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2106.02066"><span class="datestr">at June 07, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/077">TR21-077 |  Lower Bounds on Stabilizer Rank | 

	Shir Peleg, 

	Amir Shpilka, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The stabilizer rank of a quantum state $\psi$ is the minimal $r$ such that $\left| \psi \right \rangle = \sum_{j=1}^r c_j \left|\varphi_j \right\rangle$ for $c_j \in \mathbb{C}$ and stabilizer states $\varphi_j$. The running time of several classical simulation methods for quantum circuits is determined by the stabilizer rank of the $n$-th tensor power of single-qubit magic states.

We prove a lower bound of $\Omega(n)$ on the stabilizer rank of such states, improving a previous lower bound of $\Omega(\sqrt{n})$ of Bravyi, Smith and Smolin [BSS16]. Further, we prove that for a sufficiently small constant $\delta$, the stabilizer rank of any state which is $\delta$-close to those states is $\Omega(\sqrt{n}/\log n)$. This is the first non-trivial lower bound for approximate stabilizer rank.

Our techniques rely on the representation of stabilizer states as quadratic functions over affine subspaces of $\mathbb{F}_2^n$, and we use tools from analysis of boolean functions and complexity theory. The proof of the first result involves a careful analysis of directional derivatives of quadratic polynomials, whereas the proof of the second result uses Razborov-Smolensky low degree polynomial approximations and correlation bounds against the majority function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/077"><span class="datestr">at June 06, 2021 07:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/076">TR21-076 |  Pseudorandom Generators, Resolution and Heavy Width | 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Following the paper of Alekhnovich, Ben-Sasson, Razborov, Wigderson \cite{ABRW04} we call a pseudorandom generator $\mathrm{PRG}\colon \{0, 1\}^n \to \{0, 1\}^m$ hard for for a propositional proof system $\mathrm{P}$ if $\mathrm{P}$ cannot efficiently prove the (properly encoded) statement $b \notin \mathrm{Im}(\mathrm{PRG})$ for any string $b \in \{0, 1\}^m$.

In \cite{ABRW04} authors suggested the ``functional encoding'' of considered statement for Nisan--Wigderson generator that allows the introduction of ``local'' extension variables. These extension variables may potentially significantly increase the power of the proof system. In \cite{ABRW04} authors gave a lower bound $\exp\left[\frac{n^2}{m \Omega\left(2^{2^{\Delta}}\right)}\right]$ on the length of Resolution proofs where $\Delta$ is the degree of the dependency graph of the generator. This lower bound meets the barrier for the restriction technique.

In this paper, we introduce a ``heavy width'' measure for Resolution that allows showing a lower bound $\exp\left[\frac{n^2}{m 2^{O(\varepsilon \Delta)}}\right]$ on the length of Resolution proofs of the considered statement for the Nisan--Wigderson generator. This gives an exponential lower bound up to $\Delta := \log^{2 - \delta} n$ (the bigger degree the more extension variables we can use). It is a solution to an open problem from \cite{ABRW04}.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/076"><span class="datestr">at June 04, 2021 02:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/inference-is-not-a-privacy-violation/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/inference-is-not-a-privacy-violation/">Statistical Inference is Not a Privacy Violation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>On April 28, 2021, the US Census Bureau <a href="https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/2020-das-updates.html">released</a> a new demonstration of its differentially private Disclosure Avoidance System (DAS) for the 2020 US Census. The public were given a month to submit feedback before the system is finalized.
This demonstration data and the feedback has generated a lot of discussion, including media coverage on <a href="https://www.npr.org/2021/05/19/993247101/for-the-u-s-census-keeping-your-data-anonymous-and-useful-is-a-tricky-balance">National Public Radio</a>, in <a href="https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c94b46e-c30d-11eb-93f5-ee9558eecf4b_story.html">the Washington Post</a>, and via <a href="https://apnews.com/article/business-census-2020-technology-e701e313e841674be6396321343b7e49">the Associated Press</a>. The DAS is also the subject of an <a href="https://www.courtlistener.com/docket/59728874/state-v-united-states-department-of-commerce/">ongoing lawsuit</a>.</p>

<p>The following is a response from experts on differential privacy and cryptography to the <a href="https://alarm-redist.github.io/posts/2021-05-28-census-das/Harvard-DAS-Evaluation.pdf">working paper of Kenny et al.</a> on the impact of the 2020 U.S. Census Disclosure Avoidance System (DAS) on redistricting.</p>

<p>This paper makes a <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2016-06-14.md">common but serious mistake</a>, from which the authors wrongfully conclude the Census Bureau should not modernize its privacy-protection technology.  Not only do the results not support this conclusion, but they instead show the power of the methodology, known as differential privacy, adopted by the Bureau, precisely the opposite of the authors’ erroneous conclusions.</p>

<p>Trust is essential; once destroyed it can be nearly impossible to rebuild, and getting privacy wrong in this Census will have an impact on all future government surveys.  The Census Bureau has shown that their <a href="https://desfontain.es/privacy/index.html">2010 (DAS) does not survive modern privacy threats</a>, and in fact was roughly equivalent to publishing nearly three quarters of the responses.  The Census Bureau’s decision to modernize its Disclosure Avoidance System (DAS) for the 2020 Decennial Census to be differentially private is the correct response to decades of theoretical and empirical work on the privacy risks inherent in releasing large numbers of statistics derived from a dataset.</p>

<p>The importance of the Census, and the reality that no technology competing with differential privacy exists for meeting their confidentiality obligations, makes it very important that the public and policy makers have accurate information. We imagine you will be reporting on this topic in the future.  Others have <a href="https://gerrymander.princeton.edu/DAS-evaluation-Kenny-response">addressed flaws</a> in the paper regarding implications for redistricting; we want to provide you with an understanding of the privacy mistake in the study.</p>

<p>To understand the flaw in the paper’s argument, consider the role of smoking in determining cancer risk.  Statistical study of medical data has taught us that smoking causes cancer.   Armed with this knowledge, if we are told that 40 year old Mr. S is a smoker, we can conclude that he has an elevated cancer risk.  The statistical inference of elevated cancer risk—made before Mr. S was born—did not violate Mr. S’s privacy. To conclude otherwise is to define science to be a privacy attack.  This is the mistake made in the paper.</p>

<p>This is basically what Kenny et al. found.</p>

<p>The authors looked at three different predictors: one built directly from (swapped) 2010 Census data and the other two built using differential privacy applied to (swapped) 2010 Census data, and evaluated all three “on approximately 5.8 million registered voters included in the North Carolina February 2021 voter file.”  What did they find?</p>

<blockquote>
  <p>“Our analysis shows that across three main racial and ethnic groups, the predictions based on the [differential privacy based] DAS data appear to be as accurate as those based on the 2010 Census data.”</p>
</blockquote>

<p>This makes perfect sense. Bayesian Improved Surname Geocoding, or BISG, is a statistical method of building a predictor inferring ethnicity (or race) from name and geography.  Here, name and geography play the role of the information as to whether or not one smokes, and the prediction of ethnicity corresponds to the cancer risk prediction.  The predictor is constructed from census data on the ethnic makeup of individual census blocks and statistical information about the popularity of individual surnames within different ethnic groups.  With such a predictor, moving across the country can change the outcome, as can changing one’s name.  But a BISG prediction is not about the individual, it is about the statistical—population-level—relationship between name, geography, and ethnicity.</p>

<p>The differentially private DAS enabled learning to make statistical inferences about ethnicity from name and geography, without compromising the privacy of any Census respondent, exactly as it was intended to do.  In other words, the paper establishes fitness-for-use of the DAS data for the BISG statistical method!  Because differential privacy permits learning statistical patterns without compromising the privacy of individual members of the dataset, it should not interfere with learning the predictor, which is exactly what the authors found. Returning to our “smoking causes cancer” example, the researchers found that it was just as easy to detect this statistical pattern with a modern disclosure avoidance system in place as it was with the older, less protective system.</p>

<p>The authors’ conclusions –“ the DAS data may not provide universal privacy protection” – are simply not supported by their findings.</p>

<p>They have confused learning that smoking causes cancer—and applying this predictor to an individual smoker—with learning medical details of individual patients in the dataset. Change the input to the predictor—replace “smoker” with “non-smoker” or move across the country, for example—and the prediction changes.</p>

<p>The BISG prediction is not about the individual, it does not accompany her as she relocates from one neighborhood to another, it is a statistical relationship between name, geography, and ethnicity.  It is not a privacy compromise, it is science.</p>

<p>Signed:</p>
<ul>
  <li>Mark Bun, Assistant Professor of Computer Science, Boston University</li>
  <li>Damien Desfontaines, Privacy Engineer, Google</li>
  <li>Cynthia Dwork, Professor of Computer Science, Harvard University</li>
  <li>Moni Naor, Professor of Computer Science, The Weizmann Institute of Science</li>
  <li>Kobbi Nissim, Professor of Computer Science, Georgetown University</li>
  <li>Aaron Roth, Professor of Computer and Information Science, University of Pennsylvania</li>
  <li>Adam Smith, Professor of Computer Science, Boston University</li>
  <li>Thomas Steinke, Research Scientist, Google</li>
  <li>Jonathan Ullman, Assistant Professor of Computer Science, Northeastern University</li>
  <li>Salil Vadhan, Professor of Computer Science and Applied Mathematics, Harvard University</li>
</ul>

<p>Please contact Cynthia Dwork for contact information for authors happy to speak about this on the record.</p></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/inference-is-not-a-privacy-violation/"><span class="datestr">at June 03, 2021 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/075">TR21-075 |  Affine Extractors for Almost Logarithmic Entropy | 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	Jyun-Jie Liao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give an explicit construction of an affine extractor (over $\mathbb{F}_2$) that works for affine sources on $n$ bits with min-entropy $k \ge~  \log n \cdot (\log \log n)^{1 + o(1)}$. This improves prior work of Li (FOCS'16) that requires  min-entropy at least $\mathrm{poly}(\log n)$.
    
Our construction is based on the framework of using correlation breakers and resilient functions, a paradigm that was also used by Li. On a high level, the key sources of our improvement are based on the following new ingredients: (i) A new construction of  an affine somewhere random extractor, that we use in a crucial step instead of a linear seeded extractor (for which optimal constructions are not known) that was used by Li. (ii) A near optimal construction of a correlation breaker for linearly correlated sources. The construction of our correlation breaker takes inspiration from an exciting line of recent work that constructs two-source extractors for near logarithmic min-entropy.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/075"><span class="datestr">at June 03, 2021 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-583257573096655269">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html">What happened to self-driving cars?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In 2014, I wrote a blog post about a fake company <a href="https://blog.computationalcomplexity.org/2014/07/elfdrive.html">Elfdrive</a>.</p><blockquote style="border: none; margin: 0px 0px 0px 40px; padding: 0px; text-align: left;"><p>With a near record-setting investment announced last week, the self-driving car service Elfdrive is the hottest, most valuable technology start-up on the planet. It is also one of the most controversial.</p><p>The company, which has been the target of protests across Europe this week, has been accused of a reckless attitude toward safety, of price-gouging its customers, of putting existing cabbies out of work and of evading regulation. And it has been called trivial. In The New Yorker last year, George Packer huffed that Elfdrive typified Silicon Valley’s newfound focus on “solving all the problems of being 20 years old, with cash on hand.”</p><p>It is impossible to say whether Elfdrive is worth the $117 billion its investors believe it to be; like any start-up, it could fail. But for all its flaws, Elfdrive is anything but trivial. It could well transform transportation the way Amazon has altered shopping — by using slick, user-friendly software and mountains of data to completely reshape an existing market, ultimately making many modes of urban transportation cheaper, more flexible and more widely accessible to people across the income spectrum.</p></blockquote><p>It was a spoof on Uber but now it looks more like Tesla, expect that Tesla's market value is over half a trillion, about six times larger than General Motors.</p><p>The post was really about self-driving cars which I thought at the time would be commonplace by 2020. We are mostly there but there are issues of silent communication between drivers or between a driver and a pedestrian on who goes first that's hard to duplicate for a self-driving car. There is the paradox that if we make a car that will always stop if someone runs in front of it, then some people will run in front of it.</p><p>There is also the man-bites-dog problem. Any person killed by a self-driving car will be a major news item while the person killed by a human-driven car while you've been reading this post will never be reported.</p><p>We'll get to self-driving cars eventually, it just won't be all at once. We're already have basically self-driving cars on highways and in many other roads as well. As the technology improves and people see that it's safe at some point people will say, "So why do we even need the steering wheel anymore?"</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/what-happened-to-self-driving-cars.html"><span class="datestr">at June 03, 2021 08:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/074">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/074">TR21-074 |  Space characterizations of complexity measures and size-space trade-offs in propositional proof systems | 

	Theodoros Papamakarios, 

	Alexander Razborov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify two new big clusters of proof complexity measures equivalent up to
polynomial and $\log n$ factors. The first cluster contains, among others,
the logarithm of tree-like resolution size, regularized (that is, multiplied
by the logarithm of proof length) clause and monomial space, and clause
space, both ordinary and regularized, in regular and tree-like resolution. As
a consequence, separating clause or monomial space from the (logarithm of)
tree-like resolution size is the same as showing a strong trade-off between
clause or monomial space and proof length, and is the same as showing a
super-critical trade-off between clause space and depth. The second cluster
contains width, $\Sigma_2$ space (a generalization of clause
space to depth 2 Frege systems), both ordinary and regularized, as well as
the logarithm of tree-like size in the system $R(\log)$. As an application of some of
these simulations, we improve a known size-space trade-off for polynomial calculus with resolution. In
terms of lower bounds, we show a quadratic lower bound on tree-like
resolution size for formulas refutable in clause space $4$. We introduce on
our way yet another proof complexity measure intermediate between depth and
the logarithm of tree-like size that might be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/074"><span class="datestr">at June 03, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/073">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/073">TR21-073 |  Lower bounds for samplers and data structures via the cell-probe separator | 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose that a distribution $S$ can be approximately sampled by an
efficient cell-probe algorithm. It is shown to be possible to restrict
the input to the algorithm so that its output distribution is still
not too far from $S$, and at the same time many output coordinates
are almost pairwise independent.

Building on this several results are obtained, including:

- A lower bound for sampling prefix sums.

- A lower bound for sampling a variant of the predecessor problem.

- A separation between AC0 and cell-probe sampling.

- A separation between sampling with $O(q)$ and $q$ probes.

- A new proof of the Patrascu-Viola data-structure lower bound for
prefix sums, demonstrating the feasibility of obtaining data-structure
lower bounds via sampling.

- A separation between data structures making $O(q)$ and $q$ probes.

The only previous cell-probe lower bounds for sampling followed from
the AC0 lower bounds and applied to pseudorandom objects like error-correcting
and extractors, making them inadequate for the above applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/073"><span class="datestr">at June 03, 2021 02:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/">TCS+ talk: Wednesday, June 9 — Pravesh Kothari (CMU) and Ankur Moitra (MIT)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, June 9th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC).<strong> <a href="https://www.cs.cmu.edu/~praveshk/">Pravesh Kothari</a>  and <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a> </strong>from CMU and MIT will (jointly) speak about “<em>Robustly Learning Mixtures of Gaussians</em>” (abstract below).</p>
<p>Note that the seminar will be a bit longer than the usual: it’s a double feature!</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: For a while now the problem of robustly learning a high-dimensional mixture of Gaussians has had a target on its back. The first works in algorithmic robust statistics gave provably robust algorithms for learning a single Gaussian. Since then there has been steady progress, including algorithms for robustly learning mixtures of spherical Gaussians, mixtures of Gaussians under separation conditions, and arbitrary mixtures of two Gaussians. In this talk we will discuss two recent works that essentially resolve the general problem. There are important differences in their techniques, setup, and overall quantitative guarantees, which we will discuss.</p>
<p>The talk will cover the following independent works:</p>
<ul>
<li>Liu, Moitra, “Settling the Robust Learnability of Mixtures of Gaussians”</li>
<li>Bakshi, Diakonikolas, Jia, Kane, Kothari, Vempala, “Robustly Learning Mixtures of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="k" class="latex" /> Arbitrary Gaussians”</li>
</ul>
</blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/06/03/tcs-talk-wednesday-june-9-ankur-moitra-mit-and-pravesh-kothari-cmu/"><span class="datestr">at June 03, 2021 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-9052295508162981109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/06/frank-p-ramsey-on-research-and.html">Frank P. Ramsey on research and publication rates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Spurred by the excellent 1978 radio programme <a href="https://sms.cam.ac.uk/media/20145" target="_blank">'Better than the Stars'</a> by <a href="https://en.wikipedia.org/wiki/Hugh_Mellor" target="_blank">D. H. Mellor</a> about <a href="https://plato.stanford.edu/entries/ramsey/" target="_blank">Frank Ramsey</a> and by the <a href="https://traffic.libsyn.com/secure/philosophybites/Cheryl_Misak_on_Frank_Ramsey_and_Ludwig_Wittgenstein.mp3" target="_blank">Philosophy Bites interview with Cheryl Misak on Frank Ramsey and Ludwig Wittgenstein</a>, I started reading <a href="https://www.cherylmisak.com/" target="_blank">Cheryl Misak</a>'s <a href="https://global.oup.com/academic/product/frank-ramsey-9780198755357?cc=is&amp;lang=en&amp;" target="_blank">biography of Frank Ramsey</a>. (FWIW, I strongly recommend the radio programme, the podcast and the book.)<br /></p><p>The following quote from pages 169-170 of Cheryl Misak's book describes Ramsey's views on publications and research, as stated in a letter to his father Arthur:<br /></p><div style="margin-left: 40px; text-align: left;">Arthur tried a different tack, suggesting that Frank was going to be in trouble for wasting all this time on analysis, rather than on his career. On 24 September, in what seems to be his last letter home before he left Vienna, Frank wrote: </div><blockquote style="margin-left: 80px; text-align: left;">I don’t see how there can be any such inquisition into my conduct in Vienna as you suppose seem to want to guard against.... No one can suppose that you can’t research for six months without having a paper ready by the end. If everyone wrote a paper every six months the amount of trivial literature would swell beyond all bounds. Given time I shall produce a good paper. But if I hurry it will be ill written and unintelligible and unconvincing. <br /></blockquote><blockquote style="margin-left: 80px; text-align: left;">It seems to me perfectly proper to spend a scholarship being analysed, as it is likely to make me cleverer in the future, and discoveries of importance are made by remarkable people not by remarkable diligence. </blockquote><div style="margin-left: 40px; text-align: left;"> While it may not be persuasive that psychoanalysis makes one cleverer, Frank was prescient that the numbers of journal articles would eventually swell and he was right that diligence isn't enough to produce discoveries of importance.</div><div style="margin-left: 40px; text-align: left;"> </div><div style="text-align: left;">Of course, academia has changed since those times and I do value "remarkable diligence". However, I will try to remember Ramsey's words next time someone proposes to make academic hirings and promotions conditional to having a certain number of papers or citations or whatever per year on average. <br /></div></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/06/frank-p-ramsey-on-research-and.html"><span class="datestr">at June 01, 2021 09:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5536">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5536">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>Hooray, I’m <a href="https://www.acm.org/articles/people-of-acm/2021/scott-aaronson">today’s “Featured ACM Member”</a>!  Which basically means, yet another interview with me about quantum computing, with questions including what’s most surprised me about the development of QC, and what students should do to get into the field.</li><li>I’m proud to announce that <a href="https://arxiv.org/abs/2105.14697">An Automated Approach to the Collatz Conjecture</a>, a paper by Emre Yolcu, myself, and Marijn Heule that we started working on over four years ago, is finally available on the arXiv, and will be presented at the 2021 <a href="https://www.cs.cmu.edu/~mheule/CADE28/">Conference on Automated Deduction</a>.  Long story short: no, we didn’t prove <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz</a>, but we have an approach that can for the first time prove certain Collatz-like statements in a fully automated way, so hopefully that’s interesting!  There was also a <a href="https://www.quantamagazine.org/can-computers-solve-the-collatz-conjecture-20200826/"><em>Quanta</em> article</a> even before our paper had come out (I wasn’t thrilled about the timing).</li><li>The legendary <a href="https://en.wikipedia.org/wiki/Baba_Brinkman">Baba Brinkman</a> has a <a href="https://www.youtube.com/watch?v=kVcOx9Bg3a4">new rap about quantum computing</a> (hat tip to blog commenter YD).  Having just watched the music video, I see it as one of the better popularization efforts our field has seen in the past 25 years—more coherent than the average journalistic account and with a <em>much</em> better backbeat.  (I do, however, take a more guarded view than Brinkman of the potential applications, especially to e.g. autonomous driving and supply-chain optimization.)</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5536"><span class="datestr">at June 01, 2021 08:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3526">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2021/06/01/wine21-call-for-papers/">WINE’21 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>WINE 2021: The 17th Conference on Web and Internet Economics </strong></p>



<p><strong>December 14-17, 2021</strong><br /><strong>Hasso Plattner Institute, Potsdam, Germany</strong></p>



<p></p><blockquote class="wp-embedded-content"><a href="https://hpi.de/wine2021/">Overview</a></blockquote><p></p>



<p>Over the past two decades, researchers in theoretical computer science, artificial intelligence, operations research, and economics have joined forces to understand the interplay of incentives and computation. These issues are of particular importance in the Web and the Internet that enable the interaction of large and diverse populations. The Conference on Web and Internet Economics (WINE) is an interdisciplinary forum for the exchange of ideas and results on incentives and computation arising from these various fields. WINE 2021 continues the successful tradition of the Conference on Web and Internet Economics (named Workshop on Internet &amp; Network Economics until 2013), which was held annually from 2005 to present.</p>



<p>The program will feature invited talks, tutorials, paper presentations, and a poster session. All paper submissions will be peer-reviewed and evaluated on the basis of the quality of their contribution, originality, soundness, and significance. Submissions are invited in, but not limited to, the following topics:</p>



<ul><li>Algorithmic Game Theory</li><li>Algorithmic Mechanism Design</li><li>Auction Algorithms and Analysis</li><li>Computational Advertising</li><li>Computational Aspects of Equilibria</li><li>Computational Social Choice</li><li>Learning in Markets and Mechanism Design</li><li>Learning under Strategic Behavior</li><li>Coalitions, Coordination, and Collective Action</li><li>Economic Aspects of Security and Privacy</li><li>Economic Aspects of Distributed Computing and Cryptocurrencies</li><li>Econometrics, ML, and Data Science</li><li>Behavioral Economics and Behavioral Modeling</li><li>Fairness and Trust in Games and Markets</li><li>Price Differentiation and Price Dynamics</li><li>Revenue Management</li><li>Social Networks and Network Games</li></ul>



<p><strong>Authors of the accepted papers will have a choice to attend the conference virtually.</strong></p>



<p><strong>Important Dates</strong></p>



<p>Paper submission deadline: July 12, 2021, 11:59pm Pacific Time</p>



<p>Author notification: September (exact date TBA)</p>



<p><strong>Submission Format</strong></p>



<p>Authors are invited to submit extended abstracts presenting original research on any of the research fields related to WINE 2021.</p>



<p>An extended abstract submitted to WINE 2021 should start with the title of the paper, each author’s name, affiliation and e-mail address, followed by a one-paragraph summary of the results to be presented. This should then be followed by a technical exposition of the main ideas and techniques used to achieve these results, including motivation and a clear comparison with related work.</p>



<p>The extended abstract should not exceed 18 single-spaced pages (excluding references) using reasonable margins (at least one-inch margins all around) and at least 11-point font. If the authors believe that more details are essential to substantiate the claims of the paper, they may include a clearly marked appendix (with no space limit) that will be read at the discretion of the Program Committee. It is strongly recommended that submissions adhere to the specified format and length. Submissions that are clearly too long may be rejected immediately. The above specifications are meant to provide more freedom to the authors at the time of submission. Note that accepted papers will be allocated 14 pages (including references) in the LNCS format in the proceedings (see below).</p>



<p>The proceedings of the conference will be published by Springer-Verlag in the ARCoSS/LNCS series, and will be available for distribution at the conference. Accepted papers will be allocated 14 pages total in the LNCS format in the proceedings. Submissions are encouraged, though not required, to follow the LNCS format (Latex, Word). More information about the LNCS format can be found on the <a href="https://www.springer.com/cn/computer-science/lncs/conference-proceedings-guidelines" target="_blank" rel="noreferrer noopener">author instructions page of Springer-Verlag</a>. </p>



<p><strong>Best Paper Award</strong></p>



<p>The program committee will decide upon a best paper award and a best student paper award.</p>



<p><strong>Important Notice</strong></p>



<p>To accommodate the publishing traditions of different fields, authors of accepted papers can ask that only a one-page abstract of the paper appear in the proceedings, along with a URL pointing to the full paper. The authors should guarantee the link to be reliable for at least two years. This option is available to accommodate subsequent publication in journals that would not consider results that have been published in preliminary form in conference proceedings. Such papers must be submitted and formatted just like papers submitted for full-text publication.</p>



<p>Simultaneous submission of results to another conference with published proceedings is not allowed. Results previously published or presented at another archival conference prior to WINE 2021, or published (or accepted for publication) at a journal prior to the submission deadline of WINE 2021, will not be considered. Simultaneous submission of results to a journal is allowed only if the authors intend to publish the paper as a one-page abstract in WINE 2021. Papers that are accepted and appear as a one-page abstract can be subsequently submitted for publication in a journal but may not be submitted to any other conference that has a published proceeding.</p>



<p>Program PC co-chairs: Michal Feldman (chair), Hu Fu and Inbal Talgam-Cohen</p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2021/06/01/wine21-call-for-papers/"><span class="datestr">at June 01, 2021 09:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/forc-2021-is-coming-up/">FORC 2021 is coming up!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The Second annual Symposium on the Foundations of Responsible Computing (FORC) will be held virtually (on Gather.town) June 9-11, 2021.<a href="https://docs.google.com/forms/d/e/1FAIpQLSfELj0MvgcI83dmYThUPiynqWj8-G9Xve2dVf84EqKzgXXsHQ/viewform" target="_blank" rel="noreferrer noopener"> Registration is free, but required, by June 7</a>. Instructions for joining the event will be emailed to registered participants and posted online on June 8.</p>



<p>The program will feature 24 papers, six exciting panel discussions, three social hours featuring interactive board games, keynotes by Julie Owono (of the Facebook Oversight Board) and Kate Crawford (on her new book, the Atlas of AI), and a mentoring meetup.</p>



<p>The full program is here: <a href="https://responsiblecomputing.org/forc-2021-program/" target="_blank" rel="noreferrer noopener">https://responsiblecomputing.org/forc-2021-program/</a></p>



<p>The Symposium on Foundations of Responsible Computing (FORC) is a forum for mathematical research in computation and society writ large. The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>



<figure class="wp-block-image size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/white-logo-no-background.png?resize=800%2C858&amp;ssl=1" class="wp-image-1770" height="858" /></figure></div>







<p class="date">
by galoosh33 <a href="https://toc4fairness.org/forc-2021-is-coming-up/"><span class="datestr">at June 01, 2021 06:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/05/ememem-street-mosaics/">Ememem’s mosaic interventions in the potholes of Lyon</a> (<a href="https://mathstodon.xyz/@11011110/106251596474278206">\(\mathbb{M}\)</a>, <a href="https://www.ememem-flacking.net/flacking">see also</a>, <a href="https://boingboing.net/2021/05/17/this-artist-repairs-damaged-sidewalks-with-beautiful-mosaic-tiles.html">and also</a>). I find this kind of urban kintsugi pleasing, not just for its aspect of guerilla art, but for the sharp geometric patterns of the mosaics and the rough boundaries where they meet the landscape around them.</p>
  </li>
  <li>
    <p><a href="https://tilings.math.uni-bielefeld.de/">Tilings encyclopedia</a> (<a href="https://mathstodon.xyz/@11011110/106255097813445827">\(\mathbb{M}\)</a>). An online collection of many pretty substitution tilings. What’s missing to make it closer to OEIS in usefulness is a way to search by tiling rather than by keyword in the description of the tiling.</p>
  </li>
  <li>
    <p>At UC Irvine, we’ve been using Piazza for online course forums, but Piazza insists on serving ads to students (despite being paid), so we’re moving to <a href="https://edstem.org/us/">Ed Discussion</a> instead (<a href="https://mathstodon.xyz/@11011110/106258490049583719">\(\mathbb{M}\)</a>). You can see similar moves at <a href="https://rtl.berkeley.edu/news/piazza-and-new-online-discussion-platform-update">Berkeley</a> and <a href="https://news.psu.edu/story/641227/2020/12/07/academics/piazza-class-discussion-platform-start-displaying-advertisements">Penn State</a>. Usually I mistrust campus choices of software infrastructure — they tend to go for big crufty hard-to-use packages that are a poor fit for our needs — but this appears to be for good reason and I support it.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-answer-old-question-about-odd-graphs-20210519/"><em>Quanta</em> highlights one of my UCI colleagues, Asaf Ferber, for his work with Michael Krivelevich proving that every graph without isolated vertices has an induced subgraph of \(\Omega(n)\) vertices in which all vertices have odd degree</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/106265785749549559">\(\mathbb{M}\)</a>).</span> As usual, they also get important details wrong, forgetting to mention that the subgraphs must be induced, and also not mentioning the requirement of no isolated vertices. For the actual preprint see <a href="https://arxiv.org/abs/2009.05495">arXiv:2009.05495</a>.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2021/05/20/unc-chapel-hill-board-doesnt-approve-tenure-noted-journalist">Academic freedom dead at  U. of North Carolina</a> (<a href="https://mathstodon.xyz/@11011110/106269430445568583">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191525/Nikole-Hannah-Jones-Denied-Tenure-at-UNC">via</a>, <a href="https://www.nytimes.com/2021/05/19/business/media/nikole-hannah-jones-unc.html">see also</a>): right-wing board of trustees cancel-cultures MacArthur and Pulitzer winner Nikole Hannah-Jones for racist/political reasons (they don’t want anyone to talk about US slavery and ongoing systemic racism). To be clear, she still has a position at UNC; after the trustees denied her tenure, her department gave her a five-year-renewable spot not requiring trustee approval.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/">Some mathematical news</a> (<a href="https://mathstodon.xyz/@11011110/106277146476845581">\(\mathbb{M}\)</a>). Gil Kalai rounds up a lot of recent developments in knot recognition complexity, exotic spheres, numbers of fixed points, graph theory, and additive combinatorics.</p>
  </li>
  <li>
    <p><a href="https://www34.homepage.villanova.edu/robert.jantzen/notes/torus/cavatappo20/">Tilted cavatappo surfaces</a> (<a href="https://mathstodon.xyz/@11011110/106280088634102191">\(\mathbb{M}\)</a>). Robert Jantzen studies the shape of <a href="https://en.wikipedia.org/wiki/Cavatappi">corkscrew tube pasta</a> and of shortest paths on its surface. See also his preprints <a href="https://arxiv.org/abs/1301.0013">arXiv:1301.0013</a> and <a href="https://arxiv.org/abs/1402.3284">arXiv:1402.3284</a>.</p>
  </li>
  <li>
    <p>Here’s an unexpected property of hyperbolic geometry (<a href="https://mathstodon.xyz/@11011110/106288718744948273">\(\mathbb{M}\)</a>), or at least, it struck me as counterintuitive: all lines through two opposite quadrants of two perpendicular lines pass near their crossing, within distance \(\ln(1+\sqrt2)\). The figure below uses the upper halfplane model to show two quadrants (dark yellow), their convex hull (light yellow, from which the lines cannot escape), and a circle of radius \(\ln(1+\sqrt2)\) centered at the crossing, separating the quadrants.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/double-wedge-hull.svg" alt="Convex hull of the quadrants formed in the hyperbolic plane by two perpendicular lines, and a circle centered at the crossing separating the quadrants" /></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=HeQX2HjkcNo">Veritaserium on Cantor, Gödel, and Turing</a> (<a href="https://mathstodon.xyz/@gnivasch/106290995069247640">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>At a time when national borders have largely shut down, significantly decreasing the opportunities for higher education for people with the misfortune to be born in the wrong part of the world, <a href="https://www.latimes.com/california/story/2021-05-25/bold-plan-for-uc-admissions-reduce-out-of-state-students">California’s legislators are pushing to enact the same barriers to education across state lines</a> (<a href="https://mathstodon.xyz/@11011110/106297102605074297">\(\mathbb{M}\)</a>), preventing access to education and hurting the state itself by discouraging the best and brightest from coming here.</p>
  </li>
  <li>
    <p>Cellphone snapshot of the UC Irvine Ecological Preserve on a recent cloudy day (<a href="https://mathstodon.xyz/@11011110/106305347918888735">\(\mathbb{M}\)</a>). The foreground is mostly mustard, of no particular ecological significance; the ecological part is the coastal sage scrub on the sunny hillside in the background.</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="UCI Ecological Preserve" src="https://www.ics.uci.edu/~eppstein/pix/uciep2/DriedMustard-m.jpg" /></p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/BF02764015">Aperiodic tilings of the hyperbolic plane by convex polygons</a> (<a href="https://mathstodon.xyz/@11011110/106314334930093898">\(\mathbb{M}\)</a>), Margulis and Mozes, 1998. I knew about the <a href="https://en.wikipedia.org/wiki/Binary_tiling">binary tilings</a> but not this, which gets aperiodicity differently: if a tile’s area isn’t a rational multiple of \(\pi\), it cannot be periodic. This works even for certain hyperbolic rhombi, with angles chosen so they can tile. The tilings can still have 1d symmetry. If some single tile  avoids all symmetries, I don’t know about it.</p>
  </li>
  <li>
    <p><a href="http://www.geometrictomography.com/">Geometric tomography</a> (<a href="https://mathstodon.xyz/@11011110/106317414925334494">\(\mathbb{M}\)</a>). A nicely presented brief web survey of this subject, on the reconstruction of 3d shapes from 2d information (such as its brightness function, the areas of its perpendicular projections), by Richard J. Gardner based on his 1995 book of the same title.</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext">Collusion rings threaten the integrity of computer science research</a> (<a href="https://mathstodon.xyz/@11011110/106320077489874249">\(\mathbb{M}\)</a>), Michael L. Littman, CACM. Relatedly: <a href="https://www.chemistryworld.com/news/publishers-grapple-with-an-invisible-foe-as-huge-organised-fraud-hits-scientific-journals/4013652.article">“huge organised fraud” in scientific journals</a>, <a href="https://retractionwatch.com/2021/05/29/weekend-reads-gibberish-papers-persist-the-academic-who-faked-cherokee-heritage-organised-fraud-hits-scientific-journals/">via Retraction Watch</a>.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=8Sv_FaEN8zE">Five talks from CanaDAM 2021 introducing graph product structure theory and its applications</a> (<a href="https://mathstodon.xyz/@patmorin/106300452133561408">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://isohedral.ca/heesch-numbers-of-unmarked-polyforms/">Big progress in classifying polyforms by their Heesch numbers, obtained by using a SAT solver in place of an ad-hoc backtracking search for tilings</a> (<a href="https://mathstodon.xyz/@11011110/106331731989277653">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/191587/TikTok-teen-points-to-inside-elbow-bites-lip-Heeeeeeeesch">via</a>). See also <a href="https://twitter.com/mathpuzzle/status/1397040707706236928">Ed Pegg on an infinite set of polyforms with Heesch = 3</a>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/31/linkage.html"><span class="datestr">at May 31, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6576209065051505306">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html">What is a natural question? Who should decide?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>(Thanks to Timothy Chow for inspiring this post.)</p><p>My survey on Hilbert's Tenth Problem(see  <a href="https://arxiv.org/abs/2104.07220">here</a>) is about variants of the problem. One of them is as follows: </p><p>For which degrees d and number-of-vars n, is Hilbert's tenth problem decidable? undecidable? unknown? </p><p> I wondered why there was not a website with this information. More generally, the problem didn't seem to be getting much attention. (My survey does report on the attention it has gotten.) </p><p>I got several emails telling me it was the wrong question. I didn't quite know what they meant until Timothy Chow emailed me the following eloquent explanation:</p><p>-----------------------------------</p><p><i>One reason there isn't already a website of the type you envision is that from a number-theoretic (or decidability) point of view, parameterization by  degree and number of variables is not as natural as it might seem at first glance. The most fruitful lines of research have been geometric, and so geometric concepts such as smoothness, dimension, and genus are more natural than, say, degree. A nice survey by a number theorist is the book Rational Points on Varieties by Bjorn Poonen. Much of it is highly technical; however, reading the preface is very enlightening. Roughly speaking, the current state of the art is that there is really only one known way to prove that a system of Diophantine equations has no rational solution.</i></p><p>----------------------------------</p><p>AGAINST THE NUMBER THEORISTS VIEWPOINT:</p><p>1) ALICE: Why are you looking for your keys under the lamppost instead of where you dropped them?</p><p>   BOB: The light is better here.</p><p>2) I can imagine the following conversation:</p><p>BILL: I want to know about what happens with degree 3, and number of variables 3.</p><p>MATHPERSON: That's the wrong question you moron. The real question is what happens for fixed length of cohomology subchains.</p><p>BILL: Why is that more natural?</p><p>MATHPERSON: Because that is what we can solve. And besides, I've had 10 papers on it.</p><p><br /></p><p>FOR THE NUMBER THEORISTS VIEWPOINT</p><p>1) They are working on really hard problems so it is natural to gravitate towards those that can be solved.</p><p>2) I suspect that the math that comes out of studying classes of equations based on smoothness, dimension, genus is more interesting than what comes out of degree and number of vars. Or at least it has been so far. </p><p>META QUESTION</p><p>Who gets to decide what problems are natural?</p><p>People outside the field (me in this case) are asking the kind of questions that a layperson would ask and there is some merit to that.</p><p>People inside the field KNOW STUFF and hence their opinion of what's interesting to study has some merit. But they can also mistake `I cannot solve X' for `X is not interesting'</p><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/what-is-natural-question-who-should.html"><span class="datestr">at May 30, 2021 11:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21758">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/05/30/to-cheer-you-up-in-difficult-times-24-borodins-colouring-conjecture/">To Cheer You Up in Difficult times 24: Borodin’s colouring conjecture!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>An <a href="https://en.wikipedia.org/wiki/Acyclic_coloring">acyclic colouring</a> of a graph is a colouring of its vertices so that the subgraph spanned on union of every two colour classes is acyclic (a forest). Grunbaum conjectured in 1973 that </p>



<p class="has-text-align-center"><strong><span style="color: #a30042;" class="has-inline-color">Every planar graph has acyclic colouring with five colours.</span> </strong></p>



<p>This was proved by Borodin in 1976.</p>



<p>Borodin made the following stronger conjecture. Recall that a graph is <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" />-degenerate if every  subgraph has a vertex of degree at most <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="r" class="latex" />. </p>



<p class="has-text-align-center"><strong><span style="color: #a3001e;" class="has-inline-color">Every planar graph <em>G</em> can be coloured with five colours so that the union of every <em>k</em> colour classes, <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+4&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="1 \le k \le 4" class="latex" /> induces a <em>(k-1)</em>-degenerate graph. </span></strong></p>



<p>Let me mention an intermediate conjecture in-between Grunbaum’s conjecture and Borodin’s </p>



<p class="has-text-align-center"><strong><span style="color: #a30011;" class="has-inline-color">Every planar graph <em>G</em> can be coloured with five colours so that the union of every <em>k</em> colour classes, <img src="https://s0.wp.com/latex.php?latex=1+%5Cle+k+%5Cle+4&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="1 \le k \le 4" class="latex" /> induces a stress free graph for generic embedding into <img src="https://s0.wp.com/latex.php?latex=R%5E%7Bk-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="R^{k-1}" class="latex" />.</span></strong> <br /><br /></p>



<p>Grunbaum’s 1973 paper <a href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/BF02764716&amp;casa_token=vnWD-T_S73sAAAAA:mUenckyg9cyx5mrJI0hGdRqZ5WznXX-keyu4jYW61z-CkI1pslMA4w7SheVILEa1g1yHmjmFrP63EoqPODY">Acyclic<strong> </strong>colorings of planar graphs‏</a> is a very imaginative and highly cited paper. It was the first paper I ever refereed and I remember spending a lot of time reading it.  Here is the link to Borodin’s paper <a href="https://www.sciencedirect.com/science/article/pii/0012365X79900773">On acyclic colorings of planar graphs‏</a>. I am not sure what is the current world-record for the number of colours.</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/05/30/to-cheer-you-up-in-difficult-times-24-borodins-colouring-conjecture/"><span class="datestr">at May 30, 2021 08:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
