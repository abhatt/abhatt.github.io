<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 09, 2021 03:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/exponential-mechanism-bounded-range/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/exponential-mechanism-bounded-range/">The Exponential Mechanism and Bounded Range Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\),<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup> output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. Specifically, the exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{y’ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,x’)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(x’\) differing on the data of a single individual (which we denote by \(d(x,x’)\le 1\)).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! We’re going to look at more refined versions of differential privacy.</p>

<h2 id="bounded-range-privacy">Bounded Range Privacy</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em> privacy. This was observed and defined by Jinshuo Dong, David Durfee, and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range Privacy).</strong><sup id="fnref:1:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right) \in [t, t+\eta].\]</p>
</blockquote>

<p>To interpret this definition, we recall the definition of the privacy loss random variable: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \(f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right)\). Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-dfferential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) – i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range privacy is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,x’\).</p>

<p>Bounded range privacy and pure differential privacy are equivalent up to a factor of 2 in the parameters.</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range Privacy versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range privacy with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range privacy implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p>The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\) would contradict the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x’)=y] = 1\).</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (Exponential Mechanism is Bounded Range Private).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range privacy.<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x’))} \cdot \frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}.\]
Setting \(t = \log\left(\frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,x’)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,x’)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). <em>Q.E.D.</em></p>

<p>Bounded range privacy is nice on its own, but next we’re going to relate it to yet another version of differential privacy.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. Rényi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> is a relaxation of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is the privacy loss random variable.<sup id="fnref:4"><a href="https://differentialprivacy.org/feed.xml#fn:4" class="footnote" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a href="https://differentialprivacy.org/feed.xml#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact – that I want to draw more attention to – is that we can do better! 
Specifically, \(\eta\)-bounded range privacy implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range Privacy implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range private, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,x’ \in \mathcal{X}^n\) differing on a single individual’s data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range privacy (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffding’s Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffding’s Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a href="https://differentialprivacy.org/feed.xml#fn:6" class="footnote" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac12 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] = \sum_y \mathbb{P}[M(x)=y] \exp(-f(y)) = \sum_y \mathbb{P}[M(x)=y]  \cdot \frac{\mathbb{P}[M(x’)=y]}{\mathbb{P}[M(x)=y]} = 1.\]</p>

<p><em>Q.E.D.</em></p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets. <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a> <a href="https://differentialprivacy.org/feed.xml#fnref:1:1" class="reversefootnote">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta.\) (To be completely precise, we must appropriately deal with the possibility that \(Z=\infty\), which we ignore in this discussion for simplicity.) <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},x’) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},x’) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\). <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(x’)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(x’)))\) is the order \(\lambda+1\) Rényi divergence of \(M(x)\) from \(M(x’)\). <a href="https://differentialprivacy.org/feed.xml#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,x’\in\mathcal{X}^n : d(x,x’)\le1} \|q(x)-q(x’)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\) <a href="https://differentialprivacy.org/feed.xml#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(x’) )\). <a href="https://differentialprivacy.org/feed.xml#fnref:6" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Thomas Steinke <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><span class="datestr">at July 20, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/096">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/096">TR21-096 |  Keep That Card in Mind: Card Guessing with Limited Memory | 

	Boaz Menuhin, 

	Moni Naor</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A card guessing game is played between two players, Guesser and Dealer. At the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1, ..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser guesses which card was drawn, and then the card is discarded from the deck. The Guesser receives a point for each correctly guessed card. 

With perfect memory, a Guesser can keep track of all cards that were played so far and pick at random a card that has not appeared so far, yielding in expectation $\ln n$ correct guesses. With no memory, the best a Guesser can do will result in a single guess in expectation. 

We consider the case of a memory bounded Guesser that has $m &lt; n$ memory bits. We show that the performance of such a memory bounded Guesser depends much on the behavior of the Dealer. In more detail, we show that there is a gap between the static case, where the Dealer draws cards from a properly shuffled deck or a prearranged one, and the adaptive case, where the Dealer draws cards thoughtfully, in an adversarial manner. Specifically: 

1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near optimal result against any static Dealer. 

2. We show that no Guesser with $m$ bits of memory can score better than $O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min \{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal. 

3. We show an efficient adaptive Dealer against which no Guesser with $m$ memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in expectation. 

These results are (almost) tight, and we prove them using compression arguments that harness the guessing strategy for encoding.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/096"><span class="datestr">at July 08, 2021 09:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/07/08/imp-reg-tf/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/07/08/imp-reg-tf/">Implicit Regularization in Tensor Factorization&amp;#58; Can Tensor Rank Shed Light on Generalization in Deep Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In effort to understand implicit regularization in deep learning, a lot of theoretical focus is being directed at matrix factorization, which can be seen as linear neural networks.
This post is based on our <a href="https://arxiv.org/pdf/2102.09972.pdf">recent paper</a> (to appear at ICML 2021), where we take a step towards practical deep learning, by investigating <em>tensor factorization</em> — a model equivalent to a certain type of non-linear neural networks.
It is well known that <a href="https://arxiv.org/pdf/0911.1393.pdf">most tensor problems are NP-hard</a>, and accordingly, the common sentiment is that working with tensors (in both theory and practice) entails extreme difficulties.
However, by adopting a dynamical systems view, we manage to avoid such difficulties, and establish an implicit regularization towards low <em>tensor rank</em>.
Our results suggest that tensor rank may shed light on generalization in deep learning.</p>

<h2 id="challenge-finding-a-right-measure-of-complexity">Challenge: finding a right measure of complexity</h2>

<p>Overparameterized neural networks are mysteriously able to generalize even when trained without any explicit regularization.
Per conventional wisdom, this generalization stems from an <em>implicit regularization</em> — a tendency of gradient-based optimization to fit training examples with predictors of minimal ‘‘complexity.’’
A major challenge in translating this intuition to provable guarantees is that we lack measures for predictor complexity that are quantitative (admit generalization bounds), and at the same time, capture the essence of natural data (images, audio, text etc.), in the sense that it can be fit with predictors of low complexity.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/imp_reg_tf_data_complexity.png" style="width: 500px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 1:</b> 
To explain generalization in deep learning, a complexity <br />
measure must allow the fit of natural data with low complexity. On the <br />
other hand, when fitting data which does not admit generalization, <br />
e.g. random data, the complexity should be high. 
</i>
</div>
<p><br /></p>

<h2 id="a-common-testbed-matrix-factorization">A common testbed: matrix factorization</h2>

<p>Without a clear complexity measure for practical neural networks, existing analyses usually focus on simple settings where a notion of complexity is obvious. 
A common example of such a setting is <em>matrix factorization</em> — matrix completion via linear neural networks. 
This model was discussed pretty extensively in previous posts (see <a href="http://www.offconvex.org/2019/06/03/trajectories/">one</a> by Sanjeev, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">one</a> by Nadav and Wei and <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">another one</a> by Nadav), but for completeness we present it again here.</p>

<p>In <em>matrix completion</em> we’re given a subset of entries from an unknown matrix $W^* \in \mathbb{R}^{d, d’}$, and our goal is to predict the unobserved entries. 
This can be viewed as a supervised learning problem with $2$-dimensional inputs, where the label of the input $( i , j )$ is $( W^* )_{i,j}$.
Under such a viewpoint, the observed entries are the training set, and the average reconstruction error over unobserved entries is the test error, quantifying generalization.
A predictor can then be thought of as a matrix, and a natural notion of complexity is its <em>rank</em>.
Indeed, in many real-world scenarios (a famous example is the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>) one is interested in <a href="https://arxiv.org/pdf/1601.06422.pdf">recovering a low rank matrix from incomplete observations</a>.</p>

<p>A ‘‘deep learning approach’’ to matrix completion is matrix factorization, where the idea is to use a linear neural network (fully connected neural network with no non-linearity), and fit observations via gradient descent (GD). 
This amounts to optimizing the following objective:</p>

<div style="text-align: center;">
\[
 \min\nolimits_{W_1 , \ldots , W_L} ~ \sum\nolimits_{(i,j) \in observations} \big[ ( W_L \cdots W_1 )_{i , j} - (W^*)_{i,j} \big]^2 ~.
\]
</div>

<p>It is obviously possible to constrain the rank of the produced solution by limiting the shared dimensions of the weight matrices $\{ W_j \}_j$. 
However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained and the factorization can express any matrix. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>As it turns out, in practice, matrix factorization with near-zero initialization and small step size tends to accurately recover low rank matrices.
This phenomenon (first identified in <a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>) manifests some kind of implicit regularization, whose mathematical characterization drew a lot of interest.
It was initially conjectured that matrix factorization implicitly minimizes nuclear norm (<a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>), but recent evidence points to implicit rank minimization, stemming from incremental learning dynamics (see <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a>; <a href="https://papers.nips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf">Razin &amp; Cohen 2020</a>; <a href="https://openreview.net/pdf/e29b53584bc9017cb15b9394735cd51b56c32446.pdf">Li et al. 2021</a>). 
Today, it seems we have a relatively firm understanding of generalization in matrix factorization.
There is a complexity measure for predictors — matrix rank — by which implicit regularization strives to lower complexity, and the data itself is of low complexity (i.e. can be fit with low complexity). 
Jointly, these two conditions lead to generalization.</p>

<h2 id="beyond-matrix-factorization-tensor-factorization">Beyond matrix factorization: tensor factorization</h2>

<p>Matrix factorization is interesting on its own behalf, but as a theoretical surrogate for deep learning it is limited.
First, it corresponds to <em>linear</em> neural networks, and thus misses the crucial aspect of non-linearity. 
Second, viewing matrix completion as a prediction problem, it doesn’t capture tasks with more than two input variables.
As we now discuss, both of these limitations can be lifted if instead of matrices one considers tensors.</p>

<p>A tensor can be thought of as a multi-dimensional array.
The number of axes in a tensor is called its <em>order</em>.
In the task of <em>tensor completion</em>, a subset of entries from an unknown tensor $\mathcal{W}^* \in \mathbb{R}^{d_1, \ldots, d_N}$ are given, and the goal is to predict the unobserved entries. 
Analogously to how matrix completion can be viewed as a prediction problem over two input variables, order-$N$ tensor completion can be seen as a prediction problem over $N$ input variables (each corresponding to a different axis).
In fact, any multi-dimensional prediction task with discrete inputs and scalar output can be formulated as a tensor completion problem.
Consider for example the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, and for simplicity assume that image pixels hold one of two values, i.e. are either black or white. 
The task of predicting labels for the $28$-by-$28$ binary images can be seen as an order-$784$ (one axis for each pixel) tensor completion problem, where all axes are of length $2$ (corresponding to the number of values a pixel can take). 
For further details on how general prediction tasks map to tensor completion problems see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/pred_prob_to_tensor_comp.png" style="width: 550px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 2:</b> 
Prediction tasks can be viewed as tensor completion problems. <br />
For example, predicting labels for input images with $3$ pixels, each taking <br />
one of $5$ grayscale values, corresponds to completing a $5 \times 5 \times 5$ tensor.
</i>
</div>
<p><br />
Like matrices, tensors can be factorized. 
The most basic scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for information on this scheme, as well as others, see the <a href="http://www.kolda.net/publication/TensorReview.pdf">excellent survey</a> of Kolda and Bader).
In <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> and this post, we use the term <em>tensor factorization</em> to refer to solving tensor completion by fitting observations via GD over CP parameterization, i.e. over the following objective ($\otimes$ here stands for outer product):</p>

<div style="text-align: center;">
\[
\min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \sum\nolimits_{ (i_1 , ... , i_N) \in observations } \big[ \big(  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big)_{i_1 , \ldots , i_N} - (\mathcal{W}^*)_{i_1 , \ldots , i_N} \big]^2 ~.
\]
</div>

<p>The concept of rank naturally extends from matrices to tensors.
The <em>tensor rank</em> of a given tensor $\mathcal{W}$ is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that for order-$2$ tensors, i.e. for matrices, this exactly coincides with matrix rank.
We can explicitly constrain the tensor rank of solutions found by tensor factorization via limiting the number of components $R$. 
However, since our interest lies on implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>By now you might be wondering what does tensor factorization have to do with deep learning.
Apparently, as Nadav mentioned in an <a href="http://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>, analogously to how matrix factorization is equivalent to matrix completion (two-dimensional prediction) via linear neural networks, tensor factorization is equivalent to tensor completion (multi-dimensional prediction) with a certain type of <em>non-linear</em> neural networks (for the exact details behind the latter equivalence see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>). 
It therefore represents a setting one step closer to practical neural networks.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/mf_lnn_tf_nonlinear.png" style="width: 900px; padding-bottom: 10px; padding-top: 5px;" />
<br />
<i><b>Figure 3:</b> 
While matrix factorization corresponds to a linear neural network, <br />
tensor factorization corresponds to a certain non-linear neural network.
</i>
</div>
<p><br />
As a final piece of the analogy between matrix and tensor factorizations, in a <a href="https://arxiv.org/pdf/2005.06398.pdf">previous paper</a> (described in an <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>) Noam and Nadav demonstrated empirically that (similarly to the phenomenon discussed above for matrices) tensor factorization with near-zero initialization and small step size tends to accurately recover low rank tensors.
Our goal in the <a href="https://arxiv.org/pdf/2102.09972.pdf">current paper</a> was to mathematically explain this finding. 
To avoid the <a href="https://arxiv.org/pdf/0911.1393.pdf">notorious difficulty of tensor problems</a>, we chose to adopt a dynamical systems view, and analyze directly the trajectories induced by GD.</p>

<h2 id="dynamical-analysis-implicit-tensor-rank-minimization">Dynamical analysis: implicit tensor rank minimization</h2>

<p>So what can we say about the implicit regularization in tensor factorization? 
At the core of our analysis is the following dynamical characterization of component norms:</p>

<blockquote>
  <p><strong>Theorem:</strong>
Running gradient flow (GD with infinitesimal step size) over a tensor factorization with near-zero initialization leads component norms to evolve by:
[ \frac{d}{dt} || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) || \propto \color{brown}{|| \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||^{2 - 2/N}} ~,
]
where $\mathbf{w}_r^1 (t), \ldots, \mathbf{w}_r^N (t)$ denote the weight vectors at time $t \geq 0$.</p>
</blockquote>

<p>According to the theorem above, component norms evolve at a rate proportional to their size exponentiated by $\color{brown}{2 - 2 / N}$ (recall that $N$ is the order of the tensor to complete).
Consequently, they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
This suggests that when initialized near zero, components tend to remain close to the origin, and then, after passing a critical threshold, quickly grow until convergence. 
Intuitively, these dynamics induce an incremental process where components are learned one after the other, leading to solutions with a few large components and many small ones, i.e. to (approximately) low tensor rank solutions!</p>

<p>We empirically verified the incremental learning of components in many settings. 
Here is a representative example from one of our experiments (see <a href="https://arxiv.org/pdf/2102.09972.pdf">the paper</a> for more):</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/tf_dyn_exps.png" style="width: 800px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 4:</b> 
Dynamics of component norms during GD over tensor factorization. <br />
An incremental learning effect is enhanced as initialization scale decreases, <br />
leading to accurate completion of a low rank tensor.
</i>
</div>
<p><br />
Using our dynamical characterization of component norms, we were able to prove that with sufficiently small initialization, tensor factorization (approximately) follows a trajectory of rank one tensors for an arbitrary amount of time. 
This leads to:</p>

<blockquote>
  <p><strong>Theorem:</strong>
If tensor completion has a rank one solution, then under certain technical conditions, tensor factorization will reach it.</p>
</blockquote>

<p>It’s worth mentioning that, in a way, our results extend to tensor factorization the incremental rank learning dynamics known for matrix factorization (cf. <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a> and <a href="https://arxiv.org/pdf/2012.09839v1.pdf">Li et al. 2021</a>). 
As typical when transitioning from matrices to tensors, this extension entailed various challenges that necessitated use of different techniques.</p>

<h2 id="tensor-rank-as-measure-of-complexity">Tensor rank as measure of complexity</h2>

<p>Going back to the beginning of the post, recall that a major challenge towards understanding implicit regularization in deep learning is that we lack measures for predictor complexity that capture natural data. 
Now, let us recap what we have seen thus far:
$(1)$ tensor completion is equivalent to multi-dimensional prediction; 
$(2)$ tensor factorization corresponds to solving the prediction task with certain non-linear neural networks; 
and 
$(3)$ the implicit regularization of these non-linear networks, i.e. of tensor factorization, minimizes tensor rank.
Motivated by these findings, we ask the following:</p>

<blockquote>
  <p><strong>Question:</strong> 
Can tensor rank serve as a measure of predictor complexity?</p>
</blockquote>

<p>We empirically explored this prospect by evaluating the extent to which tensor rank captures natural data, i.e. to which natural data can be fit with predictors of low tensor rank.
As testbeds we used <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> and <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> datasets, comparing the resulting errors against those obtained when fitting two randomized variants: one generated via shuffling labels (‘‘rand label’’), and the other by replacing inputs with noise (‘‘rand image’’).</p>

<p>The following plot, displaying results for Fashion-MNIST (those for MNIST are similar), shows that with predictors of low tensor rank the original data is fit way more accurately than the randomized datasets. 
Specifically, even with tensor rank as low as one the original data is fit relatively well, while the error in fitting random data is close to trivial (variance of the label). 
This suggests that tensor rank as a measure of predictor complexity has potential to capture aspects of natural data! 
Note also that an accurate fit with low tensor rank coincides with low test error, which is not surprising given that low tensor rank predictors can be described with a small number of parameters.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/exp_complexity_fmnist.png" style="width: 600px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 5:</b> 
Evaluation of tensor rank as a measure of complexity — standard datasets <br />
can be fit accurately with predictors of low tensor rank (far beneath what is required by <br />
random datasets), suggesting it may capture aspects of natural data. Plot shows mean <br />
error of predictors with low tensor rank over Fashion-MNIST. Markers correspond <br />
to separate runs differing in the explicit constraint on the tensor rank.
</i>
</div>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>Overall, <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> shows that tensor rank captures both the implicit regularization of a certain type of non-linear neural networks, and aspects of natural data. 
In light of this, we believe tensor rank (or more advanced notions such as hierarchical tensor rank) might pave way to explaining both implicit regularization in more practical neural networks, and the properties of real-world data translating this implicit regularization to generalization.</p>

<p><a href="https://noamrazin.github.io/">Noam Razin</a>, <a href="https://asafmaman101.github.io/">Asaf Maman</a>, <a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/07/08/imp-reg-tf/"><span class="datestr">at July 08, 2021 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03341">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03341">Burrows Wheeler Transform on a Large Scale: Algorithms Implemented in Apache Spark</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ylenia Galluzzo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giancarlo:Raffaele.html">Raffaele Giancarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Randazzo:Mario.html">Mario Randazzo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rombo:Simona_E=.html">Simona E. Rombo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03341">PDF</a><br /><b>Abstract: </b>With the rapid growth of Next Generation Sequencing (NGS) technologies, large
amounts of "omics" data are daily collected and need to be processed. Indexing
and compressing large sequences datasets are some of the most important tasks
in this context. Here we propose algorithms for the computation of Burrows
Wheeler transform relying on Big Data technologies, i.e., Apache Spark and
Hadoop. Our algorithms are the first ones that distribute the index computation
and not only the input dataset, allowing to fully benefit of the available
cloud resources.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03341"><span class="datestr">at July 08, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03290">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03290">Defeating duplicates: A re-design of the LearnedSort algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kristo:Ani.html">Ani Kristo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaidya:Kapil.html">Kapil Vaidya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kraska:Tim.html">Tim Kraska</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03290">PDF</a><br /><b>Abstract: </b>LearnedSort is a novel sorting algorithm that, unlike traditional methods,
uses fast ML models to boost the sorting speed. The models learn to estimate
the input's distribution and arrange the keys in sorted order by predicting
their empirical cumulative distribution function (eCDF) values. LearnedSort has
shown outstanding performance compared to state-of-the-art sorting algorithms
on several datasets, both synthetic and real. However, given the nature of the
eCDF model, its performance is affected in the cases when the input data
contains a large number of repeated keys (i.e., duplicates). This work analyzes
this scenario in depth and introduces LearnedSort 2.0: a re-design of the
algorithm that addresses this issue and enables the algorithm to maintain the
leading edge even for high-duplicate inputs. Our extensive benchmarks on a
large set of diverse datasets demonstrate that the new design performs at much
higher sorting rates than the original version: an average of 4.78x improvement
for high-duplicate datasets, and 1.60x for low-duplicate datasets while taking
the lead among sorting algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03290"><span class="datestr">at July 08, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03193">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03193">Oblivious Median Slope Selection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Thore Thießen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vahrenhold:Jan.html">Jan Vahrenhold</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03193">PDF</a><br /><b>Abstract: </b>We study the median slope selection problem in the oblivious RAM model. In
this model memory accesses have to be independent of the data processed, i.e.,
an adversary cannot use observed access patterns to derive additional
information about the input. We show how to modify the randomized algorithm of
Matou\v{s}ek (1991) to obtain an oblivious version with O(n log^2 n) expected
time for n points in R^2. This complexity matches a theoretical upper bound
that can be obtained through general oblivious transformation. In addition,
results from a proof-of-concept implementation show that our algorithm is also
practically efficient.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03193"><span class="datestr">at July 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03171">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03171">On the Probabilistic Degree of an $n$-variate Boolean Function</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Srikanth Srinivasan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venkitesh:S=.html">S. Venkitesh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03171">PDF</a><br /><b>Abstract: </b>Nisan and Szegedy (CC 1994) showed that any Boolean function
$f:\{0,1\}^n\rightarrow \{0,1\}$ that depends on all its input variables, when
represented as a real-valued multivariate polynomial $P(x_1,\ldots,x_n)$, has
degree at least $\log n - O(\log \log n)$. This was improved to a tight $(\log
n - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar
statements are also known for other Boolean function complexity measures such
as Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate
degree (Ambainis and de Wolf (CC 2014)).
</p>
<p>In this paper, we address this question for \emph{Probabilistic degree}. The
function $f$ has probabilistic degree at most $d$ if there is a random
real-valued polynomial of degree at most $d$ that agrees with $f$ at each input
with high probability. Our understanding of this complexity measure is
significantly weaker than those above: for instance, we do not even know the
probabilistic degree of the OR function, the best-known bounds put it between
$(\log n)^{1/2-o(1)}$ and $O(\log n)$ (Beigel, Reingold, Spielman (STOC 1991);
Tarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).
</p>
<p>Here we can give a near-optimal understanding of the probabilistic degree of
$n$-variate functions $f$, \emph{modulo} our lack of understanding of the
probabilistic degree of OR. We show that if the probabilistic degree of OR is
$(\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is
at least $(\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\log
n)^{o(1)}$ factors.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03171"><span class="datestr">at July 08, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03153">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03153">Reshaping Convex Polyhedra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Joseph O'Rourke, Costin Vilcu <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03153">PDF</a><br /><b>Abstract: </b>Given a convex polyhedral surface P, we define a tailoring as excising from P
a simple polygonal domain that contains one vertex v, and whose boundary can be
sutured closed to a new convex polyhedron via Alexandrov's Gluing Theorem. In
particular, a digon-tailoring cuts off from P a digon containing v, a subset of
P bounded by two equal-length geodesic segments that share endpoints, and can
then zip closed.
</p>
<p>In the first part of this monograph, we primarily study properties of the
tailoring operation on convex polyhedra. We show that P can be reshaped to any
polyhedral convex surface Q a subset of conv(P) by a sequence of tailorings.
This investigation uncovered previously unexplored topics, including a notion
of unfolding of Q onto P--cutting up Q into pieces pasted non-overlapping onto
P.
</p>
<p>In the second part of this monograph, we study vertex-merging processes on
convex polyhedra (each vertex-merge being in a sense the reverse of a
digon-tailoring), creating embeddings of P into enlarged surfaces. We aim to
produce non-overlapping polyhedral and planar unfoldings, which led us to
develop an apparently new theory of convex sets, and of minimal length
enclosing polygons, on convex polyhedra.
</p>
<p>All our theorem proofs are constructive, implying polynomial-time algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03153"><span class="datestr">at July 08, 2021 11:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03140">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03140">Minimum Constraint Removal Problem for Line Segments is NP-hard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bigham:Bahram_Sadeghi.html">Bahram Sadeghi Bigham</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03140">PDF</a><br /><b>Abstract: </b>In the minimum constraint removal ($MCR$), there is no feasible path to move
from the starting point towards the goal and, the minimum constraints should be
removed in order to find a collision-free path. It has been proved that $MCR$
problem is $NP-hard$ when constraints have arbitrary shapes or even they are in
shape of convex polygons. However, it has a simple linear solution when
constraints are lines and the problem is open for other cases yet. In this
paper, using a reduction from Subset Sum problem, in three steps, we show that
the problem is NP-hard for both weighted and unweighted line segments.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03140"><span class="datestr">at July 08, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03133">A Heuristic for Direct Product Graph Decomposition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Calderoni:Luca.html">Luca Calderoni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Margara:Luciano.html">Luciano Margara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marzolla:Moreno.html">Moreno Marzolla</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03133">PDF</a><br /><b>Abstract: </b>In this paper we describe a heuristic for decomposing a directed graph into
factors according to the direct product (also known as Kronecker, cardinal or
tensor product). Given a directed, unweighted graph~$G$ with adjacency matrix
Adj($G$), our heuristic searches for a pair of graphs~$G_1$ and~$G_2$ such that
$G = G_1 \otimes G_2$, where $G_1 \otimes G_2$ is the direct product of~$G_1$
and~$G_2$. For undirected, connected graphs it has been shown that graph
decomposition is "at least as difficult" as graph isomorphism; therefore,
polynomial-time algorithms for decomposing a general directed graph into
factors are unlikely to exist. Although graph factorization is a problem that
has been extensively investigated, the heuristic proposed in this paper
represents -- to the best of our knowledge -- the first computational approach
for general directed, unweighted graphs. We have implemented our algorithm
using the MATLAB environment; we report on a set of experiments that show that
the proposed heuristic solves reasonably-sized instances in a few seconds on
general-purpose hardware.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03133"><span class="datestr">at July 08, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03123">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03123">Refined Computational Complexities of Hospitals/Residents Problem with Regional Caps</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hamada:Koki.html">Koki Hamada</a>, Shuichi Miyazaki NTT Corporation, Graduate School of Informatics, Kyoto University, Academic Center for Computing and Media Studies, Kyoto University) <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03123">PDF</a><br /><b>Abstract: </b>The Hospitals/Residents problem (HR) is a many-to-one matching problem whose
solution concept is stability. It is widely used in assignment systems such as
assigning medical students (residents) to hospitals. To resolve imbalance in
the number of residents assigned to hospitals, an extension called HR with
regional caps (HRRC) was introduced. In this problem, a positive integer
(called a regional cap) is associated with a subset of hospitals (called a
region), and the total number of residents assigned to hospitals in a region
must be at most its regional cap. Kamada and Kojima defined strong stability
for HRRC and demonstrated that a strongly stable matching does not necessarily
exist. Recently, Aziz et al. proved that the problem of determining if a
strongly stable matching exists is NP-complete in general. In this paper, we
refine Aziz et al.'s result by investigating the computational complexity of
the problem in terms of the length of preference lists, the size of regions,
and whether or not regions can overlap, and completely classify tractable and
intractable cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03123"><span class="datestr">at July 08, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03092">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03092">Reconfiguring Directed Trees in a Digraph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ito:Takehiro.html">Takehiro Ito</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iwamasa:Yuni.html">Yuni Iwamasa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakahata:Yu.html">Yu Nakahata</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wasa:Kunihiro.html">Kunihiro Wasa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03092">PDF</a><br /><b>Abstract: </b>In this paper, we investigate the computational complexity of subgraph
reconfiguration problems in directed graphs. More specifically, we focus on the
problem of determining whether, given two directed trees in a digraph, there is
a (reconfiguration) sequence of directed trees such that for every pair of two
consecutive trees in the sequence, one of them is obtained from the other by
removing an arc and then adding another arc. We show that this problem can be
solved in polynomial time, whereas the problem is PSPACE-complete when we
restrict directed trees in a reconfiguration sequence to form directed paths.
We also show that there is a polynomial-time algorithm for finding a shortest
reconfiguration sequence between two directed spanning trees.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03092"><span class="datestr">at July 08, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03076">An Approximation Algorithm for Maximum Stable Matching with Ties and Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yokoi:Yu.html">Yu Yokoi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03076">PDF</a><br /><b>Abstract: </b>We present a polynomial-time $\frac{3}{2}$-approximation algorithm for the
problem of finding a maximum-cardinality stable matching in a many-to-many
matching model with ties and laminar constraints on both sides. We formulate
our problem using a bipartite multigraph whose vertices are called workers and
firms, and edges are called contracts. Our algorithm is described as the
computation of a stable matching in an auxiliary instance, in which each
contract is replaced with three of its copies and all agents have strict
preferences on the copied contracts. The construction of this auxiliary
instance is symmetric for the two sides, which facilitates a simple symmetric
analysis. We use the notion of matroid-kernel for computation in the auxiliary
instance and exploit the base-orderability of laminar matroids to show the
approximation ratio.
</p>
<p>In a special case in which each worker is assigned at most one contract and
each firm has a strict preference, our algorithm defines a
$\frac{3}{2}$-approximation mechanism that is strategy-proof for workers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03076"><span class="datestr">at July 08, 2021 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.03020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.03020">Budgeted Dominating Sets in Uncertain Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhary:Keerti.html">Keerti Choudhary</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Avi.html">Avi Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanaswamy:N=_S=.html">N. S. Narayanaswamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peleg:David.html">David Peleg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vijayaragunathan:R=.html">R. Vijayaragunathan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.03020">PDF</a><br /><b>Abstract: </b>We study the {\em Budgeted Dominating Set} (BDS) problem on uncertain graphs,
namely, graphs with a probability distribution $p$ associated with the edges,
such that an edge $e$ exists in the graph with probability $p(e)$. The input to
the problem consists of a vertex-weighted uncertain graph $\G=(V, E, p,
\omega)$ and an integer {\em budget} (or {\em solution size}) $k$, and the
objective is to compute a vertex set $S$ of size $k$ that maximizes the
expected total domination (or total weight) of vertices in the closed
neighborhood of $S$. We refer to the problem as the {\em Probabilistic Budgeted
Dominating Set}~(PBDS) problem and present the following results.
\begin{enumerate} \dnsitem We show that the PBDS problem is NP-complete even
when restricted to uncertain {\em trees} of diameter at most four. This is in
sharp contrast with the well-known fact that the BDS problem is solvable in
polynomial time in trees. We further show that PBDS is \wone-hard for the
budget parameter $k$, and under the {\em Exponential time hypothesis} it cannot
be solved in $n^{o(k)}$ time.
</p>
<p>\item We show that if one is willing to settle for $(1-\epsilon)$
approximation, then there exists a PTAS for PBDS on trees. Moreover, for the
scenario of uniform edge-probabilities, the problem can be solved optimally in
polynomial time.
</p>
<p>\item We consider the parameterized complexity of the PBDS problem, and show
that Uni-PBDS (where all edge probabilities are identical) is \wone-hard for
the parameter pathwidth. On the other hand, we show that it is FPT in the
combined parameters of the budget $k$ and the treewidth.
</p>
<p>\item Finally, we extend some of our parameterized results to planar and
apex-minor-free graphs. \end{enumerate}
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.03020"><span class="datestr">at July 08, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.02987">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.02987">Sample complexity of hidden subgroup problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Zekun.html">Zekun Ye</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Lvzhou.html">Lvzhou Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02987">PDF</a><br /><b>Abstract: </b>The hidden subgroup problem ($\mathsf{HSP}$) has been attracting much
attention in quantum computing, since several well-known quantum algorithms
including Shor algorithm can be described in a uniform framework as quantum
methods to address different instances of it. One of the central issues about
$\mathsf{HSP}$ is to characterize its quantum/classical complexity. For
example, from the viewpoint of learning theory, sample complexity is a crucial
concept. However, while the quantum sample complexity of the problem has been
studied, a full characterization of the classical sample complexity of
$\mathsf{HSP}$ seems to be absent, which will thus be the topic in this paper.
$\mathsf{HSP}$ over a finite group is defined as follows: For a finite group
$G$ and a finite set $V$, given a function $f:G \to V$ and the promise that for
any $x, y \in G, f(x) = f(xy)$ iff $y \in H$ for a subgroup $H \in
\mathcal{H}$, where $\mathcal{H}$ is a set of candidate subgroups of $G$, the
goal is to identify $H$.
</p>
<p>Our contributions are as follows: For $\mathsf{HSP}$, we give the upper and
lower bounds on the sample complexity of $\mathsf{HSP}$. Furthermore, we have
applied the result to obtain the sample complexity of some concrete instances
of hidden subgroup problem. Particularly, we discuss generalized Simon's
problem ($\mathsf{GSP}$), a special case of $\mathsf{HSP}$, and show that the
sample complexity of $\mathsf{GSP}$ is $\Theta\left(\max\left\{k,\sqrt{k\cdot
p^{n-k}}\right\}\right)$. Thus we obtain a complete characterization of the
sample complexity of $\mathsf{GSP}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.02987"><span class="datestr">at July 08, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.02956">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.02956">Fractional homomorphism, Weisfeiler-Leman invariance, and the Sherali-Adams hierarchy for the Constraint Satisfaction Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Butti:Silvia.html">Silvia Butti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalmau:Victor.html">Victor Dalmau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02956">PDF</a><br /><b>Abstract: </b>Given a pair of graphs $\textbf{A}$ and $\textbf{B}$, the problems of
deciding whether there exists either a homomorphism or an isomorphism from
$\textbf{A}$ to $\textbf{B}$ have received a lot of attention. While graph
homomorphism is known to be NP-complete, the complexity of the graph
isomorphism problem is not fully understood. A well-known combinatorial
heuristic for graph isomorphism is the Weisfeiler-Leman test together with its
higher order variants. On the other hand, both problems can be reformulated as
integer programs and various LP methods can be applied to obtain high-quality
relaxations that can still be solved efficiently. We study so-called fractional
relaxations of these programs in the more general context where $\textbf{A}$
and $\textbf{B}$ are not graphs but arbitrary relational structures. We give a
combinatorial characterization of the Sherali-Adams hierarchy applied to the
homomorphism problem in terms of fractional isomorphism. Collaterally, we also
extend a number of known results from graph theory to give a characterization
of the notion of fractional isomorphism for relational structures in terms of
the Weisfeiler-Leman test, equitable partitions, and counting homomorphisms
from trees. As a result, we obtain a description of the families of CSPs that
are closed under Weisfeiler-Leman invariance in terms of their polymorphisms as
well as decidability by the first level of the Sherali-Adams hierarchy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.02956"><span class="datestr">at July 08, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.02922">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.02922">On the Fault-Tolerant Online Bin Packing Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikbakht:Pooya.html">Pooya Nikbakht</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02922">PDF</a><br /><b>Abstract: </b>We study the fault-tolerant variant of the online bin packing problem.
Similar to the classic bin packing problem, an online sequence of items of
various sizes should be packed into a minimum number of bins of uniform
capacity. For applications such as server consolidation, where bins represent
servers and items represent jobs of different loads, it is required to maintain
fault-tolerant solutions. In a fault-tolerant packing, any job is replicated
into f+1 servers, for some integer f &gt; 1, so that the failure of up to f
servers does not interrupt service. We build over a practical model introduced
by Li and Tang [SPAA 2017] in which each job of load $x$ has a primary replica
of load $x$ and $f$ standby replicas, each of load $x/\eta$, where $\eta &gt;1$ is
a parameter of the problem. Upon failure of up to $f$ servers, any primary
replica in a failed bin should be replaced by one of its standby replicas so
that the extra load of the new primary replica does not cause an overflow in
its bin. We study a general setting in which bins might fail while the input is
still being revealed. Our main contribution is an algorithm, named
Harmonic-Stretch, which maintains fault-tolerant packings under this general
setting. We prove that Harmonic-Stretch has an asymptotic competitive ratio of
at most 1.75. This is an improvement over the best existing asymptotic
competitive ratio 2 of an algorithm by Li and Tang [TPDS 2020], which works
under a model in which bins fail only after all items are packed.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.02922"><span class="datestr">at July 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.02882">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.02882">Twin-width and polynomial kernels</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">Édouard Bonnet</a>, Eun Jung Kim, Amadeus Reinald, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thomass=eacute=:St=eacute=phan.html">Stéphan Thomassé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watrigant:R=eacute=mi.html">Rémi Watrigant</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02882">PDF</a><br /><b>Abstract: </b>We study the existence of polynomial kernels, for parameterized problems
without a polynomial kernel on general graphs, when restricted to graphs of
bounded twin-width. Our main result is that a polynomial kernel for
$k$-Dominating Set on graphs of twin-width at most 4 would contradict a
standard complexity-theoretic assumption. The reduction is quite involved,
especially to get the twin-width upper bound down to 4, and can be tweaked to
work for Connected $k$-Dominating Set and Total $k$-Dominating Set (albeit with
a worse upper bound on the twin-width). The $k$-Independent Set problem admits
the same lower bound by a much simpler argument, previously observed [ICALP
'21], which extends to $k$-Independent Dominating Set, $k$-Path, $k$-Induced
Path, $k$-Induced Matching, etc. On the positive side, we obtain a simple
quadratic vertex kernel for Connected $k$-Vertex Cover and Capacitated
$k$-Vertex Cover on graphs of bounded twin-width. Interestingly the kernel
applies to graphs of Vapnik-Chervonenkis density 1, and does not require a
witness sequence. We also present a more intricate $O(k^{1.5})$ vertex kernel
for Connected $k$-Vertex Cover. Finally we show that deciding if a graph has
twin-width at most 1 can be done in polynomial time, and observe that most
optimization/decision graph problems can be solved in polynomial time on graphs
of twin-width at most 1.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.02882"><span class="datestr">at July 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.02866">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.02866">Telescoping Filter: A Practical Adaptive Filter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:David_J=.html">David J. Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McCauley:Samuel.html">Samuel McCauley</a>, Shikha Singh, Max Stein <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02866">PDF</a><br /><b>Abstract: </b>Filters are fast, small and approximate set membership data structures. They
are often used to filter out expensive accesses to a remote set S for negative
queries (that is, a query x not in S). Filters have one-sided errors: on a
negative query, a filter may say "present" with a tunable false-positve
probability of epsilon. Correctness is traded for space: filters only use log
(1/\epsilon) + O(1) bits per element.
</p>
<p>The false-positive guarantees of most filters, however, hold only for a
single query. In particular, if x is a false positive of a filter, a subsequent
query to x is a false positive with probability 1, not epsilon. With this in
mind, recent work has introduced the notion of an adaptive filter. A filter is
adaptive if each query has false positive epsilon, regardless of what queries
were made in the past. This requires "fixing" false positives as they occur.
</p>
<p>Adaptive filters not only provide strong false positive guarantees in
adversarial environments but also improve performance on query practical
workloads by eliminating repeated false positives.
</p>
<p>Existing work on adaptive filters falls into two categories. First, there are
practical filters based on cuckoo filters that attempt to fix false positives
heuristically, without meeting the adaptivity guarantee. Meanwhile, the broom
filter is a very complex adaptive filter that meets the optimal theoretical
bounds.
</p>
<p>In this paper, we bridge this gap by designing a practical, provably adaptive
filter: the telescoping adaptive filter. We provide theoretical false-positive
and space guarantees of our filter, along with empirical results where we
compare its false positive performance against state-of-the-art filters. We
also test the throughput of our filters, showing that they achieve comparable
performance to similar non-adaptive filters.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.02866"><span class="datestr">at July 08, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/095">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/095">TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/095"><span class="datestr">at July 07, 2021 09:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/open-problem-optimal-query-release/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/open-problem-optimal-query-release/">Open Problem - Optimal Query Release for Pure Differential Privacy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br />error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br />error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br /> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br /> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/open-problem-optimal-query-release/"><span class="datestr">at July 07, 2021 05:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8892467103290123546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">Would you take this bet (Part 1) ?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox. </p><p>Here it is:</p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br /></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>etc. </p><p>1) Expected value: </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value is </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br /></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html"><span class="datestr">at July 07, 2021 12:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=879">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/">Windows never changes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some things I didn’t want to remove, often with a lot of effort, because space was so tight that Windows didn’t even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program — a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/"><span class="datestr">at July 06, 2021 04:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1546">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1546">News for June 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A quieter month after last month’s bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. <em>(Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, Clément!)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>Learning-based Support Estimation in Sublinear Time</strong> by Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner (<a href="https://arxiv.org/abs/2106.08396">arXiv</a>). A classic problem in distribution testing is that of estimating the support size \(n\) of an unknown distribution \(\mathcal{D}\). (Assume that all elements in the support have probability at least \(1/n\).) A fundamental result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a> (2011)  proves that the sample complexity of this problem is \(\Theta(n/\log n)\). A line of work has emerged in trying to reduce this complexity, with additional sources of information. <a href="http://dspace.mit.edu/bitstream/handle/1721.1/101001/Rubinfeld_Testing%20probability.pdf;sequence=1">Canonne-Rubinfeld (2014)</a> showed that, if one can query the exact probabilities of elements, then the complexity can be made independent of \(n\). This paper studies a robust version of this assumption: suppose, we can get constant factor approximations to the probabilities. Then, the main result is that we can get a query complexity of \(n^{1-1/\log(\varepsilon^{-1})} \ll n/\log n\) (where the constant \(\varepsilon\) denotes the additive approximation to the support size). This paper also does empirical experiments to show that the new algorithm is indeed better in practice. Moreover, it shows that existing methods degraded rapidly with poorer probability estimates, while the new algorithm maintains its accuracy even with such estimates. </p>



<p><strong>The Price of Tolerance in Distribution Testing</strong> by Clément L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li (<a href="https://arxiv.org/abs/2106.13414">arXiv</a>). While we have seen many results in distribution testing, the subject of tolerance is one that hasn’t received as much attention. Consider the problem of testing if unknown distribution \(\mathcal{p}\) (over domain \([n]\)) is the same as known distribution \(\mathcal{q}\). We wish to distinguish \(\varepsilon_1\)-close from \(\varepsilon_2\)-far, under total variation distance. When \(\varepsilon_1\) is zero, this is the standard property testing setting, and classic results yield \(\Theta(\sqrt{n})\) sample complexity. If \(\varepsilon_1 = \varepsilon_2/2\), then we are looking for a constant factor approximation to the distance. And the complexity is \(\Theta(n/\log n)\). Surprisingly, nothing was known in better. Until this paper, that is. The main result gives a complete characterization of sample complexity (up to log factors), for all values of \(\varepsilon_1, \varepsilon_2\). Remarkably, the sample complexity has an additive term \((n/\log n) \cdot (\varepsilon_1/\varepsilon^2_2)\). Thus, when \(\varepsilon_1 &gt; \sqrt{\varepsilon_2}\), the sample complexity is \(\Theta(n/\log n)\). When \(\varepsilon_1\) is smaller, the main result gives a smooth dependence on the sample complexity. One the main challenges is that existing results use two very different techniques for the property testing vs constant-factor approximation regimes. The former uses simpler \(\ell_2\)-statistics (e.g. collision counting), while the latter is based on polynomial approximations (estimating moments). The upper bound in this paper shows that simpler statistics based on just the first two moments suffice to getting results for all regimes of \(\varepsilon_1, \varepsilon_2\).</p>



<p><strong>Open Problems in Property Testing of Graphs</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/088/">ECCC</a>). As the title clearly states, this is a survey covering a number of open problems in graph property testing. The broad division is based on the query model: dense graphs, bounded degree graphs, and general graphs. A reader will see statements of various classic open problems, such as the complexity of testing triangle freeness for dense graphs and characterizing properties that can be tested in \(poly(\varepsilon^{-1})\) queries. Arguably, there are more open problems (and fewer results) for testing in bounded degree graphs, where we lack broad characterizations of testable properties. An important, though less famous (?), open problem is that of the complexity of testing isomorphism. It would appear that the setting of general graphs, where we know the least, may be the next frontier for graph property testing. A problem that really caught my eye: can we transform testers that work for bounded degree graphs into those that work for bounded arboricity graphs? The latter is a generalization of bounded degree that has appeared in a number of recent results on sublinear graph algorithms.</p></div>







<p class="date">
by Seshadhri <a href="https://ptreview.sublinear.info/?p=1546"><span class="datestr">at July 06, 2021 02:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/094">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/094">TR21-094 |  New Non-FPT Lower Bounds for Some Arithmetic Formulas | 

	Sébastien Tavenas, 

	Nutan Limaye, 

	Srikanth Srinivasan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An Algebraic Formula for a polynomial $P\in F[x_1,\ldots,x_N]$ is an algebraic expression for $P(x_1,\ldots,x_N)$ using variables, field constants, additions and multiplications.  Such formulas capture an algebraic analog of the Boolean complexity class $\mathrm{NC}^1.$ Proving lower bounds against this model is thus an important problem.


It is known that, to prove superpolynomial lower bounds against algebraic formulas, it suffices to prove good enough lower bounds against restricted kinds of formulas known as Set-Multilinear formulas, for computing a polynomial $P(x_1,...,x_N)$ of degree $O(\log N/\log \log N)$. In the past, many superpolynomial lower bounds were found, but they are of the form $\Omega(f(d) poly(N))$ (where $f$ is typically a subexponential function) which is insufficient to get lower bounds for general formulas. Recently, the authors proved the first non-FPT lower bounds, i.e., a lower bound of the form $N^{\Omega{(f(d)})}$, against small-depth set-multilinear formulas (and also for circuits). In this work, we extend this result in two directions. 

1) Large-depth set-multilinear formulas. In the setting of general set-multilinear formulas, we prove a lower bound of $(\log n)^{\Omega(\log d)}$ for computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}.$ In particular, this implies the first superpolynomial lower bound against unbounded-depth set-multilinear formulas computing $IMM_{n,n}.$ 

As a corollary, this also resolves the homogeneous version of a question of Nisan (STOC 1991) regarding the relative power of Algebraic formulas and Branching programs in the non-commutative setting.

2) Stronger bounds for homogeneous non-commutative small-depth circuits. In the small-depth homogeneous non-commutative case, we prove  a lower bound of $n^{d^{1/\Delta}/2^{O(\Delta)}}$, which yields non-FPT bounds for depths up to $o(\sqrt{\log d}).$ In comparison, the previous bound works in the harder commutative set-multilinear setting, but only up to depths $o(\log \log d)$. 
Moreover, our lower bound holds for all values of $d$, as opposed to the previous set-multilinear lower bound, which holds as long as $d$ is small, i.e., $d = O(\log n)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/094"><span class="datestr">at July 06, 2021 02:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5554">Open thread on new quantum supremacy claims</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Happy 4th to those in the US!</p>



<p>The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a <em>serious</em> quantum supremacy tear lately.  Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photons—the second claim of sampling-based quantum supremacy, after Google’s in Fall 2019.  However, skeptics then poked holes in the USTC claim, showing how they could spoof the results with a classical computer, basically by reproducing the k-photon correlations for relatively small values of k.  Debate over the details continues, but the Chinese group seeks to render the debate largely moot with a <a href="https://arxiv.org/abs/2106.15534">new and better Gaussian BosonSampling experiment</a>, with 144 modes and up to 113 detected photons.  They say they were able to measure k-photon correlations for k up to about 19, which if true would constitute a serious obstacle to the classical simulation strategies that people discussed for the previous experiment.</p>



<p>In the meantime, though, an overlapping group of authors had <a href="https://arxiv.org/abs/2106.14734">put out another paper the day before</a> (!) reporting a sampling-based quantum supremacy experiment using superconducting qubits—extremely similar to what Google did (the same circuit depth and everything), except now with 56 qubits rather than 53.</p>



<p>I confess that I haven’t yet studied either paper in detail—among other reasons, because I’m on vacation with my family at the beach, and because I’m trying to spend what work-time I have on my own projects.  But anyone who <em>has</em> read them, please use the comments of this post to discuss!  Hopefully I’ll learn something.</p>



<p>To confine myself to some general comments: since Google’s announcement in Fall 2019, I’ve consistently said that sampling-based quantum supremacy is <em>not</em> yet a done deal.  I’ve said that quantum supremacy seems important enough to want independent replications, and demonstrations in other hardware platforms like ion traps and photonics, and better gate fidelity, and better classical hardness, and better verification protocols.  Most of all, I’ve said that we needed a genuine dialogue between the “quantum supremacists” and the classical skeptics: the former doing experiments and releasing all their data, the latter trying to design efficient classical simulations for those experiments, and so on in an iterative process.  Just like in applied cryptography, we’d only have real confidence in a quantum supremacy claim once it had survived at least a few years of attacks by skeptics.  So I’m delighted that this is precisely what’s now happening.  USTC’s papers are two new volleys in this back-and-forth; we all eagerly await the next volley, whichever side it comes from.</p>



<p>While I’ve been trying for years to move away from the expectation that I blog about each and every QC announcement that someone messages me about, maybe I’ll also say a word about the recent announcement by IBM of a quantum advantage in space complexity (see <a href="https://www.zdnet.com/article/ibm-researchers-demonstrate-the-advantage-that-quantum-computers-have-over-classical-computers/?fbclid=IwAR0KHVoI8W83Kwmeq9XwmuLHknlKeHjxWcvIjrqjH0QUtLZ8kaIWi6z42yk">here</a> for popular article and <a href="https://arxiv.org/abs/2008.06478">here</a> for arXiv preprint).  There appears to be a nice theoretical result here, about the ability to evaluate any symmetric Boolean function with a single qubit in a branching-program-like model.  I’d love to understand that result better.  But to answer the question I received, this is another case where, once you know the protocol, you know both that the experiment can be done <em>and</em> exactly what its result will be (namely, the thing predicted by QM).  So I think the interest is almost entirely in the protocol itself.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5554"><span class="datestr">at July 04, 2021 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/093">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/093">TR21-093 |  Bounded Indistinguishability for Simple Sources | 

	Yuval Filmus, 

	Andrej Bogdanov, 

	Yuval Ishai, 

	Akshayaram Srinivasan, 

	Krishnamoorthy Dinesh, 

	Avi Kaplan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A pair of sources $\mathbf{X},\mathbf{Y}$ over $\{0,1\}^n$ are $k$-indistinguishable if their projections to any $k$ coordinates are identically distributed. Can some $\mathit{AC^0}$ function distinguish between two such sources when $k$ is big, say $k=n^{0.1}$? Braverman's theorem (Commun. ACM 2011) implies a negative answer when $\mathbf{X}$ is uniform, whereas Bogdanov et al. (Crypto 2016) observe that this is not the case in general.

We initiate a systematic study of this question for natural classes of low-complexity sources, including ones that arise in cryptographic applications, obtaining positive results, negative results, and barriers. In particular: 

– There exist $\Omega(\sqrt{n})$-indistinguishable $\mathbf{X},\mathbf{Y}$, samplable by degree $O(\log n)$ polynomial maps (over $\mathbb{F}_2$) and by $\mathit{poly}(n)$-size decision trees, that are $\Omega(1)$-distinguishable by OR.

– There exists a function $f$ such that all $f(d, \epsilon)$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by degree-$d$ polynomial maps are $\epsilon$-indistinguishable by OR for all sufficiently large $n$.  Moreover, $f(1, \epsilon) = \lceil\log(1/\epsilon)\rceil + 1$ and $f(2, \epsilon) = O(\log^{10}(1/\epsilon))$. 

– Extending (weaker versions of) the above negative results to $\mathit{AC^0}$ distinguishers would require  settling a conjecture of Servedio and Viola (ECCC 2012).
Concretely, if every pair of $n^{0.9}$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by linear maps is $\epsilon$-indistinguishable by $\mathit{AC^0}$ circuits, then the binary inner product function can have at most an $\epsilon$-correlation with $\mathit{AC^0}\circ\oplus$ circuits.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/093"><span class="datestr">at July 04, 2021 02:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">ADFOCS 2021: Convex Optimization and Graph Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 26 – August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this year’s edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to … <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and Graph Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/"><span class="datestr">at July 03, 2021 01:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/">ITC 2021: Call for participation (guest post by Benny Applebaum)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The second edition of the recently created conference on <em>Information-Theoretic Cryptography (ITC 2021)</em> will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark Zhandry.</p>



<p>Registration to the conference is free (but required!)</p>



<p>Visit the webpage <a href="https://itcrypto.github.io/2021/index.html" target="_blank" rel="noreferrer noopener">https://itcrypto.github.io/2021/index.html</a> for more information and for the full program.  </p>



<p>Hope to see you there!</p>



<p>– The Organising Committee</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/"><span class="datestr">at July 02, 2021 01:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1542">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1542">Looking back at WOLA’21: Videos available</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The fifth <a href="http://www.local-algorithms.com/">Workshop on Local Algorithms</a> (WOLA’21) took place <a href="https://ptreview.sublinear.info/?p=1517">earlier this month</a>, and the recordings of the invited talks are now <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm">available on YouTube</a>. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule):</p>



<ul><li>James Aspnes (Yale) on <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=2"><em>Population Protocols</em></a></li><li>Uri Stemmer (Ben-Gurion University) on <em><a href="https://www.youtube.com/watch?v=yxOmND76k4M&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=4">The Local Model of Differential Privacy: A Survey</a></em></li><li>Mary Wootters (Stanford University) on <em><a href="https://www.youtube.com/watch?v=oHGvGgSdvAc&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=7">Lifted Codes and Disjoint Repair Groups</a></em></li><li>Christian Sohler (University of Cologne) on <em><a href="https://www.youtube.com/watch?v=DuJQe0pv224&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=6">Property Testing in Planar Graphs</a></em></li><li>Elaine Shi (Carnegie Mellon University) on <em><a href="https://www.youtube.com/watch?v=jq5MuJJLnDk&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=5">Game-Theoretically Secure Protocols Inspired by Blockchains</a></em></li><li>Jelani Nelson (UC Berkeley) on <em><a href="https://www.youtube.com/watch?v=ZQ1ekannlw0&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=3">Optimal bounds for approximate counting</a></em></li></ul>



<p>Thanks again to the speakers and organizers, and looking forward to WOLA’22!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1542"><span class="datestr">at July 02, 2021 04:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/092">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/092">TR21-092 |  A Note on One-way Functions and Sparse Languages | 

	Yanyi Liu, 

	Rafael Pass</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show equivalence between the existence of one-way
functions and the existence of a \emph{sparse} language that is
hard-on-average w.r.t. some efficiently samplable ``high-entropy''
distribution.
In more detail, the following are equivalent:
  - The existentence of a $S(\cdot)$-sparse language $L$ that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq 4\log n$;
  - The existentence of a $S(\cdot)$-sparse language $L \in
    \NP$, that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq n/3$;
  - The existence of one-way functions.

Our results are insipired by, and generalize, the recent elegant paper by Ilango,
Ren and Santhanam (ECCC'21), which presents similar characterizations for
concrete sparse languages.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/092"><span class="datestr">at July 02, 2021 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4580221543262282386">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html">Intersecting Classes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>If you have two complexity classes that have complete sets, the intersection might not, for example NP ∩ co-NP. The world of total-function classes acts differently.</p><p>Christos Papadimitriou and others defined a <a href="https://en.wikipedia.org/wiki/TFNP">number of classes</a> based on finding solutions to problems where solutions are known to exists for some combinatorial reason. While TFNP, the set of all such problems, might not have complete sets, all the other classes are defined basically based on the complete search problem for the class, such as <a href="https://en.wikipedia.org/wiki/TFNP#PLS">PLS</a>, finding a local minimum. If you have two such classes <b><span style="font-family: Cedarville Cursive;">A</span></b> and <b><span style="font-family: Cedarville Cursive;">B</span></b> with complete search problems A and B define the search problem D as </p><p style="text-align: center;">D(x,y): Find a solution to either A(x) or B(y)</p><p>D is complete for the intersection of <b><span style="font-family: Cedarville Cursive;">A</span></b> and <span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span style="font-family: inherit;">:</span><span style="font-family: inherit;"><span style="font-family: inherit;"> </span>First the </span><span style="font-family: inherit;">pr</span>oblem D is in <b><span style="font-family: Cedarville Cursive;">A</span></b> since you can reduce the problem of finding a solution to either A or B to finding a solution to A. Likewise D is in <span style="font-family: Cedarville Cursive;"><b>B</b></span>.</p><p>Suppose you have a problem Z in <b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span>.<span style="font-family: inherit;"> Then since Z is in </span><b><span style="font-family: Cedarville Cursive;">A</span></b><span style="font-family: inherit;"> and A is complete for </span><span style="font-family: Cedarville Cursive;">A</span><span style="font-family: inherit;">, finding a solution to Z(u) reduces a finding a solution of A(x) where x is easily computed from u. Likewise Z(u) reduces to finding a solution of B(y). So whatever solution D(x,y) gives you, it allows you to find a solution to Z(u). Thus D is complete for </span><b><span style="font-family: Cedarville Cursive;">A</span></b>∩<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span>.</span></p><p>Some people nerd out to <a href="https://mars.nasa.gov/technology/helicopter">helicopters on mars</a>. I nerd out to the complexity of complete sets. </p><p>I learned about complete sets of intersections of total function classes from the talk by one of <a href="http://acm-stoc.org/stoc2021/STOCprogram.html">last week's STOC</a> best paper awardees, <a href="https://doi.org/10.1145/3406325.3451052">The Complexity of Gradient Descent</a> by John Fearnley, Paul W. Goldberg, Alexandros Hollender and Rahul Savani. The part above was well known but the paper goes much further.</p><p>Consider <a href="https://blog.computationalcomplexity.org/2005/12/what-is-ppad.html">PPAD</a> famously with Nash Equilibrium as a complete problem and PLS. PPAD ∩ PLS has complete sets by the argument above. But we can go further.</p><p>The class <a href="https://en.wikipedia.org/wiki/TFNP#CLS">CLS</a> is a variation of PLS where you find a local minimum in a continuous domain under some Lipschitz conditions and is known to sit in the intersection of PPAD and PLS. Fearnley et al. look at finding a minimum using gradient descent (the main tool for deep learning), and showing not only is it CLS-compete but complete for PPAD ∩ PLS. As a consequence CLS = PPAD ∩ PLS. Pretty cool stuff.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html"><span class="datestr">at July 01, 2021 02:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://maxoffsky.com/code-blog/flow-lines/">Flow lines</a> (<a href="https://mathstodon.xyz/@11011110/106428733923482392">\(\mathbb{M}\)</a>). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</p>
  </li>
  <li>
    <p><a href="https://daily.jstor.org/the-soap-bubble-trope/">The soap bubble trope</a> (<a href="https://mathstodon.xyz/@11011110/106430498906810187">\(\mathbb{M}\)</a>, <a href="https://3quarksdaily.com/3quarksdaily/2021/06/soap-bubbles.html">via</a>). Soap bubbles as a recurring theme in art, literature, and popular culture, including “the roof of the Munich Olympic Stadium, Glinda the Good Witch, the first viral ad campaign of the late Victorian era, and morose Dutch still-life paintings”.</p>
  </li>
  <li>
    <p><a href="http://gosper.org/homeplate.html">Officially, home plate doesn’t exist</a> (<a href="https://mathstodon.xyz/@esoterica/106435964222477352">\(\mathbb{M}\)</a>). The rules of baseball define it as a 90-45-90-90-45 pentagon with two 12” sides at one of the right angles and a 17” side between the other two, not possible.</p>
  </li>
  <li>
    <p><a href="https://www.natureindex.com/news-blog/microsoft-academic-graph-discontinued-whats-next">Microsoft Academic Graph being discontinued</a> (<a href="https://mathstodon.xyz/@11011110/106438809410223323">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/19/weekend-reads-biotech-ceo-on-leave-after-allegations-on-pubpeer-a-researcher-disavows-his-own-paper-plagiarism-here-there-and-everywhere/">via</a>). I didn’t much use that one but I live in fear that one day Google will do the same thing to Google Scholar, as they have to so many other useful but nonprofitable Google services.</p>
  </li>
  <li>
    <p>My current workflow for preparing technical talk videos (<a href="https://mathstodon.xyz/@11011110/106444988062063872">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p>Use LaTeX+beamer (169 option) to make pdf talk slides</p>
      </li>
      <li>
        <p>For each slide, print open-in-Preview with custom 16x9 zero-margin layout then export to png</p>
      </li>
      <li>
        <p>Write a script and use quicktime to record 1-2 minute voiceover clips</p>
      </li>
      <li>
        <p>Compose slides and audio in iMovie, export to a huge mp4</p>
      </li>
      <li>
        <p>Use Handbrake to convert to reasonably-sized mp4</p>
      </li>
    </ul>

    <p>It works, but is a bit tedious and produces very dry results. The discussion includes suggestion of alternatives.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/7vEgc7cNarI">The points rotated, and the lines danced</a> (<a href="https://mastodon.social/@sarielhp/106439137323288004">\(\mathbb{M}\)</a>). Video illustrating point-line duality by Sariel Har-Peled.</p>
  </li>
  <li>
    <p><a href="https://www.bbc.com/future/article/20210616-how-the-forgotten-tricks-of-letterlocking-shaped-history">Letterlocking</a> (<a href="https://mathstodon.xyz/@11011110/106458633659042049">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27549256">via</a>, <a href="https://en.wikipedia.org/wiki/Letterlocking">see also</a>): the art of folding your letters so intricately that readers will be forced to tear the paper to unfold and read them.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Bill Gasarch summarizes an online debate</a> (<a href="https://mathstodon.xyz/@11011110/106463907058053228">\(\mathbb{M}\)</a>) with Richard DeMillo and Richard Lipton, moderated by Harry Lewis, looking back at the idea of proving programs correct and at <a href="https://doi.org/10.1145/359104.359106">a classic 1979 paper by DeMillo, Lipton, and Perlis</a> arguing that this idea was already problematic.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/X_%2B_Y_sorting">\(X+Y\) sorting</a> (<a href="https://mathstodon.xyz/@11011110/106468019908924208">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. This is on an old open problem in comparison sorting: can you sort pairs of elements from two sets by their sums, faster than unstructured data of the same length? It’s still an active topic of research; see e.g. <a href="https://doi.org/10.1145%2F3285953">Kane, Lovett, and Moran, “Near-optimal linear decision trees for \(k\)-sum and related problems”, <em>JACM</em> 2019</a>.</p>

    <p>It was not easy to persuade the GA reviewer that this article was as accessible as it could be. I have hopes of <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a> also becoming a Good Article but its “Realizability” section is far more advanced.</p>
  </li>
  <li>
    <p>This week I participated in the International Workshop on Graph-Theoretic Concepts in Computer Science, WG (<a href="https://mathstodon.xyz/@11011110/106474212936524737">\(\mathbb{M}\)</a>). The 9-hour time difference made live participation awkward for me, but fortunately prerecorded contributed talks and the three invited talks (Dujmović on product structures, Samotij on independent set numeration, and Bonnet on twin-width) are linked from <a href="https://wg2021.mimuw.edu.pl/program/">the conference program</a>. The proceedings is not yet out but many preprints of papers are also linked.</p>
  </li>
  <li>
    <p><a href="https://theintercept.com/2021/06/23/anming-hu-trial-fbi-china/">“A juror says the FBI owes an apology to University of Tennessee scientist Anming Hu”</a> (<a href="https://mathstodon.xyz/@11011110/106481625501499687">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/25/weekend-reads-the-obesity-wars-and-the-education-of-a-researcher-zombie-research-hijacked-journals/">via</a>) after putting Hu on trial for allegedly hiding ties to China despite his repeated disclosures of those ties and possibly in retaliation for his refusal to become a spy in China for the FBI. Beyond hurting US research both directly and by motivating good people to go elsewhere, this racist witch hunt has provided fuel for Chinese propaganda.</p>
  </li>
  <li>
    <p>The Wikipedia “Book:” namespace for curated collections of articles is being killed off (<a href="https://mathstodon.xyz/@11011110/106484876399456966">\(\mathbb{M}\)</a>) after the software to collate them into pdfs stopped working. I created five of these, and used two as readings for my courses. All five have moved to my user space:</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Fundamental_Data_Structures"><em>Fundamental Data Structures</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Algorithms"><em>Graph Algorithms</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Drawing"><em>Graph Drawing</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Matroid_Theory"><em>Matroid Theory</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Perfect_Graphs"><em>Perfect Graphs</em></a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://danilafe.com/blog/math_rendering_is_wrong/">Math rendering is wrong</a> (<a href="https://mathstodon.xyz/@11011110/106492765314826160">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27656446">via</a>). This blog post from a year ago argues that, for the same reasons one might write a web site in a markup language before compiling it to html, we should also compile LaTeX to html at that time rather than using browser-side scripts (as in most deploys of MathJax or KaTeX) or conversion to images (Wikipedia). It doesn’t present a solution, but is more a call for that solution to be made.</p>
  </li>
  <li>
    <p><a href="https://kleinbottle.com/#AMAZON%20BRAND%20HIJACKING">Amazon stands by and does nothing as Chinese scammers hijack Cliff Stoll’s Klein bottle business to usurp its positive reviews</a> (<a href="https://mathstodon.xyz/@11011110/106500881370367192">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27684807">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/30/linkage.html"><span class="datestr">at June 30, 2021 10:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18933">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/">Scaling and Fame</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Scaling the pandemic is different from scaling the US budget</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/tt/" rel="attachment wp-att-18935"><img width="225" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/tt.png?resize=225%2C126&amp;ssl=1" class="alignright wp-image-18935" height="126" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sydney Morning Herald interview <a href="https://www.smh.com.au/lifestyle/terence-tao-the-mozart-of-maths-20150216-13fwcv.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Terence Tao is now “properly” famous. He was cited earlier this month in the NYT <a href="https://www.nytimes.com/2021/06/17/science/math-numbers-federal-budget-tao.html">science</a> section for help in explaining large numbers. Numbers such as the US federal budget.</p>
<p>
Today we discuss caveats on such explanations, after a riff on the popular explanation of mathematics.</p>
<p>
Regarding that, let us forget Tao’s work on primes in progressions with Ben Green, forget the Erdős discrepancy problem, and forget his almost-resolution of the Collatz conjecture. Forget it all. Better than a headline, he got his name embedded into the NYT article’s URL. This was for something much less deep that he wrote in 2009—as a blogger. </p>
<p>
<em>En passant</em>, we mention that Ken has an event tomorrow (Wed. 6/30) at 3:30 ET. It is a webinar hosted by Marc Rotenberg, who heads the Washington-based Center for AI and Digital Policy (<a href="https://www.caidp.org">CAIDP</a>), on “Chess and AI: The Role of Transparency.” Registration is free at this <a href="https://www.caidp.org/events/chess/">link</a>. One aspect of transparency in Ken’s work is that he writes about his model’s methodology here—as a blogger.</p>
<p>
</p><p></p><h2> Rescaling the Budget </h2><p></p>
<p></p><p>
Tao was referenced for a <a href="https://terrytao.wordpress.com/2009/05/04/the-federal-budget-rescaled/">post</a> he wrote in May 2009 when Barack Obama was working on his first budget as President. The ratio of $100 million to $3 that he used scaled the budget income to about $75,000. </p>
<p>
With Joe Biden engaged in budget deliberations, Aiyana Green and Steven Strogatz wrote the NYT <a href="https://www.human.cornell.edu/pam/news/aiyana_green_nyt">article</a> on explaining the US federal budget. Green is a student at Cornell: she just completed her junior year in the Department of Policy Analysis and Management. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/agss/" rel="attachment wp-att-18936"><img width="355" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/AGSS.png?resize=355%2C200&amp;ssl=1" class="aligncenter wp-image-18936" height="200" /></a></p>
<p></p><p><br />
They updated Tao’s post to scale the income to $100,000. Besides being a round number, this is close to the estimated <a href="https://www.in2013dollars.com/us/inflation/2009">inflation since 2009</a>. Here is the NYT graphic of the numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/bud-2/" rel="attachment wp-att-18938"><img width="545" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/bud.png?resize=545%2C500&amp;ssl=1" class="aligncenter wp-image-18938" height="500" /></a></p>
<p>
</p><p></p><h2> A Scaling Caveat </h2><p></p>
<p></p><p>
It is attractive to apply this scaling trick elsewhere, even to grim subjects like the coronavirus pandemic. But there we find an element that does not scale.</p>
<p>
Suppose we use the same figure of 100,000 to scale down the world’s population. Besides its famous pandemic numbers <a href="https://www.worldometers.info/coronavirus/">pages</a>, Worldometer also keeps a running <a href="https://www.worldometers.info/world-population/">estimate</a> of the total world population, now nearing 7.9 billion. Scaling down means multiplying every ther human number by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\gamma =}" class="latex" /> 0.000012697445274. </p>
<p>
We can think of 100,000 people as a city that is not a metropolis. Scaling down the current pandemic figures, we get:</p>
<ul>
<li>
182,000,000 total cases become <b>2,309</b>. <p></p>
</li><li>
11,496,147 active cases become <b>145</b>. <p></p>
</li><li>
4 million total deaths (the numbers are approaching that millstone as we write) become <b>50</b> deaths. <p></p>
</li><li>
80,346 currently listed in critical or serious condition become <b>exactly one</b>.
</li></ul>
<p>
These numbers are not at all unusual for our size of city if one considers all kinds of illness and mortality. The scaling trick may seem to have reduced the scope of the pandemic, as opposed to statements such as 182 million being over half the US population. Yet in terms of the raw numbers it preserves the proportions.</p>
<p>
What the scaling doesn’t preserve is the proportion of <em>relations</em>. The number of possible binary relations—person <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> knows person <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" />—is quadratic in the number <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> of people. Suppose the number of pairs who know each other is <img src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^2}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> is a small but fixed constant. (Note: we will redo this with something more reasonable below.) If we then scale <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> down to <img src="https://s0.wp.com/latex.php?latex=%7Bn%27+%3D+n%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n' = n\gamma}" class="latex" />, the situation becomes:</p>
<ul>
<li>
If we estimate the relatedness of our city, we get <img src="https://s0.wp.com/latex.php?latex=%7Ban%27%5E2+%3D+an%5E2%5Cgamma%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an'^2 = an^2\gamma^2}" class="latex" />. <p></p>
</li><li>
But if we scaled down the number of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" />-knows-<img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> relations directly we would get <img src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^2\gamma}" class="latex" />, which is substantially bigger by a factor of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{\gamma}}" class="latex" />.
</li></ul>
<p>
Thus what the scaling really underestimates is the impact of how people are affected by their loved ones being among the 182 million (or the worse numbers). The underestimation logic applies to any form <img src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^c}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &gt; 1}" class="latex" />. This is not an issue with the budget because dollar bills don’t feel relatedness—at least not so much, even absent a line-item veto. Now we will make values of <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c}" class="latex" /> approaching <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> more reasonable.</p>
<p>
</p><p></p><h2> Small World: Tao and Strogatz Again </h2><p></p>
<p></p><p>
Let’s consider people <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" /> who have <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> degrees of separation in the graph of who-knows-who. Then there are <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> who know each other such that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> knows <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> knows <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" />. If something strikes <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> will feel a deep connection by that impact. The feeling is amplified if <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Y}" class="latex" /> have multiple pairs <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> that make a path. This quantifies the relation that Tao calls “awareness” in his “Lecture Notes 3 FOR 254A” course <a href="https://rjlipton.wpcomstaging.com/feed/LECTURE NOTES 3 FOR 254A">notes</a> (pages 51–57 overall). A fact way more basic than what Tao is actually talking about in those notes is the following:</p>
<blockquote><p><b> </b> <em> The sum over pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(X,Y)}" class="latex" /> of the number of pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(A,B)}" class="latex" /> between them who would be affected equals the sum over edges <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(A,B)}" class="latex" /> of the number of pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(X,Y)}" class="latex" /> they can be struck by: both are equal to the number of paths of length <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> in the graph. </em>
</p></blockquote>
<p></p><p>
That number of paths is what we say is reasonable to model by a function <img src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{an^c}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgg+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c \gg 1}" class="latex" />. This is the simplest way of approaching why we feel that treating the pandemic like the US budget underestimates its human effect. One can still rebut that other kinds of illness and death have the same scaling properties, but ultimately we are talking about the <em>excess</em> caused by the pandemic—the effect on top of everything else.</p>
<p>
We have not tried to make this analysis become rigorous using more-realistic models of human networks. Perhaps our readers can point us to such analysis. But one inkling of why we expect our point to be borne out comes from a key conclusion of the famous 1998 <a href="https://www.nature.com/articles/30918">paper</a> of Strogatz with Duncan Watts on ‘small-world’ networks: The phase transition from a lattice network with large average distances to a small-world network with small distances takes place in a range where small clusters cannot recognize it happening locally. </p>
<p>
Thus, if our scaling carried the intuitive picture of an isolated city, it would miss the expanding sphere of relations. At the opposite extreme would be taking the union of Monaco and central Venice, which sum to 100,000 people who fan out mightily. There is also the argument that while small-world networks are held tight by “<a href="https://sociology.stanford.edu/sites/g/files/sbiybj9501/f/publications/the_strength_of_weak_ties_and_exch_w-gans.pdf">weak ties</a>,” the shared knowledge of misery is something that most tends to strengthen ties. And of course, our point about relations extends to many other activities impacted by the pandemic.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What should govern the appropriateness of scaling down?</p>
<p>
It should be noted that if we scale down the US to 100,000 people, the numbers are appreciably higher:</p>
<ul>
<li>
34.5 million total cases become <b>10,366</b>. <p></p>
</li><li>
4,928,564 active cases become <b>1,480</b>. <p></p>
</li><li>
620,000 total deaths become <b>186</b> deaths. <p></p>
</li><li>
3,833 currently listed in critical or serious condition still become <b>exactly one</b>.
</li></ul></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/"><span class="datestr">at June 29, 2021 11:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/06/29/trace-reconstruction/">Trace Reconstruction from Complex Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> is an unknown binary string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. We are asked to recover <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> from its <em>traces</em>, and each <em>trace</em> <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is a random subsequence obtained by deleting the bits of <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> independently with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />.</p>



<p>More formally, let <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> denote the distribution of traces obtained from string <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" />. For example, when <img src="https://s0.wp.com/latex.php?latex=s+%3D+110&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = 110" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" /> assigns a probability mass of <img src="https://s0.wp.com/latex.php?latex=1%2F8&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/8" class="latex" /> to each element in the multiset<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Ctext%7Bempty%7D%2C+1%2C+1%2C+0%2C+11%2C+10%2C+10%2C+110%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{\text{empty}, 1, 1, 0, 11, 10, 10, 110\}." class="latex" /><br /></p>



<p>We then ask: what is the smallest number <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> such that we can recover any <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" /> (say, with probability <img src="https://s0.wp.com/latex.php?latex=%5Cge+0.99&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\ge 0.99" class="latex" />) given <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> independent samples from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_s" class="latex" />?</p>



<p>This <em>trace reconstruction</em> problem was first formulated by <a href="https://people.cs.umass.edu/~mcgregor/papers/04-soda.pdf" target="_blank" rel="noreferrer noopener">Batu-Kannan-Khanna-McGregor</a> in 2004, and their central motivation is from the <a href="https://en.wikipedia.org/wiki/Multiple_sequence_alignment" target="_blank" rel="noreferrer noopener">multiple sequence alignment</a> problem in computational biology. Trace reconstruction is also a fundamental problem related to the <a href="https://en.wikipedia.org/wiki/Deletion_channel" target="_blank" rel="noreferrer noopener">deletion channel</a> in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability <img src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/2" class="latex" />. Then the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> tells us the number of independent copies that need to be sent for the receiver to determine the original message.</p>



<p>Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the computational complexity). In 2017, <a href="https://arxiv.org/pdf/1612.03148.pdf" target="_blank" rel="noreferrer noopener">De-O’Donnell-Servedio</a> and <a href="https://arxiv.org/pdf/1612.03599.pdf" target="_blank" rel="noreferrer noopener">Nazarov-Peres</a> independently proved <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Cle+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \le \exp(O(n^{1/3}))" class="latex" />. A very recent breakthrough due to <a href="https://arxiv.org/pdf/2009.03296.pdf" target="_blank" rel="noreferrer noopener">Chase</a> further improved this bound to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Ctilde+O%28n%5E%7B1%2F5%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(\tilde O(n^{1/5}))" class="latex" />, which is still super-polynomial. On the other hand, the best known sample complexity lower bound is merely <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%5COmega%28n%5E%7B3%2F2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde\Omega(n^{3/2})" class="latex" />, proved by <a href="https://arxiv.org/pdf/1905.03031.pdf" target="_blank" rel="noreferrer noopener">Chase</a> in another recent work.</p>



<p>In this blog post, I will explain why this problem is much more non-trivial than it might appear at first glance. I will also give an overview on the work of [DOS17, NP17], which, interestingly, reduces this seemingly combinatorial problem to complex analysis.</p>



<p><strong>Observation: reconstruction <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\approx" class="latex" /> TV-distance.</strong> Let us start with a natural first attempt at the problem. Define <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> as the minimum statistical distance between the trace distribution of two different length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> strings:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n+%3D+%5Cmin_%7Bx+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En%7Dd_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n = \min_{x \ne y \in \{0, 1\}^n}d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)." class="latex" /><br /></p>



<p>It is not hard to show that <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n" class="latex" /> bounds <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> on both sides, up to a polynomial factor:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n+%5Clesssim+M%28n%29+%5Clesssim+n%2F%5Cdelta_n%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\delta_n \lesssim M(n) \lesssim n/\delta_n^2." class="latex" /><br /></p>



<p class="has-black-color has-text-color">The lower bound holds because any trace reconstruction algorithm must be able to distinguish <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for every pair of different strings <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" />, and this requires <img src="https://s0.wp.com/latex.php?latex=%5COmega%281+%2F+%5Cdelta_n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(1 / \delta_n)" class="latex" /> samples for the minimizer <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> in the definition of <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />. For the upper bound, we note that every pair <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> can be distinguished with an <img src="https://s0.wp.com/latex.php?latex=o%282%5E%7B-n%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="o(2^{-n})" class="latex" /> error probability using <img src="https://s0.wp.com/latex.php?latex=O%28n%2F%5Cdelta_n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n/\delta_n^2)" class="latex" /> samples. We say that string <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> “beats” <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />, if the distinguisher for <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> decides that “<img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" />“. By a union bound, the correct answer <img src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s" class="latex" /> “beats” every other string with high probability. We can then obtain a reconstruction algorithm by running the distinguisher for every string pair, and outputting the unique string that “beats” the other <img src="https://s0.wp.com/latex.php?latex=2%5En-1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^n-1" class="latex" /> strings.</p>



<p>Thus, to determine whether the sample complexity <img src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n)" class="latex" /> is polynomial in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />, it suffices to determine whether <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" /> scales as <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\mathrm{poly}(n)" class="latex" /> or is much smaller. Unfortunately, it turns out to be highly non-trivial to bound <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta_n" class="latex" />, and even bounding <img src="https://s0.wp.com/latex.php?latex=d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)" class="latex" /> for “simple” <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> can be hard. Imagine that we try to reason about the distribution <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" />: the probability mass that <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> assigns to string <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> is proportional to the number of times that <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" /> appears as a subsequence in <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, but this count is already hard to express or control, unless <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> has a very simple pattern. This is roughly where this natural attempt gets stuck.</p>



<p><strong>Distinguishing using estimators.</strong> Now we turn to a different approach that underlies the recent breakthrough on trace reconstruction algorithms. As discussed earlier, we can focus on the problem of distinguishing the case <img src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s = y" class="latex" /> for fixed strings <img src="https://s0.wp.com/latex.php?latex=x+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x \ne y \in \{0, 1\}^n" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f: \{0, 1\}^{\le n} \to \mathbb{R}" class="latex" /> be a function defined over all possible traces from a length-<img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> string. We consider the following algorithm for distinguishing <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" />:<br /><br /><strong>Step 1.</strong> Given traces <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s_1%2C+%5Ctilde+s_2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s_1, \tilde s_2, \ldots" class="latex" />, compute the average of <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s_1%29%2C+f%28%5Ctilde+s_2%29%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s_1), f(\tilde s_2), \ldots" class="latex" /><br /><strong>Step 2.</strong> Output <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> if the average is closer to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+x+%5Csim+%5Cmathcal%7BD%7D_x%7D%5Bf%28%5Ctilde+x%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde x \sim \mathcal{D}_x}[f(\tilde x)]" class="latex" /> than to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+y+%5Csim+%5Cmathcal%7BD%7D_y%7D%5Bf%28%5Ctilde+y%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde y \sim \mathcal{D}_y}[f(\tilde y)]" class="latex" />, and output <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> otherwise.<br /></p>



<p>For the above to succeed, <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> needs to satisfy the following two conditions:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_y" class="latex" /> are well-separated under <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, namely <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+x%29%5D+-+%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+y%29%5D%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\mathbb{E}[f(\tilde x)] - \mathbb{E}[f(\tilde y)]| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon &gt; 0" class="latex" />.<br /><strong>(Boundedness)</strong> Expectation of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can be efficiently estimated. This can be guaranteed if for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Cf%28%5Ctilde+s%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|f(\tilde s)| \le B" class="latex" /> holds for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%5Cin+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s \in \{0, 1\}^{\le n}" class="latex" />.<br /></p>



<p>Assuming the above, a standard concentration argument shows that we can distinguish <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> using <img src="https://s0.wp.com/latex.php?latex=O%28%28B%2F%5Cepsilon%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O((B/\epsilon)^2)" class="latex" /> samples, and thus <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Clesssim+n+%5Ccdot+%28B%2F%5Cepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) \lesssim n \cdot (B/\epsilon)^2" class="latex" />.</p>



<p>In principle, a near-optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> would be setting <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s)" class="latex" /> to be the indicator of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x%28%5Ctilde+s%29+%5Cge+%5Cmathcal%7BD%7D_y%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathcal{D}_x(\tilde s) \ge \mathcal{D}_y(\tilde s)" class="latex" />, which gives boundedness <img src="https://s0.wp.com/latex.php?latex=B+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = 1" class="latex" /> and separation <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29+%5Cge+%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y) \ge \delta_n" class="latex" />. However, as argued above, this optimal choice of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> can still be hard to analyze. Instead, we focus on choosing a simpler function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, which potentially gives a suboptimal <img src="https://s0.wp.com/latex.php?latex=B%2F%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B/\epsilon" class="latex" /> but admits simple analyses.</p>



<p><strong>Sketch of the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> Upper Bound.</strong> The main results of [DOS17] and [NP17] follow from choosing <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> to be a linear function. Given a trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%3D+%5Ctilde+s_0+%5Ctilde+s_1+%5Ctilde+s_2+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s = \tilde s_0 \tilde s_1 \tilde s_2 \cdots" class="latex" />, we consider the following polynomial with coefficients being the bits of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29+%3D+%5Csum_%7Bk%7D%5Ctilde+s_k+z%5Ek+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z) = \sum_{k}\tilde s_k z^k = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots." class="latex" /><br /></p>



<p>For some number <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> to be determined later, we consider the estimator <img src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29+%3D+%5Ctilde+S%28z%29+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\tilde s) = \tilde S(z) = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots" class="latex" />, which is indeed a linear function in the trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />.</p>



<p>At first glance, it might be unclear why we choose the coefficients to be powers of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" />. The following fact justifies this choice by showing that polynomials interact with the deletion channel very nicely: The expectation of <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde S(z)" class="latex" /> is exactly the evaluation of the polynomial <img src="https://s0.wp.com/latex.php?latex=S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S(\cdot)" class="latex" /> with coefficients <img src="https://s0.wp.com/latex.php?latex=s_0%2C+s_1%2C+%5Cldots%2C+s_%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s_0, s_1, \ldots, s_{n-1}" class="latex" />, but at a slightly different point.<br /></p>



<p><strong>Fact:</strong> For any string <img src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="s \in \{0, 1\}^n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+s+%5Csim+%5Cmathcal%7BD%7D_s%7D%5Cleft%5B%5Ctilde+S%28z%29%5Cright%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%5Cleft%28%5Cfrac%7Bz%2B1%7D%7B2%7D%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{\tilde s \sim \mathcal{D}_s}\left[\tilde S(z)\right] = \frac{1}{2}S\left(\frac{z+1}{2}\right)." class="latex" /><br /></p>



<p>Equivalently, we have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+S%282z-1%29%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{E}[\tilde S(2z-1)] = \frac{1}{2}S(z)" class="latex" />, which allows us to rephrase our requirements on the choice of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> as follows:<br /></p>



<p><strong>(Separation)</strong> <img src="https://s0.wp.com/latex.php?latex=%7CX%28z%29+-+Y%28z%29%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|X(z) - Y(z)| \ge \epsilon" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> that is not too small.<br /><strong>(Boundedness)</strong> <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)| \le B" class="latex" /> for all possible trace <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />, for some <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B" class="latex" /> that is not too large.<br /></p>



<p>After some thought, it is beneficial to choose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> such that both <img src="https://s0.wp.com/latex.php?latex=%7Cz%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|z|" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1|" class="latex" /> are close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />, since this ensures that different bits in the string are assigned weights of similar magnitudes in the polynomials above. These two conditions hold if and only if <img src="https://s0.wp.com/latex.php?latex=z+%5Capprox+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \approx 1" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="512" alt="" src="https://theorydish.files.wordpress.com/2021/06/plan.png?w=1024" class="wp-image-2442" height="230" />The overall plan for distinguishing strings <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />.</figure></div>



<p>The crucial idea in both papers [DOS17, NP17] is to consider the polynomials in the complex plane <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb{C}" class="latex" /> instead of on the real line. Fortunately, all the previous discussion still holds for complex numbers, with <img src="https://s0.wp.com/latex.php?latex=%7C%5Ccdot%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\cdot|" class="latex" /> interpreted as modulus instead of absolute value. In [NP17], the authors chose <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> from a small arc of the unit circle:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=A_L+%3D+%5C%7Be%5E%7Bi%5Ctheta%7D%3A+%5Ctheta%5Cin%5B-%5Cpi%2FL%2C+%5Cpi%2FL%5D%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L = \{e^{i\theta}: \theta\in[-\pi/L, \pi/L]\}" class="latex" />.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="311" alt="" src="https://theorydish.files.wordpress.com/2021/06/a_l.png?w=622" class="wp-image-2445" height="262" />Arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" /> marked by the red box and dotted lines.</figure></div>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" />, it is easy to upper bound <img src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|\tilde S(2z - 1)|" class="latex" />: it follows from simple calculus that <img src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C+%5Cle+1+%2B+O%28L%5E%7B-2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|2z - 1| \le 1 + O(L^{-2})" class="latex" />, and thus for every <img src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\tilde s" class="latex" />:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctilde+S%282z-1%29%5Cright%7C+%5Cle+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D%5Cleft%7C%282z-1%29%5Ek%5Cright%7C+%5Cle+n+%5Ccdot+%5Cleft%5B1+%2B+O%28L%5E%7B-2%7D%29%5Cright%5D%5En+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\left|\tilde S(2z-1)\right| \le \sum_{k=0}^{n-1}\left|(2z-1)^k\right| \le n \cdot \left[1 + O(L^{-2})\right]^n = e^{O(n/L^2)}" class="latex" />.</p>



<p>To lower bound <img src="https://s0.wp.com/latex.php?latex=X%28z%29+-+Y%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X(z) - Y(z)" class="latex" />, note that since both <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" /> are binary, all the coefficients of <img src="https://s0.wp.com/latex.php?latex=X-Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X-Y" class="latex" /> are in <img src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+0%2C+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\{-1, 0, 1\}" class="latex" />. Such polynomials are known as <em>Littlewood polynomials</em>. The separation condition is then reduced to the following claim in complex analysis:</p>



<p class="has-text-align-center"><em><strong>Littlewood polynomials cannot to be too “flat” over a short arc around 1.</strong></em></p>



<p>This is indeed the case:</p>



<p id="lemma-1"><strong>Lemma 1.</strong> (<a href="https://www.jstor.org/stable/pdf/24899666.pdf" target="_blank" rel="noreferrer noopener">[Borwein and Erdélyi, 1997]</a>) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge e^{-O(L)}" class="latex" />.<br /></p>



<p>By <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a>, there exists <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z \in A_L" class="latex" /> such that the separation condition holds for <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\epsilon = e^{-O(L)}" class="latex" />. Recall that the boundedness holds for <img src="https://s0.wp.com/latex.php?latex=B+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="B = e^{O(n/L^2)}" class="latex" />. This shows that the sample complexity is upper bounded by <img src="https://s0.wp.com/latex.php?latex=%28B%2F%5Cepsilon%29%5E2+%3D+%5Cexp%28O%28n%2FL%5E2+%2B+L%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="(B/\epsilon)^2 = \exp(O(n/L^2 + L))" class="latex" />, which is minimized at <img src="https://s0.wp.com/latex.php?latex=L+%3D+n%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L = n^{1/3}" class="latex" />. This proves the upper bound <img src="https://s0.wp.com/latex.php?latex=M%28n%29+%3D+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(n) = \exp(O(n^{1/3}))" class="latex" />.</p>



<p><strong>Proof of a weaker lemma.</strong> While the original proof of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> is a bit technical, [NP17] presented a beautiful and much simpler proof of the following weaker result, in which the <img src="https://s0.wp.com/latex.php?latex=e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="e^{-O(L)}" class="latex" /> lower bound is replaced by <img src="https://s0.wp.com/latex.php?latex=n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{-O(L)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is the degree of the Littlewood polynomial.</p>



<p><strong>Lemma 2.</strong> ([Lemma 3.1, NP17]) For every nonzero Littlewood polynomial <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> of degree <img src="https://s0.wp.com/latex.php?latex=%3C+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt; n" class="latex" />,<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\max_{z \in A_L}|p(z)| \ge n^{-O(L)}" class="latex" />.<br /></p>



<p><strong>Proof.</strong> Without loss of generality, we assume that the constant term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" />. Suppose otherwise, that the lowest order term of <img src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z^m" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=m+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m \ge 1" class="latex" />. We may consider the polynomial <img src="https://s0.wp.com/latex.php?latex=p%28z%29+%2F+z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p(z) / z^m" class="latex" /> instead.</p>



<p>Let <img src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M = \max_{z \in A_L}|p(z)|" class="latex" /> be the maximum modulus of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> over arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+e%5E%7B2%5Cpi+i%2FL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega = e^{2\pi i/L}" class="latex" /> be an <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />-th root of unity. Consider the following polynomial:<br /></p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=q%28z%29+%3D+%5Cprod_%7Bj%3D0%7D%5E%7BL-1%7Dp%28%5Comega%5Ej+z%29+%3D+p%28z%29+%5Ccdot+p%28%5Comega+z%29+%5Ccdot+%5Ccdots+%5Ccdot+p%28%5Comega%5E%7BL-1%7D+z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z) = \prod_{j=0}^{L-1}p(\omega^j z) = p(z) \cdot p(\omega z) \cdot \cdots \cdot p(\omega^{L-1} z)" class="latex" />.</p>



<p>For every <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, at least one point <img src="https://s0.wp.com/latex.php?latex=%5Comega%5Ej+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^j z" class="latex" /> falls into the arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7Cp%28%5Comega%5Ej+z%29%7C+%5Cle+M&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|p(\omega^j z)| \le M" class="latex" />. (See <a href="https://theorydish.blog/feed/#windmill">figure below</a> for a proof by picture.) The moduli of the remaining <img src="https://s0.wp.com/latex.php?latex=L+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L - 1" class="latex" /> factors are trivially bounded by <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. Thus, <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cle+M%5Ccdot+n%5E%7BL-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \le M\cdot n^{L-1}" class="latex" />.</p>



<p>On the other hand, we note <img src="https://s0.wp.com/latex.php?latex=q%280%29+%3D+%5Bp%280%29%5D%5EL+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(0) = [p(0)]^L = 1" class="latex" />. The <a href="https://en.wikipedia.org/wiki/Maximum_modulus_principle" target="_blank" rel="noreferrer noopener">maximum modulus principle</a> implies that for some <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z" class="latex" /> on the unit circle, we have <img src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="|q(z)| \ge 1" class="latex" />. Therefore, we must have <img src="https://s0.wp.com/latex.php?latex=M+%5Ccdot+n%5E%7BL+-+1%7D+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \cdot n^{L - 1} \ge 1" class="latex" />, which implies <img src="https://s0.wp.com/latex.php?latex=M+%5Cge+n%5E%7B-%28L+-+1%29%7D+%3D+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M \ge n^{-(L - 1)} = n^{-O(L)}" class="latex" /> and completes the proof. <img src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\square" class="latex" /></p>



<div class="wp-block-image" id="windmill"><figure class="aligncenter size-large is-resized"><img width="462" alt="" src="https://theorydish.files.wordpress.com/2021/06/example.png?w=924" class="wp-image-2449" height="261" />Points involved in the definition of <img src="https://s0.wp.com/latex.php?latex=q%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="q(z)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=L%3D4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L=4" class="latex" />.<br />In this case, <img src="https://s0.wp.com/latex.php?latex=%5Comega%5E3+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\omega^3 z" class="latex" /> is inside arc <img src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="A_L" class="latex" />, which is marked by the red lines.</figure></div>



<p><strong>Beyond linear estimators?</strong> The work of [DOS17, NP17] not only proved the <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\exp(O(n^{1/3}))" class="latex" /> upper bound, but also showed that this is the best sample complexity we can get from linear estimator <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />. The recent work of [Cha20] goes beyond these linear estimators by taking the higher moments of the trace into account. The idea turns out to be a natural one: consider the “<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-grams” (i.e., length-<img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" /> substrings) of the string for some <img src="https://s0.wp.com/latex.php?latex=k+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k &gt; 1" class="latex" />. In more detail, we fix some string <img src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5C%7B0%2C+1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w \in \{0, 1\}^k" class="latex" />, and consider the binary polynomial with coefficients corresponding to the positions at which <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> appears as a (contiguous) substring. The key observation is that there exists a choice of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="w" class="latex" /> such that the resulting polynomial is sparse (in the sense that the degrees of the nonzero monomials are far away). The technical part of [Cha20] is then devoted to proving an analogue of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> that is specialized to these “sparse” Littlewood polynomials.</p>



<p><strong>Acknowledgments.</strong> I would like to thank Moses Charikar, Li-Yang Tan and Gregory Valiant for being on my quals committee and for helpful discussions about this problem.</p></div>







<p class="date">
by Mingda Qiao <a href="https://theorydish.blog/2021/06/29/trace-reconstruction/"><span class="datestr">at June 29, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html">Greedy orderings with transposition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I’m a big fan of using <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a> to model vertex-ordering processes in graphs such as the construction of <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological orderings</a> in <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> and perfect elimination orderings in <a href="https://en.wikipedia.org/wiki/Chordal_graph">chordal graphs</a>. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</p>

<p>But there are some greedy vertex-ordering processes that do not form antimatroids, even though they do have the same inability to make mistakes. Two of these are the dismantling orders of <a href="https://en.wikipedia.org/wiki/Cop-win_graph">cop-win graphs</a> and the reverse construction orders of <a href="https://en.wikipedia.org/wiki/Distance-hereditary_graph">distance-hereditary graphs</a>. I wrote about cop-win graphs <a href="https://11011110.github.io/blog/2016/08/18/game-of-cop.html">here in 2016</a>; a graph is cop-win if a cop can always land on the same vertex as a robber when they take turns either moving from a vertex to a neighboring vertex or staying put. In distance-hereditary graphs, all induced subgraphs have the same distances; <a href="https://11011110.github.io/blog/2005/10/11/delta-confluent-drawing-paper.html">these graphs also have nice confluent drawings</a>. Both of these classes of graphs can be recognized by greedy algorithms that remove one vertex at a time until either getting stuck (for graphs not in the class) or succeeding by reaching a single-vertex graph. But although the conditions for removing vertices in these algorithms are local, they are not antimatroidal.</p>

<h1 id="an-example-graph">An example graph</h1>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/Ptolemaic.svg" alt="A six-vertex graph with six vertices A, B, C, D, E, and F, and seven edges AD, BC, BD, BE, CE, and EF" /></p>

<p>The graph shown above happens to be chordal, distance-hereditary, and cop-win, making it a convenient example both of how to order the vertices of these graph classes and of why the distance-hereditary and cop-win orderings are not antimatroidal.</p>

<ul>
  <li>
    <p>In chordal graphs, a perfect elimination ordering can be constructed by repeatedly removing <em>simplicial vertices</em>, vertices whose neighborhoods form a clique. For an elimination ordering of the example graph, vertices \(A\), \(C\), and \(F\) are already available to be listed: \(A\) and \(F\) only have one neighbor (automatically a clique), and \(C\) has two neighbors forming a two-vertex clique. The other vertices will become available later in the removal process, once enough of their neighbors have been removed and all remaining vertices become adjacent. For instance, once \(A\) has been removed, \(D\) will become available, and once \(C\) has been removed, \(B\) will become available. Once this removal process makes a vertex simplicial, it remains simplicial until removed, so elimination orderings form an antimatroid.</p>
  </li>
  <li>
    <p>Distance-hereditary graphs can be constructed from a single vertex by repeatedly adding leaf vertices (with one neighbor connecting to previous vertices) or twins (duplicates of previous vertices). Reversing this process, these graphs can be deconstructed by repeatedly removing leaves or twins. The graph above has no twins, but \(A\) and \(F\) are leaves, and can be removed immediately. If \(A\) is removed, \(C\) and \(D\) become false twins (not adjacent to each other), and either of them can be removed. Similarly, if \(F\) is removed, \(B\) and \(E\) become true twins (adjacent to each other), after which one can be removed, but not both: after removing \(F\) and \(B\), \(E\) is no longer a leaf or a twin (because its twin, \(B\), has gone), and must remain until later steps. Because the removal orders can start \(FB\) or \(FE\) but not \(FBE\), they are not described by an antimatroid.</p>
  </li>
  <li>
    <p>Similarly, cop-win graphs can be dismantled by repeatedly removing a vertex \(v\) that is dominated by another vertex \(w\), meaning that the neighborhood of \(v\) (including \(v\) itself) is a subset of the neighborhoood of \(w\). In the given graph, \(A\) is dominated by \(D\), \(B\) is dominated by \(E\), \(C\) is dominated by both \(B\) and \(E\), and \(F\) is dominated by \(E\). So any one of these four dominated vertices starts out as removable. But if we remove first \(F\) and then \(E\) (dominated by \(B\) after the removal of \(F\)) we can no longer remove \(B\). So because the ability to be removed can go away before the removal happens, we do not have an antimatroid.</p>
  </li>
</ul>

<p>There’s another complication here as well. For both distance-hereditary graphs and cop-win graphs, removing leaves and twins or dominated vertices will never eliminate all graph vertices. Instead, both removal processes stop when we reach a single remaining vertex. But this is different from antimatroids, where all elements must be included in all orderings.</p>

<h1 id="some-axiomatics">Some axiomatics</h1>

<p>To understand why greedy orderings still work in these cases, I think it’s helpful to start by understanding why they work for antimatroids, as a general class of structures. The following is not quite the usual system of axioms for antimatroids, but they can be defined as non-empty formal languages (that is, sets of strings over a finite alphabet) with the following properties:</p>

<dl>
  <dt>Hereditary:</dt>
  <dd>
    <p>Every prefix of a string in the language is also in the language. Thinking about this in the other direction: every string in the language can be built up by adding one character at a time, starting from the empty string, at all times remaining within the language.</p>
  </dd>
  <dt>Normal:</dt>
  <dd>
    <p>Every character occurs at most once in any string in the language. An element can only be added to the sequence of elements once. Because we are assuming the alphabet to be finite, this means that the language itself is also finite.</p>
  </dd>
  <dt>Oblivious:</dt>
  <dd>
    <p>If \(S\) and \(T\) are permutations of each other in the language, then for every character \(x\), \(Sx\) is in the language if and only if \(Tx\) is in the language. This means that what can be added next depends only on the set of characters that have been added already, forgetting about the order in which they were added.</p>
  </dd>
  <dt>Anti-exchange:</dt>
  <dd>
    <p>If \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, then so does \(Sxy\). Adding \(x\) doesn’t prevent \(y\) from being added later. This is the key property of an antimatroid and the one that is violated by the distance-hereditary and cop-win orderings.</p>
  </dd>
</dl>

<p>Usually a stronger version of obliviousness is used, stating that when \(S\) and a permutation of \(Sx\) are in the language, then \(Sx\) is in the language, but it’s not immediately obvious why this should be true for the vertex-ordering processes I’m considering here, so I’ve gone with a weaker version. We’ll see later that the stronger version is implied by a combination of this and other properties. It is standard to also require that all characters be usable, but I haven’t done this, because I want to understand the behavior of antimatroidal greedy algorithms on graphs not in the given graph class, for which they get stuck before ordering the whole graph. But this is not important, because one could instead redefine the alphabet to consist only of usable characters.</p>

<p>Given a language that satisfies all of these properties, one can show that all non-extendable strings are equally long and use the same alphabet as each other. For, if we have two different non-extendable strings \(S\) and \(T\), we can morph \(S\) into \(T\) one step at a time, never shortening it or changing its character set, by finding the first position at which \(S\) and \(T\) differ, finding the character \(t\) that \(T\) has at that position (necessarily also used later in \(S\) because it was usable at that position and would have remained usable until it was used), and repeatedly using the anti-exchange axiom to swap \(t\) for the previous character in \(S\) until it has been swapped into a match with \(T\). The oblivious property ensures that the part of the string after the swap remains valid. So \(S\) cannot be shorter than or miss any characters from \(T\), nor vice versa.</p>

<p>Instead of the anti-exchange axiom, the distance-hereditary and cop-win orderings satisfy a weaker property, based on the notion of swapping two characters.</p>

<dl>
  <dt>Transposition:</dt>
  <dd>
    <p>Suppose \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, but \(Sxy\) does not. Then for all \(T\) not containing \(x\) or \(y\), \(SxT\) is in the language if and only if \(SyT\) is also in the language, and \(SxTy\) is in the language if and only if \(SyTx\) is also in the language.</p>
  </dd>
</dl>

<p>The last part of the transposition property, about \(SxTy\) and \(SyTx\), is only included because we used a weak version of obliviousness; if we used the stronger version, it would follow from the earlier part of the transposition property.</p>

<h1 id="cop-win-and-distance-hereditary-orderings-have-the-transposition-property">Cop-win and distance-hereditary orderings have the transposition property</h1>

<p>Let’s suppose we’re trying to dismantle a cop-win graph by repeatedly removing dominated vertices, in the hope of getting down to a single vertex. After we’ve removed some vertices already in a sequence \(S\), two vertices \(x\) and \(y\) might become included in the set of dominated vertices. This can happen in several different ways:</p>

<ul>
  <li>It might be the case that \(x\) is dominated by a vertex that is not \(y\), and that \(y\) is dominated by a vertex that is not \(x\). When this happens, they can be removed in either order: removing one won’t change the fact that the other is dominated by whatever other vertex dominated it already.</li>
  <li>It might be the case that \(x\) is dominated by \(y\), and \(y\) is dominated by a third vertex \(z\). But then \(x\) is also dominated by \(z\), and again they can be removed in either order. When one is removed, the other is still dominated by \(z\).</li>
  <li>The only remaining case is that \(x\) and \(y\) are each dominated only by the other of these two vertices. In this case, we can remove one or the other but not both. But if \(x\) and \(y\) dominate each other, they have the same neighbors (they are twins), and there is a symmetry of the remaining subgraph swapping \(x\) and \(y\). So in this case, any continuation of the removal sequence \(S\) can have \(x\) replaced by \(y\) and vice versa, and still be a valid continuation. This is exactly what the transposition property states.</li>
</ul>

<p>The argument for distance-hereditary orderings is even easier. If \(x\) and \(y\) are not twins of each other, then removing one won’t affect the removability of the other. If they are twins, then they are symmetric and any continuation of the removal sequence can exchange \(x\) for \(y\) without changing its validity.</p>

<h1 id="orderings-with-transposition-form-greedoids">Orderings with transposition form greedoids</h1>

<p>If we can’t obtain an antimatroid from the cop-win or distance-hereditary graphs, we might at least hope for a more general structure, a greedoid. The key property of greedoids (viewed as hereditary normal languages rather than their usual definition as set systems) is the following axiom:</p>

<dl>
  <dt>Exchange:</dt>
  <dd>If \(S\) is a longer string in the language of a greedoid, and \(T\) is a longer string in the same language, then there is a character \(x\) in \(T\) such that \(Sx\) is a string in the language.</dd>
</dl>

<p>This implies that all maximal strings in the language have the same length, and in the cop-win and distance-hereditary cases it implies that all greedy dismantling or deconstruction sequences reach a single vertex without getting stuck along the way. The greedoid exchange property also immediately implies the strong version of the obliviousness property, by plugging in a permutation of \(Sx\) as the string \(T\) in the exchange property.</p>

<p>To prove that indistinguishability implies the exchange property, let \(S\) be any string in an indistinguishable (hereditary normal oblivious) language, and let \(T\) be a longer string, which we might as well assume to be maximal. If \(S\) is a prefix of \(T\), then obviously we can satisfy the exchange property: just take the prefix of \(T\) that has one more character.</p>

<p>Otherwise, I claim that we can replace \(T\) by a different string \(T'\) of the same length that agrees with \(S\) for more positions. To find \(T'\), let \(y\) be the first character of \(S\) that differs from the corresponding character of \(T\); this must exist by the assumption that \(S\) is not a prefix of \(T\). Obviously, at the position of \(y\) in \(S\), we could have added it to \(T\), but instead some other character was chosen. Maybe, \(y\) remained available to be chosen throughout the remaining positions of \(T\), until it actually was chosen. If so, just as in the antimatroid case, we could repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position. Alternatively, maybe at some point during the construction of sequence \(T\), we chose a character \(z\) causing \(y\) to become unavailable. In this case, by the transposition property, we can swap \(y\) for \(z\) in \(T\) and then as before repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position.</p>

<p>By repeatedly replacing \(T\) by equally long strings that agree with more and more positions of \(S\), we eventually reach a string for which \(S\) is a prefix, and can append one more character. This construction of \(T'\) from \(T\) does not include any new characters that weren’t already in \(S\) or \(T\), so the appended character must have come from \(T\), proving the exchange axiom.</p>

<p>The use of the transposition property to form greedoids is standard; these greedoids are called transposition greedoids, and are described e.g. by Björner and Ziegler in their introduction to greedoids in the book <em>Matroid Applications</em>. Another <a href="https://doi.org/10.1007/978-3-642-58191-5_10">book chapter on transposition greedoids</a>, in the book <em>Greedoids</em> by Korte, Schrader, and Lovász, includes another graph-theoretic example where the elements are edges of series-parallel graphs. The part that appears to be less standard is the use of this property to explain the ability of greedy algorithms to recognize cop-win and distance-hereditary graphs. I looked, but was unable to find publications observing that these two classes of graphs lead to greedoids or transposition greedoids, despite some suspiciously-similar terminology (“twins”, “dismantling”) on both sides. If anyone knows of such publications, I’d appreciate hearing of them, so that I could add this connection to their Wikipedia articles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106495619434427598">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html"><span class="datestr">at June 29, 2021 12:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/091">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/091">TR21-091 |  Expander Random Walks: The General Case and Limitations | 

	Gil Cohen, 

	Dor Minzer, 

	Shir Peleg, 

	Aaron Potechin, 

	Amnon Ta-Shma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Cohen, Peri and Ta-Shma (STOC'21) considered the following question: Assume the vertices of an expander graph are labelled by $\pm 1$. What "test" functions $f : \{\pm 1\}^t \to \{\pm1 \}$ can or cannot distinguish $t$ independent samples from those obtained by a random walk? [CPTS'21] considered only balanced labelling, and proved that all symmetric functions are fooled by random walks on expanders with constant spectral gap. Furthermore, it was shown that functions computable by $\mathbf{AC}^0$ circuits are fooled by expanders with vanishing spectral expansion. 

We continue the study of this question and, in particular, resolve all open problems raised by [CPTS'21]. First, we generalize the result to all labelling, not merely balanced. In doing so, we improve the known bound for symmetric functions and prove that the bound we obtain is optimal (up to a multiplicative constant). Furthermore, we prove that a random walk on expanders with constant spectral gap does not fool $\mathbf{AC}^0$. In fact, we prove that the bound obtained by [CPTS'21] for $\mathbf{AC}^0$ circuits is optimal up to a polynomial factor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/091"><span class="datestr">at June 29, 2021 07:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5479161699112157490">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html">Someone thinks I am a fine artist! Why?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A while back I got an  email asking me to submit to a Fine Arts Journal. Why me? Here are some possibilities:</p><p>1) They were impressed with my play: </p><p><b>Sure he created the universe, but would he get Tenure?</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/god.html">here</a>) which did get into a play-writing contest and was performed (one of the actresses  scolded me since I took a slot from <i>a real</i> <i>playwrigh</i>t).</p><p>2) They were impressed  with my <b>Daria Fan Fiction</b> (see the four entries <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/mywritings.html">here</a> labelled as Daria Fan Fiction).</p><p>3) They were impressed with my play <b>JFK: The Final chapter</b> (see <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/jfk.html">here</a>). Unlikely since this was rejected by a play writing contest and is not well known (as opposed to my other works in the fine arts which are well known?)</p><p>4) They were impressed with my collection of satires of  Nobel Laureate Bob Dylan (<a href="https://www.cs.umd.edu/users/gasarch/dylan/dylan.html">here</a>) .</p><p>5) They were impressed with some subset of (a) complexityblog, (b) <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279729/ref=sr_1_3?dchild=1&amp;keywords=gasarch&amp;qid=1609867944&amp;sr=8-3" target="_blank">Problems with a Point</a>,  (c)  <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215979/ref=sr_1_2?dchild=1&amp;keywords=gasarch&amp;qid=1609868018&amp;sr=8-2">Mathematical Muffin Morsels</a>, and (d) <a href="https://www.amazon.com/Bounded-Queries-Recursion-Progress-Computer-ebook/dp/B000W98WU4/ref=sr_1_4?dchild=1&amp;keywords=gasarch&amp;qid=1609868084&amp;sr=8-4">Bounded Queries in Recursion Theory</a>. Or maybe just having 3 books on amazon is their threshold.  If it's complexityblog then Lance and I should co-author something for them.</p><p>6) It is a vanity-journal where you pay to  publish. So why email me who (a)  is not an artist, (b) is  not a fine artist, and most important (3) <b>does not think of himself as a fine artist</b>. The PRO of emailing me or people like me is they cast a wide net. The CON is--- there is no CON! It costs nothing to email me, and emailing me does not affect their credibility. That still raises the question of how they got my name.</p><p>7) Could it be a phishing? If I click on something in the email would they  get my credit card number? Their email begins <i>Dear Professor</i> not  <i>Dear Professor Gasarch. </i>  So they know I am a professor. Then again, I have known of ugrads who get emails that begin <i>Dear Professor</i>. (The emails to HS student Naveen and ugrad Nichole in the story I tell <a href="https://blog.computationalcomplexity.org/search?q=Naveen">here</a> were addressed to <i>Dear Professor.) </i></p><p>8) They mistook me for my parents who, in 1973,  put together an anthology of short stories titled <i>Fiction:The Universal elements</i>,  for a Freshman Comp course my mom taught, see <a href="https://www.amazon.com/Fiction-Universal-Element-P-Gasarch/dp/0442226322">here</a>. I note that their book ranks around 18,000,000, so even that explanation is unlikely. Actually the rank changes a lot- it was 12,000,000 this morning. Still, not what one would call a best seller. It's fun to see what is doing better: <i>Bounded Queries in Recursion Theory (currently at around rank 6.000.000) </i> or <i>Fiction: The Universal Elements.</i></p><p> If I ever get one of these emails from a History Journal I will submit my Satirical <i>Ramsey Theory and the History of Pre-Christian England: An Example of Interdisciplinary Research</i> (see <a href="https://www.cs.umd.edu/~gasarch/COURSES/389/W14/ramseykings.pdf">here</a>) just to see what happens- but  I will stop short of paying-to-publish. Or maybe I will pay-to-publish so that the next time I try to fool a class with it I can point to a seemingly real journal which has the article. </p><p><br /></p><div style="background-color: white; color: #222222; cursor: auto; font-family: Roboto, RobotoDraft, Helvetica, Arial, sans-serif; font-size: 0.875rem; padding: 20px 0px 0px;" class="gE iv gt"><table cellpadding="0" class="cf gJ"><tbody style="display: block;"><tr style="display: flex; height: auto;" class="acZ"><td style="display: block; line-height: 20px; margin: 0px; padding: 0px; vertical-align: top; white-space: nowrap; width: 571.172px;" class="gF gK"><br /></td></tr></tbody></table></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html"><span class="datestr">at June 28, 2021 03:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/090">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/090">TR21-090 |  On Secret Sharing, Randomness, and Random-less Reductions for Secret Sharing | 

	Divesh Aggarwal, 

	Maciej Obremski, 

	Eldon Chung, 

	Joao Ribeiro</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Secret-sharing is one of the most basic and oldest primitives in cryptography, introduced by Shamir and Blakely in the 70s. It allows to strike a meaningful balance between availability and confidentiality of secret information. It has a host of applications most notably in threshold cryptography and multi-party computation. All known constructions of secret sharing (with the exception of those with a pathological choice of parameters) require access to uniform randomness. In practice, it is extremely challenging to generate a source of uniform randomness. This has led to a large body of research devoted to designing randomized algorithms and cryptographic primitives from imperfect sources of randomness.

Motivated by this, 15 years ago, Bosley and Dodis asked whether it is even possible to build 2-out-of-2 secret sharing without access to uniform randomness. In this work, we make progress towards resolving this question.

We answer this question for secret sharing schemes with important additional properties, i.e., either leakage-resilience or non-malleability. We prove that, unfortunately, for not too small secrets, it is impossible to construct any of 2-out-of-2 leakage-resilient secret sharing or 2-out-of-2 non-malleable secret sharing without access to uniform randomness.

Given that the problem whether 2-out-of-2 secret sharing requires uniform randomness has been open for a long time, it is reasonable to consider intermediate problems towards resolving the open question. In a spirit similar to NP-completeness, we study how the existence of a t-out-of-n secret sharing without access to uniform randomness is related to the existence of a t'-out-of-n' secret sharing without access to uniform randomness for a different choice of the parameters t,n,t',n'.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/090"><span class="datestr">at June 27, 2021 05:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/089">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/089">TR21-089 |  A Relativization Perspective on Meta-Complexity | 

	Rahul Santhanam, 

	Hanlin Ren</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Meta-complexity studies the complexity of computational problems about complexity theory, such as the Minimum Circuit Size Problem (MCSP) and its variants. We show that a relativization barrier applies to many important open questions in meta-complexity. We give relativized worlds where:

* MCSP can be solved in deterministic polynomial time, but the search version of MCSP cannot be solved in deterministic polynomial time, even approximately. In contrast, Carmosino, Impagliazzo, Kabanets, Kolokolova [CCC'16] gave a randomized approximate search-to-decision reduction for MCSP with a relativizing proof.

* The complexities of MCSP[2^{n/2}] and MCSP[2^{n/4}] are different, in both worst-case and average-case settings. Thus the complexity of MCSP is not "robust" to the choice of the size function.

* Levin's time-bounded Kolmogorov complexity Kt(x) can be approximated to a factor (2+epsilon) in polynomial time, for any epsilon &gt; 0.

* Natural proofs do not exist, and neither do auxiliary-input one-way functions. In contrast, Santhanam [ITCS'20] gave a relativizing proof that the non-existence of natural proofs implies the existence of one-way functions under a conjecture about optimal hitting sets.

* DistNP does not reduce to GapMINKT by a family of "robust" reductions. This presents a technical barrier for solving a question of Hirahara [FOCS'20].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/089"><span class="datestr">at June 25, 2021 03:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
