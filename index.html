<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 25, 2021 02:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10822">Lower Bounds for Maximally Recoverable Tensor Code and Higher Order MDS Codes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopi:Sivakanth.html">Sivakanth Gopi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makam:Visu.html">Visu Makam</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10822">PDF</a><br /><b>Abstract: </b>An $(m,n,a,b)$-tensor code consists of $m\times n$ matrices whose columns
satisfy `$a$' parity checks and rows satisfy `$b$' parity checks (i.e., a
tensor code is the tensor product of a column code and row code). Tensor codes
are useful in distributed storage because a single erasure can be corrected
quickly either by reading its row or column. Maximally Recoverable (MR) Tensor
Codes, introduced by Gopalan et al., are tensor codes which can correct every
erasure pattern that is information theoretically possible to correct. The main
questions about MR Tensor Codes are characterizing which erasure patterns are
correctable and obtaining explicit constructions over small fields.
</p>
<p>In this paper, we study the important special case when $a=1$, i.e., the
columns satisfy a single parity check equation. We introduce the notion of
higher order MDS codes (MDS$(\ell)$ codes) which is an interesting
generalization of the well-known MDS codes, where $\ell$ captures the order of
genericity of points in a low-dimensional space. We then prove that a tensor
code with $a=1$ is MR iff the row code is an MDS$(m)$ code. We then show that
MDS$(m)$ codes satisfy some weak duality. Using this characterization and
duality, we prove that $(m,n,a=1,b)$-MR tensor codes require fields of size
$q=\Omega_{m,b}(n^{\min\{b,m\}-1})$. Our lower bound also extends to the
setting of $a&gt;1$. We also give a deterministic polynomial time algorithm to
check if a given erasure pattern is correctable by the MR tensor code (when
$a=1$).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10822"><span class="datestr">at July 24, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10797">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10797">Fourier growth of structured $\mathbb{F}_2$-polynomials and applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jarosław Błasiok, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ivanov:Peter.html">Peter Ivanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Yaonan.html">Yaonan Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Chin_Ho.html">Chin Ho Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Servedio:Rocco_A=.html">Rocco A. Servedio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viola:Emanuele.html">Emanuele Viola</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10797">PDF</a><br /><b>Abstract: </b>We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$
(denoted $L_{1,k}$), of various well-studied classes of "structured"
$\mathbb{F}_2$-polynomials. This study is motivated by applications in
pseudorandomness, in particular recent results and conjectures due to
[CHHL19,CHLT19,CGLSS20] which show that upper bounds on Fourier growth (even at
level $k=2$) give unconditional pseudorandom generators.
</p>
<p>Our main structural results on Fourier growth are as follows:
</p>
<p>- We show that any symmetric degree-$d$ $\mathbb{F}_2$-polynomial $p$ has
$L_{1,k}(p) \le \Pr[p=1] \cdot O(d)^k$, and this is tight for any constant $k$.
This quadratically strengthens an earlier bound that was implicit in [RSV13].
</p>
<p>- We show that any read-$\Delta$ degree-$d$ $\mathbb{F}_2$-polynomial $p$ has
$L_{1,k}(p) \le \Pr[p=1] \cdot (k \Delta d)^{O(k)}$.
</p>
<p>- We establish a composition theorem which gives $L_{1,k}$ bounds on disjoint
compositions of functions that are closed under restrictions and admit
$L_{1,k}$ bounds.
</p>
<p>Finally, we apply the above structural results to obtain new unconditional
pseudorandom generators and new correlation bounds for various classes of
$\mathbb{F}_2$-polynomials.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10797"><span class="datestr">at July 24, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10777">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10777">Randomized Online Algorithms for Adwords</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vazirani:Vijay_V=.html">Vijay V. Vazirani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10777">PDF</a><br /><b>Abstract: </b>The general adwords problem has remained largely unresolved. We define a
subcase called {\em $k$-TYPICAL}, $k \in \Zplus$, as follows: the total budget
of all the bidders is sufficient to buy $k$ bids for each bidder. This seems a
reasonable assumption for a ``typical'' instance, at least for moderate values
of $k$. We give a randomized online algorithm achieving a competitive ratio of
$\left(1 - {1 \over e} - {1 \over k} \right) $ for this problem. We also give
randomized online algorithms for other special cases of adwords.
</p>
<p>The key to these results is a simplification of the proof for RANKING, the
optimal algorithm for online bipartite matching, given in \cite{KVV}. Our
algorithms for adwords can be seen as natural extensions of RANKING.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10777"><span class="datestr">at July 24, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10764">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10764">Nonlinear transformation of complex amplitudes via quantum singular value transformation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Naixu Guo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitarai:Kosuke.html">Kosuke Mitarai</a>, Keisuke Fujii <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10764">PDF</a><br /><b>Abstract: </b>Due to the linearity of quantum operations, it is not straightforward to
implement nonlinear transformations on a quantum computer, making some
practical tasks like a neural network hard to be achieved. In this work, we
define a task called nonlinear transformation of complex amplitudes and provide
an algorithm to achieve this task. Specifically, we construct a block-encoding
of complex amplitudes from a state preparation oracle. This allows us to
transform the complex amplitudes by using quantum singular value
transformation. We evaluate the required overhead in terms of input dimension
and precision, which reveals that the algorithm depends on the roughly square
root of input dimension and achieves an exponential speedup on precision
compared with previous work. We also discuss its possible applications to
quantum machine learning, where complex amplitudes encoding classical or
quantum data are processed by the proposed method. This paper provides a
promising way to introduce highly complex nonlinearity of the quantum states,
which is essentially missing in quantum mechanics.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10764"><span class="datestr">at July 24, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10689">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10689">Testing isomorphism of chordal graphs of bounded leafage is fixed-parameter tractable</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arvind:Vikraman.html">Vikraman Arvind</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nedela:Roman.html">Roman Nedela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Ponomarenko:Ilia.html">Ilia Ponomarenko</a>, Peter Zeman <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10689">PDF</a><br /><b>Abstract: </b>It is known that testing isomorphism of chordal graphs is as hard as the
general graph isomorphism problem. Every chordal graph can be represented as
the intersection graph of some subtrees of a tree. The leafage of a chordal
graph, is defined to be the minimum number of leaves in the representing tree.
We construct a fixed-parameter tractable algorithm testing isomorphism of
chordal graphs with bounded leafage. The key point is a fixed-parameter
tractable algorithm finding the automorphism group of a colored order-3
hypergraph with bounded sizes of color classes of vertices.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10689"><span class="datestr">at July 24, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10675">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10675">Theory and Practice of Algorithm Engineering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mendling:Jan.html">Jan Mendling</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Depaire:Beno=icirc=t.html">Benoît Depaire</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leopold:Henrik.html">Henrik Leopold</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10675">PDF</a><br /><b>Abstract: </b>There is an ongoing debate in computer science how algorithms should best be
studied. Some scholars have argued that experimental evaluations should be
conducted, others emphasize the benefits of formal analysis. We believe that
this debate less of a question of either-or, because both views can be
integrated into an overarching framework. It is the ambition of this paper to
develop such a framework of algorithm engineering with a theoretical foundation
in the philosophy of science. We take the empirical nature of algorithm
engineering as a starting point. Our theoretical framework builds on three
areas discussed in the philosophy of science: ontology, epistemology and
methodology. In essence, ontology describes algorithm engineering as being
concerned with algorithmic problems, algorithmic tasks, algorithm designs and
algorithm implementations. Epistemology describes the body of knowledge of
algorithm engineering as a collection of prescriptive and descriptive
knowledge, residing in World 3 of Popper's Three Worlds model. Methodology
refers to the steps how we can systematically enhance our knowledge of specific
algorithms. In this context, we identified seven validity concerns and discuss
how researchers can respond to falsification. Our framework has important
implications for researching algorithms in various areas of computer science.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10675"><span class="datestr">at July 24, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10654">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10654">Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fahrbach:Matthew.html">Matthew Fahrbach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghadiri:Mehrdad.html">Mehrdad Ghadiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fu:Thomas.html">Thomas Fu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10654">PDF</a><br /><b>Abstract: </b>Low-rank tensor decomposition generalizes low-rank matrix approximation and
is a powerful technique for discovering low-dimensional structure in
high-dimensional data. In this paper, we study Tucker decompositions and use
tools from randomized numerical linear algebra called ridge leverage scores to
accelerate the core tensor update step in the widely-used alternating least
squares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,
is a highly-structured ridge regression problem where the design matrix is a
Kronecker product of the factor matrices. We show how to use approximate ridge
leverage scores to construct a sketched instance for any ridge regression
problem such that the solution vector for the sketched problem is a
$(1+\varepsilon)$-approximation to the original instance. Moreover, we show
that classical leverage scores suffice as an approximation, which then allows
us to exploit the Kronecker structure and update the core tensor in time that
depends predominantly on the rank and the sketching parameters (i.e., sublinear
in the size of the input tensor). We also give upper bounds for ridge leverage
scores as rows are removed from the design matrix (e.g., if the tensor has
missing entries), and we demonstrate the effectiveness of our approximate ridge
regressioni algorithm for large, low-rank Tucker decompositions on both
synthetic and real-world data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10654"><span class="datestr">at July 24, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10516">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10516">The Stationary Prophet Inequality Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Kristen Kessel, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saberi:Amin.html">Amin Saberi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shameli:Ali.html">Ali Shameli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wajc:David.html">David Wajc</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10516">PDF</a><br /><b>Abstract: </b>We study a continuous and infinite time horizon counterpart to the classic
prophet inequality, which we term the stationary prophet inequality problem.
Here, copies of a good arrive and perish according to Poisson point processes.
Buyers arrive similarly and make take-it-or-leave-it offers for unsold items.
The objective is to maximize the (infinite) time average revenue of the seller.
</p>
<p>Our main results are pricing-based policies which (i) achieve a
$1/2$-approximation of the optimal offline policy, which is best possible, and
(ii) achieve a better than $(1-1/e)$-approximation of the optimal online
policy. Result (i) improves upon bounds implied by recent work of Collina et
al. (WINE'20), and is the first optimal prophet inequality for a stationary
problem. Result (ii) improves upon a $1-1/e$ bound implied by recent work of
Aouad and Sarita\c{c} (EC'20), and shows that this prevalent bound in online
algorithms is not optimal for this problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10516"><span class="datestr">at July 24, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10454">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10454">The Traveling Firefighter Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farhadi:Majid.html">Majid Farhadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Toriello:Alejandro.html">Alejandro Toriello</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tetali:Prasad.html">Prasad Tetali</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10454">PDF</a><br /><b>Abstract: </b>We introduce the $L_p$ Traveling Salesman Problem ($L_p$-TSP), given by an
origin, a set of destinations, and underlying distances. The objective is to
schedule a destination visit sequence for a traveler of unit speed to minimize
the Minkowski $p$-norm of the resulting vector of visit/service times. For $p =
\infty$ the problem becomes a path variant of the TSP, and for $p = 1$ it
defines the Traveling Repairman Problem (TRP), both at the center of classical
combinatorial optimization. We provide an approximation preserving
polynomial-time reduction of $L_p$-TSP to the segmented-TSP Problem [Sitters
'14] and further study the case of $p = 2$, which we term the Traveling
Firefighter Problem (TFP), when the cost due to a delay in service is quadratic
in time.
</p>
<p>We also study the all-norm-TSP problem [Golovin et al. '08], in which the
objective is to find a route that is (approximately) optimal with respect to
the minimization of any norm of the visit times, and improve corresponding
(in)approximability bounds on metric spaces.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10454"><span class="datestr">at July 24, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10450">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10450">Learning Sparse Fixed-Structure Gaussian Bayesian Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Arnab Bhattacharyya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choo:Davin.html">Davin Choo</a>, Rishikesh Gajjala, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gayen:Sutanu.html">Sutanu Gayen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yuhao.html">Yuhao Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10450">PDF</a><br /><b>Abstract: </b>Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation
models) are widely used to model causal interactions among continuous
variables. In this work, we study the problem of learning a fixed-structure
Gaussian Bayesian network up to a bounded error in total variation distance. We
analyze the commonly used node-wise least squares regression (LeastSquares) and
prove that it has a near-optimal sample complexity. We also study a couple of
new algorithms for the problem:
</p>
<p>- BatchAvgLeastSquares takes the average of several batches of least squares
solutions at each node, so that one can interpolate between the batch size and
the number of batches. We show that BatchAvgLeastSquares also has near-optimal
sample complexity.
</p>
<p>- CauchyEst takes the median of solutions to several batches of linear
systems at each node. We show that the algorithm specialized to polytrees,
CauchyEstTree, has near-optimal sample complexity.
</p>
<p>Experimentally, we show that for uncontaminated, realizable data, the
LeastSquares algorithm performs best, but in the presence of contamination or
DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares
respectively perform better.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10450"><span class="datestr">at July 24, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.10339">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.10339">Finding surfaces in simplicial complexes with bounded-treewidth 1-skeleton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Black:Mitchell.html">Mitchell Black</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayyeri:Amir.html">Amir Nayyeri</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.10339">PDF</a><br /><b>Abstract: </b>We consider the problem 2-Dim-Bounding-Surface. 2-Dim-Bounded-Surface asks
whether or not there is a subcomplex $S$ of a simplicial complex $K$
homeomorphic to a given compact, connected surface bounded by a given
subcomplex $B\subset K$. 2-Dim-Bounding-Surface is NP-hard. We show it is
fixed-parameter tractable with respect to the treewidth of the 1-skeleton of
the simplicial complex $K$. Using some of the techniques we developed for the
2-Dim-Bounded-Surface problem, we obtain fixed parameter tractable algorithms
for other topological problems such as computing an optimal chain with a given
boundary and computing an optimal chain in a given homology class.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.10339"><span class="datestr">at July 24, 2021 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/109">TR21-109 |  QRAT Polynomially Simulates Merge Resolution. | 

	Sravanthi Chede, 

	Anil Shukla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Merge Resolution (MRes [Beyersdorff et al. J. Autom. Reason.'2021] ) is a refutational proof system for quantified Boolean formulas (QBF). Each line of MRes consists of clauses with only existential literals, together with information of countermodels stored as merge maps. As a result, MRes has strategy extraction by design. The QRAT [Heule et al. J. Autom. Reason.'2017] proof system was designed to capture QBF preprocessing. QRAT can simulate both the expansion-based proof system $\forall$Exp+Res and CDCL-based QBF proof system LD-Q-Res. 

A family of false QBFs called SquaredEquality formulas were introduced in [Beyersdorff et al. J. Autom. Reason.'2021] and shown to be easy for MRes but need exponential size proofs in Q-Res, QU-Res, CP+$\forall$red, $\forall$Exp+Res, IR-calc and reductionless LD-Q-Res. As a result none of these systems can simulate MRes. In this paper, we show a short QRAT refutation of the SquaredEquality formulas. We further show that QRAT strictly p-simulates MRes. 
Besides highlighting the power of QRAT system, this work also presents the first simulation result for MRes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/109"><span class="datestr">at July 23, 2021 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">Postdoc at University of Vienna (apply by August 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two full-time 3-year postdoc positions in algorithms are available starting Jan 1, 2022 as part of the ERC Advanced Grant “Modern Dynamic Data Structures” to joint the algorithms research group headed by Monika Henzinger. Prior knowledge in fair algorithms and differential privacy is a plus. Please send your CV, a letter of motivation, and the names of 3 references to applications.taa@univie.ac.at</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br />
Email: applications.taa@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/"><span class="datestr">at July 23, 2021 06:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5690">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/">A Basic Question About Multiplicative Energy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As part of some research project, I got to a basic question about multiplicative energy. Embarrassingly , I wasn’t able to get any non-trivial bound for it. Here is the problem. Any information about it would be highly appreciated. Problem. Let . Let be a set of real numbers. How large can the multiplicative energy […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/"><span class="datestr">at July 23, 2021 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6975981189321037535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html">Technical Difficulties</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s4032/PXL_20210711_224807389.jpg"><img width="320" border="0" src="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s320/PXL_20210711_224807389.jpg" /></a></div><br /><p>After returning from vacation last weekend (hello North Dakota--my 49th state visited), all sorts of odd problems arose. This blog stopped working, a P v NP paper was published on the ACM Transactions of Computing website and my personal emails were getting marked as spam. All is better, I hope.</p><p>Years ago I donated the URL computationalcomplexity.org to the <a href="https://www.computationalcomplexity.org/">Computational Complexity Conference</a>, coincidentally held this past week, with the condition that I could continue to use the "blog" subdomain for this blog. Organizations continue but the people in them change, and when the website was "upgraded" on Saturday the pointers to make this blog work were left out. Thanks to Ashwin Nayak for getting it all straightened out and we're back online.</p><p>For the ToCT paper, a paper claiming to reduce 3-SAT to 2-SAT, and thus show P = NP, was originally rejected by the journal but a "disposition field" got inadvertently set to accept and wasn't caught until it showed up online. Editor-in-Chief Ryan O'Donnell quickly got on the case and ACM has <a href="https://dl-acm-org.ezproxy.gl.iit.edu/doi/10.1145/3460950">removed the paper</a>. P v NP remains as open as ever.</p><p>Fixing the email required me to learn far more about <a href="https://en.wikipedia.org/wiki/Sender_Policy_Framework">SPFs</a> than I ever wanted to know.</p><p>By the way if anyone in Idaho wants to invite me to give a talk, I might be interested.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html"><span class="datestr">at July 23, 2021 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">Average-Case Fine-Grained Hardness, Part I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I recently finished my qualifying exam for which I surveyed worst-case to average-case reductions for problems in NP. As part of my survey, I reviewed recent results on average-case hardness in P (a.k.a. average-case fine-grained hardness). I would like to give an overview of some of these results in a series of blog posts, and I want to start by giving some motivations.</p>



<p><strong>Why do we care about average-case fine-grained hardness?</strong></p>



<p>(i) We hope to explain why we are struggling to find faster algorithms for problems in P such as DNA sequencing even for some random large datasets.</p>



<p>(ii) Average-case hard problems in P is useful to cryptography. For example, constructing <a href="https://en.wikipedia.org/wiki/One-way_function">one-way functions</a> (i.e., functions that are easy to compute but hard to invert) based on worst-case complexity assumptions such as NP <img src="https://s0.wp.com/latex.php?latex=%5Cnot%5Csubseteq&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\not\subseteq" class="latex" /> BPP has been a long-standing open question. In this context, “easy” means polynomial-time computable. If we consider “easy” to be computable in time like <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B100%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{100})" class="latex" /> instead, then a natural alternative question is whether we can construct such one-way functions based on worst-case fine-grained complexity assumptions. Another example is <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof of work</a> <a href="https://link.springer.com/chapter/10.1007/3-540-48071-4_10">[DN92]</a>. When a miner tries to mine the <a href="https://en.wikipedia.org/wiki/Cryptocurrency">cryptocurrency</a>, the miner is asked to solve a random puzzle, that is average-case hard but still tractable, and then the miner needs to prove they have indeed solved the puzzle through efficient protocols. An interesting and more recent follow-up is proof of useful work <a href="https://eprint.iacr.org/2017/203.pdf">[BRSV17b]</a>, which proposes that instead of wasting computing power on a random puzzle that comes from nowhere, we can first reduce a computational task of practical interest to multiple random puzzles and then ask the miner to solve those puzzles.</p>



<p>The most common approach for proving average-case fine-grained hardness is arguably worst-case to average-case reduction, i.e., reducing an arbitrary instance to a number of random instances of which the distribution is polynomial-time sampable. Before I give concrete examples, I want to describe a general recipe for designing such worst-case to average-case reductions. (Some reader might notice that the step 3 of the recipe is same as the argument for proving <a href="https://en.wikipedia.org/wiki/Random_self-reducibility#Permanent_of_a_matrix">computing permanent is self-reducible</a> <a href="https://www.semanticscholar.org/paper/New-Directions-In-Testing-Lipton/fb2eba4d69bdab34c2d240380d4370be2feeacb9">[L91]</a>, which essentially uses <a href="https://en.wikipedia.org/wiki/Locally_decodable_code#The_Reed%E2%80%93Muller_code">local decoding for Reed-Muller codes</a>.)</p>



<p><strong>General recipe for worst-case to average-case reductions:</strong></p>



<ol><li>Choose our favorite hard problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}" class="latex" /> in P.</li><li>Construct a low-degree (degree-<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />) polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^n" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li><li>To solve <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />: sample a uniformly random <img src="https://s0.wp.com/latex.php?latex=y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y\in\mathbf{F}_{p}^n" class="latex" />, compute <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%2Bt_1+y%29%2C%5Cdots%2Cf_%7BL%7D%28x%2Bt_%7Bd%2B1%7D+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x+t_1 y),\dots,f_{L}(x+t_{d+1} y)" class="latex" /> for distinct nonzero <img src="https://s0.wp.com/latex.php?latex=t_1%2C%5Cdots%2Ct_%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t_1,\dots,t_{d+1}" class="latex" /> using average-case solver (note each <img src="https://s0.wp.com/latex.php?latex=x%2Bt_%7Bi%7D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x+t_{i} y" class="latex" /> is uniformly random), interpolate univariate polynomial <img src="https://s0.wp.com/latex.php?latex=g%28t%29%3A%3Df_%7BL%7D%28x%2Bt+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(t):=f_{L}(x+t y)" class="latex" /> using these points, and output <img src="https://s0.wp.com/latex.php?latex=g%280%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(0)" class="latex" /> which is <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" />. (This step can be replaced by decoding algorithms for <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> to handle larger errors of average-case solver.)</li><li>(The above steps already show that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> uniformly random inputs is as hard as solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case input.) If we want to show <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> itself is average-case fine-grained hard, it suffices to give a reduction from computing <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\mathbf{F}_{p}^n" class="latex" /> back to solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />.</li></ol>



<p>Notice that the above general recipe reduces a worst-case instance to <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> random instances at the step 3, and thus, we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be small (like <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />) such that it does not blow up the total runtime significantly. Typically, the step 4 would also blow up the runtime, and sometimes it depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />. For all the problems (or the techniques) in this series, I will explicitly quantify the runtime blow-up in the step 4 (if there is any) and explain how small we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be (if the blow-up depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />).</p>



<p>Now let us go through a concrete example. Consider one of the flagship problems in fine-grained complexity — orthogonal vector problem (OV): Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%27%5Cin%5Comega%28%5Clog+n%29%5Ccap+n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'\in\omega(\log n)\cap n^{o(1)}" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. It is known OV has no sub-quadratic algorithm assuming strong <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential-time hypothesis</a> (SETH) <a href="https://link.springer.com/chapter/10.1007/978-3-540-27836-8_101">[W05]</a>, and there is a generalization of OV called <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV problem that has no <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7Bk-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{k-o(1)})" class="latex" /> algorithm under the same assumption <a href="https://dl.acm.org/doi/10.1145/3300150.3300158">[W18]</a>.</p>



<p><strong>Polynomial evaluation.</strong> Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=d%27%3Dn%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'=n^{o(1)}" class="latex" />, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=2d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2d'" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D%28X%2CY%29%3A%3D%5Csum_%7Bi%2Cj%5Cin%5Bn%5D%7D%5Cprod_%7Bt%5Cin%5Bd%27%5D%7D%281-x_%7Bi%2Ct%7Dy_%7Bi%2Ct%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}(X,Y):=\sum_{i,j\in[n]}\prod_{t\in[d']}(1-x_{i,t}y_{i,t})" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7B2nd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{2nd'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2Ct%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_{i,t}" class="latex" /> denotes the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-th coordinate of <img src="https://s0.wp.com/latex.php?latex=x_i%5Cin+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i\in X" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> simply enumerates all the pairs of vectors and counts the number of orthogonal pairs for the OV instance, and obviously counting is at least as hard as decision. Using the aforementioned general recipe, it follows immediately that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> requires <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-o(1)})" class="latex" /> time for average case assuming randomized version of SETH. This result was shown in <a href="https://eprint.iacr.org/2017/202.pdf">[BRSV17a]</a>, and analogously, they constructed a polynomial for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV, which implies an average-case time hierarchy assuming randomized SETH.</p>



<p>However, the polynomial evaluation problem is algebraic. Next, let us consider a natural combinatorial problem — counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertices graph (for simplicity, think of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> as a large constant). This problem has worst-case complexity <img src="https://s0.wp.com/latex.php?latex=n%5E%7B%5CTheta%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{\Theta(t)}" class="latex" /> assuming ETH <a href="https://core.ac.uk/download/pdf/82508832.pdf">[CHKX06]</a>.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques.</strong> It was first shown in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{4}" class="latex" /> in some polynomial-time sampable random graph. The proof also follows our general recipe. First, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%3A%3D%5Csum_%7B%5Ctextrm%7Bsize-%7Dt%5C%2C+T%5Csubseteq+%5Bn%5D%7D%5Cprod_%7Bi%3Cj%5Cin+T%7D+X_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X):=\sum_{\textrm{size-}t\, T\subseteq [n]}\prod_{i&lt;j\in T} X_{i,j}" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{n^2}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> is the adjacency matrix. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by enumerating each size-<img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> subset of vertices and checking whether it is a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. It remains to work out the step 4 of the general recipe. This was done by a gadget reduction, that runs in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, from evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" />-vertices graph <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a>.</p>



<p>Although this gadget reduction is nice, I will not explain it here, because later works <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a> show that if the polynomial constructed at the step 2 has certain structure, then there is a general technique to reduce evaluating this polynomial back to the original problem (at the cost of requiring smaller error probability of average-case solver), which I will discuss in the next post. Finally, let me point out an issue in the previous paragraph — <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is too large for the gadget reduction to be useful! We need <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" /> (note <img src="https://s0.wp.com/latex.php?latex=n%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^t" class="latex" /> is a trivial upper bound of the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques) such that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> indeed outputs the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, but the gadget reduction takes <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, and moreover, we do not know how to find a prime <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" /> time. This issue was handled in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> using <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a>. Specifically, we choose <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> many distinct primes <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" />‘s upper bounded by <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> whose product is <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> (the existence of such primes follows from <a href="https://en.wikipedia.org/wiki/Prime_number_theorem">asymptotic distribution of the prime numbers</a>). Then, we apply the whole reduction described so far to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%5Cmod+p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)\mod p_i" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" /> and then use Chinese remainder theorem to recover <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)" class="latex" />. (We can use Chinese remaindering with errors <a href="https://dl.acm.org/doi/10.1109/18.850672">[GRS99]</a> to handle larger error probability.)</p>



<p>Hopefully, the above examples give you a flavor of worst-case to average-case reductions for fine-grained hard problems. As promised, in the next post, I will continue to discuss the new technique in  <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a>, which automatizes the step 4 of the general recipe by requiring more structural properties for the polynomial constructed in the step 2 of the general recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p>



<p><br /></p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/"><span class="datestr">at July 23, 2021 01:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6127">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/laplace-method/">Approximating integrals with Laplace’s method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Integrals appear everywhere in all scientific fields, and their numerical computation is an active area of research. In the playbook of approximation techniques, my personal favorite is “la méthode de Laplace”, a must-know for students that like to cut integrals into pieces, that comes with lots of applications.</p>



<p class="justify-text">We will be concerned with integrals of the form $$I(t) =  \int_K h(x) e^{ \, – \, t f(x) } dx, $$ where \(K\) is compact (bounded and closed) subset of \(\mathbb{R}^d\), and \(h\) and \(f\) are two real-valued functions defined on \(K\) such that the integral is well defined for large enough \(t \in \mathbb{R}\). The goal is to obtain an asymptotic equivalent when \(t\) tends to infinity.</p>



<p class="justify-text">Within machine learning, as explained below, this is useful where computing and approximating integrals is important. This thus comes up naturally in Bayesian inference where \(t\) will be the number of observations in a statistical model. But let’s start with presenting this neat asymptotic result that Laplace discovered for a particular one-dimensional case in 1774 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">1</a>].</p>



<h2>Laplace approximation</h2>



<p>Let’s first state the main result. Assume that:</p>



<ul class="justify-text"><li>\(h\) is continuous on \(K\) and that \(f\) is twice continuously differentiable on \(K\).</li><li>\(f\) has a strict global minimizer \(x_\ast\) on \(K\), which is in the interior of \(K\), where the gradient \(f^\prime(x_\ast)\) is thus equal to zero, and where the Hessian \(f^{\prime \prime}(x_\ast)\) is a positive definite matrix (it is always positive semi-definite because \(x_\ast\) is a local minimizer of \(f\)); moreover, \(h(x_\ast) \neq 0\).</li></ul>



<p class="justify-text">Then, as \(t\) tends to infinity, we have the following asymptotic equivalent: $$ I(t) \sim \frac{h(x_\ast)}{\sqrt{ \det f^{\prime \prime}(x_\ast) }} \Big( \frac{2\pi}{t}\Big)^{d/2}  e^{ \, – \, t f(x_\ast) }.$$</p>



<p class="justify-text"><strong>Where does it come from?</strong> The idea is quite simple: for \(t&gt;0\), the exponential term \(e^{ \, – \, t f(x) }\) is largest when \(x\) is equal to the minimizer \(x_\ast\). Hence only contributions close to \(x_\ast\) will count in the integral. Then we can do Taylor expansions of the two functions around \(x_\ast\), as \(h(x) \approx h(x_\ast)\) and \(f(x) \approx f(x_\ast) + \frac{1}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\), and approximate \(I(t)\) as $$ I(t) \approx \int_K h(x_\ast) \exp\Big[ – t f(x_\ast) \ – \frac{t}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\Big] dx.$$ We can then make a change of variable \(y = \sqrt{t} f^{\prime \prime}(x_\ast)^{1/2}( x – x_\ast)\) (where \(f^{\prime \prime}(x_\ast)^{1/2}\) is the positive square root of \(f^{\prime \prime}(x_\ast)\)), to get, with the Jacobian of the transformation leading to the term \(  (\det f^{\prime \prime}(x_\ast) )^{1/2} t^{d/2} \): $$I(t) \approx \frac{ h(x_\ast) e^{ \, – \, t f(x_\ast) }}{( \det f^{\prime \prime}(x_\ast) )^{1/2}t^{d/2} } \int_{\sqrt{t}f^{\prime \prime}(x_\ast)^{1/2}( K \ – x_\ast)} \!\!\! \exp\big[ – \frac{1}{2} y^\top  y  \big] dy.$$ Since \(x_\ast\) is in the interior of \(K\), when \(t\) tends to infinity, the set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\) tends to \(\mathbb{R}^d\) (see illustration below), and we obtain the usual Gaussian integral that leads to the normalizing constant of the Gaussian distribution, which is equal to \((2\pi)^{d/2} \).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="544" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/Klaplace-1024x288.png" class="wp-image-6263" height="153" />Left: set \(K\) with \(x_\ast\) in its interior. Right: set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\), which is a translated (by \(x_\ast\)), tilted (by \(f^{\prime\prime}(x_\ast)\)) and scaled (by \(\sqrt{t}\)) version of \(K\).</figure></div>



<p class="justify-text"><strong>Formal proof.</strong> Let’s first make our life simpler: without loss of generality, we may assume that \(f(x_\ast) = 0\) (by subtracting the minimal value), that \(f^{\prime \prime}(x_\ast) = I\) (by a change of variable whose Jacobian is the square root of \(f^{\prime \prime}(x_\ast)\), leading to the determinant term), and that \(x_\ast = 0\). With the dominated convergence theorem (which was unfortunately unknown to me when I first learned about the method in high school, forcing students to cut integrals into multiple pieces), the proof sketch above is almost exact. We simply need a simple argument, based on the existence of a continuous function \(g: K \to \mathbb{R}\) such that $$ g(x) = \frac{f(x) } {\|  x  \|^2}$$ for \(x \neq 0\) and \(g(0) = \frac{1}{2}\) (here \(\| \! \cdot \! \|\) denotes the standard Euclidean norm). The function \(g\) is trivially defined and continuous on \(K \backslash \{0\}\), the value and continuity at zero is a simple consequence of the Taylor expansion $$ f(x) = f(0) + f^\prime(0)^\top x + \frac{1}{2} x^\top f^{\prime\prime}(0)x + o( \| x\|^2) = \frac{1}{2} \| x\|^2 + o( \| x\|^2). $$</p>



<p class="justify-text">We may then use the same change of variable as above to get the <em>equality</em>: $$I(t) =  \frac{ 1 }{ t^{d/2}  }  \int_{\sqrt{t}  K } h\big( \frac{1}{\sqrt{t}}  y\big) \exp\big[ – \frac{1}{2} y^\top y \cdot g\big(  \frac{1}{\sqrt{t}} y\big) \big] dy.$$ We can write the integral part of the expression above as $$J(t) = \int_{\mathbb{R}^d} a(y,t) dy, $$ with $$a(y,t) = 1_{ y \in \sqrt{t} K } h\big(  \frac{1}{\sqrt{t}}  y\big)\exp\big[ – \frac{1}{2} y^\top y \cdot g\big( \frac{1}{\sqrt{t}} y\big) \big].$$ We have for all \(t&gt;0\) and \(y \in \mathbb{R}^d\), \(|a(y,t)| \leqslant \max_{z \in K} | h(z)| \exp\Big[ – \| y\|^2 \cdot \min_{ z \in K } g(z) \Big]\), which is integrable because \(h\) is continuous on the compact set \(K\) and thus bounded, and \(g\) is strictly positive on \(K\) (since \(f\) is strictly positive except at zero as \(0\) is a strict global minimum), and by continuity, its minimal value is strictly positive. Thus by the <a href="https://en.wikipedia.org/wiki/Dominated_convergence_theorem">dominated convergence theorem</a>: $$\lim_{t \to +\infty} J(t) = \int_{\mathbb{R}^d} \big( \lim_{t \to +\infty} a(y,t) \big) dy = \int_{\mathbb{R}^d}\exp\big[ – \frac{1}{2} y^\top y \big] dy = ( 2\pi)^{d/2}.$$ This leads to the desired result since \(I(t) = J(t) / t^{d/2}\).</p>



<h2>Classical examples</h2>



<p>Two cute examples are often mentioned as applications (adapted from [2]).</p>



<p><strong><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s formula</a></strong>. We have, by definition of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a> \(\Gamma\), and the change of variable \(u = tx\):<br />$$\Gamma(1+t) = \int_0^\infty \!\! e^{-u} u^{t} du = \int_0^\infty \!\! e^{-tx}t^t x^t t dx = t^{t+1} \int_0^\infty \!\! e^{-t(x-\log x)} dx.$$ Since \(x \mapsto x\, – \log x\) is minimized at \(x=1\) with second derivative equal to \(1\), we get: $$\Gamma(1+t) \sim t^{t+1} e^{-t} \sqrt{2\pi / t} = \big( \frac{t}{e} \big)^t \sqrt{ 2\pi t}.,$$ which is exactly Stirling’s formula, often used when \(t\) is an integer, and then, \(\Gamma(1+t) = t!\).</p>



<p class="justify-text"><strong>Convergence of \(L_p\)-norms to the \(L_\infty\)-norm.</strong> We consider the \(L_p\)-norm of a positive twice continuously differentiable function on the compact set \(K\), with a unique global maximizer at \(x_\ast\) in the interior of \(K\). Then we can write its \(L_p\)-norm \(\|g\|_p\) through $$\| g\|_p^p = \int_K g(x)^p dx = \int_K e^{ p \log g(x)} dx.$$ The function \(f: x \mapsto\  – \log g(x)\) has gradient \(f^\prime(x) = \ – \frac{1}{g(x)}g^\prime(x)\) and Hessian \(f^{\prime\prime}(x)=\  – \frac{1}{g(x)} g^{\prime\prime}(x) + \frac{1}{g(x)^2} g^\prime(x) g^\prime(x)^\top .\) At \(x_\ast\), we get \(f^{\prime\prime}(x_\ast) = \ – \frac{1}{g(x_\ast)} g^{\prime \prime}(x_\ast)\). Thus, using Laplace’s method, we have: $$ \|g\|_p^p =  \frac{A}{p^{d/2}} g(x_\ast)^p (1 + o(1) ),$$ with \(\displaystyle A = \frac{(2\pi g(x_\ast))^{d/2}}{(\det (-g^{\prime\prime}(x_\ast)))^{1/2}}\).</p>



<p class="justify-text">Taking the power \(1/p\), we get: $$ \|g\|_p = \exp \big( \frac{1}{p} \log \|g\|_p^p\big) =  \exp \Big( \frac{1}{p} \log A \ – \frac{d}{2p} \log p +   \log g(x_\ast) + o(1/p) \Big).$$ This leads to, using \(\exp(u) = 1+u + o(u)\) around zero: $$ \| g\|_p = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + \frac{1}{p} \log A + o(1/p) \Big) = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + O(1/p) \Big).$$ A surprising fact is that the second-order term does not depend on anything but \(g(x_\ast)\) (beyond \(p\) and \(d\)). Note that this applies also to continuous log-sum-exp functions.</p>



<h2>Applications in Bayesian inference</h2>



<p class="justify-text">It turns out that Laplace’s motivation in deriving this general technique for approximating integrals was exactly Bayesian inference, which he in fact essentially re-discovered and extended (see an interesting account <a href="https://ebrary.net/118879/history/laplaces_bayesian_analysis_1774_1781">here</a>). Let me now explain how Laplace’s method applies.</p>



<p class="justify-text">We consider a parameterized family of probability distributions with density \(p(x|\theta)\) with respect to some base measure \(\mu\) on \(x \in \mathcal{X}\), with \(\theta \in \Theta \subset \mathbb{R}^d\) a set of parameters. </p>



<p class="justify-text">As a running example (the one from Laplace in 1774), we consider the classical Bernoulli distribution for \(x \in \mathcal{X} = \{0,1\}\), and densities with respect to the counting measure, that is, the parameter \(\theta \in [0,1]\) is the probability that \(x=1\).</p>



<p class="justify-text"><strong>From frequentist to Bayesian inference. </strong>We are given \(n\) independent and identically distributed observations \(x_1,\dots,x_n \in \mathcal{X}\), from an unknown probability distribution \(q(x)\). One classical goal of statistical inference is to find the parameter \(\theta \in \Theta\) so that \(p(\cdot| \theta)\) is as close to \(q\) as possible.</p>



<p class="justify-text">In the frequentist framework, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> amounts to maximizing $$ \sum_{i=1}^n \log p(x_i | \theta)$$ with respect to \(\theta \in \Theta\). It comes with a series of guarantees, in particular (but not only) when \(q = p(\cdot | \theta)\) for a certain \(\theta \in \Theta\). For our running example, the maximum likelihood estimator \(\hat{\theta}_{\rm ML} = \frac{1}{n} \sum_{i=1}^n x_i\) is the frequency of non-zero outcomes.</p>



<p class="justify-text">In the Bayesian framework, the data are assumed to be generated by a certain \(\theta\), but now with \(\theta\) itself random with some prior probability density \(p(\theta)\) (with respect to the Lebesgue measure). Statistical inference typically does not lead to a point estimator (like the maximum likelihood estimator), but to the full posterior distribution of the parameter \(\theta\) given the observations \(x_1,\dots,x_n\).</p>



<p class="justify-text">The celebrated <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a> states that the posterior density \(p(\theta | x_1,\dots,x_n)\) can be written as: $$p(\theta | x_1,\dots,x_n) = \frac{ p(\theta) \prod_{i=1}^n p(x_i|\theta) }{p(x_1,\dots,x_n)},$$ where \(p(x_1,\dots,x_n)\) is the marginal density of the data (once the parameter has been marginalized out).</p>



<p class="justify-text">If pressured, a Bayesian will end up giving you a point estimate, but a true Bayesian will not give away the maximum a posteriori estimate (see <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Limitations">here</a> for some reasons why), but if you give him or her a loss function (e.g., the square loss), he or she will give away (reluctantly) the estimate that minimizes the posterior risk (e.g., the posterior mean for the square loss).</p>



<p class="justify-text"><strong>Bernoulli and Beta distributions. </strong>In our running Bernoulli example for coin tossing, it is standard to put a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> on \(\theta \in [0,1]\), which is here a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> with parameters \(\alpha\) and \(\beta\), that is, $$p(\theta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1} ( 1- \theta)^{\beta-1}.$$ The posterior distribution is then also a Beta distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\beta + \sum_{i=1}^n ( 1- x_i)\). The posterior mean is $$\mathbb{E} [ \theta | x_1,\dots,x_n ] = \frac{\alpha+ \sum_{i=1}^n x_i}{n + \alpha + \beta},$$ and corresponds to having pre-observed \(\alpha\) ones and \(\beta\) zeroes (this is sometimes referred to as Laplace smoothing). However, it is rarely possible to compute the posterior distribution in closed form, hence the need for approximations.</p>



<p class="justify-text"><strong>Laplace approximation. </strong>Thus, in Bayesian inference, integrals of the form $$ \int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta$$ for some function \(h: \Theta \to \mathbb{R}\), are needed. For example, computing the marginal likelihood corresponds to \(h=1\).</p>



<p class="justify-text">By taking logarithms, we can write $$\int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta = \int_{\Theta} h(\theta) \exp\Big(  \log p(\theta) + \sum_{i=1}^n \log p(x_i|\theta) \Big) d\theta, $$ and with \(f_n(\theta) = \ – \frac{1}{n} \log p(\theta) \ – \frac{1}{n} \sum_{i=1}^n \log p(x_i|\theta),\) we have an integral in Laplace form, that is, $$\int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta,$$ with a function \(f_n\) that now varies with \(n\). This simple variation does not matter as because of the law of large numbers, when \(n\) is large, \(f_n(\theta)\) tends to a fixed function \(\mathbb{E} \big[ \log p(x|\theta) \big]\). </p>



<p class="justify-text">The Laplace approximation thus requires to compute the minimizer of \(f_n(\theta)\), which is exactly the maximum a posteriori estimate \(\hat{\theta}_{\rm MAP}\), and use the approximation: $$ \int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta \approx (2\pi / n)^{d/2} \frac{h( \hat{\theta}_{\rm MAP})}{(\det f_n^{\prime \prime}( \hat{\theta}_{\rm MAP}))^{1/2}} \exp( – n f_n( \hat{\theta}_{\rm MAP})).$$</p>



<p class="justify-text"><strong>Gaussian posterior approximation.</strong> Note that the Laplace approximation exactly corresponds to approximating the log-posterior density by a quadratic form and thus approximating the posterior by a Gaussian distribution with mean \(\hat{\theta}_{\rm MAP}\) and covariance matrix \(\frac{1}{n} f_n^{\prime \prime}( \hat{\theta}_{\rm MAP})^{-1}\). Note that Laplace’s method gives one natural example of such Gaussian approximation and that <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational inference</a> can be used to find better ones.</p>



<p class="justify-text"><strong>Example.</strong> We consider the Laplace approximation of a Beta random variable, that is a Gaussian with mean at the mode of the original density and variance equal to the inverse of the second derivative of the log-density. Below, the mean \(\alpha / (\alpha +\beta)\) is set to a constant, while the variance shrinks due to an increasing \(\alpha+\beta\) (which corresponds to the number of observations in the Bayesian interpretation above).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="598" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/plot_beta-3.gif" class="wp-image-6335" height="248" />Left: densities. Right: negative log-densities translated so that they have matched first two derivatives at their minimum.</figure></div>



<p class="justify-text">We see above, that for large variances (small \(\alpha +\beta\)), the Gaussian approximation is not tight, while it becomes tighter as the mass gets concentrated around the mode.</p>



<h2>Extensions</h2>



<p class="justify-text"><strong>High-order expansion.</strong> The approximation is based on Taylor expansions of the functions \(h\) (order \(0\)) and \(f\) (order \(2\)). In order to obtain extra terms of the form \(t^{d/2+\nu}\), for \(\nu\) a positive integer, we need higher-order derivatives of \(h\) and \(f\). In more than one dimension, that quickly gets complicated (see, e.g., [3, 4]).</p>



<p class="justify-text">One particular case which is interesting in one dimension is \(h(x) \sim A ( x- x_\ast)^\alpha\), and \(f(x)-f(x_\ast) = B|x-x_\ast|^{\beta}\). Note that \(\alpha=0\) and \(\beta=2\) corresponds to the regular case. A short calculation gives the equivalent \(I(t) \sim \frac{2}{2+\beta}  \frac{A \Gamma\big( \frac{\alpha+1}{\beta} \big)}{(tB)^{\frac{\alpha+1}{\beta}}}\).</p>



<p class="justify-text"><strong><a href="https://en.wikipedia.org/wiki/Stationary_phase_approximation">Stationary phase approximation</a>.</strong> We can also consider integrals of the form $$I(t) = \int_K h(x) \exp( – i t f(x) ) dx,$$ where \(i\) is the usual square root of \(-1\). Here, the main contribution also comes from vectors \(x\) where the gradient of \(f\) is zero. This can be further extended to more general <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">complex integrals</a>.</p>



<h2>When Napoléon meets Laplace and Lagrange</h2>



<p class="justify-text">As a conclusion, I cannot resist mentioning a <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#I_had_no_need_of_that_hypothesis">classical (potentially not totally authentic) anecdote</a> about encounters between Laplace and Lagrange, two mathematical heroes of mine, and Napoléon, as described in [<a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">5</a>, page 343]:</p>



<blockquote class="wp-block-quote justify-text"><p>Laplace went in state to beg Napoleon to accept a copy of his work, and the following account of the interview is well authenticated, and so characteristic of all the parties concerned that I quote it in full. Someone had told Napoleon that the book contained no mention of the name of God; Napoleon, who was fond of putting embarrassing questions, received it with the remark, “M. Laplace, they tell me you have written this large book on the system of the universe, and have never even mentioned its Creator.” Laplace, who, though the most supple of politicians, was as stiff as a martyr on every point of his philosophy, drew himself up and answered bluntly, “Je n’avais pas besoin de cette hypothèse-là.” Napoleon, greatly amused, told this reply to Lagrange, who exclaimed, “Ah! c’est une belle hypothèse; ça explique beaucoup de choses.”</p><cite>W. W. Rouse Ball, A Short Account of the History of Mathematics, 1888.</cite></blockquote>



<h2>References</h2>



<p class="justify-text">[1] Pierre-Simon Laplace. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">Mémoire sur la probabilité des causes par les événements</a>, Mémoires de l’Académie royale des sciences de Paris (Savants étrangers), t. VI. p. 621, 27-65, 1774.<br />[2] Norman Bleistein, Richard A. Handelsman. Asymptotic Expansions of Integrals. Dover<br />Publications, 1986.[3] Stephen M. Stigler. <a href="https://www.jstor.org/stable/pdf/2245475.pdf">Laplace’s 1774 memoir on inverse probability</a>. <em>Statistical Science</em>, 1(3):359-378, 1986.<br />[3] Luke Tierney, Robert E. Kass, Joseph B. Kadane. <a href="https://www.jstor.org/stable/pdf/2289652.pdf">Fully exponential Laplace approximations to expectations and variances of nonpositive functions</a>. <em>Journal of the American Statistical Association</em>, 84(407): 710-716, 1989.<br />[4] Zhenming Shun, Peter McCullagh. <a href="https://www.jstor.org/stable/pdf/2345941.pdf">Laplace approximation of high dimensional integrals</a>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 57(4): 749-760, 1995.<br />[5] W. W. Rouse Ball. <a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">A Short Account of the History of Mathematics</a>, 1888.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/laplace-method/"><span class="datestr">at July 23, 2021 06:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/108">TR21-108 |  Limitations of the Impagliazzo--Nisan--Wigderson Pseudorandom Generator against Permutation Branching Programs | 

	Edward Pyne, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The classic Impagliazzo--Nisan--Wigderson (INW) psesudorandom generator (PRG) (STOC `94) for space-bounded computation uses a seed of length $O(\log n \cdot \log(nwd/\varepsilon))$ to fool ordered branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$. A series of works have shown that the analysis of the INW generator can be improved for the class of $\textit{permutation}$ branching programs or the more general $\textit{regular}$ branching programs, improving the $O(\log^2 n)$ dependence on the length $n$ to $O(\log n)$ or $\tilde{O}(\log n)$.  However, when also considering the dependence on the other parameters, these analyses still fall short of the optimal PRG seed length $O(\log(nwd/\varepsilon))$.
    
    In this paper, we prove that any ``spectral analysis'' of the INW generator requires seed length $$\Omega\left(\log n\cdot \log\log(\min\{n,d\})+\log n\cdot \log(w/\varepsilon)+\log d\right)$$ to fool ordered permutation branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$.  By ``spectral analysis'' we mean an analysis of the INW generator that relies only on the spectral expansion of the graphs used to construct the generator; this encompasses all prior analyses of the INW generator.  Our lower bound matches the upper bound of Braverman--Rao--Raz--Yehudayoff (FOCS 2010, SICOMP 2014) for regular branching programs of alphabet size $d=2$ except for a gap between their $O(\log n \cdot \log\log n)$ term and our $O(\log n \cdot \log\log \min\{n,d\})$ term.  It also matches the upper bounds of Koucky--Nimbhorkar--Pudlak (STOC 2011), De (CCC 2011), and Steinke (ECCC 2012) for constant-width ($w=O(1)$) permutation branching programs of alphabet size $d=2$ to within a constant factor.  
    
    To fool permutation branching programs in the stronger measure of $\textit{spectral norm}$, we prove that any spectral analysis of the INW generator requires a seed of length $\Omega(\log n\cdot \log\log n+\log n\cdot\log(1/\varepsilon)+\log d)$ when the width is at least polynomial in $n$ ($w=n^{\Omega(1)}$), matching the recent upper bound of Hoza--Pyne--Vadhan (ITCS `21) to within a constant factor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/108"><span class="datestr">at July 23, 2021 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/107">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/107">TR21-107 |  Classification of OBDD size for monotone 2-CNFs | 

	igor razgon</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce a new graph parameter called linear upper maximum induced
matching width \textsc{lu-mim width}, denoted for a graph $G$ by $lu(G)$.
We prove that the smallest size of the \textsc{obdd} for $\varphi$,
the monotone 2-\textsc{cnf} corresponding to $G$, is sandwiched 
between $2^{lu(G)}$ and $n^{O(lu(G))}$. 
The upper bound is based on a combinatorial statement that might 
be of an independent interest.
We show that the bounds in terms of this parameter are best possible.

The new parameter is closely related to two existing parameters:
linear maximum induced matching width (\textsc{lmim width}) and linear symmetric induced matching width 
(\textsc{lsim width}). We prove that \textsc{lu-mim width} lies strictly in between these two
parameters, being dominated by \textsc{lsim width} and dominating \textsc{lmim width}.
We conclude that neither of the two existing parameters can be used instead of \textsc{lu-mim width}
to characterize the size of \textsc{obdd}s for monotone $2$-\textsc{cnf}s and this justifies introduction
of the new parameter.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/107"><span class="datestr">at July 23, 2021 06:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/106">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/106">TR21-106 |  The Space Complexity of Sampling | 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	David Zuckerman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Recently, there has been exciting progress in understanding the complexity of distributions. Here, the goal is to quantify the resources required to generate (or sample) a distribution. Proving lower bounds in this new setting is more challenging than in the classical setting, and has yielded interesting new techniques and surprising applications. In this work, we initiate a study of the complexity of sampling with limited memory, and prove small-space analogs of several results previously known only for sampling in AC$^0$.

1. We exhibit an explicit boolean function $b:\{0,1\}^n\to\{0,1\}$ that cannot be computed by width $2^{\Omega(n)}$ read-once branching programs (ROBPs), even on average, but such that $(\mathbf{U}_n,b(\mathbf{U}_n))$ can be exactly sampled by ROBPs of width $O(n)$.

2. We exhibit an explicit boolean function $b:\{0,1\}^n\to\{0,1\}$ such that any distribution sampled by an ROBP of width $2^{\Omega(n)}$ has statistical distance $\frac{1}{2}-2^{-\Omega(n)}$ from $(\mathbf{U}_n,b(\mathbf{U}_n))$. We show that any such $b$ witnesses exponentially small correlation bounds against ROBPs, and we extend these results to hold for the unknown-order setting.

3. We show that any distribution sampled by an ROBP of width $2^{\Omega(n)}$ has statistical distance $1-2^{-\Omega(n)}$ from any distribution that is uniform over a good code. More generally, we obtain sampling lower bounds for any list decodable code, which are nearly tight. Using a known connection, we also obtain data structure lower bounds for storing codewords.

Along the way, we prove a direct product theorem and several equivalence theorems. These tools offer a generic method to construct distributions with strong sampling lower bounds, and translate these lower bounds into correlation bounds against ROBPs. As an application of our direct product theorem, we show a strong complexity separation between sampling with AC$^0$ circuits and sampling with ROBPs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/106"><span class="datestr">at July 23, 2021 05:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18973">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/">The Reach of Dichotomy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Congratulations on the 2021 Gödel Prize</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/godelprize2021/" rel="attachment wp-att-18975"><img width="222" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/GodelPrize2021.png?resize=222%2C148&amp;ssl=1" class="alignright wp-image-18975" height="148" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite including Richerby’s <a href="https://www.youtube.com/watch?v=4vLdQ-vkVtg">talk</a> at STOC 2021</font></td>
</tr>
</tbody>
</table>
<p>
Andrey Bulatov, Martin Dyer and David Richerby, and Jin-Yi Cai and Xi Chen are the authors of three papers awarded the 2021 Gödel Prize. The citations by the <a href="https://sigact.org/prizes/gödel/citation2021.html">ACM</a> and <a href="https://eatcs.org/index.php/component/content/article/1-news/2885-2021-05-07-21-20-13">EATCS</a> say clearly that the papers not the people win the prize, but our “blog invariant” is to show photos of people at the top, so there they are.</p>
<p>
Today we congratulate the people—not the papers—and tell some of the technical story behind this richly deserved award.</p>
<p>
Dick and I have intended to write about this since the prize was announced in early May. Travel and personal events have slowed the blog, plus in my case, June was by far the most pressing month of the “chess cheating pandemic” since chess moved online, and that carried into July. The award to our friend Jin-Yi—who has been a faculty colleague of both of us—is the “something else about UW Madison on our mind” that was mentioned in the June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a>, during which chess cases arrived in real time. I am thankful that this month, to judge by the weekly roundups of games by <a href="https://en.chessbase.com/">ChessBase</a> and <a href="https://theweekinchess.com/">The Week in Chess</a>, most major tournaments are back to being played in-person—where the yearly volume of cases coming to my attention has been on either side of ten, rather than in middle triple digits.</p>
<p>
That the papers win the award calls something else to mind: We can no longer point to an “original manuscript” with papers being electronic. Perhaps the continued rise of <em>non-fungible tokens</em> (<a href="https://en.wikipedia.org/wiki/Non-fungible_token">NFTs</a>) will restore this distinction. Tim Berners-Lee recently <a href="https://www.cnn.com/style/article/tim-berners-lee-nft-auction/index.html">created</a> and auctioned off an NFT of his original code for the World Wide Web. This may be done to prize-winning papers in the future. Then the prize money could create an inherent base value, such as we discussed for “semi-fungible tokens” at the end of this <a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/">post</a>, so that the prize really would go to the papers. </p>
<p>
</p><p></p><h2> The Beginning of Dichotomy </h2><p></p>
<p></p><p>
The term “dichotomy” in complexity theory first became prominent with Thomas Schaefer’s 1978 <a href="http://aleteya.cs.buap.mx/~jlavalle/papers/complexity/p216-schaefer.pdf">paper</a> about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{SAT}}" class="latex" />-like problems. For a large and natural class <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{S}}" class="latex" /> of such problems, he proved that either the problem is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-complete like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{SAT}}" class="latex" /> itself or it is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />—never <a href="https://en.wikipedia.org/wiki/NP-intermediate">intermediate</a> between them. The famous 1975 <a href="https://en-academic.com/dic.nsf/enwiki/2272652">theorem</a> of Richard Ladner showed that if <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP \neq P}}" class="latex" /> then plenty of intermediate languages exist, but no problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{S}}" class="latex" /> can be among them. That is the dichotomy: everything in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{S}}" class="latex" /> is either easy or maximally hard. A similar project was taken up for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" /> by Nadia Creignou and Nicolas (Miki) Hermann in a 1996 <a href="https://www.sciencedirect.com/science/article/pii/S0890540196900164">paper</a> showing that every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B%5C%23SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{\#SAT}}" class="latex" />-like counting problem is either in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> or is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" />-complete. </p>
<p>
What does “<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{SAT}}" class="latex" />-like” mean? Consider the clause <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cbar%7Bx%7D_i+%5Cvee+%5Cbar%7Bx%7D_j+%5Cvee+x_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\bar{x}_i \vee \bar{x}_j \vee x_k)}" class="latex" />. It is satisfied if and only if the values of the variables belong to the relation </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R+%3D+%5C%7B%280%2C0%2C0%29%2C%280%2C0%2C1%29%2C%280%2C1%2C0%29%2C%280%2C1%2C1%29%2C%281%2C0%2C0%29%2C%281%2C0%2C1%29%2C%281%2C1%2C1%29%5C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  R = \{(0,0,0),(0,0,1),(0,1,0),(0,1,1),(1,0,0),(1,0,1),(1,1,1)\}, " class="latex" /></p>
<p>
which excludes only the non-satisfying assignment <img src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1,1,0)}" class="latex" />. Note that if we had the clause <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cbar%7Bx%7D_i+%5Cvee+x_j+%5Cvee+%5Cbar%7Bx%7D_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\bar{x}_i \vee x_j \vee \bar{x}_k)}" class="latex" /> instead, we could re-use <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" /> by switching <img src="https://s0.wp.com/latex.php?latex=%7Bx_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_j}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_k}" class="latex" /> in the given sequence of variables. Putting negated literals first, if we instead want to require exactly one literal to be true we could apply one of the following relations to each clause, depending on how many negated variables it has: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++R%5E0+%26%3D%26+%5C%7B%281%2C0%2C0%29%2C%280%2C1%2C0%29%2C%280%2C0%2C1%29%5C%7D%5C%5C+R%5E1+%26%3D%26+%5C%7B%280%2C0%2C0%29%2C%281%2C1%2C0%29%2C%281%2C0%2C1%29%5C%7D%5C%5C+R%5E2+%26%3D%26+%5C%7B%280%2C1%2C0%29%2C%281%2C0%2C0%29%2C%281%2C1%2C1%29%5C%7D%5C%5C+R%5E3+%26%3D%26+%5C%7B%280%2C1%2C1%29%2C%281%2C0%2C1%29%2C%281%2C1%2C0%29%5C%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  R^0 &amp;=&amp; \{(1,0,0),(0,1,0),(0,0,1)\}\\ R^1 &amp;=&amp; \{(0,0,0),(1,1,0),(1,0,1)\}\\ R^2 &amp;=&amp; \{(0,1,0),(1,0,0),(1,1,1)\}\\ R^3 &amp;=&amp; \{(0,1,1),(1,0,1),(1,1,0)\} \end{array} " class="latex" /></p>
<p>
Then we can say that both the “Exactly One 3SAT” decision problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> and the associated counting problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" /> are <em>defined by</em> the collection <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma+%3D+%5C%7BR%5E0%2CR%5E1%2CR%5E2%2CR%5E3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma = \{R^0,R^1,R^2,R^3\}}" class="latex" /> of relations. The following shows how they are subcases of the general constraint satisfaction problem (<a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem">CSP</a>):</p>
<p></p><p></p>
<ul>
<li>
Parameter: A set <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> of relations over some domain <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" />. <p></p>
</li><li>
Instance: A set <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Phi}" class="latex" /> of tuples <img src="https://s0.wp.com/latex.php?latex=%7BC_1%2C%5Cdots%2CC_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_1,\dots,C_m}" class="latex" /> over variables <img src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{X = \{x_1,\dots,x_n\}}" class="latex" />, with each <img src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_j}" class="latex" /> associated to some relation <img src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_j}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" />. <p></p>
</li><li>
Question: Is there an assignment <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3A+X+%5Crightarrow+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha: X \rightarrow D}" class="latex" /> that makes <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29+%5Cin+R_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha(C_j) \in R_j}" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{j}" class="latex" />? For the counting version, called <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23CSP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#CSP}}" class="latex" />, how many such assignments are there?
</li></ul>
<p></p><p><br />
Here <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha(C_j)}" class="latex" /> means the ordered tuple of values <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha(x_i)}" class="latex" /> of the variables <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_i}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_j}" class="latex" />. If <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> are considered part of the instance—where they must be finite—the problem is <em>uniform</em>, but if they are fixed (and possibly infinite), it is <em>nonuniform</em>. For a simple analogy: for every fixed context-free grammar <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, its language <img src="https://s0.wp.com/latex.php?latex=%7BL%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{L(G)}" class="latex" /> belongs to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />, but a much stronger fact is that the uniform acceptance problem <img src="https://s0.wp.com/latex.php?latex=%7BA_%7BCFG%7D+%3D+%5C%7B%5Clangle+G%2Cx%5Crangle%3A+x+%5Cin+L%28G%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_{CFG} = \{\langle G,x\rangle: x \in L(G)\}}" class="latex" /> also belongs to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />. Another relaxation is to allow different domains <img src="https://s0.wp.com/latex.php?latex=%7BD_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D_i}" class="latex" /> for different variables <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_i}" class="latex" />.</p>
<p>
In the Boolean case, <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> is fixed to be <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{0,1\}}" class="latex" />. We can now say what Schaefer and Creignou-Hermann proved. Schaefer showed that every nonuniform Boolean CSP with finite <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> is either in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-complete. He showed the former holds only when <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> falls into one of six explicit cases, where which case is also decidable in polynomial time given <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" />. Creignou and Hermann gave a similar result for Boolean #CSP but with a smaller list of cases—because there are cases where the decision problem is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> but the counting problem remains <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" />-complete. A simple such case is where <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> has only the relation <img src="https://s0.wp.com/latex.php?latex=%7BR+%3D+%5C%7B%281%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R = \{(1,0),(0,1),(1,1)\}}" class="latex" />, which defines monotone <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B2SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{2SAT}}" class="latex" />.</p>
<p>
</p><p></p><h2> Extending the Reach </h2><p></p>
<p></p><p>
Also in the 1990s, Tomás Feder and Moshe Vardi wrote <a href="https://dl.acm.org/doi/10.1145/167088.167245">two</a> <a href="https://epubs.siam.org/doi/abs/10.1137/S0097539794266766?journalCode=smjcat">papers</a> on extending Schaefer’s dichotomy to a wider class of problems. To quote the first sentence of the latter’s abstract:</p>
<blockquote><p><b> </b> <em> This paper starts with the project of finding a large subclass of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> which exhibits a dichotomy. </em>
</p></blockquote>
<p></p><p>
The first step was to go to larger domains <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" />. For example, consider <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D = \{1,2,3\}}" class="latex" />. Take <img src="https://s0.wp.com/latex.php?latex=%7BR+%3D+%5C%7B%281%2C2%29%2C%282%2C3%29%2C%281%2C3%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R = \{(1,2),(2,3),(1,3)\}}" class="latex" />, which is simply the relation <img src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\neq}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" />. Given an undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, its edges become the tuples. The resulting CSP is satisfiable if and only if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is 3-colorable. Of course, this is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-complete. Feder and Vardi arrived at this by going down from large subclasses of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" /> defined by logic in which Ladner-type constructions can still be expressed, so that dichotomy does <em>not</em> hold. When they imposed all of their conditions, they were left with nonuniform CSPs, and conjectured dichotomy for their decision problems.</p>
<p>
The corresponding question for counting problems was solved <a href="https://www.cs.sfu.ca/~abulatov/papers/counting-acm.pdf">first</a>, by Bulatov. Dyer and Richerby <a href="https://arxiv.org/pdf/1003.3879.pdf">followed</a> by simplifying Bulatov’s proof in a way that also allows deciding whether a given finite <img src="https://s0.wp.com/latex.php?latex=%7B%28%5CGamma%2CD%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\Gamma,D)}" class="latex" /> falls into one of the polynomial-time cases (assuming <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P \neq P}}" class="latex" /> to begin with). The latter task runs in polynomial time with an oracle for testing what they call <em>strong balance</em>, which in turn polynomial-time mapping-reduces to Graph Isomorphism—so by Laci Babai’s celebrated theorem they can decide in time quasi-polynomial in <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5CGamma%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|\Gamma|}" class="latex" /> whether it is a hard or easy case.</p>
<p>
Finally—and not awarded any of this prize money—a 2017 <a href="https://arxiv.org/abs/1703.03021">paper</a> by Bulatov proved dichotomy for the decision case of nonuniform CSPs. Independently, so did a <a href="https://arxiv.org/abs/1704.01914">paper</a> by Dmitriy Zhuk. We will explore just a few of the underlying ideas in the next section. </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5CS%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  {\S} " class="latex" /></p>
<p>Here we continue with the further extension made by Jin-Yi and Xi Chen. This starts by writing the counting problem as <a name="partitionfn"></a></p><a name="partitionfn">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%23%5CPhi+%3D+%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Cprod_%7Bj%3D1%7D%5Em+w%28%5Calpha%28C_j%29%2CR_j%29%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \#\Phi = \sum_{\alpha: X \rightarrow D} \prod_{j=1}^m w(\alpha(C_j),R_j), \ \ \ \ \ (1)" class="latex" /></p>
</a><p><a name="partitionfn"></a> where for simple #CSP, <img src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w(\alpha(C_j),R_j) = 1}" class="latex" /> if the values assigned by <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha}" class="latex" /> to the variables in <img src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_j}" class="latex" /> satisfy the associated relation <img src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_j}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w(\alpha(C_j),R_j) = 0}" class="latex" /> otherwise. However, one need not score an unsatisfied constraint as zero. Giving a nonzero partial credit <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda}" class="latex" /> for a wrong answer <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha(C_j)}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_j}" class="latex" /> might make you more magnetic as a teacher. It would literally be magnetism: the simple case where <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> consists of just the binary equality relation on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{0,1\}}" class="latex" />, so that for all binary clauses (edges) <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(u,v)}" class="latex" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w%280%2C0%29+%3D+w%281%2C1%29+%3D+1%2C+%5Cqquad+w%280%2C1%29+%3D+w%281%2C0%29+%3D+%5Clambda%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  w(0,0) = w(1,1) = 1, \qquad w(0,1) = w(1,0) = \lambda, " class="latex" /></p>
<p>yields the simplest case of the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> of magnetism. But we can also take a quantum leap and make you complex: we can allow the <b>weights</b> <img src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{w(\alpha(C_j),R_j)}" class="latex" /> to have complex values. In particular, we can have functions of the form </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Cprod_%7Bj%3D1%7D%5Em+%5Comega%5E%7Bg%28%5Calpha%28C_j%29%2CR_j%29%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{\alpha: X \rightarrow D} \prod_{j=1}^m \omega^{g(\alpha(C_j),R_j)}, " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> is a complex root of unity. Writing the product of powers of <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> as just <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\omega}" class="latex" /> raised to a sum of terms ranging over all the clauses <img src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_j}" class="latex" /> in the instance <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Phi}" class="latex" />, and inserting a normalizing constant <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" />, we obtain functions of the form </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z%28f_%5CPhi%29+%3D+r%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Comega%5E%7Bf_%5CPhi%28%5Calpha%29%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Z(f_\Phi) = r\sum_{\alpha: X \rightarrow D} \omega^{f_\Phi(\alpha)}, " class="latex" /></p>
<p>which are generally called <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition functions</a>. Indeed, we long ago <a href="https://rjlipton.wpcomstaging.com/2012/07/08/grilling-quantum-circuits/">discussed</a> various efficient ways to convert quantum circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" /> into polynomials <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_C(x_1,\dots,x_n)}" class="latex" /> so that, with <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D = \{0,1\}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> depending only on <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BZ%28f_C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Z(f_C)}" class="latex" /> gives the acceptance amplitude of <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" />. The acceptance probability can be represented in like manner. See also this <a href="https://arxiv.org/pdf/1409.5627.pdf">paper</a> for another view of the bridge from the Ising model to quantum circuits.</p>
<p>
Thus, the dichotomy theorems that Jin-Yi and Xi have obtained for complex weighted CSPs take matters into the quantum realm. What this may say about the class <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{BQP}}" class="latex" /> (bounded-error quantum polynomial time) we leave to the end.</p>
<p>
</p><p></p><h2> Some of the Ideas </h2><p></p>
<p></p><p>
One key idea can be approached as generalizing a familiar feature of linear algebra. Suppose we take some number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell}" class="latex" /> of vectors of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> that belong to a linear subspace <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />. We can arrange them into an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell \times k}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />. Now let us multiply <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> on the left by a row vector <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%28a_1%2C%5Cdots%2Ca_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a = (a_1,\dots,a_\ell)}" class="latex" />. Then <img src="https://s0.wp.com/latex.php?latex=%7BaM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{aM}" class="latex" /> is another vector of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />. This vector belongs to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> because it is just a linear combination of our <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell}" class="latex" />-many vectors in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />.</p>
<p>
The insight for abstraction is that we can regard <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> as having been applied individually to each of the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> columns of <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" />. We got one new <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />-tuple from this application, but it still belonged to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" />. So now, let us consider “multiplying” <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> on the left by an arbitrary <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell}" class="latex" />-ary function <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x_1%2C%5Cdots%2Cx_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x_1,\dots,x_\ell)}" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=%7BhM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{hM}" class="latex" /> is always a <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />-tuple. And let us relax <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> to be any (finite) set <img src="https://s0.wp.com/latex.php?latex=%7BR+%5Csubseteq+D%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R \subseteq D^k}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D}" class="latex" /> is our (finite) domain. </p>
<blockquote><p><b>Definition 1</b> <em> <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> as a <b>polymorphism</b> if for any <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> made from <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell}" class="latex" /> members of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" />, allowing repeats, <img src="https://s0.wp.com/latex.php?latex=%7BhM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{hM}" class="latex" /> belongs to <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" />. A set <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> of relations <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> as a polymorphism if every <img src="https://s0.wp.com/latex.php?latex=%7BR+%5Cin+%5CGamma%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R \in \Gamma}" class="latex" /> has <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> as a polymorphism. </em>
</p></blockquote>
<p></p><p>
With <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell = 2}" class="latex" />, the binary <em>and</em> function is a polymorphism of the relation <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R}" class="latex" /> above that represented 3-clauses with two negated literals. This is because the excluded triple <img src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1,1,0)}" class="latex" /> cannot be written as the <em>and</em> of the other triples. This extends to the relation <img src="https://s0.wp.com/latex.php?latex=%7BR%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R'}" class="latex" /> for satisfiability with three negated literals, which excludes <img src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1,1,1)}" class="latex" />, so <em>and</em> is a polymorphism of <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma+%3D+%5C%7BR%2CR%27%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma = \{R,R'\}}" class="latex" />, and also for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k &gt; 3}" class="latex" /> to pertain to <a href="https://en.wikipedia.org/wiki/Horn_clause">Horn clauses</a> in general. Dually, <em>or</em> is a polymorphism for satisfiability of dual-Horn clauses. </p>
<p>
The case of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B2SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathrm{2SAT}}" class="latex" /> is trickier. Its <img src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Gamma}" class="latex" /> has three constituent relations, depending on the number of negated literals: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++R%5E0+%26%3D%26+%5C%7B%281%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%5C%5C+R%5E1+%26%3D%26+%5C%7B%280%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%5C%5C+R%5E2+%26%3D%26+%5C%7B%280%2C0%29%2C%280%2C1%29%2C%281%2C0%29%5C%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{array}{rcl}  R^0 &amp;=&amp; \{(1,0),(0,1),(1,1)\}\\ R^1 &amp;=&amp; \{(0,0),(0,1),(1,1)\}\\ R^2 &amp;=&amp; \{(0,0),(0,1),(1,0)\}. \end{array} " class="latex" /></p>
<p>Is there an <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> that is simultaneously a polymorphism of each one? Neither <em>and</em> nor <em>or</em> works: the former fails on <img src="https://s0.wp.com/latex.php?latex=%7BR%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R^0}" class="latex" /> because the component-wise <em>and</em> of <img src="https://s0.wp.com/latex.php?latex=%7B%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1,0)}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(0,1)}" class="latex" /> is the excluded <img src="https://s0.wp.com/latex.php?latex=%7B%280%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(0,0)}" class="latex" />, and <em>or</em> similarly fails on <img src="https://s0.wp.com/latex.php?latex=%7BR%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R^2}" class="latex" />. In fact, <em>none</em> of the sixteen binary Boolean functions is a homomorphism of all three. But when we go up to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell = 3}" class="latex" />, however, it turns out that the <em>majority-of-3</em> function is a polymorphism: any duplicated tuple rules, so only the three instances of three separate tuples need to be checked.</p>
<p>
Going back to the linear dependence idea, the ternary function <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x_1%2Cx_2%2Cx_3%29+%3D+x_1+%2B+x_2+%2B+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x_1,x_2,x_3) = x_1 + x_2 + x_3}" class="latex" /> is a polymorphism of <em>affine</em> relations. For <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{D = \{0,1\}}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+}" class="latex" /> is the same as exclusive-or, it has the property that <img src="https://s0.wp.com/latex.php?latex=%7Bh%28x%2Cy%2Cy%29+%3D+h%28y%2Cy%2Cx%29+%3D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(x,y,y) = h(y,y,x) = x}" class="latex" />. This property is named for Anatoly Maltsev, who used it long before any of this complexity work began. It turned out to be a key to demarcating the easy cases for the dichotomy in higher dimensions. Affine relations were already the lone easy case in Creignou-Hermann for Boolean #CSP, while we can now crisply state Schaefer’s original dichotomy for decision CSP: affine, majority-of-3, <em>and</em>, <em>or</em>, plus two trivial cases where the all-zero or all-one assignment always works, are the polymorphisms that characterize the SAT-like problems in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" />.</p>
<p>
A second basic idea is to re-cast the counting problems as ones of counting <em>homomorphisms</em> between relational structures. For instance, let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> be any graph and <img src="https://s0.wp.com/latex.php?latex=%7BK_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K_r}" class="latex" /> the complete graph on <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> nodes. A homomorphism <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h}" class="latex" /> has the property that when <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(u,v)}" class="latex" /> is an edge of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B%28h%28u%29%2Ch%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(h(u),h(v)}" class="latex" /> is an edge of <img src="https://s0.wp.com/latex.php?latex=%7BK_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K_r}" class="latex" />—in particular, <img src="https://s0.wp.com/latex.php?latex=%7Bh%28u%29+%5Cneq+h%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{h(u) \neq h(v)}" class="latex" />. This is possible if and only if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" />-colorable. This approach was greatly enhanced in a 1998 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.40.2693">paper</a> by Peter Jeavons and its follow-ups with other authors. It was particularly useful to the decidability part of Dyer and Richerby’s paper.</p>
<p>
A third idea used by Cai and Chen originated in a 2010 <a href="https://arxiv.org/pdf/1012.5659.pdf">paper</a> of theirs with Pinyan Lu for the case of non-negative weights. It is to stratify the sum in (<a href="https://rjlipton.wpcomstaging.com/feed/#partitionfn">1</a>) over all <img src="https://s0.wp.com/latex.php?latex=%7Bt+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t \leq n}" class="latex" /> by mapping </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28a_1%2C%5Cdots%2Ca_t%29+%3D+%5Csum_%7B%5Calpha%3A+%5Calpha%28x_1%29%3Da_1%2C%5Cdots%2C%5Calpha%28x_t%29%3Da_t%7D+%5C%3B%5C%3B%5C%3B+%5Cprod_%7Bj%3D1%7D%5Em+w%28%5Calpha%28C_j%29%2CR_j%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(a_1,\dots,a_t) = \sum_{\alpha: \alpha(x_1)=a_1,\dots,\alpha(x_t)=a_t} \;\;\; \prod_{j=1}^m w(\alpha(C_j),R_j). " class="latex" /></p>
<p>With <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t = n}" class="latex" /> this is just a single evaluation; with <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=0}" class="latex" /> this is the original function. They create an oracle that either evaluates the whole sum or allows progressing from <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t-1}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />. They create a matrix <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bd%5E%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d^{t-1}}" class="latex" /> rows for the possible fixed parts <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Ba%7D+%3D+%28a_1%2C%5Cdots%2Ca_%7Bt-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\vec{a} = (a_1,\dots,a_{t-1})}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> columns for the possible choices of <img src="https://s0.wp.com/latex.php?latex=%7Ba_t+%3D+%5Calpha%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a_t = \alpha(x_t)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+%7CD%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d = |D|}" class="latex" />. The entries are <img src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2Cc%5D+%3D+f%28a_1%2C%5Cdots%2Ca_%7Bt-1%7D%2Cc%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M[\vec{a},c] = f(a_1,\dots,a_{t-1},c)}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M[\vec{a},\cdot]}" class="latex" /> stands for the row vector over all <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c \in D}" class="latex" />. The oracle, given <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Ba%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\vec{a}}" class="latex" />, returns the multiple of <img src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M[\vec{a},\cdot]}" class="latex" /> that normalizes the first non-zero entry to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, or the all-zero vector if <img src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M[\vec{a},\cdot]}" class="latex" /> is all zero. </p>
<p>
A big extra challenge in the complex-weights case is how to handle cancellations, as begun in this <a href="https://arxiv.org/abs/0903.4728">paper</a> by Cai, Chen, and Lu. Implementing the oracle efficiently requires the Maltsev property, a kind of “proto-unitarity” condition on <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> (under which <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" /> has at most <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> pairwise linearly independent rows and every two such rows are orthogonal), and a third property they call the “Type Partition” condition to avert blowup in the number of queries. Whether the classification they get is decidable remains open.</p>
<p>
Here is where we say to go to the papers for details, but we hope we have expressed ideas and their motivation as a guide. We have a little more to say about interpretation and making further progress on the dichotomy.</p>
<p>
</p><p></p><h2> Taking Dichotomy Further </h2><p></p>
<p></p><p>
Progress on the counting dichotomy increases the “internal evidence” for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P \neq P}}" class="latex" />, which of course is implied by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP \neq P}}" class="latex" />. To paraphrase how our blogging friends Scott and Lance and Bill have often put it:</p>
<blockquote><p><b> </b> <em> Why would all this finely filigreed structure with sharp demarcation between hard and easy cases exist if <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P \neq NP}}" class="latex" /> were a false illusion? </em>
</p></blockquote>
<p></p><p>
At the very least, this puts the onus on a believer in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P = NP}}" class="latex" /> to give an alternative explanation of how such structure could arise. </p>
<p>
It strikes us, however, that this also puts a squeeze on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{BQP}}" class="latex" /> under the conventional wisdom that it is intermediate between <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P}}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" />, without being <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-hard in particular. This is not in technical conflict with the Cai-Chen dichotomy because exactly computing the acceptance probabilities of a class of quantum circuits (on inputs built into the circuits) can be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{\#P}}" class="latex" />-complete while the circuits use only a gap in probabilities to solve problems that are easy or intermediate. But we suspect that this still limits possible ways to capture <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{BQP}}" class="latex" /> “naturally” by descriptive logic.</p>
<p>
The prize paper with Chen is just one part of a long-term project by Jin-Yi and co-workers on classifying counting problems and showing where dichotomy applies. A 2010 <a href="https://arxiv.org/abs/1004.0803">paper</a> of Jin-Yi with Lu and Sangxia Huang bridged from #CSP and a subcase of counting homomorphisms between graphs to a wider kind of sum-over-products formula originally called a <b>holant</b> <a href="http://pages.cs.wisc.edu/~jyc/papers/LAA-journalversion.pdf">here</a>. Recent progress on dichotomy for holants is represented by this FOCS 2020 <a href="https://arxiv.org/abs/2005.07906">paper</a> with Shuai Shao. This is also both directly relevant to quantum computation and maps out progress that is still out there to make. The paper has a one-page prologue that summons characters from legendary fantasy (and <em>Star Wars</em>) to conquer quantum issues.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Again, we congratulate the winners—the human winners. What will the full implications of this work become in the years ahead?</p>
<p></p><p><br />
[some minor word changes]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/"><span class="datestr">at July 23, 2021 01:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/105">TR21-105 |  Functional lower bounds for restricted arithmetic circuits of depth four | 

	Suryajith Chillara</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Recently, Forbes, Kumar and Saptharishi [CCC, 2016] proved that there exists an explicit $d^{O(1)}$-variate and degree $d$ polynomial $P_{d} \in VNP$ such that if any depth four circuit $C$ of bounded formal degree $d$ which computes a polynomial of bounded individual degree $O(1)$, that is functionally equivalent to $P_d$, then $C$ must have size $2^{\Omega(\sqrt{d}\log{d})}$.

The motivation for their work comes from Boolean Circuit Complexity. Based on a characterization for $ACC^0$ circuits by Yao [FOCS, 1985] and Beigel and Tarui [CC, 1994], Forbes, Kumar and Saptharishi [CCC, 2016] observed that functions in $ACC^0$ can also be computed by algebraic $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits (i.e., circuits of the form -- sums of powers of polynomials) of $2^{\log^{O(1)}n}$ size. Thus they argued that a $2^{\omega(\log^{O(1)}{n})}$ "functional" lower bound for an explicit polynomial $Q$ against $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits would imply a lower bound for the "corresponding Boolean function" of $Q$ against non-uniform $ACC^0$. In their work, they ask if their lower bound be extended to $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits.

In this paper, for large integers $n$ and $d$ such that $\Omega(\log^2{n})\leq d\leq n^{0.01}$, we show that any $\Sigma\mathord{\wedge}\Sigma\Pi$ circuit of bounded individual degree at most $O(\frac{d}{k^2})$ that functionally computes Iterated Matrix Multiplication polynomial $IMM_{n,d}$ ($\in VP$) over $\{0,1\}^{n^2d}$ must have size $n^{\Omega(k)}$. Since Iterated Matrix Multiplication $IMM_{n,d}$ over $\{0,1\}^{n^2d}$ is functionally in $GapL$, improvement of the afore mentioned lower bound to hold for quasipolynomially large values of individual degree would imply a fine-grained separation of $ACC^0$ from $GapL$.

For the sake of completeness, we also show a syntactic size lower bound against any $\Sigma\mathord{\wedge}\Sigma\Pi$ circuit computing $IMM_{n,d}$ (for the same regime of $d$) which is tight over large fields. Like Forbes, Kumar and Saptharishi [CCC, 2016], we too prove lower bounds against circuits of bounded formal degree which functionally compute $IMM_{n,d}$, for a slightly larger range of individual degree.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/105"><span class="datestr">at July 22, 2021 01:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5561">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5561">Slowly emerging from blog-hibervacation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Alright everyone:</p>



<ol><li>Victor Galitski has an <a href="https://www.linkedin.com/pulse/quantum-computing-hype-bad-science-victor-galitski-1c?trk=public_post-content_share-article_title">impassioned rant against out-of-control quantum computing hype</a>, which I enjoyed and enthusiastically recommend, although I wished Galitski had engaged a bit more with the strongest arguments for optimism (e.g., the recent sampling-based supremacy experiments, the extrapolations that show gate fidelities crossing the fault-tolerance threshold within the next decade).  Even if I’ve been saying similar things on this blog for 15 years, I clearly haven’t been doing so in a style that works for everyone.  Quantum information needs as many people as possible who will tell the truth as best they see it, unencumbered by any competing interests, and has nothing legitimate to fear from that.  The modern intersection of quantum theory and computer science has raised profound scientific questions that will be with us for decades to come.  It’s a lily that need not be gilded with hype.<br /></li><li>Last month Limaye, Srinivasan, and Tavenas posted <a href="https://eccc.weizmann.ac.il/report/2021/081/">an exciting preprint to ECCC</a>, which apparently proves the first (slightly) superpolynomial lower bound on the size of constant-depth arithmetic circuits, over fields of characteristic 0.  Assuming it’s correct, this is another small victory in the generations-long war against the P vs. NP problem.<br /></li><li>I’m grateful to the Texas Democratic legislators who fled the state to prevent the legislature, a couple miles from my house, having a quorum to enact new voting restrictions, and who thereby drew national attention to the enormity of what’s at stake.  It should go without saying that, if a minority gets to rule indefinitely by forcing through laws to suppress the votes of a majority that would otherwise unseat it, thereby giving itself the power to force through more such laws, etc., then we no longer live in a democracy but in a banana republic.  And there’s no symmetry to the situation: no matter how terrified you (or I) might feel about wokeists and their denunciation campaigns, the Democrats have no comparable effort to suppress Republican votes.  Alas, I don’t know of any solutions beyond the obvious one, of trying to deal the conspiracy-addled grievance party crushing defeats in 2022 and 2024.<br /></li><li><strong><span class="has-inline-color has-vivid-red-color">Added:</span></strong> <a href="https://www.youtube.com/watch?v=DEYUt1tJlck">Here’s the video</a> of my recent Astral Codex Ten ask-me-anything session.</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5561"><span class="datestr">at July 21, 2021 08:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blog.simons.berkeley.edu/?p=304">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/simons.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/">Trends in Machine Learning Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series is disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/" target="_blank" rel="noreferrer noopener">Learning Theory Alliance</a> and is overseen by <a href="http://www.gautamkamath.com/" target="_blank" rel="noreferrer noopener">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" target="_blank" rel="noreferrer noopener">Learning Theory Alliance blog</a>.</p>



<p>This is the sixth and final post in the series, on trends in machine learning theory, written by <a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz" target="_blank" rel="noreferrer noopener">Michal Moshkovitz</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/" target="_blank" rel="noreferrer noopener">Cyrus Rashtchian</a>.</p>



<p><strong>Introduction</strong><br />Throughout the last few decades, we have witnessed unprecedented growth of machine learning. Originally a topic formalized by a small group of computer scientists, machine learning now impacts many areas: the physical sciences, medicine, commerce, finance, urban planning, and more. The rapid growth of machine learning can be partially attributed to the availability of large amounts of data and the development of powerful computing devices. Another important factor is that machine learning has foundations in many other fields, such as theoretical computer science, algorithms, applied mathematics, statistics, and optimization. </p>



<p>If machine learning is already mathematically rooted in many existing research areas, why do we need a field solely dedicated to learning theory? According to <a href="https://www.cs.columbia.edu/~djhsu/" target="_blank" rel="noreferrer noopener">Daniel Hsu</a>, “Learning theory serves (at least) two purposes: to help make sense of machine learning, and also to explore the capabilities and limitations of learning algorithms.” Besides finding innovative applications for existing tools, learning theorists also provide answers to long-standing problems and ask new fundamental questions. </p>



<p>Modern learning theory goes beyond classical statistical and computer science paradigms by: </p>



<ul><li>developing insights about specific computational models (e.g., neural networks) </li><li>analyzing popular learning algorithms (e.g., stochastic gradient descent)</li><li>taking into account data distributions (e.g., margin bounds or manifold assumptions)</li><li>adding auxiliary goals (e.g., robustness or privacy), and </li><li>rethinking how algorithms interact with and access data (e.g., online or reinforcement learning).</li></ul>



<p>By digging deep into the basic questions, researchers generate new concepts and models that change the way we solve problems and help us understand emerging phenomena.</p>



<p>This article provides a brief overview of three key areas in machine learning theory: new learning paradigms, trustworthy machine learning, and reinforcement learning. We describe the main thrust of each of these areas, as well as point to a few papers from <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT 2021</a> (the 32nd International Conference on Algorithmic Learning Theory) that touch each of these topics. To share a broader view, we also asked experts in the areas to comment on the field and on their recent papers. Needless to say, this article only scratches the surface. At the end, we point to places to learn more about learning theory.</p>



<span id="more-304"></span>



<p><strong>New Machine Learning Paradigms</strong><br />The traditional learning theory framework, probably approximately correct (PAC) learning, defines what it means to learn a ground-truth classifier from a candidate class of possible classifiers. Alongside PAC learning is Vapnik-Chervonenkis (VC) theory, which characterizes the number of samples needed and sufficient to learn a classifier from a given class. The generalization analysis from VC theory is restricted to guarantees that hold independently of the data distribution — that is, even for worst-case distributions. Additionally, the VC/PAC learning paradigm suggests that whenever learning is possible, it can be accomplished by choosing the classifier that minimizes loss on the training data, called the empirical risk minimizer (ERM). </p>



<p>This classical framework unfortunately fails to explain the empirical success of machine learning (ML). “The distribution-free setting, while it comes with the elegant VC theory, turned out to be unsatisfactory,” says Csaba Szepesvári. “Due to the oversimplified setting, the theory could not contribute meaningfully to understanding all kinds of learning methods such as learning with trees, boosting, neural networks, SVMs, or using any other nonparametric methods.” Researchers posit that stronger guarantees should be possible if we leverage natural assumptions about the data distribution, though identifying the right “natural assumptions” is a challenging task. Similarly, understanding which of many possible ERM solutions a learning algorithm chooses may yield better generalization results than those yielded by VC theory. </p>



<p>Methods that provide <em>distribution-specific </em>guarantees aren’t new to learning theory. A canonical example is known as a margin bound, where the test error of a classifier is analyzed in terms of the margin that separates the different prediction categories. In <a href="http://proceedings.mlr.press/v132/hanneke21a.html" target="_blank" rel="noreferrer noopener">one</a> of the ALT best papers, Steve Hanneke and Aryeh Kontorovich prove generalization guarantees in terms of the size of the margin for two popular classification algorithms: <a href="https://en.wikipedia.org/wiki/Support-vector_machine" target="_blank" rel="noreferrer noopener">support vector machines</a> (SVMs) and the <a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank" rel="noreferrer noopener">perceptron</a> algorithm. The authors answer a core open question, showing that SVMs achieve the optimal margin bound!</p>



<p>Further work at ALT uses an assumption that data lies on a low-dimensional manifold to prove guarantees for generative models. Generative models synthesize <em>original</em> samples, such as images or text, that resemble training data, but without copying the data directly. While so-called <em>generative adversarial networks</em> work well in practice, few guarantees exist because it is challenging to statistically formulate the requirement of generating original samples. Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak S. Dalalyan consider a new framework in their <a href="http://proceedings.mlr.press/v132/schreuder21a.html" target="_blank" rel="noreferrer noopener">paper</a>, in which they guarantee originality by outputting a continuous distribution, which ensures that it is very unlikely to output training examples. If the training data is generated from a low-dimensional manifold, they show that it is possible to learn a good generator, which outputs a smooth transformation of a random point.</p>



<p>In another <a href="http://proceedings.mlr.press/v132/tosh21a.html" target="_blank" rel="noreferrer noopener">paper</a>, Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu use distributional assumptions to show when unsupervised methods can use unlabeled data to learn useful <em>representations </em>of data. A linear function trained on these representations and some labeled data can then be used for downstream prediction of the labels. The key idea is to look at when data has <em>multiview redundancy (MVR), </em>which arises, for instance, when data is augmented: Under MVR, each data point can be viewed as a pair (<em>X,</em> <em>Z</em>), and the label <em>Y</em> can be predicted almost as well from <em>X</em> or <em>Z</em> as from the full pair. For instance, each pair might be two halves of an article, or two rotations of an image. The authors show how a theoretical approach called <em>landmark embedding </em>can produce a representation that enables low-error linear classification. Additionally, they analyze when the representations are learned implicitly while training a model to predict whether two views <em>X</em> and <em>Z</em> correspond to the same example, which is close to what is done in practice. </p>



<p>Another new paradigm considers how the specific<em> training algorithm</em> affects which of many candidate ERM solutions are chosen. This is called the <em>implicit bias </em>of a training algorithm: If there are multiple equally good solutions, then why does an algorithm choose one over the other? This is particularly relevant when studying neural networks, which are typically<em> overparameterized</em> and can be trained to find many solutions with zero empirical risk. In one <a href="http://proceedings.mlr.press/v132/ji21a/ji21a.pdf" target="_blank" rel="noreferrer noopener">paper</a>, Ziwei Ji and Matus Telgarsky characterize the implicit bias of using gradient descent to train a linear classifier with a general loss function. They show that the solution relates to the optimizer of a particular smoothed margin function. While this paper does not yield generalization guarantees, this type of implicit bias analysis can sometimes lead to generalization guarantees via margin bounds.</p>



<p>The goal of this area is to go beyond traditional learning theory paradigms by leveraging distributional and algorithmic properties. But a major open question is designing a more general mathematical theory that exploits such properties. <a href="http://www.cs.technion.ac.il/~shaymrn/" target="_blank" rel="noreferrer noopener">Shay Moran</a> offers a standard for new theory: “I hope that in the next 10 years we will develop more realistic models of learning, but I will insist that they still be mathematically clean.”</p>



<p><strong>Trustworthy ML</strong><br />Machine learning has inspired many new areas and technologies: personalized health care, drug discovery, advertising, résumé screening, credit loans, and more. However, these critical and user-centered applications require a higher standard of testing and verification because mistakes may deeply affect many people. Addressing these challenges has inspired a new field of research centered on making machine learning more trustworthy and reliable, which is the motivation for many of the ALT papers this year as well. An expert in the area, <a href="http://cseweb.ucsd.edu/~kamalika/" target="_blank" rel="noreferrer noopener">Kamalika Chaudhuri</a>, says, “For my field, which is trustworthy ML, the theoretical goal and challenge remains modeling and frameworks.” She elaborates: “Coming up with new conceptual frameworks for learning has always been one of the core challenges in learning theory since its early days, and it is doubly important now.” Researchers have been exploring this direction in many areas, including privacy, data deletion, robustness, fairness, interpretability, and causality. </p>



<p>Several ALT 2021 papers cover questions in privacy. The general goal is to understand how to modify existing learning methods to take into account privacy constraints. One of the <a href="http://proceedings.mlr.press/v132/wang21a.html" target="_blank" rel="noreferrer noopener">papers</a>, by Di Wang, Huanyu Zhang, Marco Gaboardi, and Jinhui Xu, considers generalized linear models in a differential privacy model. A central motivation is to understand the role of public, unlabeled data in improving the learnability of these problems in a private setting. Summarizing another direction, <a href="http://www.gautamkamath.com" target="_blank" rel="noreferrer noopener">Gautam Kamath</a> comments on his <a href="http://proceedings.mlr.press/v132/aden-ali21a.html" target="_blank" rel="noreferrer noopener">paper</a> with co-authors Ishaq Aden-Ali and Hassan Ashtiani, “This paper focuses on a very simple but surprisingly challenging question: Can we learn a general multivariate Gaussian under the constraint of differential privacy? Prior works focused on restricted settings — for example, with bounded parameters or known covariance. We gave the first finite sample complexity bound for this problem, which evidence suggests is near optimal. The next question is to design a computationally efficient algorithm for this problem.” Another <a href="http://proceedings.mlr.press/v132/cesar21a.html" target="_blank" rel="noreferrer noopener">paper</a> on privacy, by Mark Cesar and Ryan Rogers, studies the <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf" target="_blank" rel="noreferrer noopener">composition</a> of various privacy mechanisms in the context of real-world data analytics pipelines.</p>



<p>Another aspect of respecting user privacy is allowing people to choose to stop sharing their data. Concretely, this means removing their data from data sets and ensuring that existing and future models do not make use of their data in any way. One name for this process is <em>machine unlearning</em>, and the main challenge is removing the data efficiently without retraining all models from scratch. One <a href="http://proceedings.mlr.press/v132/neel21a.html" target="_blank" rel="noreferrer noopener">paper</a>, by Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi, addresses this challenge. They propose ways to strategically update the model by using modified gradient descent methods. They also analyze this approach, and prove new upper and lower bounds for updating models after data deletion with their new optimization algorithm. </p>



<p>Robust methods for machine learning and statistics aim to provide rigorous guarantees in the presence of outliers or adversarially modified data points. The field of robustness has been steadily growing as researchers uncover more and more models where deviations in the data can lead to unexpected and dramatic changes in model behavior. In ALT 2021, a <a href="http://proceedings.mlr.press/v132/shen21a.html" target="_blank" rel="noreferrer noopener">paper</a> by Jie Shen and Chicheng Zhang covers learning half-spaces nearly optimally even in the presence of malicious noise.</p>



<p>As a final and thought-provoking direction in trustworthy ML, <a href="https://omereingold.wordpress.com" target="_blank" rel="noreferrer noopener">Omer Reingold</a> points out that we need to better understand “the meaning of individual probabilities/risk scores,” which are common ways that ML systems summarize or justify decisions. Ideally, the output of a model should be something that people can interpret directly and use to potentially modify their future actions. He elaborates that it is important to think about “the individual quantities (which imply important decisions) that ML is trying to approximate” and to answer “what does fitting the parameters of a model on the entire population imply for individuals and subcommunities?” This question brings to the forefront the fact that ML systems affect both individuals and groups of people, which is an important consideration when formulating rigorous definitions of fairness (e.g., see <a href="https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207" target="_blank" rel="noreferrer noopener">this book</a> or <a href="https://fairmlbook.org/" target="_blank" rel="noreferrer noopener">this one</a>).</p>



<p><strong>Reinforcement Learning</strong><br />Reinforcement learning (RL) is a framework for interactive learning where an agent interacts with an environment, and the agent’s actions govern the rewards it receives from the environment. Part of the motivation for studying RL is that relevant problems are everywhere. Sometimes the agents are autonomous vehicles. Other times, they are programs playing games like chess or go. People interact more and more with ML models, and hence, living our lives is actually being a part of a multiagent game where we, humans, and the ML models are the agents. “The most exciting direction in learning theory of recent years,” says <a href="https://www.ehazan.com" target="_blank" rel="noreferrer noopener">Elad Hazan</a>, “is adding rigorous theoretical guarantees to reinforcement learning.” </p>



<p>The RL environment is typically modeled as a <em>Markov decision process (MDP)</em>: a set of states, actions, and transition probabilities that determine the next state and reward given the agent’s current state and action. The agent uses a <em>policy</em> to choose its action from each state with the goal of maximizing its cumulative reward over time. A central challenge is balancing<em> exploration</em> (learning about the environment) and <em>exploitation</em> (spending time choosing actions in states where they can collect high rewards).</p>



<p>In the most basic setting, <em>multi-armed bandits</em>, there is a single state, and each action (or “arm”) leads to a stochastic reward. “Here, the theory is quite mature, though interesting problems remain in connection to the limits of how structure can be exploited,” <a href="https://sites.ualberta.ca/~szepesva/" target="_blank" rel="noreferrer noopener">Csaba Szepesvári</a> says. In two works at ALT, by <a href="http://proceedings.mlr.press/v132/jourdan21a.html" target="_blank" rel="noreferrer noopener">Marc Jourdan, Mojmír Mutný, Johannes Kirschner, and Andreas Krause</a> and by <a href="http://proceedings.mlr.press/v132/cuvelier21a.html" target="_blank" rel="noreferrer noopener">Thibaut Cuvelier, Richard Combes, and Eric Gourdin</a>, the authors show that efficient exploration is possible in a <em>combinatorial semi-bandit </em>setting. Here, the agent can choose an allowed <em>set</em> of arms in each step, and it receives a distinct reward for each chosen arm. While the number of action choices for the agent is larger, this more detailed feedback makes the problem tractable.</p>



<p>Beyond the stateless bandit setting, ML theorists are still figuring out how fast agents can learn how to play optimally in MDPs with <em>finite</em> state spaces and action spaces. Recent progress on this front has given lower bounds on the sample complexity required for agents to learn the best policy. In one ALT <a href="http://proceedings.mlr.press/v132/domingues21a.html" target="_blank" rel="noreferrer noopener">paper</a>, the authors give a unified view of lower bounds for three distinct, but related, problems in RL. The hard MDP instances they construct to show lower bounds are based on hard instances for multi-armed bandit problems.</p>



<p>In the more challenging setting where the state space is infinite, a central question is whether the agent can learn from exploring a finite number of states, and generalize<em> </em>to perform well on unknown areas of the environment. For certain MDPs, generalizing is impossible, but some assumptions on the structure of the MDP may enable generalization. “While algorithm independent problem formulations existed and have been studied in the finite case, a quite recent development is to extend these to the case of ‘large’ environments where the use of <em>function approximation </em>techniques becomes crucial for achieving nontrivial results,” explains Csaba Szepesvári. </p>



<p>Function approximation has to do with the optimal <em>action-value function, </em>which captures the long-term reward of playing a certain action from a given state. This function can sometimes be approximated by some simple class of functions. One of the strongest such assumptions is <em>linear realizability, </em>where the optimal action-value function<em> </em>is a linear function of some representation of the action and state. In one of the <a href="http://proceedings.mlr.press/v132/weisz21a.html" target="_blank" rel="noreferrer noopener">papers</a> receiving a best paper award in ALT, Gellert Weisz, Philip Amortila, and Csaba Szepesvári show that even under this strong assumption of linear realizability, the agent needs a number of samples exponential in the length of the episode or the dimension of the representation in order to generalize. Looking forward, the goal is to follow the lead of these papers and better understand the landscape of sample complexity: When can we learn models with a polynomial number of samples, and when is an exponential number necessary?</p>



<p>Nearly every offline learning problem can be studied in an interactive setting, where inputs arrive in an online fashion and need to be processed immediately, which is common in many real-world settings. Models for interactive machine learning provide a framework for studying problems and algorithms in this more challenging setting. Beyond the MDP setting, interactive learning spans online learning (e.g., <a href="http://proceedings.mlr.press/v132/moshkovitz21a.html" target="_blank" rel="noreferrer noopener">no-substitution</a> <a href="http://proceedings.mlr.press/v132/bhattacharjee21a.html" target="_blank" rel="noreferrer noopener">clustering</a>), nonstochastic control theory (e.g., <a href="https://arxiv.org/abs/1911.12178" target="_blank" rel="noreferrer noopener">robust controllers for dynamical systems</a>), <a href="https://arxiv.org/abs/1909.05207" target="_blank" rel="noreferrer noopener">online convex optimization</a>, and many more domains. </p>



<p><strong>Conclusion</strong><br />We hope this provides a fairly broad view on some of the topics that people are researching right now in learning theory. Of course, there are many more areas that we don’t have space to describe: theory of deep neural networks, quantum algorithms for machine learning problems, human-centered considerations, learning with strategic agents and multiplayer games, convex/nonconvex optimization, federated and distributed learning algorithms, and many more. In general, as Gautam Kamath observes, “A lot of important questions in learning theory arise through interplay between the theoretical and applied machine learning communities.” To have a greater impact, it is important to collaborate with people doing empirical research, and to learn from the front lines about the most interesting phenomena to explain, or the challenges that do not seem surmountable by combining existing tools.</p>



<p>To learn more and to get more involved, we have listed a variety of resources (blogs, workshops, videos, etc.) that can help you get started in this area. As a final motivation for writing this article, we remark that people in the area are keenly aware that we need more young talent to help uncover truth and contribute groundbreaking ideas. As Gautam Kamath puts it, “There are far more interesting questions in learning theory than there are researchers to solve them.”</p>



<h2>Places to learn more</h2>



<p><strong>Blogs:</strong> <a href="https://ucsdml.github.io" target="_blank" rel="noreferrer noopener">UCSD ML blog</a>, <a href="http://www.offconvex.org" target="_blank" rel="noreferrer noopener"><em>Off the Convex Path</em></a>, <a href="https://windowsontheory.org" target="_blank" rel="noreferrer noopener"><em>Windows On Theory</em></a>, <a href="https://blogs.princeton.edu/imabandit/" target="_blank" rel="noreferrer noopener"><em>I’m a bandit</em></a>, <a href="https://francisbach.com" target="_blank" rel="noreferrer noopener">Francis Bach’s blog</a>, <a href="https://differentialprivacy.org" target="_blank" rel="noreferrer noopener">Differential Privacy blog</a>, <a href="https://distill.pub" target="_blank" rel="noreferrer noopener"><em>Distill</em></a>, <a href="http://thegradient.pub" target="_blank" rel="noreferrer noopener"><em>The Gradient</em></a></p>



<p><strong>Conferences:</strong> <a href="http://algorithmiclearningtheory.org/alt2021/" target="_blank" rel="noreferrer noopener">ALT</a>, <a href="http://learningtheory.org/colt2021/" target="_blank" rel="noreferrer noopener">COLT</a>, <a href="https://icml.cc" target="_blank" rel="noreferrer noopener">ICML</a>, <a href="https://nips.cc" target="_blank" rel="noreferrer noopener">NeurIPS</a>, <a href="https://aistats.org/aistats2021/" target="_blank" rel="noreferrer noopener">AISTATS</a>, <a href="https://www.auai.org/uai2021/" target="_blank" rel="noreferrer noopener">UAI</a>, <a href="https://responsiblecomputing.org" target="_blank" rel="noreferrer noopener">FORC</a>, <a href="http://acm-stoc.org/stoc2021/" target="_blank" rel="noreferrer noopener">STOC</a>, <a href="http://ieee-focs.org" target="_blank" rel="noreferrer noopener">FOCS</a>, <a href="http://itcs-conf.org" target="_blank" rel="noreferrer noopener">ITCS</a>, <a href="https://iclr.cc/virtual_2020/index.html" target="_blank" rel="noreferrer noopener">ICLR</a>, <a href="https://dl.acm.org/conference/soda" target="_blank" rel="noreferrer noopener">SODA</a></p>



<p><strong>Podcasts:</strong> <a href="https://twimlai.com" target="_blank" rel="noreferrer noopener"><em>TWIML</em></a>, <a href="https://wandb.ai/site/podcast" target="_blank" rel="noreferrer noopener"><em>Gradient Dissent</em></a>, <a href="https://www.quantamagazine.org/tag/the-joy-of-x/" target="_blank" rel="noreferrer noopener"><em>Joy of x</em></a>, <a href="https://shows.acast.com/the-robot-brains" target="_blank" rel="noreferrer noopener"><em>The Robot Brains</em></a>, <a href="https://www.thetalkingmachines.com/home" target="_blank" rel="noreferrer noopener"><em>Talking Machines</em></a>, <a href="https://www.talkrl.com/episodes/marc-bellemare" target="_blank" rel="noreferrer noopener"><em>TalkRL</em></a>, <a href="https://www.listennotes.com/podcasts/underrated-ml-sara-hooker-sean-hooker-nG4owP2C8s3/" target="_blank" rel="noreferrer noopener"><em>Underrated ML</em></a></p>



<p><strong>Videos:</strong> <a href="https://simons.berkeley.edu" target="_blank" rel="noreferrer noopener">Simons Institute</a>, <a href="https://www.youtube.com/watch?v=IXotICNx2qk&amp;list=PLdDZb3TwJPZ5dqqg_S-rgJqSFeH4DQqFQ" target="_blank" rel="noreferrer noopener">IAS deep learning workshop</a>, <a href="https://www.oneworldml.org/home" target="_blank" rel="noreferrer noopener">One World ML</a>, <a href="https://www.trustworthyml.org" target="_blank" rel="noreferrer noopener">Trustworthy ML</a>, <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener"><em>Foundations of Data Science</em>,</a> <a href="https://sites.google.com/view/rltheoryseminars/home" target="_blank" rel="noreferrer noopener">RL Theory Virtual Seminars</a>, <a href="https://www.imsi.institute/the-multifaceted-complexity-of-machine-learning/" target="_blank" rel="noreferrer noopener">iMSi: The Multifaceted Complexity of Machine Learning</a>, <em><a href="https://sites.google.com/view/control-meets-learning/home" target="_blank" rel="noreferrer noopener">Control Meets Learning</a></em> </p>



<p><strong>Acknowledgments:</strong> We thank Kamalika Chaudhuri, Elad Hazan, Daniel Hsu, Gautam Kamath, Shay Moran, Omer Reingold, Csaba Szepesvári, and Claire Vernade for helpful comments and thoughtful quotes. We thank Kush Bhatia, Lee Cohen, Neha Gupta, Nika Haghtalab, Max Hopkins, Gautam Kamath, Gaurav Mahajan, and Uri Sherman for helpful feedback on initial drafts.</p>



<div class="wp-block-image"><figure class="alignleft is-resized"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/GlasgowMargalit-2-edited-1.jpg" class="wp-image-339" height="150" /></figure></div>



<p class="has-text-align-left"><a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener"><strong>Margalit Glasgow</strong></a> is a PhD student in Stanford’s Computer Science Department, advised by Mary Wootters. Her research focuses on theoretical machine learning and random matrices.</p>



<div style="height: 20px;" class="wp-block-spacer"></div>



<div class="wp-block-image"><figure class="alignright size-large is-resized"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/MoshkovitzMichal.jpg" class="wp-image-327" height="150" /></figure></div>



<p><a href="https://sites.google.com/view/michal-moshkovitz" target="_blank" rel="noreferrer noopener"><strong>Michal Moshkovitz</strong></a> is a postdoc at the Qualcomm Institute at UC San Diego. She received her PhD and MSc in computational neuroscience from the Hebrew University of Jerusalem, and her MSc in computer science from Tel Aviv University. Her research focuses on the foundations of AI, exploring how different constraints affect learning. She has worked on bounded-memory learning, explainable machine learning, and online decision-making in unsupervised learning. </p>



<div class="wp-block-image"><figure class="alignleft size-thumbnail"><img width="150" alt="" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/RashtchianCyrus-150x150.jpg" class="wp-image-332" height="150" /></figure></div>



<p><strong><a href="https://sites.google.com/site/cyrusrashtchian/" target="_blank" rel="noreferrer noopener">Cyrus Rashtchian</a></strong> is a postdoc at UC San Diego in computer science and engineering, and he received his PhD from the University of Washington. His research focuses on trustworthy machine learning, algorithms for big data, statistical reconstruction, and DNA data storage.</p></div>







<p class="date">
by 1737780 <a href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/"><span class="datestr">at July 19, 2021 09:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2541777959928040133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/political-intersections-trump-honors.html">Political Intersections: Trump honors Antifa member who was shot dead by police</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> 1) Trump and other reps have said the following about the Jan 6 event at various times:</p><p>a) The Jan 6 event was freedom fighters who were fighting the noble fight to overturn a fraudulent election. Rah Rah!</p><p>b) The Jan 6 event was a peaceful protest to overturn a fraudulent election. </p><p>c) The Jan 6 event was democrats trying to make republicans look bad.</p><p>d) The Jan 6 event was Antifa. (See <a href="https://www.reuters.com/article/us-usa-election-trump/trump-privately-blamed-antifa-people-for-storming-u-s-capitol-axios-idUSKBN29H0DR">here</a>)</p><p>e) Ashli Babbitt (a protestor who was shot by police at the Jan 6 event) should have been honored by flying the flag at half-mast (see <a href="https://www.mediaite.com/trump/trump-reportedly-regrets-not-lowering-american-flag-at-the-white-house-to-honor-ashli-babbitt/">here</a>) </p><p>If we take the intersection of d and e we find that Trump wants to honor a member of Antifa who was shot by police. </p><p>To be fair, Trump is entitled to change his mind. But I wonder- did he ever really think it was Antifa or was that a talking point? If he really thought so then  when did he change his mind? I ask non rhetorically---  NOT a `gotcha question'</p><p>(NOTE: I wrote this post a while back. Since then Trump and some other republicans are tending towards the Freedom Fighters narrative.) </p><p>2) Vaccines:</p><p>a) Some people think that the vaccines are bad to take. I suspect they would give some (incorrect) health reasons, while the real reason may be political. (One reason is that the vaccine make you magnetic. That sounds awesome! Others think that there is a microchip in the vaccine so that Bill Gates can track our movements. Gee, Mark Zuckerberg can already do that. Some thing it will rewrite our DNA. A bio major I know  tells me that such people are confusing messenger RNA with DNA. Great- we can now have a nice conversation and point out where they are wrong.) </p><p>b) Some people think we should NOT give vaccines to poor countries that need them, or to people in prison,  since Americans should have priority. (NOTE- from a purely health-viewpoint this is not correct since a pandemic does not respect boundaries- if there is an outbreak in a diff country or in a prison it will affect people not in those countries and not in prison.) </p><p>Are there people who believe a and b? I ask non-rhetorically. </p><p><br /></p><p>I am not surprised when people hold contradictory thoughts in their heads, but these two cases just struck me as particularly strange. Not sure why.</p><p> </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/political-intersections-trump-honors.html"><span class="datestr">at July 18, 2021 06:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/104">TR21-104 |  Does QRAT simulate IR-calc? QRAT simulation algorithm for $\forall$Exp+Res cannot be lifted to IR-calc | 

	Sravanthi Chede, 

	Anil Shukla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that the QRAT simulation algorithm of $\forall$Exp+Res from [B. Kiesl and M. Seidl, 2019] cannot be lifted to IR-calc.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/104"><span class="datestr">at July 18, 2021 04:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/103">TR21-103 |  Elliptic Curve Fast Fourier Transform (ECFFT) Part I: Fast Polynomial Algorithms over all Finite Fields | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Swastik Kopparty, 

	David Levit</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Over finite fields $F_q$ containing a root of unity of smooth order $n$ (smoothness means $n$ is the product of small primes), the Fast Fourier Transform (FFT) leads to the fastest known algebraic algorithms for many basic polynomial operations, such as multiplication, division, interpolation and multi-point evaluation. These operations can be computed by constant fan-in arithmetic circuits over $F_q$ of quasi-linear size; specifically, $O(n \log n)$ for multiplication and division, and $O(n \log^2 n)$ for interpolation and evaluation.

However, the same operations over fields with no smooth order root of unity suffer from an asymptotic slowdown, typically due to the need to introduce “synthetic” roots of unity to enable the FFT. The classical algorithm of Schönhage and Strassen incurred a multiplicative slowdown factor of $\log \log n$ on top of the smooth case. Recent remarkable results of Harvey, van der Hoeven and Lecerf dramatically reduced this multiplicative overhead to $\exp(\log^* (n))$.

We introduce a new approach to fast algorithms for polynomial operations over all large finite fields. The key idea is to replace the group of roots of unity with a set of points $L \subset F_q$ suitably related to a well-chosen elliptic curve group over $F_q$ (the set L itself is not a group). The key advantage of this approach is that elliptic curve groups can be of any size in the Hasse–Weil interval $[q + 1 \pm 2\sqrt{q}]$ and thus can have subgroups of large, smooth order, which an FFT-like divide and conquer algorithm can exploit. Compare this with multiplicative subgroups over $F_q$ whose order must divide $q-1$. By analogy, our method extends the standard, multiplicative FFT in a similar way to how Lenstra’s elliptic curve method extended Pollard’s $p-1$ algorithm for factoring integers.

For polynomials represented by their evaluation over subsets of $L$, we show that  multiplication, division, degree-computation, interpolation, evaluation and Reed–Solomon encoding (also known as low-degree extension) with fixed evaluation points can all be computed with arithmetic circuits of size similar to what is achievable with the classical FFTs when the field size $q$ is special. For several problems, this yields the asymptotically smallest known arithmetic circuits even in the standard monomial representation of polynomials.

The efficiency of the classical FFT follows from using the 2-to-1 squaring map to reduce the evaluation set of roots of unity of order $2^k$ to similar groups of size $2^{k?i}$, $i &gt; 0$. Our algorithms operate similarly, using isogenies of elliptic curves with kernel size 2 as 2-to-1 maps to reduce $L$ of size $2^k$ to sets of size $2^{k?i}$ that are, like $L$, suitably related to elliptic curves, albeit different ones.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/103"><span class="datestr">at July 18, 2021 02:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">5th SIAM Symposium on Simplicity in Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 10-11, 2022 Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa22 Submission deadline: August 9, 2021 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding of the … <a href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">5th SIAM Symposium on Simplicity in Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/"><span class="datestr">at July 18, 2021 01:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-07-17-simplifying-raft-with-chaining/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-07-17-simplifying-raft-with-chaining/">Simplifying Raft with Chaining</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Raft is a consensus algorithm for deciding a sequence of commands to execute on a replicated state machine. Raft is famed for its understandability (relative to other consensus algorithms such as Paxos) yet some aspects of the protocol still require careful treatment. For instance, determining when it is safe for...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-07-17-simplifying-raft-with-chaining/"><span class="datestr">at July 17, 2021 03:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/">Assistant/Associate/Full Professorship in Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by September 12, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The researcher will join a rapidly growing department, with strong research sections in the areas of Algorithms and Complexity, Machine Learning, Natural Language Processing, Human-Centered Computing, Software Engineering and Data Management, Programming Languages, and Image Analysis.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642</a><br />
Email: simonsen@di.ku.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/"><span class="datestr">at July 16, 2021 08:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia.html">Linkage with many Wikipedia Good Articles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>There are two reasons for the large number of Good Articles in this set. First, I had previously been trying to keep my nominations and reviews in balance, but there were too few nominations to review on topics of interest to me, and the inability to find things to review was preventing me from nominating other articles when they were ready. So I started nominating more often. And second, the Wikipedia Good Articles editors are having a drive this month to clean out old nominations, as they tend to do a couple of times per year.</p>

<ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/07/steve-lindsay-fractal-vise/">Morphing fractal engraving vice jaws</a> (<a href="https://mathstodon.xyz/@11011110/106506737270461558">\(\mathbb{M}\)</a>). Circular arcs nested within circular arcs rotate to conform to whatever shape is being gripped.</p>
  </li>
  <li>
    <p>Christian Lawson-Perfect sets up a new wiki, <a href="https://whystartat.xyz">Why start at \(x,y,z\)?</a> (<a href="https://mathstodon.xyz/@christianp/106500170446647463">\(\mathbb{M}\)</a>). Its aim is to collect ambiguous, inconsistent, or just plain unpleasant conventions in mathematical notation. My contribution: <a href="https://whystartat.xyz/wiki/Big_O_notation">big O notation</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Pick%27s_theorem">Pick’s theorem </a> (<a href="https://mathstodon.xyz/@11011110/106523554824974108">\(\mathbb{M}\)</a>). The area of a grid polygon equals its number of interior grid points, plus half the boundary points, minus one. Good Article #1.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/07/03/the-problem-with-combinatorics-textbooks/">The problem with combinatorics textbooks</a> (<a href="https://mathstodon.xyz/@11011110/106526891677914307">\(\mathbb{M}\)</a>). Igor Pak on the difficulty of teaching combinatorics in a comprehensive way in a single term. Instead, he suggests teaching courses on narrower subtopics: “the more specific you make the combinatorics course the more interesting it is to the students”.</p>
  </li>
  <li>
    <p>I recently returned from a relaxing early-long-weekend mini-vacation to Avila Beach (<a href="https://mathstodon.xyz/@11011110/106532239057852428">\(\mathbb{M}\)</a>), with seafood sunset beach dinners (the coast faces south so the sun sets over land), wine tasting (near the setting of Sideways), and sulphur springs hot tub soaks. The photo below is a garden in a field of rusted pylons in the flood basin of San Luis Obispo Creek. I liked its contrast of natural growth and regular artificial forms.</p>

    <p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/sycsprings/SecretGarden-m.jpg" style="border-style: solid; border-color: black;" alt="Secret garden, Sycamore Springs Resort, Avila Beach" /></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formula">Viète’s formula</a> (<a href="https://mathstodon.xyz/@11011110/106535094271822431">\(\mathbb{M}\)</a>), an infinite product of nested roots evaluating to \(2/\pi\), “the first formula of European mathematics to represent an infinite process”. Good Article #2.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2021/06/25/science/puzzles-fonts-math-demaine.html">The <em>New York Times</em> on Erik and Marty Demaine’s mathematical typefaces</a> (<a href="https://mathstodon.xyz/@11011110/106545874983076062">\(\mathbb{M}\)</a>, <a href="https://archive.ph/oJ8xG">also</a>, <a href="http://stormbear.com/carnival-of-mathematics-195-july-2021/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Euclid%E2%80%93Euler_theorem">Euclid–Euler theorem</a> (<a href="https://mathstodon.xyz/@11011110/106554476459806645">\(\mathbb{M}\)</a>). A 2-millenium-long collab in which Euclid proved that all Mersenne primes produce even perfect numbers, and Euler proved that all even perfect numbers come from Mersenne primes. But let’s not forget <a href="https://en.wikipedia.org/wiki/Ibn_al-Haytham">Ibn al-Haytham</a> (Alhazen), halfway between them in time, who conjectured Euler’s result but couldn’t prove it. Good Article #3.</p>
  </li>
  <li>
    <p><a href="https://mirtitles.org/2021/07/07/convex-figures-and-polyhedra-lyusternik/">Lyusternik’s book <em>Convex Figures and Polyhedra</em></a> (<a href="https://mathstodon.xyz/@jarban/106551202604578180">\(\mathbb{M}\)</a>), one of the Mir translations from Russian to English, <a href="https://archive.org/details/lyusternik-convex-figures-and-polyhedra">available without restrictions on archive.org</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Bucket_queue">Bucket queue</a> (<a href="https://mathstodon.xyz/@11011110/106570282017522543">\(\mathbb{M}\)</a>). This priority queue is a bit out of fashion, but good for small integer priorities or for shortest paths when the ratio of longest to shortest edge is small. Good Article #4, despite a reviewer who had <a href="https://en.wikipedia.org/wiki/WP:CHEESE">somehow become convinced that deletion from doubly linked lists is nonconstant</a>. The issue is off-topic but real: updating objects in data structures often needs the objects to track their location in the structure; for instance, changing priorities in a binary heap requires each object to know its index. Most introductory material on these topics and even some standard library implementations (like Python’s heapq) fail to address this complication.</p>
  </li>
  <li>
    <p><a href="https://www.cs.ru.nl/~freek/100/">Formalizing 100 theorems</a> (<a href="https://mathstodon.xyz/@11011110/106579903443188306">\(\mathbb{M}\)</a>). Freek Wiedijk uses a rather arbitrary collection of 100 favorite theorems from some 1999 web page as a benchmark set for the progress of automatic proof assistants. I’m sad that Pick’s theorem has seen so little love.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.06490">Greedy spanners in Euclidean spaces admit sublinear separators</a> (<a href="https://mathstodon.xyz/@11011110/106583477388316730">\(\mathbb{M}\)</a>). My <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">SoCG’21 result with Hadi Khodabandeh that 2d greedy spanners have separators of size \(O(\sqrt{n})\)</a> used crossing-based methods heavily dependent on planarity. This new preprint by Hung Le and Cuong Than uses different ideas to find separators of size \(O(n^{1-1/d})\), optimal in any dimension \(d\). Their work also extends from Euclidean spaces to doubling spaces.</p>
  </li>
  <li>
    <p><a href="https://oscarcunningham.com/670/unique-distancing-problem/">Unique distancing</a> (<a href="https://mathstodon.xyz/@11011110/106587574216525670">\(\mathbb{M}\)</a>). How many points can you place in an \(n\times n\) grid so that all pairwise distances are distinct? The linked post concerns whether \(n\) points are possible (no for all but finitely many cases because there are too many pairs and too few sums of squares) but it also looks interesting to maximize the number of points.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia.html"><span class="datestr">at July 15, 2021 06:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18952">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">Socially Reproduced Experiments</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>We must avoid becoming one</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img width="151" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" class="alignright size-full wp-image-18954" height="151" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseball’s All-Star break, we review this and other social experiments that have quite a bit more data.<br />
<span id="more-18952"></span></p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankees’ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astros’ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankees’ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p></p><h2> Examples and Non-Examples </h2><p></p>
<p></p><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a “social proof” of an assertion—especially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p></p><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" />-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of “statistical proof.” Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, I’ve regularly observed poor pitching (by the closers on my “fantasy team”) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no “save” to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" />-scores I use in chess, found none. “Meltdowns” like Chapman’s are offset by cases where closers pitched better. The <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z}" class="latex" />-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. England’s and Italy’s teams brought their long tortured histories together in the tiebreak of Sunday’s European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p></p><h2> Larger Scale </h2><p></p>
<p></p><p>
Dick and I are really interested in “experiments” that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as “Developing a blueprint for a science of cybersecurity.” Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, “Cybersecurity: From engineering to science.” <p></p>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, “Blueprint for a science of cybersecurity.” <p></p>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, “Making experiments dependable,” which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxion’s main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this nature—okay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneider’s paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> “Some health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.” </em>
</p></blockquote>
<p></p><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a ‘cure’ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, however—in both health and security—is uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p></p><h2> Summer Pandemic Update </h2><p></p>
<p></p><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summer—for Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img width="450" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" class="aligncenter wp-image-18955" height="440" /></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at London’s Wembley Stadium for the semis and final—and anything similar in baseball—would stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the “breakthrough” positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Biden’s vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Dick and I tried to come up with other examples—from computer security in particular—to sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p></p><p><br />
[some word fixes and changes]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/"><span class="datestr">at July 13, 2021 07:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/102">TR21-102 |  Tight bounds on the Fourier growth of bounded functions on the hypercube | 

	Siddharth Iyer, 

	Anup Rao, 

	Victor Reis, 

	Thomas Rothvoss, 

	Amir Yehudayoff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give tight bounds on the degree $\ell$  homogenous parts $f_\ell$ of a bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow [-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by $d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell e^{{\ell+1 \choose 2}} n^{\frac{\ell-1}{2}}$. We describe applications to pseudorandomness and learning theory. We use similar methods to generalize the classical Pisier's inequality from convex analysis. Our analysis  involves properties of real-rooted polynomials that may be useful elsewhere.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/102"><span class="datestr">at July 13, 2021 05:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/101">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/101">TR21-101 |  A Parallel Repetition Theorem for the GHZ Game: A Simpler Proof | 

	Uma Girish, 

	Justin Holmgren, 

	Kunal Mittal, 

	Ran Raz, 

	Wei Zhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give a new proof of the fact that the parallel repetition of the (3-player) GHZ game reduces the value of the game to zero polynomially quickly. That is, we show that the value of the $n$-fold GHZ game is at most $n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We present a new proof of this theorem that we believe to be simpler and more direct. Unlike most previous works on parallel repetition, our proof makes no use of information theory, and relies on the use of Fourier analysis.

The GHZ game [GHZ89] has played a foundational role in the understanding of quantum information theory, due in part to the fact that quantum strategies can win the GHZ game with probability $1$. It is possible that improved parallel repetition bounds may find applications in this setting.

Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game as a simple three-player game, which is in some sense maximally far from the class of multi-player games whose behavior under parallel repetition is well understood. Dinur et al. conjectured that parallel repetition decreases the value of the GHZ game exponentially quickly, and speculated that progress on proving this would shed light on parallel repetition for general multi-player (multi-prover) games.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/101"><span class="datestr">at July 13, 2021 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://sarielhp.org/blog/?p=9420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sariel.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://sarielhp.org/blog/?p=9420">FSTTCS 2021 deadline is this Monday…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The link is <a href="https://www.fsttcs.org.in/2021/" target="_blank" rel="noreferrer noopener">here</a>. Due to the pandemic it is going to be virtual.</p>



<p>Quoting Bob Dylan,  this blog is not dead, it is just asleep.</p></div>







<p class="date">
by Sariel <a href="https://sarielhp.org/blog/?p=9420"><span class="datestr">at July 13, 2021 03:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4532">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/">What a difference a few months can make</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Piazza Duomo, in Milan, on December 26, 2020</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>Piazza Duomo, in Milan, on July 11, 2021</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/"><span class="datestr">at July 12, 2021 05:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/exponential-mechanism-bounded-range/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/exponential-mechanism-bounded-range/">A Better Privacy Analysis of the Exponential Mechanism</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\), output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection – in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for private selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. The exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{y’ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,x’)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(x’\) differing on the data of a single individual (which we denote by \(d(x,x’)\le 1\)).</p>

<p>In terms of utility, we can easily show that <a href="https://arxiv.org/abs/1511.02513" title="Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman. Algorithmic Stability for Adaptive Data Analysis. STOC 2016."><strong>[BNSSSU16]</strong></a> \[\mathbb{E}[\ell(M(x),x)] \le \min_{y \in \mathcal{Y}} \ell(y,x) + \frac{2\Delta}{\varepsilon} \log |\mathcal{Y}|\] for all \(x \in \mathcal{X}^n\) (and we can also give high probability bounds).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! We’re going to look at a more refined privacy analysis.</p>

<h2 id="bounded-range">Bounded Range</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em>. This was observed and defined by David Durfee and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> and further analyzed later <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range).</strong><sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right) \in [t, t+\eta].\] Here \(t\) may depend on the pair of input datasets \(x,x’\), but not on the output \(y\).</p>
</blockquote>

<p>To interpret this definition, we <a href="https://differentialprivacy.org/flavoursofdelta/">recall the definition of the privacy loss random variable</a>: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \[f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right).\] Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-differential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) – i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,x’\).</p>

<p>Bounded range and pure differential privacy are equivalent up to a factor of 2 in the parameters:</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p><em>Proof.</em> The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\)  would imply \(\forall y ~ \mathbb{P}[M(x)=y]&gt;\mathbb{P}[M(x’)=y]\) or \(\forall y ~ \mathbb{P}[M(x)=y]&lt;\mathbb{P}[M(x’)=y]\), contradicting the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x’)=y] = 1\). ∎</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (The Exponential Mechanism is Bounded Range).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range .<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x’))} \cdot \frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}.\]
Setting \(t = \log\left(\frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,x’)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,x’)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). ∎</p>

<p>Bounded range is not really a useful privacy definition on its own. Thus we’re going to relate it to a relaxed version of differential privacy next.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and its variants <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. Rényi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> are relaxations of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is the privacy loss random variable.<sup id="fnref:4"><a href="https://differentialprivacy.org/feed.xml#fn:4" class="footnote" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a href="https://differentialprivacy.org/feed.xml#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact – that we want to draw more attention to – is that we can do better! 
Specifically, \(\eta\)-bounded range implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>.
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers, but with some simplification.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,x’ \in \mathcal{X}^n\) differing on a single individual’s data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffding’s Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffding’s Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a href="https://differentialprivacy.org/feed.xml#fn:6" class="footnote" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac18 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] \!=\! \sum_y \mathbb{P}[M(x)\!=\!y] \exp(-f(y)) \!=\! \sum_y \mathbb{P}[M(x)\!=\!y]  \!\cdot\! \frac{\mathbb{P}[M(x’)\!=\!y]}{\mathbb{P}[M(x)\!=\!y]} \!=\! 1.\]
∎</p>

<p>This brings us to the TL;DR of this post:</p>

<blockquote>
  <p><strong>Corollary 7.</strong> The exponential mechanism (given by Equation 1) is \(\frac18 \varepsilon^2\)-concentrated differentially private.</p>
</blockquote>

<p>This is great news. The standard analysis only gives \(\frac12 \varepsilon^2\)-concentrated differential privacy. Constants matter when applying differential privacy, and we save a factor of 4 in the concentrated differential privacy analysis of the exponential mechanism for free with this improved analysis.</p>

<p>Combining Lemma 2 with Theorem 5 also gives a simpler proof of the conversion from pure differential privacy to concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>:</p>

<blockquote>
  <p><strong>Corollary 8.</strong> \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy.</p>
</blockquote>

<h2 id="beyond-the-exponential-mechanism">Beyond the Exponential Mechanism</h2>

<p>The exponential mechanism is not the only algorithm for private selection. A closely-related algorithm is <em>report noisy max/min</em>:<sup id="fnref:7"><a href="https://differentialprivacy.org/feed.xml#fn:7" class="footnote" rel="footnote">7</a></sup> Draw independent noise \(\xi_y\) from some distribution for each \(y \in \mathcal{Y}\) then output \[M(x) = \underset{y \in \mathcal{Y}}{\mathrm{argmin}} ~ \ell(y,x) - \xi_y.\]</p>

<p>If the noise distribution is an appropriate <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, then report noisy max is exactly the exponential mechanism. (This equivalence is known as the “Gumbel max trick.”)</p>

<p>We can also use the Laplace distribution or the exponential distribution. Report noisy max with the exponential distribution is equivalent to the <em>permute and flip</em> algorithm <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>. However, these algorithms don’t enjoy the same improved bounded range and concentrated differential privacy guarantees as the exponential mechanism.</p>

<p>There are also other variants of the selection problem. For example, in some cases we can assume that only a few options have low loss and the rest of the options have high loss – i.e., there is a gap between the minimum loss and the second-lowest loss (or, more generally, the \(k\)-th lowest loss). In this case there are algorithms that attain better accuracy than the exponential mechanism under relaxed privacy definitions <a href="https://arxiv.org/abs/1409.2177" title="Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014."><strong>[CHS14]</strong></a> <a href="https://dl.acm.org/doi/10.1145/3188745.3188946" title=" Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018."><strong>[BDRS18]</strong></a> <a href="https://arxiv.org/abs/1905.13229" title="Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019."><strong>[BKSW19]</strong></a>.</p>

<p>There are a lot of interesting aspects of private selection, including questions for further research! We hope to have further posts about some of these topics.</p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets. <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta\) <a href="https://arxiv.org/abs/2004.00010" title="Clément L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020."><strong>[CKS20]</strong></a>. (To be completely precise, we must appropriately deal with the \(Z=\infty\) case, which we ignore in this discussion for simplicity.) <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},x’) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},x’) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\). <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(x’)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(x’)))\) is the order \(\lambda+1\) Rényi divergence of \(M(x)\) from \(M(x’)\). <a href="https://differentialprivacy.org/feed.xml#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,x’\in\mathcal{X}^n : d(x,x’)\le1} \|q(x)-q(x’)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\) <a href="https://differentialprivacy.org/feed.xml#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(x’) )\). <a href="https://differentialprivacy.org/feed.xml#fnref:6" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:7">
      <p>We have presented selection here in terms of minimization, but most of the literature is in terms of maximization. <a href="https://differentialprivacy.org/feed.xml#fnref:7" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Thomas Steinke <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><span class="datestr">at July 12, 2021 05:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/100">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/100">TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/100"><span class="datestr">at July 12, 2021 07:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6138760775046652444">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html">Would you take this bet (Part 2) ?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br /></p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br /></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br /></p><p>The expected value is infinity.</p><p><br /></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below. </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i> might be a paradox since most people would say NO even though the expected value is infinity.  See <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a> for more background.</p><p>Shapley (see <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would not  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it. </p><p><br /></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars. </p><p>The amount of money in the world is, according to <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a> around 1.2 quadrillion dollars  which is roughly 2^{40} dollars. </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play. </p><p><br /></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20. </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win. </p><p>Does Shapley's discussions  <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then Shapley  resolves the paradox  by contrasting the real world to the math world. </p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html"><span class="datestr">at July 12, 2021 12:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html">Angles of arc-triangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the <a href="https://en.wikipedia.org/wiki/Lune_of_Hippocrates">lune of Hippocrates</a>, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a>, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</p>

<p>I looked at one of these properties, <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">the ability of arc-triangles to tile the plane</a>, in an earlier post. Another of these properties involves the feasible combinations of angles of these shapes. As is well known, in a straight-sided triangle in the plane, the three interior angles always sum to exactly \(\pi\), and any three positive angles summing to \(\pi\) are possible. Let \(T\) be the set of triples of angles \((\theta_1,\theta_2,\theta_3)\) from triangles, and reinterpret these triples as coordinates of points in Euclidean space. Then \(T\) is itself an equilateral triangle, with corners at the three points \((\pi,0,0)\), \((0,\pi,0)\), and \((0,0\pi)\). (More precisely, it’s the relative interior of this triangle.)</p>

<p>What about arc-triangles? Are their angles similarly constrained? What shape do their triples of angles make? First of all, their angles don’t have a fixed sum (except for the tilers, for which this sum is again \(\pi\)). The arbelos has three interior angles that are all zero, summing to zero. The Reuleux triangle has three angles of \(2\pi/3\), summing to \(2\pi\). <a href="https://11011110.github.io/blog/2021/05/15/linkage.html">Boscovitch’s cardioid</a>, below, uses three semicircles like the arbelos, but with one interior angle of \(2\pi\) and two others equal to \(\pi\), summing to \(4\pi\). The <a href="https://en.wikipedia.org/wiki/Trefoil">trefoil</a>, a common architectural motif, bulges outward from its three corners, forming interior angles that are much larger, up to \(2\pi\) each for a trefoil made from three \(5/6\)-circle arcs, for a total interior angle of \(6\pi\).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/boscovich.svg" alt="Boscovich's cardioid" /></p>

<p>Nevertheless, for a non-self-crossing arc-triangle, not all combinations of angles are possible. For instance, it’s not possible to have one angle that is zero and two that are \(2\pi\). My new preprint, “Angles of arc-polygons and lombardi drawings of cacti” (<a href="https://arxiv.org/abs/2107.03615">arXiv:2107.03615
</a>, with UCI students Daniel Frishberg and Martha Osegueda, to appear at CCCG) proves a precise characterization: beyond the obvious requirement that each angle \(\theta_i\) be in the range \(0\le\theta_i\le 2\pi\), we have only the additional inequalities</p>

\[-\pi &lt; \frac{\pi - \theta_i + \theta_{i+1} - \theta_{i+2}}{2} &lt; \pi\]

<p>where the index arithmetic is done modulo three. The formula in the middle of each of these inequalities is itself an angle, the angle of incidence between one of the circular arcs of the arc-triangle and the circle through its three corners. Where the straight triangles had an equilateral-triangle feasible region, the arc-triangles have a more complicated shape. The obvious constraints \(0\le\theta_i\le 2\pi\) would produce a cubical feasible region \([0,2\pi]^3\), but the additional inequalities above cut off six corners of the cube, leaving a feasible region looking like this:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/feasible-arc-triangles.svg" alt="The feasible region for triples of angles of arc-triangles" /></p>

<p>The motivating application for all of this is graph drawing, and more specifically Lombardi drawing, in which edges are circular arcs meeting at equal angles at each vertex. Using our new understanding of arc-polygons, we prove that every <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus graph</a> has a planar Lombardi drawing for its natural embedding (the one in which each cycle of the cactus forms a face) but might not for some other embeddings, including the one below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/badhat.svg" alt="An embedded cactus that has no planar Lombardi drawing" /></p>

<p>But beyond graph drawing, I think that the long history and many applications of arc-polygons justifies more study of their general properties. For instance, what about arc-polygons with more than three sides? What can their angles be? Our paper has a partial answer, enough to answer the questions we asked in our Lombardi drawing application, but a complete characterization for arc-polygons of more than three sides is still open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106557711895868173">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html"><span class="datestr">at July 10, 2021 11:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
