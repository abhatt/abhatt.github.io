<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 07, 2021 02:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1584">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/">News for October 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled su-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br />(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/"><span class="datestr">at November 07, 2021 02:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03046">Introduction to Coresets: Approximated Mean</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maalouf:Alaa.html">Alaa Maalouf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jubran:Ibrahim.html">Ibrahim Jubran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Dan.html">Dan Feldman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03046">PDF</a><br /><b>Abstract: </b>A \emph{strong coreset} for the mean queries of a set $P$ in ${\mathbb{R}}^d$
is a small weighted subset $C\subseteq P$, which provably approximates its sum
of squared distances to any center (point) $x\in {\mathbb{R}}^d$. A \emph{weak
coreset} is (also) a small weighted subset $C$ of $P$, whose mean approximates
the mean of $P$. While computing the mean of $P$ can be easily computed in
linear time, its coreset can be used to solve harder constrained version, and
is in the heart of generalizations such as coresets for $k$-means clustering.
In this paper, we survey most of the mean coreset construction techniques, and
suggest a unified analysis methodology for providing and explaining classical
and modern results including step-by-step proofs. In particular, we collected
folklore and scattered related results, some of which are not formally stated
elsewhere. Throughout this survey, we present, explain, and prove a set of
techniques, reductions, and algorithms very widespread and crucial in this
field. However, when put to use in the (relatively simple) mean problem, such
techniques are much simpler to grasp. The survey may help guide new researchers
unfamiliar with the field, and introduce them to the very basic foundations of
coresets, through a simple, yet fundamental, problem. Experts in this area
might appreciate the unified analysis flow, and the comparison table for
existing results. Finally, to encourage and help practitioners and software
engineers, we provide full open source code for all presented algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03046"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03034">Optimal Mixing Time for the Ising Model in the Uniqueness Regime</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xiaoyu.html">Xiaoyu Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Weiming.html">Weiming Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yitong.html">Yitong Yin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Xinyuan.html">Xinyuan Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03034">PDF</a><br /><b>Abstract: </b>We prove an optimal $O(n \log n)$ mixing time of the Glauber dynamics for the
Ising models with edge activity $\beta \in \left(\frac{\Delta-2}{\Delta},
\frac{\Delta}{\Delta-2}\right)$. This mixing time bound holds even if the
maximum degree $\Delta$ is unbounded.
</p>
<p>We refine the boosting technique developed in [CFYZ21], and prove a new
boosting theorem by utilizing the entropic independence defined in [AJK+21].
The theorem relates the modified log-Sobolev (MLS) constant of the Glauber
dynamics for a near-critical Ising model to that for an Ising model in a
sub-critical regime.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03034"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03033">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03033">Computational thresholds for the fixed-magnetization Ising model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlson:Charlie.html">Charlie Carlson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Ewan.html">Ewan Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolla:Alexandra.html">Alexandra Kolla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perkins:Will.html">Will Perkins</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03033">PDF</a><br /><b>Abstract: </b>The ferromagnetic Ising model is a model of a magnetic material and a central
topic in statistical physics. It also plays a starring role in the algorithmic
study of approximate counting: approximating the partition function of the
ferromagnetic Ising model with uniform external field is tractable at all
temperatures and on all graphs, due to the randomized algorithm of Jerrum and
Sinclair. Here we show that hidden inside the model are hard computational
problems. For the class of bounded-degree graphs we find computational
thresholds for the approximate counting and sampling problems for the
ferromagnetic Ising model at fixed magnetization (that is, fixing the number of
$+1$ and $-1$ spins). In particular, letting $\beta_c(\Delta)$ denote the
critical inverse temperature of the zero-field Ising model on the infinite
$\Delta$-regular tree, and $\eta_{\Delta,\beta,1}^+$ denote the mean
magnetization of the zero-field $+$ measure on the infinite $\Delta$-regular
tree at inverse temperature $\beta$, we prove, for the class of graphs of
maximum degree $\Delta$:
</p>
<p>1. For $\beta &lt; \beta_c(\Delta)$ there is an FPRAS and efficient sampling
scheme for the fixed-magnetization Ising model for all magnetizations $\eta$.
</p>
<p>2. For $\beta &gt; \beta_c(\Delta)$, there is an FPRAS and efficient sampling
scheme for the fixed-magnetization Ising model for magnetizations $\eta$ such
that $|\eta| &gt;\eta_{\Delta,\beta,1}^+ $.
</p>
<p>3. For $\beta &gt; \beta_c(\Delta)$, there is no FPRAS for the
fixed-magnetization Ising model for magnetizations $\eta$ such that $|\eta|
&lt;\eta_{\Delta,\beta,1}^+ $ unless NP=RP\@.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03033"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.03005">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.03005">Parallel Global Edge Switching for the Uniform Sampling of Simple Graphs with Prescribed Degrees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allendorf:Daniel.html">Daniel Allendorf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyer:Ulrich.html">Ulrich Meyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penschuck:Manuel.html">Manuel Penschuck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tran:Hung.html">Hung Tran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03005">PDF</a><br /><b>Abstract: </b>The uniform sampling of simple graphs matching a prescribed degree sequence
is an important tool in network science, e.g., to construct graph generators or
null-models. Here, the Edge Switching Markov Chain (ES-MC) is a common choice.
Given an arbitrary simple graph with the required degree sequence, ES-MC
carries out a large number of small changes involving at most four edges to
eventually obtain a uniform sample. In practice, reasonably short runs
efficiently yield approximate uniform samples.
</p>
<p>We first engineer a simple sequential ES-MC implementation representing the
graph in a hash-set. Despite its simplicity and to the best of our knowledge,
our implementation significantly outperforms all openly available solutions.
</p>
<p>Secondly, we propose the Global Edge Switching Markov Chain (G-ES-MC) and
show that it, too, converges to a uniform distribution. We provide empirical
evidence that G-ES-MC requires not more switches than ES-MC (and often fewer).
</p>
<p>Thirdly, we engineer shared-memory parallel algorithms for ES-MC and G-ES-MC;
we find that they benefit from the easier dependency structure of the G-ES-MC.
In an empirical evaluation, we demonstrate the scalability of our
implementations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.03005"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02992">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02992">The Shortest Even Cycle Problem is Tractable</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bj=ouml=rklund:Andreas.html">Andreas Björklund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Husfeldt:Thore.html">Thore Husfeldt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaski:Petteri.html">Petteri Kaski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02992">PDF</a><br /><b>Abstract: </b>Given a directed graph, we show how to efficiently find a shortest (directed,
simple) cycle on an even number of vertices. As far as we know, no
polynomial-time algorithm was previously known for this problem. In fact,
finding any even cycle in a directed graph in polynomial time was open for more
than two decades until Robertson, Seymour, and Thomas (Ann. of Math. (2) 1999)
and, independently, McCuaig (Electron. J. Combin. 2004; announced jointly at
STOC 1997) gave an efficiently testable structural characterisation of
even-cycle-free directed graphs.
</p>
<p>Methodologically, our algorithm relies on algebraic fingerprinting and
randomized polynomial identity testing over a finite field, and uses a
generating polynomial implicit in Vazirani and Yannakakis ( Discrete Appl.
Math. 1989) that enumerates weighted cycle covers as a difference of a
permanent and a determinant polynomial. The need to work with the permanent is
where our main technical contribution occurs. We design a family of finite
commutative rings of characteristic 4 that simultaneously (i) give a
nondegenerate representation for the generating polynomial identity via the
permanent and the determinant, (ii) support efficient permanent computations,
and (iii) enable emulation of finite-field arithmetic in characteristic 2. Here
our work is foreshadowed by that of Bj\"orklund and Husfeldt (SIAM J. Comput.
2019), who used a considerably less efficient ring design to obtain a
polynomial-time algorithm for the shortest two disjoint paths problem.
</p>
<p>Building on work of Gilbert and Tarjan (Numer. Math. 1978) as well as Alon
and Yuster (J. ACM 2013), we also show how ideas from the nested dissection
technique for solving linear equation systems leads to faster algorithm designs
when we have control on the separator structure of the input graph; for
example, this happens when the input has bounded genus.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02992"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02773">Danzer's Problem, Effective Constructions of Dense Forests and Digital Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsokanos:Ioannis.html">Ioannis Tsokanos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02773">PDF</a><br /><b>Abstract: </b>A 1965 problem due to Danzer asks whether there exists a set in Euclidean
space with finite density intersecting any convex body of volume one. A recent
approach to this problem is concerned with the construction of dense forests
and is obtained by a suitable weakening of the volume constraint. A dense
forest is a discrete point set of finite density getting uniformly close to
long enough line segments. The distribution of points in a dense forest is then
quantified in terms of a visibility function. Another way to weaken the
assumptions in Danzer's problem is by relaxing the density constraint. In this
respect, a new concept is introduced in this paper, namely that of an optical
forest. An optical forest in $\mathbb{R}^{d}$ is a point set with optimal
visibility but not necessarily with finite density. In the literature, the best
constructions of Danzer sets and dense forests lack effectivity. The goal of
this paper is to provide constructions of dense and optical forests which yield
the best known results in any dimension $d \ge 2$ both in terms of visibility
and density bounds and effectiveness. Namely, there are three main results in
this work: (1) the construction of a dense forest with the best known
visibility bound which, furthermore, enjoys the property of being
deterministic; (2) the deterministic construction of an optical forest with a
density failing to be finite only up to a logarithm and (3) the construction of
a planar Peres-type forest (that is, a dense forest obtained from a
construction due to Peres) with the best known visibility bound. This is
achieved by constructing a deterministic digital sequence satisfying strong
dispersion properties.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02773"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02755">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02755">A Compound Logic for Modification Problems: Big Kingdoms Fall from Within</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stamoulis:Giannos.html">Giannos Stamoulis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thilikos:Dimitrios_M=.html">Dimitrios M. Thilikos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02755">PDF</a><br /><b>Abstract: </b>We introduce a novel model-theoretic framework inspired from graph
modification and based on the interplay between model theory and algorithmic
graph minors. We propose a new compound logic operating with two types of
sentences, expressing graph modification: the modulator sentence, defining some
property of the modified part of the graph, and the target sentence, defining
some property of the resulting graph. In our framework, modulator sentences are
in monadic second-order logic and have models of bounded treewidth, while
target sentences express first-order logic properties along with
minor-exclusion. Our logic captures problems that are not definable in first
order logic and, moreover, may have instances of unbounded treewidth. Also, it
permits the modelling of wide families of problems involving vertex/edge
removals, alternative modulator measures (such as elimination distance or
G-treewidth), multistage modifications, and various cut problems. Our main
result is that, for this compound logic, model checking can be done in
quadratic time. This algorithmic meta-theorem encompasses, unifies, and extends
all known meta-algorithmic results on minor-closed graph classes. Moreover, all
derived algorithms are constructive and this, as a byproduct, extends the
constructibility horizon of the algorithmic applications of the Graph Minors
theorem of Robertson and Seymour. The proposed logic can be seen as a general
framework to capitalize on the potential of the irrelevant vertex technique.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02755"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02657">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02657">Average Sensitivity of Dynamic Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumabe:Soh.html">Soh Kumabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02657">PDF</a><br /><b>Abstract: </b>When processing data with uncertainty, it is desirable that the output of the
algorithm is stable against small perturbations in the input. Varma and Yoshida
[SODA'21] recently formalized this idea and proposed the notion of average
sensitivity of algorithms, which is roughly speaking, the average Hamming
distance between solutions for the original input and that obtained by deleting
one element from the input, where the average is taken over the deleted
element.
</p>
<p>In this work, we consider average sensitivity of algorithms for problems that
can be solved by dynamic programming. We first present a
$(1-\delta)$-approximation algorithm for finding a maximum weight chain (MWC)
in a transitive directed acyclic graph with average sensitivity
$O(\delta^{-1}\log^3 n)$, where $n$ is the number of vertices in the graph. We
then show algorithms with small average sensitivity for various dynamic
programming problems by reducing them to the MWC problem while preserving
average sensitivity, including the longest increasing subsequence problem, the
interval scheduling problem, the longest common subsequence problem, the
longest palindromic subsequence problem, the knapsack problem with integral
weight, and the RNA folding problem. For the RNA folding problem, our reduction
is highly nontrivial because a naive reduction generates an exponentially large
graph, which only provides a trivial average sensitivity bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02657"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02614">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02614">Finding All Leftmost Separators of Size $\leq k$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belbasi:Mahdi.html">Mahdi Belbasi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/F=uuml=rer:Martin.html">Martin Fürer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02614">PDF</a><br /><b>Abstract: </b>We define a notion called leftmost separator of size at most $k$. A leftmost
separator of size $k$ is a minimal separator $S$ that separates two given sets
of vertices $X$ and $Y$ such that we "cannot move $S$ more towards $X$" such
that $|S|$ remains smaller than the threshold. One of the incentives is that by
using leftmost separators we can improve the time complexity of treewidth
approximation. Treewidth approximation is a problem which is known to have a
linear time FPT algorithm in terms of input size, and only single exponential
in terms of the parameter, treewidth. It is not known whether this result can
be improved theoretically. However, the coefficient of the parameter $k$ (the
treewidth) in the exponent is large. Hence, our goal is to decrease the
coefficient of $k$ in the exponent, in order to achieve a more practical
algorithm. Hereby, we trade a linear-time algorithm for an $\mathcal{O}(n \log
n)$-time algorithm. The previous known $\mathcal{O}(f(k) n \log n)$-time
algorithms have dependences of $2^{24k}k!$, $2^{8.766k}k^2$ (a better analysis
shows that it is $2^{7.671k}k^2$), and higher. In this paper, we present an
algorithm for treewidth approximation which runs in time
$\mathcal{O}(2^{6.755k}\ n \log n)$,
</p>
<p>Furthermore, we count the number of leftmost separators and give a tight
upper bound for them. We show that the number of leftmost separators of size
$\leq k$ is at most $C_{k-1}$ (Catalan number). Then, we present an algorithm
which outputs all leftmost separators in time
$\mathcal{O}(\frac{4^k}{\sqrt{k}}n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02614"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02598">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02598">Universal Private Estimators</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Wei.html">Wei Dong</a>, Ke Yi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02598">PDF</a><br /><b>Abstract: </b>We present $\textit{universal}$ estimators for the statistical mean,
variance, and scale (in particular, the interquartile range) under pure
differential privacy. These estimators are universal in the sense that they
work on an arbitrary, unknown distribution $\mathcal{P}$ over $\mathbb{R}$,
while yielding strong utility guarantees except for ill-behaved $\mathcal{P}$.
For certain distribution families like Gaussians or heavy-tailed distributions,
we show that our universal estimators match or improve existing estimators,
which are often specifically designed for the given family and under
$\textit{priori}$ boundedness assumptions on the mean and variance of
$\mathcal{P}$. The removal of these boundedness assumptions is surprising, as
existing work believes that they are necessary under pure differential privacy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02598"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02591">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02591">Minimum-Complexity Graph Simplification under Fr\'echet-Like Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirzanezhad:Majid.html">Majid Mirzanezhad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wenk:Carola.html">Carola Wenk</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02591">PDF</a><br /><b>Abstract: </b>Simplifying graphs is a very applicable problem in numerous domains,
especially in computational geometry. Given a geometric graph and a threshold,
the minimum-complexity graph simplification asks for computing an alternative
graph of minimum complexity so that the distance between the two graphs remains
at most the threshold. In this paper, we propose several NP-hardness and
algorithmic results depending on the type of input and simplified graphs, the
vertex placement of the simplified graph, and the distance measures between
them (graph and traversal distances [1,2]). In general, we show that for
arbitrary input and output graphs, the problem is NP-hard under some specific
vertex-placement of the simplified graph. When the input and output are trees,
and the graph distance is applied from the simplified tree to the input tree,
we give an $O(kn^5)$ time algorithm, where $k$ is the number of the leaves of
the two trees that are identical and $n$ is the number of vertices of the
input.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02591"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02579">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02579">Reallocation Problems with Minimum Completion Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ishii:Toshimasa.html">Toshimasa Ishii</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawahara:Jun.html">Jun Kawahara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makino:Kazuhisa.html">Kazuhisa Makino</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ono:Hirotaka.html">Hirotaka Ono</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02579">PDF</a><br /><b>Abstract: </b>Reallocation scheduling is one of the most fundamental problems in various
areas such as supply chain management, logistics, and transportation science.
In this paper, we introduce the reallocation problem that models the scheduling
in which products are with fixed cost, non-fungible, and reallocated in
parallel, and comprehensively study the complexity of the problem under various
settings of the transition time, product size, and capacities. We show that the
problem can be solved in polynomial time for a fundamental setting where the
product size and transition time are both uniform. We also show that the
feasibility of the problem is NP-complete even for little more general
settings, which implies that no polynomial-time algorithm constructs a feasible
schedule of the problem unless P$=$NP. We then consider the relaxation of the
problem, which we call the capacity augmentation, and derive a reallocation
schedule feasible with the augmentation such that the completion time is at
most the optimal of the original problem. When the warehouse capacity is
sufficiently large, we design constant-factor approximation algorithms under
all the settings. We also show the relationship between the reallocation
problem and the bin packing problem when the warehouse and carry-in capacities
are sufficiently large.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02579"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02572">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02572">A Constant-Factor Approximation for Quasi-bipartite Directed Steiner Tree on Minor-Free Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friggstad:Zachary.html">Zachary Friggstad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mousavi:Ramin.html">Ramin Mousavi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02572">PDF</a><br /><b>Abstract: </b>We give the first constant-factor approximation algorithm for quasi-bipartite
instances of Directed Steiner Tree on graphs that exclude fixed minors. In
particular, for $K_r$-minor-free graphs our approximation guarantee is
$O(r\cdot\sqrt{\log r})$ and, further, for planar graphs our approximation
guarantee is 20.
</p>
<p>Our algorithm uses the primal-dual scheme. We employ a more involved method
of determining when to buy an edge while raising dual variables since, as we
show, the natural primal-dual scheme fails to raise enough dual value to pay
for the purchased solution. As a consequence, we also demonstrate integrality
gap upper bounds on the standard cut-based linear programming relaxation for
the Directed Steiner Tree instances we consider.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02572"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02544">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02544">Polygon Placement Revisited: (Degree of Freedom + 1)-SUM Hardness and an Improvement via Offline Dynamic Rectangle Union</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=nnemann:Marvin.html">Marvin Künnemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nusser:Andr=eacute=.html">André Nusser</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02544">PDF</a><br /><b>Abstract: </b>We revisit the classical problem of determining the largest copy of a simple
polygon $P$ that can be placed into a simple polygon $Q$. Despite significant
effort, known algorithms require high polynomial running times. (Barequet and
Har-Peled, 2001) give a lower bound of $n^{2-o(1)}$ under the 3SUM conjecture
when $P$ and $Q$ are (convex) polygons with $\Theta(n)$ vertices each. This
leaves open whether we can establish (1) hardness beyond quadratic time and (2)
any superlinear bound for constant-sized $P$ or $Q$.
</p>
<p>In this paper, we affirmatively answer these questions under the $k$SUM
conjecture, proving natural hardness results that increase with each degree of
freedom (scaling, $x$-translation, $y$-translation, rotation): (1) Finding the
largest copy of $P$ that can be $x$-translated into $Q$ requires time
$n^{2-o(1)}$ under the 3SUM conjecture. (2) Finding the largest copy of $P$
that can be arbitrarily translated into $Q$ requires time $n^{2-o(1)}$ under
the 4SUM conjecture. (3) The above lower bounds are almost tight when one of
the polygons is of constant size: we obtain an $\tilde O((pq)^{2.5})$-time
algorithm for orthogonal polygons $P,Q$ with $p$ and $q$ vertices,
respectively. (4) Finding the largest copy of $P$ that can be arbitrarily
rotated and translated into $Q$ requires time $n^{3-o(1)}$ under the 5SUM
conjecture.
</p>
<p>We are not aware of any other such natural $($degree of freedom $+ 1)$-SUM
hardness for a geometric optimization problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02544"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02480">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02480">Linear-time Minimization of Wheeler DFAs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alanko:Jarno.html">Jarno Alanko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cotumaccio:Nicola.html">Nicola Cotumaccio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02480">PDF</a><br /><b>Abstract: </b>Wheeler DFAs (WDFAs) are a sub-class of finite-state automata which is
playing an important role in the emerging field of compressed data structures:
as opposed to general automata, WDFAs can be stored in just $\log\sigma + O(1)$
bits per edge, $\sigma$ being the alphabet's size, and support optimal-time
pattern matching queries on the substring closure of the language they
recognize. An important step to achieve further compression is minimization.
When the input $\mathcal A$ is a general deterministic finite-state automaton
(DFA), the state-of-the-art is represented by the classic Hopcroft's algorithm,
which runs in $O(|\mathcal A|\log |\mathcal A|)$ time. This algorithm stands at
the core of the only existing minimization algorithm for Wheeler DFAs, which
inherits its complexity. In this work, we show that the minimum WDFA equivalent
to a given input WDFA can be computed in linear $O(|\mathcal A|)$ time. When
run on de Bruijn WDFAs built from real DNA datasets, an implementation of our
algorithm reduces the number of nodes from 14% to 51% at a speed of more than 1
million nodes per second.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02480"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02478">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02478">HOLZ: High-Order Entropy Encoding of Lempel-Ziv Factor Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02478">PDF</a><br /><b>Abstract: </b>We propose a new representation of the offsets of the Lempel-Ziv (LZ)
factorization based on the co-lexicographic order of the processed prefixes.
The selected offsets tend to approach the k-th order empirical entropy. Our
evaluations show that this choice of offsets is superior to the rightmost LZ
parsing and the bit-optimal LZ parsing on datasets with small high-order
entropy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02478"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02318">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02318">Nearly Tight Lower Bounds for Succinct Range Minimum Query</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Mingmou.html">Mingmou Liu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02318">PDF</a><br /><b>Abstract: </b>Given an array of distinct integers $A[1\ldots n]$, the Range Minimum Query
(RMQ) problem requires us to construct a data structure from $A$, supporting
the RMQ query: given an interval $[a,b]\subseteq[1,n]$, return the index of the
minimum element in subarray $A[a\ldots b]$, i.e. return
$\text{argmin}_{i\in[a,b]}A[i]$. The fundamental problem has a long history.
The textbook solution which uses $O(n)$ words of space and $O(1)$ time by
Gabow, Bentley, Tarjan (STOC 1984) and Harel, Tarjan (SICOMP 1984) dates back
to 1980s. The state-of-the-art solution is presented by Fischer, Heun (SICOMP
2011) and Navarro, Sadakane (TALG 2014). The solution uses
$2n+n/\left(\frac{\log n}{t}\right)^t+\tilde{O}(n^{3/4})$ bits of space and
$O(t)$ query time, assuming the word-size is $\Theta(\log n)$ bits. On the
other hand, the only known lower bound is proven by Liu and Yu (STOC 2020).
They show that any data structure which solves RMQ in $t$ query time must use
$2n+n/(\log n)^{O(t^2\log^2t)}$ bits of space, assuming the word-size is
$\Theta(\log n)$ bits.
</p>
<p>In this paper, we prove nearly tight lower bound for this problem. We show
that, for any data structure which solves RMQ in $t$ query time, $2n+n/(\log
n)^{O(t\log^2t)}$ bits of space is necessary in the cell-probe model with
word-size $\Theta(\log n)$. We emphasize that, for any $r$, we present a lower
bound of $t=\Omega(t_{opt}/\log^3 t_{opt})$, where $t_{opt}$ is the optimal
time cost when the data structure is allowed to consume $2n+r$ bits of space.
Hence our lower bound is nearly tight.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02318"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2004.05738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2004.05738">Lower Bound for Succinct Range Minimum Query</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Mingmou.html">Mingmou Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Huacheng.html">Huacheng Yu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2004.05738">PDF</a><br /><b>Abstract: </b>Given an integer array $A[1..n]$, the Range Minimum Query problem (RMQ) asks
to preprocess $A$ into a data structure, supporting RMQ queries: given $a,b\in
[1,n]$, return the index $i\in[a,b]$ that minimizes $A[i]$, i.e.,
$\mathrm{argmin}_{i\in[a,b]} A[i]$. This problem has a classic solution using
$O(n)$ space and $O(1)$ query time by Gabow, Bentley, Tarjan (STOC, 1984) and
Harel, Tarjan (SICOMP, 1984). The best known data structure by Fischer, Heun
(SICOMP, 2011) and Navarro, Sadakane (TALG, 2014) uses $2n+n/(\frac{\log
n}{t})^t+\tilde{O}(n^{3/4})$ bits and answers queries in $O(t)$ time, assuming
the word-size is $w=\Theta(\log n)$. In particular, it uses
$2n+n/\mathrm{poly}\log n$ bits of space as long as the query time is a
constant.
</p>
<p>In this paper, we prove the first lower bound for this problem, showing that
$2n+n/\mathrm{poly}\log n$ space is necessary for constant query time. In
general, we show that if the data structure has query time $O(t)$, then it must
use at least $2n+n/(\log n)^{\tilde{O}(t^2)}$ space, in the cell-probe model
with word-size $w=\Theta(\log n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2004.05738"><span class="datestr">at November 07, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02999">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02999">Quantum search-to-decision reductions and the state synthesis problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Irani:Sandy.html">Sandy Irani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natarajan:Anand.html">Anand Natarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nirkhe:Chinmay.html">Chinmay Nirkhe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Sujit.html">Sujit Rao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuen:Henry.html">Henry Yuen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02999">PDF</a><br /><b>Abstract: </b>It is a useful fact in classical computer science that many search problems
are reducible to decision problems; this has led to decision problems being
regarded as the $\textit{de facto}$ computational task to study in complexity
theory. In this work, we explore search-to-decision reductions for quantum
search problems, wherein a quantum algorithm makes queries to a classical
decision oracle to output a desired quantum state. In particular, we focus on
search-to-decision reductions for $\mathsf{QMA}$, and show that there exists a
quantum polynomial-time algorithm that can generate a witness for a
$\mathsf{QMA}$ problem up to inverse polynomial precision by making one query
to a $\mathsf{PP}$ decision oracle. We complement this result by showing that
$\mathsf{QMA}$-search does $\textit{not}$ reduce to $\mathsf{QMA}$-decision in
polynomial-time, relative to a quantum oracle.
</p>
<p>We also explore the more general $\textit{state synthesis problem}$, in which
the goal is to efficiently synthesize a target state by making queries to a
classical oracle encoding the state. We prove that there exists a classical
oracle with which any quantum state can be synthesized to inverse polynomial
precision using only one oracle query and to inverse exponential precision
using two oracle queries. This answers an open question of Aaronson from 2016,
who presented a state synthesis algorithm that makes $O(n)$ queries to a
classical oracle to prepare an $n$-qubit state, and asked if the query
complexity could be made sublinear.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02999"><span class="datestr">at November 06, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02967">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02967">An Empirical Comparison of the Quadratic Sieve Factoring Algorithm and the Pollard Rho Factoring Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zongxia.html">Zongxia Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gasarch:William.html">William Gasarch</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02967">PDF</a><br /><b>Abstract: </b>One of the most significant challenges on cryptography today is the problem
of factoring large integers since there are no algorithms that can factor in
polynomial time, and factoring large numbers more than some limits(200 digits)
remain difficult. The security of the current cryptosystems depends on the
hardness of factoring large public keys. In this work, we want to implement two
existing factoring algorithms - pollard-rho and quadratic sieve - and compare
their performance. In addition, we want to analyze how close is the theoretical
time complexity of both algorithms compared to their actual time complexity and
how bit length of numbers can affect quadratic sieve's performance. Finally, we
verify whether the quadratic sieve would do better than pollard-rho for
factoring numbers smaller than 80 bits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02967"><span class="datestr">at November 06, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.02688">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.02688">The No Endmarker Theorem for One-Way Probabilistic Pushdown Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamakami:Tomoyuki.html">Tomoyuki Yamakami</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.02688">PDF</a><br /><b>Abstract: </b>In various models of one-way pushdown automata, the explicit use of two
designated endmarkers on a read-once input tape has proven to be extremely
useful for making a conscious, final decision on the acceptance/rejection of
each input word right after reading the right endmarker. With no endmarkers, by
contrast, a machine must constantly stay in either accepting or rejecting
states at any moment since it never notices the end of the input instance. This
situation, however, helps us analyze the behavior of the machine whose tape
head makes the consecutive moves on all prefixes of a given extremely long
input word. Since those two machine formulations have their own advantages, it
is natural to ask whether the endmarkers are truly necessary to correctly
recognize languages. In the deterministic and nondeterministic models, it is
well-known that the endmarkers are removable without changing the acceptance
criteria of each input instance. This paper proves that, for a more general
model of one-way probabilistic pushdown automata, the endmarkers are also
removable. This is proven by employing probabilistic transformations from an
"endmarker" machine to an equivalent "no-endmarker" machine at the cost of
double exponential state complexity without compromising its error probability.
By setting this error probability appropriately, our proof also provides an
alternative proof to both the deterministic and the nondeterministic models as
well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.02688"><span class="datestr">at November 06, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/">Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br />
Email: See the url</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/"><span class="datestr">at November 05, 2021 10:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/">Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br />
Email: bmkapron@uvic.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/"><span class="datestr">at November 05, 2021 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/">Assistant Professor at Charles University (apply by January 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered.</p>
<p>Website: <a href="https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022">https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022</a><br />
Email: koucky@iuuk.mff.cuni.cz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/"><span class="datestr">at November 05, 2021 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/">Summer Research Intern at Adobe Research  (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. Interested candidates should send their CV to Zhao.</p>
<p>Website: <a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en</a><br />
Email: zsong@adobe.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/"><span class="datestr">at November 05, 2021 04:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/">Faculty at University of Haifa at Oranim College (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022.</p>
<p>Website: <a href="https://mathphys.haifa.ac.il/en/announcements/">https://mathphys.haifa.ac.il/en/announcements/</a><br />
Email: ackerman@sci.haifa.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/"><span class="datestr">at November 04, 2021 01:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-3367673395710171015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html">HotNets Presentation : Zero-CPU Collection with Direct Telemetry Access</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>HotNets has asked that we let people know that the 2021 presentations <a href="https://www.youtube.com/channel/UCZZ5nf4RNDIIe4nifI8grwQ/videos">are available here</a>.  I'm using that an excuse to highlight our paper on Zero-CPU Collection with Direct Telemetry Access (<a href="https://arxiv.org/abs/2110.05438">arxiv version here</a>), but really I want to highlight the talk by graduate student Jonatan Langlet (Queen Mary University of London) who, as is the nature of graduate students, did all of the real work, and who really did a great job on <a href="https://www.youtube.com/watch?v=_M8AbF_f8Kk&amp;t=2s">the talk (direct link)</a>.  If you guessed from my involvement this involves hashing in some way, your maximum likelihood estimate turns out to be correct.</p><p>I think our work fits the HotNets call, which asks for new approaches and preliminary work.  Specifically, the call for the HotNets workshop says this:</p><p></p><blockquote><p>We invite researchers and practitioners to submit short position papers. We encourage papers that identify fundamental open questions, advocate a new approach, offer a constructive critique of the state of networking research, re-frame or debunk existing work, report unexpected early results from a deployment, report on promising but unproven ideas, or propose new evaluation methods. Novel ideas need not be supported by full evaluations; well-reasoned arguments or preliminary evaluations can support the possibility of the paper’s claims.</p><p>We seek early-stage work, where the authors can benefit from community feedback. An ideal submission has the potential to open a line of inquiry for the community that results in multiple conference papers in related venues (SIGCOMM, NSDI, CoNEXT, SOSP, OSDI, MobiCom, MobiSys, etc.), rather than a single follow-on conference paper. The program committee will explicitly favor early work and papers likely to stimulate reflection and discussion over “conference papers in miniature”.</p></blockquote><p>There are similar other "Hot" workshops in other areas, and it was about <a href="http://mybiasedcoin.blogspot.com/2007/08/hottheory-workshop.html">14 years ago that I asked whether CS theory should have a HotTheory workshop</a>.  There's been a proliferation of new conferences and workshops in theory since then, but none of them really seem to have this flavor.  So maybe it's worth asking again whether a HotTheory workshop would make sense?  Or do existing theory events meet the theory community needs?</p><p></p><p><br /></p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html"><span class="datestr">at November 04, 2021 01:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/">Lecturer in Theoretical Computer Science, Sheffield (UK) at University of Sheffield (apply by November 16, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science">https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science</a><br />
Email: g.j.brown@sheffield.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/"><span class="datestr">at November 04, 2021 12:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">Professor, Associate Professor, Assistant Professor at George Washington University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications to multiple faculty positions at all ranks. Our search is focused on machine learning; artificial intelligence; computer and distributed systems; security and privacy; and candidates that can support our multidisciplinary initiatives in Smart and Trustworthy Systems and Meaningful Computing, broadly defined.</p>
<p>Website: <a href="https://www.gwu.jobs/postings/87400">https://www.gwu.jobs/postings/87400</a><br />
Email: cssearch@gwu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/"><span class="datestr">at November 04, 2021 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2021/11/04/perceptron/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2021/11/04/perceptron/">The Perceptron as a prototype for machine learning theory.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Just as many of the algorithms and community practices of machine learning were invented <a href="http://www.argmin.net/2021/10/20/highleyman/">in the late 1950s and early 1960s</a>, the foundations of machine learning theory were also established during this time. Many of the analyses of this period were strikingly simple, had surprisingly precise constants, and provided prescient guidelines for contemporary machine learning practice. Here, I’ll summarize the study of the Perceptron, highlighting both its algorithmic and statistical analyses, and using it as a prototype to illustrate further how prediction deviates from the umbrella of classical statistics.</p>

<p>Let’s begin with a classification problem where each individual from some population has a feature vector $x$ and an associated binary label $y$ that we take as valued $\pm 1$ for notational convenience. The goal of the Perceptron is to find a linear separator such that $\langle w, x \rangle&gt;0$ for when $y=1$ and $\langle w, x \rangle&lt;0$ when $y=-1$. We can write this compactly as saying that we want to find a $w$ for which $y \langle w, x \rangle &gt;0$ for as many individuals in the population as possible.</p>

<p>Rosenblatt’s Perceptron provides a simple algorithm for finding such a $w$. The Perceptron inputs an example, checks if it makes the correct classification. If yes, it does nothing and proceeds to the next example. If no, the decision boundary is nudged in the direction of classifying the example correctly next time.</p>

<p><strong>Perceptron</strong></p>

<ul>
  <li>Start from the initial solution $w_0=0$</li>
  <li>At each step $t=0,1,2,…$:
    <ul>
      <li>Select an individual from the population and look up their attributes: (x_t,y_t).</li>
      <li>Case 1: If $y_t\langle w_t, x_t\rangle \leq 0$, put
\(w_{t+1} = w_t + y_t x_t\)</li>
      <li>Case 2: Otherwise put $w_{t+1} = w_t$.</li>
    </ul>
  </li>
</ul>

<p>If the examples were selected at random, machine learners would recognize this algorithm as an instance of stochastic gradient descent, still the most ubiquitous way to train classifiers whether they be deep or shallow. Stochastic gradient descent minimizes sums of functions</p>

\[f(w) = \frac{1}{N} \sum_{i=1}^N \mathit{loss}( f(x_i; w) , y_i)\]

<p>with the update</p>

\[w_{t+1} = w_t - \alpha_t \nabla_w \mathit{loss}( f(x_t; w_t) , y_t)\,.\]

<p>When the examples are sampled randomly, the Perceptron is stochastic gradient descent with $\alpha_t=1$, $f(x;w) = \langle w,x \rangle$, and loss function $\mathit{loss}(\hat{y},y) = \max(-\hat{y} y, 0)$.</p>

<p>Stochastic gradient methods were invented a few years before the Perceptron. And the relations between these methods were noted by the mid-60s. Vapnik discusses some of this history in Chapter 1.11 of <a href="https://link.springer.com/book/10.1007/978-1-4757-3264-1"><em>The Nature of Statistical Learning Theory</em></a>.</p>

<p>While we might be tempted to use a standard stochastic gradient analysis to understand the optimization properties of the Perceptron, it turns out that a more rarified proof technique applies that uses no randomization whatsoever. Moreover, the argument will not only bound errors in optimization but also in generalization. Optimization is concerned with errors on a training data set. Generalization is concerned with errors on data we haven’t seen. The analysis from the 1960s links these two by first understanding the dynamics of the algorithm.</p>

<p><a href="https://cs.uwaterloo.ca/~y328yu/classics/novikoff.pdf">A celebrated result by Al Novikoff in 1962</a> showed that under reasonable conditions the algorithm makes a bounded number of updates no matter how large the sample size. Novikoff’s result is typically referred to as a <em>mistake bound</em> as it bounds the number of total misclassifications made when running the Perceptron on some data set. The key assumption in Novikoff’s argument is that the positive and negative examples are cleanly separated by a linear function. People often dismiss the Perceptron because of this <em>separability</em> assumption. But for any finite data set, can always add features and end up with a linearly separable problem. And if we add enough features, we’ll usually be separable no matter how many points we have.</p>

<p>This has been the trend in modern machine learning: don’t fear big models and don’t fear getting zero errors on your training set. This is no different than what was being proposed in the Perceptron. In fact, <a href="https://cs.uwaterloo.ca/~y328yu/classics/kernel.pdf">Aizerman, Braverman, and Roeznoer</a> recognized the power of such overparameterization, and extended Novikoff’s argument to “potential functions” that we now recognize as functions belonging to an infinite dimensional Reproducing Kernel Hilbert Space.</p>

<p>To state Novikoff’s result, we make the following assumptions: First, we assume as input a set of examples $S$. We assume every data point has norm at most $R(S)$ and that there exists a hyperplane that correctly classifies all of the data points and is of distance at least $\gamma(S)$ from every data point. This second assumption is called a <em>margin condition</em> that quantifies how separated the given data is. With these assumptions, Novikoff proved the Perceptron algorithm makes at most</p>

\[{\small
\frac{R(S)^2}{\gamma(S)^{2}}
}\]

<p>mistakes when run on $S$. No matter what the ordering of the data points in $S$, the algorithm makes a bounded number of errors.</p>

<p>The algorithmic analysis of Novikoff has many implications. First, if the data is separable, we can conclude that the Perceptron will terminate if it is run over the data set several times. This is because we can think of $k$ epochs of the Perceptron as running on the union of $k$ distinct copies of $S$, and the Perceptron eventually stops updating when run on this enlarged data set. Hence, the mistake bound tells us something particular about optimization: the Perceptron converges to a solution with zero training errors and hence a global minimizer of the empirical risk.</p>

<p>Second, we can think of the Perceptron algorithm as an <em>online learning algorithm</em>. We need not assume anything distributional about the sequence $S$. We can instead think about how long it takes for the Perceptron to converge to a solution that would have been as good as the optimal classifier. We can quantify this convergence by measuring the <em>regret</em>, equal to</p>

\[\mathcal{R}_T = \sum_{t=1}^T \mathrm{error}(w_t, (x_t,y_t)) - \sum_{t=1}^T \mathrm{error}(w_\star, (x_t,y_t))\,,\]

<p>where $w_\star$ denotes the optimal hyperplane. That is, the regret counts how frequently the classifier at step $t$ misclassifies the next example in the sequence. Novikoff’s argument shows that, if a sequence is perfectly classifiable, then the accrued regret is a constant that does not scale with T.</p>

<p>A third, less well known application of Novikoff’s bound is as a building block for a  <em>generalization bound</em>. A generalization bound estimates the probability of making an error on a new example given that the new example is sampled from the same population as the data thus far sceen. To state the generalization bound for the Perceptron, I <em>now</em> need to return to statistics. Generalization theory concerns statistical validity, and hence we need to define some notion of sampling from the population. I will use the same sampling model I have been using in this blog series. Rather than assuming a statistical model of the population, I will assume we have some population of data from which we can uniformly sample. Our training data will consist of $n$ points sampled uniformly from this population: $S={(x_1,y_1)\ldots, (x_n,y_n) }$.</p>

<p>We know that the Perceptron will find a good linear predictor for the training data if it exists. What we now show is that this predictor also works on new data sampled uniformly from the same population.</p>

<p>To analyze what happens on new data, I will employ an elegant argument I learned from Sasha Rakhlin. This argument appears in a book on Learning Theory by Vapnik and Chervonenkis from 1974, which, to my knowledge, is only available in Russian. Sasha also believes this argument is considerably older as <a href="http://www.mit.edu/~rakhlin/papers/chervonenkis_chapter.pdf">Aizermann and company were making similar “online to batch” constructions in the 1960s</a>. The proof here leverages the assumption that the data are sampled in such a way that they are identically distributed, so we can swap the roles of training and test examples in the analysis. It foreshadows later studies of stability and generalization that would be revisited decades later.</p>

<p><strong>Theorem</strong> <em>Let $w(S)$ be the output of the Perceptron on a dataset $S$ after running until the hyperplane makes no more mistakes on $S$. Let $S_n$ denote a training set of $n$ samples uniformly at random from some population. And let $(x,y)$ be an additional independent uniform sample from the same population. Then, the probability of making a mistake on $(x,y)$ is bounded as</em></p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \leq \frac{1}{n+1} {\mathbb{E}}_{S_{n+1}}\left[ \frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2} \right]\,.\]

<p>To prove the theorem, define the “leave-one-out set” to be the set where we drop $(x_k,y_k)$:</p>

\[{\scriptsize
S^{-k}=\{(x_1,y_1),\dots,(x_{k-1},y_{k-1}),(x_{k+1},y_{k+1}),...,(x_{n+1},y_{n+1})\}\,.
}\]

<p>With this notation, since all of the data are sampled identically and independently, we can rewrite the probability of a mistake on the final data point as the expectation of the leave-one-out error</p>

\[{\small
\Pr[y \langle w(S_n), x \rangle   \leq 0]
= \frac1{n+1}\sum_{k=1}^{n+1} \mathbb{E}[\mathbb{1}\{y_k \langle w(S^{-k}), x_k \rangle \leq 0\}]\,.
}\]

<p>Novikoff’s mistake bound asserts the Perceptron makes at most</p>

\[{\small
m=\frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2}
}\]

<p>mistakes when run on the entire sequence $S_{n+1}$. Let $I={i_1,\dots,i_m}$ denote the indices on which the algorithm makes a mistake in any of its cycles over the data. If $k$ is not in $I$, the output of the algorithm remains the same after we remove the $k$-th sample from the sequence. It follows that such $k \in S_{n+1}\setminus I$ satisfy  $y_k w(S^{-k})x_k \geq 0$ and therefore do not contribute to the right hand side of the summation. The other terms can at most contribute $1$ to the summation.
Hence,</p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \le \frac{\mathbb{E}[m]}{n+1}\,,\]

<p>which is what we wanted to prove.</p>

<p>What’s most stunning to me about this argument is that there are no numerical constants or logarithms. The generalization error is perfectly quantified by a simple formula of $R$, $\gamma$, and $n$. There are a variety of other arguments that get the $\tilde{O}(R/(n\gamma))$ scaling with far more complex arguments and large constants and logarithmic terms. For example, one can show that the set of hyperplanes in Euclidean space with norm bounded by $\gamma^{-1}$ has <a href="https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034">VC dimension $R/\gamma$</a>. Similarly, a <a href="https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">Rademacher complexity argument will achieve a similar scaling</a>. These arguments apply to far more algorithms than the Perceptron, but it’s frustrating how this simple algorithm from 1956 gets such a tight bound with such a short argument whereas analyzing more “powerful” algorithms often takes pages of derivations.</p>

<p>It’s remarkable that these bounds on optimization, regret, and generalization worked out in the 1960s all turned out to be optimal for classification theory. This strikes me as particularly odd because when I was in graduate school I was taught that the Perceptron was a failed enterprise. But as fads in AI have come and gone, the role of the Perceptron has remained central for 65 years. We’ve made more progress in machine learning theory since then, but it’s not always at the front of our minds just how long ago we had established our modern learning theory framework.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2021/11/04/perceptron/"><span class="datestr">at November 04, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8862914568867887724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html">A Complexity View of Machine Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Complexity is at its best when it models new technologies so we can study it in a principled way. Quantum computing comes to mind as a good relatively recent example. With machine learning playing an every growing role in computing, how can complexity play a role?</p><p>The theory community questions about machine learning typically look at finding mathematical reasons to explain why the models well with little overfitting or trying to get good definitions of privacy, fairness, explainability to mitigate the social challenges of ML. But what about from a computational complexity point of view? I don't have a great answer yet but here are some thoughts.</p><p>In much of structural complexity, we use relativization to understand the relative power of complexity classes. We define an oracle as a set A where a machine can ask questions about membership to A and magically get an answer. Relativization can be used to help us define classes like Σ<sub>2</sub><sup>P</sup> = NP<sup>NP</sup> or allow us to succinctly state <a href="https://doi.org/10.1137/0220053">Toda's theorem</a> as PH in P<sup>#P</sup>.</p><p>As I <a href="https://twitter.com/fortnow/status/1453827400383488002">tweeted</a> last week, machine learning feels like an oracle, after all machine learning models and algorithms are typically accessed through APIs and Python modules. What kind of oracle? Definitely not an NP-complete problem like SAT since machine learning fails miserably if you try to use it to break cryptography. </p><p>The real information in machine learning comes from the data. For a length parameter n, consider a string x which might be exponential in n. Think of x as a list of labeled or unlabeled examples of some larger set S. Machine learning creates a model M from x that tries to predict whether x is in S. Think of M as the oracle, as some compressed version of S.</p><p>Is there a computational view of M? We can appeal to Ockham's razor and consider the simplest model consistent with the data for which x as a set are random in the S that M generates. One can formalize this Minimum Description Length approach using <a href="https://doi.org/10.1109/18.825807">Kolmogorov Complexity</a>. This model is too ideal, for one it can also break cryptography, and typical deep learning models are not simple at all with sometimes millions of parameters.</p><p>This is just a start. One could try time bounds on the Kolmogorov definitions or try something different completely. Adversarial and foundational learning models might yield different kinds of oracles. </p><p>If we can figure out even a rough complexity way to understand learning, we can start to get a hold of learning's computational power and limitations, which is the purpose of studying complexity complexity in the first place. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html"><span class="datestr">at November 03, 2021 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/148">TR21-148 |  Explicit Exponential Lower Bounds for Exact Hyperplane Covers | 

	Benjamin Diamond, 

	Amir Yehudayoff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We describe an explicit and simple subset of the discrete hypercube which cannot be exactly covered by fewer than exponentially many hyperplanes. The proof exploits a connection to communication complexity, and relies heavily on Razborov's lower bound for disjointness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/148"><span class="datestr">at November 03, 2021 11:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=581">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/">TCS+ talk: Wednesday, November 10 — Kuikui Liu, University of Washington</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 10th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="https://homes.cs.washington.edu/~liukui17/"><strong>Kuikui Liu</strong></a> from the University of Washington will speak about “<em>Spectral Independence: A New Tool to Analyze Markov Chains</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Markov chain Monte Carlo is a widely used class of algorithms for sampling from high-dimensional probability distributions, both in theory and in practice. While simple to implement, analyzing the rate of convergence to stationarity, i.e. the “mixing time”, remains a challenging problem in many settings. We introduce a new technique to bound mixing times called “spectral independence”, which says that certain pairwise correlation matrices all have bounded spectral norm. This surprisingly powerful technique originates in the emerging study of high-dimensional expanders, and has allowed us to “unify” nearly all existing approaches to approximate counting and sampling by building new connections with other areas, including statistical physics, geometry of polynomials, functional analysis, and more. Through these connections, several long-standing open problems have recently been answered, including counting bases of matroids and optimal mixing of the Glauber dynamics/Gibbs sampler up to the algorithmic phase transition threshold.</p>
<p>Based on several joint works with Dorna Abdolazimi, Nima Anari, Zongchen Chen, Shayan Oveis Gharan, Eric Vigoda, Cynthia Vinzant, and June Vuong.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/"><span class="datestr">at November 03, 2021 09:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5760">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/">CS Tenure Track Positions at CUNY</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Interested in a computer science position in Manhattan? Apply to our positions here! We are starting to form a computer science program at CUNY’s Baruch College. Joining us at the beginning of this process will give you a chance to influence how computer science will look like at our college: the research and teaching directions that […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/"><span class="datestr">at November 03, 2021 01:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">Postdoc at UC Santa Barbara (apply by January 10, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position is available to work with Eric Vigoda (and other theory faculty) at UC Santa Barbara. The position is for 2 years (no teaching required). Start date is flexible.<br />
Interested candidates should send their CV to Eric Vigoda.</p>
<p>Website: <a href="https://sites.cs.ucsb.edu/~vigoda/">https://sites.cs.ucsb.edu/~vigoda/</a><br />
Email: ericvigoda@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/"><span class="datestr">at November 02, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular.html">Gilbert tessellations from a cellular automaton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A <a href="https://en.wikipedia.org/wiki/Gilbert_tessellation">Gilbert tessellation</a> is what you get from choosing a random set of points-with-slopes in the plane, growing line segments in both directions with the chosen slope from each chosen point, at constant speed, and stopping the growth when each line segment runs into something else. The slopes can be in uniformly random directions but one standard variant of the Gilbert tessellation uses only horizontal and vertical slopes.</p>

<p style="text-align: center;"><a href="https://commons.wikimedia.org/wiki/File:Gilbert_tessellation_axis.svg"><img src="https://11011110.github.io/blog/assets/2018/Gilbert-rectangles.svg" alt="Axis-aligned Gilbert tessellation subdivides the plane into rectangles, by Claudio Rocchini" /></a></p>

<p>My paper “<a href="https://arxiv.org/abs/0911.2890">Growth and decay in life-like cellular automata</a>” observed that the Life-like cellular automaton rule B017/S1, when started with a sufficiently sparse random set of live cells, forms lines of replicators that look sort of like one of these axis-parallel Gilbert tessellations, as I discussed in <a href="https://11011110.github.io/blog/2018/12/27/motorcycle-graphs-eventual.html">a previous post on sparse Life</a>.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2018/b017s1.png" alt="Replicator chaos in B017/S1" /></p>

<p>But it’s a bit messy: you also get stable or oscillating blobs of live cells, and replicators can sometimes make large gaps big enough for something else to get across them, rather than forming the impenetrable barriers that the segments of a true Gilbert tessellation would do. So I wondered: how easy is it to get Gilbert tessellations with less mess, in a cellular automaton?</p>

<p>Very easy, if you’re willing to make an automaton that hardcodes into its rules the construction process of a Gilbert tessellation. For instance, make an automaton on an infinite square grid with three states, empty, horizontal, and vertical. Horizontal cells always stay horizontal, and vertical cells always stay vertical. Empty cells with exactly one non-empty neighbor that is either a horizontally-adjacent horizontal cell or a vertically-adjacent vertical cell take on the same state as that neighbor, and otherwise stay empty. Then any horizontal cells will grow into horizontal walls, and vertical cells will grow into vertical walls, at constant speed, until running into other non-empty cells, just as the Gilbert tessellation definition demands. If you start with a sparse random set of non-empty cells of both types, you should get a Gilbert tessellation, more or less by definition. But controlling horizontal versus vertical growth by different cell states rather than by the pattern of live cells seems kind of a cheat. It makes creating Gilbert tessellations the only thing this three-state automaton can do, rather than an emergent behavior of the automaton. And it isn’t even symmetric under 90-degree rotations (although it does have a symmetry that combines rotation with state-swapping). Is there an automaton with only two states, and natural symmetric rules that can do other things but that when seeded randomly generates non-messy Gilbert tessellations?</p>

<p>Yes! I don’t know of a Life-like rule that does this (<a href="https://en.wikipedia.org/wiki/Life_without_Death">Life without death</a> does make nice impenetrable walls but with too much other stuff). But I think the rule below fits the bill. It’s the first thing I tried, at least, so I didn’t have to do any fine adjustments of the rules to make it work.</p>

<p>Here’s the rule:</p>

<ul>
  <li>
    <p>The cells form a square grid with the <a href="https://en.wikipedia.org/wiki/Moore_neighborhood">Moore 8-cell neighborhood</a>.</p>
  </li>
  <li>
    <p>There are two states of cells, live and dead.</p>
  </li>
  <li>
    <p>A dead cell becomes live only under two conditions:</p>

    <ol>
      <li>
        <p>It has exactly two live neighbors (among its eight possible neighbors) that are orthogonally adjacent to each other.</p>
      </li>
      <li>
        <p>It has exactly four live neighbors at the corners of a rectangle (necessarily in two orthogonally adjacent pairs, because we don’t count squares as being rectangles).</p>
      </li>
    </ol>
  </li>
  <li>
    <p>All live cells immediately die.</p>
  </li>
</ul>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/rule.svg" alt="Rules for the Gilbert cellular automaton" /></p>

<p>Or, in <a href="http://golly.sourceforge.net/">Golly</a> rule format:</p>

<pre>@RULE Gilbert
@TABLE
n_states:2
neighborhood:Moore
symmetries:rotate8reflect
var a={0,1}
var b={0,1}
var c={0,1}
var d={0,1}
var e={0,1}
var f={0,1}
var g={0,1}
var h={0,1}
0,1,1,0,0,0,0,0,0,1
0,1,1,0,1,1,0,0,0,1
1,a,b,c,d,e,f,g,h,0</pre>

<p>Then an initial pattern of two orthogonally adjacent live cells (a “domino”) will in the next step form two side-by-side dominos, in the step after that three dominos (with the center one in the initial location), and so on, building a wall two cells wide that alternates between dominos and dead cells, and oscillates with period two.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/wall.svg" alt="Wall of alternating dominos in the Gilbert cellular automaton" /></p>

<p>Here’s what it looks like when I selected a large rectangle, randomly filled it with 2% live cells, and ran it in Golly. The red lines in this image are walls like the one above.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/wide.png" alt="Gilbert cellular automaton on a sparse random field" /></p>

<p>Of course, at the edges of the randomly filled rectangle, the walls shoot off to infinity with no more obstructions. Here’s a closeup, showing the detailed pattern of the walls and how they meet:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/crop.png" alt="Close-up of Gilbert cellular automaton on a sparse random field" /></p>

<p>It looks a lot like a Gilbert tessellation to me! Even starting with a 50% random fill produces the same sort of pattern at a finer scale:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/5050.png" alt="Gilbert cellular automaton on a 50/50 random field" /></p>

<p>But rather than just going by intuitive visual appearance here’s some analysis showing that, if the plane is filled with live cells with some small probability \(p\), and then this rule is run on the result, then as \(p\to0\) the probability of seeing something that looks like a Gilbert tessellation in the neighborhood of any cell will tend to one. By “neighborhood” I mean everything within distance \(r\), where \(r\) should be chosen as a function of \(p\) that grows faster than linearly in \(1/p\) (so that we have nontrivial probability of seeing something other than just empty space) but slower than \((1/p)^{4/3}\).</p>

<p>The reason we need to limit the radius of the neighborhoods is that, in a large enough neighborhood, you will likely see something that deviates from a Gilbert tessellation. In the runs above, for instance, there are some right-angle corners where two line segments both meet and stop, or points where it appears that walls met head-to-head, but this shouldn’t happen in a Gilbert tessellation (or more precisely it happens with probability zero). There are two natural ways of getting a corner: you could start with an L-tromino of live cells, or two growing walls could coincidentally run into each other. But the expected number of triples of nearby live cells within the neighborhood is \(O(p^3r^2)\), and the expected number of pairs of dominos whose walls would meet at the same point is \(O(p^4r^3)\). It might also be the case that three or more initial live cells produce more exotic behavior; if so, the expected number of things like this that happen within the neighborhood is still \(O(p^3r^2)\). With our assumption on the growth rate of \(r\) relative to \(p\), these expected numbers, and therefore the probability of seeing any of these situations within the neighborhood, is negligible. For the same reason they have low probability of occurring close enough to the neighborhood to impinge on it before the Gilbert tessellation within the neighborhood forms.</p>

<p>So with high probability, in neighborhoods of radius \(r\), you’ll only see single live cells and double live cells in the initial state, and the pairs of double cells won’t be horizontally, vertically, or diagonally aligned with each other. Some of the double live cells will be dominos, with density \(\Theta(p^2)\). The only thing that can happen with such a state is that dominos start building walls which grow until they hit each other, exactly as described by a Gilbert tessellation. Once the Gilbert tessellation has been set up, it appears indestructible: the alternating live cells along the wall prevent any births into the layers of dead cells on either side, and if a wall is perturbed at its end it quickly grows back. However, proving this indestructability rigorously would require a more careful case analysis.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107211819080878015">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular.html"><span class="datestr">at November 02, 2021 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19270">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/">Quantum Trick or Treat</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Are crazy quantum walks fact or fiction?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/photo-by-patrick-campbell-university-of-colorado/" rel="attachment wp-att-19272"><img width="150" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/LesleySmith.jpg?resize=150%2C155&amp;ssl=1" class="wp-image-19272" height="155" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NOAA <a href="https://psl.noaa.gov/people/lesley.l.smith/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Lesley Smith is a climate researcher at the University of Colorado’s Cooperative Institute for Research in Environmental Sciences. She is also <a href="https://psl.noaa.gov/people/lesley.l.smith/">associated</a> to the National Oceanic and Atmospheric Administration’s Physical Science Laboratory and has worked on behalf of several other national organizations. Her PhD was in particle physics at the University of Kansas, and she incorporates quantum physics into some of her non-nonfiction writings.</p>
<p>
Today we discuss a quantum walk on a simple graph with behavior wild enough to inspire at least the latter kind of writing.</p>
<p>
Smith was most recently second author on a <a href="https://repository.library.noaa.gov/view/noaa/31032">paper</a> in the April, 2021 <em>Journal of Climate</em> whose title immediately speaks a vital subject: “Explaining the Spatial Pattern of U.S. Extreme Daily Precipitation Change.” The years-long <a href="https://www.courthousenews.com/drought-grips-american-west-with-no-relief-in-sight/">drought</a> in the US West has critically depleted lakes and reservoirs and rivers. But just one week ago, much of California had its <a href="https://www.washingtonpost.com/weather/2021/10/25/atmospheric-river-record-rain-california/">wettest</a> day ever. Both the drought and the pattern of extremes have spread across the country, as many easily-found stories this year attest. The question is how to distinguish standard fluctuations from deep-seated causal factors.</p>
<p>
As for her <a href="http://www.lesleylsmith.com/index.html">other</a> <a href="http://www.lesleylsmith.com/blog.html">writings</a>, this is representative:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumtricksandtreatscover/" rel="attachment wp-att-19274"><img width="167" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumTricksAndTreatsCover.jpg?resize=167%2C250&amp;ssl=1" class="aligncenter wp-image-19274" height="250" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Amazon <a href="https://www.amazon.com/Quantum-Tricks-Treats-Madison-Martin-ebook/dp/B0763DZZ8Q">page</a></font>
</td>
</tr>
</tbody></table>
<p>
The first lines are likewise topical: “Despite it being October, the sun felt warm on my skin. Forest-fire smoke from states west of Colorado gave the air a three-dimensional quality…” She has also written novels titled <em>Quantum Murder</em>, <em>The Quantum Cop</em>, and <em>Quantum Juneteenth</em>, plus just now a <a href="http://www.lesleylsmith.com/shortfiction.html">short story</a>, “Lucky Halloween,” about a quantum computer scientist. She is writing a series called <em>Kat Cubed</em>—well we will raise Schrödinger cat-walks to powers well beyond cubed below.</p>
<p>
</p><p></p><h2> Climate and Dynamism </h2><p></p>
<p></p><p>
The climate paper’s opening paragraphs review the the general prediction of greater precipitation from the thermal component of climate change and the effects of changes in atmospheric currents. Then it observes:</p>
<blockquote><p><b> </b> <em> Yet, the regional pattern of observed heavy precipitation trends across the United States departs from this theoretical expectation, being remarkable for its heterogeneity rather than the more uniform structure that thermodynamic effects alone would predict. … The regionally diverse trends in extreme precipitation are likely related to dynamical factors. </em>
</p></blockquote>
<p></p><p>
The paper speaks to care needed to distinguish causes and draws two conclusions:</p>
<blockquote><p><b> </b> <em> First, the absence of appreciable increases [in annual maximum single day of rain] over the West is attributed to a dynamical effect of the observed centennial-scale trends in [sea-surface temperature], sea ice, and atmospheric composition. … A second factor is sampling variability […from…] a combination of moderate forced increases comingled with a strong articulation of internal atmospheric variability. </em>
</p></blockquote>
<p></p><p>
The nub of the latter from my perspective is distinguishing what we can infer on the basis of increased dynamism alone. In our <a href="https://rjlipton.wpcomstaging.com/2014/01/30/global-warming/">post</a> seven years ago on global warming, I opined that greater energy and dynamism should be treated as more-primary effects than temperature. The challenge of inferring further effects from dynamism is recognized by another excerpt from the conclusions:</p>
<blockquote><p><b> </b> <em> Recognizing the potency of unforced atmospheric dynamics, [one cannot discount] a scenario in which internal variability could mute if not reverse the observed upward trend in eastern U.S. [extreme rains]. </em>
</p></blockquote>
<p></p><p>
In the rest of this post, we’ll offer an example of wild dynamics from a small quantum system that may speak to both her <em>métiers</em>.</p>
<p>
</p><p></p><h2> Classical Random and Quantum Walks </h2><p></p>
<p></p><p>
Consider the following graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. It looks like a Q-tip. The two self-loops depart from usual examples of undirected graphs. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/propellergraph/" rel="attachment wp-att-19275"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/PropellerGraph.png?resize=550%2C72&amp;ssl=1" class="aligncenter size-large wp-image-19275" height="72" /></a></p>
<p></p><p><br />
A classical random walk on <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> starts at some node—say node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />—and at each step flips a coin to determine the next node. The “walker” stays on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> if the result is heads (<img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />) but goes to node <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> if tails (<img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" />). The walker cannot go from <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> in one step because there is no edge. The <b>probabilistic transition matrix</b> <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> is given at right. The vector </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Bbmatrix%7D+0.5+%5C%5C+0.25+%5C%5C+0.25+%5Cend%7Bbmatrix%7D+%3D+A_G%5Ccdot+%5Cbegin%7Bbmatrix%7D+0.5+%5C%5C+0.5+%5C%5C+0+%5Cend%7Bbmatrix%7D+%3D+A_G%5E2+%5Ccdot+x%5E%7B%280%29%7D+%5Cqquad%5Ctext%7Bwhere%7D%5Cqquad+x%5E%7B%280%29%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%5C%5C+0+%5C%5C+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{bmatrix} 0.5 \\ 0.25 \\ 0.25 \end{bmatrix} = A_G\cdot \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix} = A_G^2 \cdot x^{(0)} \qquad\text{where}\qquad x^{(0)} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} " class="latex" /></p>
<p>gives the probabilities for each node after two steps. (Although multiplying row vectors on the left is commonly used for classical walks, we will use column vectors for consistency with quantum usage; that <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> is symmetric moots the difference anyway.) For any <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B%28t%29%7D+%3D+A_G%5Et+x_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{(t)} = A_G^t x_0}" class="latex" /> can be called the “classical state” of the walk after <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> steps. </p>
<p>
The state <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> whose entries <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Bi%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x[i]}" class="latex" /> are given by the degree of node <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> divided by the sum of the degrees makes <img src="https://s0.wp.com/latex.php?latex=%7BA_G+x+%3D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G x = x}" class="latex" />, and thus gives a <b>stable distribution</b> for the walk. A central theorem states that for every <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> that is not bipartite, the powers of <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> converge entrywise in every column to a unique stable distribution. When <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is regular, as here, this is simply the uniform distribution. The convergence in our case is quite quick, as signified by the exact values </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_G%5E%7B10%7D+%3D+%5Cbegin%7Bbmatrix%7D+0.333984375+%26+0.3330078125+%26+0.3330078125+%5C%5C+0.3330078125+%26+0.333984375+%26+0.3330078125+%5C%5C+0.3330078125+%26+0.3330078125+%26+0.333984375+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  A_G^{10} = \begin{bmatrix} 0.333984375 &amp; 0.3330078125 &amp; 0.3330078125 \\ 0.3330078125 &amp; 0.333984375 &amp; 0.3330078125 \\ 0.3330078125 &amp; 0.3330078125 &amp; 0.333984375 \end{bmatrix}. " class="latex" /></p>
<p>
The quantum case needs to bind the graph and the “coin” into one system, so its domain has six members, which we list in order </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathcal%7BS%7D+%3D+%5C%7B%281%2CH%29%2C%281%2CT%29%2C%282%2CH%29%2C%282%2CT%29%2C%283%2CH%29%2C%283%2CT%29%5C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathcal{S} = \{(1,H),(1,T),(2,H),(2,T),(3,H),(3,T)\}. " class="latex" /></p>
<p>In quantum notation, this is the tensor product of the “node space” <img src="https://s0.wp.com/latex.php?latex=%7BV+%3D+%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V = \{1,2,3\}}" class="latex" /> and the “coin space” <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D+%3D+%5C%7BH%2CT%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{C} = \{H,T\}}" class="latex" />. The meaning of <img src="https://s0.wp.com/latex.php?latex=%7B%28v%2C%5Cmathfrak%7Bc%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(v,\mathfrak{c})}" class="latex" /> is that the walker just arrived at node <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> via the coin outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" />. It is incumbent that every node <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> can be reached on either outcome. Our <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />–<img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> labeling above complies. Then we get a permutation <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> from the action </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%28v%2C%5Cmathfrak%7Bc%7D%29+%3D+%28v%27%2C%5Cmathfrak%7Bc%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P(v,\mathfrak{c}) = (v',\mathfrak{c}), " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bv%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v'}" class="latex" /> is the vertex reached from <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> by the outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" /> from the classical walk, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" /> is kept the same. The penultimate trick is to expand this to a mapping <img src="https://s0.wp.com/latex.php?latex=%7BP%27%3A+%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BC%7D+%5Crightarrow+%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P': \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{S}}" class="latex" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%27%28%28v%2C%5Cmathfrak%7Bb%7D%29%2C%5Cmathfrak%7Bc%7D%29+%3D+%28v%27%2C%5Cmathfrak%7Bc%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P'((v,\mathfrak{b}),\mathfrak{c}) = (v',\mathfrak{c}), " class="latex" /></p>
<p>whereby the previous coin outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{b}}" class="latex" /> is “forgotten” while the current flip is preserved in the state. We can represent this by the <em>directed</em> graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> shown next. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumgraph/" rel="attachment wp-att-19281"><img width="360" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumGraph.png?resize=360%2C210&amp;ssl=1" class="aligncenter wp-image-19281" height="210" /></a></p>
<p></p><p><br />
Incidentally, <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> is planar (switch nodes <img src="https://s0.wp.com/latex.php?latex=%7B2H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2H}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B2T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2T}" class="latex" /> to see). The last trick is to label its edges by quantum amplitudes for the coin outcomes.</p>
<p>
</p><p></p><h2> Realm of the Coin </h2><p></p>
<p></p><p>
Our quantum coin can be “flipped” by applying any chosen <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2 \times 2}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> to the current state of the coin, provided <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> is a unitary matrix. In general, if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular graph and we have a suitable labeling for results of a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-sided die, then <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> can be a <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d \times d}" class="latex" /> matrix acting on a single <b>qudit</b>, or we can use <img src="https://s0.wp.com/latex.php?latex=%7B%5Clceil+%5Clog_2+d+%5Crceil%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lceil \log_2 d \rceil}" class="latex" /> qubits to encode the coin. But here we just need one qubit. Before fixing the choice of coin matrix, we can represent it abstractly as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BU%7D+%3D+%5Cbegin%7Bbmatrix%7D+a+%26+b+%5C%5C+c+%26+d+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{U} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} " class="latex" /></p>
<p>To apply the coin on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{S}}" class="latex" />, we form the <img src="https://s0.wp.com/latex.php?latex=%7B6+%5Ctimes+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6 \times 6}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%27%7D+%3D+%5Cmathbf%7BI%7D+%5Cotimes+%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U'} = \mathbf{I} \otimes \mathbf{U}}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BI%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{I}}" class="latex" /> is the <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3}" class="latex" /> identity matrix. To move the walker after the coin result, we apply the <img src="https://s0.wp.com/latex.php?latex=%7B6+%5Ctimes+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6 \times 6}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{P}}" class="latex" /> of the permutation <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" />. Since we use column vectors for states, the walk matrix is given by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D+%3D+%5Cmathbf%7BP%7D%5Ccdot%5Cmathbf%7BU%27%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+0+%26+1+%5C%5C+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+a+%26+b+%26+0+%26+0+%26+0+%26+0+%5C%5C+c+%26+d+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+a+%26+b+%26+0+%26+0+%5C%5C+0+%26+0+%26+c+%26+d+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+a+%26+b+%5C%5C+0+%26+0+%26+0+%26+0+%26+c+%26+d+%5Cend%7Bbmatrix%7D+%3D+%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7Ccc%7Ccc%7D+a+%26+b+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+c+%26+d+%26+0+%26+0+%5C%5C+%5Chline+0+%26+0+%26+0+%26+0+%26+a+%26+b+%5C%5C+c+%26+d+%26+0+%26+0+%26+0+%26+0+%5C%5C+%5Chline+0+%26+0+%26+a+%26+b+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+c+%26+d+%5Cend%7Barray%7D%5Cright%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W} = \mathbf{P}\cdot\mathbf{U'} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \cdot \begin{bmatrix} a &amp; b &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ c &amp; d &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; a &amp; b &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; c &amp; d &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; a &amp; b \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; c &amp; d \end{bmatrix} = \left[\begin{array}{cc|cc|cc} a &amp; b &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; c &amp; d &amp; 0 &amp; 0 \\ \hline 0 &amp; 0 &amp; 0 &amp; 0 &amp; a &amp; b \\ c &amp; d &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ \hline 0 &amp; 0 &amp; a &amp; b &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; c &amp; d \end{array}\right] " class="latex" /></p>
<p>We have drawn lines to show how the coin acts between each pair of nodes. Notice that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> is no longer symmetric, so specifying column-vector inputs matters. For instance, the <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> in the top row comes in from the column at <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" /> and exits at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />. This means the previous coin result was tails and took the walker (from node 2) to node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />; now the coin gives heads so the walker stays at node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />. We can diagram this action also on the expanded graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" />, so that the <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> goes on the arrow from <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />, and so on:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumgprime/" rel="attachment wp-att-19278"><img width="300" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumGprime.png?resize=300%2C193&amp;ssl=1" class="aligncenter wp-image-19278" height="193" /></a></p>
<p>
The first step has no “previous coin result” but we still have to specify one to initialize the coin state as well as the walker’s state. For starting on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> there is still an infinitude of combinations of the basis states <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" />. If we fix the start as <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />, this is like saying the coin was heads-up before the initial act of flipping it.</p>
<p>
</p><p></p><h2> Walking Into Chaos </h2><p></p>
<p></p><p>
A natural and popular choice of coin is the Hadamard matrix </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}. " class="latex" /></p>
<p>We need only mark the edges with <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d = -1}" class="latex" />, keeping the normalizing <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{\sqrt{2}}}" class="latex" /> in mind. Here are <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> and the three steps of the quantum walk after the first:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/fouriterations/" rel="attachment wp-att-19279"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/FourIterations.png?resize=550%2C282&amp;ssl=1" class="aligncenter size-large wp-image-19279" height="282" /></a></p>
<p>
To visualize the <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />-step walk, say starting from <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_0+%3D+1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_0 = 1H}" class="latex" />, we take each possible path of length <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> and multiply the numbers on its edges. Over all paths that end at the same <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D%3D%28v%2C%5Cmathfrak%7Bc%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}=(v,\mathfrak{c})}" class="latex" /> we add those products to give the <b>amplitude</b> <img src="https://s0.wp.com/latex.php?latex=%7Ba_%7Bu%2Cv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a_{u,v}}" class="latex" />, which in our column-first notation equals the entry <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5Et%5B%5Cmathfrak%7Bs%7D%2C%5Cmathfrak%7Bs%7D_0%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}^t[\mathfrak{s},\mathfrak{s}_0]}" class="latex" />. </p>
<p>
For example, there are two paths of <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=3}" class="latex" /> steps from <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> back to itself. One takes the loop at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> three times and contributes the phase value <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, but the other does <img src="https://s0.wp.com/latex.php?latex=%7B1H%5Crightarrow+2T+%5Crightarrow+1T+%5Crightarrow+1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H\rightarrow 2T \rightarrow 1T \rightarrow 1H}" class="latex" /> and picks up the value <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" />. Adding those values cancels them, leaving the red <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> shown at upper left in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}^3}" class="latex" />. If our walker is a Schrödinger cat, that part of it is not just “dead” but completely annihilated.</p>
<p>
It is still possible for the cat to end on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> after being cubed, because the path <img src="https://s0.wp.com/latex.php?latex=%7B1H+%5Crightarrow+1H+%5Crightarrow+2T+%5Crightarrow+1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H \rightarrow 1H \rightarrow 2T \rightarrow 1T}" class="latex" /> is not canceled. That the cat ends with phase <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" /> does not matter, because to get the probabilities—given we started at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />—we take the squared magnitude of each entry in column <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> and add the adjacent pairs denoting the same vertex of the original graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Thus the cat has probability only </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_1%5E%7B%283%29%7D+%3D+%5Cfrac%7B0%5E2+%2B+%28-1%29%5E2%7D%7B%282%5Csqrt%7B2%7D%29%5E2%7D+%3D+%5Cfrac%7B1%7D%7B8%7D+%3D+0.125+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  p_1^{(3)} = \frac{0^2 + (-1)^2}{(2\sqrt{2})^2} = \frac{1}{8} = 0.125 " class="latex" /></p>
<p>of being still on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> after three time steps. Given the ease of looping at node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, this is counterintuitive. After nine and ten steps, we have: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D%5E9+%3D+%5Cfrac%7B1%7D%7B2%5E%7B4.5%7D%7D%5Cbegin%7Bbmatrix%7D+18+%26+4+%26+-1+%26+13+%26+-1+%26+-1%5C%5C+13+%26+1+%26+4+%26+-18+%26+-1+%26+1%5C%5C+-1+%26+13+%26+-1+%26+-1+%26+18+%26+4%5C%5C+4+%26+-18+%26+-1+%26+1+%26+13+%26+1%5C%5C+-1+%26+-1+%26+18+%26+4+%26+-1+%26+13%5C%5C+-1+%26+1+%26+13+%26+1+%26+4+%26+-18+%5Cend%7Bbmatrix%7D+.%5Cqquad+%5Cmathbf%7BW%7D%5E%7B10%7D+%3D+%5Cfrac%7B1%7D%7B32%7D%5Cbegin%7Bbmatrix%7D+31+%26+5+%26+3+%26+-5+%26+-2+%26+0%5C%5C+-5+%26+31+%26+0+%26+-2+%26+5+%26+3%5C%5C+-2+%26+0+%26+31+%26+5+%26+3+%26+-5%5C%5C+5+%26+3+%26+-5+%26+31+%26+0+%26+-2%5C%5C+3+%26+-5+%26+-2+%26+0+%26+31+%26+5%5C%5C+0+%26+-2+%26+5+%26+3+%26+-5+%26+31+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W}^9 = \frac{1}{2^{4.5}}\begin{bmatrix} 18 &amp; 4 &amp; -1 &amp; 13 &amp; -1 &amp; -1\\ 13 &amp; 1 &amp; 4 &amp; -18 &amp; -1 &amp; 1\\ -1 &amp; 13 &amp; -1 &amp; -1 &amp; 18 &amp; 4\\ 4 &amp; -18 &amp; -1 &amp; 1 &amp; 13 &amp; 1\\ -1 &amp; -1 &amp; 18 &amp; 4 &amp; -1 &amp; 13\\ -1 &amp; 1 &amp; 13 &amp; 1 &amp; 4 &amp; -18 \end{bmatrix} .\qquad \mathbf{W}^{10} = \frac{1}{32}\begin{bmatrix} 31 &amp; 5 &amp; 3 &amp; -5 &amp; -2 &amp; 0\\ -5 &amp; 31 &amp; 0 &amp; -2 &amp; 5 &amp; 3\\ -2 &amp; 0 &amp; 31 &amp; 5 &amp; 3 &amp; -5\\ 5 &amp; 3 &amp; -5 &amp; 31 &amp; 0 &amp; -2\\ 3 &amp; -5 &amp; -2 &amp; 0 &amp; 31 &amp; 5\\ 0 &amp; -2 &amp; 5 &amp; 3 &amp; -5 &amp; 31 \end{bmatrix}. " class="latex" /></p>
<p>The corresponding node probabilities are <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%289%29%7D+%3D+%5B+0.96289%2C+0.0332%2C+0.00391%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(9)} = [ 0.96289, 0.0332, 0.00391]}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2810%29%7D+%3D+%5B+0.96289%2C+0.02832%2C+0.00879%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(10)} = [ 0.96289, 0.02832, 0.00879]}" class="latex" />. The probability of the first node is the same between every odd and even step, echoing how starting at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> presumes a previous coin flip of heads, which kept the cat on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t = -1}" class="latex" />. At <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=10}" class="latex" /> the cat has almost come back fully to node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, indeed to the initial quantum state <img src="https://s0.wp.com/latex.php?latex=%7B%7C1H%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|1H\rangle}" class="latex" />, but not quite. The swing is even closer at <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D38%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=38}" class="latex" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D%5E%7B38%7D+%3D+%5Cfrac%7B1%7D%7B524288%7D%5Cbegin%7Bbmatrix%7D+522919+%26+-23939+%26+-11285+%26+23939+%26+12654+%26+0%5C%5C+23939+%26+522919+%26+0+%26+12654+%26+-23939+%26+-11285%5C%5C+12654+%26+0+%26+522919+%26+-23939+%26+-11285+%26+23939%5C%5C+-23939+%26+-11285+%26+23939+%26+522919+%26+0+%26+12654%5C%5C+-11285+%26+23939+%26+12654+%26+0+%26+522919+%26+-23939%5C%5C+0+%26+12654+%26+-23939+%26+-11285+%26+23939+%26+522919+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W}^{38} = \frac{1}{524288}\begin{bmatrix} 522919 &amp; -23939 &amp; -11285 &amp; 23939 &amp; 12654 &amp; 0\\ 23939 &amp; 522919 &amp; 0 &amp; 12654 &amp; -23939 &amp; -11285\\ 12654 &amp; 0 &amp; 522919 &amp; -23939 &amp; -11285 &amp; 23939\\ -23939 &amp; -11285 &amp; 23939 &amp; 522919 &amp; 0 &amp; 12654\\ -11285 &amp; 23939 &amp; 12654 &amp; 0 &amp; 522919 &amp; -23939\\ 0 &amp; 12654 &amp; -23939 &amp; -11285 &amp; 23939 &amp; 522919 \end{bmatrix} " class="latex" /></p>
<p>with state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_%7B38%7D+%3D+%5B0.99739%2C+0.04566%2C+0.02414%2C+-0.04566%2C+-0.02152%2C+0%5D%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_{38} = [0.99739, 0.04566, 0.02414, -0.04566, -0.02152, 0]^T}" class="latex" /> and node probabilities <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2838%29%7D+%3D+%5B+0.99687%2C+0.00267%2C+0.00046%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(38)} = [ 0.99687, 0.00267, 0.00046]}" class="latex" />. The next probabilities, however, are <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2839%29%7D+%3D+%5B+0.54641%2C+0.45313%2C+0.00046%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(39)} = [ 0.54641, 0.45313, 0.00046]}" class="latex" />, which is not close to the first-step distribution <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%281%29%7D+%3D+%5B0.5%2C0.5%2C0%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(1)} = [0.5,0.5,0]}" class="latex" />. </p>
<p>
Most treatments of quantum walks, including a seminal <a href="https://arxiv.org/abs/quant-ph/0012090">paper</a> by Dorit Aharonov, Andris Ambainis, Julia Kempe, and Umesh Vazirani, and an influential 2003 <a href="https://arxiv.org/abs/quant-ph/0303081">survey</a> by Kempe, emphasize the dispersion time being linear rather than quadratic as in classical random walks on undirected graphs. Here, the quadratic nature effects quick divergence between close states like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_%7B38%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_{38}}" class="latex" />, which is the signature of dynamical chaos. </p>
<p>
The coin <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BJ%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+i+%5C%5C+i+%26+1%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{J} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; i \\ i &amp; 1\end{bmatrix}}" class="latex" /> is reputed to make quantum walks smoother. That works to some extent for our three-node graph, but there appears to be no periodic stability, let alone convergence as in the classical case. Simple Python code for trying other coins is available <a href="https://cse.buffalo.edu/~regan/cse610/qtipwalk.py">here</a>.</p>
<p>
</p><p></p><h2> Real or Fiction? </h2><p></p>
<p></p><p>
Our main question is simple: </p>
<blockquote><p><b> </b> <em> Are these crazy walks real? </em>
</p></blockquote>
<p></p><p>
The answer is <em>yes</em> if we can engineer a qutrit+qubit system that evolves by repeating <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" />. <em>Simulating</em> such a system throws up a second issue: No one would say that the classical computer executing my Python code is behaving chaotically. We can program <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> via basic gates available to run on real hardware at <a href="https://quantum-computing.ibm.com/">IBM Quantum</a> or some other services. </p>
<blockquote><p><b> </b> <em> Is quantum hardware that applies powers of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> behaving chaotically? Does that pose a concretely physical impediment to maintaining its coherence? </em>
</p></blockquote>
<p></p><p>
On expecting a <em>yes</em> answer to the walks’ physical existence in the microworld, our questions next try to connect to Smith’s paper:</p>
<blockquote><p><b> </b> <em> Does the mathematical existence of chaotic quantum systems on such small scales have macroscopic effects in our world? Such as on the chaotic dynamics of our atmosphere? </em>
</p></blockquote>
<p></p><p>
Again, the answer should be <em>yes</em> at the level of <a href="https://link.springer.com/chapter/10.1007/3-540-09718-X_73">connecting</a> Brownian motion to quantum processes, or of quantum-walk <a href="https://repository.tudelft.nl/islandora/object/uuid:08ad4a94-483d-46db-8840-6f73c3e48a70/datastream/OBJ/download">interpretations</a> of physical systems such as the quantum harmonic oscillator (<a href="https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator">QHO</a>). So the most specific form of our question is:</p>
<blockquote><p><b> </b> <em> Does our (not-so-)simple three-node quantum walk appear as a non-negligible term in a Feynman-style summation needed to understand a macroscopic physical system? </em>
</p></blockquote>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can the quantum chaos of the Q-tip graph be tamed? See <a href="https://arxiv.org/pdf/2008.00316.pdf">this</a> for success on other graphs including the 3-cycle. There are are affinities between the 3-cycle and Q-tip walks, especially at even timesteps, and the 3-cycle is an option in the Python code.</p>
<p>
Smith also has many activities promoting STEM, including her <a href="http://www.physicsisfun.net/">Phyiscs Is Fun</a> website and a <a href="https://books2read.com/u/bMavLX">book</a> on particle physics.  She maintains a <a href="http://www.lesleylsmith.com/blog.html">blog</a> of her writing and reading, plus much else on her <a href="http://www.lesleylsmith.com/">author site</a>.  Dick and I were already wondering about women in the community of math/theory bloggers, such as <a href="https://blog.tanyakhovanova.com/">Tanya Khovanova</a>, <a href="https://fractalkitty.com/">Sophia Wood</a>, <a href="http://vihart.com/">Vi Hart</a>, and <a href="https://mathmunch.org/author/aweltman/">Anna Weltman</a>.  </p>
<p></p><p><br />
[fixed link for Python code, some word changes, fixed entry of J matrix]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/"><span class="datestr">at November 02, 2021 03:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6098">Q2B 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This is a quick post to let people know that the 2021 <a href="https://q2b.qcware.com/">Q2B (Quantum 2 Business) conference</a> will be this December 7-9 at the Santa Clara Convention Center.  (Full disclosure: Q2B is hosted by <a href="https://qcware.com/">QC Ware, Inc.</a>, to which I’m the scientific adviser.)  Barring a dramatic rise in cases or the like, I’m planning to attend to do my Ask-Me-Anything session, in what’s become an annual tradition.  Notably, this will be my first in-person conference, and in fact my first professional travel of any kind, since before covid shut down the US in late March 2020.  I hope to see many of you there!  And if you <em>won’t</em> be at Q2B, but you’ll be in the Bay Area and would like to meet otherwise, let me know and we’ll try to work something out.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6098"><span class="datestr">at November 01, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2754344269552524293">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html">When did Math Get So Hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
I have been on many Math PhD thesis defense's  as the Dean's Representative. This means I don't have to understand the work, just make sure the rules are followed. I've done this for a while and I used to understand some of it but now there are times I understand literally none of it. As a result, when the student leaves the room and we talk among ourselves I ask<br />
<br />
<br />
When did Math get so hard?<br />
<br />
I mean it as a statement and maybe a joke, but I decided to email various people and ask for a serious answer. Here are some thoughts of mine and others<div><br /></div><div>1) When you get older math got harder. Lance blogged on this <a href="https://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html">here</a></div><div><br /></div><div>2) When math got more abstract it got harder. Blame Grothendieck.</div><div><br /></div><div>3) When math stopped being tied to the real work it got harder. Blame Hardy. </div><div><br /></div><div>4) Math has always been hard. We NOW understand some of the older math better so it seems easy to us, but it wasn't at the time. </div><div><br /></div><div>5) With the web and more people working in math, new results come out faster so its harder to keep up.</div><div><br /></div><div>6) All fields of math have a period of time when they are easy, at the beginning, and then as the low-hanging fruit gets picked it gets harder and harder.  So if a NEW branch was started it might initially be easy. Counterthought- even a new branch might be hard now since it can draw on so much prior math. Also, the low hanging fruit may be picked rather quickly. </div><div><br />
<br /><br />
</div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html"><span class="datestr">at October 31, 2021 07:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
