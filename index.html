<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at October 27, 2020 06:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17726">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/">A Vast and Tiny Breakthrough</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Christofides bound beaten by an epsilon’s idea of epsilon</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/karlinkleinoveisgharan/" rel="attachment wp-att-17728"><img width="216" alt="" src="https://rjlipton.files.wordpress.com/2020/10/karlinkleinoveisgharan.jpg?w=216&amp;h=107" class="alignright wp-image-17728" height="107" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.seattletimes.com/education-lab/yes-women-can-and-do-want-to-code-uw-professors-and-alumnae-say/">src1</a>, <a href="https://homes.cs.washington.edu/~nwklein/">src2</a>, <a href="https://www.engr.washington.edu/facresearch/newfaculty/2013/shayanoveisgharan.html">src3</a></font></td>
</tr>
</tbody>
</table>
<p>
Anna Karlin, Nathan Klein, and Shayan Oveis Gharan have made a big splash with the number </p>
<p>         <img src="https://s0.wp.com/latex.php?latex=%7E%7E%7E%7E%7E%7E%5Cfrac%7B1%7D%7B1%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="~~~~~~\frac{1}{1,000,000,000,000,000,000,000,000,000,000,000,000}. " class="latex" title="~~~~~~\frac{1}{1,000,000,000,000,000,000,000,000,000,000,000,000}. " /></p>
<p>No that is not the amount of the US debt, or the new relief bill. It is the fraction by which the hallowed 44-year-old upper bound of <img src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.5}" class="latex" title="{1.5}" /> on the approximation ratio of the metric Traveling Salesperson Problem has been improved. With the help of randomization, we hasten to add.</p>
<p>
Today we discuss the larger meaning of their tiny breakthrough. </p>
<p>
The abstract of their <a href="https://arxiv.org/abs/2007.01409">paper</a> is as pithy as can be:</p>
<blockquote><p><b> </b> <em> For some <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+10%5E%7B-36%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 10^{-36}}" class="latex" title="{\epsilon &gt; 10^{-36}}" /> we give a <img src="https://s0.wp.com/latex.php?latex=%7B3%2F2+-+%5Cepsilon%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3/2 - \epsilon}" class="latex" title="{3/2 - \epsilon}" /> approximation algorithm for metric TSP. </em>
</p></blockquote>
<p></p><p>
Metric TSP means that the cost of the tour <img src="https://s0.wp.com/latex.php?latex=%7B%28v_1%2Cv_2%2C%5Cdots%2Cv_n%2Cv_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(v_1,v_2,\dots,v_n,v_1)}" class="latex" title="{(v_1,v_2,\dots,v_n,v_1)}" /> is the sum of the distances of the edges </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28v_1%2Cv_2%29+%2B+%5Cmu%28v_2%2Cv_3%29+%2B+%5Ccdots+%2B+%5Cmu%28v_%7Bn-1%7D%2Cv_n%29+%2B+%5Cmu%28v_n%2Cv_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mu(v_1,v_2) + \mu(v_2,v_3) + \cdots + \mu(v_{n-1},v_n) + \mu(v_n,v_1) " class="latex" title="\displaystyle  \mu(v_1,v_2) + \mu(v_2,v_3) + \cdots + \mu(v_{n-1},v_n) + \mu(v_n,v_1) " /></p>
<p>according to a given metric <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu}" class="latex" title="{\mu}" />. When the points are in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb{R}^m}" class="latex" title="{\mathbb{R}^m}" /> with the Euclidean metric, an <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{O(1)}}" class="latex" title="{n^{O(1)}}" />-time algorithm can come within a factor <img src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1+\delta)}" class="latex" title="{(1+\delta)}" /> of the optimal cost for any prescribed <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt; 0}" class="latex" title="{\delta &gt; 0}" />. Sanjeev Arora and Joseph Mitchell jointly won the 2002 Gödel Prize for their randomized algorithms doing exactly that. The rub is the constant in the “<img src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(1)}" class="latex" title="{O(1)}" />” depends on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" title="{\delta}" />—indeed, nobody knows how to make it scale less than linearly in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{\delta}}" class="latex" title="{\frac{1}{\delta}}" />. But for general metrics, getting within a factor of <img src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1+\delta)}" class="latex" title="{(1+\delta)}" /> is known to be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-hard for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta}" class="latex" title="{\delta}" /> up to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B122%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{122}}" class="latex" title="{\frac{1}{122}}" />.</p>
<p>
Some intermediate cases of metrics had <a href="https://arxiv.org/abs/1201.1870">allowed</a> getting within a factor of <img src="https://s0.wp.com/latex.php?latex=%7B1.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.4}" class="latex" title="{1.4}" />, but for general metrics the <img src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.5}" class="latex" title="{1.5}" /> factor <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a025602.pdf">found</a> in 1976 by the <a href="https://www.imperial.ac.uk/news/196798/obituary-nicos-christofides-1942-2019/">late</a> Nicos Christofides, and <a href="http://nas1.math.nsc.ru/aim/journals/us/us17/us17_007.pdf">concurrently</a> by Anatoliy Serdyukov, stood like a brick wall. Well, we didn’t expect it to be a brick wall at first. Let me tell a story.</p>
<p>
</p><p></p><h2> A Proof in a Pub </h2><p></p>
<p></p><p>
Soon after starting as a graduate student at Oxford in 1981, I went with a bunch of dons and fellow students down to London for a one-day workshop where Christofides was among the speakers and presented his result along with newer work. I’d already heard it spoken of as a combinatorial gem and perfect motivator for a graduate student to appreciate the power of combining simplicity and elegance:</p>
<ol>
<li>
Calculate the (or a) minimum spanning tree <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> given points. <p></p>
</li><li>
Take <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" /> to be the leaves and any other odd-degree nodes of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> and calculate a minimum matching <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{M}" class="latex" title="{M}" /> of them. <p></p>
</li><li>
The graph <img src="https://s0.wp.com/latex.php?latex=%7BT%2BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T+M}" class="latex" title="{T+M}" /> has all nodes of even degree so it has an easily-found Eulerian cycle <img src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_E}" class="latex" title="{C_E}" />. <p></p>
</li><li>
The cycle <img src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_E}" class="latex" title="{C_E}" /> may repeat vertices, but by the triangle inequality for the distance metric <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu}" class="latex" title="{\mu}" />, we can bypass repeats to create a Hamilton cycle <img src="https://s0.wp.com/latex.php?latex=%7BC_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_H}" class="latex" title="{C_H}" /> giving <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28C_H%29+%5Cleq+%5Cmu%28C_E%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(C_H) \leq \mu(C_E)}" class="latex" title="{\mu(C_H) \leq \mu(C_E)}" />.
</li></ol>
<p>
Now any optimal TSP tour <img src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_O}" class="latex" title="{C_O}" /> arises as a spanning tree plus an edge, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28T%29+%3C+%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(T) &lt; \mu(C_O)}" class="latex" title="{\mu(T) &lt; \mu(C_O)}" />. And <img src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_O}" class="latex" title="{C_O}" /> can be partitioned into two sets of paths with endpoints in <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" />. One of those sets has weight at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{2}\mu(C_O)}" class="latex" title="{\frac{1}{2}\mu(C_O)}" /> and yet matches all pairs of <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28M%29+%5Cleq+%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(M) \leq \frac{1}{2}\mu(C_O)}" class="latex" title="{\mu(M) \leq \frac{1}{2}\mu(C_O)}" />. It follows that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28C_H%29+%5Cleq+%5Cmu%28T%29+%2B+%5Cmu%28M%29+%3C+%5Cmu%28C_0%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(C_H) \leq \mu(T) + \mu(M) &lt; \mu(C_0) = \frac{1}{2}\mu(C_0)}" class="latex" title="{\mu(C_H) \leq \mu(T) + \mu(M) &lt; \mu(C_0) = \frac{1}{2}\mu(C_0)}" /> and we’re done.</p>
<p>
My memory of what we did after the workshop is hazy but I’m quite sure we must have gone to a pub for dinner and drinks before taking the train back up to Oxford. My point is, the above proof is <em>the kind that can be told and discussed in a pub</em>. It combines several greatest hits of the field: minimum spanning tree, perfect matching, Euler tour, Hamiltonian cycle, triangle inequality. The proof needs no extensive calculation; maybe a napkin to draw <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" title="{A}" /> on <img src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_0}" class="latex" title="{C_0}" /> and the partition helps. </p>
<p>
The conversation would surely have gone to the question,</p>
<blockquote><p><b> </b> <em> Can the <img src="https://s0.wp.com/latex.php?latex=%7B1.5%5C%3B%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.5\;}" class="latex" title="{1.5\;}" /> factor be beaten? </em>
</p></blockquote>
<p></p><p>
A perfect topic for mathematical pub conversation. Let’s continue as if that’s what happened next—I wish I could recall it.</p>
<p>
</p><p></p><h2> Trees That Snake Around </h2><p></p>
<p></p><p>
Note that the proof already “beats” it in the sense of there being a strict inequality, and it really shows </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28C_H%29+%5Cleq+%281.5+-+%5Cfrac%7B1%7D%7Bn%7D%29%5Cmu%28C_0%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mu(C_H) \leq (1.5 - \frac{1}{n})\mu(C_0). " class="latex" title="\displaystyle  \mu(C_H) \leq (1.5 - \frac{1}{n})\mu(C_0). " /></p>
<p>The advantage <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{n}}" class="latex" title="{\frac{1}{n}}" /> shrinks to zero as <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> grows, however. Moreover, examples where Christofides’s algorithm does no better than approach <img src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.5}" class="latex" title="{1.5}" /> are easy to draw. Pub walls are often covered with emblems of local organizations, and if one has a <a href="https://en.wikipedia.org/wiki/Caduceus">caduceus</a> symbol it can serve as the drawing:</p>
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/caduceus/" rel="attachment wp-att-17729"><img width="154" alt="" src="https://rjlipton.files.wordpress.com/2020/10/caduceus.png?w=154&amp;h=184" class="aligncenter wp-image-17729" height="184" /></a></p>
<p></p><p><br />
The staff is a path <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> nodes while the snakes alternate edges of weight <img src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1 + \gamma}" class="latex" title="{1 + \gamma}" /> between nodes two apart on the path. Going up one snake and down the other gives an optimal tour of weight <img src="https://s0.wp.com/latex.php?latex=%7B%281+%2B+%5Cgamma%29%28n-2%29+%2B+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1 + \gamma)(n-2) + 2}" class="latex" title="{(1 + \gamma)(n-2) + 2}" /> (using the two outermost path edges to switch between the snakes), which <img src="https://s0.wp.com/latex.php?latex=%7B%5Csim+%281+%2B+%5Cgamma%29n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sim (1 + \gamma)n}" class="latex" title="{\sim (1 + \gamma)n}" />. The snake edges don’t change the path’s being the minimum spanning tree, and for <img src="https://s0.wp.com/latex.php?latex=%7BC_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_H}" class="latex" title="{C_H}" /> this costs <img src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n-1}" class="latex" title="{n-1}" /> plus the weight required to match the path’s endpoints. The extra weight is reckoned as the length of one snake, which <img src="https://s0.wp.com/latex.php?latex=%7B%5Csim+n%5Cfrac%7B1%2B%5Cgamma%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sim n\frac{1+\gamma}{2}}" class="latex" title="{\sim n\frac{1+\gamma}{2}}" />, so the ratio approaches <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{3}{2}}" class="latex" title="{\frac{3}{2}}" /> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\gamma \rightarrow 0}" class="latex" title="{\gamma \rightarrow 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \rightarrow \infty}" class="latex" title="{n \rightarrow \infty}" />. Here are some tantalizing aspects:</p>
<ul>
<li>
The <img src="https://s0.wp.com/latex.php?latex=%7Bn-2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n-2}" class="latex" title="{n-2}" /> snake edges, plus one path edge to connect them, make a <em>maximum</em>-weight spanning tree <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T'}" class="latex" title="{T'}" /> in the graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" /> formed by the two kinds of edges. Yet <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T'}" class="latex" title="{T'}" /> followed by the same steps 2–4 of Christofides’s algorithm would yield and optimum tour. <p></p>
</li><li>
When one is given only the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> points plus the <em>graph metric</em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu}" class="latex" title="{\mu}" /> induced by <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />, not <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" /> itself, then there are much worse spanning trees. The single edge connecting the endpoints <img src="https://s0.wp.com/latex.php?latex=%7B%28v_1%2Cv_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(v_1,v_n)}" class="latex" title="{(v_1,v_n)}" /> of the previous path has weight <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28v_1%2Cv_n%29+%5Capprox+%5Cfrac%7Bn%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu(v_1,v_n) \approx \frac{n}{2}}" class="latex" title="{\mu(v_1,v_n) \approx \frac{n}{2}}" />. <p></p>
</li><li>
Thus <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T'}" class="latex" title="{T'}" /> has relatively low weight compared to these possible other trees. And its weight approaches that of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\gamma \rightarrow 0}" class="latex" title="{\gamma \rightarrow 0}" />. This means that small changes in the size of the tree yield large changes in the quality of the induced tour. <p></p>
</li><li>
The advantage of <img src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T'}" class="latex" title="{T'}" /> its odd-valence nodes have small distance under <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu}" class="latex" title="{\mu}" />. As a path it snakes around so that its ends are near each other, unlike those of the minimum spanning tree <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" />. This raises the question of weighting spanning trees according to a slightly different measure <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mu'}" class="latex" title="{\mu'}" /> that incorporates a term for “<em>odd-closeness</em>.”
</li></ul>
<p>
In 1981, we would not have known about Arora’s and Mitchell’s results, so we would have felt fully on the frontier by embedding the points in the plane and sketching spanning trees and cycles on a piece of paper. After a couple pints of ale we might have felt sure that a simple proof with such evident slack ought to yield to a more sophisticated attack. </p>
<p>
</p><p></p><h2> Helpful Trees </h2><p></p>
<p></p><p>
There is one idea that we might have come up with in a pub. The motivation for choosing <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> to be a minimum spanning tree is that many of its edges go into the Euler tour <img src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_E}" class="latex" title="{C_E}" /> and those bound the final <img src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_O}" class="latex" title="{C_O}" /> even if <img src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C_O}" class="latex" title="{C_O}" /> shortcuts them. So making the total edge weight of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> minimum seems to be the best way to help at that stage. We might have wondered, however, whether there is a way to create <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> to have a stronger direct relation to good tours, if not to the optimal tour. </p>
<p>
Oveis Gharan did have such an idea jointly with a different group of authors a decade ago, in the <a href="https://homes.cs.washington.edu/~shayan/atsp.pdf">best paper</a> of SODA 2010. We cannot seem to get our hands on the optimal tour, nor even a “good” tour if that means a better than <img src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1.5}" class="latex" title="{1.5}" /> factor approximation—that is what we are trying to find to begin with. But there is another “tour” <img src="https://s0.wp.com/latex.php?latex=%7BO%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O^*}" class="latex" title="{O^*}" /> that we <em>can</em> compute. This is an optimum of the <em>linear programming relaxation</em> of TSP, whose relation to the exact-TSP methods of Michael Held and Dick Karp we <a href="https://rjlipton.wordpress.com/2012/05/08/inexact-remarks-on-exact-tsp-algorithms/">covered</a> long back. <img src="https://s0.wp.com/latex.php?latex=%7BO%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O^*}" class="latex" title="{O^*}" /> is not a single tour but rather an ensemble of “fractional tours” where each edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e}" class="latex" title="{e}" /> has a rational number <img src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_e}" class="latex" title="{z_e}" /> representing its contribution to the LP solution. The higher <img src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_e}" class="latex" title="{z_e}" />, the more helpful the edge.</p>
<p>
The objective then becomes to design distributions <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" /> of spanning trees <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> so that:</p>
<ol>
<li>
Sampling <img src="https://s0.wp.com/latex.php?latex=%7BT+%5Cleftarrow+%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T \leftarrow {\cal T}}" class="latex" title="{T \leftarrow {\cal T}}" /> is polynomial-time efficient. <p></p>
</li><li>
For every edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{e}" class="latex" title="{e}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr_%7BT+%5Cleftarrow+%7B%5Ccal+T%7D%7D%5Be+%5Cin+T%5D+%5Cpropto+%281+%2B+%5Cdelta_n%29+z_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Pr_{T \leftarrow {\cal T}}[e \in T] \propto (1 + \delta_n) z_e}" class="latex" title="{\Pr_{T \leftarrow {\cal T}}[e \in T] \propto (1 + \delta_n) z_e}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta_n}" class="latex" title="{\delta_n}" /> is tiny. <p></p>
</li><li>
The distribution <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" /> promotes trees <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> with fewer leaves and odd-valence interior nodes.
</li></ol>
<p>
The algorithmic strategy this fits into is to sample <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> from <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" />, plug <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" title="{T}" /> into the first step of the Christofides algorithm, and continue as before.</p>
<p>
</p><p></p><h2> The Proof and the Pudding </h2><p></p>
<p></p><p>
The first two conditions are solidly defined. Considerable technical details in the SODA 2010 paper and another <a href="https://homes.cs.washington.edu/~shayan/tsp.pdf">paper</a> at FOCS 2011 that was joint with Amin Saberi and Mohit Singh are devoted to them. A third desideratum is that the distribution <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" /> not be over-constrained but rather have maximum entropy, so that for efficiently computable numbers <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lambda_e}" class="latex" title="{\lambda_e}" /> approaching <img src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{z_e}" class="latex" title="{z_e}" /> one has also: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr_%7B%5Ccal+T%7D%28T%29+%5Cpropto+%5Cprod_%7Be+%5Cin+T%7D%5Clambda_e.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \Pr_{\cal T}(T) \propto \prod_{e \in T}\lambda_e. " class="latex" title="\displaystyle  \Pr_{\cal T}(T) \propto \prod_{e \in T}\lambda_e. " /></p>
<p>The third condition, however, follows the <a href="https://www.phrases.org.uk/meanings/proof-of-the-pudding.html">maxim</a>,</p>
<blockquote><p><b> </b> <em> “the proof of the pudding is in the eating.” </em>
</p></blockquote>
<p></p><p>
As our source makes clear, this does not refer to American-style dessert pudding, but rather savory British pub fare going back to 1605 at least. The point is that we ultimately know a choice of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" /> is good by proving it gives a better approximation factor than <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{3}{2}}" class="latex" title="{\frac{3}{2}}" />.</p>
<p>
In America, we tend to say the maxim a different way:</p>
<blockquote><p><b> </b> <em> “the proof is in the pudding.” </em>
</p></blockquote>
<p></p><p>
The new paper uses the “pudding” from the 2011 paper but needed to deepen the proof. Here is where we usually say to refer to the <a href="https://arxiv.org/abs/2007.01409">paper</a> for the considerable details. But in this case we find that a number of the beautiful concepts laid out in the paper’s introduction, such as <em>real stability</em> and <em>strong Rayleigh distributions</em>, are more accessibly described in the notes for the first half of a <a href="https://homes.cs.washington.edu/~shayan/courses/polynomials/">course</a> taught last spring by Oveis Gharan with Klein as TA. One nub is that if a set of complex numbers all have positive imaginary part, then any product of two of the numbers has a negative real contribution, and this rules out assignments drawn from the set from being solutions to certain polynomials as well as setting up odd/even parity properties elsewhere.</p>
<p>
</p><p></p><h2> Rigidity of the TSP Universe </h2><p></p>
<p></p><p>
I’ll close instead with some remarks while admitting that my own limited time—I have been <a href="https://www.cbc.ca/radio/asithappens/as-it-happens-tuesday-edition-1.5769434">dealing</a> with more chess <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">cases</a>—prevents them from being fully informed. </p>
<p>
The main remark is to marvel that the panoply of polynomial properties and deep analysis buy such a tiny improvement. It is hard to believe that the true space of TSP approximation methods is so <em>rigid</em>. In this I am reminded of Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=2651">calculations</a> that a collision of two stellar black holes a mere 3,000 miles away would stretch space near you by only a millimeter. There is considerable belief that the approximation factor ought to be improvable at least as far as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B4%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{4}{3}}" class="latex" title="{\frac{4}{3}}" />.</p>
<p>
It strikes me that the maximum-entropy condition, while facilitating the analysis, works against the objective of making the trees more special. It cannot come near the kind of snaky tree <img src="https://s0.wp.com/latex.php?latex=%7BT_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T_O}" class="latex" title="{T_O}" /> obtained by deleting any edge from a good tour <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O}" class="latex" title="{O}" />, such that plugging <img src="https://s0.wp.com/latex.php?latex=%7BT_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T_O}" class="latex" title="{T_O}" /> into step 1 yields <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O}" class="latex" title="{O}" /> back again. The theory of polynomials and distributions that they develop has a plug-and-play element, so that they can condition the distributions <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{{\cal T}}" class="latex" title="{{\cal T}}" /> toward the third objective using the parity properties. But their framework has inflexibility represented by needing to postulate a real-valued function on the optimum edges whose expectation is of order the <em>square</em> of a parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\eta}" class="latex" title="{\eta}" /> already given the tiny value <img src="https://s0.wp.com/latex.php?latex=%7B10%5E%7B-12%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{10^{-12}}" class="latex" title="{10^{-12}}" />. Of the requirement that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\eta}" class="latex" title="{\eta}" /> be a small fraction of their governing epsilon parameter, they say in section 3:</p>
<blockquote><p><b> </b> <em> This forces us to take <img src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\eta}" class="latex" title="{\eta}" /> very small, which is why we get only a “very slightly” improved approximation algorithm for TSP. Furthermore, since we use OPT edges in our construction, we don’t get a new upper bound on the integrality gap. We leave it as an open problem to find a reduction to the “cactus” case that doesn’t involve using a slack vector for OPT (or a completely different approach). </em>
</p></blockquote>
<p></p><p>
What may be wanting is a better way of getting the odd-valence tree nodes to be closer, not just fewer in number. To be sure, ideas for “closer” might wind up presupposing a metric topology on the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> given points, leading to cases that have already been improved by other means.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Will the tiny but fixed wedge below <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{3}{2}}" class="latex" title="{\frac{3}{2}}" /> become a lever by which to find better approximations?</p>
<p>
There is also the kvetch that the algorithm is randomized, whereas the original by Christofides and Serdyukov is deterministic. Can the new methods be derandomized?</p>
<p></p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/"><span class="datestr">at October 27, 2020 04:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12455">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12455">Primal-Dual Mesh Convolutional Neural Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milano:Francesco.html">Francesco Milano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Loquercio:Antonio.html">Antonio Loquercio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosinol:Antoni.html">Antoni Rosinol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scaramuzza:Davide.html">Davide Scaramuzza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlone:Luca.html">Luca Carlone</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12455">PDF</a><br /><b>Abstract: </b>Recent works in geometric deep learning have introduced neural networks that
allow performing inference tasks on three-dimensional geometric data by
defining convolution, and sometimes pooling, operations on triangle meshes.
These methods, however, either consider the input mesh as a graph, and do not
exploit specific geometric properties of meshes for feature aggregation and
downsampling, or are specialized for meshes, but rely on a rigid definition of
convolution that does not properly capture the local topology of the mesh. We
propose a method that combines the advantages of both types of approaches,
while addressing their limitations: we extend a primal-dual framework drawn
from the graph-neural-network literature to triangle meshes, and define
convolutions on two types of graphs constructed from an input mesh. Our method
takes features for both edges and faces of a 3D mesh as input and dynamically
aggregates them using an attention mechanism. At the same time, we introduce a
pooling operation with a precise geometric interpretation, that allows handling
variations in the mesh connectivity by clustering mesh faces in a task-driven
fashion. We provide theoretical insights of our approach using tools from the
mesh-simplification literature. In addition, we validate experimentally our
method in the tasks of shape classification and shape segmentation, where we
obtain comparable or superior performance to the state of the art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12455"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12227">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12227">Geometric Separability using Orthogonal Objects</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Abidha V P, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashok:Pradeesha.html">Pradeesha Ashok</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12227">PDF</a><br /><b>Abstract: </b>Given a bichromatic point set $P=\textbf{R}$ $ \cup$ $ \textbf{B}$ of red and
blue points, a separator is an object of a certain type that separates
$\textbf{R}$ and $\textbf{B}$. We study the geometric separability problem when
the separator is a) rectangular annulus of fixed orientation b) rectangular
annulus of arbitrary orientation c) square annulus of fixed orientation d)
orthogonal convex polygon. In this paper, we give polynomial time algorithms to
construct separators of each of the above type that also optimizes a given
parameter. Specifically, we give an $O(n^3 \log n)$ algorithm that computes
(non-uniform width) separating rectangular annulus in arbitrary orientation, of
minimum possible width. Further, when the orientation is fixed, we give an
$O(n\log n)$ algorithm that constructs a uniform width separating rectangular
annulus of minimum possible width and an $O(n\log^2 n)$ algorithm that
constructs a minimum width separating concentric square annulus. We also give
an optimal algorithm that computes a separating orthogonal convex polygon with
minimum number of edges, that runs in $O(n\log n)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12227"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11981">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11981">A novel auction system for selecting advertisements in Real-Time bidding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miralles=Pechu=aacute=n:Luis.html">Luis Miralles-Pechuán</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jim=eacute=nez:Fernando.html">Fernando Jiménez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garc=iacute=a:Jos=eacute=_Manuel.html">José Manuel García</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11981">PDF</a><br /><b>Abstract: </b>Real-Time Bidding is a new Internet advertising system that has become very
popular in recent years. This system works like a global auction where
advertisers bid to display their impressions in the publishers' ad slots. The
most popular system to select which advertiser wins each auction is the
Generalized second-price auction in which the advertiser that offers the most
wins the bet and is charged with the price of the second largest bet. In this
paper, we propose an alternative betting system with a new approach that not
only considers the economic aspect but also other relevant factors for the
functioning of the advertising system. The factors that we consider are, among
others, the benefit that can be given to each advertiser, the probability of
conversion from the advertisement, the probability that the visit is
fraudulent, how balanced are the networks participating in RTB and if the
advertisers are not paying over the market price. In addition, we propose a
methodology based on genetic algorithms to optimize the selection of each
advertiser. We also conducted some experiments to compare the performance of
the proposed model with the famous Generalized Second-Price method. We think
that this new approach, which considers more relevant aspects besides the
price, offers greater benefits for RTB networks in the medium and long-term.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11981"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/158">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/158">TR20-158 |  A Note on Hardness under Projections for Graph Isomorphism and Time-Bounded Kolmogorov Complexity | 

	Eric Allender, 

	Azucena Garvia Bosshard, 

	Amulya Musipatla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This paper focuses on a variant of the circuit minimization problem (MCSP), denoted MKTP, which studies resource-bounded Kolmogorov complexity in place of circuit size. MCSP is not known to be hard for any complexity class under any kind of m-reducibility, but recently MKTP was shown to be hard for DET under m-reductions computable in NC0, by presenting an AC0 reduction from the rigid graph isomorphism problem to MKTP, and combining that with a theorem of Toran, showing that DET AC0-reduces to the rigid graph isomorphism problem, and then appealing to the "Gap Theorem" of [Agrawal, Allender, Rudich]. Here, we show that these reductions can be accomplished by means of projections. Thus MKTP is hard for DET under projections, and the rigid graph isomorphism problem is hard for DET under uniform projections.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/158"><span class="datestr">at October 26, 2020 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/10/26/graphs-whose-cycles">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/10/26/graphs-whose-cycles.html">Graphs whose cycles all touch</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>An interesting recent question on MathOverflow asks about <a href="https://mathoverflow.net/q/374793/440">graphs in which all cycles touch</a>. Here, touching is meant in the same sense as a <a href="https://en.wikipedia.org/wiki/Bramble_(graph_theory)">bramble</a> in graph structure theory: every two cycles either share a vertex or contain the two endpoints of an edge from one cycle to the other. The graphs with this property include all the complete graphs (girth 3), complete bipartite graphs (girth 4), and theta graphs (arbitrarily high girth but very simple structure). As originally phrased, it asked whether there exists \(g\) such that graphs of girth \(\ge g\) with all cycles touching have bounded treewidth. Partial results given there by Tony Huynh and me show that the condition of bounded treewidth can be replaced by bounded vertex cover number or a bounded number of vertex-disjoint cycles without changing the answer.</p>

<p>This led me to look for graphs that have high girth, all cycles touching, and as many vertex-disjoint cycles as I could construct. So far, the best I have found is four vertex-disjoint cycles, as shown in graphs of the following form:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/4-disjoint-touching-cycles.svg" alt="A graph with four vertex-disjoint long cycles, and all cycles touching" /></p>

<p>It consists of four theta graphs (the pairs of blue vertices connected by multiple long paths of yellow vertices, with the eight blue pole vertices of the theta graphs connected into two four-vertex paths. I’ve drawn it with yellow paths of length 16, and three paths per theta, but these numbers are arbitrary. One can easily find four vertex-disjoint cycles, within each of the four thetas, ignoring the edges between the pole vertices.</p>

<p>There is no cycle using only the blue pole vertices, so every cycle in the overall graph must include at least one complete yellow path connecting its two poles. Therefore, every cycle is at least as long as this yellow path length. These paths can be made arbitrarily long, so the graphs constructed in this way can have arbitrarily large girth.</p>

<p>The six edges of the two four-vertex paths between the pole vertices include an edge between each of the six pairs of pole vertices. But each cycle uses at least one pair of pole vertices, so this implies that every two cycles touch, either by sharing a pole vertex or by each containing one endpoint of one of these path edges.</p>

<p>Therefore the graphs constructed in this way have arbitrarily large girth, have all cycles touching, and contain four vertex-disjoint cycles. It also has feedback vertex number four. The MathOverflow question asks whether the four vertex-disjoint cycles can be replaced by an arbitrarily large number of cycles, or equivalently whether the feedback vertex number can be increased, but at this point I don’t even know whether either number can be replaced by five.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105103937279022043">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/10/26/graphs-whose-cycles.html"><span class="datestr">at October 26, 2020 04:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12397">Quickly excluding a non-planar graph</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawarabayashi:Ken=ichi.html">Ken-ichi Kawarabayashi</a>, Robin Thomas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wollan:Paul.html">Paul Wollan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12397">PDF</a><br /><b>Abstract: </b>A cornerstone theorem in the Graph Minors series of Robertson and Seymour is
the result that every graph $G$ with no minor isomorphic to a fixed graph $H$
has a certain structure. The structure can then be exploited to deduce
far-reaching consequences. The exact statement requires some explanation, but
roughly it says that there exist integers $k,n$ depending on $H$ only such that
$0&lt;k&lt;n$ and for every $n\times n$ grid minor $J$ of $G$ the graph $G$ has a a
$k$-near embedding in a surface $\Sigma$ that does not embed $H$ in such a way
that a substantial part of $J$ is embedded in $\Sigma$. Here a $k$-near
embedding means that after deleting at most $k$ vertices the graph can be drawn
in $\Sigma$ without crossings, except for local areas of non-planarity, where
crossings are permitted, but at most $k$ of these areas are attached to the
rest of the graph by four or more vertices and inside those the graph is
constrained in a different way, again depending on the parameter $k$.
</p>
<p>The original and only proof so far is quite long and uses many results
developed in the Graph Minors series. We give a proof that uses only our
earlier paper [A new proof of the flat wall theorem, {\it J.~Combin.\ Theory
Ser.\ B \bf 129} (2018), 158--203] and results from graduate textbooks.
</p>
<p>Our proof is constructive and yields a polynomial time algorithm to construct
such a structure. We also give explicit constants for the structure theorem,
whereas the original proof only guarantees the existence of such constants.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12397"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12265">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12265">Model Interpretability through the Lens of Computational Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barcel=oacute=:Pablo.html">Pablo Barceló</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Monet:Mika=euml=l.html">Mikaël Monet</a>, Jorge Pérez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subercaseaux:Bernardo.html">Bernardo Subercaseaux</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12265">PDF</a><br /><b>Abstract: </b>In spite of several claims stating that some models are more interpretable
than others -- e.g., "linear models are more interpretable than deep neural
networks" -- we still lack a principled notion of interpretability to formally
compare among different classes of models. We make a step towards such a notion
by studying whether folklore interpretability claims have a correlate in terms
of computational complexity theory. We focus on local post-hoc explainability
queries that, intuitively, attempt to answer why individual inputs are
classified in a certain way by a given model. In a nutshell, we say that a
class $\mathcal{C}_1$ of models is more interpretable than another class
$\mathcal{C}_2$, if the computational complexity of answering post-hoc queries
for models in $\mathcal{C}_2$ is higher than for those in $\mathcal{C}_1$. We
prove that this notion provides a good theoretical counterpart to current
beliefs on the interpretability of models; in particular, we show that under
our definition and assuming standard complexity-theoretical assumptions (such
as P$\neq$NP), both linear and tree-based models are strictly more
interpretable than neural networks. Our complexity analysis, however, does not
provide a clear-cut difference between linear and tree-based models, as we
obtain different results depending on the particular post-hoc explanations
considered. Finally, by applying a finer complexity analysis based on
parameterized complexity, we are able to prove a theoretical result suggesting
that shallow neural networks are more interpretable than deeper ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12265"><span class="datestr">at October 26, 2020 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12122">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12122">Quantum Meets Fine-grained Complexity: Sublinear Time Quantum Algorithms for String Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gall:Fran=ccedil=ois_Le.html">François Le Gall</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seddighin:Saeed.html">Saeed Seddighin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12122">PDF</a><br /><b>Abstract: </b>Longest common substring (LCS), longest palindrome substring (LPS), and Ulam
distance (UL) are three fundamental string problems that can be classically
solved in near linear time. In this work, we present sublinear time quantum
algorithms for these problems along with quantum lower bounds. Our results shed
light on a very surprising fact: Although the classic solutions for LCS and LPS
are almost identical (via suffix trees), their quantum computational
complexities are different. While we give an exact $\tilde O(\sqrt{n})$ time
algorithm for LPS, we prove that LCS needs at least time $\tilde
\Omega(n^{2/3})$ even for 0/1 strings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12122"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12081">Codes over integers, and the singularity of random matrices with large entries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karingula:Sankeerth_Rao.html">Sankeerth Rao Karingula</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12081">PDF</a><br /><b>Abstract: </b>The prototypical construction of error correcting codes is based on linear
codes over finite fields. In this work, we make first steps in the study of
codes defined over integers. We focus on Maximum Distance Separable (MDS)
codes, and show that MDS codes with linear rate and distance can be realized
over the integers with a constant alphabet size. This is in contrast to the
situation over finite fields, where a linear size finite field is needed.
</p>
<p>The core of this paper is a new result on the singularity probability of
random matrices. We show that for a random $n \times n$ matrix with entries
chosen independently from the range $\{-m,\ldots,m\}$, the probability that it
is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12081"><span class="datestr">at October 26, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.12000">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.12000">Computationally and Statistically Efficient Truncated Regression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daskalakis:Constantinos.html">Constantinos Daskalakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gouleakis:Themis.html">Themis Gouleakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzamos:Christos.html">Christos Tzamos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.12000">PDF</a><br /><b>Abstract: </b>We provide a computationally and statistically efficient estimator for the
classical problem of truncated linear regression, where the dependent variable
$y = w^T x + \epsilon$ and its corresponding vector of covariates $x \in R^k$
are only revealed if the dependent variable falls in some subset $S \subseteq
R$; otherwise the existence of the pair $(x, y)$ is hidden. This problem has
remained a challenge since the early works of [Tobin 1958, Amemiya 1973,
Hausman and Wise 1977], its applications are abundant, and its history dates
back even further to the work of Galton, Pearson, Lee, and Fisher. While
consistent estimators of the regression coefficients have been identified, the
error rates are not well-understood, especially in high dimensions.
</p>
<p>Under a thickness assumption about the covariance matrix of the covariates in
the revealed sample, we provide a computationally efficient estimator for the
coefficient vector $w$ from $n$ revealed samples that attains $l_2$ error
$\tilde{O}(\sqrt{k/n})$. Our estimator uses Projected Stochastic Gradient
Descent (PSGD) without replacement on the negative log-likelihood of the
truncated sample. For the statistically efficient estimation we only need
oracle access to the set $S$.In order to achieve computational efficiency we
need to assume that $S$ is a union of a finite number of intervals but still
can be complicated. PSGD without replacement must be restricted to an
appropriately defined convex cone to guarantee that the negative log-likelihood
is strongly convex, which in turn is established using concentration of
matrices on variables with sub-exponential tails. We perform experiments on
simulated data to illustrate the accuracy of our estimator.
</p>
<p>As a corollary, we show that SGD learns the parameters of single-layer neural
networks with noisy activation functions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.12000"><span class="datestr">at October 26, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2010.11983">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2010.11983">Learnability and Complexity of Quantum Samples</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niu:Murphy_Yuezhen.html">Murphy Yuezhen Niu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Andrew_M=.html">Andrew M. Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Li.html">Li Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Odena:Augustus.html">Augustus Odena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Zhengli.html">Zhengli Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smelyanskyi:Vadim.html">Vadim Smelyanskyi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neven:Hartmut.html">Hartmut Neven</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boixo:Sergio.html">Sergio Boixo</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11983">PDF</a><br /><b>Abstract: </b>Given a quantum circuit, a quantum computer can sample the output
distribution exponentially faster in the number of bits than classical
computers. A similar exponential separation has yet to be established in
generative models through quantum sample learning: given samples from an
n-qubit computation, can we learn the underlying quantum distribution using
models with training parameters that scale polynomial in n under a fixed
training time? We study four kinds of generative models: Deep Boltzmann machine
(DBM), Generative Adversarial Networks (GANs), Long Short-Term Memory (LSTM)
and Autoregressive GAN, on learning quantum data set generated by deep random
circuits. We demonstrate the leading performance of LSTM in learning quantum
samples, and thus the autoregressive structure present in the underlying
quantum distribution from random quantum circuits. Both numerical experiments
and a theoretical proof in the case of the DBM show exponentially growing
complexity of learning-agent parameters required for achieving a fixed accuracy
as n increases. Finally, we establish a connection between learnability and the
complexity of generative models by benchmarking learnability against different
sets of samples drawn from probability distributions of variable degrees of
complexities in their quantum and classical representations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2010.11983"><span class="datestr">at October 26, 2020 11:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-9110359134467856278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html">Do not become obsessed with the Polls unless...</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I know someone who checks the polls 3 times a day to see who looks like they will be elected prez. She cares A LOT about the election. It is irrelevant to this post who she supports. <div><br /></div><div>I've asked her `if you find out that, say, Biden is up  8 points instead of 9 in Penn. or that Georgia is looking pretty safe for Trump, or that Texas is in play (really?) how will that change your life? What will YOU do differently?'</div><div><br /></div><div>She had no answer. Unfortunately she is still a poll-watcher (I know that means something else usually, but you know what I mean.)</div><div><br /></div><div>So who should be poll-watching or poll-obsessing?</div><div><br /></div><div>1) To be fair to my friend, she might decide to GIVE MORE to her candidate if the polls are saying that he will lose (whoops- by saying `he' I gave away that her candidate is NOT Jo Jorgenson- Libertarian).  I doubt my friend could say to the campaign `I want to you to spend it in state X since I read that its close there' (I read that some big donors in 2016 demanded more say in where the money was spend. I doubt that's a good idea since I suspect the party knows more about how to best spend the money then the donor does.) </div><div><br /></div><div>2) The Biden and the Trump Campaigns SHOULD be poll-watching to decide where to put their efforts. And I suspect they are doing just that.</div><div><br /></div><div>3) A really big donor (my friend is not one of those) MIGHT want to poll watch to decide if the candidate they want needs money. (I wonder if EITHER candidate needs money since they get so much free media.)</div><div><br /></div><div>4) Nate Silver-being a poll-watcher is kind-of his job. And of course writing columns about them and making predictions based on what he sees. My friend is not Nate Silver. </div><div><br /></div><div>5) Other people who have Nate Silver's job. I can't name any- is Nate Silver the most famous... Gee, not sure what job title he has... SO this is now two questions: What is his job title, call it X, and is he the most famous person who does X?</div><div><br /></div><div><br /></div><div>SO- my point- DO NOT be a poll-obsessive unless the information you get will lead to an action you can take. And I suspect that mostly it does not. </div><div><br /></div><div>The primaries are different: If a poll says A can beat X but B cannot beat X, that might guide who you vote for. </div><div><br /></div><div>Misc thought: </div><div><br /></div><div> I've heard the phrase `democratic pollster' and `republican pollster' These terms do not make sense. Would I call myself a `democratic Muffin Mathematician' ? My political leanings do not affect my search for truth about mathematical Muffins. Similarly, one would think that a pollster wants to find the TRUTH, even if its bad news for their employer, ESPECIALLY if its bad news for their employer, so they can help their employer fix it. The phrase `pollster employed by the X party' would make more sense-- however, whenever they are on TV they seem to always say that their candidate is doing well, even when they are not. </div><div><br /></div><div>ADDED LATER: Lance had a great tweet about this post: <i>do not obsess about polls, but DO obsess bout prediction markets. </i>I think in the past prediction markets have been better predictors but some group-think has set in so its no longer clear. (I could be wrong- but thats why I have heard.) </div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html"><span class="datestr">at October 25, 2020 08:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/157">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/157">TR20-157 |  Batch Verification and Proofs of Proximity with Polylog Overhead | 

	Guy Rothblum, 

	Ron Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose Alice wants to convince Bob of the correctness of k NP statements. Alice could send k witnesses to Bob, but as k grows the communication becomes prohibitive. Is it possible to convince Bob using smaller communication (without making cryptographic assumptions or bounding the computational power of a malicious Alice)? This is the question of batch verification for NP statements.

Our main result is a new interactive proof protocol for verifying the correctness of k UP statements (NP statements with a unique witness) using communication that is poly-logarithmic in k (and a fixed polynomial in the length of a single witness).

This result is obtained by making progress on a different question in the study of interactive proofs. Suppose Alice wants to convince Bob that a huge dataset has some property. Can this be done if Bob can't even read the entire input? In other words, what properties can be verified in sublinear time? An Interactive Proof of Proximity guarantees that Bob accepts if the input has the property, and rejects if the input is far (say in Hamming distance) from having the property. Two central complexity measures of such a protocol are the query and communication complexities (which should both be sublinear). For every query parameter $q$, and for every language in logspace uniform NC, we construct an interactive proof of proximity with query complexity $q$ and communication complexity $(n/q) \cdot \polylog(n)$.

Both results are optimal up to poly-logarithmic factors, under reasonable complexity-theoretic or cryptographic assumptions. The second result, which is our main technical contribution, builds on a distance amplification technique introduced in a beautiful recent work of Ben-Sasson, Kopparty and Saraf [CCC 2018].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/157"><span class="datestr">at October 25, 2020 10:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/">Tenure-Track/Tenured position at University of California, Davis (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of California, Davis, seeks an Assistant Professor in any area of theoretical computer science; exceptional candidates at the Associate and Full ranks may be considered.</p>
<p>Website: <a href="https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic">https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic</a><br />
Email: amenta@cs.ucdavis.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/"><span class="datestr">at October 24, 2020 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/">Simons-Berkeley Research Fellowships for Fall 2021 and Spring 2022 at The Simons Institute for the Theory of Computing (apply by December 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Simons Institute invites applications for Simons-Berkeley Postdoctoral Fellowships and Simons-Berkeley Research Fellowships, to collaborate with UC Berkeley faculty and to participate in the semester-long programs in Fall 2021 and Spring 2022: “Computational Complexity of Statistical Inference”, “Geometric Methods in Optimization and Sampling”, “Causality”, and “Learning and Games”.</p>
<p>Website: <a href="https://simons.berkeley.edu">https://simons.berkeley.edu</a><br />
Email: simonsvisitorservices@berkeley.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/"><span class="datestr">at October 24, 2020 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17707">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/">Can We Solve It?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>It is a Friday</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/maynard/" rel="attachment wp-att-17709"><img src="https://rjlipton.files.wordpress.com/2020/10/maynard.jpg?w=600" alt="" class="alignright size-full wp-image-17709" /></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
James Maynard is a number theorist. He attended Cambridge as an undergrad and then moved to do his grad work at Oxford at Balliol College. He is now a professor at Oxford. He is one of the world experts on prime density type theorems. </p>
<p></p><p>
Today, since it is Friday, I thought we would discuss a <em>timely</em> idea of Maynard. Not an idea about time complexity or time in physics, but involving the use of time.<br />
<span id="more-17707"></span></p>
<p>
</p><p></p><h2> Decimal Digits </h2><p></p>
<p></p><p>
No it’s not a technical idea of his. He has had many ideas, for instance, that shed light on the beautiful structure of primes. For example, he <a href="https://arxiv.org/abs/1604.01041">proved</a> in 2016 that</p>
<blockquote><p><b>Theorem 1</b> <em> For each decimal digit <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" />, there are infinitely many prime numbers that do not have <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" title="{d}" /> in their decimal expansion. </em>
</p></blockquote>
<p></p><p>
This is not known for all digit systems: For binary, our favorite system as complexity theorists, this is still an open problem. Of course a binary prime with only <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" title="{1}" />‘s must be of the form: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Bp%7D+-+1+%3D+111%5Cdots+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  2^{p} - 1 = 111\dots 1, " class="latex" title="\displaystyle  2^{p} - 1 = 111\dots 1, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> must be a prime.</p>
<p>These are the famous <a href="https://en.wikipedia.org/wiki/Mersenne_prime">Mersenne primes</a> named for Marin Mersenne. The largest prime is <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B82%2C589%2C933%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{82,589,933} - 1}" class="latex" title="{2^{82,589,933} - 1}" /> as of today—at least I believe this is true. For further discussion, see a 2001 <a href="https://projecteuclid.org/euclid.em/999188636">paper</a> by Samuel Wagstaff titled, “Prime Numbers with a Fixed Number of One Bits or Zero Bits in Their Binary Representation.” </p>
<p></p><h2> Maynard’s Friday Rule </h2><p></p>
<p></p><p>
Maynard’s idea is based on his quest to understand whether known techniques can solve some problem. Of course the best way to understand this is to solve the problem. His above theorem is a perfect example of this. In the abstract he says: </p>
<blockquote><p><b> </b> <em> The proof is an application of the Hardy-Littlewood circle method to a binary problem, and rests on obtaining suitable `Type I’ and `Type II’ arithmetic information for use in Harman’s sieve to control the minor arcs. </em>
</p></blockquote>
<p>The proof may be based on known techniques, but is still very hard. He needs <img src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{70}" class="latex" title="{70}" /> pages to make it work. </p>
<p>
Maynard’s idea is to set aside time to remind himself why existing techniques have not worked against math’s biggest open problems.</p>
<blockquote><p><b> </b> <em> I often spend Friday afternoons just thinking about trying to directly attack some famous problem. This is much less because I think there’s a realistic way of solving the problem, but more because I think it’s important for me to understand where plausible techniques fail. </em>
</p></blockquote>
<p></p><p>
One can imagine that he had a Friday afternoon think. During it he asked himself:</p>
<blockquote><p><b> </b> <em> Suppose I try to show that there are primes without some particular digit. This is a density type theorem. Well could I use the Hardy-Littlewood method. But it cannot work because <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /> Wait here is a possible way around the roadblock. Hmmm. </em>
</p></blockquote>
<p></p><p>
Maybe he looked at Terence Tao’s blog post on this very <a href="https://terrytao.wordpress.com/2012/05/20/heuristic-limitations-of-the-circle-method/">issue</a>. It helped that Maynard is an expert on the Hardy-Littlewood method, but perhaps thinking why it could not work helped him figure out how it could work. </p>
<p>
</p><p></p><h2> Our Friday Rule </h2><p></p>
<p></p><p>
Today is Friday, so I though what should I think about? What problems and what techniques? Here is a possible example. Let’s look at the <a href="https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/">On Lower Bounds for the Separating Word Problem</a>. </p>
<p>
An approach is based on the following. Let <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F(n)}" class="latex" title="{F(n)}" /> be the set of all <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(x)}" class="latex" title="{f(x)}" /> degree <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> polynomials, with coefficients <img src="https://s0.wp.com/latex.php?latex=%7B-1%2C0%2C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1,0,+1}" class="latex" title="{-1,0,+1}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> be a constant. Our hypothesis <strong>H</strong><img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" /> is: For every polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(x)}" class="latex" title="{f(x)}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F(n)}" class="latex" title="{F(n)}" />, there is some prime <img src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cle+Cn%5E%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p \le Cn^{\epsilon}}" class="latex" title="{p \le Cn^{\epsilon}}" />, so that for some <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1 \le k \le n}" class="latex" title="{1 \le k \le n}" /> 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%29+%5Cnot%5Cequiv+0+%5Cbmod+p.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(k) \not\equiv 0 \bmod p. " class="latex" title="\displaystyle  f(k) \not\equiv 0 \bmod p. " /></p>
<p>How small can <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" title="{\epsilon}" /> be so that <strong>H</strong><img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" /> is true? What are the methods that we should think about? What methods can we see that cannot prove H<img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(\epsilon)}" class="latex" title="{(\epsilon)}" />? Can we, for example, show that we can use a random argument? Can we should that they are not enough primes <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> in the range? Hmmm <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Do you like Maynard’s Friday rule? What problems and what techniques would you think about? </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/"><span class="datestr">at October 23, 2020 07:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/">3 postdocs at University of Lyon (apply by January 6, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Excellence Laboratory Milyon (Labex Milyon) opens the 2021 campaign of postdoctoral researchers in Lyon–Saint-Etienne in Mathematics, Computer Science and their interactions (including some aspects of theoretical physics).</p>
<p>Milyon offers three postdoctoral positions of two years with no teaching load for 2021–2023. The application is open to all research areas of labex Milyon.</p>
<p>Website: <a href="https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023--130160.kjsp?RH=1571748911317">https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023–130160.kjsp?RH=1571748911317</a><br />
Email: sabot@math.univ-lyon1.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/"><span class="datestr">at October 23, 2020 07:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7831">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/">Full-replica-symmetry-breaking based algorithms for dummies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.<br />`<br />This blog post is mainly a note to myself (i.e., I’m the “dummy” <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png" style="height: 1em;" class="wp-smiley" alt="😃" />), trying to work out some basic facts in some of this line of work. it was inspired by this <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">excellent talk of Eliran Subag</a>, itself part of a great <a href="https://simons.berkeley.edu/workshops/schedule/14243">Simons institute workshop</a> which I am still planning to watch the talks of. I am posting this in case it’s useful for others, but this is quite rough, missing many references, and I imagine I have both math mistakes as well as inaccuracies in how I refer to the literature – would be grateful for comments!</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw.png"><img src="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw-e1603479127948.png" alt="" class="wp-image-7833" /></a>Screen shot from <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">Eliran Subag’s talk</a> demonstrating the difference between “easy” and “hard” instances.</figure>



<p>In computer science, <em>optimization</em> is the task of finding an assignment <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> that minimizes some function <img src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x_1,\ldots,x_n)" class="latex" title="J(x_1,\ldots,x_n)" />. In statistical physics we think of <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> as the states of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> particles, and <img src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x_1,\ldots,x_n)" class="latex" title="J(x_1,\ldots,x_n)" /> as the <em>energy</em> of this state. Finding the minimum assignment corresponds to finding the <em>ground state</em>, and another computational problem is sampling from the <em>Gibbs distribution</em> where the probability of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is proportional to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+J%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-\beta J(x))" class="latex" title="\exp(-\beta J(x))" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta&gt;0" class="latex" title="\beta&gt;0" />.</p>



<p>Two prototypical examples of such problems are:</p>



<ol><li>Random 3SAT – in this case <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in { \pm 1 }^n" class="latex" title="x\in { \pm 1 }^n" /> and <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is the number of clauses violated by the assignment <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> for a random formula.</li><li>Sherrington-Kirpatrick model – in this case <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in { \pm 1 }^n" class="latex" title="x \in { \pm 1 }^n" /> and <img src="https://s0.wp.com/latex.php?latex=J%28x%29%3D+%5Csum_%7Bi%2Cj%7D+J_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)= \sum_{i,j} J_{i,j}x_ix_j" class="latex" title="J(x)= \sum_{i,j} J_{i,j}x_ix_j" /> where <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_{i,j}" class="latex" title="J_{i,j}" /> are independent normal variables with variance <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/n" class="latex" title="1/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" /> and variance <img src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2/n" class="latex" title="2/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />. (Another way to say it is that <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> is the matrix <img src="https://s0.wp.com/latex.php?latex=A%2BA%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A+A^\top" class="latex" title="A+A^\top" /> where <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />‘s entries are chosen i.i.d from <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{2n}))" class="latex" title="N(0,\tfrac{1}{2n}))" />.)</li></ol>



<p>The physics and CS intuition is that these two problems have very different computational properties. For random 3SAT (of the appropriate density), it is believed that the set of solutions is “shattered” in the sense that it is partitioned to exponentially many clusters, separated from one another by large distance. It is conjectured that in this setting the problem will be computationally hard. Similarly from the statistical physics point of view, it is conjectured that if we were to start with the uniform distribution (i.e., a “hot” system) and “lower the temperature” (increase <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta" class="latex" title="\beta" />) at a rate that is not exponentially slow then we will get “stuck” at a “metastable” state. This is analogous how when we heat up sand and then cool it quickly then rather than returning to its original state, the sand will get stuck at the metastable state of glass.</p>



<p>In contrast for the Sherrington-Kirpatrick (SK) model, the geometry is more subtle, but interestingly this enables better algorithms. The SK model is extermely widely studied, with hundreds of papers, and was the inspiration for the simulated annealing algorithm. If memory serves me right, Sherrington and Kirpatrick made the wrong conjecture on the energy of the ground state, and then Parisi came up in 1979 with a wonderful and hugely influential way to compute this value. Parisi’s calculation was heuristic, but about 30 years later, first Talagrand and later Panchenko proved rigorously many of Parisi’s conjectures. (See this <a href="https://arxiv.org/abs/1211.1094v1">survey of Panchenko</a>.)</p>



<p>Recently <a href="https://arxiv.org/abs/1812.10897">Montanari</a> gave a polynomial time algorithm to find a state with energy that is arbitrarily close to the ground state’s. The algorithm relies on Parisi’s framework and in particular on the fact that the solution space has a property known as “full replica symmetry breaking (RSB)” / “ultrametricity”. Parisi’s derivations (and hence also Montanari’s analysis) are highly elaborate and I admit that I have not yet been able to fully follow it. The nice thing is that (as we’ll see) it is possible to describe at least some of the algorithmic results without going into this theory. In the end of the post I will discuss a bit some of the relation to this theory, which is the underlying inspiration for Subag’s results described here.</p>



<p><strong>Note:</strong> These papers and this blog post deal with the <em>search problem</em> of finding a solution that minimizes the objective. The <em>refutation problem</em> of certifying that this minimum is at least <img src="https://s0.wp.com/latex.php?latex=-C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C" class="latex" title="-C" /> for some <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" title="C" /> has often been studied. The computational complexity of these problems need not be identical. In particular there are cases where the search problem has an efficient algorithm achieving value <img src="https://s0.wp.com/latex.php?latex=-C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C^*" class="latex" title="-C^*" /> but the best refutation algorithm can only certify that the value is at most <img src="https://s0.wp.com/latex.php?latex=-C%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-C'" class="latex" title="-C'" /> for <img src="https://s0.wp.com/latex.php?latex=C%27+%5Cgg+C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C' \gg C^*" class="latex" title="C' \gg C^*" />.</p>



<h2>Analysis of a simpler setting</h2>



<p>Luckily, there is a similar computational problem, for which the analysis of analogous algorithm, which was <a href="https://arxiv.org/abs/1812.04588">discovered by Subag</a> and was the partial inspiration for Montanari’s work, is much simpler. Specifically, we consider the case where <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is an element of the unit sphere, and <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is a degree <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> polynomial with random Gaussian coefficients. Specifically, for every vector <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%28%5Cgamma_2%2C%5Cldots%2C%5Cgamma_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (\gamma_2,\ldots,\gamma_d)" class="latex" title="\gamma = (\gamma_2,\ldots,\gamma_d)" />, we let <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" /> where for every <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p \in {2,\ldots, d }" class="latex" title="p \in {2,\ldots, d }" />, <img src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_p" class="latex" title="J_p" /> is a random tensor of order <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> whose <img src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^p" class="latex" title="n^p" /> coefficients are all chosen i.i.d in <img src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,1/n)" class="latex" title="N(0,1/n)" />. (We assume that polynomial does not have constant or linear components.)</p>



<p>Depending on <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma" class="latex" title="\gamma" />, the computational and geometrical properties of this problem can vary considerably. The case that <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (1,0,\ldots,0)" class="latex" title="\gamma = (1,0,\ldots,0)" /> (i.e., only <img src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^2" class="latex" title="J^2" /> has a non-zero coeffiecent) corresponds to finding the unit vector <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> minimizing <img src="https://s0.wp.com/latex.php?latex=x%5E%5Ctop+J+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\top J x" class="latex" title="x^\top J x" /> for a random matrix <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, which of course corresponds to the efficiently solveable minimum eigenvector problem. In contrast, the case <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C1%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (0,1,0,\ldots,0)" class="latex" title="\gamma = (0,1,0,\ldots,0)" /> corresponds to finding a rank one component of a random three-tensor, which is believed to be computationally difficult. The Parisi calculations give a precise condition <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> on the vector <img src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma" class="latex" title="\gamma" /> such that if <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds then the solution space has the “full RSB” property (and hence the problem is computationally easy) and if <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> does not hold then the solution space does not have this property (and potentially the problem is hard).</p>



<p>These calculations also give rise to the following theorem:</p>



<p><strong>Theorem (<a href="https://arxiv.org/abs/1512.08492">Chen and Sen, Proposition 2</a>):</strong> If <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds then in the limit <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow \infty" class="latex" title="n \rightarrow \infty" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%3A+%7Cx%7C%3D1%7D+J%28x%29+%3D+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" class="latex" title="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29+%3D+%5Csum_%7Bp+%5Cgeq+2%7D+%5Cgamma_p%5E2+q%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" class="latex" title="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" />. (That is, <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''" class="latex" title="\nu''" /> is the second derivative of this univariate polynomial)</p>



<p>We will not discuss the proof of this theorem, but rather how, taking it as a black box, it leads to an algorithm for minimizing <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> that achieves a near-optimal value (assuming <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> holds).</p>



<p>It is a nice exercise to show that for every two vectors <img src="https://s0.wp.com/latex.php?latex=x%2Cx%27%5Cin%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x,x'\in\mathbb{R}^n" class="latex" title="x,x'\in\mathbb{R}^n" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_J+%5BJ%28x%29J%28x%27%29%5D+%3D+%5Cnu%28%5Clangle+x%2Cx%27+%5Crangle%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" class="latex" title="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" />. Hence for any unit vector <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> is a random variable with mean zero and standard deviation <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%281%29%2Fn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\nu(1)/n}" class="latex" title="\sqrt{\nu(1)/n}" />. Since (after some coarsening) the number of unit vectors of dimension <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> can be thought of as <img src="https://s0.wp.com/latex.php?latex=c%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c^n" class="latex" title="c^n" /> for some <img src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c&gt;1" class="latex" title="c&gt;1" />, and we expect the probability of deviating <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" title="t" /> standard deviations to be <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c%27+t%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-c' t^2)" class="latex" title="\exp(-c' t^2)" />, the minimum value of <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /> should be <img src="https://s0.wp.com/latex.php?latex=-c%27%27+%5Csqrt%7B%5Cnu%281%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-c'' \sqrt{\nu(1)}" class="latex" title="-c'' \sqrt{\nu(1)}" /> for some constant <img src="https://s0.wp.com/latex.php?latex=c%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c''" class="latex" title="c''" />. However determining this constant is non trivial and is the result of the Parisi theory.</p>



<p>To get a better sense for the quantity <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq" />, let’s consider two simple cases:</p>



<ul><li>If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (1,0,\ldots)" class="latex" title="\gamma = (1,0,\ldots)" /> (i.e., <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Csum_%7Bi%2Cj%7DM_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \sum_{i,j}M_{i,j}x_ix_j" class="latex" title="J(x) = \sum_{i,j}M_{i,j}x_ix_j" /> for random matrix <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" />) then <img src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29%3D+q%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu(q)= q^2" class="latex" title="\nu(q)= q^2" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28q%29+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''(q) = 2" class="latex" title="\nu''(q) = 2" />, meaning that <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-%5Csqrt%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" />. This turns out to be the actual minimum value. Indeed in this case <img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7B%7Cx%7C%5E2%3D1%7D+J%28x%29+%3D+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda_%7Bmin%7D%28M+%2B+M%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" class="latex" title="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" />. But the matrix <img src="https://s0.wp.com/latex.php?latex=A%3D%5Ctfrac%7B1%7D%7B2%7D%28M%2BM%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A=\tfrac{1}{2}(M+M^\top)" class="latex" title="A=\tfrac{1}{2}(M+M^\top)" />‘s non diagonal entries are distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{2n})" class="latex" title="N(0,\tfrac{1}{2n})" /> and the diagonal entries like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{1}{n})" class="latex" title="N(0,\tfrac{1}{n})" /> which means that <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> is distributed as <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{1}{\sqrt{2}}" class="latex" title="\tfrac{1}{\sqrt{2}}" /> times a random matrix <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> from the <em>Gaussian Orthogonal Ensemble (GOE)</em> where <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B_{i,j} \sim N(0,1/n)" class="latex" title="B_{i,j} \sim N(0,1/n)" /> for off diagonal entries and <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B_{i,i} \sim N(0,2/n)" class="latex" title="B_{i,i} \sim N(0,2/n)" /> for diagonal entries. The minimum eigenvalue of such matrices is known to be <img src="https://s0.wp.com/latex.php?latex=-2%5Cpm+o%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-2\pm o(1)" class="latex" title="-2\pm o(1)" /> with high probability.<br /></li><li>If <img src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C%5Cldots%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma = (0,\ldots,1)" class="latex" title="\gamma = (0,\ldots,1)" /> (i.e. <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+T+%5Ccdot+x%5E%7B%5Cotimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = T \cdot x^{\otimes d}" class="latex" title="J(x) = T \cdot x^{\otimes d}" /> for a random <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />-tensor <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" />) then <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> does not hold. Indeed, in this case the value <img src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-+%5Cint_0%5E1+%5Csqrt%7Bd+%28d-1%29q%5E%7Bd-2%7D%7D+dq+%3D+-+%5Csqrt%7Bd%28d-1%29%7D%5Ctfrac%7B1%7D%7Bd%2F2-1%7D+%5Capprox+-2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" class="latex" title="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" /> for large <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />. However I believe (though didn’t find the reference) that the actual minimum tends to <img src="https://s0.wp.com/latex.php?latex=-%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\infty" class="latex" title="-\infty" /> with <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />. </li></ul>



<p>While the particular form of the property <img src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P(\gamma)" class="latex" title="P(\gamma)" /> is not important for this post, there are several equivalent ways to state it, see Proposition 1 in <a href="https://arxiv.org/abs/1812.04588">Subag’s paper</a>. One of them is that the function <img src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28t%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nu''(t)^{-1/2}" class="latex" title="\nu''(t)^{-1/2}" /> (note the negative exponent) is concave on the interval <img src="https://s0.wp.com/latex.php?latex=%280%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(0,1]" class="latex" title="(0,1]" />.<br />It can be shown that this condition cannot be satisfied if <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 = 0" class="latex" title="\gamma_2 = 0" />, and that for every setting of <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3%2C%5Cldots%2C%5Cgamma_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3,\ldots,\gamma_d" class="latex" title="\gamma_3,\ldots,\gamma_d" />, if <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2" class="latex" title="\gamma_2" /> is large enough then it will be satisfied.</p>



<p>The central result of Subag’s paper is the following:</p>



<p><strong>Theorem:</strong> For every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" />, there is a polynomial-time algorithm <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> such that on input random <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> chosen according to the distribution above, with high probability <img src="https://s0.wp.com/latex.php?latex=A%28J%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(J)=x" class="latex" title="A(J)=x" /> such that <img src="https://s0.wp.com/latex.php?latex=J%28x%29+%5Cleq+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" class="latex" title="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" />.</p>



<p>The algorithm itself, and the idea behind the analysis are quite simple. In some sense it’s the second algorithm you would think of (or at least the second algorithm according to some ordering).</p>



<p>The first algorithm one would think of is gradient descent. We start at some initial point <img src="https://s0.wp.com/latex.php?latex=x%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0" class="latex" title="x^0" />, and repeat the transformation <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+%5Cnabla+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" class="latex" title="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" /> for some small <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> (and normalizing the norm). Unfortunately, we will generally run into <em>saddle points</em> when we do so, with the gradient being zero. In fact, for simplicity, below we will always make the pessimistic assumption that we are constantly on a saddle point. (This assumption turns out to be true in the actual algorithm, and if it was not then we can always use gradient descent until we hit a saddle.)</p>



<p>The second algorithm one could think of would be to use the Hessian instead of the gradient. That is, repeat the transformation <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" title="x^{t+1} \leftarrow x^t - \eta u" /> where <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> is the minimal eigenvector of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> (i.e., the Hessian matrix <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> such that <img src="https://s0.wp.com/latex.php?latex=H_%7Bi%2Cj%7D+%3D+%5Ctfrac%7B%5Cpartial+J%28x%5Et%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" class="latex" title="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" /> ). By the Taylor approximation <img src="https://s0.wp.com/latex.php?latex=J%28x+-+%5Ceta+u%29+%5Capprox+J%28x%29+-+%5Ceta+%5Cnabla+J%28x%29+%5Ccdot+u+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+u%5E%5Ctop+%5Cnabla%5E2+J%28x%29+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" class="latex" title="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" /> (and since we assume the gradient is zero) the change in the objective will be roughly <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" class="latex" title="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" />. (Because we assume the gradient vanishes, it will not make a difference whether we update with <img src="https://s0.wp.com/latex.php?latex=-%5Ceta+u+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\eta u " class="latex" title="-\eta u " /> or <img src="https://s0.wp.com/latex.php?latex=%2B%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="+\eta u" class="latex" title="+\eta u" />, but we use <img src="https://s0.wp.com/latex.php?latex=-%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\eta" class="latex" title="-\eta" /> for consistency with gradient descent.)</p>



<p>The above approach is promising, but we still need some control over the norm. The way that Subag handles this is that he starts with <img src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0 = 0" class="latex" title="x^0 = 0" />, and at each step takes a step in an orthogonal direction, and so within <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta^2" class="latex" title="1/\eta^2" /> steps he will get to a unit norm vector. That is, the algorithm is as follows:</p>



<p><strong>Algorithm:</strong></p>



<p><strong>Input:</strong> <img src="https://s0.wp.com/latex.php?latex=J%3A%5Cmathbb%7BR%7D%5En+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J:\mathbb{R}^n \rightarrow \mathbb{R}" class="latex" title="J:\mathbb{R}^n \rightarrow \mathbb{R}" />.</p>



<p><strong>Goal:</strong> Find unit <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> approximately minimizing <img src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x)" class="latex" title="J(x)" /></p>



<ol><li>Initialize <img src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^0 = 0^n" class="latex" title="x^0 = 0^n" /></li><li>For <img src="https://s0.wp.com/latex.php?latex=t%3D0%2C%5Cldots%2C1%2F%5Ceta%5E2-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t=0,\ldots,1/\eta^2-1" class="latex" title="t=0,\ldots,1/\eta^2-1" />: i. Let <img src="https://s0.wp.com/latex.php?latex=u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^t" class="latex" title="u^t" /> be a unit vector <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> such that <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> is orthogonal to <img src="https://s0.wp.com/latex.php?latex=u%5E0%2C%5Cldots%2Cu%5E%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^0,\ldots,u^{t-1}" class="latex" title="u^0,\ldots,u^{t-1}" /> and <img src="https://s0.wp.com/latex.php?latex=u%5E%5Ctop+%5Cnabla%5E2+J%28x%5Et%29+u+%5Capprox+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" class="latex" title="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" />. (Since the bottom eigenspace of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> has large dimention, we can find a vector <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> that is not only nearly minimal eigenvector but also orthogonal to all prior ones. Also, as mentioned, we assume that <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+%5Ccdot+u+%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla \cdot u \approx 0" class="latex" title="\nabla \cdot u \approx 0" />.) ii. Set <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u^t" class="latex" title="x^{t+1} \leftarrow x^t - \eta u^t" />.</li><li>Output <img src="https://s0.wp.com/latex.php?latex=x%5E%7B1%2F%5Ceta%5E2%7D+%3D+-%5Ceta%5Csum_%7Bt%3D0%7D%5E%7B1%2F%5Ceta%5E2-1%7D+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" class="latex" title="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" /></li></ol>



<p>(The fact that the number of steps is <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta^2" class="latex" title="1/\eta^2" /> and not <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\eta" class="latex" title="1/\eta" /> is absolutely crucial for the algorithm’s success; without it we would not have been able to use the second order contribution that arise from the Hessian.)</p>



<p>If we define <img src="https://s0.wp.com/latex.php?latex=%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda_t" class="latex" title="\lambda_t" /> to be the minimum eigenvalue at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" title="t" />, we get that the final objective value achieved by the algorithm satisfies</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" class="latex" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" /></p>



<p>Now due to rotation invariance, the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J" class="latex" title="\nabla^2 J" /> at the point <img src="https://s0.wp.com/latex.php?latex=x%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^t" class="latex" title="x^t" /> is the same as <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J" class="latex" title="\nabla^2J" /> at the point <img src="https://s0.wp.com/latex.php?latex=%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(|x^t|,0,\ldots,0)" class="latex" title="(|x^t|,0,\ldots,0)" />. Using concentration of measure arguments, it can be shown that the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(x^t)" class="latex" title="\nabla^2 J(x^t)" /> will be close with high probability to the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(|x^t|,0,\ldots,0)" class="latex" title="\nabla^2 J(|x^t|,0,\ldots,0)" />.<br />Since <img src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5Et%5C%7C%5E2+%3D+%5Ceta%5E2+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\|x^t\|^2 = \eta^2 t" class="latex" title="\|x^t\|^2 = \eta^2 t" /> we can write</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda%28%5Csqrt%7B%5Ceta%5E2+t%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" class="latex" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda(q)" class="latex" title="\lambda(q)" /> is the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J" class="latex" title="\nabla^2 J" /> at the point <img src="https://s0.wp.com/latex.php?latex=%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(q,0,\ldots,0)" class="latex" title="(q,0,\ldots,0)" />.<br />Taking <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> to zero, we get that (approximately) the value of the solution output by the algorithm will satisfy</p>



<p><img src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Cint_0%5E1+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda%28%5Csqrt%7Bq%7D%29+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" class="latex" title="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" /></p>



<p>and hence the result will be completed by showing that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clambda%28%5Csqrt%7Bq%7D%29+%3D+2+%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" class="latex" title="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" /></p>



<p>To do this, let’s recall the definition of <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />:</p>



<p><img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" /> where for every <img src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p \in {2,\ldots, d }" class="latex" title="p \in {2,\ldots, d }" />, <img src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J_p" class="latex" title="J_p" /> is a random tensor of order <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> whose <img src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n^p" class="latex" title="n^p" /> coefficients are all chosen i.i.d in <img src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,1/n)" class="latex" title="N(0,1/n)" />.</p>



<p>For simplicity, let’s assume that <img src="https://s0.wp.com/latex.php?latex=d%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=3" class="latex" title="d=3" /> and hence</p>



<p><img src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D%5C%3B.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." class="latex" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." /></p>



<p>(The calculations in the general case are similar)</p>



<p>The <img src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i,j" class="latex" title="i,j" /> entry of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%5Csqrt%7Bq%7D%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2 J(\sqrt{q},0,\ldots,0)" class="latex" title="\nabla^2 J(\sqrt{q},0,\ldots,0)" /> equals <img src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+J%28q%2C0%2C%5Cldots%2C0%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" class="latex" title="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" />. The contribution of the <img src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^2" class="latex" title="J^2" /> component to this term only arises from the terms corresponding to either <img src="https://s0.wp.com/latex.php?latex=x_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_ix_j" class="latex" title="x_ix_j" /> or <img src="https://s0.wp.com/latex.php?latex=x_jx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_jx_i" class="latex" title="x_jx_i" /> and hence for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" /> it equals <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%28J_%7Bi%2Cj%7D%2BJ_%7Bj%2Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 (J_{i,j}+J_{j,i})" class="latex" title="\gamma_2 (J_{i,j}+J_{j,i})" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+N%280%2C%5Ctfrac%7B2%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B2%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" class="latex" title="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" />. For <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />, since <img src="https://s0.wp.com/latex.php?latex=%28x_i%5E2%29%27%27+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_i^2)'' = 2" class="latex" title="(x_i^2)'' = 2" />, the contributioon equals <img src="https://s0.wp.com/latex.php?latex=2+%5Cgamma_2+J_%7Bi%2Ci%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2 \gamma_2 J_{i,i}" class="latex" title="2 \gamma_2 J_{i,i}" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B4%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{4\gamma_2^2}{n})" class="latex" title="N(0,\tfrac{4\gamma_2^2}{n})" />.</p>



<p>The contribution from the <img src="https://s0.wp.com/latex.php?latex=J%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J^3" class="latex" title="J^3" /> component comes (in the case <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />) from all the <img src="https://s0.wp.com/latex.php?latex=6%3D3%21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="6=3!" class="latex" title="6=3!" /> terms involving <img src="https://s0.wp.com/latex.php?latex=1%2Ci%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1,i,j" class="latex" title="1,i,j" /> that is, <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+%28J_%7B1%2Ci%2Cj%7D%5Csqrt%7Bq%7D+%2B+J_%7B1%2Cj%2Ci%7D%5Csqrt%7Bq%7D+%2B+J_%7Bi%2C1%2Cj%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2C1%2Ci%7D%5Csqrt%7Bq%7D%2BJ_%7Bi%2Cj%2C1%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2Ci%2C1%7D%29%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" class="latex" title="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" /> which is distributed like <img src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+N%280%2C+%5Ctfrac%7B6%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B6%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" class="latex" title="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" />. For <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" /> the contribution will be from the <img src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3" class="latex" title="3" /> terms involving <img src="https://s0.wp.com/latex.php?latex=1%2Ci&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1,i" class="latex" title="1,i" />, with each yielding a contribution of <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2\sqrt{q}" class="latex" title="2\sqrt{q}" />, and hence the result will be distributed like <img src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B12+%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,\tfrac{12 \gamma_3^2 q}{n})" class="latex" title="N(0,\tfrac{12 \gamma_3^2 q}{n})" />.</p>



<p>(More generally, for larger <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, the number of terms for distinct <img src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i,j" class="latex" title="i,j" /> is <img src="https://s0.wp.com/latex.php?latex=p%28p-1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(p-1)" class="latex" title="p(p-1)" />, each contributing a Gaussian of standard deviation <img src="https://s0.wp.com/latex.php?latex=%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" title="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" />, while for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" /> we have <img src="https://s0.wp.com/latex.php?latex=p%28p-1%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(p-1)/2" class="latex" title="p(p-1)/2" /> terms, each contributing a Gaussian of standard deviation <img src="https://s0.wp.com/latex.php?latex=2%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" title="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" />.)</p>



<p>Since the sum of Gaussians is a Gaussian we get that <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J(q,0,\ldots,0){i,j}" class="latex" title="\nabla^2J(q,0,\ldots,0){i,j}" /> is distributed like a Gaussian with variance <img src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cgamma_p%5E2+p%28p-1%29q%5E%7Bp-2%7D%2Fn+%3D+%5Cnu%27%27%28q%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" class="latex" title="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" /> for <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />, and twice that for <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />. This means that the minimum eigenvalue of <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla^2J(q,0,\ldots,0)" class="latex" title="\nabla^2J(q,0,\ldots,0)" /> equals <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\nu''(q)}" class="latex" title="\sqrt{\nu''(q)}" /> times the minimum eigenvalue of a random matrix <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" /> from the Gaussian Orthogonal Ensemble (i.e. <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" title="M" /> is sampled via <img src="https://s0.wp.com/latex.php?latex=M%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M{i,j} \sim N(0,1/n)" class="latex" title="M{i,j} \sim N(0,1/n)" />, <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,i} \sim N(0,2/n)" class="latex" title="M_{i,i} \sim N(0,2/n)" />). As mentioned above, it is known that this minimum eigenvalue is <img src="https://s0.wp.com/latex.php?latex=-2%2Bo%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-2+o(1)" class="latex" title="-2+o(1)" />, and in fact by the semi-circle law, for every <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon&gt;0" class="latex" title="\epsilon&gt;0" />, the number of eigenvalues of value <img src="https://s0.wp.com/latex.php?latex=%5Cleq+-2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\leq -2+\epsilon" class="latex" title="\leq -2+\epsilon" /> is <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Omega(n)" class="latex" title="\Omega(n)" />, and so we can also pick one that is orthogonal to the previous directions. QED</p>



<h2>Full replica symmetry breaking and ultra-metricity</h2>



<p>The point of this blog post is that at least in the “mixed <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> spin” case considered by Subag, we can understand what the algorithm does and the value that it achieves without needing to go into the theory of the geometry of the solution space, but let me briefly discuss some of this theory. (As I mentioned, I am still reading through this, and so this part should be read with big grains of salt.)</p>



<p>The key object studied in this line of work is the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\xi" class="latex" title="\xi" /> of the dot product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x,x' \rangle" class="latex" title="\langle x,x' \rangle" /> for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" /> sampled independently from the Gibbs distribution induced by <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />. (This probability distribution will depend on the number of dimensions <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />, but we consider the case that <img src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n \rightarrow \infty" class="latex" title="n \rightarrow \infty" />.)</p>



<p>Intuitively, there are several ways this probability distribution can behave, depending on how the solution space is “clustered”:</p>



<ul><li>If all solutions are inside a single “cluster”, in the sense that they are all of the form <img src="https://s0.wp.com/latex.php?latex=x+%3D+x_%2A+%2B+e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x = x_* + e" class="latex" title="x = x_* + e" /> where <img src="https://s0.wp.com/latex.php?latex=x_%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_*" class="latex" title="x_*" /> is the “center” of the cluster and <img src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e" class="latex" title="e" /> is some random vector, then <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x,x' \rangle" class="latex" title="\langle x,x' \rangle" /> will be concentrated on the point <img src="https://s0.wp.com/latex.php?latex=%5C%7C+x_%2A%5C%7C%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\| x_*\|^2" class="latex" title="\| x_*\|^2" />.<br /></li><li>If the solutions are inside a finite number <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> of clusters, with centers <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_k" class="latex" title="x_1,\ldots,x_k" />, then the support of the distribution will be on the <img src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k^2" class="latex" title="k^2" /> points <img src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Clangle+x_i%2Cx_j+%5Crangle+%5C%7D_%7Bi%2Cj+%5Cin+%5Bk%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" class="latex" title="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" />.<br /></li><li>Suppose that the solutions are inside a <em>hierarchy</em> of clusters. That is, suppose we have some rooted tree <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" /> (e.g., think of a depth <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> full binary tree), and we associate a vector <img src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_v" class="latex" title="x_v" /> with every vertex <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" />, with the property that <img src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_v" class="latex" title="x_v" /> is orthogonal to all vectors associated with <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />‘s ancestors on the tree. Now imagine that the distribution is obtained by taking a random leaf <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" /> of <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" title="\mathcal{T}" /> and outputting <img src="https://s0.wp.com/latex.php?latex=%5Csum+x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum x_v" class="latex" title="\sum x_v" /> for all vertices <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" /> on the path from the root to <img src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u" class="latex" title="u" />. In such a case the dot product of <img src="https://s0.wp.com/latex.php?latex=x_u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_u" class="latex" title="x_u" /> and <img src="https://s0.wp.com/latex.php?latex=x_%7Bu%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{u'}" class="latex" title="x_{u'}" /> will be <img src="https://s0.wp.com/latex.php?latex=%5Csum_v+%5C%7Cx_v%5C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_v \|x_v\|^2" class="latex" title="\sum_v \|x_v\|^2" /> taken over all the common ancestors of <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" title="v" />. As the dimension and depth of the tree goes to infinity, the distribution over dot product can have continuous support, and it is this setting (specifically when the support is an interval <img src="https://s0.wp.com/latex.php?latex=%5B0%2Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[0,q)" class="latex" title="[0,q)" />) that is known as <em>full replica symmetry breaking</em>. Because the dot product is determined by common ancestor, for every three vectors <img src="https://s0.wp.com/latex.php?latex=x_u%2Cx_v%2Cx_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_u,x_v,x_w" class="latex" title="x_u,x_v,x_w" /> in the support of the distribution <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_v%2Cx_w+%5Crangle+%5Cgeq+%5Cmin+%5C%7B+%5Clangle+x_u%2Cx_v+%5Crangle%2C+%5Clangle+x_u%2Cx_w+%5Crangle+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" class="latex" title="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" /> or <img src="https://s0.wp.com/latex.php?latex=%5C%7C+x_v+-+x_w+%5C%7C+%5Cleq+%5Cmax+%5C%7B+%5C%7Cx_u+-+x_v+%5C%7C%2C+%5C%7Cx_u+-x_w+%5C%7C%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" class="latex" title="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" />. It is this condition that known as <em>ultra-metricity</em>.</li></ul>



<p>In Subag’s algorithm, as mentioned above, at any given step we could make an update of either <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" title="x^{t+1} \leftarrow x^t - \eta u" /> or <img src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+%2B+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^{t+1} \leftarrow x^t + \eta u" class="latex" title="x^{t+1} \leftarrow x^t + \eta u" />. If we think of all the possible choices for the signs in the <img src="https://s0.wp.com/latex.php?latex=d%3D1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=1/\eta^2" class="latex" title="d=1/\eta^2" /> of the algorithms, we see that the algorithm does not only produce a single vector but actually <img src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^d" class="latex" title="2^d" /> such vectors that are arranged in an ultrametric tree just as above. Indeed, this ultrametric structure was the inspiration for the algorithm and is the reason why the algorithm produces the correct result precisely in the full replica symmetry breaking regime.</p>



<p><strong>Acknowledgements:</strong> Thanks to Tselil Schramm for helpful comments.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/"><span class="datestr">at October 23, 2020 07:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/">Tenure-Track Assistant Professor at Rutgers University (apply by January 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered.</p>
<p>Website: <a href="http://jobs.rutgers.edu/postings/120527">http://jobs.rutgers.edu/postings/120527</a><br />
Email: martin@farach-colton.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/"><span class="datestr">at October 22, 2020 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/">Professorship/Chair at TU Hamburg (apply by November 29, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1">https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1</a><br />
Email: berufungen@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/"><span class="datestr">at October 22, 2020 04:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17694">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/">Vaccines are Not Developing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>The search for a vaccine—is not a development.</em><br />
</span></p>
<p>Edward Jenner was an English physician who created the first vaccine, one for smallpox.<br />
<a href="https://rjlipton.wordpress.com/jenner/"><img src="https://rjlipton.files.wordpress.com/2020/10/jenner.jpg?w=600" alt="" class="alignright size-full wp-image-17696" /></a><br />
In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have <i>saved more lives than any other human.</i></p>
<p>Today there is an attempt to create a vaccine against our small-pox of the 21st century.</p>
<p>In his day small-pox killed 10% or more of populations. In our day there is a similar threat. and thus the immense interest in the development of a vaccine. However, there is a misunderstanding about vaccines for COVID-19 that is pervasive. Read the New York Times or watch cable news—CNN, FOX, MSNBC—where “experts” explain how AstraZeneca, Johnson &amp; Johnson, Novavax, and other drug companies are developing a vaccine. What developing means could potentially affect all of us, and a better understanding could save millions of lives.</p>
<p>They are not currently <i>developing</i> the vaccines, they are <i>testing</i> them.  The point we want to emphasize is:</p>
<blockquote>
<p><b> </b> <em> <i>The development of a vaccine does not change the vaccine. The vaccine is the same at the start of its testing trials, and remains the same throughout.</i> </em></p>
</blockquote>
<p>The Oxford vaccine AZD1222 is the same today as it was months ago when it was created. The same is true for the other vaccines currently being tested around the world.</p>
<p>A vaccine is <b>not</b> developed in the usual sense. Drug companies can modify: how the drug is made, how it is stored, how it is given, how many doses are needed, and so on. Drug companies cannot modify the vaccine without starting over—the vaccine must remain the same. Trials can lead to a vaccine being adopted, or it can cause the vaccine to be abandoned. In the later case the drug company can try again, but with a different vaccine.</p>
<h2>Not Development</h2>
<p>Think of the what development means elsewhere.</p>
<ul>
<li>In programming an app: We build a version and try it out. We find bugs and fix them. We use version numbers. Note, there is no AZD1222 version 3.</li>
<li>In writing a book: We make a draft. We have people read the draft. We fix typos and inaccuracies. Our quantum book’s <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bnd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2^{nd}}" class="latex" title="{2^{nd}}" /> edition is on version 11.</li>
<li>In engineering a product: You get the idea.</li>
</ul>
<p>Here is a sample explaining vaccine <a href="https://www.rivm.nl/en/novel-coronavirus-covid-19/vaccine-against-covid-19">development</a>:</p>
<ul>
<li>Phase I: The vaccine is given to healthy volunteers to see whether it is safe and what dosage (quantity) is most effective.</li>
<li>Phase II: The vaccine is given to target groups that would be vaccinated to see what dosage (quantity) is most effective and how well the immune system responds to it.</li>
<li>Phase III: The vaccine is given to an even larger group, consisting of thousands of people, to see how well the vaccine works to prevent COVID-19. People who do receive the vaccine are then compared with people who did not receive the vaccine.Note: there is no step that modifies the vaccine.
</li></ul>
<h2>Consequences</h2>
<p>There are several consequences from this insight about vaccines. For one it makes sense to order millions of doses of a vaccine, even one that has not yet been proved to be safe and effective. For example,</p>
<blockquote>
<p><b> </b> <em> The European Commission has placed its first advance order for a coronavirus vaccine, snapping up 300 million doses of AstraZeneca’s AZD1222 candidate developed by the University of Oxford, with an option on another 100 million. </em></p>
</blockquote>
<p>Note we would never order a large number of copies of a book before all editing and typos were fixed. This is a “proof” that the vaccine is the same.</p>
<p>Actually it may make sense to even begin to take the vaccine. Especially for high risk people. In the past inventors of vaccines have often taken their own new vaccine, even before they were sure they worked.</p>
<h2>Open Problems</h2>
<p>I am a computer scientist with no experience in vaccines. In 1954 I did help test the Jonas Salk polio vaccine. My help was in the form supplying an arm that got a shot of the Salk polio vaccine, I was nine years old then. But I have a math view of vaccines—a viewpoint that sheds light on this misunderstanding.</p></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/"><span class="datestr">at October 22, 2020 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=499">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/">TCS+ talk: Wednesday, October 28 — Omar Montasser, TTIC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Omar Montasser</strong> from TTIC will speak about “<em>Adversarially Robust Learnability: Characterization and Reductions</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We study the question of learning an adversarially robust predictor from uncorrupted samples. We show that any VC class <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> is robustly PAC learnable, but we also show that such learning must sometimes be improper (i.e. use predictors from outside the class), as some VC classes are not robustly properly learnable.  In particular, the popular robust empirical risk minimization approach (also known as adversarial training), which is proper, cannot robustly learn all VC classes.  After establishing learnability, we turn to ask whether having a tractable non-robust learning algorithm is sufficient for tractable robust learnability and give a reduction algorithm for robustly learning any hypothesis class <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" /> using a non-robust PAC learner for <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="H" class="latex" title="H" />, with nearly-optimal oracle complexity.<br />This is based on joint work with Steve Hanneke and Nati Srebro, available at <a href="https://arxiv.org/abs/1902.04217">https://arxiv.org/abs/1902.04217</a>.</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/"><span class="datestr">at October 22, 2020 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/">Computer Science, Tenured/Tenure-track at NYU Shanghai (apply by February 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on.</p>
<p>Website: <a href="https://apply.interfolio.com/80168">https://apply.interfolio.com/80168</a><br />
Email: shanghai.faculty.recruitment@nyu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/"><span class="datestr">at October 22, 2020 01:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/156">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/156">TR20-156 |  Codes over integers, and the singularity of random matrices with large entries | 

	Sankeerth Rao Karingula, 

	Shachar Lovett</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The prototypical construction of error correcting codes is based on linear codes over finite fields. In this work, we make first steps in the study of codes defined over integers. We focus on Maximum Distance Separable (MDS) codes, and show that MDS codes with linear rate and distance can be realized over the integers with a constant alphabet size. This is in contrast to the situation over finite fields, where a linear size finite field is needed.

The core of this paper is a new result on the singularity probability of random matrices. We show that for a random $n \times n$ matrix with entries chosen independently from the range $\{-m,\ldots,m\}$, the probability that it is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/156"><span class="datestr">at October 22, 2020 05:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2020/10/21/intrinsicLR/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2020/10/21/intrinsicLR/">Mismatches between Traditional Optimization Analyses and Modern Deep Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>You may remember our <a href="http://www.offconvex.org/2020/04/24/ExpLR1/">previous blog post</a> showing that it is possible to do state-of-the-art deep learning with learning rate that increases exponentially during training.  It was meant to be a dramatic illustration that what we learned in optimization classes and books isn’t always a good fit for modern deep learning, specifically, <em>normalized nets</em>, which is our term for nets that use any one of popular normalization schemes,e.g. <a href="https://arxiv.org/abs/1502.03167">BatchNorm (BN)</a>, <a href="https://arxiv.org/abs/1803.08494">GroupNorm (GN)</a>, <a href="https://arxiv.org/abs/1602.07868">WeightNorm (WN)</a>. Today’s post (based upon <a href="https://arxiv.org/abs/2010.02916">our paper</a> with Kaifeng Lyu at NeurIPS20)  identifies other surprising incompatibilities between normalized nets and traditional analyses. We hope this will change the way you teach and think about deep learning!</p>

<p>Before diving into the results, we recall that normalized nets  are typically trained with weight decay (aka $\ell_2$ regularization). Thus the $t$th iteration of Stochastic Gradient Descent (SGD) is:</p>

\[w_{t+1} \gets (1-\eta_t\lambda)w_t - \eta_t \nabla \mathcal{L}(w_t; \mathcal{B}_t),\]

<p>where $\lambda$ is the weight decay (WD) factor (or $\ell_2$-regularization coefficient),  $\eta_t$ the learning rate, $\mathcal{B}_t$ the batch, and $\nabla \mathcal{L}(w_t,\mathcal{B}_t)$ the batch gradient.</p>

<p>As sketched in our previous blog post, under fairly mild assumptions (namely, fixing the top layer during random initialization —which empirically does not hurt final accuracy) the loss function for training such normalized nets is <em>scale invariant</em>, which means $\mathcal{L}(w _ t; \mathcal{B}_ t)=\mathcal{L}(cw _ t; \mathcal{B} _ t)$, $\forall c&gt;0$.</p>

<p>A consequence of scale invariance is that the $ \nabla _ w \mathcal{L} \vert _ {w = w _ 0} = c \nabla _ w \mathcal{L}\vert _  {w = cw _ 0}$ and $\nabla ^ 2 _ w \mathcal{L} \vert _ {w = w _ 0} = c ^ 2 \nabla ^ 2 _ w \mathcal{L} \vert _  {w = cw _ 0}$, for any $c&gt;0$.</p>

<h2 id="some-conventional-wisdoms-cws">Some Conventional Wisdoms (CWs)</h2>

<p>Now we briefly describe some conventional wisdoms. Needless to say, by the end of this post these will turn out to be very very suspect! Possibly they were OK in earlier days of deep learning, and with shallower nets.</p>

<blockquote>
  <p>CW 1) As we reduce LR to zero, optimization dynamic converges to a deterministic path (Gradient Flow) along which training loss strictly decreases.</p>
</blockquote>

<p>Recall that in traditional explanation of (deterministic) gradient descent, if LR is smaller than roughly the inverse of the smoothness of the loss function, then each step reduces the loss. SGD, being stochastic, has a distribution over possible paths. But very tiny LR can be thought of as full-batch Gradient Descent (GD), which in the limit of infinitesimal step size approaches Gradient Flow (GF).</p>

<p>The above reasoning shows very small LR is guaranteed to decrease the loss at least, as well as any higher LR, can. Of course, in deep learning, we care not only about optimization but also generalization. Here small LR is believed to hurt.</p>

<blockquote>
  <p>CW 2) To achieve the best generalization the LR must be large initially for quite a few epochs.</p>
</blockquote>

<p>This is primarily an empirical finding: using too-small learning rates or too-large batch sizes from the start (all other hyper-parameters being fixed) is known to lead to worse generalization (<a href="https://arxiv.org/pdf/1206.5533.pdf">Bengio, 2012</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>).</p>

<p>A popular explanation for this phenomenon is  that the noise in gradient estimation during SGD is beneficial for generalization. (As noted, this noise tends to average out when LR is very small.)  Many authors have suggested that the noise helps becauses it keeps the trajectory away from sharp minima which are believed to generalize worse, although there is some difference of opinion here (<a href="http://www.bioinf.jku.at/publications/older/3304.pdf">Hochreiter&amp;Schmidhuber, 1997</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>; <a href="https://arxiv.org/abs/1712.09913">Li et al., 2018</a>; <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>; <a href="https://arxiv.org/pdf/1902.00744.pdf">He et al., 2019</a>). <a href="https://arxiv.org/abs/1907.04595">Li et al., 2019</a> also gave an example (a simple two-layer net) where this observation of worse generalization due to small LR is mathematically proved and also experimentally verified.</p>

<blockquote>
  <p>CW 3) Modeling SGD via a Stochastic Differential Equation (SDE) in the continuous-time limit with a fixed Gaussian noise. Namely, think of SGD as a diffusion process that <strong>mixes</strong>  to some Gibbs-like distribution on trained nets.</p>
</blockquote>

<p>This is the usual approach to  formal understanding of CW 2 (<a href="https://arxiv.org/abs/1710.06451">Smith&amp;Le, 2018</a>; <a href="https://arxiv.org/abs/1710.11029">Chaudhari&amp;Soatto, 2018</a>; <a href="https://arxiv.org/abs/2004.06977">Shi et al., 2020</a>). The idea is that SGD is gradient descent with a noise term, which has a continuous-time approximation as a diffusion process described as</p>

\[dW_t = - \eta_t \lambda W_t dt - \eta_t \nabla \mathcal{L}(W_t) dt + \eta_t \Sigma_{W_t}^{1/2} dB_t,\]

<p>where �$\sigma_{W_t}$ is the covariance of stochastic gradient $ \nabla \mathcal{L}(w_t; \mathcal{B}_t)$,  and $B_t$ denotes Brownian motion of the appropriate dimension. Several works have adopted this SDE view and given some rigorous analysis of the effect of noise.</p>

<p>In this story, SGD turns into a geometric random walk in the landscape, which can in principle explore the landscape more thoroughly, for instance by occasionally making loss-increasing steps. While an appealing view, rigorous analysis is difficult because we lack a mathematical description of the loss landscape.  Various papers assume the noise in SDE is isotropic Gaussian, and then derive an expression for the stationary distribution of the random walk in terms of the familiar Gibbs distribution. This view gives intuitively appealing explanation of some deep learning phenomena since the magnitude of noise (which is related to LR and batch size) controls the convergence speed and other properties. For instance it’s well-known that this SDE approximation implies the well-known <em>linear scaling rule</em> (Goyal et. al., 2017](https://arxiv.org/pdf/1706.02677.pdf)).</p>

<p>Which raises the question: <em>does SGD really behave like a diffusion process that mixes in the loss landscape?</em></p>



<h2 id="conventional-wisdom-challenged">Conventional Wisdom challenged</h2>

<p>We now describe the actual discoveries for normalized nets, which show that the above CW’s are quite off.</p>

<blockquote>
  <p>(Against CW1): Full batch gradient descent $\neq$ gradient flow.</p>
</blockquote>

<p>It’s well known that if LR is smaller than the inverse of the smoothness, then the trajectory of gradient descent will be close to that of gradient flow. But for normalized networks, the loss function is scale-invariant and thus provably non-smooth (i.e., smoothness becomes unbounded)  around the origin (<a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>). We show that this non-smoothness issue is very real and makes training unstable and even chaotic for full batch SGD with any nonzero learning rate. This occurs both empirically and provably so with some toy losses.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/gd_not_gf.png" style="width: 60%;" />
</div>

<p><strong>Figure 1.</strong> WD makes GD on scale-invariant loss unstable and chaotic.
(a) Toy model with scale-invariant loss $L(x,y) = \frac{x^2}{x^2+y^2}$  (b)(c) Convergence never truly happens for  ResNet trained on sub-sampled
CIFAR10 containing 1000 images with full-batch GD (without momentum).  ResNet
can easily get to 100% training accuracy but then veers off.  When WD is turned off at epoch 30000 it converges.</p>

<p>Note that WD plays a crucial role in this effect since without WD the parameter norm increases monotonically 
 (<a href="https://arxiv.org/abs/1812.03981">Arora et al., 2018</a>) which implies SGD moves away from the origin at all times.</p>

<p>Savvy readers might wonder whether using a smaller LR could fix this issue. Unfortunately, getting close to the origin is unavoidable because once the gradient gets small,  WD will dominate the dynamics and decrease the norm at a geometric rate, causing the gradient to rise again due to the scale invariance! (This happens so long as the gradient gets arbitrarily small, but not actually zero, as is the case in practice.)</p>

<p>In fact, this is an excellent (and rare) place where early stopping is necessary even for correct optimization of the loss.</p>

<blockquote>
  <p>(Against CW 2) Small LR can generalize equally well as large LR.</p>
</blockquote>

<p>This actually was a prediction of the new theoretical analysis we came up with. We ran extensive experiments to test this prediction and found that initial large LR is <strong>not necessary</strong> to match the best performance, even when <em>all the other hyperparameters are fixed</em>. See Figure 2.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_test_acc.png" style="width: 300px;" />
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_train_acc.png" style="width: 300px;" />
</div>

<p><strong>Figure 2</strong>. ResNet trained on CIFAR10 with SGD with normal LR schedule (baseline) as well as a schedule with 100 times smaller initial LR.  The latter matches performance of baseline after one more LR decay!  Note it needs  5000 epochs which is 10x higher! See our paper for details. (Batch size is 128, WD is 0.0005, and LR is divided by 10 for each decay.)</p>

<p>Note the  surprise here is that generalization was not hurt from drastically smaller LR even  <em>when no other hyperparameter changes</em>.  It is known empirically as well as rigorously (Lemma 2.4 in <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  that it is possible to compensate for small LR by other hyperparameter changes.</p>

<blockquote>
  <p>(Against Wisdom 3) Random walk/SDE view of SGD is way off. There is no evidence of mixing as  traditionally understood, at least within normal training times.</p>
</blockquote>

<p>Actually the evidence against global mixing exists already via the phenomenon of Stochastic Weight Averaging (SWA) (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>). Along the trajectory of SGD, if  the network parameters from two different epochs are averaged, then the average has test loss lower than either.  Improvement via averaging continues to  work for run times 10X longer  than usual as shown in Figure 3. However, the accuracy improvement doesn’t happen for SWA between two solutions obtained from different initialization.  Thus checking whether SWA holds distinguishes between  pairs of solutions drawn from the same trajectory and pairs drawn from different trajectories, which  shows the diffusion process hasn’t mixed to stationary distribution within normal training times. (This is not surprising, since the theoretical analysis of mixing does not suggest it happens rapidly at all.)</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_test_acc.png" style="width: 300px;" />
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_dist.png" style="width: 300px;" />
</div>

<p><strong>Figure 3</strong>. Stochastic Weight Averaging improves the test accuracy of ResNet trained with
SGD on CIFAR10. <strong>Left:</strong> Test accuracy. <strong>Right:</strong> Pairwise distance between parameters from different epochs.</p>

<p>Actually <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a> already noticed the implication that SWA rules out that SGD is a diffusion process which mixes to a unique global equilibrium. They suggested instead that perhaps the trajectory of SGD could be well-approximated by a multivariate Ornstein-Uhlenbeck (OU) process around the <em>local minimizer</em> $W^ * $, assuming the loss surface is locally strongly convex. As the corresponding stationary is multi-dimensional Gaussian, $N(W^ *, \Sigma)$, around the local minimizer, $W^ *$, this explains why SWA helps to reduce the training loss.</p>

<p>However, we note that (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>)’s suggestion is also refuted by the fact that we can show $\ell_2$ distance between weights from epochs $T$ and $T+\Delta$ monotonically increases with $\Delta$ for every $T$ (See Figure 3), while $ \mathbf{E} [ | W_ T-W_ {T+\Delta} |^2]$ should converge to the constant $2Tr[\Sigma]$ as $T, \Delta \to +\infty$ in the OU process. This suggests that all these weights are correlated, unlike the hypothesized OU process.</p>

<h2 id="so-whats-really-going-on">So what’s really going on?</h2>

<p>We develop a new theory (some parts rigorously proved and others supported by experiments) suggesting that <strong>LR doesn’t play the role assumed in most discussions.</strong></p>

<p>It’s widely believed that LR $\eta$ controls the convergence rate of SGD and affects the generalization via changing the magnitude of noise because LR $\eta$ adjusts the magnitude of gradient update per step. 
 
 However, for normalized networks trained with SGD + WD, the effect of LR is more subtle as now it has two roles: (1). the multiplier before the gradient of the loss. (2). the multiplier before WD. Intuitively, one imagines the WD part is  useless since the loss function is scale-invariant, and thus the first role must be more important. But surprisingly, this intuition is completely wrong and it turns out that the second role is way more important than the first one. 
Further analysis shows that a better measure of speed of learning is   $\eta \lambda$, which we call the <em>intrinsic learning rate</em> or <em>intrinsic LR</em>, denoted $\lambda_e$.</p>

<p>While previous papers have noticed qualitatively that LR and WD have a close interaction, our ExpLR paper   <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  gave mathematical proof that <em>if WD* LR, i.e., $\lambda\eta$ is fixed, then the effect of changing LR on the dynamics is equivalent to rescaling the initial parameters</em>.  As far as we can tell, performance of SGD on modern architectures is quite robust to (indeed usually independent of) scale of the initialization, so the effect of changing initial LR while keeping intrinsic LR fixed is also negligible.</p>

<p>Our paper gives insight into the role of intrinsic LR $\lambda_e$ by giving a new SDE-style analysis of SGD for normalized nets, leading to the following conclusion (which rests in part on experiments):</p>

<blockquote>
  <p>In normalized nets SGD does indeed lead to rapid mixing, but in <strong>function space</strong> (i.e., input-output behavior of the net). Mixing happens after $O(1/\lambda_e)$ iterations, in contrast to the exponentially slow mixing guaranteed in the parameter space by traditional analysis of diffusion walks.</p>
</blockquote>

<p>To explain the meaning of mixing in function space, let’s view SGD (carried out for a fixed number of steps) as a way to sample a trained net from a  distribution over trained nets. Thus the end result of SGD from a fixed initialization can be viewed as a probabilistic classifier whose output on any datapoint is the $K$-dimenstional vector whose $i$th coordinate is the probability of outputting label $i$. (Here $K$ is the total number of labels.) Now if two different initializations both cause SGD to produce classifiers with error $5$ percent on heldout datapoints, then  <em>a priori</em> one would imagine that  on a given held-out datapoint the classifier from the first distribution <strong>disagrees</strong>  with the classifier from the second distribution with roughly $2 * 5 =10$ percent probability. (More precisely, $2 * 5 * (1-0.05) = 9.5$ percent.) However, convergence to an equilibrium distribution in function space means that the probability of disagreement is almost $0$, i.e., the distribution is almost the same regardless of the initialization! This is indeed what we experimentally find, to our big surprise. Our theory is built around this new phenomenon.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/conjecture.png" style="width: 500px;" />
</div>
<p><strong>Figure 4</strong>: A simple 4-layer normalized CNN trained on MNIST with three schedules converge to the same equilibrium after intrinsic LRs become equal at epoch 81. We use Monte Carlo ($500$ trials) to estimate $\ell_1$ distances between distributions.</p>

<p>In the next post, we will explain our new theory and the partial new analysis of SDEs arising from SGD in normalized nets.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2020/10/21/intrinsicLR/"><span class="datestr">at October 21, 2020 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/">Tenure-track faculty at Portland State University (apply by December 31, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of<br />
Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science.</p>
<p>Website: <a href="https://www.pdx.edu/computer-science/open-faculty-position">https://www.pdx.edu/computer-science/open-faculty-position</a><br />
Email: cssearch@pdx.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/"><span class="datestr">at October 21, 2020 04:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/reconstruction-theory/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/reconstruction-theory/">The Theory of Reconstruction Attacks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>We often see people asking whether or not differential privacy might be overkill.  Why do we need strong privacy protections like differential privacy when we’re only releasing approximate, aggregate statistical information about a dataset?  Is it really possible to extract information about specific users from releasing these statistics?  The answer turns out to be a resounding yes!  The textbook by Dwork and Roth <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">[DR14]</a> calls this phenomenon the Fundamental Law of Information Recovery:</p>

<blockquote>
  <p>Giving overly accurate answers to too many questions will inevitably destroy privacy.</p>
</blockquote>

<p>So what exactly does this fundamental law mean precisely, and how can we prove it?  We can formalize and prove the law via <em>reconstruction attacks</em>, where an attacker can recover secret information from nearly every user in the dataset, simply by observing noisy answers to a modestly large number of (surprisingly simple) queries on the dataset. Reconstruction attacks were introduced in a seminal paper by Dinur and Nissim in 2003 <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>.  Although this paper predates differential privacy by a few years, the discovery of reconstruction attacks directly led to the definition of differential privacy, and shaped a lot of the early research on the topic. We now know that differentially private algorithms can, in some cases, match the limitations on accuracy implied by reconstruction attacks. When this is the case, we have a remarkably sharp transition from a blatant privacy violation when the accuracy is high enough to enable a reconstruction attack, to the strong protection given by differential privacy at the cost of only slightly lower accuracy.</p>

<p>Aside from the theoretical importance of reconstruction attacks, one may wonder if they can be carried out in practice, or if the attack model is unrealistic and can be avoided with some simple workarounds?  In this series of posts, we argue that reconstruction attacks can be quite practical.  In particular, we describe successful attacks by some of this post’s authors on a family of systems called <em>Diffix</em>, that attempt to prevent reconstruction without introducing as much noise as the reconstruction attacks suggest is necessary. To the best of our knowledge, these attacks represent the first successful attempt to reconstruct data from a commercial statistical-database system that is specifically designed to protect the privacy of the underlying data.  A larger and much more significant demonstration of the practical power of reconstruction attacks was carried out by the US Census Bureau in 2018, motivating the Bureau’s adoption of differential privacy for data products derived from the 2020 decennial census <a href="https://queue.acm.org/detail.cfm?ref=rss&amp;id=3295691">[GAM18]</a>.</p>

<p>This series will come in two parts: In this post, we will review the theory of reconstruction attacks, and present a model for reconstruction attacks that corresponds more directly to real attacks than the one that is typically presented.  In the second post, we will describe attacks that were launched against various iterations of the <em>Diffix</em> system. \(
\newcommand{\uni}{\mathcal{X}} % The universe
\newcommand{\usize}{T} % Universe size
\newcommand{\elem}{x} % Generic universe element. 
\newcommand{\pbs}{z} %Non-secret bits
\newcommand{\pbsuni}{\mathcal{Z}}
\renewcommand{\sb}{b} % Secret bit
\newcommand{\pds}{Z} %non-secret part of the data set
\newcommand{\ddim}{d} % Data dimension
\newcommand{\queries}{Q} % A set/workload of queries
\newcommand{\qmat}{\mat{Q}} % Query matrix
\newcommand{\qent}{w} % Entry of the query matrix
\newcommand{\hist}{h} % Histogram vector
\newcommand{\mech}{\mathcal{M}} % Generic Mechanism
\newcommand{\query}{q}
\newcommand{\queryfunc}{\varphi}
\newcommand{\ans}{a} % query answer
\newcommand{\qsize}{k}
\newcommand{\ds}{X}
\newcommand{\dsrow}{\elem} % same as elem above
\newcommand{\dsize}{n}
\newcommand{\priv}{\eps}
\newcommand{\privd}{\delta}
\newcommand{\acc}{\alpha}
\newcommand{\from}{:}
\newcommand{\set}[1]{\left{#1\right}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\eps}{\varepsilon}
\newcommand{\pmass}{\mathbbm{1}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\mat}[1]{#1} % matrix notation: for now nothing
\)</p>
<h3 id="a-model-of-reconstruction-attacks">A Model of Reconstruction Attacks</h3>

<p>This part presents the basic theory of reconstruction attacks.  We’ll introduce a model of reconstruction attacks that is a little different from what you would see if you read the papers, and then describe the main results of Dinur and Nissim.  At the end we will briefly mention some variations that have been considered in the nearly two decades since.</p>

<p>Let us fix a dataset model, so that we can describe the attack precisely. (These attacks are very flexible and the ideas can usually be adapted to new models, as we’ll see at the end of this part.) We take the dataset to be a collection of \(\dsize\) records \(\ds = \{\elem_1,\dots,\elem_n\}\), each corresponding to the data of a single person.  The attacker’s goal is to learn some piece of secret information about as many individuals as possible, so we think of each record as having the form \(\elem_i = (\pbs_i,\sb_i)\) where \(\pbs_i\) is some identifying information, and \(\sb_i \in \zo\) is some secret. We assume that the secret is binary, although this aspect of the model can be generalized. We can visualize such a dataset as a matrix \([\pds \mid \sb]\) with two blocks as follows:
\[ \left[ \begin{array}{c|c} \pbs_1 &amp; \sb_1 \\ \vdots &amp; \vdots \\ \pbs_n &amp; \sb_n \end{array} \right] \]
For a concrete example, suppose each element in the dataset contains \(d\) binary attributes, and the attacker’s goal is to learn the last attribute of each user.  In this case we would write each element as a pair \((\pbs_i, \sb_i)\) where \(\pbs_i \in \zo^{d-1}\) and \(\sb_i \in \zo\).</p>

<p>Note that this distinction between \(\pbs_i\) and \(\sb_i\) is only in the mind of the attacker, who has some prior information about the users, but is trying to learn some specific secret information.  In order to make the attack simpler to describe, we will also assume that the attacker knows \(\pbs_1,\dots,\pbs_\dsize\), which is everything about the dataset except the secret bits, although this assumption can also be relaxed to a large extent. As a shorthand, we will refer to \(\pbs_1, \ldots, \pbs_\dsize\) as the prior information, and to \(\sb_1, \ldots,\sb_\dsize\) as the secret bits.</p>

<p>Our goal is to understand whether asking aggregate queries defined by the prior information can allow an attacker to learn non-trivial information about the secret bits.  Perhaps the most basic type of aggregate query we can ask is a <em>counting query</em>, which is a query that asks what number of the data points satisfy a given property. The Dinur-Nissim attacks assume that the attacker can get approximate answers to a type of counting queries that ask how many data points satisfy some property defined in terms of the prior information, and also have the sensitive bit set to \(1\).  Let us use the notation \(\pbsuni\) for the set of all possible values that the prior information can take. For the purposes of the attack, each query \(\query\) will be specified by a function \(\queryfunc \from \pbsuni \to \zo\) and have the specific form
\[
\query(\ds) = \sum_{j=1}^{\dsize} \queryfunc(\pbs_j) \cdot \sb_j.
\]
This is a good time to make one absolutely crucial point about this model, which is that</p>
<blockquote>
  <p>all the users are treated completely symmetrically by the queries, and the attacker cannot issue a query that targets a specific user \(x_i\) by name or a specific subset of users.  The different users are distinguished only by their data.  Nonetheless, we will see how to learn information about specific users from the answers to these queries.</p>
</blockquote>

<p>Returning to our example with binary attributes, consider the very natural set of queries that asks for the inner product of the secret bits with each attribute in the prior information, which is a measure of the correlation between these two attributes.  Then each query takes the form \(\query_i(\ds) = \sum_{j=1}^{n} \pbs_{j,i} \cdot \sb_{j}\).</p>

<p>The nice thing about this type of query is that we can express the answers to a set of queries \({\query_1,\dots,\query_\qsize}\) defined by \(\queryfunc_1, \ldots, \queryfunc_\qsize\) as the following matrix-vector product \(\qmat_{\pds}\cdot \mat{b}\):
\[ \left[ \begin{array}{c}\query_1(\ds) \\ \vdots  \\ \query_\qsize(\ds) \end{array} \right] = \left[ \begin{array}{ccc} \queryfunc_1(\pbs_1) &amp; \dots &amp; \queryfunc_1(\pbs_\dsize) \\ \vdots &amp;  \ddots  &amp; \vdots \\ \queryfunc_\qsize(\pbs_1) &amp;   \dots &amp; \queryfunc_k(\pbs_\dsize)  \end{array} \right] \left[ \begin{array}{c} \sb_1 \\ \vdots  \\ \sb_n  \end{array} \right]
\]
so we can study this model using tools from linear algebra.</p>

<h3 id="an-inefficient-attack">An Inefficient Attack</h3>

<p>Exact answers to such queries are clearly revealing, because, the attacker can use the predicates \[ \queryfunc_i(z) = \begin{cases} 1 &amp; \textrm{if } \pbs = \pbs_i  \\ 0 &amp;  \textrm{otherwise} \end{cases} \] to single out a specific user and receive their bit \(\sb_i\).  It is less obvious, however, that an attacker can learn a lot about the private bits even given noisy answers to the queries.</p>

<p>The first Dinur-Nissim attack shows that this is indeed possible—if the attacker can ask an unbounded number of counting queries, and each query is answered with, for example, 5% error, then the attacker can reconstruct 80% of the secret bits.  This attack requires exponentially many queries to run, making it somewhat impractical, but it is a proof of concept that an attack can reconstruct a large amount of private information even from very noisy statistics. Later we will see how to scale down the attack to use fewer queries at the cost of requiring more accurate answers.</p>

<p>The attack itself is quite simple:</p>

<ul>
  <li>
    <p>For simplicity, assume all the \(\pbs_1, \ldots, \pbs_\dsize\) are distinct so that each user is uniquely identified by the prior information.</p>
  </li>
  <li>
    <p>The attacker chooses the queries \(\query_1, \ldots, \query_\qsize\) so that the matrix \(\qmat_\pds\) has as its rows all of \(\zo^\dsize\). Namely, \(\qsize=2^\dsize\) and the functions \(\queryfunc_1, \ldots, \queryfunc_\qsize\) defining the queries take all possible values on \(\pbs_1, \ldots, \pbs_\dsize\).</p>
  </li>
  <li>
    <p>The attacker receives a vector \(\ans\) of noisy answers to the queries, where \( |\query_{i}(\ds) - \ans_{i}| &lt; \acc \dsize \) for each query \( \query_i \).  In matrix notation, this means \[ \max_{i = 1}^\qsize  |(\qmat_\pds\cdot {\sb})_i -\ans_i|= \| \qmat_\pds \cdot \sb -\ans\|_\infty  \leq \alpha \dsize. \]
  Note that, for \(\{0,1\}\)-valued queries, the answers range from \(0\) to \(\dsize\), so answers with additive error \(\pm 5\%\) corresponds to \(\acc = 0.05\).</p>
  </li>
  <li>
    <p>Finally, the attacker outputs any guess \(\hat{\sb} = (\hat{\sb}_{1}, \ldots, \hat{\sb}_{n})\) of the private bits vector that is consistent with the answers and the additive error bound \(\acc\). In other words, \(\hat{\sb}\) just needs to satisfy \[\max_{i = 1}^\qsize |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i|= \| \qmat_\pds \cdot \hat\sb - a \|_{\infty} \leq \alpha \dsize \]
  Note that a solution always exists, since the true private bits \(\sb\) will do.</p>
  </li>
</ul>

<p>Our claim is that any such guess \(\hat{b}\) in fact agrees with the true private bits \(b\) for all but \(4\acc \dsize\) of the users. The reason is that if \(\hat{\sb}\) disagreed with more than \(4\acc \dsize\) of the secret bits, then the answer to some query would have eliminated \(\hat{\sb}\) from contention.  To see this, fix some \(\hat{\sb}\in \zo^\dsize\), and let \[ S_{01} = \{j: \hat{\sb}_j = 0,  \sb_j = 1\} \textrm{ and } S_{10} = \{j: \hat{\sb}_j = 1,  \sb_j = 0\}\] 
If \(\hat{\sb}\) and \(\sb\) disagree on more than \(4\acc \dsize\) bits, then at least one of these two sets has size larger than \(2\acc \dsize\). Let us assume that this set is \(S_{01}\), and we’ll deal with the other case by symmetry.  Suppose that the \(i\)-th row of \(\qmat_\pds\) is the indicator vector of \(S_{01}\), i.e., \[(\qmat_\pds)_{i,j} = 1 \iff j \in S_{01}.\] We then have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i|= |S_{01}| &gt; 2 \acc \dsize,
\]
but, at the same time, if \(\hat{\sb}\) were output by the attacker, we would have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i| \le |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i| + |(\qmat_\pds \cdot \sb)_i - \ans_{i}| \le 2\acc \dsize, \]
which is a contradiction. An important point to note is that the attacker does not need to know the set \(S_{10}\), or the corresponding \(i\)-th row of \(\qmat_\pds\) and query \(\query_i\). Since the attacker asks all possible queries determined by the prior information, we can be sure \(\query_i\) is one of these queries, and an accurate answer to it rules out this particular bad choice of \(\hat{\sb}\).  To give you something concrete to cherish, we can summarize this discussion in the following theorem.</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is a reconstruction attack that issues \(2^n\) queries to a dataset of \(n\) users, obtains answers with error \(\alpha n\), and reconstructs the secret bits of all but \(4 \alpha n\) users.</p>
</blockquote>

<h3 id="an-efficient-attack">An Efficient Attack</h3>

<p>The exponential Dinur-Nissim attack is quite powerful, as it recovers 80% of the secret bits even from answers with 5% error, but it has the drawback that it requires asking \(2^\dsize\) queries to a dataset with \(\dsize\) users.  Note that this is inherent to some extent.  Suppose we randomly subsample 50% of the dataset and answer the queries using only this subset by rescaling appropriately.  Although this random subsampling does not guarantee any meaningful privacy, clearly no attacker can reconstruct 75% of the secret bits, since some of them are effectively deleted.  However, the guarantees of random sampling tell us that any set of \(\qsize\) queries will be answered with maximum error \( \acc n =  O(\sqrt{n \log \qsize})\), so we can answer \( 2^{\Omega(n)} \) queries with \(5\%\) error while provably preventing this sort of reconstruction.</p>

<p>However, Dinur and Nissim showed that if we obtain <em>highly accurate</em> answers—still noisy, but with error smaller than the sampling error—then we can reconstruct the dataset to high accuracy.  We can also make the reconstruction process computationally efficient by using linear programming to replace the exhaustive search over all \(2^\dsize\) possible vectors of secrets.  Specifically, we change the attack as follows:</p>

<ul>
  <li>
    <p>The attacker now chooses \(\qsize\) <em>randomly chosen</em> functions \( \varphi_i \from \pbsuni \to \{0,1\} \) for a much smaller \(\qsize = O(\dsize) \).</p>
  </li>
  <li>
    <p>Upon receiving an answer vector \(\ans\), the attacker now searches for a <em>real-valued</em> \( \tilde{b} \in [0,1]^{\dsize} \) such that \( \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq \acc n \).  Note that this vector can be found efficiently via linear programming.  The attacker then rounds each \( \tilde{b}_{i} \) to the nearest \( \hat{b}_{i} \in \{0,1\}\).</p>
  </li>
</ul>

<p>It’s now much trickier to analyze this attack and show that it achieves low reconstruction error, and we won’t go into details in this post.  However, the key idea is that, because the queries are chosen randomly, \( \qmat_\pds \) is a random matrix with entries in \( \{0,1\} \), and we can use the statistical properties of this random matrix to argue that, with high probability,
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim |{i: \sb_i \neq \hat{\sb}_i}|.
\]
By the way we chose \(\tilde{\sb}\), we have 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty \le \|\qmat_\pds \cdot \sb - \ans\|_\infty + \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq 2\acc n,
\]
so, by combining the inequalities we get that the reconstruction error is about \( O(\alpha^2 n^2) \). Note that, in order to reconstruct 80% of the secret bits using this attack, we now need the error to be \( \alpha n  \ll \sqrt{n} \), but as long as this condition on the error is satisfied, we will have a highly accurate reconstruction.  Let’s add this theorem to your goodie bags:</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is an efficient reconstruction attack that issues \(O(n)\) random queries to a dataset of \(n\) users, obtains answer with error \(\alpha n\), and, with high probability, reconstructs the secret bits of all but \( O(\alpha^2 n^2)\) users.</p>
</blockquote>

<p>Although we modeled the queries, and thus the matrix \(\qmat_\pds\) as uniformly random, it’s important to note that we really only relied on the fact that 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim
|\{i: \sb_i \neq \hat{\sb}_i\}|,
\]
and we can reconstruct while tolerating the same \(\Omega(\sqrt{n})\) error for any family of queries that gives rise to a matrix with this property.  Intuitively, any <em>random-enough</em> family of queries will have this property.  More specifically, the property is satisfied by any matrix with no small singular values <a href="https://dl.acm.org/doi/10.1007/978-3-540-85174-5_26">[DY08]</a> or with large discrepancy <a href="https://arxiv.org/abs/1203.5453">[MN12]</a>.  There is a large body of work showing that many specific families of queries lead to reconstruction. For example, we can perform reconstruction using <em>conjunction queries</em> that ask for the marginal distribution of small subsets of the attributes <a href="https://dl.acm.org/doi/abs/10.1145/1806689.1806795">[KLSU10]</a>.  That is, queries of the form “count the number of people with blue eyes and brown hair and a birthday in August.”  In fairness, there are also families of queries that do not satisfy the property, or only satisfy quantitatively weaker versions of it, such as histograms and threshold queries, and for these queries it is indeed possible to achieve differential privacy with \( \ll \sqrt{n} \) error.</p>

<h3 id="conclusion">Conclusion</h3>

<p>This is going to be the end of our technical discussion, but before signing off, let’s mention some of the important extensions of this theorem that have been developed over the years:</p>

<ul>
  <li>
    <p>We can allow the secret information \(\sb\) to be integers or real numbers, rather than bits. The queries still return \(\qmat_\pds\cdot \sb\). The exponential attack then guarantees that, given answers with error \(\acc n\), the reconstruction \(\hat{\sb}\) satisfies \(\|\hat{\sb}-\sb\|_1 \le 4\acc n\). This means, for example, that the reconstructed secrets of all but \(4\alpha n\) users are within \(\pm 1\) of the true secrets. The efficient attack guarantees that \(\|\hat{\sb}-\sb\|_2^2 \le O(\acc^2 n^2)\), which means that the reconstructed secrets are within \(\pm 1\) for all but \(O(\acc^2 n^2)\) users.</p>
  </li>
  <li>
    <p>It’s not crucial that <em>every</em> query be answered with error \( \ll \sqrt{n} \).  If we are willing to settle 
  for an inefficient attack, then we can reconstruct even if only 51% of the queries have small error.  If at least 75% have small error, then we can reconstruct efficiently <a href="https://dl.acm.org/doi/10.1145/1250790.1250804">[DMT07]</a>.</p>
  </li>
  <li>
    <p>The reconstruction attacks still apply to the seemingly more general data model in which the private 
  dataset \(\ds\) is a subset of some arbitrary (but public) data universe \(\uni\).  To see this, note that we can take \(\uni = \{\pbs_1, \ldots, \pbs_\dsize\}\), and we can interpret the secret bits \(\sb_i\) to indicate whether \(\pbs_i\) is an element of \(\ds\). Then the reconstruction attacks allow us to determine, up to some error, which elements of \(\uni\) are contained in \(\ds\). In the setting, the attack is sometimes called <em>membership inference</em>.</p>
  </li>
  <li>
    <p>The fact that the efficient Dinur-Nissim reconstruction attack fails when the error is \( \gg \sqrt{n} \) 
  does not mean it’s easy to achieve privacy with error of that magnitude.  As we mentioned earlier, we can achieve non-trivial error guarantees for a large number of queries simply by using a random subsample of half of the dataset, which is not a private algorithm in any reasonable sense of the word, as it can reveal everything about the chosen subset.  As this example shows,</p>

    <blockquote>
      <p>preventing reconstruction attacks does not mean preserving privacy.</p>
    </blockquote>

    <p>In particular, there are membership-inference attacks that succeed in violating privacy even when the queries are answered with \( \gg \sqrt{n}\) error. We refer the reader to the survey <a href="https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data">[DSSU17]</a> for a somewhat more in-depth survey of reconstruction and membership-inference attacks.</p>
  </li>
</ul>

<p>Many types of queries give rise to the conditions under which reconstruction is possible.  Stay tuned for our next post, where we show how to generate those types of queries in practice against a family of systems known as <em>Diffix</em> that are specifically designed to thwart reconstruction.</p></div>







<p class="date">
by Jonathan Ullman <a href="https://differentialprivacy.org/reconstruction-theory/"><span class="datestr">at October 21, 2020 04:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/">Assistant and Associate Professors to contribute to the future of the Department of Computer Science at Department of Computer Science, Aarhus University (apply by January 11, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration on societal challenges.</p>
<p>Website: <a href="https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en">https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en</a><br />
Email: kgronbak@cs.au.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/"><span class="datestr">at October 21, 2020 07:02 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/">Tenure-track Faculty Positions at Simon Fraser University (apply by December 17, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html#tenure">https://www.sfu.ca/computing/job-opportunities.html#tenure</a><br />
Email: cs_faculty_affairs@sfu.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/"><span class="datestr">at October 20, 2020 08:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/">Three-year and tenure-track positions at Toyota Technological Institute at Chicago (apply by December 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship.</p>
<p>Website: <a href="https://www.ttic.edu/faculty-hiring/">https://www.ttic.edu/faculty-hiring/</a><br />
Email: recruiting@ttic.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/"><span class="datestr">at October 20, 2020 05:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html">The graphs of stably matchable pairs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The <a href="https://en.wikipedia.org/wiki/Stable_marriage_problem">stable matching problem</a> takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the <a href="https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm">Gale–Shapley algorithm</a>, but there are generally many solutions, described by the <a href="https://en.wikipedia.org/wiki/Lattice_of_stable_matchings">lattice of stable matchings</a>. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the <em>graph of stably matchable pairs</em>. This graph is the subject and title of my latest preprint, <a href="https://arxiv.org/abs/2010.09230">arXiv:2010.09230</a>, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</p>

<p>For some answers, see the preprint. One detail is connected to <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html">my previous post, on polyhedra with no two disjoint faces</a> (even though there are no polyhedra in the new preprint): the (prism,\(K_{3,3}\))-minor-free graphs discussed there come up in proving an equivalence between outerplanar graphs of stably matchable pairs and lattices of <a href="https://en.wikipedia.org/wiki/Closure_problem">closures</a> of <a href="https://en.wikipedia.org/wiki/Polytree">oriented trees</a>. Instead of providing any technical details of any the other results in the paper, though, I thought it would be more fun to show a few visual highlights.</p>

<p>The following figure shows a cute mirror-inversion trick (probably already known, although I don’t know where or by whom) for embedding an arbitrary bipartite graph as an induced subgraph of a regular bipartite graph. I use it to show that graphs of stably matchable pairs have no forbidden induced subgraphs:</p>

<p style="text-align: center;"><img width="60%" alt="Embedding a bipartite graph as an induced subgraph of a regular bipartite graph" src="https://11011110.github.io/blog/assets/2020/regularize.svg" /></p>

<p>This next one depicts a combinatorial description of a stable matching instance having a \(6\times 5\) grid as its graph, in terms of the top and bottom matchings in the lattice of matchings, the “rotations” that can be used to move between matchings in this lattice, and a partial order on the rotations. For what I was doing in this paper, these rotation systems were much more convenient to work with than preferences.</p>

<p style="text-align: center;"><img width="80%" alt="Rotation system describing a system of stable matchings having a 6x5 grid as its graph" src="https://11011110.github.io/blog/assets/2020/5x6.svg" /></p>

<p>All the main ideas for a proof of NP-completeness of recognizing these graphs, by reduction from <a href="https://en.wikipedia.org/wiki/Not-all-equal_3-satisfiability">not-all-equal 3-satisfiability</a>, are visible in the next picture. The proof now in the paper is significantly more complicated, though, because the construction in this image produces nonplanar graphs but I wanted a proof that would also apply in the planar case.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/nae3sat-to-matching.svg" alt="NP-completeness reduction from NAE3SAT to recognizing graphs of stably matchable pairs" /></p>

<p>The last one shows a sparse graph that can be represented as a graph of stably-matching pairs (because it’s outerplanar, bipartite, and biconnected) but has a high-degree vertex. If we tried to test whether it could be realized by doing a brute-force search over preference systems, the time would be factorial in the degree, but my preprint provides faster algorithms that are only singly exponential in the number of edges.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/factorial.svg" alt="Outerplanar graph of stably matchable pairs with a factorial number of potential preference systems" /></p>

<p>(<a href="https://mathstodon.xyz/@11011110/105065476283424319">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html"><span class="datestr">at October 19, 2020 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6623846086903606036">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html">Nature vs Nurture close to my birthday</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> Since I was born on Oct 1, 1960 (that's not true---if I posted  my real birthday I might get my  identity stolen), I will do a nature vs nurture post based on my life, which seems less likely to offend then doing it on someone else's life. I'll just rattle off some random points on Nature vs Nurture.</p><p>1) Is it plausible that I was born with some math talent? Is plausible that I was born with some talent to understand the polynomial van der Warden theorem? What is the granularity of nature-given or nurture-given abilities?</p><p>2) My dad was a HS English teacher and later Vice-Principal. My mom taught English at a Community college. Readers of the blog might think, given my spelling and grammar, that I was switched at birth. My mom says (jokingly?) that I was switched at birth since she thinks I am good at math.</p><p>a) I am not THAT good at math. Also see next point.</p><p>b) While there are some math families, there are not many. See my post <a href="https://blog.computationalcomplexity.org/2009/02/baseball-families-and-math-families.html">here</a>.</p><p>c) I think being raised in an intellectual atmosphere by two EDUCATORS who had the money to send me to college and allowed me the freedom to study what I wanted to  is far more important than the rather incidental matter of what field I studied.</p><p>d) Since my parents never went into math or the sciences it is very hard to tell  if they were `good at math' or even what that means.</p><p>3) There were early signs I was INTERESTED in math, though not that I was good at it.</p><p>a) In fourth grade I wanted to know how many SECONDS were in a century so I spend some time figuring it out on paper. Did I get the right answer?  I forgot about leap years.</p><p>b) I was either a beneficiary of, or a victim of, THE NEW MATH. So I learned about comm. and assoc. operations in 8th grade. We were asked to come up with our own operations. I wanted to come up with an operation that was comm. but not assoc. I did! Today I would write it as f(x,y) = |x-y|. This is the earliest I can think of where I made up a nice math problem. Might have been the last time I made up a nice math problem AND solved it without help. </p><p>c) In 10th grade I took some Martin Gardner books out of the library. The first theorem I learned not-in-school was that a graph is Eulerian iff every vertex has even degree. I read the chapter on Soma cubes and bought a set. (Soma cubes are explained <a href="https://en.wikipedia.org/wiki/Soma_cube">here</a>.) </p><p>d) I had a talent (nature?) at Soma Cubes.  I did every puzzle in the book in a week, diagrammed them, and even understood (on some level) the proofs that some could not be done. Oddly I am NOT good at 3-dim geom. Or even 2-dim geom.  For 1-dim I hold my own!</p><p>e) Throughout my childhood I noticed odd logic and odd math things that were said: </p><p>``Here at WCOZ (a radio station) we have an AXIOM, that's like a saying man, that weekends should be SEVEN DAYS LONG'' (Unless that axiom resolves CH, I don't think it should be assumed.) </p><p>``To PROVE we have the lowest prices in town we will give you a free camera!'' (how does that prove anything?) </p><p>``This margarine tastes JUST LIKE BUTTER'' (Okay-- so why not just buy butter?)</p><p>e) In 9th grade when I learned the quadratic formula I re-derived it once-a-month since I though it was important that one can prove such things.  I heard (not sure from where) that there was no 5th degree equation. At that very moment I told my parents:</p><p><i>I am going to major in math so I can find out why there is no 5th degree equation.</i></p><p>There are worse things for parents to hear from their children. See <a href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html">here</a> for dad's reaction. </p><p>f) When I learned that the earth's orbit around the sun is an ellipse and that the earth was one of the foci, I wondered where the other foci is and if its important. I still wonder about this one. Google has not helped me here, though perhaps I have not phrased the question properly. If you know the answer please leave a comment. </p><p>g) I also thought about The Ketchup problem and other problems, that I won't go into since I already blogged about them  <a href="https://blog.computationalcomplexity.org/2012/06/ketchup-problem.html">here</a></p><p>4) I was on the math team in high school, but wasn't very good at it. I WAS good at making up math team problems. I am now on the committee that makes up the Univ of MD HS math competition. I am still not good at solving the problems but good at making them up. </p><p>5) From 9th grade on before I would study for an exam by making up what I thought would be a good exam and doing that. Often my exam was a better test of knowledge than the exam given. In college I helped people in Math and Physics by making up exams for them to work on as practice. </p><p>6) I was good at reading, understanding, and explaining papers. </p><p>7) I was never shy about asking for help. My curiosity exeeded by ego... by a lot!</p><p>8) Note that items 5,6, and 7 above do not mention SOLVING problems. The papers I have written are of three (overlapping) types:</p><p>a) I come up with the problem, make some inroads on it based on knowledge, and then have people cleverer (this is often) or with more knowledge (this is rarer) help me solve the problems.</p><p>b) I come up with the problem, and combine two things I know from other papers to solve it. </p><p>c) Someone else asks for my help on something and I have the knowledge needed. I can only recall one time where this lead to a paper. </p><p>NOTE- I do not think I have ever had a clever or new technique. CAVEAT: the diff between combining  known knowledge in new ways and having a clever or new technique is murky. </p><p>8) Over time these strengths and weaknesses have gotten more extreme. It has become a self-fulfilling prophecy where I spend A LOT of time making up problems without asking for help, but when I am trying to solve a problem I early on ask for help. Earlier than I should? Hard to know. </p><p>9) One aspect is `how good am I at math' But a diff angle is that I like to work on things that I KNOW are going to work out, so reading an article is better than trying to create new work. This could be a psychological thing. But is that nature or nurture?  </p><p>10) Could I be a better problem solver? Probably. Could I be a MUCH better problem solver? NO. Could I have been a better problem solver  I did more work on that angle when I was younger? Who knows? </p><p>11) Back to the Quintic: I had the following thought in ninth grade, though I could not possibly have expressed it: The question of, given a problem, how hard is it, upper and lower bounds, is a fundamental one that is worth a lifetime of study. As such my interest in complexity theory and recursion theory goes back to ninth grade or even further. My interest in Ramsey Theory for its own sake (and not in the service of complexity theory) is much more recent and does not quite fit into my narrative. But HEY- real life does not have as well defined narratives as fiction does. </p><p>12) Timing and Luck: IF I had been in grad student at a slight diff time I can imagine doing work on  algorithmic  Galois theory. <a href="https://singer.math.ncsu.edu/Algorithmic_slides.pdf">Here</a>  is a talk on Algorithmic  Galois theory. Note that one of the earliest results is by Landau and Miller from 1985---I had a course from Miller on Alg. Group Theory in 1982. This is NOT a wistful `What might have been' thought. Maybe I would have sucked at it, so its just as well I ended up doing recursion theory, then Ramsey theory, then recursion-theoretic Ramsey Theory, then muffins. </p><p><br /></p><p><br /></p><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html"><span class="datestr">at October 19, 2020 03:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7803">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">Understanding generalization requires rethinking deep learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em><a href="https://yaminibansal.com/about/">Yamini Bansal</a>, <a href="https://www.galkaplun.com/">Gal Kaplun</a>, and Boaz Barak</em></p>



<p>(See also <em><a href="https://arxiv.org/abs/2010.08508">paper on arxiv</a></em>,  <a href="https://gitlab.com/harvard-machine-learning/rationality-generalization">code on gitlab</a>,  <a href="https://cmsa.fas.harvard.edu/10-28-2020-new-technologies-in-mathematics-seminar/">upcoming talk by Yamini&amp;Boaz</a>,  <a href="https://youtu.be/89ixhju1hJ0">video of past talk</a>)</p>



<p>A central <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">puzzle</a> of deep learning is the question of <em>generalization</em>. In other words, what can we deduce from the <em>training performance</em> of a neural network about its <em>test performance</em> on <em>fresh unseen examples</em>. An <a href="https://arxiv.org/abs/1611.03530">influential paper</a> of Zhang, Bengio, Hardt, Recht, and Vinyals showed that the answer could be “nothing at all.” </p>



<p>Zhang et al. gave examples where modern deep neural networks achieve 100% accuracy on classifying their training data, but their performance on unseen data may be no better than chance. Therefore we cannot give meaningful guarantees for deep learning using traditional “generalization bounds” that bound the difference between test and train performance by some quantity that tends to zero as the number of datapoints <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> increases. This is why (to quote their title), Zhang et al. claimed that <strong>“understanding deep learning requires rethinking generalization”</strong>.</p>



<p>But what if the issue isn’t that we’ve been doing generalization bounds wrong, but rather that we’ve been doing deep learning (or more accurately, supervised deep learning) wrong?</p>



<h3>Self Supervised + Simple fit (SSS) learning</h3>



<p>To explain what we mean, let’s take a small detour to contrast “traditional” or “end-to-end” supervised learning with a different approach to supervised learning, which we’ll call here “Self-Supervised + Simple fit” or “SSS algorithms.” (While the name “SSS algorithms” is new, the approach itself has a <a href="http://people.idsia.ch/~juergen/FKI-126-90_%28revised%29bw_ocr.pdf">long history</a> and has recently been used with great success in practice; our work gives no new methods—only new analysis.)</p>



<p>The classical or “end-to-end” approach for supervised learning can be phrased as <em>“ask and you shall receive”</em>. Given labeled data, you ask (i.e., run an optimizer) for a complex classifier (e.g., a deep neural net) that fits the data (i.e., outputs the given labels on the given data points) and hope that it will be successful on future, unseen, data points as well. End-to-end supervised learning achieves state-of-art results for many classification problems, particularly for computer vision datasets ImageNet and CIFAR-10.</p>



<p>However, end-to-end learning does not directly correspond to the way humans learn to recognize objects (see also <a href="https://youtu.be/7I0Qt7GALVk?t=2475">this talk of LeCun</a>). A baby may see millions of images in the first year of her life, but most of them do not come with explicit labels. After seeing those images, a baby can make future classifications using very few labeled examples. For example, it might be enough to show her once what is a dog and what is a cat for her to correctly classify future dogs and cats, even if they look quite different from these examples.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/LXRaTQq.png" alt="End-to-end learning vs SSS algorithms." /><strong>Figure 1:</strong> Cartoon of end-to-end vs SSS learning </figure>



<p>In recent years, practitioners have proposed algorithms that are more similar to human learning than supervised learning. Such methods separate the process into two stages. In the <em>first stage</em>, we do <strong>representation learning</strong> whereby we use <em>unlabeled</em> data to learn a <em>representation</em>: a complex map (e.g., a deep neural net) mapping the inputs into some “representation space.” In the <em>second stage</em>, we fit a simple classifier (e.g., a linear threshold function) to the representation of the datapoints and the given labels. We call such algorithms <strong>“Self-Supervision + Simple fit”</strong> or <strong>SSS algorithms</strong>. (Note that, unlike other representation-learning based classifiers, the complex representation is “frozen” and not “fine-tuned” in the second stage, where only a simple classifier is used on top of it.)</p>



<p>While we don’t have a formal definition, a “good representation” should make downstream tasks easier, in the sense of allowing for fewer examples or simpler classifiers. We typically learn a representation via <strong>self supervision</strong> , whereby one finds a representation minimizing an objective function that intuitively requires some “insight” into the data. Approaches for self-supervision include reconstruction, where the objective involves recovering data points from partial information (e.g., recover missing <a href="https://arxiv.org/abs/1810.04805">words</a> or <a href="https://arxiv.org/abs/1604.07379">pixels</a>), and <em><a href="https://arxiv.org/abs/2002.05709">contrastive learning</a></em>, where the objective is to find a representation that make similar points close and dissimilar points far (e.g., in Euclidean space).</p>



<p>SSS algorithms have been traditionally used in natural language processing, where unlabeled data is plentiful, but labeled data for a particular task is often scarce. But recently SSS algorithms were also <a href="https://arxiv.org/abs/1902.06162">used with great success</a> even for vision tasks such as ImageNet and CIFAR10 where <em>all data is labeled!</em> While SSS algorithms do not yet beat the state-of-art supervised learning algorithms, they do get <a href="https://arxiv.org/abs/2006.10029">pretty close</a>. SSS algorithms also have other practical advantages over “end-to-end supervised learning”: they can make use of unlabeled data, the representation could be useful for non-classification tasks, and may have improved out of distribution performance. There has also been recent theoretical analysis of contrastive and reconstruction learning under certain statistical assumptions (see <a href="https://arxiv.org/abs/1902.09229">Arora et al</a> and  <a href="https://arxiv.org/abs/2008.01064">Lee et al</a>).</p>



<h3>The generalization gap of SSS algorithms</h3>



<p>In <a href="https://arxiv.org/abs/2010.08508">a recent paper</a>, we show that SSS algorithms not only work in practice, but work in theory too.</p>



<p>Specifically, we show that such algorithms have <strong>(1)</strong> small generalization gap and <strong>(2)</strong> we can <strong>prove</strong> (under reasonable assumptions) that their generalization gap tends to zero with the number of samples, with bounds that are meaningful for many modern classifiers on the CIFAR-10 and ImageNet datasets. We consider the setting where all data is labeled, and the <em>same dataset</em> is used for both learning the representation and fitting a simple classifier. The resulting classifier includes the overparameterized representation, and so we cannot simply apply “off the shelf” generalization bounds. Indeed, a priori it’s not at all clear that the generalization gap for SSS algorithms should be small.</p>



<p>To get some intuition for the generalization gap of SSS algorithms, consider the experiment where we inject some <em>label noise</em> into our distribution. That is, we corrupt an <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" title="\eta" /> fraction of the labels in both the train and test set, replacing them with random labels. Already in the noiseless case (<img src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta=0" class="latex" title="\eta=0" />), the generalization gap of SSS algorithms is noticeably smaller than that of end-to-end supervised learning. As we increase the noise, the difference becomes starker. End-to-end supervised learning algorithms can always achieve 100% training accuracy, even as the test accuracy deteriorates, since they can “memorize” all the training labels they are given. In contrast, for SSS algorithms, both training and testing accuracy decrease together as we increase the noise, with training accuracy correlating with test performance. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"><img src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif?w=812" alt="" class="wp-image-7826" /></a><strong>Figure 2:</strong> Generalization gap of end-to-end and SSS algorithms on CIFAR 10 as a function of noise (since there are 10 classes, 90% noisy samples corresponds to the Zhang et al experiment). See also <a href="https://plotly.com/~yaminibansal/1.embed" target="_blank" rel="noreferrer noopener">interactive version</a>.</figure>



<p></p>



<p>Our main theoretical result is a formal proof of the above statement. To do so, we consider training with a small amount of label noise (say <img src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta=5\%" class="latex" title="\eta=5\%" />) and define the following quantities:</p>



<ul><li>The <strong>robustness gap</strong> is the amount by which training accuracy degrades between the “clean” (<img src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta=0" class="latex" title="\eta=0" />) experiment and the noisy one. (In this and all other quantities, the training accuracy is measured with respect to the original uncorrupted labels.)</li><li>The <strong>memorization gap</strong> considers the noisy experiment (<img src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta=5\%" class="latex" title="\eta=5\%" />) and measures the amount by which performance on the corrupted data samples (where we received the wrong label) is worse than performance on the overall training set. If the algorithm can memorize all given labels, it will be perfectly wrong on the corrupted data samples, leading to a large memorization gap.</li><li>The <strong>rationality gap</strong> is the difference between the performance on the corrupted data samples and performance on unseen test examples. For example, if <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is an image of a dog, then it measures the difference between the probability that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=\text{&quot;dog&quot;}" class="latex" title="f(x)=\text{&quot;dog&quot;}" /> when <img src="https://s0.wp.com/latex.php?latex=%28x%2C%5Ctext%7B%22cat%22%7D%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,\text{&quot;cat&quot;})" class="latex" title="(x,\text{&quot;cat&quot;})" /> is in the training set and the probability that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=\text{&quot;dog&quot;}" class="latex" title="f(x)=\text{&quot;dog&quot;}" /> when <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is not in the training set at all. Since intuitively, getting the wrong label should be worse than getting no label at all, we typically expect the rationality gap to be around zero or negative. Formally we define the rationality gap to the maximum between <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> and the difference above, so it is always non-negative. We think of an algorithm with a significant positive rationality gap as “irrational.”</li></ul>



<p>By summing up the quantities above, we get the following inequality, which we call the <strong>RRM bound</strong></p>



<p><em><span style="color: #0693e3;" class="has-inline-color">generalization gap</span></em> <img src="https://s0.wp.com/latex.php?latex=%5Cleq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\leq" class="latex" title="\leq" /> <em><span style="color: #00d084;" class="has-inline-color">robustness gap</span></em> <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="+" class="latex" title="+" /> <em><span style="color: #fcb900;" class="has-inline-color">rationality gap</span></em> <img src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" alt="+" class="latex" title="+" /> <em><span style="color: #cf2e2e;" class="has-inline-color">memorization gap</span></em></p>



<p>In practice, the <strong>robustness</strong> and <strong>rationality</strong> gaps are always small, both for end-to-end supervised algorithms (which have a large generalization gap), and for SSS algorithms (which have a small generalization gap). Thus the main contribution to the generalization gap comes from the <strong>memorization gap</strong>. Roughly speaking, our main result is the following:</p>



<p><em>If the complexity of the second-stage classifier of an SSS algorithm is smaller than the number of samples then the generalization gap is small.</em></p>



<p>See the <a href="https://arxiv.org/abs/2010.08508">paper</a> for the precise definition of “complexity,” but it is bounded by the number of bits that it takes to describe the simple classifier (no matter how complex is the representation used in the first stage). Our bound yields non-vacuous results in various practical settings; see the figures below or their <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" target="_blank" rel="noreferrer noopener">interactive version</a>. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png"><img src="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png?w=1024" alt="" class="wp-image-7815" /></a><strong>Figure 3:</strong> Empirical study of the generalization gap of a variety of of SSS algorithms on CIFAR-10. Each vertical line corresponds to one model, sorted by generalization gap. The RRM bound is typically near-tight, and our complexity upper bound is often non vacuous. Use <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" target="_blank" rel="noreferrer noopener">this webpage</a> to interact with figures 3 and 4.</figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png"><img src="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png?w=1024" alt="" class="wp-image-7817" /></a><strong>Figure 4:</strong> Empirical study of gaps for the ImageNet dataset. Because of limited computational resources, we only evaluated the theoretical bound for two models in this dataset.</figure>



<h3>What’s next</h3>



<p>There are still many open questions. Can we prove rigorous bounds on robustness and rationality? We have some preliminary results in the paper, but there is much room for improvement. Similarly, our complexity-based upper bound is far from tight at the moment, though the RRM bound itself is often surprisingly tight. Our work only applies to SSS algorithms, but people have the intuition that even end-to-end supervised learning algorithms implicitly learn a representation. So perhaps these tools can apply to such algorithms as well. As mentioned, we don’t yet have formal definitions for “good representations,” and the choice of the self-supervision task is still somewhat of a “black art” – can we find a more principled approach?</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/"><span class="datestr">at October 19, 2020 12:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html">Polyhedra without disjoint faces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from <a href="https://en.wikipedia.org/wiki/Forbidden_graph_characterization">forbidden minors</a> to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), <a href="https://en.wikipedia.org/wiki/Wheel_graph">wheel graphs</a>, or the graph \(K_5-e\) of the <a href="https://en.wikipedia.org/wiki/Triangular_bipyramid">triangular bipyramid</a>. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/prism-k33-free.svg" alt="A (prism, K_{3,3})-minor-free graph, with its nontrivial triconnected components colored red and yellow" /></p>

<p>Some definitions:</p>

<ul>
  <li>
    <p>Here by the prism graph I mean the graph of the triangular prism. Any other prism has this one as a minor, and so is irrelevant as a forbidden minor. However, the pyramids in this structure can have any polygon as their base, corresponding to wheel graphs with arbitrarily many vertices.</p>
  </li>
  <li>
    <p>\(K_{3,3}\) is a complete bipartite graph with three vertices on each side of its bipartition, famous as the <a href="https://en.wikipedia.org/wiki/Three_utilities_problem">utility graph</a>, one of the two forbidden minors for planar graphs. The triangular prism graph and \(K_{3,3}\) are the only two <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular graphs</a> with six vertices.</p>
  </li>
</ul>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/prism-k33.svg" alt="The prism graph and K_{3,3}" /></p>

<ul>
  <li>
    <p>The triconnected components of a graph are the graphs associated with the nodes of its <a href="https://en.wikipedia.org/wiki/SPQR_tree">SPQR tree</a>, or of the SPQR trees of its biconnected components. These are cycle graphs, dipole multigraphs, or 3-connected graphs, and by “nontrivial” I mean the ones that are not cycles or dipoles. A triconnected component might not be a subgraph of the given graph, because it can have additional edges that correspond to paths in the given graph. For instance, subdividing the edges of any graph into paths, or more generally replacing edges by arbitrary series-parallel graphs, does not change its set of nontrivial triconnected components.</p>
  </li>
  <li>
    <p>I’m using “face” in the usual three-dimensional meaning, a two-dimensional subset of the boundary of the polyhedron. For higher-dimensional polytopes, “face” has a different meaning that also includes vertices and edges, and “facet” would be used to refer to the \((d-1)\)-dimensional faces, but using that terminology seems overly pedantic here.</p>
  </li>
</ul>

<p>Sketch of proof of the characterization of polyhedra without two disjoint faces: Consider any polyhedron without disjoint faces. If one face shares an edge with all the others, it’s a <a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graph</a>, a graph formed by linking the leaves of a tree into a cycle; if the tree is a star, it’s a pyramid, and otherwise contracting all but one of the interior edges of the tree, and then all but four of the cycle edges, will produce a prism minor. In the remaining case, some two faces share only a vertex \(v\), which must have degree four or more. Each face that is disjoint from \(v\) must touch all that faces incident to \(v\), which can only happen when there is one face disjoint from \(v\) (a pyramid) or two faces disjoint from \(v\), neither of which has an edge disjoint from the other one (a bipyramid).</p>

<p>Sketch of a lemma that every convex polyhedron with two disjoint faces has a prism minor: glue a pyramidal cap into each of the two faces, producing a larger convex polyhedron which by either <a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> or <a href="https://en.wikipedia.org/wiki/Balinski%27s_theorem">Balinski’s theorem</a> is necessarily 3-connected, and find three vertex-disjoint paths between the apexes of the attached pyramids. The parts of these paths outside the two glued pyramids, together with the boundaries of the two faces, form a subdivision of a prism.</p>

<p>Sketch of proof of the characterization of (prism,\(K_{3,3}\))-minor-free graphs: The nontrivial triconnected components are exactly the maximal triconnected minors of the given graph, so if either of the two triconnected forbidden minors is to be found in the given graph, it will be found in one of the triconnected components. \(K_5\) and the triangular bipyramid are too small to have one of the forbidden minors. The only 3-connected minors of the pyramid graphs are smaller pyramids, obtained by contracting one of the cycle edges of the pyramid, so these also do not have a forbidden minor. Therefore the graphs of the stated form are all (prism,\(K_{3,3}\))-minor-free.</p>

<p>In the other direction, suppose that a graph is (prism,\(K_{3,3}\))-minor-free.
Each triconnected component is a minor, so it must also be (prism,\(K_{3,3}\))-minor-free. What can these components look like? Forbidding \(K_{3,3}\) as a minor rules out nonplanar components other than \(K_5\), by a theorem of Wagner<sup id="fnref:wagner"><a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:wagner" class="footnote">1</a></sup> and Hall.<sup id="fnref:hall"><a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:hall" class="footnote">2</a></sup> So the remaining components that we need to consider are triconnected planar graphs with no prism minor. These cannot have two disjoint faces by the lemma, and so they can only be pyramids or the triangular bipyramid.</p>

<div class="footnotes">
  <ol>
    <li id="fn:wagner">
      <p>K. Wagner. Über eine Erweiterung des Satzes von Kuratowski. <em>Deutsche Mathematik</em>, 2:280–285, 1937. <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:wagner" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:hall">
      <p>D. W. Hall. A note on primitive skew curves. <em>Bulletin of the American Mathematical Society</em>, 49(12):935–936, 1943. <a href="https://doi.org/10.1090/ S0002-9904-1943-08065-2">doi:10.1090/ S0002-9904-1943-08065-2</a>. <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:hall" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105058649830809584">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html"><span class="datestr">at October 18, 2020 05:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/155">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/155">TR20-155 |  Log-rank and lifting for AND-functions | 

	Sam McGuire, 

	Shachar Lovett, 

	Alexander Knop, 

	Weiqiang Yuan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x, y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes bit-wise AND. We study the deterministic communication complexity of $f_\land$ and show that, up to a $\log n$ factor, it is bounded by a polynomial in the logarithm of the real rank of the communication matrix of $f_\land$. This comes within a $\log n$ factor of establishing the log-rank conjecture for AND-functions with no assumptions on $f$. Our result stands in contrast with previous results on special cases of the log-rank 
conjecture, which needed significant restrictions on $f$ such as monotonicity or low $\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log n$ factor) a lifting theorem for AND-functions, stating that the deterministic communication complexity of $f_\land$ is polynomially-related to the AND-decision tree complexity of $f$.

The results rely on a new structural result regarding boolean functions $f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may be of independent interest. We show that if the polynomial computing $f$ has few monomials then the set system of the monomials has a small hitting set, of size poly-logarithmic in its sparsity. We also establish extensions of this result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger range.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/155"><span class="datestr">at October 18, 2020 02:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=493">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/">TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathsf{NC}^0" class="latex" title="\mathsf{NC}^0" /> with arbitrary polynomial stretch, that is, mapping <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> bits to <img src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="n^{1+\tau}" class="latex" title="n^{1+\tau}" /> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathbb{Z}_p" class="latex" title="\mathbb{Z}_p" /> with error-rate <img src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\ell^{-\delta}" class="latex" title="\ell^{-\delta}" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\ell" class="latex" title="\ell" /> is the dimension of the secret and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\delta&gt;0" class="latex" title="\delta&gt;0" /> is an arbitrarily small constant.<br />Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathbb{Z}_p" class="latex" title="\mathbb{Z}_p" /> and PRGs in <img src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathsf{NC}^0" class="latex" title="\mathsf{NC}^0" />. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br /><br />Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/"><span class="datestr">at October 16, 2020 06:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/10/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/10/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/mirzakhani-and-meanders/">Mirzakhani and meanders</a> (<a href="https://mathstodon.xyz/@11011110/104963847400612388">\(\mathbb{M}\)</a>). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich <a href="https://arxiv.org/abs/1705.05190">in a new preprint</a> for numbers of <a href="https://en.wikipedia.org/wiki/Meander_(mathematics)">meanders</a>, closed curves with a given number of intersections with a line.</p>
  </li>
  <li>
    <p><a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">Retraction of “a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, a key ingredient in the proof of the quantum low-degree test, itself a key ingredient in the \(\mathsf{MIP}^*=\mathsf{RE}\) paper”</a> (<a href="https://mathstodon.xyz/@11011110/104969573344196233">\(\mathbb{M}\)</a>). \(\mathsf{MIP}^*=\mathsf{RE}\) is patched and remains believed true but not fully refereed. This post provides a lot more than the standard we-found-a-bug notice: a good description of what happened, what it implies technically, and how it affects the authors and community.</p>
  </li>
  <li>
    <p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2020/09/30/for-academic-publishing-to-be-trans-inclusive-authors-must-be-allowed-to-retroactively-change-their-names/">For academic publishing to be trans-inclusive, authors must be allowed to retroactively change their names</a> (<a href="https://mathstodon.xyz/@11011110/104972193066839079">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/10/03/weekend-reads-unicorn-poo-and-other-fraudulent-covid-19-treatments-disgraced-researchers-and-drug-company-payouts-a-fictional-account-of-real-fraud/">via</a>). I agree — more than once in researching Wikipedia bios I found past publications under deadnames. If the authors prefer this to be better hidden, while continuing to be credited for their past work, we should try to honor that preference.</p>
  </li>
  <li>
    <p>It’s easy to point and laugh at the <a href="https://tex.stackexchange.com/questions/565387/mathbb-r-is-not-showing-in-reference-bibtex">researcher who thought bibtex from Google scholar was usable</a> (<a href="https://mathstodon.xyz/@11011110/104980666583964923">\(\mathbb{M}\)</a>), but their question brings up a more serious question: why is Google’s bibtex so bad? Even the junk I get from <code class="language-plaintext highlighter-rouge">curl -LH "Accept: application/x-bibtex" http://doi.org/...</code> is mostly usable in comparison. I’m tempted to suggest that they go to MathSciNet for the good stuff but I’m worried they won’t have access.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/09/30/ten-kinetic-sculptures-by-anne-lilly.html">Ten kinetic sculptures by Anne Lilly</a> (<a href="https://mathstodon.xyz/@11011110/104988795486796768">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://jix.one/the-assembly-language-of-satisfiability/">The assembly language of satisfiability</a> (<a href="https://mathstodon.xyz/@jix/104971574457861322">\(\mathbb{M}\)</a>). Why Boolean satisfiability is too low-level to work well as a way to express the kind of problems satisfiability-solvers can solve, and how <a href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">satisfiability modulo theories</a> can help.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/10/01/subsumptions-of-regular-polytopes/">Which regular polytopes have their vertices a subset of other regular polytopes in the same dimension</a> (<a href="https://mathstodon.xyz/@11011110/104998010300898992">\(\mathbb{M}\)</a>)? We don’t know! The answer is closely connected to the existence of <a href="https://en.wikipedia.org/wiki/Hadamard_matrix">Hadamard matrices</a>, which are famously conjectured to exist in dimensions divisible by four. A solution to the Hadamard matrix existence problem would also solve the polytope problem.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/computer-scientists-break-traveling-salesperson-record-20201008/">Computer scientists break traveling salesperson record</a> (<a href="https://mathstodon.xyz/@11011110/105006269895209659">\(\mathbb{M}\)</a>). I <a href="https://11011110.github.io/blog/2020/07/15/linkage.html">linked to this back in July</a> when <a href="https://arxiv.org/abs/2007.01409">Karlin, Klein, and Gharan’s preprint</a> giving a \((1/2-\varepsilon)\)-approximation to TSP first came out, but now it’s getting wider publicity in <em>Quanta</em>. See also <a href="https://www.sciencenews.org/article/shayan-oveis-gharan-theoretical-computer-scientist-sn-10-scientists-watch">an earlier (paywalled) piece on the same story in <em>ScienceNews</em></a>.</p>
  </li>
  <li>
    <p>Symmetry, quasisymmetry, and kite-rhomb tessellations in the mathematical modeling of virus surface structures: <a href="https://ima.org.uk/721/fighting-infections-with-symmetry/">IMA</a>,
<a href="https://inference-review.com/article/mathematical-virology"><em>Inference</em></a>,
<a href="https://archive.bridgesmathart.org/2018/bridges2018-237.pdf">Bridges</a> (<a href="https://mathstodon.xyz/@11011110/105009372623320055">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://shop.deutschepost.de/freies-quadrat-briefmarke-zu-1-70-eur-10er-bogen">New German postage stamp features the missing square puzzle</a> (<a href="https://muensterland.social/@rgx/105007333917605810">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://www.newstatesman.com/international/science-tech/2020/07/ra-fisher-and-science-hatred">R. A. Fisher and the science of hatred</a> (<a href="https://mathstodon.xyz/@11011110/105020588148970072">\(\mathbb{M}\)</a>). If you’ve been wondering why noted academics of yesteryear like <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R. A. Fisher</a> (a major figure in statistics) and <a href="https://en.wikipedia.org/wiki/David_Starr_Jordan">David Starr Jordan</a> (founding president of Stanford University) have been having their names taken off things lately, the link looks like a good explainer of their views on eugenics, and why those views are now regarded as deeply racist, even for their times.</p>
  </li>
  <li>
    <p><a href="http://hardmath123.github.io/minimal-surface.html">Sol LeWitt and the soapy pit</a> (<a href="https://mathstodon.xyz/@11011110/105023612862469185">\(\mathbb{M}\)</a>, <a href="https://abhikjain360.github.io/2020/08/01/The-186th-Carnival-of-Mathematics.html">via</a>, <a href="https://aperiodical.com/2020/10/carnival-of-mathematics-186/">via2</a>). LeWitt was an artist who in 1974 made a piece exhibiting all of the possible subsets of edges of the cube. The comfortably numbered blog examines what you get if you use these as frames for making soap films.</p>
  </li>
  <li>
    <p><a href="http://landezine.com/index.php/2013/02/funenpark-by-landlab/">Funenpark</a> (<a href="https://mathstodon.xyz/@11011110/105029637909838642">\(\mathbb{M}\)</a>). To be clear, Funenpark is not a fun-park. It is a high-density residential development on former industrial land near Amsterdam. What interests me is their <a href="https://www.flickr.com/photos/shiratski/2242870712/">pentagonal tiles</a>. It’s not one of the <a href="https://en.wikipedia.org/wiki/Pentagonal_tiling">15 monohedral pentagon tilings</a>: the tiles have two shapes, one forming half of a regular hexagon (all angles \(&gt; 60^\circ\)) and another surrounding the hexagons (sharp angle \(= 60^\circ\)). Still, a nice pattern.</p>
  </li>
  <li>
    <p>Sometimes when I’ve been doing big literature searches on jstor (manually clicking on dozens of links because jstor’s search results don’t tell me which book is being reviewed, delayed by maybe a second or so per click so that I don’t get stopped by jstor’s anti-bot filters) I then get locked out of Google Scholar for a day or so on the same IP address because Google thinks I’m a bot. It doesn’t happen when I search Scholar directly. Has anyone else noticed this? Any idea how to avoid it? (<a href="https://mathstodon.xyz/@11011110/105037500352288970">\(\mathbb{M}\)</a>)</p>
  </li>
  <li>
    <p>While I’m linking Dutch pentagonal tiling architecture, here’s <a href="https://www.19hetatelier.nl/nieuws/wiskundige-vijfhoek-op-gevel-basisschool-de-garve-lochem/">an elementary school in Lochem decorated with the Mann–McLoud–Von Derau tile</a> (<a href="https://mathstodon.xyz/@11011110/105042753206122004">\(\mathbb{M}\)</a>, <a href="https://twitter.com/alexvdbrandhof/status/1004661466149085184">via</a>), which in 2015 became the 15th and final Euclidean monohedral pentagonal tile to be found. The link is in Dutch but Google translate works well except at one point: the school’s name, “De Garve”, means “the sheaf”, and the article remarks that this is appropriate for a pattern that looks like ears of corn.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/10/15/linkage.html"><span class="datestr">at October 15, 2020 10:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4895611208507130576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html">50 Years of PBS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The Public Broadcasting Service (PBS) launched fifty years ago this month in the United States. The New York Times talks about its <a href="https://www.nytimes.com/2020/10/13/arts/television/pbs-50-anniversary.html">fifty reasons</a> how the network mattered. I'll throw in my thoughts.</p><p>I was just slightly too old for shows like Sesame Street, Electric Company, Mr. Rogers and Zoom, not that that stopped me from watching them. My kids grew up on Barney and Friends. My daughter even had a toy Barney that interacted with the show, which went <a href="https://blog.computationalcomplexity.org/2012/02/barney-evil-dinosaur.html">as well as you'd expect</a>. </p><p>PBS introduced me to those great British TV shows for young nerds like me including Monty Python and Doctor Who. I wasn't into Nova but did watch Carl Sagan's Cosmos religiously in high school.</p><p>My favorite PBS show was the American Experience, short documentaries about US culture. I remember learning about this history of Coney Island and the quiz show scandals before Robert Redford made a movie about it.</p><p>Siskel and Ebert got their start on PBS and became my go to source for movie reviews.</p><p>In 1987 PBS broadcasted Ivy League football games. One Saturday I sat down expecting to watch my alma mater and instead got supreme court hearings. Only on PBS could Cornell football get Borked.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html"><span class="datestr">at October 15, 2020 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7800">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/">ITC 2021 (guest post by Benny Applebaum)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Following last year’s successful launch, we are happy to announce the second edition of the conference on <a href="https://itcrypto.github.io/" target="_blank" rel="noreferrer noopener"><em>Information-Theoretic Cryptography</em></a><em> (ITC)</em>.</p>



<p>The <a href="https://itcrypto.github.io/2021/" target="_blank" rel="noreferrer noopener">call for papers</a> for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song <a href="https://youtu.be/kZT1icVoTp8" target="_blank" rel="noreferrer noopener">https://youtu.be/kZT1icVoTp8</a>  </p>



<p>Feel free to add your own verse <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;" class="wp-smiley" alt="😉" /></p>



<p>The submission deadline is <strong>February 1st</strong>. Please submit your best work to ITC 2021! We hope to see many of you there!</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/"><span class="datestr">at October 14, 2020 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
