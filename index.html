<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 19, 2021 07:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1517">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1517">Announcing WOLA’21 (Workshop on Local Algorithms)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The fifth <a href="http://www.local-algorithms.com">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter </li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>To stay up to date with the latest announcements concerning WOLA, <a href="https://lists.csail.mit.edu/mailman/listinfo/wola">join our mailing list</a>!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1517"><span class="datestr">at May 19, 2021 12:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07612">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07612">PPCA: Privacy-preserving Principal Component Analysis Using Secure Multiparty Computation(MPC)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fan:Xiaoyu.html">Xiaoyu Fan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Guosai.html">Guosai Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Kun.html">Kun Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Xu.html">Xu He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Wei.html">Wei Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07612">PDF</a><br /><b>Abstract: </b>Privacy-preserving data mining has become an important topic. People have
built several multi-party-computation (MPC)-based frameworks to provide
theoretically guaranteed privacy, the poor performance of real-world algorithms
have always been a challenge. Using Principal Component Analysis (PCA) as an
example, we show that by considering the unique performance characters of the
MPC platform, we can design highly effective algorithm-level optimizations,
such as replacing expensive operators and batching up. We achieve about
200$\times$ performance boost over existing privacy-preserving PCA algorithms
with the same level of privacy guarantee. Also, using real-world datasets, we
show that by combining multi-party data, we can achieve better training
results.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07612"><span class="datestr">at May 18, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07608">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07608">Hamiltonian Cycle Problem is in P</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hou:Aimin.html">Aimin Hou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07608">PDF</a><br /><b>Abstract: </b>In this paper we present the first deterministic polynomial time algorithm
for determining the existence of a Hamiltonian cycle and finding a Hamiltonian
cycle in general graphs. Our algorithm can also resolve the Hamiltonian path
problem in the traceable graphs. The space complexity of our algorithm is
O(n^4). The time complexity are theoretically O(n^5*d^2) on average and
O(n^6*d^2) in the worst case respectively, where d is the maximum degree of
vertex. With parallel computing, the space complexity can be improved to O(n^3)
and the time complexity to O(n^3*d^2) on average and O(n^4*d^2) in the worst
case. We construct the corresponding path hologram transformed from the
original graph and compute the path set, which is a collection of segment sets
consisting of all the vertices located on the same segment layer among all the
longest basic paths, of every vertex with dynamic programming. The path
hologram is a multi-segment graph with the vertex &lt;u, k&gt; where u is a vertex
and k is the segment layer of u in the path hologram. To ensure each path
stored in the path set is legal and each segment set of the path set contains
only valid vertices, the key strategy of our method is the "consecutive"
deleting-replenishing operations recursively on the left/right action field of
a vertex, respectively. The greatest contribution of our method is the path set
in which the (n-1)! paths can be stored in O(n^2) space for a complete graph of
order n. In fact, our algorithm can be directly applied to the original graph.
Besides, our algorithm can deal with the finite general graphs including
undirected, directed, and mixed. As a well-known problem in NPC, the
Hamiltonian Cycle Problem can be now resolved practically in deterministic
polynomial time, so this problem is in P and we prove that the conjecture of
P=NP holds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07608"><span class="datestr">at May 18, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07560">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07560">Dynamic Routing and Spectrum Assignment based on the Availability of Consecutive Sub-channels in Flexi-grid Optical Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lohani:Varsha.html">Varsha Lohani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharma:Anjali.html">Anjali Sharma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Yatindra_Nath.html">Yatindra Nath Singh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07560">PDF</a><br /><b>Abstract: </b>Using Optical Orthogonal Frequency Multiplexing (O-OFDM), variable bandwidth
channels can be created in Elastic Optical Networks (EON). This allows the use
of spectrum more efficiently by allocating integral multiple of basic bandwidth
slots to the lightpath requests. Consequently, such networks are also called
flexible grid optical networks. It also adds a constraint of keeping all the
allocated slots together when deciding the routes for the requests. This
constraint called the contiguity constraint makes the routing and spectrum
algorithms more challenging. In any network, the lightpath requests will arrive
and depart dynamically and will invariably lead to spectrum fragmentation, and
hence network will have a reduction in maximum possible utilization due to
increased blocking probability. In this paper, we have presented an improvised
RSA algorithm that leads to lesser fragmentation. It is evident from the
results that the presented RSA algorithm uses adaptive parameters to reduce the
blocking probability and fragmentation compared to other algorithms reported in
the recent past.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07560"><span class="datestr">at May 18, 2021 10:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07531">First-Order Reasoning and Efficient Semi-Algebraic Proofs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Part:Fedor.html">Fedor Part</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thapen:Neil.html">Neil Thapen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzameret:Iddo.html">Iddo Tzameret</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07531">PDF</a><br /><b>Abstract: </b>Semi-algebraic proof systems such as sum-of-squares (SoS) have attracted a
lot of attention recently due to their relation to approximation algorithms:
constant degree semi-algebraic proofs lead to conjecturally optimal
polynomial-time approximation algorithms for important NP-hard optimization
problems. Motivated by the need to allow a more streamlined and uniform
framework for working with SoS proofs than the restrictive propositional level,
we initiate a systematic first-order logical investigation into the kinds of
reasoning possible in algebraic and semi-algebraic proof systems. Specifically,
we develop first-order theories that capture in a precise manner constant
degree algebraic and semi-algebraic proof systems: every statement of a certain
form that is provable in our theories translates into a family of constant
degree polynomial calculus or SoS refutations, respectively; and using a
reflection principle, the converse also holds.
</p>
<p>This places algebraic and semi-algebraic proof systems in the established
framework of bounded arithmetic, while providing theories corresponding to
systems that vary quite substantially from the usual propositional-logic ones.
</p>
<p>We give examples of how our semi-algebraic theory proves statements such as
the pigeonhole principle, we provide a separation between algebraic and
semi-algebraic theories, and we describe initial attempts to go beyond these
theories by introducing extensions that use the inequality symbol, identifying
along the way which extensions lead outside the scope of constant degree SoS.
Moreover, we prove new results for propositional proofs, and specifically
extend Berkholz's dynamic-by-static simulation of polynomial calculus (PC) by
SoS to PC with the radical rule.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07531"><span class="datestr">at May 18, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07529">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07529">Infinitely growing configurations in Emil Post's tag system problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurilenko:Nikita_V=.html">Nikita V. Kurilenko</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07529">PDF</a><br /><b>Abstract: </b>Emil Post's tag system problem is the question of whether or not a tag system
$\{N=3, P(0)=00, P(1)=1101\}$ has a configuration, simulation of which will
never halt or end up in a loop. For the past decades, there were several
attempts to find an answer to this question, including a recent study by
Wolfram (2021), during which the first $2^{84}$ initial configurations were
checked. This paper presents a family of configurations of this type in a form
of strings $a^{n} b c^{m}$, that evolve to $a^{n+1} b c^{m+1}$ after a finite
amount of steps. The proof of this behavior for all non-negative $n$ and $m$ is
described further in a paper as a finite verification procedure, which is
computationally bounded by 20000 iterations of tag. All corresponding code can
be found at
https://github.com/nikitakurilenko/post-tag-infinitely-growing-configurations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07529"><span class="datestr">at May 18, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07525">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07525">Monomial-size vs. Bit-complexity in Sums-of-Squares and Polynomial Calculus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hakoniemi:Tuomas.html">Tuomas Hakoniemi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07525">PDF</a><br /><b>Abstract: </b>In this paper we consider the relationship between monomial-size and
bit-complexity in Sums-of-Squares (SOS) in Polynomial Calculus Resolution over
rationals (PCR/$\mathbb{Q}$). We show that there is a set of polynomial
constraints $Q_n$ over Boolean variables that has both SOS and PCR/$\mathbb{Q}$
refutations of degree 2 and thus with only polynomially many monomials, but for
which any SOS or PCR/$\mathbb{Q}$ refutation must have exponential
bit-complexity, when the rational coefficients are represented with their
reduced fractions written in binary.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07525"><span class="datestr">at May 18, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07518">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07518">Near-Optimal Time-Energy Trade-Offs for Deterministic Leader Election</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chang:Yi=Jun.html">Yi-Jun Chang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duan:Ran.html">Ran Duan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Shunhua.html">Shunhua Jiang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07518">PDF</a><br /><b>Abstract: </b>We consider the energy complexity of the leader election problem in the
single-hop radio network model, where each device has a unique identifier in
$\{1, 2, \ldots, N\}$. Energy is a scarce resource for small battery-powered
devices. For such devices, most of the energy is often spent on communication,
not on computation. To approximate the actual energy cost, the energy
complexity of an algorithm is defined as the maximum over all devices of the
number of time slots where the device transmits or listens. Much progress has
been made in understanding the energy complexity of leader election in radio
networks, but very little is known about the trade-off between time and energy.
</p>
<p>$\textbf{Time-energy trade-off:}$ For any $k \geq \log \log N$, we show that
a leader among at most $n$ devices can be elected deterministically in
$O(n^{1+\epsilon}) + O(k \cdot N^{1/k})$ time and $O(k)$ energy if each device
can simultaneously transmit and listen, where $\epsilon &gt; 0$ is any small
constant. This improves upon the previous $O(N)$-time $O(\log \log N)$-energy
algorithm by Chang et al. [STOC 2017]. We provide lower bounds to show that the
time-energy trade-off of our algorithm is near-optimal.
</p>
<p>$\textbf{Dense instances:}$ For the dense instances where the number of
devices is $n = \Theta(N)$, we design a deterministic leader election algorithm
using only $O(1)$ energy. This improves upon the $O(\log^* N)$-energy algorithm
by Jurdzi\'{n}ski et al. [PODC 2002] and the $O(\alpha(N))$-energy algorithm by
Chang et al. [STOC 2017]. More specifically, we show that the optimal
deterministic energy complexity of leader election is
$\Theta\left(\max\left\{1, \log \frac{N}{n}\right\}\right)$ if the devices
cannot simultaneously transmit and listen, and it is $\Theta\left(\max\left\{1,
\log \log \frac{N}{n}\right\}\right)$ if they can.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07518"><span class="datestr">at May 18, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07517">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07517">A Stress-Free Sum-of-Squares Lower Bound for Coloring</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh_K=.html">Pravesh K. Kothari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manohar:Peter.html">Peter Manohar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07517">PDF</a><br /><b>Abstract: </b>We prove that with high probability over the choice of a random graph $G$
from the Erd\H{o}s-R\'enyi distribution $G(n,1/2)$, a natural
$n^{O(\varepsilon^2 \log n)}$-time, degree $O(\varepsilon^2 \log n)$
sum-of-squares semidefinite program cannot refute the existence of a valid
$k$-coloring of $G$ for $k = n^{1/2 +\varepsilon}$. Our result implies that the
refutation guarantee of the basic semidefinite program (a close variant of the
Lov\'asz theta function) cannot be appreciably improved by a natural $o(\log
n)$-degree sum-of-squares strengthening, and this is tight up to a $n^{o(1)}$
slack in $k$. To the best of our knowledge, this is the first lower bound for
coloring $G(n,1/2)$ for even a single round strengthening of the basic SDP in
any SDP hierarchy.
</p>
<p>Our proof relies on a new variant of instance-preserving non-pointwise
complete reduction within SoS from coloring a graph to finding large
independent sets in it. Our proof is (perhaps surprisingly) short, simple and
does not require complicated spectral norm bounds on random matrices with
dependent entries that have been otherwise necessary in the proofs of many
similar results [BHK+16, HKP+17, KB19, GJJ+20, MRX20].
</p>
<p>Our result formally holds for a constraint system where vertices are allowed
to belong to multiple color classes; we leave the extension to the formally
stronger formulation of coloring, where vertices must belong to unique colors
classes, as an outstanding open problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07517"><span class="datestr">at May 18, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07511">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07511">Efficient Algorithms for Quantitative Attack Tree Analysis</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Budde:Carlos_E=.html">Carlos E. Budde</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stoelinga:Mari=euml=lle.html">Mariëlle Stoelinga</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07511">PDF</a><br /><b>Abstract: </b>Numerous analysis methods for quantitative attack tree analysis have been
proposed. These algorithms compute relevant security metrics, i.e. performance
indicators that quantify how good the security of a system is, such as the most
likely attack, the cheapest, or the most damaging one. This paper classifies
attack trees in two dimensions: proper trees vs. directed acyclic graphs (i.e.
with shared subtrees); and static vs. dynamic gates. For each class, we propose
novel algorithms that work over a generic attribute domain, encompassing a
large number of concrete security metrics defined on the attack tree semantics.
We also analyse the computational complexity of our methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07511"><span class="datestr">at May 18, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07480">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07480">In Congestion Games, Taxes Achieve Optimal Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paccagnan:Dario.html">Dario Paccagnan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gairing:Martin.html">Martin Gairing</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07480">PDF</a><br /><b>Abstract: </b>We consider the problem of minimizing social cost in atomic congestion games
and show, perhaps surprisingly, that efficiently computed taxation mechanisms
yield the same performance achievable by the best polynomial time algorithm,
even when the latter has full control over the players' actions. It follows
that no other tractable approach geared at incentivizing desirable system
behavior can improve upon this result, regardless of whether it is based on
taxations, coordination mechanisms, information provision, or any other
principle. In short: Judiciously chosen taxes achieve optimal approximation.
</p>
<p>Three technical contributions underpin this conclusion. First, we show that
computing the minimum social cost is NP-hard to approximate within a given
factor depending solely on the admissible resource costs. Second, we design a
tractable taxation mechanism whose efficiency (price of anarchy) matches this
hardness factor, and thus is optimal. As these results extend to coarse
correlated equilibria, any no-regret algorithm inherits the same performances,
allowing us to devise polynomial time algorithms with optimal approximation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07480"><span class="datestr">at May 18, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07329">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07329">Dynamic Matching under Spatial Frictions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanoria:Yash.html">Yash Kanoria</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07329">PDF</a><br /><b>Abstract: </b>We consider demand and supply which arise i.i.d. uniformly in the unit
hypercube [0,1]^d in d dimensions, and need to be matched with each other while
minimizing the expected average distance between matched pairs (the "cost"). We
characterize the scaling behavior of the achievable cost in three models as a
function of the dimension d: (i) Static matching of N demand units with N+M
supply units. (ii) A semi-dynamic model where N+M supply units are present
beforehand and N demand units arrive sequentially and must be matched
immediately. (iii) A fully dynamic model where there are always m supply units
present in the system, one supply and one demand unit arrive in each period,
and the demand must be matched immediately. We show that one can achieve nearly
the same cost under the semi-dynamic model as under the static model, despite
uncertainty about the future, and that, under these two models, d=1 is the only
case where cost far exceeds distance to the nearest neighbor (which is
\Theta(1/N^{1/d})) and where adding excess supply M substantially reduces cost
(by smoothing stochastic fluctuations at larger spatial length scales). In the
fully dynamic model, we show that, remarkably, for all d we can achieve a cost
only slightly more than the optimistic distance to the nearest neighbor
\Theta(1/m^{1/d}). Thus, excess supply m reduces cost in the fully dynamic
model for all $d$ by reducing the distance to the nearest neighbor. This is a
fundamentally different phenomenon than that seen in the other two models,
where excess supply reduces cost while leaving the distance to the nearest
neighbor unchanged, only for d=1. Our achievability results are based on
analysis of a certain "Hierarchical Greedy" algorithm which separately handles
stochastic fluctuations at different length scales.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07329"><span class="datestr">at May 18, 2021 10:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07219">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07219">Peak Demand Minimization via Sliced Strip Packing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deppert:Max_A=.html">Max A. Deppert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Klaus.html">Klaus Jansen</a>, Arindam Khan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rau:Malin.html">Malin Rau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tutas:Malte.html">Malte Tutas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07219">PDF</a><br /><b>Abstract: </b>We study Nonpreemptive Peak Demand Minimization (NPDM) problem, where we are
given a set of jobs, specified by their processing times and energy
requirements. The goal is to schedule all jobs within a fixed time period such
that the peak load (the maximum total energy requirement at any time) is
minimized. This problem has recently received significant attention due to its
relevance in smart-grids. Theoretically, the problem is related to the
classical strip packing problem (SP). In SP, a given set of axis-aligned
rectangles must be packed into a fixed-width strip, such that the height of the
strip is minimized. NPDM can be modeled as strip packing with slicing and
stacking constraint: each rectangle may be cut vertically into multiple slices
and the slices may be packed into the strip as individual pieces. The stacking
constraint forbids solutions where two slices of the same rectangle are
intersected by the same vertical line. Nonpreemption enforces the slices to be
placed in contiguous horizontal locations (but may be placed at different
vertical locations).
</p>
<p>We obtain a $(5/3+\epsilon)$-approximation algorithm for the problem. We also
provide an asymptotic efficient polynomial-time approximation scheme (AEPTAS)
which generates a schedule for almost all jobs with energy consumption
$(1+\epsilon)OPT$. The remaining jobs fit into a thin container of height $1$.
The previous best for NPDM was 2.7 approximation based on FFDH [Ranjan et al.
2015]. One of our key ideas is providing several new lower bounds on the
optimal solution of a geometric packing, which could be useful in other related
problems. These lower bounds help us to obtain approximative solutions based on
Steinberg's algorithm in many cases. In addition, we show how to split
schedules generated by the AEPTAS into few segments and to rearrange the
corresponding jobs to insert the thin container mentioned above.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07219"><span class="datestr">at May 18, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07203">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07203">Pebbles, Graphs, and a Pinch of Combinatorics: Towards Tight I/O Lower Bounds for Statically Analyzable Programs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kwasniewski:Grzegorz.html">Grzegorz Kwasniewski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=Nun:Tal.html">Tal Ben-Nun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gianinazzi:Lukas.html">Lukas Gianinazzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Calotoiu:Alexandru.html">Alexandru Calotoiu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schneider:Timo.html">Timo Schneider</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziogas:Alexandros_Nikolaos.html">Alexandros Nikolaos Ziogas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07203">PDF</a><br /><b>Abstract: </b>Determining I/O lower bounds is a crucial step in obtaining
communication-efficient parallel algorithms, both across the memory hierarchy
and between processors. Current approaches either study specific algorithms
individually, disallow programmatic motifs such as recomputation, or produce
asymptotic bounds that exclude important constants. We propose a novel approach
for obtaining precise I/O lower bounds on a general class of programs, which we
call Simple Overlap Access Programs (SOAP). SOAP analysis covers a wide variety
of algorithms, from ubiquitous computational kernels to full scientific
computing applications. Using the red-blue pebble game and combinatorial
methods, we are able to bound the I/O of the SOAP-induced Computational
Directed Acyclic Graph (CDAG), taking into account multiple statements,
input/output reuse, and optimal tiling. To deal with programs that are outside
of our representation (e.g., non-injective access functions), we describe
methods to approximate them with SOAP. To demonstrate our method, we analyze 38
different applications, including kernels from the Polybench benchmark suite,
deep learning operators, and -- for the first time -- applications in
unstructured physics simulations, numerical weather prediction stencil
compositions, and full deep neural networks. We derive tight I/O bounds for
several linear algebra kernels, such as Cholesky decomposition, improving the
existing reported bounds by a factor of two. For stencil applications, we
improve the existing bounds by a factor of up to 14. We implement our method as
an open-source tool, which can derive lower bounds directly from provided C
code.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07203"><span class="datestr">at May 18, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07120">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07120">Communication Complexity of Private Simultaneous Quantum Messages Protocols</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawachi:Akinori.html">Akinori Kawachi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nishimura:Harumichi.html">Harumichi Nishimura</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07120">PDF</a><br /><b>Abstract: </b>The private simultaneous messages model is a non-interactive version of the
multiparty secure computation, which has been intensively studied to examine
the communication cost of the secure computation. We consider its quantum
counterpart, the private simultaneous quantum messages (PSQM) model, and
examine the advantages of quantum communication and prior entanglement of this
model. In the PSQM model, $k$ parties $P_1,\ldots,P_k$ initially share a common
random string (or entangled states in a stronger setting), and they have
private classical inputs $x_1,\ldots, x_k$. Every $P_i$ generates a quantum
message from the private input $x_i$ and the shared random string (entangled
states), and then sends it to the referee $R$. Receiving the messages, $R$
computes $F(x_1,\ldots,x_k)$. Then, $R$ learns nothing except for
$F(x_1,\ldots,x_k)$ as the privacy condition. We obtain the following results
for this PSQM model. (1) We demonstrate that the privacy condition inevitably
increases the communication cost in the two-party PSQM model as well as in the
classical case presented by Applebaum, Holenstein, Mishra, and Shayevitz. In
particular, we prove a lower bound $(3-o(1))n$ of the communication complexity
in PSQM protocols with a shared random string for random Boolean functions of
$2n$-bit input, which is larger than the trivial upper bound $2n$ of the
communication complexity without the privacy condition. (2) We demonstrate a
factor two gap between the communication complexity of PSQM protocols with
shared entangled states and with shared random strings by designing a
multiparty PSQM protocol with shared entangled states for a total function that
extends the two-party equality function. (3) We demonstrate an exponential gap
between the communication complexity of PSQM protocols with shared entangled
states and with shared random strings for a two-party partial function.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07120"><span class="datestr">at May 18, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07073">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07073">N-ary Huffman Encoding Using High-Degree Trees -- A Performance Comparison</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xezonakis:Ioannis_S=.html">Ioannis S. Xezonakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leivadaros:Svoronos.html">Svoronos Leivadaros</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07073">PDF</a><br /><b>Abstract: </b>In this paper we implement an n-ary Huffman Encoding and Decoding application
using different degrees of tree structures. Our goal is to compare the
performance of the algorithm in terms of compression ratio, decompression speed
and weighted path length when using higher degree trees, compared to the 2-ary
Huffman Code. The Huffman tree degrees that we compare are 2-ary, 3-ary, 4-ary,
5-ary, 6-ary, 7-ary, 8-ary and 16-mal. We also present the impact that branch
prediction has on the performance of the n-ary Huffman Decoding.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07073"><span class="datestr">at May 18, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07025">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07025">Minimal Cycle Representatives in Persistent Homology using Linear Programming: an Empirical Study with User's Guide</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Lu.html">Lu Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thompson:Connor.html">Connor Thompson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henselman=Petrusek:Gregory.html">Gregory Henselman-Petrusek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giusti:Chad.html">Chad Giusti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziegelmeier:Lori.html">Lori Ziegelmeier</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07025">PDF</a><br /><b>Abstract: </b>Cycle representatives of persistent homology classes can be used to provide
descriptions of topological features in data. However, the non-uniqueness of
these representatives creates ambiguity and can lead to many different
interpretations of the same set of classes. One approach to solving this
problem is to optimize the choice of representative against some measure that
is meaningful in the context of the data. In this work, we provide a study of
the effectiveness and computational cost of several $\ell_1$-minimization
optimization procedures for constructing homological cycle bases for persistent
homology with rational coefficients in dimension one, including
uniform-weighted and length-weighted edge-loss algorithms as well as
uniform-weighted and area-weighted triangle-loss algorithms. We conduct these
optimizations via standard linear programming methods, applying general-purpose
solvers to optimize over column bases of simplicial boundary matrices.
</p>
<p>Our key findings are: (i) optimization is effective in reducing the size of
cycle representatives, (ii) the computational cost of optimizing a basis of
cycle representatives exceeds the cost of computing such a basis in most data
sets we consider, (iii) the choice of linear solvers matters a lot to the
computation time of optimizing cycles, (iv) the computation time of solving an
integer program is not significantly longer than the computation time of
solving a linear program for most of the cycle representatives, using the
Gurobi linear solver, (v) strikingly, whether requiring integer solutions or
not, we almost always obtain a solution with the same cost and almost all
solutions found have entries in {-1, 0, 1} and therefore, are also solutions to
a restricted $\ell_0$ optimization problem, and (vi) we obtain qualitatively
different results for generators in Erd\H{o}s-R\'enyi random clique complexes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07025"><span class="datestr">at May 18, 2021 11:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.07006">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.07006">Optimal Virtual Network Embeddings for Tree Topologies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Figiel:Aleksander.html">Aleksander Figiel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kellerhals:Leon.html">Leon Kellerhals</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rost:Matthias.html">Matthias Rost</a>, Stefan Schmid, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.07006">PDF</a><br /><b>Abstract: </b>The performance of distributed and data-centric applications often critically
depends on the interconnecting network. Applications are hence modeled as
virtual networks, also accounting for resource demands on links. At the heart
of provisioning such virtual networks lies the NP-hard Virtual Network
Embedding Problem (VNEP): how to jointly map the virtual nodes and links onto a
physical substrate network at minimum cost while obeying capacities.
</p>
<p>This paper studies the VNEP in the light of parameterized complexity. We
focus on tree topology substrates, a case often encountered in practice and for
which the VNEP remains NP-hard. We provide the first fixed-parameter algorithm
for the VNEP with running time $O(3^r (s+r^2))$ for requests and substrates of
$r$ and $s$ nodes, respectively. In a computational study our algorithm yields
running time improvements in excess of 200x compared to state-of-the-art
integer programming approaches. This makes it comparable in speed to the
well-established ViNE heuristic while providing optimal solutions. We
complement our algorithmic study with hardness results for the VNEP and related
problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.07006"><span class="datestr">at May 18, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.09815">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.09815">Solving Equation Systems in $\omega$-categorical Algebras</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodirsky:Manuel.html">Manuel Bodirsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quinn=Gregson:Thomas.html">Thomas Quinn-Gregson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.09815">PDF</a><br /><b>Abstract: </b>We study the computational complexity of deciding whether a given set of term
equalities and inequalities has a solution in an $\omega$-categorical algebra
$\mathfrak{A}$. There are $\omega$-categorical groups where this problem is
undecidable. We show that if $\mathfrak{A}$ is an $\omega$-categorical
semilattice or an abelian group, then the problem is in P or NP-hard. The hard
cases are precisely those where Pol$(\mathfrak{A},\neq)$ has a uniformly
continuous minor-preserving map to the clone of projections on a two-element
set. The results provide information about algebras $\mathfrak{A}$ such that
Pol$(\mathfrak{A},\neq)$ does not satisfy this condition, and they are of
independent interest in universal algebra. In our proofs we rely on the
Barto-Pinsker theorem about the existence of pseudo-Siggers polymorphisms. To
the best of our knowledge, this is the first time that the pseudo-Siggers
identity has been used to prove a complexity dichotomy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.09815"><span class="datestr">at May 18, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-6555947.post-8966314939306062304">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/suresh.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html">Transitions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I've been at the U of Utah and Salt Lake City for 14 years (14.5 really). It was my first academic job and the longest time I've spent anywhere (throughout my whole life). So it's a little hard to accept that I'm moving to my next adventure. </p><p>It's a two-part adventure, because why make one move when you can make two. </p><p>Firstly, as of today, I'm going to working with Alondra Nelson at the White House Office of Science and Technology Policy, advising on matters relating to fairness and bias in tech systems. This is a scary and exciting new position, and I hope to help to nudge things along just a bit further in the direction of tech that can help more than it harms, especially for those who've been left behind in our rush to an algorithmically controlled future. </p><p>Secondly, I'm moving to Brown University to join the CS department there as well as their Data Science Initiative. Together with Seny Kamara and others, I'm going to start a new center on Computing for the People, to help think through what it means to do computer science that truly responds to the needs of people, instead of hiding behind a neutrality that merely gives more power to those already in power. </p><p>Lots of changes, and because of the pandemic, all this will happen in slow machine, but it's a whirlwind of emotions (and new clothes - apparently tech conference T-shirts don't work in formal settings - WHO KNEW!!!). </p><p><br /></p><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=aZaRzVm3CmE:2HobITiPYuo:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=yIl2AUoC8zA" border="0" /></a> <a href="http://feeds.feedburner.com/~ff/TheGeomblog?a=aZaRzVm3CmE:2HobITiPYuo:63t7Ie-LG7Y"><img src="http://feeds.feedburner.com/~ff/TheGeomblog?d=63t7Ie-LG7Y" border="0" /></a>
</div><img width="1" alt="" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/aZaRzVm3CmE" height="1" /></div>







<p class="date">
by Suresh Venkatasubramanian (noreply@blogger.com) <a href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html"><span class="datestr">at May 17, 2021 02:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7172198197646487541">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html">Why do countries and companies invest their own money (or is it?) in Quantum Computing (Non-Rhetorical)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> There have been some recent blog posts by Scott (see <a href="https://www.scottaaronson.com/blog/?p=5387">here</a>) and Lance (see <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">here</a>)  about the hype for SHORT TERM APPLICATIONS of Quantum Computing, which they both object to. </p><p>I have a question that has been touched on but I want to get it more out there.</p><p>PREAMBLE TO QUESTION:  The following scenarios, while distasteful, do make sense: </p><p>a) A researcher on their grants exaggerates or even out-right lies about the applications of their work. </p><p>b) A journalist in their articles exaggerates or even out-right lies about the applications of the science they are covering.</p><p>c) A company exaggerates or even out-right lies about the applications of their project to a venture capitalist or other kind of investor. </p><p>QUESTION: </p><p>Why does a company or country invest THEIR OWN MONEY into Quantum Computing which is unlikely to have a short term profit or benefit? Presumably they hire honest scientists to tell them the limits of the applications in the short term. </p><p>ANSWERS I HAVE GOTTEN FROM ASKING THIS</p><p>1) QC might be able to do something cool and profitable, like factoring, or simulating physics quantum experiments or something else, in the short term. Quantum Crypto is already happening, and that's a close cousin of Quantum Computing. </p><p>2) QC might be able to do something cool and profitable (like in (1)) in the long term, and both companies and countries think they will be around for a long time. (For a list of America's 10 oldest companies see <a href="https://www.businessnewsdaily.com/8122-oldest-companies-in-america.html">here</a>.) </p><p>3) The company or country is in this for the long term, not for a practical project, but because they realize that doing GOOD SCIENCE is of a general benefit (this might make more sense for a country than a company). And funding Quantum Computing is great for science. </p><p>4) Bait and Switch: The company claims they are doing Quantum to attract very smart people to work with them, and then have those smart people do something else.</p><p>5) (this is a variant of 1) While Quantum Computing may not have any short term applications, there will be classical applications INSPIRED by it (this has already happened, see <a href="https://www.scottaaronson.com/blog/?p=3880">here</a>).</p><p>6) Some of these companies make money by applying for GRANTS to do QC, so its NOT their money. In fact, they are using QC to  GET money.</p><p>7) For a country its not the leaders money, its that Taxpayer's money- though that still leaves the question of why spend Taxpayer money on this and not on something else.</p><p>8) For a company its not their money- its Venture Capitalists  and others (though for a big company like Google I would think it IS their money). </p><p>9) The scientists advising the company or country are giving them bad (or at least self-serving) advice so that those scientists can profit- and do good science. So this is a variant of (3) but without the company or country knowing it. </p><p>10) In some countries and companies group-think sets in, so if the leader (who perhaps is not a scientist) thinks intuitively that QC is good, the scientists who work for them, who know better, choose to not speak up, or else they would lose their jobs...or worse. </p><p>11) For countries this could be like going to the moon: Country A wants to beat Country B to the moon for bragging rights. Scientists get to do good research even if they don't care about bragging rights. </p><p>12) (similar to 11 but for a company) If a company does great work on QC then it is good publicity for that company. </p><p>13) Some variant of the <a href="https://en.wikipedia.org/wiki/Greater_fool_theory">greater fool theory</a>. If so, will there be a bubble? A bail-out? </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html"><span class="datestr">at May 16, 2021 05:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5490">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5490">What I told my kids</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa.  And that’s true.</p>



<p>But it’s also not so complicated that there are no clearly identifiable good guys or bad guys.  It’s just that they cut across the sides.</p>



<p>The good guys are anyone, on either side, whose ideal end state is two countries, Israel and Palestine, living side by side in peace.</p>



<p>The bad guys are anyone, on either side, whose ideal end state is the other side being, if not outright exterminated, then expelled from its current main population centers (ones where it’s been for several generations or more) and forcibly resettled someplace far away.</p>



<p>(And those whose ideal end state is everyone living together with no border — possibly as part of the general abolition of nation-states?   They’re not bad guys; they can plead insanity.  [<strong>Update:</strong> <a href="https://www.scottaaronson.com/blog/?p=5490#comment-1889918">See here</a> for clarifications!])</p>



<p>Hamas are bad guys.  They fire rockets indiscriminately at population centers, hoping to kill as many civilians as they can.  (Unfortunately for them and fortunately for Israel, they’re not great at that, and also they’re aiming at a target that’s world-historically good at defending itself.)</p>



<p>The IDF, whatever else you say about it, sends evacuation warnings to civilians before it strikes the missile centers that are embedded where they live.  Even if Hamas could aim its missiles, the idea of it extending the same courtesy to Israeli civilians is black comedy.</p>



<p>Netanyahu is not as bad as Hamas, because he has the power to kill millions of Palestinians and yet kills only hundreds … whereas if Hamas had the power to kill all Jews, it told the world in its charter that it would immediately do so, and it’s acted consistently with its word.</p>



<p>(An aside: I’m convinced that Hamas has the most top-heavy management structure of any organization in the world.  Every day, Israel takes out another dozen of its <em>most senior, highest-level</em> commanders, apparently leaving hundreds more.  How many senior commanders do they have?  Do they have even a single junior commander?)</p>



<p>Anyway, not being as bad as Hamas is an extremely low bar, and Netanyahu is a thoroughly bad guy.  He’s corrupt and power-mad.  Like Trump, he winks at his side’s monstrous extremists without taking moral responsibility for them.  And if it were ever possible to believe that he wanted two countries as the ideal end state, it hasn’t been possible to believe that for at least a decade.</p>



<p>Netanyahu and Hamas are allies, not enemies.  Both now blatantly, obviously rely on the other to stay in power, to demonstrate their worldview and thereby beat their internal adversaries.</p>



<p>Whenever you see anyone opine about this conflict, on Facebook or Twitter or in an op-ed or anywhere else, keep your focus relentlessly on the question of what that person <em>wants</em>, of what they’d do if they had unlimited power.  If they’re a Zionist who talks about how “there’s no such place as Palestine,” how it’s a newly invented political construct: OK then, does that mean they’d relocate the 5 million self-described Palestinians to Jordan?  Or where?  If, on the other side, someone keeps talking about the “Zionist occupation,” always leaving it strategically unspecified whether they mean just the West Bank and parts of East Jerusalem or also Tel Aviv and Haifa, if they talk about the Nakba (catastrophe) of Israel’s creation in 1947 … OK then, what’s to be done with the 7 million Jews now living there?  Should they go back to the European countries that murdered their families, or the Arab countries that expelled them?  Should the US take them all?  Out with it!</p>



<p>Don’t let them dodge the question.  Don’t let them change the subject to something they’d much rather talk about, like the details of the other side’s latest outrage.  Those details always seem so important, and yet everyone’s stance on every specific outrage is like 80% predictable if you know their desired end state.  So just keep asking directly about their desired end state.</p>



<p>If, like me, you favor two countries living in peace, then you need never fear anyone asking you the same thing.  You can then shout your desired end state from the rooftops, leaving unsettled only the admittedly-difficult “engineering problem” of how to get there.  Crucially, whatever their disagreements or rivalries, everyone trying to solve the same engineering problem is in a certain sense part of the same team.  At least, there’s rarely any reason to kill someone trying to solve the same problem that you are.</p>



<p>“What is this person’s ideal end state?”  Just keep asking that and there’s a limit to how wrong you can ever be about this.  You can still make factual mistakes, but it’s then almost impossible to make a moral mistake.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5490"><span class="datestr">at May 16, 2021 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://www.amstat.org/asa/News/Greek-Statistician-Found-Liable-for-Slander-Continues-to-Face-Persecution.aspx">Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou</a> (<a href="https://mathstodon.xyz/@11011110/106164322094769198">\(\mathbb{M}\)</a>) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/arxivabs">That other microblogging site has a bot specifically devoted to replacing links to pdf versions of arxiv preprints by links to the abstracts of the same preprint</a> (<a href="https://mathstodon.xyz/@11011110/106169351037107488">\(\mathbb{M}\)</a>). Is there something like that for mastodon? If not there should be.</p>
  </li>
  <li>
    <p>For a centrally symmetric star-shaped set in the plane, each line through the center cuts its perimeter into two equal-length curves. But these are not the only shapes with this property: 18th-century Jesuit polymath <a href="https://en.wikipedia.org/wiki/Roger_Joseph_Boscovich">Roger Boscovich</a> observed that a heart-like shape formed by three semicircles has the same property (<a href="https://mathstodon.xyz/@11011110/106175759357772585">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.2307/2974900">via</a>).</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/boscovich.svg" alt="Boscovich's cardioid, with its perimeter bisected by a line through its cusp" /></p>
  </li>
  <li>
    <p><a href="https://picturethismaths.wordpress.com/2020/10/31/three-correlations-and-a-samosa/">Three correlations and a samosa</a> (<a href="https://mathstodon.xyz/@11011110/106180193506681489">\(\mathbb{M}\)</a>, <a href="https://picturethismaths.wordpress.com/2021/02/03/three-correlations-and-other-statistical-models/">see also</a>, <a href="https://picturethismaths.wordpress.com/2021/03/18/three-correlations-and-an-algebraic-classification/">see also</a>). \(3\times 3\) symmetric matrices with unit diagonals form a three-dimensional linear space, in which the samosa is a curvy 3d convex set representing the positive definite matrices. Taking sections of it allows you to infer the possible correlations between two variables, given each of their correlations with a third.</p>
  </li>
  <li>
    <p>I couldn’t resist picking up a copy of <em>The Architecture of Trees</em> (<a href="https://mathstodon.xyz/@11011110/106184861337043968">\(\mathbb{M}\)</a>), a coffee-table book centered on pen-and-ink illustrations of the summer and winter forms of over 200 types of tree, on a recent visit to the Mendocino Coast Botanical Gardens (beautiful this time of year with many flowers in bloom). Some reviews: <a href="https://www.startribune.com/new-book-is-tree-tome-like-few-others-part-science-part-art-marvel/507788702/">1</a>, <a href="https://www.thedailybeast.com/the-architecture-of-trees-travel-with-the-book-that-captures-the-beauty-of-trees">2</a>, <a href="http://spacing.ca/national/2020/08/04/book-review-the-architecture-of-trees/ https://dirt.asla.org/2019/07/10/the-architecture-of-trees-2/">3</a>.</p>
  </li>
  <li>
    <p><a href="http://www.xl-muse.com/html/en/index.php?ac=article&amp;at=read&amp;did=239">Dujiangyan Zhongshuge</a> (<a href="https://mathstodon.xyz/@11011110/106192492039422969">\(\mathbb{M}\)</a>, <a href="https://www.thisiscolossal.com/2021/05/x-living-dujiangyan-zhongshuge/">via</a>), bookstore in Chengdu with mirrored floors and ceilings creating the feeling of an infinite Escher palace of books.</p>
  </li>
  <li>
    <p><a href="http://keenan.is/illustrating/">Illustrating geometry</a> (<a href="https://mathstodon.xyz/@11011110/106204129125568328">\(\mathbb{M}\)</a>). An apparently-defunct blog from 2016–2017 with several interesting posts about technical illustrations in mathematics.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/391885/440">Which \(n\times n\times n\) grids have Hamiltonian cycles that turn at every step?</a> (<a href="https://mathstodon.xyz/@11011110/106218352172585271">\(\mathbb{M}\)</a>) After I linked to this, a later answer pointed to the recent book <em>Bicycle or Unicycle？A Collection of Intriguing Mathematical Puzzles</em>, by Stan Wagon and Daniel Velleman, which has solutions for all even \(n\) on pp. 89–96. A simple parity argument shows that it’s impossible on odd grids, but the same book conjectures that these have Hamiltonian paths except in the case \(n=3\).</p>
  </li>
  <li>
    <p><a href="https://wg2021.mimuw.edu.pl/accepted-papers/">Accepted papers to the International Workshop
on Graph-Theoretic Concepts in Computer Science</a> (<a href="https://mathstodon.xyz/@11011110/106220947095567586">\(\mathbb{M}\)</a>). My paper “<a href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html">The Graphs of Stably Matchable Pairs</a>” is one of them. The conference will be online June 23–25. Unlike many conferences, WG prepares the proceedings after the conference, to allow authors to incorporate feedback from presentations. Details of how to participate don’t seem to be online but I’m sure they’ll be made available soon.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/05/13/why-you-shouldnt-be-too-pessimistic/">Why you shouldn’t be too pessimistic</a> (<a href="https://mathstodon.xyz/@11011110/106230884405427138">\(\mathbb{M}\)</a>). Igor Pak on the nature of mathematical conjectures.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/careers/2021/05/two-surnames-no-hyphen-claiming-my-identity-latin-american-scientist">Two surnames, no hyphen: Claiming my identity as a Latin American scientist</a> (<a href="https://mathstodon.xyz/@11011110/106241178695539953">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/05/15">via</a>). Johana Goyes Vallejos in <em>Science</em>, on respect for diversity in naming styles in academia.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/15/linkage.html"><span class="datestr">at May 15, 2021 05:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html">Congratulations, Dr. Havvaei!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, <a href="https://www.ics.uci.edu/~ehavvaei/">Elham Havvaei</a> (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</p>

<p>Haleh is Iranian, but came to UC Irvine via Florida in 2016, after working for a master’s degree with Narsingh Deo at the University of Central Florida. I’ve written here already about the research she’s done with me: <a href="https://11011110.github.io/blog/2018/10/07/recognizing-sparse-leaf.html">reconstructing unknown trees from graphs connecting close-together leaves</a> (from IPEC 2018 and Algorithmica 2020), <a href="https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone.html">simplifying graphs whose vertices and edges represent the milestones, tasks, and ordering constraints of a project</a> (SWAT 2020), and <a href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html">classifying problems of finding large subgraphs with one property in graphs with another property</a> (not yet published).</p>

<p>Her next step will be taking a position at Twitter in San Francisco, as a data scientist.</p>

<p>Congratulations, Haleh!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106236739159044402">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html"><span class="datestr">at May 14, 2021 06:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/070">TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/070"><span class="datestr">at May 13, 2021 05:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/">Matrix—The Meeting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img width="192" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" class="alignright wp-image-18739" height="128" /></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{20-60}" class="latex" /> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img width="384" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" class="alignright wp-image-18742" height="216" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br />
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p></p>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p></p><h2> The Question </h2><p></p>
<p></p><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Ax = b " class="latex" /></p>
<p>Alice could go off and return the <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> that solves the system. Or she could say there is no such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. Or she could say there are many such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> I have. <p></p>
</li><li>
I plan to see what the norm of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is. <p></p>
</li><li>
Or, I plan to see what <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_1}" class="latex" /> is. <p></p>
</li><li>
Or, I could be just happy to know that there is some <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. <p></p>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p></p><h2> One View </h2><p></p>
<p></p><p>
What does it mean to solve the equation <img src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Ax=b}" class="latex" />, for an invertible matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" />? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br />
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br />
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/"><span class="datestr">at May 13, 2021 12:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1929264998273205739">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html">Cryptocurrency, Blockchains and NFTs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p></p><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br /></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html"><span class="datestr">at May 13, 2021 11:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html">The constructive solid geometry of piecewise-linear functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/minmax.svg" alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" /></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg" alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" /></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html"><span class="datestr">at May 12, 2021 05:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/069">TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/069"><span class="datestr">at May 12, 2021 08:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/glm_saga/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/glm_saga/">Debuggable Deep Networks: Sparse Linear Models (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/intro.png" alt="Overview" /></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png" alt="Examples of feature visualization" /></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png" alt="Examples of word cloud visualization" /></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png" alt="Feature visualization dump" /></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas width="400" id="ablation_dense_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img src="https://gradientscience.org/feed.xml" id="reg" />
        <div id="reg_slider"></div>
        <div class="quarterblock"> </div>
        <div style="text-align: center;" class="quarterblock">Accuracy: <span id="reg_accuracy"></span>%</div>
        <div style="text-align: center;" class="quarterblock">Non-zero: <span id="reg_sparsity"></span>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas width="400" id="ablation_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
    </div>
    <div id="zoom">
        <img src="https://gradientscience.org/feed.xml" id="feature_big" />
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>







<p class="date">
<a href="https://gradientscience.org/glm_saga/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/debugging/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/debugging/">Debuggable Deep Networks: Usage and Evaluation (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png" height="350" /></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png" alt="Wordcloud visualization of feature used in unbiased BERT" /></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" value="" id="toxic_">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="christianity" id="toxic_christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="African" id="toxic_African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="Catholic" id="toxic_Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"></span>
        <canvas width="400" id="toxic" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png" height="350" /></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"></div>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>

</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas width="400" id="sst_canvas" height="200"></canvas>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png" alt="ImageNet counterfactuals" /></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/problematic.png" alt="Problematic features" /></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png" height="350" /></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"></div>
  <div style="float: none;" class="blocktxt" id="mislabels"> </div>
  <div style="clear: both;" id="misimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"></span><br /><span style="color: red;" class="choices_info_text"><b></b></span><br /><span style="color: green;" class="choices_info_text"><b></b></span><br /><hr /><h3 style="text-align: center;"><h3><div style="text-align: center; font-weight: 300; margin: 0.75em auto;" class="sp_txt"></div><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png" /><hr style="margin: 0.75em auto;" /><div style="text-align: center; font-weight: 200;" class="sp_txt"><span></span></div><hr style="margin: 0.3em auto;" /><h3 style="text-align: center;"><h3><div style="text-align: center;" class="sp_txt"><span style="font-weight: 200;"></span></div><br /><span></span></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"></span><span class="choices_info_text"></span><br /><span class="choices_info_text"></span><div class="mis_txt blocktxt thirdblock"><br /><span class="widgetheading"></span></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;" /></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div></div></h3></h3></div>







<p class="date">
<a href="https://gradientscience.org/debugging/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">STOC Test of time award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br /><br />The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br /><br />Please nominate the papers you think have most influenced our field!</li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/"><span class="datestr">at May 11, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br />
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/"><span class="datestr">at May 10, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5486">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br /></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br /></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5486"><span class="datestr">at May 10, 2021 05:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8609684815037895352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html">Trump, Facebook, and ComplexityBlog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html"><span class="datestr">at May 10, 2021 12:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">Arc-triangle tilings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg" alt="Tiling by ordinary triangles" /></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg" alt="Tiling by arc-triangles with two curved sides" /></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg" alt="Tiling by arc-triangles with three curved sides" /></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pinwheels.svg" alt="Tiling by arc-triangles with two curved sides, without reflection" /></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html"><span class="datestr">at May 09, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1512">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1512">News for April 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper.<em> (We may have missed some; if so, please let us know in the comments!)</em></p>



<p><strong>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</strong>, by Sepehr Assadi and Vishvajeet N (<a href="https://arxiv.org/abs/2104.04908">arXiv</a>). This paper establishes space vs. pass trade-offs lower bounds for streaming algorithms, for a variety of graph tasks: that is, of the sort “any \(m\)-pass-streaming algorithm for task \(\mathcal{T}\) must use memory at least \(f(m)\).” The tasks considered include graph property estimation (size of the maximum matching, of the max cut, of the  weight of the MST) and property testing for sparse graphs (connectivity, bipartiteness, and cycle-freeness). The authors obtained exponentially improved lower bounds for those, via reductions to a relatively standard problem, (noisy) gap cycle counting, for which they establish their main lower bound. As a key component of their proof, they prove a general direct product result (XOR lemma) for the streaming setting, showing that the advantage for solving the XOR of \(\ell\) copies of a streaming predicate \(f\) decreases exponentially with \(\ell\). </p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1512"><span class="datestr">at May 08, 2021 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/068">TR21-068 |  Quantum Proofs of Proximity | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Subhayan Roy Moulik, 

	Justin Thaler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the systematic study of QMA algorithms in the setting of property testing, to which we refer as QMA proofs of proximity (QMAPs). These are quantum query algorithms that receive explicit access to a sublinear-size untrusted proof and are required to accept inputs having a property $\Pi$ and reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a minuscule portion of their input.

Our algorithmic results include a general-purpose theorem that enables quantum speedups for testing an expressive class of properties, namely, those that are succinctly decomposable. Furthermore, we show quantum speedups for properties that lie outside of this family, such as graph bipartitneness.

We also investigate the complexity landscape of this model, showing that QMAPs can be exponentially stronger than both classical proofs of proximity and quantum testers. To this end, we extend the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to prove quantum property testing lower bounds via reductions from communication complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory of Computing, 2016).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/068"><span class="datestr">at May 08, 2021 11:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/067">TR21-067 |  Variety Evasive Subspace Families | 

	Zeyu Guo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of constructing explicit variety evasive subspace families. Given a family $\mathcal{F}$ of subvarieties of a projective or affine space, a collection $\mathcal{H}$ of projective or affine $k$-subspaces is $(\mathcal{F},\epsilon)$-evasive if for every $\mathcal{V}\in\mathcal{F}$, all but at most $\epsilon$-fraction of $W\in\mathcal{H}$ intersect every irreducible component of $\mathcal{V}$ with (at most) the expected dimension. The problem of constructing such an explicit subspace family generalizes both deterministic black-box polynomial identity testing (PIT) and the problem of constructing explicit (weak) lossless rank condensers. 

Using Chow forms, we construct explicit $k$-subspace families of polynomial size that are evasive for all varieties of bounded degree in a projective or affine $n$-space. As one application, we obtain a complete derandomization of Noether's normalization lemma for varieties of bounded degree in a projective or affine $n$-space. In another application, we obtain a simple polynomial-time black-box PIT algorithm for depth-4 arithmetic circuits with bounded top fan-in and bottom fan-in that are not in the Sylvester-Gallai configuration, improving and simplifying a result of Gupta (ECCC TR 14-130).

As a complement of our explicit construction, we prove a lower bound for the size of $k$-subspace families that are evasive for degree-$d$ varieties in a projective $n$-space. When $n-k=\Omega(n)$, the lower bound is superpolynomial unless $d$ is bounded. The proof uses a dimension-counting argument on Chow varieties that parametrize projective subvarieties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/067"><span class="datestr">at May 08, 2021 06:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html">Congratulations, Dr. Matias!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://pmatias.com/">Pedro Ascensao Ferreira Matias</a>, one of the students working with Mike Goodrich in the UC Irvine <a href="https://www.ics.uci.edu/~theory/">Center for Algorithms and Theory of Computation</a>, passed his Ph.D. defense today.</p>

<p>Pedro is Portuguese, and came to UCI after a bachelor’s degree from the University of Coimbra in Portugal and a master’s degree from Chalmers University of Technology in Sweden.</p>

<p>The general topic of Pedro’s research is “exact learning”, the inference of structured information from queries or other smaller pieces of data. I’ve written here before about my work with Matias on <a href="https://11011110.github.io/blog/2019/02/21/mutual-nearest-neighbors.html">nearest-neighbor chains</a> and on <a href="https://11011110.github.io/blog/2019/08/17/footprints-in-snow.html">tracking paths in planar graphs</a>, the problem of placing sensors on a small subset of vertices so that, by detecting the order in which a path reaches each sensor, you can uniquely determine the whole path. His dissertation combines the tracking paths work with a second paper on tracking paths (“How to Catch Marathon Cheaters: New Approximation Algorithms for Tracking Paths”, <a href="https://arxiv.org/abs/2104.12337">arXiv:2104.12337</a>, to appear at WADS 2021), and a paper on reconstructing periodic and near-periodic strings from sublinear numbers of queries (“Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors and Jumbled-Index Queries in String Reconstruction”, <a href="https://arxiv.org/abs/2007.08787">arXiv:2007.08787</a>, in SPIRE 2020). He also has recent papers on reconstructing trees in SPAA 2020 and ESA 2020.</p>

<p>After finishing his doctorate, Pedro’s next position will be working for Facebook.</p>

<p>Congratulations, Pedro!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106196168129163033">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html"><span class="datestr">at May 07, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
