<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at August 09, 2021 07:23 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1648249705477846335">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html">Combing two posts: Blankface (Scott Aa) and Is Science Slowing Down? (Scott Al)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(I also posted this to the Less Wrong Website. At least I tried to- I don't quite know if or when it will appear there as its my first post there.) </p><p>Some papers result from taking two papers and combining them. Perhaps nobody else had read both of them so you can say something new! Or (looking over this post) it may guide people to two really good papers, or in this case two really good posts. </p><p>This blog will draw from two excellent blog posts.</p><p>Scott Aaronson  blogged on  his website Aug 2, 2021 about <a href="https://www.scottaaronson.com/blog/?p=5675#comments">blankfaces</a>, people who let stupid or undefined rules dictate what you can do  without apology (see his post for a better explanation). One example that struck me I quote</p><p><i>No, I never applied for that grant. I spend two hours struggling to log in to a web portal designed by the world's top blankfaces until I finally gave up in despair. </i></p><p><i><br /></i></p><p>Scott Alexander blogged  on LessWrong on Nov 26, 2018 about <a href="https://www.lesswrong.com/posts/v7c47vjta3mavY3QC/is-science-slowing-down">Is science slowing down?</a> which answers with an emphatic <i>yes.</i> His point is science-per-researcher is much less than it used to be, and he has graphs and stats to prove it (see his post for the evidence and some speculation as to why this is) One of the reasons he gave struck me which I quote</p><p><i>Certain features of the modern academic system like undepaid PhD's, interminably long postdocs, endless grant writing drudgery, and clueless funders have lowered productivity. The 1930's academic system was ineed 25x more effective at getting researchers to actually do good research.</i></p><p>(I note that he gives other reasons as well, most notably for our field that the low hanging fruit is gone. Our lack of progress on P vs NP is likely that its a hard problem, rather than the reason above. Of course, if its solved tomorrow by an outsider without funding, I will happily be proven wrong.) </p><p>Scott Alexander hits upon two types of blankfaces (without using the term).</p><p><i>Grant writing drudgery</i>: the rules for how to submit get more and more detailed an onerous. This is  what Scott Aaronson was alluding to. There are other ways its drudgery as well. </p><p><i>Clueless Funders</i>: the people deciding who gets funded might not know the area (actually in my experience the grant I've reviews have been quite good and the problem is more not enough money to award all that are deserving.) </p><p>SO I pose the following non-rhetorically as always</p><p>1) How big a factor is the slowing down of science that blankfaces get in the way?</p><p>2) What can we do about it?</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><i><br /></i></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html"><span class="datestr">at August 09, 2021 01:50 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">School on Modern Directions in Discrete Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 13-17, 2021 Online https://www.him.uni-bonn.de/programs/future-programs/future-trimester-programs/discrete-optimization/discrete-optimization-school/ Aims and Scope: The school provides an introduction to some of the main topics of the trimester program on discrete optimization. The lectures will address the interface between tropical geometry and discrete optimization; recent developments in continuous optimization with applications to combinatorial problems; topics in approximation algorithms; and fixed parameter … <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/" class="more-link">Continue reading <span class="screen-reader-text">School on Modern Directions in Discrete Optimization</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/"><span class="datestr">at August 08, 2021 02:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/115">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/115">TR21-115 |  On quantum versus classical query complexity | 

	Scott Aaronson, 

	Andris Ambainis, 

	Andrej Bogdanov, 

	Krishnamoorthy Dinesh, 

	Cheung Tsun Ming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Aaronson and Ambainis (STOC 2015, SICOMP 2018) claimed that the acceptance probability of every quantum algorithm that makes $q$ queries to an $N$-bit string can be estimated to within $\epsilon$ by a randomized classical algorithm of query complexity $O_q((N/\epsilon^2)^{1-1/2q})$.  We describe a flaw in their argument but prove that the dependence on $N$ in this upper bound is correct for one-query quantum algorithms ($q = 1$).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/115"><span class="datestr">at August 08, 2021 12:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02770">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02770">Scheduling with Communication Delay in Near-Linear Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Quanquan_C=.html">Quanquan C. Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svitkina:Zoya.html">Zoya Svitkina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vee:Erik.html">Erik Vee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Joshua_R=.html">Joshua R. Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02770">PDF</a><br /><b>Abstract: </b>We consider the problem of efficiently scheduling jobs with precedence
constraints on a set of identical machines in the presence of a uniform
communication delay. In this setting, if two precedence-constrained jobs $u$
and $v$, with ($u \prec v$), are scheduled on different machines, then $v$ must
start at least $\rho$ time units after $u$ completes. The scheduling objective
is to minimize makespan, i.e. the total time between when the first job starts
and the last job completes. The focus of this paper is to provide an efficient
approximation algorithm with near-linear running time. We build on the
algorithm of Lepere and Rapine [STACS 2002] for this problem to give an
$O\left(\frac{\ln \rho}{\ln \ln \rho} \right)$-approximation algorithm that
runs in $\tilde{O}(|V| + |E|)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02770"><span class="datestr">at August 08, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02635">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02635">Generation of High-Order Coarse Quad Meshes on CAD Models via Integer Linear Programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Mattéo Couplet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reberol:Maxence.html">Maxence Reberol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Remacle:Jean=Fran=ccedil=ois.html">Jean-François Remacle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02635">PDF</a><br /><b>Abstract: </b>We propose an end-to-end pipeline to robustly generate high-quality,
high-order and coarse quadrilateral meshes on CAD models. This kind of mesh
enables the use of high-order analysis techniques such as high-order finite
element methods or isogeometric analysis. An initial unstructured mesh is
generated; this mesh contains a low number of irregular vertices but these are
not necessarily aligned, causing a very dense quad layout. A T-mesh is built on
the mesh which allows to modify its topology by assigning new integer lengths
to the T-mesh arcs. The task of simplifying the quad layout can be formulated
as an Integer Linear Program which is solved efficiently using an adequate
solver. Finally, a high-order quad mesh is extracted from the optimized
topology. Validation on several CAD models shows that our approach is fast,
robust, strictly respects the CAD features, and achieves interesting results in
terms of coarseness and quality.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02635"><span class="datestr">at August 08, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02585">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02585">Geometric Embeddability of Complexes is $\exists \mathbb R$-complete</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abrahamsen:Mikkel.html">Mikkel Abrahamsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleist:Linda.html">Linda Kleist</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02585">PDF</a><br /><b>Abstract: </b>We show that the decision problem of determining whether a given (abstract
simplicial) $k$-complex has a geometric embedding in $\mathbb R^d$ is complete
for the Existential Theory of the Reals for all $d\geq 3$ and $k\in\{d-1,d\}$.
This implies that the problem is polynomial time equivalent to determining
whether a polynomial equation system has a real root. Moreover, this implies
NP-hardness and constitutes the first hardness results for the algorithmic
problem of geometric embedding (abstract simplicial) complexes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02585"><span class="datestr">at August 08, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02534">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02534">Existence and polynomial time construction of biregular, bipartite Ramanujan graphs of all degrees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Aurelien Gribinski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marcus:Adam_W=.html">Adam W. Marcus</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02534">PDF</a><br /><b>Abstract: </b>We prove that there exist bipartite, biregular Ramanujan graphs of every
degree and every number of vertices provided that the cardinalities of the two
sets of the bipartition divide each other. This generalizes a result of Marcus,
Spielman, and Srivastava and, similar to theirs, the proof is based on the
analysis of expected polynomials. The primary difference is the use of some new
machinery involving rectangular convolutions, developed in a companion paper.
We also prove the constructibility of such graphs in polynomial time in the
number of vertices, extending a result of Cohen to this biregular case.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02534"><span class="datestr">at August 08, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02367">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02367">Evacuating from ell_p Unit Disks in the Wireless Model</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiou:Konstantinos.html">Konstantinos Georgiou</a>, Sean Leizerovich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucier:Jesse.html">Jesse Lucier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kundu:Somnath.html">Somnath Kundu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02367">PDF</a><br /><b>Abstract: </b>The search-type problem of evacuating 2 robots in the wireless model from the
(Euclidean) unit disk was first introduced and studied by Czyzowicz et al.
[DISC'2014]. Since then, the problem has seen a long list of follow-up results
pertaining to variations as well as to upper and lower bound improvements. All
established results in the area study this 2-dimensional search-type problem in
the Euclidean metric space where the search space, i.e. the unit disk, enjoys
significant (metric) symmetries.
</p>
<p>We initiate and study the problem of evacuating 2 robots in the wireless
model from $\ell_p$ unit disks, $p \in [1,\infty)$, where in particular robots'
moves are measured in the underlying metric space. To the best of our
knowledge, this is the first study of a search-type problem with mobile agents
in more general metric spaces. The problem is particularly challenging since
even the circumference of the $\ell_p$ unit disks have been the subject of
technical studies. In our main result, and after identifying and utilizing the
very few symmetries of $\ell_p$ unit disks, we design \emph{optimal evacuation
algorithms} that vary with $p$. Our main technical contributions are two-fold.
First, in our upper bound results, we provide (nearly) closed formulae for the
worst case cost of our algorithms. Second, and most importantly, our lower
bounds' arguments reduce to a novel observation in convex geometry which
analyzes trade-offs between arc and chord lengths of $\ell_p$ unit disks as the
endpoints of the arcs (chords) change position around the perimeter of the
disk, which we believe is interesting in its own right. Part of our argument
pertaining to the latter property relies on a computer assisted numerical
verification that can be done for non-extreme values of $p$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02367"><span class="datestr">at August 08, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2108.02288">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2108.02288">The Gotsman-Linial Conjecture is False</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chapman:Brynmor.html">Brynmor Chapman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02288">PDF</a><br /><b>Abstract: </b>In 1991, Craig Gotsman and Nathan Linial conjectured that for all $n$ and
$d$, the average sensitivity of a degree-$d$ polynomial threshold function on
$n$ variables is maximized by the degree-$d$ symmetric polynomial which
computes the parity function on the $d$ layers of the hypercube with Hamming
weight closest to $n/2$. We refute the conjecture for almost all $d$ and for
almost all $n$, and we confirm the conjecture in many of the remaining cases.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2108.02288"><span class="datestr">at August 08, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1563">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/08/workshop-on-algorithms-for-large-data-we-found-waldo-and-so-can-you/">Workshop on Algorithms for Large Data: We found WALD(O), and so can you!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Ainesh Bakshi, Rajesh Jayaram, and Samson Zhou are organizing a 3-day <a href="https://waldo2021.github.io/">Workshop on Algorithms for Large Data</a> (nicely abbreviated as WALD(O), the O standing for Online), featuring many talks which should be of interest to the readers of this blog, as well as an open problems and a poster sessions, and a junior/senior lunch. As the organizers describe it:</p>



<blockquote class="wp-block-quote"><p>This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</p></blockquote>



<p>The workshop will take place on <strong>August 23 — August 25</strong> (ET). Attendance is free, but <a href="https://docs.google.com/forms/d/1VMtDFay1MoiKMAErfkg2ZkAswQQdNWhiDQUKGtPBrzA/viewform">registration</a> is required by <strong>August 20th</strong>. More details at <a href="https://waldo2021.github.io/">https://waldo2021.github.io/</a></p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/08/workshop-on-algorithms-for-large-data-we-found-waldo-and-so-can-you/"><span class="datestr">at August 07, 2021 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1560">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2021/08/new-for-july-2021/">New for July 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This month saw three papers appear online, together covering a rather broad range of topics: testing of regular languages, distribution testing under differential privacy, and local testability from high-dimensional expanders. Let’s dive in!</p>



<p><strong>Property Testing of Regular Languages with Applications to Streaming Property Testing of Visibly Pushdown Languages</strong>, by Gabriel Bathie and Tatiana Starikovskaya (<a href="https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=14188">paper</a>). Let \(L\in \Sigma^\ast\) be a regular language recognized by an automation with \(m\) states and \(k\) connected components: given as input a word \(u\in \Sigma^n\), what is the query complexity to test membership to \(L\) in Hamming distance? Edit distance? Or, more generally, <em>weighted</em> edit distance, where each letter of the word \(u\) comes with a weight? In this paper, the authors focus on non-adaptive, one-sided errors testing algorithms, for which they show an upper bound of \(q=O(k m \log(m/\varepsilon)/\varepsilon)\) queries (with running time \(O(m^2 q)\)), which they complement by a query complexity lower bound of \(\Omega(\log(1/\varepsilon)/\epsilon)\), thus matching the upper bound for languages recognized by constant-size automata. The guarantee for the upper bound is with respected to weighted edit distance, and thus implies the same upper bound for testing with respect to Hamming distance. <br />To conclude, the authors use an existing connection to streaming property testing to obtain new algorithms for property testing of visibly pushdown languages (VPL) in the <em>streaming</em> model, along with a new lower bound in that model.</p>



<p><strong>High dimensional expansion implies amplified local testability</strong>, by Tali Kaufman and Izhar Oppenheim (<a href="https://arxiv.org/abs/2107.10488">arXiv</a>). This paper sets out to show that codes that arise from high-dimensional expanders are locally testable (membership to the code can be tested using very few queries). To do so, the authors define a new notion of <em>high-dimensional expanding system</em> (HDE system), as well as that of <em>amplified</em> local testability, a stronger notion than local testability; they then prove that a code based on a HDE system satisfies this stronger notion. Moreover, they show that many well-known families of codes are, in fact, HDE system codes, and therefore satisfy this stronger notion of local testability as well.</p>



<p>Finally, a survey on differential privacy, with a foray into distribution testing:</p>



<p><strong>Differential Privacy in the Shuffle Model: A Survey of Separations</strong>, by Albert Cheu (<a href="https://arxiv.org/abs/2107.11839">arXiv</a>). If you are familiar with differential privacy (DP), you may recall that there are several notions of DP, each meant to address a different “threat model” (depending on whom you trust with your data). <em>Shuffle DP</em> is one of them, intermediate between “central” DP and the more stringent “local” DP. Long story short: with shuffle DP, the tradeoff between privacy and accuracy can be strictly in-between what’s achievable in central and local DP, and that’s the case for one of the usual suspects of distribution testing, uniformity testing (<em>“I want to test if the data uniformly distributed, but now, with privacy of that data in mind”</em>). The survey discusses what is known about this in Sections 3.3 and 6, and what the implications are; but there are quite a few questions left unanswered… Long story short: a very good introduction to shuffle privacy, and to open problems in that area!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/2021/08/new-for-july-2021/"><span class="datestr">at August 07, 2021 06:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-551003207026816768">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award.html">Interview with CONCUR 2021 ToT Award Recipients: Uwe Nestmann and Benjamin Pierce</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I am pleased to re-post <a href="https://www.imperial.ac.uk/people/n.yoshida" target="_blank">Nobuko Yoshida</a>'s splendid <a href="http://mrg.doc.ic.ac.uk/concur-tot/" target="_blank">interview</a> with CONCUR 2021 Test-of-Time Award recipients <a href="https://www.mtv.tu-berlin.de/nestmann/" target="_blank">Uwe Nestmann</a> and <a href="https://www.cis.upenn.edu/~bcpierce/" target="_blank">Benjamin Pierce</a>. I thoroughly enjoyed reading it and learnt much from the many pearls of wisdom that pepper the interview. </p><p>Thanks to Benjamin and Uwe for their answers and to Nobuko for conducting such an inspiring interview. Enjoy!<br /></p><p><br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award.html"><span class="datestr">at August 06, 2021 09:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/">Workshop on Algorithms for Large Data (Online) 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 23-25, 2021 Online https://waldo2021.github.io/ Registration deadline: August 20, 2021 This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/"><span class="datestr">at August 06, 2021 09:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2754">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">Average-Case Fine-Grained Hardness, Part III</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Continuing our previous discussion, I will show another application of the new recipe described in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> (i.e., constructing a “good” polynomial for a problem of interest), which will establish average-case hardness of a problem related to the orthogonal vector (OV) problem. (Recall the OV problem: Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3D%5Comega%28%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\omega(\log n)" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. The reader can look up the worst-case hardness of the OV problem in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">first post</a> of the series.)</p>



<p>The motivating question is can we show average-case hardness for counting the number of orthogonal pairs in the OV problem by constructing a “good” polynomial? (One motivation is that such average-case hardness result could serve as the source of reductions for proving average-case hardness for many other problems, because OV is a main source of fine-grained hardness.) There is a good reason to believe the answer is no. Specifically, an <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-%5Cdelta%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-\delta})" class="latex" />-time algorithm for counting orthogonal pairs for average-case OV instances (here “average-case” is Erdős–Rényi random input model, and <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\delta" class="latex" /> is a constant that depends on the parameter of the input model) was given in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>, while constructing a “good” polynomial would prove <img src="https://s0.wp.com/latex.php?latex=%5COmega%28n%5E%7B2-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\Omega(n^{2-\varepsilon})" class="latex" /> average-case hardness for any constant <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon&gt;0" class="latex" /> assuming randomized SETH. This indicates that sometimes constructing a “good” polynomial might be a little too ambitious goal.</p>



<p>Instead, can we first come up with a nice combinatorial problem that encodes OV on a slightly larger binary input space and then construct a “good” polynomial for counting solutions of this combinatorial problem? (The motivation is that since this combinatorial problems encodes OV, it is at least as hard as OV for worst case, and moreover, if we can construct a “good” polynomial for counting solutions of this combinatorial problem, by the new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>, counting solutions of this combinatorial problem for average case is (almost) as hard as that for worst case. Therefore, counting solutions of this combinatorial problem for average case is at least as hard as OV for worst case. More importantly, this combinatorial problem could serve as the source of reductions thanks to its combinatorial structure.) The factored OV problem introduced in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> gives a positive answer to this question. Analogously, they proposed factored variants for many other flagship fine-grained hard problems. By reductions to these factored problems, they managed to prove average-case fine-grained hardness for many natural combinatorial problems such as counting regular expression matchings (the featured image of this post is the web of reductions in their paper).</p>



<p>Next, for the purpose of exposition, I briefly sketch the high-level idea behind the factored OV problem (without even explicitly describing its combinatorial interpretation, since I will not show any reduction from this problem). </p>



<p><strong>Counting solutions for factored OV.</strong>  Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" /> (actually, <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog%5E2+n%2F%5Clog%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log^2 n/\log\log n)" class="latex" /> would also work, and the choice here is for simplicity), we encode <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%3A%3D%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29%5Ccirc+%5Ctextrm%7BLONG%7D%28x%5B%5Csqrt%7Bd%7D%2B1%3A2%5Csqrt%7Bd%7D%5D%29%5Ccirc%5Cdots%5Ccirc%5Ctextrm%7BLONG%7D%28x%5Bd-%5Csqrt%7Bd%7D%2B1%3Ad%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x):=\textrm{LONG}(x[1:\sqrt{d}])\circ \textrm{LONG}(x[\sqrt{d}+1:2\sqrt{d}])\circ\dots\circ\textrm{LONG}(x[d-\sqrt{d}+1:d])" class="latex" />,<br />where <img src="https://s0.wp.com/latex.php?latex=x%5Bi%3Aj%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x[i:j]" class="latex" /> represents the subvector (block) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> from the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th to the <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="j" class="latex" />-th coordinate, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> is the long code encoding (specifically, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}" class="latex" /> maps a <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sqrt{d}" class="latex" />-dimensional binary vector <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> to a <img src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2^{\sqrt{d}}" class="latex" />-dimensional vector binary vector <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x')" class="latex" /> of which all the coordinates are zero except the coordinate indexed by <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x'" class="latex" />). Namely, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> is the concatenation of the long code encoding of each block of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" />, and thus, <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" />, and for our choice of <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />, we have that <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7Bn%5E%7Bo%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)\in\{0,1\}^{n^{o(1)}}" class="latex" />. Therefore, by taking such encoding for the vectors in the OV instance, we blow up the size of input (and hence weaken the worst-case hardness of OV) very mildly.</p>



<p>The key advantage of such encoding is that the indicator function <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%5Cperp+y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[1:\sqrt{d}]\perp y[1:\sqrt{d}])" class="latex" /> (which outputs <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> if the two vectors are orthogonal and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0" class="latex" /> otherwise) can be represented as the sum of degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> monomials, of which the variables are the coordinates of <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" />. Indeed, we can first enumerate all the orthogonal pairs of vectors <img src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2%5Cin+%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1,v_2\in \{0,1\}^{\sqrt{d}}" class="latex" />, and we check whether <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_1" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" /> has value <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1" class="latex" /> at coordinate <img src="https://s0.wp.com/latex.php?latex=v_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_2" class="latex" />, by taking product of these two coordinates, and then, we take the sum of all the products.</p>



<p>Using this approach, for each <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, for each <img src="https://s0.wp.com/latex.php?latex=i%5Cin%5B%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i\in[\sqrt{d}]" class="latex" />, we get a degree-<img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2" class="latex" /> polynomial that computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%5Cperp+y%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x[(i-1)\sqrt{d}+1:i\sqrt{d}]\perp y[(i-1)\sqrt{d}+1:i\sqrt{d}])" class="latex" /> on input <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(y)" class="latex" />. The product of these polynomials obviously computes <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5Cperp+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{1}(x\perp y)" class="latex" />, and moreover, this product is a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial (<img src="https://s0.wp.com/latex.php?latex=d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'" class="latex" />-partite polynomial is defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>), where each part corresponds to the coordinates of the long code encoding of a block (subvector) of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y" class="latex" />. If we sum up these <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomials for all pairs <img src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in X, y\in Y" class="latex" />, we get a <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial that precisely counts orthogonal pairs for the OV instance. We can let the field size of this polynomial be a prime <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" /> (<img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^2" class="latex" /> is a trivial upper bound of the number of orthogonal pairs) such that the output of this polynomial is indeed the number of orthogonal pairs.</p>



<p>Notice that (i) Since the encoding only blows up the input size mildly, the worst-case hardness of OV (almost) carries over to evaluating this polynomial. (ii) Since <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o((\log n/\log\log n)^2)" class="latex" />, the <img src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2\sqrt{d}" class="latex" />-partite polynomial is a “good” polynomial (defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>). It follows from our new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> that evaluating this polynomial on binary input for average case (here “average case” means Erdős–Rényi random input model) is (almost) as hard as worst case. (iii) Last but not least, evaluating this polynomial on any <img src="https://s0.wp.com/latex.php?latex=z%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="z\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" /> (not just <img src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\textrm{Enc}(x)" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^d" class="latex" />) can be interpreted as counting solutions for a combinatorial problem, which is the factored OV problem in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>. As I mentioned earlier, I will not explain the combinatorial interpretation in details. The takeaway is that counting solutions of such factored problem is average-case fine-grained hard, and its combinatorial structure allows possible reductions to other natural combinatorial problems.</p>



<p>Finally, I mention two broad research directions in this area: (i) design cryptographic primitives, e.g., one-way functions, based on these fine-grained average-case hardness results (or show complexity barriers) and (ii) prove fine-grained average-case hardness for decision problems (or design more efficient algorithms).</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk. </p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/"><span class="datestr">at August 06, 2021 03:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks.html">Predicting weighted ranks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This week’s events have included Olympic sport climbing, for the first time. The NBC livestream of the women’s qualification suffered a bit of an embarrassment, though, as the computer display of the results incorrectly showed some competitors as being guaranteed to qualify when they were not. (Rest of post contains spoilers; don’t click if you don’t want to know about the outcomes of the event.)</p>

<p>For example, the screenshot below shows Viktoriia Meshkova as a qualifier, with Janja Garnbret still to climb, but Meshkova was actually eliminated after Garnbret’s climb. The commentators noticed the problem and had to tell the audience not to pay attention to that part of the display. What went wrong, and how could it have been done correctly?</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" /></p>

<h1 id="background">Background</h1>

<p>The important things to know about this event are:</p>

<ul>
  <li>
    <p>It involves three disciplines, speed, bouldering, and lead, in that order. One discipline finishes before the next one starts (in fact there is a long rest period between each two disciplines).</p>
  </li>
  <li>
    <p>The scores from each discipline are turned into orderings, giving each of the competitors a number from 1 to 20, before combining them into a single outcome.
Each of 20 competitors gets a rank from 1 to 20 in each of the three disciplines.</p>
  </li>
  <li>
    <p>These numbers are multiplied and the eight competitors with the <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">smallest product of ranks</a> advance to the final round.</p>
  </li>
  <li>
    <p>In lead climbing, the ranks are based on how high each competitor climbs, with ties broken by time, so any competitor who has not yet climbed can slot into the ranking in any position.</p>
  </li>
</ul>

<p>In screens like the one shown above, the livestream showed the standings of the competitor, ordered by their product of ranks: the product of two ranks for competitors who had not yet climbed and the product of all three current ranks for competitors who had already climbed. It predicted qualification for competitors who had already climbed and were in the top eight in this ordering. This seems reasonable, at first glance: the ordering among the people who have already climbed is set, and the people who have not yet climbed can only go down in the ordering, so they will stay in the top eight.</p>

<p>But the ordering among competitors who had already climbed is <em>not</em> set. In the example shown above, before Janja Garnbret climbed, Meshkova was ahead of two other climbers, Aleksandra Mirosław and Anouck Jaubert, both of whom had done well in speed and badly in lead. Garnbret climbed better than Meshkova, bumping Meshkova’s lead ranking down from fourth to fifth. That hurt Meshkova’s combined score a lot more than it hurt Mirosław’s and Jaubert’s, because Mirosław and Jaubert already had very high ranks in lead. At the end of the competition, Meshkova was behind Mirosław and Jaubert, and out of the competition.</p>

<h1 id="analysis">Analysis</h1>

<p>We can formulate this as an algorithms problem: Suppose we have \(n\) competitors, numbered \(1\dots n\), each with a weight \(w_i\) (their combined score from the previous disciplines). We have selected an ordering on a set \(X\) of the competitors, while the remaining set \(Y\) have yet to be ordered. The eventual ordering on \(X\cup Y\) must be consistent with the ordering we already know on \(X\). If competitor \(i\) ends up in position \(p_i\), they get a combined score \(w_i\cdot p_i\) and a combined rank based on sorting these combined scores. (To keep the terminology from being confused, I’ll stick to “ordering” for the result of a single discipline, and “ranking” for the combined result of the whole competition.) What we want to know, for each competitor number \(i\), is: what is the maximum possible combined rank \(r_i\)? Before formulating an algorithm for this problem, let’s do some analysis to find simplifying assumptions that can make the algorithm fast.</p>

<p>For the climbing competition, \(n=20\) and we only care whether \(r_i\le 8\) or \(r_i&gt;8\): is competitor \(i\) guaranteed a spot in the final or not? More generally, we can ask the same question for any \(n\) and any threshold on the combined rank. We can also ask this question regardless of whether competitor \(i\) has already competed (that is, whether \(i\in X\)): if not, it’s safe to assume that they will end up last in the ordering, because that’s the slot that will give them the maximum combined rank.</p>

<p>For each competitor \(j\) in the set \(Y\) of not-yet-ordered competitors, it’s always safe to assume that \(j\) will slot in somewhere above \(i\) in the final ordering. Slotting in immediately above \(i\) is always worse for \(i\) than slotting in anywhere below \(i\), because it hurts the ranking for \(i\) more than it hurts anyone else. Therefore, it can only cause \(i\) to go down among the combined rankings of competitors who are already ordered. Once we make this assumption, we know the final position \(p_i\) of competitor \(i\) in our ordering, and therefore we also know the final combined score \(w_i\cdot p_i\). This assumption lets us determine which final scores beat \(i\), and (because the orderings are also fixed by this assumption) determines which of the competitors ordered later than \(i\) in \(X\) end up beating \(i\). What remains to be determined is which of the competitors ordered earlier than \(i\) in \(X\) might beat \(i\), and which of the competitors in \(Y\) might <span style="white-space: nowrap;">beat \(i\).</span></p>

<p>We don’t know how many competitors in \(Y\) beat \(i\), but we can guess; there are only \(\vert Y\vert\lt n\) possibilities to try. Suppose that we guess that this number is \(k\). If our guess is correct, then we can safely assume that these \(k\) better competitors are the ones in \(Y\) with the \(k\) smallest weights. Any other outcome that puts \(k\) competitors in \(Y\) ahead of \(i\) can be swapped to an outcome that puts these \(k\) competitors ahead, without changing the ranks of any competitors <span style="white-space: nowrap;">in \(X\).</span></p>

<p>Once we know which \(k\) competitors in \(Y\) beat \(i\), we also know how good a position \(p_j\) each of these competitors will need to attain, to beat \(i\). We can assign these competitors to these positions, breaking ties by moving some up into higher positions, determining where they all slot into the overall ordering. Among all assignments of positions to these competitors that puts them ahead of \(i\), this is the one that hurts the other competitors of \(i\) the least. Once this is done, we can safely assign all of the remaining competitors in \(Y\) to an ordering that slots them in just ahead of \(i\), again hurting \(i\) while causing the least hurt to the competitors <span style="white-space: nowrap;">of \(i\).</span></p>

<p>With the ordering of competitors completely determined by the choice of \(k\), all we need to do is try all choices of \(k\) and test which ones put the largest number of competitors ahead <span style="white-space: nowrap;">of \(i\)!</span></p>

<p>There is a complication here with tiebreaks that I am not handling. If two competitors get the same combined score, the one who is ahead in two of the three disciplines gets the higher combined rank. If three competitors get the same combined score, and they have a cyclic ordering on tiebreaks, I don’t know what happens, and I suspect the rules don’t cover that situation. To simplify things I will just assume that all tiebreaks go against the candidate (so we might not guarantee qualification until the ties are resolved).</p>

<h1 id="algorithm">Algorithm</h1>

<p>Based on these simplifications, our algorithm for computing the maximum combined rank \(r_i\) of competitor \(i\) performs the following steps:</p>

<ul>
  <li>
    <p>If \(i\) is not already in \(X\), add it to \(X\) with a position after all of the other competitors <span style="white-space: nowrap;">in \(X\).</span></p>
  </li>
  <li>
    <p>Loop through all choices of \(k\) in \(0\dots\vert Y\vert\). For each choice:</p>

    <ul>
      <li>
        <p>Determine, for each \(j\) in the \(k\) smallest-weight competitors in \(Y\), the position \(p_j\) that \(j\) would need to obtain to <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
      <li>
        <p>While any two of these competitors have the same value for \(p_j\), decrement one of these two values. If this causes any value to become non-positive, continue the outer loop with the next choice <span style="white-space: nowrap;">of \(k\).</span></p>
      </li>
      <li>
        <p>Place the competitors in \(Y\) that are not among the \(k\) smallest immediately above \(i\) in the ordering</p>
      </li>
      <li>
        <p>Place the \(k\) smallest-weight competitors into positions \(p_j\), preserving the ordering of the remaining competitors.</p>
      </li>
      <li>
        <p>In the resulting ordering, determine how many competitors <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Return the maximum number of competitors beating \(i\) in all of the orderings that have been examined</p>
  </li>
</ul>

<p>With some care it should be possible to do all of this in time \(O(nk)\). <a href="https://11011110.github.io/blog/assets/2021/rank-product/qualify.py">My implementation</a> is slower because I was more interested in getting it to work than in optimizing it, and for \(n=20\) it is blazingly fast regardless.</p>

<h1 id="outcomes">Outcomes</h1>

<p>With all that in mind, and assuming that (somehow) I’ve implemented this correctly, let’s look at what this algorithm predicts for the actual data.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with five climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/2.jpg" /></p>

<p>At this stage of the qualifications, five competitors are left to climb. NBC predicted that Seo, Raboutou, and Pilz had already qualified, and my implementation agrees for Seo and Raboutou (both can finish at worst 8th) but it was incorrect for Pilz, who could drop to 9th if Mirosław miraculously finished 2nd and Garnbret 3rd. More surprising to me, Garnbret was still not an automatic qualifier: if she finished 20th, enough other competitors could better her score to put her into 9th place.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with three climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/3.jpg" /></p>

<p>After two more climbs, the faulty NBC algorithm claims that five climbers have qualified. My program agrees for Seo (worst rank 6), Nonaka (worst rank 5), Raboutou (worst rank 7), but still not Pilz (worst rank 9) or Jaubert (worst rank 10). My code now thinks Garnbret has locked in a rank of at worst 7th, qualifying without even climbing yet.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with two climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/4.jpg" /></p>

<p>Shauna Coxsey, climbing with an injured knee, has climbed into the middle of the pack, falling off the top ten ranking, and her place has been taken by Kyra Condie. Garnbret is now at worst 6th and should have been marked as qualified. Noguchi is not quite guaranteed yet, with a scenario in which she could rank 9th. Seo is now at worst 5th, Nonaka at worst 4th, Raboutou at worst 5th, and Pilz at worst 8th (now guaranteeing her spot). But Jaubert and Mirosław each could end 9th; at most one of Jaubert, Mirosław, or Noguchi will be eliminated, but we can’t yet guarantee any of their spots.</p>

<p style="text-align: center;"><img width="80%" alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" /></p>

<p>Until now, all of the “Q” markings shown on the livestream, while mathematically incorrect in many cases, were at least correct in hindsight: the people marked that way did end up qualifying. This one, though, a repeat of the first image in this post, gets it wrong in practice as well as in theory. Meshkova is marked as qualifying, but did not. Coxsey has moved back into the top ten. Mirosław and Jaubert could still have ended up 9th (under different scenarios, obviously) and should not have been marked as qualifying. Seo is at worst 4th, Nonaka is at most 3rd, Noguch is at most 4th, Raboutou is at most 5th, and Pilz is at most 6th.</p>

<p>And the final ranking:</p>

<p style="text-align: center;"><img width="80%" alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, top ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6a.jpg" /></p>

<p style="text-align: center;"><img width="80%" alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, bottom ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6b.jpg" /></p>

<h1 id="the-future">The future</h1>

<p>I think the moral of the story is that this ranking system is too hard to understand and a little too random (with players trading places too much depending on what other players do). To some extent this is unavoidable (a version of <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem">Arrow’s impossibility theorem for rank aggregation</a>), but the scuttlebutt seems to be that this system will be replaced for future competitions. Which, sadly, makes all of this algorithm design a little redundant…</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106707759220927820">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks.html"><span class="datestr">at August 05, 2021 11:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5445216218770736629">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html">Pole Vault Live Blogging</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As I write this I'm watching the women's pole vault final in the Olympics. Of the 15 women who made the finals, only four remain after two heights.</p><p>To expand on my <a href="https://twitter.com/fortnow/status/1421510617878437891">tweet</a>, I find the pole vault the purest of the Olympic Sports. No electronic monitors and timers, no biased judges, no video review. No points deducted for bad form or failing to stick the landing. No disqualification for a false start or stepping over a line. Either you clear the bar without knocking it down, or you don't.</p><p>The high jump has similar properties, but just not as cool looking.</p><p>All four made the third height. Now onto 4.85 meters. An American, a Greek, a Brit and a Russian (sorry I meant member of the Russian Olympic Committee).</p><p>Back in the day, the TV coverage was rather limited. We'd only see the Americans and the medal winners with too much time spend on human interest backgrounds. Now in the streaming world I can watch every competitor. The good and the bad. Live as it happens.</p><p>The Russian Anzhelika Sidorova just cleared 4.85 on her first attempt. So did the Brit Holly Bradshaw and the American Katie Nageotte. The Greek Katerina Stefanidi missed her first attempt but decided to pass on the rest. All now go to 4.90 but Stefanidi only gets two attempts while the rest get three.</p><p>Stefanidi missed her first attempt at 4.90. She gets one attempt left.</p><p>Sidorova and Bradshaw fail to even reach the bar. Nageotte can't clear the bar.</p><p>Now the moment that means everything for Stefanidi. Her last attempt. Make it or the rest get the medals. Stefaidi fails to get a good plant and doesn't get into the air at all. Her Olympics are over.</p><p>Second attempt for the others. Sidorva and Bardshaw knock down the bar. Nageotte clears the bar, putting her in prime position. Go USA!</p><p>Imagine if we judged research papers this way. Either they get into a conference or they don't. Wait, that is they way they happen, although not always without biased judging.</p><p>Sidorova is passing on her last attempt at 4.90. Bradshaw goes for it but hits the bar. She has to settle for Bronze.</p><p>Bar is now at 4.95 meters. </p><p>Sidorova gets only one attempt at 4.95. If she makes it, she takes the lead, if she misses, she gets the silver. </p><p>Sidorova doesn't clear and the gold goes to the American Katie Nageotte! </p><p>Just for excitement Nageotte is going for 5.01 meters, which would be her first over five meters in competition. In the men's pole vault, the Swede Armand Duplantis (great pole vault name!) easily won the gold. He moved the bar to 6.19 meters to break his own world record. Came all so close in his first attempt but failed to clear. </p><p>Nageotte is just too excited winning the gold to focus enough to make a serious attempt at 5.01. Can't blame her.</p><p>Thus ends the best sport in the Olympics.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/s797/polevault.png"><img width="400" src="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/w400-h229/polevault.png" border="0" height="229" /></a></div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html"><span class="datestr">at August 05, 2021 01:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21878">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/">Let me tell you about three of my recent papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img width="605" alt="michaelrabin" src="https://gilkalai.files.wordpress.com/2021/08/michaelrabin.jpg" class="alignnone size-full wp-image-21893" height="403" /></p>
<p> </p>
<p>Let me tell you briefly about three of my papers that were recently accepted for publication. <a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i≥d and S⊂V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V∖σ];F)=0 for all i≥d and σ∈Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a> with Roy Meshulam, <a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly-type problems</a> with Imre Bárány, and <a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a> with Yosi Rinott and Tomer Shoham.</p>
<h3><a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i≥d and S⊂V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V∖σ];F)=0 for all i≥d and σ∈Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a></h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice. </em></span></p>
</blockquote>
<p>Roy and I have a<a href="https://scholar.google.com/scholar?hl=iw&amp;as_sdt=0%2C5&amp;q=kalai+and+meshulam&amp;btnG="> long term project</a> of studying topological Helly type theorems. Often, results from convexity give a simple and strong manifestation of theorems from topology: For example, Helly’s theorem manifests the nerve theorem from algebraic topology, and Radon’s theorem can be regarded as an early “linear” version of the Borsuk–Ulam theorem. We have a few more “linear” theorems in need of topologizing on our list. Actually the paper <a href="https://londmathsoc.onlinelibrary.wiley.com/doi/full/10.1112/mtk.12103">already appeared in Mathematika</a> on June 26, 2021. It is dedicated to our dear teacher, colleague and friend  Michael O. Rabin. </p>
<blockquote>
<h3><span style="color: #993300;"> Dedicated to Michael O. Rabin, a trailblazing mathematician and computer scientist</span></h3>
</blockquote>
<h3><a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly type problems</a>, to appear in the Bulletin of the American Mathematical Society </h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We present a variety of problems in the interface between combinatorics and geometry around the theorems of Helly, Radon, Carath ́eodory, and Tverberg. Through these problems we describe the fascinating area of Helly-type theorems, and explain some of its main themes and goals.</em></span></p>
</blockquote>
<p>Imre and I have long term common interest in Helly-type problems and often discussed it since we first met in 1982.  We wrote a first joint paper in 2016 and last year we wrote two additional papers with Attila Por.  Last year Imre wrote a great book  “Combinatorial convexity” (AMS, 2021, in press) largely devoted to Helly-type theorems. As for me, I plan on gradually writing on open problems related to my areas of interest. (See <a href="https://www.renyi.hu/conferences/erdos100/slides/kalai.pdf">these slides</a> for some problems.)   </p>
<h3><a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a></h3>
<p>Yosi Rinott, Tomer Shoham and I started this project about a year an a half ago. Our paper have now been accepted to Statistical Science where you can <a href="https://www.e-publications.org/ims/submission/STS/user/submissionFile/47360?confirm=ed13d436">download the accepted version</a> along <a href="https://imstat.org/journals-and-publications/statistical-science/statistical-science-future-papers/">many other future papers</a>. This is my second paper in Statistical Science. The first one was “<a href="https://projecteuclid.org/journals/statistical-science/volume-14/issue-2/Solving-the-Bible-Code-Puzzle/10.1214/ss/1009212243.full">Solving the bible code puzzle</a>” with Brendan McKay, Dror Bar-Nathan and Maya Bar-Hillel, that appeared in 1999.     </p>
<blockquote>
<p><span style="color: #ff0000;"><em>In quantum computing, a demonstration of quantum supremacy (or quantum advantage) consists of presenting a task, possibly of no practical value, whose computation is feasible on a quantum device, but cannot be performed by classical computers in any feasible amount of time. The notable claim of quantum supremacy presented by Google’s team in 2019 consists of demonstrating the ability of a quantum circuit to generate, albeit with considerable noise, bitstrings from a distribution that is considered hard to simulate on classical computers. Very </em></span><span style="color: #ff0000;"><em>recently, in 2020, a quantum supremacy claim was presented by a group from the University of Science and Technology of China, using a different technology and generating a different distribution, but sharing some statistical principles with Google’s demonstration. </em></span></p>
<p><span style="color: #ff0000;"><em>Verifying that the generated data is indeed from the claimed distribution and assessing the circuit’s noise level and its fidelity is a statistical undertaking. The objective of this paper is to explain the relations between quantum computing and some of the statistical aspects involved in demonstrating quantum supremacy in terms that are accessible to statisticians, computer scientists, and mathematicians. Starting with the statistical modeling and analysis in Google’s demonstration, which we explain, we study various estimators of the fidelity, and different approaches to testing the distributions generated by the quantum computer. We propose different noise models, and discuss their implications. A preliminary study of the Google data, focusing mostly on circuits of 12 and 14 qubits is given in different parts of the paper</em></span></p>
</blockquote>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/"><span class="datestr">at August 05, 2021 11:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19028">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/">Turning the Tables on Cheating?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Colonel Stok: Do you play chess?</em></font></p><font color="#0044cc"><em>
</em><p><em>
Harry Palmer: Yes, but I prefer a game with a better chance of cheating.</em><br />
<font color="#000000"></font></p><font color="#000000">
<p></p><p></p>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/sleuth-1972-screencaps-michael-caine-5575427-550-330/" rel="attachment wp-att-19031"><img width="154" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/Sleuth-1972-Screencaps-michael-caine-5575427-550-330.jpg?resize=154%2C100&amp;ssl=1" class="alignright size-full wp-image-19031" height="100" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><i>Sleuth</i> <a href="https://www.storypick.com/memorable-michael-caine-roles/">source</a></font></td>
</tr>
</tbody>
</table>
<p></p><p>
Michael Caine played <a href="https://en.wikipedia.org/wiki/Harry_Palmer">Harry Palmer</a> in the movie <a href="https://www.quotes.net/mquote/35048">Funeral in Berlin</a>. This was released in 1966—long before computers could play terrific chess. Perhaps he would have a different answer today?</p>
<p>
Today I thought we might look at chess cheating in a way that complements what Ken does.</p>
<p>
Of course Ken refers to Ken Regan. He is one of the world experts at detecting chess cheating. Detection is based solely on the statistics of move choice. The only data given to Ken about the games are the moves that were played and the overall time allowance. But this ignores the players and equipment on the scene. People can cheat in ways that are closer to issues in computer security and protocols.</p>
<p>
Caine knows a lot about the latter. He is a doyen of <a href="https://filmsane.com/5-of-my-favorite-michael-caine-heist-movies/">heist movies</a> and those increasingly involve security. In the 1966 comedy <a href="https://en.wikipedia.org/wiki/Gambit_(1966_film)">Gambit</a> it is as simple as working around an alarm, but the 1969 version of <a href="https://en.wikipedia.org/wiki/The_Italian_Job">The Italian Job</a> has a switch of computer data reels and jamming traffic cameras. His latest movie <a href="https://en.wikipedia.org/wiki/Tenet_(film)">Tenet</a> centers on an algorithm for inverting time and entropy on Earth. He is also a fan of chess. He tangles with his co-star Laurence Olivier <a href="http://www.chess-in-the-cinema.de/showfilm.php?filmfile=7258.txt&amp;pfad=7079">amid</a> chess sets in the movie <a href="https://en.wikipedia.org/wiki/Sleuth_(1972_film)">Sleuth</a>. Most of all, his character in 2009’s <a href="https://en.wikipedia.org/wiki/Harry_Brown_(film)">Harry Brown</a> is a chess player, who <a href="https://www.dailymotion.com/video/xqr41b">discourses</a> on the <a href="https://www.chessgames.com/perl/chessgame?gid=1044731">17th</a> (not 7th as said) game of the 1972 championship between Bobby Fischer and Boris Spassky. </p>
<p>
</p><p></p><h2> Cheating at Chess—the Easy Way </h2><p></p>
<p></p><p>
Derren Brown is an <a href="https://derrenbrown.co.uk">illusionist</a>—a magician. He claims that he is a weak chess player. But he had Britain’s Channel 4 broadcast him playing nine strong players, including two grandmasters. Yet he won the match <b>5-4</b>. </p>
<p>
This is how he did it. He used an ancient <a href="https://en.wikipedia.org/wiki/Cheating_in_chess#Simultaneous_games">trick</a>. Say he plays two games: one against Alice and one against Bob. He plays black against Alice and white against Bob. After he gets Alice’s first move he plays that exact move against Bob. Then after he gets Bob’s move he plays that one against Alice. And so on.</p>
<p>
Suppose he loses both games. Thus Alice wins and Bob wins. But that means that Bob’s answer to Alices’ move was a winner and so on. This is impossible and so he must win at least one of the games. Thus he wins one game unless both end in a tie. </p>
<p>
The illusion is, how could he win five games against nine players given two were grandmasters? Brown played one of the nine games for real—he won that one. The “table trick” only works for an even number of games. It is not, of course, a new <a href="https://en.chessbase.com/post/the-magical-che-experiment">idea</a>: </p>
<blockquote><p><b> </b> <em> Alekhine and his twice world championship challenger Bogoljubov were once challenged separately by a relative patzer to games of correspondence chess at money odds. In effect, of course, the anonymous opportunist was playing in neither game. The story goes that the two players, who were friends away from the board, met up one day and latched on to what was happening. </em>
</p></blockquote>
<p></p><p>
The imitation trick is a real potential issue in <em>Basque chess</em>, where two players play two games simultaneously, one as white and one as black. By copying each other’s moves, they would always tie. An early <a href="https://en.chessbase.com/post/che-magazine-basque-che-does-it-work-for-you-">description</a> noted the issue but Ken has not been able to find how the rules forbid it. </p>
<p>
A similar situation <a href="https://www.chess.com/article/view/chess-arbiters">happened</a> recently in a real tournament—a world championship qualifier, no less. Two games at adjacent tables played almost twenty of the same opening moves. The chief arbiter—someone Ken corresponds with several times a week—moved one of the games to a different area. International Master Danny Rensch, who heads the major online playing site <a href="https://www.chess.com/">Chess.com</a>, made a <a href="https://youtu.be/-6QX53BmDbg">video</a> “Are You Copying Me?” of the incident. None of the four players involved was cheating, but this illustrates the kind of people dynamics one needs to watch for.</p>
<p>
</p><p></p><h2> Cheating at Chess—the Too Easy Way </h2><p></p>
<p></p><p>
This is to cheat by consulting a computer program that is stronger than all human players, such as the free program <a href="https://en.wikipedia.org/wiki/Stockfish_(chess)">Stockfish</a>, without anything impeding one’s ability to access the program’s recommendations during the game. This is often the case in online chess without sophisticated measures to detect the access. </p>
<p>
Ken’s statistical model can still judge the moves, but this is after the fact. We would like to <em>prevent</em> cheating. This hasn’t happened. Already last year, Ken was <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">quoted</a> in the UK Guardian newspaper saying, “The pandemic has brought me as much work in a single day as I have had in a year previously.” A Wall Street Journal <a href="https://www.wsj.com/articles/the-real-queens-gambit-catching-chess-cheaters-11607439491">story</a> that featured Ken also noted:</p>
<blockquote><p><b> </b> <em> The data showed something curious. More people were playing chess. Yet the fair play violations were surging even faster than the number of overall games. “Which makes us think that there has been an uptick in the rate of cheating,” said Gerard Le-Marechal, head of cheat detection for Chess.com. </em>
</p></blockquote>
<p></p><p>
This year has brought no letup—see Ken’s statement prefacing a non-chess <a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/">post</a> that June and July were the worst. Chess.com and Lichess and other playing sites have the final say but Ken is often used for both early warning (his “screening” step is agile and gives officials an informative snapshot of an entire tournament) and for explaining verdicts afterwards, since his model is transparent and not compromised by divulging explanations.</p>
<p>
But again, this is after the fact. I recall a <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> we wrote about ways computer security is like “closing the barn door after the horse has already left.” The open question that I find interesting is not how cheaters can be detected, but is there some way to make it hard for them to cheat at all. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/cainebbb/" rel="attachment wp-att-19037"><img width="295" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/CaineBBB.jpg?resize=295%2C279&amp;ssl=1" class="aligncenter size-full wp-image-19037" height="279" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2">ARPANET history <a href="https://computer.howstuffworks.com/arpanet.htm#pt1">source</a><br />
</font>
</td>
</tr>
</tbody></table>
<p>
Of course, this is not just about chess, or other games forced online by the pandemic. It extends to administering courses and tests, at a time when the prospect of a “normal” in-person Fall semester is being roiled by the surge we’ve <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> and <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">tracked</a> on this blog. </p>
<p>
</p><p></p><h2> Cheating at Chess—the Harder Way </h2><p></p>
<p></p><p>
The number of ways people have cheated at in-person chess is legion. Wikipedia has a long <a href="https://en.wikipedia.org/wiki/Cheating_in_chess">list</a>. Ken put the ways in cases he’d encountered to a Dr. Seuss rhyme midway through his 2014 TEDx Buffalo <a href="https://www.youtube.com/watch?v=9W3D8xVAKao">talk</a>. </p>
<p>
On March 15, 2020, the New York Times published an <a href="https://www.nytimes.com/2020/03/15/sports/chess-cheating.html">article</a> on tech in chess cheating. It drew analogy to the Houston Astros scandal, including the same example we just <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">covered</a> of whether José Altuve was wired for his series-winning home run in 2019. It touched on online chess and quotes Le-Marechal but <em>showed no inkling of</em> the impending pandemic and its effect on chess. Its first sentence about chess alludes to the 1978 incident in which Viktor Korchnoi alleged that Anatoly Karpov could receive coded information about their match games via the flavor of yogurt delivered to the table. </p>
<p>
I, Ken writing this part, have been part of discussions of how a yogurt <em>spoon</em> dropped audibly could be one of myriad possible signals from the audience. The pandemic caused this year’s Tata Steel tournament to be played <a href="https://www.dutchnews.nl/news/2021/01/no-women-at-tata-steel-chess-this-year-but-game-is-growing-in-popularity/">without</a> audience, while some other elite events are played in an “<a href="https://www.chess.com/news/view/bilbao-chess-outside-in-a-glass-cube">aquarium</a>” with one-way glass. But that does not work for larger-scale Open tournaments. Jamming RF signals is generally illegal. I agree with those recommending that an illusionist like Brown—someone with an eagle eye for watching people—be employed to help the arbiters at large events.</p>
<p>
Yet for all the ways and means out there, it is still <em>hard</em> to cheat at in-person chess. Its state is one that organizers of <em>online chess</em> would gladly reach if they could. FIDE has promoted a <a href="https://en.chessbase.com/post/the-rise-of-hybrid-chess">hybrid</a> form in which players travel to regional rooms watched by arbiters, but this is hard to manage on large scale. Dick and I have debated all year what to do for online chess, and we’ve converged on two poles of answers.</p>
<p>
</p><p></p><h2> Way #1: Standardized Playing Tabletops </h2><p></p>
<p></p><p>
The paradox, noted this week by International Master Nisha Mohota in her recent <a href="https://youtu.be/hpLiG1UgIus">video</a> on cheating, is that the popularity of chess online has burgeoned during the pandemic. But this also enhances the following dilemma:</p>
<ul>
<li>
Having a second camera—side view supplementing screen view—has been an effective measure. <p></p>
</li><li>
But requiring even one camera has been an acknowledged obstacle to expanding the reach of chess tournaments. <p></p>
</li><li>
And it takes extra human resources to monitor two video feeds per player.
</li></ul>
<p>
Online education has also <a href="https://www.erasmusmagazine.nl/en/2021/01/15/students-will-have-to-use-phone-as-a-second-camera-in-proctored-online-exams/">recognized</a> the importance of a second camera, <a href="https://www.theabr.org/announcements/remote-exam-information">requiring</a> one in some cases. Yet allowing the user to control how the side camera is positioned may allow circumventions, and mandating one connotes distrust and negativity in a bare sense. </p>
<p>
My suggestion is to try to turn around the negative aspect into a positive by marketing a standardized and hopefully-inexpensive “Online Tabletop Arena.” It would have three walls to feel like an alcove. The walls, one with a side camera built in, would limit hand movements as well as sight lines. Standardization would lessen stigma and help monitoring. It could also be used for online test taking.</p>
<p>
</p><p></p><h2> Way #2: Give In </h2><p></p>
<p></p><p>
The real import of our mentioned security <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> is to stop trying to stick thumbs in all the dam holes. Mohota in her video laments kids being exposed to chess programs and advocates training without them, but that strikes us as trying to close ten thousand barn doors while a million free horses are out there. </p>
<p>
So let’s give in: Allow the players to use computers freely. The more, the merrier. But as in the 1967 Caine-as-Harry-Palmer movie <a href="https://en.wikipedia.org/wiki/Billion_Dollar_Brain">Billion Dollar Brain</a>, we reward the humans for how they <em>disobey</em> the computer calling the shots.</p>
<p>
One way to implement this would be to have the chess playing site appoint one unknown (say, randomly selected) strong chess program <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> as the official scorer of all games. A player’s score for a won or drawn game would be proportional to the total difference from <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> in Ken’s metrics. Perhaps credit could be given also for a valiantly lost game.</p>
<p>
This scheme would directly reward players for the amount of <em>non-</em>cheating they do. Or put more positively, human ingenuity apart from computers would bring the reward. The ability to sleuth strategy beyond computer moves was already <a href="https://www.psychologytoday.com/us/blog/seeing-what-others-dont/201710/the-age-centaurs">demonstrated</a> in so-called <a href="https://en.wikipedia.org/wiki/Advanced_chess">freestyle</a> tournaments held in 2007-08 and 2014. A particularly nice example of playing a sacrifice that the computer does not like was executed at turn 18 by Magnus Carlsen in his World Cup <a href="https://www.chessbomb.com/arena/2021-fide-world-cup/08-01-Fedoseev_Vladimir-Carlsen_Magnus">win</a> today over Vladimir Fedoseev.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We admit the tabletop suggestion is far from electrifying, but has anyone come up with better? As always we welcome suggestions from our readers, or pointers to forum discussions that you agree with.</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/"><span class="datestr">at August 04, 2021 09:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">Complexity Postdoctoral Fellowship at Santa Fe Institute (apply by October 24, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Unique among postdocs, these 3-year fellowships offer the opportunity to join a collaborative community that nurtures creative, transdisciplinary thought in pursuit of key insights about the complex systems that matter most for science and society. Benefits include research/collaboration funds, paid family leave, and a professional leadership &amp; development program.</p>
<p>Website: <a href="https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021">https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021</a><br />
Email: sfifellowship@santafe.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/"><span class="datestr">at August 02, 2021 10:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5675">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5675">On blankfaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For years, I’ve had a private term I’ve used with my family.  To give a few examples of its use:</p>



<blockquote class="wp-block-quote"><p>No, I never applied for that grant. I spent two hours struggling to log in to a web portal designed by the world’s top blankfaces until I finally gave up in despair.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, I paid for that whole lecture trip out of pocket; I never got the reimbursement they promised.  Their blankface administrator just kept sending me back the form, demanding more and more convoluted bank details, until I finally got the hint and dropped it.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, my daughter Lily isn’t allowed in the swimming pool there.  She easily passed their swim test last year, but this year the blankface lifeguard made up a new rule on the spot that she needs to retake the test, so Lily took it again and passed even <em>more</em> easily, but then the lifeguard said she didn’t like the stroke Lily used, so she failed her and didn’t let her retake it.  I complained to their blankface athletic director, who launched an ‘investigation.’  The outcome of the ‘investigation’ was that, regardless of the ground truth about how well Lily can swim, their blankface lifeguard said she’s not allowed in the pool, so being blankfaces themselves, they’re going to stand with the lifeguard.</p></blockquote>



<blockquote class="wp-block-quote"><p>Yeah, the kids spend the entire day indoors, breathing each other’s stale, unventilated air, then they finally go outside and they aren’t allowed on the playground equipment, because of the covid risk from them touching it.  Even though we’ve known for more than a year that covid is an airborne disease.  Everyone I’ve talked there agrees that I have a point, but they say their hands are tied.  I haven’t yet located the blankface who actually made this decision and stands by it.</p></blockquote>



<p>What exactly is a blankface?  He or she is often a mid-level bureaucrat, but not every bureaucrat is a blankface, and not every blankface is a bureaucrat.  A blankface is anyone who enjoys wielding the power entrusted in them to make others miserable by acting like a cog in a broken machine, rather than like a human being with courage, judgment, and responsibility for their actions.  A blankface meets every appeal to facts, logic, and plain compassion with the same repetition of rules and regulations and the same blank stare—a blank stare that, more often than not, conceals a contemptuous smile.</p>



<p>The longer I live, the more I see blankfacedness as one of the fundamental evils of the human condition.  Yes, it contains large elements of stupidity, incuriosity, malevolence, and bureaucratic indifference, but it’s not reducible to any of those.  After enough experience, the first two questions you ask about any organization are:</p>



<ol><li>Who are the blankfaces here?</li><li>Who are the people I can talk with to get around the blankfaces?</li></ol>



<p>As far as I can tell, blankfacedness cuts straight across conventional political ideology, gender, and race.  (Age, too, except that I’ve never once encountered a blankfaced child.)  Brilliance and creativity do seem to offer some protection against blankfacedness—possibly because the smarter you are, the harder it is to justify idiotic rules to yourself—but even there, the protection is far from complete.</p>



<hr class="wp-block-separator" />



<p>Twenty years ago, all the conformists in my age cohort were obsessed with the <em>Harry Potter</em> books and movies—holding parties where they wore wizard costumes, etc.  I decided that the <em>Harry Potter</em> phenomenon was a sort of collective insanity: from what I could tell, the stories seemed like startlingly puerile and unoriginal mass-marketed wish-fulfillment fantasies.</p>



<p>Today, those same conformists in my age cohort are more likely to condemn the <em>Harry Potter</em> series as Problematically white, male, and cisnormative, and J. K. Rowling herself as a monstrous bigot whose acquaintances’ acquaintances should be shunned.  Naturally, then, there was nothing for me to do but finally read the series!  My 8-year-old daughter Lily and I have been partner-reading it for half a year; we’re just finishing book 5.  (<em>After</em> we’ve finished the series, we might start on <em><a href="http://www.hpmor.com/">Harry Potter and the Methods of Rationality</a></em> … which, I confess, I’ve also never read.)</p>



<p>From book 5, I learned something extremely interesting.  The most despicable villain in the <em>Harry Potter</em> universe is not Lord Voldemort, who’s mostly just a faraway cipher and abstract embodiment of pure evil, no more hateable than an earthquake.  Rather, it’s <a href="https://en.wikipedia.org/wiki/Dolores_Umbridge">Dolores Jane Umbridge</a>, the toadlike Ministry of Magic bureaucrat who takes over Hogwarts school, forces out Dumbledore as headmaster, and terrorizes the students with increasingly draconian “Educational Decrees.”  Umbridge’s decrees are mostly aimed at punishing Harry Potter and his friends, who’ve embarrassed the Ministry by telling everyone the truth that Voldemort has returned and by readying themselves to fight him, thereby defying the Ministry’s head-in-the-sand policy.</p>



<p>Anyway, I’ll say this for <em>Harry Potter</em>: Rowling’s portrayal of Umbridge is so spot-on and merciless that, for anyone who knows the series, I could simply <em>define</em> a blankface to be anyone sufficiently Umbridge-like.</p>



<hr class="wp-block-separator" />



<p>This week I <em>also</em> finished reading <em><a href="https://www.amazon.com/Premonition-Pandemic-Story-Michael-Lewis-ebook/dp/B08V91YY8R">The Premonition</a></em>, the thrilling account of the runup to covid by <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> (who also wrote <em><a href="https://www.amazon.com/Big-Short-Inside-Doomsday-Machine/dp/0393338827">The Big Short</a></em>, <em><a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a></em>, etc).  Lewis tells the stories of a few individuals scattered across US health and government bureaucracies who figured out over the past 20 years that the US was breathtakingly unprepared for a pandemic, and who struggled against official indifference, mostly unsuccessfully, to try to fix that.  As covid hit the US in early 2020, these same individuals frantically tried to pull the fire alarms, even as the Trump White House, the CDC, and state bureaucrats all did everything in their power to block and sideline them.  We all know the results.</p>



<p>It’s no surprise that, in Lewis’s telling, Trump and his goons come in for world-historic blame: however terrible you thought they were, they were worse.  It seems that John Bolton, in particular, gleefully took an ax to everything the two previous administrations had done to try to prepare the federal government for pandemics—after Tom Bossert, the one guy in Trump’s inner circle who’d actually taken pandemic preparation seriously, was forced out for contradicting Trump about Russia and Ukraine.</p>



<p>But the left isn’t spared either.  The most compelling character in <em>The Premonition</em> is <a href="https://en.m.wikipedia.org/wiki/Charity_Dean">Charity Dean</a>, who escaped from the Christian fundamentalist sect in which she was raised to put herself through medical school and become a crusading public-health officer for Santa Barbara County.  Lewis relates with relish how, again and again, Dean startled the bureaucrats around her by taking matters into her own hands in her war against pathogens—e.g., slicing into a cadaver herself to take samples when the people whose job it was wouldn’t do it.</p>



<p>In 2019, Dean moved to Sacramento to become California’s next chief public health officer, but then Governor Gavin Newsom blocked her expected promotion, instead recruiting someone from the outside named Sonia Angell, who had no infectious disease experience but to whom Dean would have to report.  Lewis reports the following as the reason:</p>



<blockquote class="wp-block-quote"><p>“It was an optics problem,” says a senior official in the Department of Health and Human Services.  “Charity was too young, too blond, too Barbie.  They wanted a person of color.”  Sonia Angell identified as Latina.</p></blockquote>



<p>After it became obvious that the White House and the CDC were both asleep at the wheel, the competent experts’ Plan B was to get California to set a national standard, one that would shame all the other states into acting, by telling the truth about covid and by aggressively testing, tracing, and isolating.  And here comes the tragedy: Charity Dean spent from mid-January till mid-March trying to do exactly that, and Sonia Angell blocked her.  Angell—who comes across as a real-life Dolores Umbridge—banned Dean from using the word “pandemic,” screamed at her for her insubordination, and systematically shut her out of meetings.  Angell’s stated view was that, until and unless the CDC said that there was a pandemic, <em>there was no pandemic</em>—regardless of what hospitals across California might be reporting to the contrary.</p>



<p>As it happens, California <em>was</em> the first state to move aggressively against covid, on March 19—basically because as the bodies started piling up, Dean and her allies finally managed to maneuver around Angell and get the ear of Governor Newsom directly.  Had the response started earlier, the US might have had an outcome more in line with most industrialized countries.  Half of the 630,000 dead Americans might now be alive.</p>



<p>Sonia Angell fully deserves to have her name immortalized by history as one of the blankest of blankfaces.  But of course, Angell was far from alone.  Robert Redfield, Trump’s CDC director, was a blankface extraordinaire.  Nancy Messonnier, who lied to stay in Trump’s good graces, was a blankface too.  The entire CDC and FDA seem to have teemed with blankfaces.  As for Anthony Fauci, he became a national hero, maybe even deservedly so, merely by <em>not being 100%</em> a blankface, when basically every other “expert” in the US with visible power was.  Fauci cleared a depressingly low bar, one that the people profiled by Lewis cleared at Simone-Biles-like heights.</p>



<p>In March 2020, the fundamental question I had was: where are the supercompetent rule-breaking American heroes from the disaster movies?  What’s taking them so long?  <em>The Premonition</em> satisfyingly answers that question.  It turns out that the heroes did exist, scattered across the American health bureaucracy.  They were screaming at the top of their lungs.  But they were outvoted by the critical mass of blankfaces that’s become one of my country’s defining features.</p>



<hr class="wp-block-separator" />



<p>Some people will object that the term “blankface” is dehumanizing.  The reason I disagree is that a blankface is someone who freely chose to dehumanize <em>themselves</em>: to abdicate their human responsibility to see what’s right in front of them, to <em>act like</em> malfunctioning pieces of electronics even though they, like all of us, were born with the capacity for empathy and reason.</p>



<p>With many other human evils and failings, I have a strong inclination toward mercy, because I understand how someone could’ve succumbed to the temptation—indeed, I worry that I myself might’ve succumbed to it “but for the grace of God.”  But here’s the thing about blankfaces: in all my thousands of dealings with them, not once was I ever given cause to wonder whether I might have done the same in their shoes.  It’s like,<em> of course</em> I wouldn’t have!  Even if I were forced (by my own higher-ups, an intransigent computer system, or whatever else) to foist some bureaucratic horribleness on an innocent victim, I’d be sheepish and apologetic about it.  I’d acknowledge the farcical absurdity of what I was making the other person do, or declaring that they couldn’t do.  Likewise, even if I were useless in a crisis, at least I’d <em>get out of the way</em> of the people trying to solve it.  How could I live with myself otherwise?</p>



<p>The fundamental mystery of the blankfaces, then, is how they can be so alien and yet so common.</p>



<hr class="wp-block-separator" />



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 3):</span></strong> Surprisingly many people seem to have read this post, and come away with the notion that a “blankface” is simply anyone who’s a stickler for rules and formalized procedures.  They’ve then tried to refute me with examples of where it’s <em>good</em> to be a stickler, or where I in particular would believe that it’s good.</p>



<p>But no, that’s not it at all.</p>



<p>Rules can be either good or bad.  All things considered, I’d probably rather be on a plane piloted by a robotic stickler for safety rules, than by someone who ignored the rules at his or her discretion.  And as I said in the post, in the first months of covid, it was ironically the <em>anti</em>-blankfaces who were screaming for rules, regulations, and lockdowns; the blankfaces wanted to continue as though nothing had changed!</p>



<p>Also, “blankface” (just like “homophobe” or “antisemite”) is a serious accusation.  I’d never call anyone a blankface merely for sticking with a defensible rule when it turned out, in hindsight, that the rule could’ve been relaxed.</p>



<p>Here’s how to tell a blankface: suppose you see someone enforcing or interpreting a rule in a way that strikes you as <em>obviously</em> absurd.  And suppose you point it out to them.</p>



<p>Do they say “I disagree, here’s why it actually <em>does</em> make sense”?  They might be mistaken but they’re not a blankface.</p>



<p>Do they say “tell me about it, it makes <em>zero</em> sense, but it’s above my pay grade to change”?  You might wish they were more dogged or courageous but again they’re not a blankface.</p>



<p>Or do they ignore all your arguments and just restate the original rule—seemingly angered by what they understood as a challenge to their authority, and delighted to reassert it?  <em>That’s</em> the blankface.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5675"><span class="datestr">at August 02, 2021 08:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3764738535229441946">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/do-four-colors-suffice.html">Do Four Colors Suffice?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(Guest Post by David Marcus)</p><p><i>Comment by Bill:</i> Haken and Appel proved that all planar maps are 4-colorable. Or did they? David Marcus emailed me that its not quite true and I asked him to post on it, so here it is. The meta point is that math can be very subtle.</p><p>And now <i>David Marcus's post:</i></p><p>Is the Four Color Map Theorem true?</p><p>It is commonly believed that the Four Color Map Theorem says that <i>four colors suffice to color a planar</i> <i>map</i>. While this is true for any map a non-mathematician would dream up, it is not true for maps a mathematician might dream up without some restriction on the regions that are allowed. This is shown in Hud Hudson's <a href="https://www.jstor.org/stable/pdf/3647828.pdf">Four Colors Do Not Suffice</a> which appeared in the American Math Monthly, Volume 110,  No. 5, May 2003, pages 417--423. </p><p>Hudson's article is written in a very entertaining style. I recommend that you read it. He constructs a map consisting of six regions R1,...,R6.  Each region is bounded and path connected. There is a line segment B that is  in the boundary of all six regions.  So, six colors are needed, since all six regions share a common boundary. The construction is similar to the topologist's sine curve. For each i , the union of Ri and B is not path connected. Hudson also shows that for any n, there is a map that requires at least n colors.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-oSGhAkWvF3M/YQf6m1FWr0I/AAAAAAAB9lQ/6E21cZ9aCnsEU-bh2PNr9P26C5_lcW1sQCLcBGAsYHQ/s737/6%2Bcolors.png"><img width="320" src="https://1.bp.blogspot.com/-oSGhAkWvF3M/YQf6m1FWr0I/AAAAAAAB9lQ/6E21cZ9aCnsEU-bh2PNr9P26C5_lcW1sQCLcBGAsYHQ/s320/6%2Bcolors.png" border="0" height="222" /></a></div><br /><p>Hudson thus disproves the following statement:</p><div><div>1)  Four colors are sufficient to color any map drawn in the plane or on a sphere so that no two regions with a common boundary line are colored with the same color.</div><div><br /></div><div>Appel and Haken actually proved the following:</div><div><br /></div><div>2) Four colors are sufficient to color any planar graph so that no two vertices connected by an edge are colored with the same color.</div></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/do-four-colors-suffice.html"><span class="datestr">at August 01, 2021 09:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21845">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/08/01/mathematical-news-to-cheer-you-up/">Mathematical news to cheer you up</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h3><img width="592" alt="Anna-Kiesenhofer-Olympics-2021-930x620" src="https://gilkalai.files.wordpress.com/2021/08/anna-kiesenhofer-olympics-2021-930x620-1.webp" class="alignnone  wp-image-21856" height="395" /></h3>
<h3>1.  <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">Anna Kiesenhofer, a PhD mathematician researching PDEs at Ecole Polytechnique Federale Lausanne (EPFL), won the gold medal in the women’s bicycle road race at the Olympics.</span></h3>
<p>Here are two trivia question: a) Which hero of a recent post over here is related to <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">Kiesenhofer’s mathematical side? b) Name another famous connection between EPFL-mathematics and sport. <br /></span></p>
<h3>2. János Nagy and Péter Pál Pach <a href="https://arxiv.org/abs/2107.03956">proved the Alon-Jaeger-Tarsi conjecture via group ring identities</a></h3>
<blockquote>
<p><em>The abstract says it all: In this paper we resolve the Alon-Jaeger-Tarsi conjecture for sufficiently large primes. Namely, we show that for any finite field <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="texatom" id="MathJax-Span-3"><span class="mrow" id="MathJax-Span-4"><span class="mi" id="MathJax-Span-5">F</span></span></span></span></span></span> of size <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-6"><span class="mrow" id="MathJax-Span-7"><span class="mn" id="MathJax-Span-8">61</span><span class="mo" id="MathJax-Span-9">&lt;</span><span class="texatom" id="MathJax-Span-10"><span class="mrow" id="MathJax-Span-11"><span class="mo" id="MathJax-Span-12">|</span></span></span><span class="texatom" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15">F</span></span></span><span class="texatom" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mo" id="MathJax-Span-18">|</span></span></span><span class="mo" id="MathJax-Span-19">≠</span><span class="mn" id="MathJax-Span-20">79</span></span></span></span> and any nonsingular matrix <span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23">M</span></span></span></span> over <span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-24"><span class="mrow" id="MathJax-Span-25"><span class="texatom" id="MathJax-Span-26"><span class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28">F</span></span></span></span></span></span> there exists a vector <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-29"><span class="mrow" id="MathJax-Span-30"><span class="mi" id="MathJax-Span-31">x</span></span></span></span> such that neither <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-32"><span class="mrow" id="MathJax-Span-33"><span class="mi" id="MathJax-Span-34">x</span></span></span></span> nor <span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-35"><span class="mrow" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37">A</span><span class="mi" id="MathJax-Span-38">x</span></span></span></span> has a 0 component.</em></p>
</blockquote>
<h3>3. Michael Simkin asymptotically solved <a href="https://arxiv.org/abs/2107.13460">the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens problem!</a> (We mentioned this classic problem and earlier progress by Zur Luria<a href="https://gilkalai.wordpress.com/2018/05/10/zur-luria-on-the-n-queens-problem/"> in this post</a>.)</h3>
<blockquote>
<p><em>Abstract: The <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens problem is to determine <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+Q%7D%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal Q}(n)" class="latex" />, the number of ways to place <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" /> mutually non-threatening queens on an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n \times n" class="latex" /> board. We show that there exists a constant α=1.942±3×10<sup>-3</sup> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+Q%7D%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal Q}(n)" class="latex" /><img src="https://s0.wp.com/latex.php?latex=%3D%28%281+%5Cpm+o%281%29%29n+e+%5E%7B-%5Calpha%7D%29%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="=((1 \pm o(1))n e ^{-\alpha})^n" class="latex" />. The constant α is characterized as the solution to a convex optimization problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+P%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal P}" class="latex" />([−1/2,1/2]<sup>2</sup>), the space of Borel probability measures on the square. The chief innovation is the introduction of limit objects for <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations, which we call “queenons”. These are a convex set in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+P&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathcal P" class="latex" />([−1/2,1/2]<sup>2</sup>). We define an entropy function that counts the number of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations that approximate a given queenon. The upper bound uses the entropy method. For the lower bound we describe a randomized algorithm that constructs a configuration near a prespecified queen on and whose entropy matches that found in the upper bound. The enumeration of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations is then obtained by maximizing the (concave) entropy function in the space of queenons. Along the way we prove a large deviations principle for n-queens configurations that can be used to study their typical structure.</em></p>
</blockquote>
<p><img width="1600" alt="sun" src="https://gilkalai.files.wordpress.com/2021/08/sun.jpeg" class="alignnone size-full wp-image-21875" height="1200" /></p>
<p>Intermission: the sun over Tel Aviv sea</p>
<h3 class="title mathjax">4. Boris Bukh demonstrated <a href="https://arxiv.org/abs/2107.04167">Extremal graphs without exponentially-small bicliques</a></h3>
<blockquote>
<p><em>Abstract: The Turán problem asks for the largest number of edges in an $latex <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">n$</span></span></span></span>-vertex graph not containing a fixed forbidden subgraph <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6">F</span></span></span></span>. We construct a new family of graphs not containing<span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-7"><span class="mrow" id="MathJax-Span-8"><span class="msubsup" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10"></span><span class="texatom" id="MathJax-Span-11"><span class="mrow" id="MathJax-Span-12"><span class="mi" id="MathJax-Span-13"></span><span class="mo" id="MathJax-Span-14"></span><span class="mi" id="MathJax-Span-15"> <img src="https://s0.wp.com/latex.php?latex=K_%7Bs%2Ct%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="K_{s,t}" class="latex" /></span></span></span></span></span></span></span>, for <span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18"></span><span class="mo" id="MathJax-Span-19"></span><span class="msubsup" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21"></span><span class="mi" id="MathJax-Span-22"><img src="https://s0.wp.com/latex.php?latex=t%3DC%5Es&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="t=C^s" class="latex" /></span></span></span></span></span>, with  <img src="https://s0.wp.com/latex.php?latex=%5COmega+%28n%5E%7B2-1%2Fs%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\Omega (n^{2-1/s})" class="latex" /> edges matching the upper bound of Kövári, Sós and Turán. </em></p>
</blockquote>
<h3 class="title mathjax">5. Michael Capalbo, <a href="https://link.springer.com/content/pdf/10.1007/s00493-020-3989-0.pdf">Explicit 𝑁-vertex graphs with maximum degree 𝐾 and diameter [1+𝑜(1)]log<sub>𝐾-1</sub> 𝑁 for each 𝐾-1 a prime power</a>,</h3>
<blockquote>
<p><em>Abstract:   Here we first present the solution of a long-standing open question–the explicit construction of an infinite family of N-vertex cubic graphs that have diameter [1+o(1)]log<sub>2</sub> N. We then extend the techniques to construct, for each K of the form 2<sup>s</sup>+1 or K=p<sup>s</sup>+1; s an integer and p a prime, an infinite family of K-regular graphs on N vertices with diameter [1+o(1)]log<sub>K−1</sub> N.</em></p>
</blockquote>
<p>I missed this breakthrough in STOC 2019 but now it appeared in Combinatorica, and Nati told me about it.</p>
<h3 class="title mathjax">6.  A beautiful survey article: <a href="https://arxiv.org/abs/2107.06371">Intersection Problems in Extremal Combinatorics: Theorems, Techniques and Questions Old and New</a>, by David Ellis,</h3>
<blockquote>
<p><em>Abstract: The study of intersection problems in Extremal Combinatorics dates back perhaps to 1938, when Paul Erdős, Chao Ko and Richard Rado proved the (first) `Erdős-Ko-Rado theorem’ on the maximum possible size of an intersecting family of <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">k</span></span></span></span>-element subsets of a finite set. Since then, a plethora of results of a similar flavour have been proved, for a range of different mathematical structures, using a wide variety of different methods. Structures studied in this context have included families of vector subspaces, families of graphs, subsets of finite groups with given group actions, and of course uniform hypergraphs with stronger or weaker intersection conditions imposed. The methods used have included purely combinatorial ones such as shifting/compressions, algebraic methods (including linear-algebraic, Fourier analytic and representation-theoretic), and more recently, analytic, probabilistic and regularity-type methods. As well as being natural problems in their own right, intersection problems have connections with many other parts of Combinatorics and with Theoretical Computer Science (and indeed with many other parts of Mathematics), both through the results themselves, and the methods used. In this survey paper, we discuss both old and new results (and both old and new methods), in the field of intersection problems. Many interesting open problems remain; we will discuss several. For expositional and pedagogical purposes, we also take this opportunity to give slightly streamlined versions of proofs (due to others) of several classical results in the area. This survey is intended to be useful to PhD students, as well as to more established researchers. It is a personal perspective on the field, and is not intended to be exhaustive; we apologise for any omissions. It is an expanded version of a paper that will appear in the Proceedings of the 29th British Combinatorial Conference.</em></p>
</blockquote>
<h3 class="title mathjax">7.  <a href="http://ecajournal.haifa.ac.il/Volume2022/ECA2022_S3I7.pdf">An interview with me</a>; interviewer Toufik Mansour.</h3>
<p> </p>
<div class="authors"> </div>
<div> </div>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/08/01/mathematical-news-to-cheer-you-up/"><span class="datestr">at August 01, 2021 07:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/114">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/114">TR21-114 |  The Space Complexity of Sum Labelling | 

	Henning Fernau, 

	Kshitij Gajjar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A graph is called a sum graph if its vertices can be labelled by distinct positive integers such that there is an edge between two vertices if and only if the sum of their labels is the label of another vertex of the graph. Most papers on sum graphs consider combinatorial questions like the minimum number of isolated vertices that need to be added to a given graph to make it a sum graph. In this paper, we initiate the study of sum graphs from the viewpoint of computational complexity. Notice that every $n$-vertex sum graph can be represented by a sorted list of $n$ positive integers where edge queries can be answered in $O(\log n)$ time. Therefore, limiting the size of the vertex labels upper-bounds the space complexity of storing the graph in the database.

We show that every $n$-vertex, $m$-edge, $d$-degenerate graph can be made a sum graph by adding at most $m$ isolated vertices to it, such that the size of each vertex label is at most $O(n^2d)$. This enables us to store the graph using $O(m\log n)$ bits of memory. For sparse graphs (graphs with $O(n)$ edges), this matches the trivial lower bound of $\Omega(n\log n)$. As planar graphs and forests have constant degeneracy, our result implies an upper bound of $O(n^2)$ on their label size. The previously best known upper bound on the label size of general graphs with the minimum number of isolated vertices was $O(4^n)$, due to Kratochvil, Miller &amp; Nguyen (2001). Furthermore, their proof was existential, whereas our labelling can be constructed in polynomial time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/114"><span class="datestr">at August 01, 2021 07:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/113">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/113">TR21-113 |  Tight Bounds for the Randomized and Quantum Communication Complexities of Equality with Small Error | 

	Nikhil Mande, 

	Ronald de Wolf</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We investigate the randomized and quantum communication complexities of the well-studied Equality function with small error probability $\epsilon$, getting the optimal constant factors in the leading terms in a number of different models.

The following are our results in the randomized model:

1) We give a general technique to convert public-coin protocols to private-coin protocols by incurring a small multiplicative error at a small additive cost. This is an improvement over Newman's theorem [Inf. Proc. Let.'91] in the dependence on the error parameter.

2) As a consequence we obtain a $(\log(n/\epsilon^2) + 4)$-cost private-coin communication protocol that computes the $n$-bit Equality function, to error $\epsilon$. This improves upon the $\log(n/\epsilon^3) + O(1)$ upper bound implied by Newman's theorem, and matches the best known lower bound, which follows from Alon [Comb. Prob. Comput.'09], up to an additive $\log\log(1/\epsilon) + O(1)$.

The following are our results in the quantum model:

1) We exhibit a one-way protocol with $\log(n/\epsilon) + 4$ qubits of communication, that uses only pure states and computes the $n$-bit Equality function to error $\epsilon$. This bound was implicitly already shown by Nayak [PhD thesis'99]. 

2) We give a near-matching lower bound, showing that any $\epsilon$-error one-way protocol for $n$-bit Equality that uses only pure states communicates at least $\log(n/\epsilon) - \log\log(1/\epsilon) - O(1)$ qubits.

3) We exhibit a one-way protocol with $\log(\sqrt{n}/\epsilon) + 3$ qubits of communication, that uses mixed states and computes the $n$-bit Equality function to error $\epsilon$. This is also tight up to an additive $\log\log(1/\epsilon) + O(1)$, which follows from Alon's result.

Our upper bounds also yield upper bounds on the approximate rank, approximate nonnegative-rank, and approximate psd-rank of the Identity matrix. As a consequence we also obtain improved upper bounds on these measures for the distributed SINK function, which was recently used to refute the randomized and quantum versions of the log-rank conjecture (Chattopadhyay, Mande and Sherif [J. ACM'20], Sinha and de Wolf [FOCS'19], Anshu, Boddu and Touchette [FOCS'19]).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/113"><span class="datestr">at August 01, 2021 06:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>Sariel Har-Peled demonstrates how to make animated pdfs in beamer talk slides and present them on Linux using okular (<a href="https://mastodon.social/@sarielhp/106585915134108096">\(\mathbb{M}\)</a>): <a href="https://www.youtube.com/watch?v=-fBREHIdTLI">YouTube demo</a>; <a href="http://sarielhp.org/misc/blog/21/07/15/bemaer_example.zip">beamer source code</a>. On OS X, the same technique works when presenting using Acrobat, but not Preview.</p>
  </li>
  <li>
    <p><a href="https://www.laphamsquarterly.org/roundtable/evidence-elements">Evidence of the Elements: Finding Euclid on scattered pot shards</a> (<a href="https://mathstodon.xyz/@11011110/106602699662553907">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/192071/Euclidean-Cover-Bands-of-the-Ancient-World">via</a>). Excerpt from Benjamin Wardhaugh’s new book <em>Encounters with Euclid: How an Ancient Greek Geometry Text Shaped the World</em>.</p>
  </li>
  <li>
    <p><a href="https://blogs.ams.org/beyondreviews/2021/07/18/yoshimura-crush-patterns/">Yoshimura Crush Patterns</a> (<a href="https://mathstodon.xyz/@11011110/106604389376060790">\(\mathbb{M}\)</a>). See also <a href="https://en.wikipedia.org/wiki/Yoshimura_buckling">Yoshimura buckling on Wikipedia</a>, which repeats Robert Lang’s observation that these patterns can be seen on Mona Lisa’s sleeves.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/MonaLisa.jpg" style="border-style: solid; border-color: black;" alt="Mona Lisa" /></p>
  </li>
  <li>
    <p>Two more new Wikipedia Good Articles (<a href="https://mathstodon.xyz/@11011110/106614589027257857">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Cairo_pentagonal_tiling">Cairo pentagonal tiling</a>, a tiling of the plane by congruent but irregular pentagons, formed by overlaying two hexagonal tilings. It appears in street pavings, crystal structures, and the art of M. C. Escher.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graphs</a>, the planar graphs formed from trees by connecting their leaves into a cycle. Studied by Kirkman long before Halin and significant in graph algorithms because of their low treewidth.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=O4UpNSlzKAM">Building a trivalent graph of harmonic relations among major and minor triads</a> (<a href="https://mathstodon.xyz/@jsiehler/106619540053169137">\(\mathbb{M}\)</a>). The graph has a vertex for each major or minor chord and an edge when you can get from one chord to another by changing a single note. It has 24 vertices (12 bottom notes of chords in 12-TET tuning, and two chords per note). It is vertex-transitive: shifting major or minor chords up or down the scale doesn’t change their adjacency patterns, and reversing the scale swaps major for minor. But it is not the <a href="https://en.wikipedia.org/wiki/Nauru_graph">Nauru graph</a> because it is not edge-transitive: you can walk up or down the scale by removing the bottom note from a chord and adding a new top note, and this walk forms a 24-cycle, but these edges are different from the ones where you change a major chord into a minor or vice versa by changing the middle note. The resulting graph is the one with <a href="https://en.wikipedia.org/wiki/LCF_notation">LCF notation</a> \([7,-7]^{12}\), shown below.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/triads.svg" alt="Graph of major and minor chords, with edges for one-note changes" /></p>
  </li>
  <li>
    <p>My 5-year-old Macbook Pro decided to start shutting down the screen if you open it too wide (<a href="https://mathstodon.xyz/@11011110/106626560960499738">\(\mathbb{M}\)</a>), so I had to replace it. New hardware is nice but I had been holding off on several versions of OS updates and the disruption in my software setup was a bit of a pain. The biggest issue: losing all my old paid-for non-subscription copies of Adobe software, because they won’t run on the new OS. Fortunately my campus has a subscription, for now.</p>
  </li>
  <li>
    <p>If you modify the sieve of Eratosthenes so that each generated number \(p\) knocks out the numbers \(pn+2\) instead of the usual \(pn\), you get <a href="https://oeis.org/A076974">the prime-like sequence 2, 3, 7, 13, 19, 25, 31, 39, 43, 49, 55, 61, 69, …</a> (<a href="https://mathstodon.xyz/@11011110/106632015633732162">\(\mathbb{M}\)</a>). Although many non-primes are in this sequence, and many primes are not, Bill McEachen has observed that with one exception the larger prime in every twin prime pair is part of this sequence! The proof is not difficult; see the OEIS link for spoilers.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/journals/notices/202107/rnoti-p1106.pdf">Henry Segerman on rolling acrobatic apparatus in the <em>Notices</em></a> (<a href="https://mathstodon.xyz/@henryseg/106631292717611856">\(\mathbb{M}\)</a>). Unfortunately he missed MOMIX dancer Alan Boeding’s work in this area from the late 1970s and early 1980s.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2021/07/20/hamming-cube-of-primes/">Hamming cube of primes</a> (<a href="https://mathstodon.xyz/@11011110/106644162088249713">\(\mathbb{M}\)</a>). Make an infinite graph whose vertices are the binary representations of prime numbers and whose edges represent flipping a single bit of this representation. (For instance, 2 and 3 are neighbors.) Surprisingly, it is not connected! 2131099 has no neighbors. See also <a href="https://mathoverflow.net/q/363083/440">the same question on MathOverflow, a year ago</a>, and a <a href="https://www.youtube.com/watch?v=p3Khnx0lUDE">new Matt Parker video on the same concept in decimal</a>.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2021/07/20/a-person-in-a-dream-co-authored-a-math-paper.html">A dead mathematician co-authored a paper after appearing in a dream</a> (<a href="https://mathstodon.xyz/@11011110/106651389132857850">\(\mathbb{M}\)</a>). The paper is “Higher algebraic <span style="white-space: nowrap;">\(K\)-theory</span> of schemes and of derived categories” by Robert Wayne Thomason and Thomas Trobaugh (2007), <a href="https://doi.org/10.1007/978-0-8176-4576-2_10">doi:10.1007/978-0-8176-4576-2_10</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=1106918">MR1106918</a>. It appears to have been quite an influential one; the MR review calls it a landmark, and it has over 1000 citations on Google Scholar.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1073/pnas.2103605118">Imperfect comb construction reveals the architectural abilities of honeybees</a> (<a href="https://mathstodon.xyz/@11011110/106657012385182296">\(\mathbb{M}\)</a>, <a href="https://arstechnica.com/science/2021/07/mergers-twists-and-pentagons-the-architecture-of-honeycombs/">via</a>). How do bees cope with making hexagonal honeycombs when some kinds of cells have different sizes and some patches of honeycomb don’t align when they come close to first meeting up? Answer appears to be: they see the problems coming and accommodate them gradually by intermediate variations in size and degree of cells.</p>
  </li>
  <li>
    <p><a href="https://publicdomainreview.org/collection/solid-objects">Solid Objects: 16th-Century Geometric and Perspective Drawings from the Herzog August Bibliothek in Wolfenbüttel</a> (<a href="https://mathstodon.xyz/@11011110/106662683881820595">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27896003">via</a>).</p>
  </li>
  <li>
    <p>You remember that simple fruit pictogram equation with the ridiculously complicated answer (<a href="https://mathstodon.xyz/@11011110/106668276488591830">\(\mathbb{M}\)</a>)? David Roberts has <a href="https://thehighergeometer.wordpress.com/2021/07/27/diophantine-fruit/">another one in the same style for which we don’t even know the answer</a>.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5661">Scott Aaronson takes a break from quantum supremacy to tell us about busy beavers</a> (<a href="https://mathstodon.xyz/@11011110/106674016557738368">\(\mathbb{M}\)</a>). These are Turing Machines that take as long as possible to do stuff. “As long as possible” is an explosively-quickly growing function of the number of states, but the gist of the post is that the “do stuff” part can be defined in various ways, some of which make the explosion happen earlier than others.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir posts a nice photo of <a href="https://mathstodon.xyz/@antoinechambertloir/106657956305987782">multicolored foam on the surface of a cup of coffee</a>.</p>
  </li>
  <li>
    <p><a href="https://annals.math.princeton.edu/2021/193-3/p03">A new paper by Asperó and Schindler</a> (<a href="https://mathstodon.xyz/@11011110/106678194765282364">\(\mathbb{M}\)</a>) argues that principles of maximal forcing, unified in their paper, provide natural models for set theory in which many natural questions that are independent of ZF have clear answers. For instance, in these models, there are \(\aleph_2\) real numbers, not \(\aleph_1\). I got to this via <a href="https://www.quantamagazine.org/how-many-numbers-exist-infinity-proof-moves-math-closer-to-an-answer-20210715/">a popularized treatment in <em>Quanta</em></a>, but I think the introduction of the paper is quite readable. (The rest is not.)</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/31/linkage.html"><span class="datestr">at July 31, 2021 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18993">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/">Pandemic Lag</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>In chess ratings and what other measures of cognitive development?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/didonbook/" rel="attachment wp-att-18995"><img width="160" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/DidonBook.jpg?resize=160%2C222&amp;ssl=1" class="alignright wp-image-18995" height="222" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.amazon.com/Influence-morale-sports-athl%C3%A9tiques-French-ebook/dp/B00YOIOKQY">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Henri Didon was a French priest and promoter of youth sports in the late 1800s. He coined the phrase <em>Citius, Altius, Fortius</em>, meaning <em>faster-higher-stronger</em>, which became the motto of the Olympic Games between their reinception in 1896 and its proclamation when the Games were held in Paris in 1924. For the 2020 Games being held now in 2021 they have added the word <em>Communiter</em>, meaning <em>together</em>, which is said to express solidarity during the pandemic. </p>
<p>
Today we review how the official measure of being faster, higher, and stronger at chess has been impacted by the pandemic.<br />
<span id="more-18993"></span></p>
<p>
Didon spoke of his words as “the foundation and <em>raison d’être</em> of athletics” amid the progress of humanity. They have been borne out by the steady progression of athletic records over the Games’ 125-year history. Whether the Tokyo Games will continue that trend is open. Besides the year delay and the pandemic’s impact on qualifying competitions and athletic conditioning in general, there has emerged a question of mental effects amid the lack of spectators and straitened atmosphere. The one example I’ll quote is the <a href="https://www.huffpost.com/entry/kristof-milak-tokyo-olympics_n_61013f01e4b00fa7af7db9fb">claim</a> by the Hungarian swimmer Kristof Milak that a pre-race mishap with his favorite swimming trunks cost him a record in an event he still won:</p>
<blockquote><p><b> </b> <em> “They split 10 minutes before I entered the pool and in that moment I knew the world record was gone. I lost my focus and knew I couldn’t do it.” </em>
</p></blockquote>
<p>
At least the means of measuring athletic performances have not been disrupted. For <b>psychometrics</b>—a word <a href="https://www.morgan.edu/psychology/psychometrics">meaning</a> <em>the science of measuring mental capacities and processes</em>—the standardized tests most often used to measure aptitude have themselves been curtailed. This makes all the more open the question of how our youth have progressed during the pandemic in education on the whole. We will examine the special case of chess, where the official instrument has been almost entirely frozen for 15 months, but my own work carries both the ability and the responsibility to make up the difference.</p>
<p>
</p><p></p><h2> Chess Ratings and Lag </h2><p></p>
<p></p><p>
The <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> rating system is simple but accurate enough for use by sporting federations besides chess. In chess, 1000 is a typical rating for a novice player, 1600 means a good club player, 2200 is the threshold for “master,” and 2800 is world championship standard. A player’s rating measures skill in a way that the <em>difference</em> to the opponent’s rating yields probabilities by which to predict the outcomes of games between them. Elo is the main prediction engine of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for <a href="https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/">basketball</a>, <a href="https://projects.fivethirtyeight.com/complete-history-of-mlb/">baseball</a>, and <a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">football</a> (but not <a href="https://fivethirtyeight.com/methodology/how-our-club-soccer-predictions-work/">soccer</a>). </p>
<p>
Although the prediction formula uses only differences, so that an additive shift in all ratings would not affect the chances, I have shown that the ratings administered by the International Chess Federation (FIDE) have stayed stable in absolute regard to the objective quality of moves played as measured by my own predictive model, via my Intrinsic Performance Ratings (IPRs) geared to the FIDE rating scale. Having stable numbers is vital not only to my cheating tests but to the public understanding of the system on the whole.  This goes for FIDE, for Internet gaming federations, and even for the use of Elo by <a href="https://web.archive.org/web/20170819190821/https://killscreen.com/articles/tinder-matchmaking-is-more-like-warcraft-than-you-might-think/">Tinder</a>. </p>
<p>
Thus it is all the more sad for me to see things like this happen not only to FIDE’s Elo ratings but also those of the US Chess Federation (USCF), who adopted Arpad Elo’s formulas in the 1950s:</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinggraphann2/" rel="attachment wp-att-19013"><img width="527" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingGraphann2.jpg?resize=527%2C256&amp;ssl=1" class="aligncenter wp-image-19013" height="256" /></a></p>
<p></p><p><br />
This is the FIDE Rating Progress <a href="https://ratings.fide.com/profile/2053900/chart">Chart</a> of Annie Wang, who just won the US Junior Women’s Championship played in-person at the <a href="https://saintlouischessclub.org/">Saint Louis Chess Club</a> last week. Her FIDE rating has been stuck at <b>2384</b> ever since the April 2020 rating list. One glance at the chart suffices to project her rating into the neighborhood of 2500 by now. Her USCF rating is closer at 2457, but this is offset by a long-known inflation of USCF ratings relative to FIDE, <a href="https://chessgoals.com/rating-comparison-old/">measured</a> about 75 points at that level in May 2020. Wang’s USCF rating has been similarly frozen. You can find the same for a plethora of young players down to aspiring kids of single-digit age blasting out of three-digit ratings, as Wang did.  They have a flat line like the ones circled in blue, but located where she had a sharp rise (circled in green):</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinguscfann/" rel="attachment wp-att-18998"><img width="460" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingUSCFann.jpg?resize=460%2C280&amp;ssl=1" class="aligncenter wp-image-18998" height="280" /></a></p>
<p></p><h2> The Need to Adjust </h2><p></p>
<p></p><p>
The lag mattered immediately for me as I gave daily statistical reports to the tournament’s chief arbiter last week. Using Wang’s official rating would have underestimated her true strength and biased my reports in the direction of false positives. Instead, having developed a formula that I won’t claim is anything more than <a href="https://en.wikipedia.org/wiki/Fermi_problem">Fermi</a>–<a href="https://brilliant.org/wiki/fermi-estimate/">estimated</a>, I calculated her effective FIDE rating as <b>2482</b>, adding almost 100 points. I would have upped her USCF rating to 2543 by the same formula. </p>
<p>
Wang was both the highest rated among the ten competitors and the oldest, with a long enough record of international play to have her FIDE <a href="https://en.wikipedia.org/wiki/Elo_rating_system#Most_accurate_K-factor">K-factor</a> reduced from 40 to 20. My formula adds more points for lower ratings, higher K-factor, and younger age—all reflecting the arc of many improving junior players. My average increase to the women’s ratings was <b>199.1</b> points, versus <b>57.4</b> to the ten players in the junior men’s/mixed championship, who had mostly higher ratings to begin with. </p>
<p>
Also playing in St. Louis were ten in the US Senior Championship, including last year’s winner Joel Benjamin, whom I knew and played in the 1970s when we were kids. Their ratings have been likewise frozen. Rating points in chess are zero-sum, so the triple-digit gains I have credited to the young would in normal reality have been taken out of other players—most plausibly, us geezers. There are more of us than keen juniors, so the presumed individual losses would be less. </p>
<p>
Did that prove out? My IPRs furnish a way to verify. They differ from other deployed quality metrics by organically involving the difficulty of the positions a player faces, in several ways besides the complexity and temptation factors I <a href="https://rjlipton.wpcomstaging.com/2019/08/15/predicting-chess-and-horses/">incorporated</a> two years ago. Here are the results—but bear in mind that these three 10-player tournaments are small data: their two-sigma error bars on the average IPRs are about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+80%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pm 80}" class="latex" /> Elo points.</p>
<ul>
<li>
<b>US Jr. W</b>: Avg. rating 2101, adjusted <b>2300</b>, avg. IPR <b>2337</b> (+37). <p></p>
</li><li>
<b>US Jr. M</b>: Avg. rating 2492, adjusted <b>2550</b>, avg. IPR <b>2527</b> (-23). <p></p>
</li><li>
<b>US Sr. M</b>: Avg. rating <b>2494</b> (no adjustment), avg. IPR <b>2459</b> (-35).
</li></ul>
<p>
The truly significant result is that the women performed much closer to my adjustment than to their official ratings. The men were only slightly closer amid general insignificance, which applies also to the seniors. The juniors combined were highly close to my projections. </p>
<p>
Right now I am gathering data from larger Open tournaments in this first month of widespread in-person play. There have been some hits and misses, and I have not yet evaluated all (un-)controllable factors. But gathering the original large data for my adjustment formula required coping with a major factor: the 100–200x higher evident cheating rate I’ve observed in online chess.</p>
<p>
</p><p></p><h2> How To Be Not Very Wrong </h2><p></p>
<p></p><p>
I first perceived the phenomenon when monitoring the European Youth Online Rapid Chess Championship last September. I compiled full analysis on all 689 competitors in women’s and men’s/mixed sections ranging from Under-12 to Under-18. Besides four particular cases, my results said that probably at least four of another five were cheating, but without the confidence needed to flag any one. Removing the high outliers did not, however, equate either the IPRs or my sharper test of conformance to the bell curve to my projections. The Under-12 M and W and Under-14 M sections had IPRs averaging 83, 235, and 125 higher, respectively. The Under-14 W and U16 and U18 sections were close to my projections, so I did not suspect general modeling issues. </p>
<p>
The online World Youth Rapid Championships in November-December, which added an under-10 division, brought the lag phenomenon out in force, on all continents. The correction I postulated even before that tournament finished was:</p>
<blockquote><p><b> </b> <em> 15 Elo <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" /> (months since April 2020), higher for those under 13 (50% to 2x higher). </em>
</p></blockquote>
<p></p><p>
There are several reasons I have not tried to be more precise. There is uncertainty about how many high outliers to remove, about faster time controls, and about <a href="https://en.chessbase.com/post/why-do-some-countries-always-gain-and-other-always-lose-rating-points">geographical</a> drifts in ratings. The effect depends on how much a junior player is disposed to improve in the first place; I found it absent in the lower divisions of the UK’s junior leagues played online last winter. In an individual cheating case I take a more-particular fix on the appropriate rating. What the equation is for is <em>to show the fairness of my baseline relative to the field on the whole</em>. There are also non-cheating purposes, which should come to the fore as FIDE and other federations emerge from the pandemic, and which I discuss next. </p>
<p>
I have been using essentially this formula ever since. From large scholastic tournaments across the globe this spring, I settled on fixing the adjustment for those with birth year 2008 or later as 25 Elo <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" /> (months since April 2020). For players with official rating <img src="https://s0.wp.com/latex.php?latex=%7BR+%3E+2000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R &gt; 2000}" class="latex" /> I apply the rough multiplier <img src="https://s0.wp.com/latex.php?latex=%7B%283000+-+R%29%2F1000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(3000 - R)/1000}" class="latex" />, and for those with <img src="https://s0.wp.com/latex.php?latex=%7BK+%3C+40%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K &lt; 40}" class="latex" /> I (also) multiply by <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7BK%2F40%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{K/40}}" class="latex" />.</p>
<p>
I won’t claim the ’15’ and ’25’ are right, compared to multipliers that are 1 or 2 higher or lower. But the results I have been getting all year say that my 15 and 25 are most often closer than factors of 10 or 20 or 30 would be. In almost all cases, like for the US Jr. W above, my pre-set rating calibration has come an order of magnitude closer to the IPR verification than the adjustments themselves. Taking a cue from the title of Jordan Ellenberg’s <a href="https://en.wikipedia.org/wiki/How_Not_to_Be_Wrong">predecessor</a> to his book I <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> last month, my main concern is to be not very wrong.</p>
<p>
</p><p></p><h2> A Dilemma Moving Forward </h2><p></p>
<p></p><p>
Providing an accurate and stable rating system has long been recognized as a prime service of FIDE. A legal dimension has been added insofar as evaluating cheating allegations requires a prior assessment of the natural skill of the accused player. The pandemic has made me take over much of the latter responsibility, but the former presents a wider dilemma doubtless faced in some form by other impacted sporting federations and educational assessment agencies on the whole:</p>
<blockquote><p><b> </b> <em> Is it a higher responsibility to provide the most accurate assessment of current ability obtainable now, or to maintain continuity of the official assessment mechanism? </em>
</p></blockquote>
<p></p><p>
I could go even wider to analogize this to the US Census debate over whether estimations, presuming demonstration of their greater accuracy, should be used in preference to the conducted count. The latter is enshrined in the US Constitution, while the principle that chess rating points should be won or lost only in actual combat is similarly <a href="https://handbook.fide.com/chapter/B022017">hallowed</a>. But I have certainly “demonstrated the obvious”: that the current official ratings of almost all the keenest young players are very wrong.</p>
<p>
Mathematically, the rating system <em>will</em> re-establish equilibrium if the current discrepancy is left alone. The trouble is that the mathematical nature of the update and the relative paucity of chess games also guarantees that the process will be <em>slow</em>, measured in years. FiveThirtyEight has remarked in <a href="https://fivethirtyeight.com/features/60-games-arent-enough-to-crown-the-best-mlb-team-but-neither-are-162-games/">several</a> <a href="https://fivethirtyeight.com/features/no-mlb-team-is-great-and-fewer-are-awful-is-this-the-parity-we-wanted/">recent</a> <a href="https://fivethirtyeight.com/features/bad-teams-may-be-posing-as-good-teams-in-a-60-game-baseball-season/">article</a> about the long update times in baseball as measured by Elo ratings. My cheating tests often cannot wait a day.  I have to use my cross-check and validation features to detect and remove a huge amount of mathematically the same kind of bias believed to afflict other currently-deployed predictive models less transparently.</p>
<p>
There is precedent for a large-scale adjustment of ratings by FIDE. Women’s chess used to be even more segregated from men than today. In 1986, Arpad Elo himself—as secretary of FIDE’s Qualifications Commission—<a href="http://www.anusha.com/elo.htm">reported</a> that women’s ratings had drifted down by about “one half of a class interval.” FIDE added 100 points to the rating of every active female player except Susan Polgar, whose rating was already ‘well-mixed’ according to the report, since she had faced many more male players than the others.</p>
<p>
Attempting to resolve that historical controversy by computing IPRs for Polgar and the other players in Elo’s study has never reached my front burner. But the point remains that my work is uniquely capable of informing the state of ratings in a radical manner. The pandemic has created both a need and an opportunity for a reset that could also solve other issues previously noted—while ensuring that ratings on all continents are on a common scale.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How pronounced is the lag of assessment in education and other competitive arenas, both physical and in mind-sports?</p>
<p>
I had not noticed that Tyler Cowen had already used the term “psychometric test” in a <a href="https://marginalrevolution.com/marginalrevolution/2020/03/the-world-is-running-a-disturbing-psychometric-test.html">post</a> on the <em>Marginal Revolution</em> blog at the beginning of the pandemic, until he <a href="https://marginalrevolution.com/marginalrevolution/2021/07/the-great-psychometric-test-continues.html">repeated</a> it just today.</p>
<p>
I have hinted at some other issues in chess but stopped short of addressing them. One is whether online play—where play at 5-minute “Blitz” down to 1-minute “Bullet” time controls predominates even over “Rapid” beginning at 10 minutes—has a similar effect on development in the absence of any in-person “Classical” chess. Another is whether the observed increase in the ranks of players with 2700+ elite ratings is really <em>Fortius</em> or merely rating <em>inflation</em>. A third is whether the current conditions for in-person chess will last long enough to get a good fix on the ‘post-pandemic’ state of skill, and a fourth—coming back to what I quoted about the current Olympics—is whether they are truly “normal” enough even now.</p>
<p></p><p><br />
[changed first figure to show the March 2020 pandemic start accurately; some minor word changes.]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/"><span class="datestr">at July 30, 2021 08:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">Average-Case Fine-Grained Hardness, Part II</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, we did two examples of proving average-case fine-grained hardness via worst-case to average-case reductions. In this post, I want to continue the discussion of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques (in particular, on <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi graphs</a>) to showcase a new technique, which builds on the general recipe and the example of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques described in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>. In the <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">next post</a>, I will discuss how this new technique can be applied to some other combinatorial problems.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in Erdős–Rényi graph.</strong> A strong follow-up <a href="https://arxiv.org/abs/1903.08247">[BBB19]</a> of the result <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> we discussed in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> shows that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertex graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B%5Clog%5E%7BO%281%29%7D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{\log^{O(1)} n}" class="latex" /> in Erdős–Rényi graph (whereas the sampable distribution of the random graph in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> is somewhat unnatural). The key idea is a decomposition lemma which says for sufficiently large prime <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog%28p%29%5Clog%28p%2F%5Cvarepsilon%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log(p)\log(p/\varepsilon))" class="latex" />, for any constants <img src="https://s0.wp.com/latex.php?latex=0%3Cp%5E%7B%281%29%7D%2C%5Cdots%2Cp%5E%7B%28k%29%7D%3C1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0&lt;p^{(1)},\dots,p^{(k)}&lt;1" class="latex" /> (for our application, these constants will be equal), given independent Bernoulli random variables <img src="https://s0.wp.com/latex.php?latex=y_%7B%5Cell%7D%5Csim%5Ctextrm%7BBern%7D%28p%5E%7B%28%5Cell%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{\ell}\sim\textrm{Bern}(p^{(\ell)})" class="latex" />, the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B%5Cell%7D%5Cmod+p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sum_{\ell=0}^k 2^{\ell}y_{\ell}\mod p" class="latex" /> is close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}" class="latex" />, i.e., the statistical distance is less than <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> (later when we apply this lemma, we want <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" />, and therefore <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog+p%5Ccdot%28%5Clog+p%2B%5Clog+n%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log p\cdot(\log p+\log n))" class="latex" /> ). We skip the proof of this lemma which is a nice application of basic Fourier analysis.</p>



<p>As in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> denotes a constructed polynomial that computes the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques when the input is an adjacency matrix of a graph. The step 3 of the general recipe from the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> reduces counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for the worst-case graph to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> many uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> (recall in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> is the degree of the polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> after the Chinese remaindering trick). Based on the decomposition lemma, using standard sampling scheme (this is essentially <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>, which I will not go into the details, but I just want to mention that we would like the <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> in the decomposition lemma to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" /> such that the sampling scheme succeeds w.h.p. by a few attempts), we can further reduce evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" /> is a random 0-1 valued matrix that is statistically close to the adjacency matrix of an Erdős–Rényi graph. Now, if we can “pull out” the weighted sum in <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, then we are done, because evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on the adjacency matrix of an Erdős–Rényi graph is precisely counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for Erdős–Rényi graph.</p>



<p>When can we “pull out” the weighted sum for a polynomial <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />? One answer is when the polynomial is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite.</p>



<p>An <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m" class="latex" />-variate polynomial <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite if there is a partition of the set of variables <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7B%5Cbigcup%7D_%7Bj%5Cin%5Bd%5D%7D+S_j%3D%5Bm%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\dot{\bigcup}_{j\in[d]} S_j=[m]" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is the sum of monomials in which each monomial contains exactly one variable from each part <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_j" class="latex" /> (more formally, <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29%3D%5Csum_%7B%28i_1%2Ci_2%2C%5Cdots%2Ci_d%29%5Cin+S%7D%5Cprod_%7Bj%5Cin%5Bd%5D%7Dx_%7Bi_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)=\sum_{(i_1,i_2,\dots,i_d)\in S}\prod_{j\in[d]}x_{i_j}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+S_1%5Ctimes+S_2%5Ctimes%5Cdots%5Ctimes+S_d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S\subseteq S_1\times S_2\times\dots\times S_d" class="latex" />).</p>



<p>For <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, it is not hard to show that<br /><img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B1%2C%5Cell%7D%2C%5Cdots%2C%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7Bm%2C%5Cell%7D%29%3D%5Csum_%7B%5Cell_1%3D0%7D%5Ek%5Csum_%7B%5Cell_2%3D0%7D%5Ek%5Cdots%5Csum_%7B%5Cell_d%3D0%7D%5Ek+2%5E%7B%5Cell_1%2B%5Cdots%2B%5Cell_d%7D%5Ccdot+f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell}y_{1,\ell},\dots,\sum_{\ell=0}^k 2^{\ell}y_{m,\ell})=\sum_{\ell_1=0}^k\sum_{\ell_2=0}^k\dots\sum_{\ell_d=0}^k 2^{\ell_1+\dots+\ell_d}\cdot f(y_{1,\ell_1},\dots,y_{m,\ell_m})." class="latex" /><br />(Essentially, because two variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> never appear in the same monomial, we can enumerate variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> in the same order.)</p>



<p>Let us think of each <img src="https://s0.wp.com/latex.php?latex=y_%7Bi%2C%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{i,\ell}" class="latex" /> as a coordinate of Erdős–Rényi adjacency matrix <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(y_{1,\ell_1},\dots,y_{m,\ell_m})" class="latex" /> is evaluating <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> on an ensemble of distinct coordinates of <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />‘s, and such ensemble is obviously Erdős–Rényi as well. Therefore, we have managed to decompose <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> into sum of <img src="https://s0.wp.com/latex.php?latex=k%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^d" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s where each <img src="https://s0.wp.com/latex.php?latex=Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y^{(\ell_1,\dots,\ell_d)}" class="latex" /> denotes an Erdős–Rényi adjacency matrix. In the next paragraph, we will construct a <img src="https://s0.wp.com/latex.php?latex=d%3D%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, and therefore, we have reduced computing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> to computing <img src="https://s0.wp.com/latex.php?latex=k%5E%7B%5Cbinom%7Bt%7D%7B2%7D%7D%3D%5Clog%5E%7BO%281%29%7D+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^{\binom{t}{2}}=\log^{O(1)} n" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D+%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}} (Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s, which is a mild blow-up of the number of the random instances which the reduction needs to solve. (In general, we consider the reduction to be efficient when the number of random instances it needs to solve is <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />, and hence, as long as <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" />, we are good to go.)</p>



<p>Unfortunately, the <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> given in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> does not work. Instead, we first reduce counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <a href="https://en.wikipedia.org/wiki/Multipartite_graph"><img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph</a>, and then we construct a <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph. Reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph is standard. Simply consider the <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor product</a> between the original graph and another <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. The number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in the tensor product graph is exactly <img src="https://s0.wp.com/latex.php?latex=t%21&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t!" class="latex" /> times that in the original graph. Now given the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-paritite graph, let <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> denote the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th part of vertices, and let <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> (for <img src="https://s0.wp.com/latex.php?latex=i%3Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i&lt;j" class="latex" />) denote the adjacency matrix between <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_j" class="latex" />. Consider the new polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%5E%7B%281%2C2%29%7D%2CX%5E%7B%281%2C3%29%7D%2C%5Cdots%2CX%5E%7B%28t-1%2Ct%29%7D%29%3A%3D%5Csum_%7Bv_1%5Cin+V_1%7D%5Csum_%7Bv_2%5Cin+V_2%7D%5Cdots%5Csum_%7Bv_t%5Cin+V_t%7D%5Cprod_%7B%28i%2Cj%29%5Cin%5Cbinom%7B%5Bt%5D%7D%7B2%7D%7D+X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X^{(1,2)},X^{(1,3)},\dots,X^{(t-1,t)}):=\sum_{v_1\in V_1}\sum_{v_2\in V_2}\dots\sum_{v_t\in V_t}\prod_{(i,j)\in\binom{[t]}{2}} X_{v_i,v_j}^{(i,j)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X_{v_i,v_j}^{(i,j)}" class="latex" /> denotes the coordinate that indicates if there is an edge between <img src="https://s0.wp.com/latex.php?latex=v_i%2Cv_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_i,v_j" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by picking one vertex for each part and checking if these <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> vertices form a clique. It is indeed <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite as each <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> corresponds to a part of variables.</p>



<p><strong>New recipe for worst-case to average-case reductions.</strong> Let us take a minute to think about what structural properties of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques we have used in the entire reduction except for constructing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />.</p>



<p>The answer is none! The only part specific to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques was cooking up a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" /> (let us call such polynomial “good”) that encodes this problem. The reduction we showed above works as long as we can construct such “good” polynomial for a problem. Therefore, a new recipe, which was explicitly formulated in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> for proving average-case (here “average-case” means Erdős–Rényi random input model) fine-grained hardness for a problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D%5Ccap+%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}\cap \textrm{poly}(n)" class="latex" /> in P, is </p>



<ol><li>Construct a “good” polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_p%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_p^n" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=p%3D%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p=\textrm{poly}(n)" class="latex" />, such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li></ol>



<p>Short and sweet.</p>



<p>In the <a href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/">final post</a>, I will present another instantiation of this new recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/"><span class="datestr">at July 30, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/">POSTDOC at Ben-Gurion University (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>My group at Ben-Gurion University has an open postdoctoral position, part of an ERC Starting Grant project. Starting date, as well as duration, are flexible.<br />
The position includes a generous salary, as well as funding for equipment and travel.</p>
<p>Website: <a href="https://www.cs.bgu.ac.il/~klim/Links/Call">https://www.cs.bgu.ac.il/~klim/Links/Call</a><br />
Email: klim@bgu.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/"><span class="datestr">at July 30, 2021 12:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/112">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/112">TR21-112 |  CNF Satisfiability in a Subspace and Related Problems | 

	Vikraman Arvind, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of finding a satisfying assignment to a CNF formula that must further belong to a prescribed input subspace. Equivalent formulations of the problem include finding a point outside a union of subspaces (the Union-of-Subspace Avoidance (USA) problem), and finding a common zero of a system of polynomials over the field of two elements each of which is a product of affine forms.
    
We focus on the case of $k$-CNF formulas (the $k$-SUB-SAT problem). Clearly, $k$-SUB-SAT is no easier than $k$-SAT, and might be harder. Indeed, via simple reductions we show that 2-SUB-SAT is NP-hard, and W[1]-hard when parameterized by the co-dimension of the subspace. We also prove that the optimization version Max-2-SUB-SAT is NP-hard to approximate better than the trivial $3/4$ ratio even on satisfiable instances.
    
On the algorithmic front, we investigate fast exponential algorithms which give non-trivial savings over brute-force algorithms. We give a simple branching algorithm with runtime $(1.5)^r$ for 2-SUB-SAT, where $r$ is the subspace dimension, as well as a $(1.4312)^n$ time algorithm where $n$ is the number of variables.

Turning to $k$-SUB-SAT for $k \ge 3$, while known algorithms for solving a system of degree $k$ polynomial equations already imply a solution with runtime $\approx 2^{r(1-1/2k)}$, we explore a more combinatorial approach.  Based on an analysis of critical variables (a key notion underlying the randomized $k$-SAT algorithm of Paturi, Pudlak, and Zane), we give an algorithm with runtime $\approx {n\choose {\le t}} 2^{n-n/k}$ where $n$ is the number of variables and $t$ is the co-dimension of the subspace. This improves upon the runtime of the polynomial equations approach for small co-dimension. Our combinatorial approach also achieves polynomial space in contrast to the algebraic approach that uses exponential space. We also give a PPZ-style algorithm for $k$-SUB-SAT with runtime $\approx 2^{n-n/2k}$. This algorithm is in fact oblivious to the structure of the subspace, and extends when the subspace-membership constraint is replaced by any constraint for which partial satisfying assignments can be efficiently completed to a full satisfying assignment.  Finally, for systems of $O(n)$ polynomial equations in $n$ variables over the field of two elements, we give a fast exponential algorithm when each polynomial has bounded degree irreducible factors (but can otherwise have large degree) using a degree reduction trick.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/112"><span class="datestr">at July 30, 2021 02:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5413385044107468315">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html">Covid Stats</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>A stat often quoted: <a href="https://www.nytimes.com/2021/07/22/health/coronavirus-breakthrough-infections-delta.html">About 97% of hospitalized coronavirus patients have not been vaccinated. </a> </div><div><br /></div><div>People take this as proof that once vaccinated no worries. But I have so many challenges with this statistic.</div><div><ul style="text-align: left;"><li>This statistic is down from 100% a year ago. Are vaccines working poorer now?</li><li>There are likely correlations to those vaccinated and those who take precautions like mask wearing and social distancing, though I'm sure which way those correlations go.</li><li>Those unvaccinated are more likely to be near others unvaccinated so more likely get infected and hospitalized.</li></ul><div>Even though one shouldn't draw the conclusion from the statistic, that doesn't mean the conclusion is false. The gold standard are the double-blind vaccine trials which clearly showed the vaccines more efficient and safe. So get the vaccine, not that this blog post will convince those who have been making the conscious choice not to vaccinate to change their minds.</div><div><br /></div></div><div>The other stat I found odd was that the life expectancy dropped 1.5 years in 2020. This doesn't mean on average we'll live 1.5 year less. Rather it means that someone who lives their whole life in 2020 conditions, widespread Covid without vaccines, would on average live 1.5 years less than someone who live their entire life in 2019 conditions pre-Covid.</div><div><br /></div><div>Oddly enough if you made it to 2021 your life expectancy will probably increase (assuming you've been vaccinated). We made tremendous progress in understanding vaccine technology in 2020. Also people with conditions that could have limited their later life span were more likely to be fatal Covid victims, meaning those who are left would live longer.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html"><span class="datestr">at July 29, 2021 07:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=891">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">Chess.com: 5|5 &gt; 1000</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today for the first time, I surpassed score 1000 on Chess.com playing 5|5, which means you start with a 5-minute budget, and every move you get 5 more seconds.  For a while I also played 3|2 (2-second increments), but it takes me about 2 seconds to move a piece, which means I lost games in which I knew exactly what to do, but simply couldn’t move the pieces fast enough, which I found frustrating.  Longer games I tried but I don’t seem to have the patience for.</p>



<p>I won’t reveal my id, because I feel bad about how much time I am spending losing at chess (and I think you could see all my games with my id, but I am not sure).  My self-imposed limit is losing no more than one game a day, which means on average playing 2 games per day.  (I had to stop and Google <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+i%2F2%5Ei+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\sum_i i/2^i = 2" class="latex" />; there’s a neat calculation-free proof of it which hopefully will make me remember this fact next time.)</p>



<p>However not being a robot, I sometimes get upset at the way I lose.  Most of my games are classified as <em>giveaway</em>, which means I was winning according to the computer (and myself), but then because of some stupid mistake I end up losing the game.  And so what the heck, I am better than this!, I break the rule and start another match — only to lose again, chess seems not to forgive hot heads.</p>



<p>The main reason why I play seems to be that fast-paced chess has the ability to completely absorb my mind, so it’s a good quick escape.  Of course, there are also the little feel-good voices reminding me that it’s better than watching TV and that by playing I sharpen my mind.</p>



<p>While 1000 can of course be a ridiculously low bar by some standard, I found reaching it more difficult than I expected, and I like to think that the 5|5 format attracts stronger players, so that the competition is tougher, even though it may not be true.   (But it does seem true that a certain score in a certain format does not correspond to the same score in a different format.)  For one thing, I had to familiarize myself with several basic openings.  I bought a little cute book <em>Chess openings for kids</em> which is good for people like me whose knowledge of chess openings was “e4 e5.”  I don’t do anything fancy, but it was fun to read about common openings.  I think I also wouldn’t mind playing random chess, but it seems harder to find opponents.</p>



<p>So why don’t you try and see what is your 5|5 score?  And if you want to play sometimes, drop me a line.</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/"><span class="datestr">at July 28, 2021 06:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5661">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5661">Striking new Beeping Busy Beaver champion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For the past few days, I was bummed about the sooner-than-expected loss of Steven Weinberg.  Even after putting up my <a href="https://www.scottaaronson.com/blog/?p=5566">post</a>, I spent hours just watching old interviews with Steve on YouTube and reading his old essays for gems of insight that I’d missed.  (Someday, I’ll tackle Steve’s celebrated quantum field theory and general relativity textbooks … but that day is not today.)</p>



<p>Looking for something to cheer me up, I was delighted when <em>Shtetl-Optimized</em> reader Nick Drozd reported a significant new discovery in BusyBeaverology—one that, I’m proud to say, was directly inspired by my <a href="https://www.scottaaronson.com/papers/bb.pdf">Busy Beaver survey article</a> from last summer (<a href="https://www.scottaaronson.com/blog/?p=4916">see here for blog post</a>).</p>



<p>Recall that BB(n), the n<sup>th</sup> Busy Beaver number (technically, “Busy Beaver shift number”), is defined as the maximum number of steps that an n-state Turing machine, with 1 tape and 2 symbols, can make on an initially all-0 tape before it invokes a Halt transition.  Famously, BB(n) is not only uncomputable, it grows faster than any computable function of n—indeed, computing anything that grows as quickly as Busy Beaver is equivalent to solving the halting problem.</p>



<p>As of 2021, here is the extent of human knowledge about concrete values of this function:</p>



<ul><li>BB(1) = 1 (trivial)</li><li>BB(2) = 6 (Lin 1963)</li><li>BB(3) = 21 (Lin 1963)</li><li>BB(4) = 107 (Brady 1983)</li><li>BB(5) ≥ 47,176,870 (Marxen and Buntrock 1990)</li><li>BB(6) &gt; 7.4 × 10<sup>36,534</sup> (Kropitz 2010)</li><li>BB(7) &gt; 10<sup>2×10^10^10^18,705,352</sup> (“Wythagoras” 2014)</li></ul>



<p>As you can see, the function is reasonably under control for n≤4, then “achieves liftoff” at n=5.</p>



<p>In my survey, inspired by a suggestion of Harvey Friedman, I defined a variant called Beeping Busy Beaver, or BBB.  Define a <em>beeping Turing machine</em> to be a TM that has a single designated state where it emits a “beep.”  The <em>beeping number</em> of such a machine M, denoted b(M), is the largest t such that M beeps on step t, or ∞ if there’s no finite maximum.  Then BBB(n) is the largest finite value of b(M), among all n-state machines M.</p>



<p>I noted that the BBB function grows uncomputably <em>even given an oracle for the ordinary BB function</em>.  In fact, computing anything that grows as quickly as BBB is equivalent to solving any problem in the second level of the <a href="https://en.wikipedia.org/wiki/Arithmetical_hierarchy">arithmetical hierarchy</a> (where the computable functions are in the zeroth level, and the halting problem is in the first level).  Which means that pinning down the first few values of BBB should be <em>even</em> <em>more</em> breathtakingly fun than doing the same for BB!</p>



<p>In my survey, I noted the following four concrete results:</p>



<ul><li>BBB(1) = 1 = BB(1)</li><li>BBB(2) = 6 = BB(2)</li><li>BBB(3) ≥ 55 &gt; 21 = BB(3)</li><li>BBB(4) ≥ 2,819 &gt; 107 = BB(4)</li></ul>



<p>The first three of these, I managed to get on my own, with the help of a little program I wrote.  The fourth one was communicated to me by Nick Drozd even before I finished my survey.</p>



<p>So as of last summer, we knew that BBB coincides with the ordinary Busy Beaver function for n=1 and n=2, then breaks away starting at n=3.  We didn’t know how quickly BBB “achieves liftoff.”</p>



<p>But Nick continued plugging away at the problem all year, and he now claims to have resolved the question.  More concretely, he claims the following two results:</p>



<ul><li>BBB(3) = 55 (via exhaustive enumeration of cases)</li><li>BBB(4) ≥ 32,779,478 (via a newly-discovered machine)</li></ul>



<p>For more, see Nick’s <a href="https://cs.nyu.edu/pipermail/fom/2021-July/022743.html">announcement on the Foundations of Mathematics email list</a>, or his own <a href="https://nickdrozd.github.io/2021/07/11/self-cleaning-turing-machine.html">blog post</a>.</p>



<p>Nick actually writes in terms of yet another Busy Beaver variant, which he calls BLB, or “Blanking Beaver.”  He defines BLB(n) to be the maximum finite number of steps that an n-state Turing machine can take before it first “wipes its tape clean”—that is, sets all the tape squares to 0, as they were at the very beginning of the computation, but as they were <em>not</em> at intermediate times.  Nick has discovered a 4-state machine that takes 32,779,477 steps to blank out its tape, thereby proving that</p>



<ul><li>BLB(4) ≥ 32,779,477.</li></ul>



<p>Nick’s construction, when investigated, turns out to be based on a “Collatz-like” iterative process—exactly like the BB(5) champion and most of the other strong Busy Beaver contenders currently known.  A simple modification of his construction yields the lower bound on BBB.</p>



<p>Note that the Blanking Beaver function does <em>not</em> have the same sort of super-uncomputable growth that Beeping Busy Beaver has: it merely grows “normally” uncomputably fast, like the original BB function did.  Yet we see that BLB, just like BBB, already “achieves liftoff” by n=4, rather than n=5.  So the real lesson here is that <em>4-state Turing machines can already do fantastically complicated things on blank tapes</em>.  It’s just that the usual definitions of the BB function artificially prevent us from seeing that; they hide the uncomputable insanity until we get to 5 states.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5661"><span class="datestr">at July 27, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/">Postdoc at IST Austria (apply by August 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>My group at IST Austria has an open postdoctoral position, part of an ERC Starting Grant project, whose goal is to develop new theory, and algorithms for scalable machine learning.</p>
<p>For questions, please contact dan.alistarh@ist.ac.at. The application should contain a CV, publication list, and a 1-page statement describing motivation and research interests.</p>
<p>Website: <a href="https://scaleml.pages.ist.ac.at/">https://scaleml.pages.ist.ac.at/</a><br />
Email: dan.alistarh@ist.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/"><span class="datestr">at July 26, 2021 05:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-161863856324090968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html">I wish problems I have with computers really were my fault</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> As you know, the website for out for a few days, as Lance explained <a href="https://blog.computationalcomplexity.org/2021/07/technical-difficulties.html">here</a>.</p><p>When I first could not get to the this blog  my thought was</p><p><i>OH, I must have changed some setting by accident. When Lance gets back (he was on vacation) he'll know how to fix it. Bad timing that it happened when he was gone, though prob not an accident- with him on vacation I was at the site more often and had more of a chance to screw things up. AND Lance will tell me what I did and I'll know to not do it again. And I will learn more about how this all works which will help me in the future!</i></p><p>When Lance got back we found out that NO Bill didn't do anything wrong. The blog site company  that we work with did an update and BLAH BLAH BLAH.  Reminds me of the theme behind the TV show Seinfeld: <i>No Hugs, No Learning.</i> At least no learning. I am not in the slightest more enlightened. </p><p>Lance worked with them and YADA YADA YADA the problem is fixed, so I am very happy about that. </p><p>On the one hand I wish it had been my fault so I would learn something.  On he other hand, if it was my fault would it have been as easy to fix? Would I really have learned something? </p><p>When something does not work my protocol is</p><p>1) Turn the machine off and on again (e.g., log out and log in again). I want to say </p><p><i>this works surprisingly often</i></p><p>but I doubt this surprises any of my readers, or is even news to them.</p><p>2) Spend at most 5 minutes <i>trying to fix it myself . </i>You will soon see that 5 minutes is a good choice for me.</p><p>3) Ask staff or Lance or Darling or my TA  (depending on the problem). </p><p>4) They tell me to log off and log on again. When I tell them I already have they do something magical and it works again. I then ask them:</p><p>a) Could I have fixed this myself. 2/3 of the time the answer is no. They don't mean intellectually. They mean that I do not have access to what I need to fix it.</p><p>b) Did I do something wrong? I want to know so I won't do it again. about 99/100 times the answer is that I did nothing wrong (I don't recall that last time that I did).</p><p>Given a and b, I think 5 minutes is all the time I want to spend to try to fix it myself. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html"><span class="datestr">at July 26, 2021 04:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/111">TR21-111 |  Influence of a Set of Variables on a Boolean Function | 

	Aniruddha  Biswas, 

	Palash Sarkar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The influence of a set of variables on a Boolean function has three separate definitions in the literature, the first due to Ben-Or and Linial (1989), the second due to Fischer et al. (2002) and Blais (2009) and the third due to Tal (2017). The goal of the present work is to carry out a comprehensive study of the notion of influence of a set of variables on a Boolean function. To this end, we introduce a           definition of this notion using the auto-correlation function. A modification of the definition leads to the notion of pseudo-influence. Somewhat surprisingly, it turns out that the auto-correlation based definition of influence is equivalent to the definition introduced by Fischer et al. (2002) and Blais (2009) and the notion of pseudo-influence is equivalent to the definition of influence considered by Tal (2017). Extensive analysis of influence and pseduo-influence as well as the Ben-Or and Linial notion of influence is carried out and the relations between these notions are established.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/111"><span class="datestr">at July 25, 2021 04:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/110">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/110">TR21-110 |  Fourier growth of structured $\mathbb{F}_2$-polynomials and applications | 

	Jaroslaw Blasiok, 

	Peter Ivanov, 

	Yaonan Jin, 

	Chin Ho Lee, 

	Rocco Servedio, 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$ (denoted $L_{1,k}$), of various well-studied classes of "structured" $\mathbb{F}_2$-polynomials. This study is motivated by applications in pseudorandomness, in particular recent results and conjectures due to [CHHL19,CHLT19,CGLSS20] which show that upper bounds on Fourier growth (even at level $k=2$) give unconditional pseudorandom generators.

  Our main structural results on Fourier growth are as follows:

  - We show that any symmetric degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot O(d)^k$, and this is tight for any constant $k$. This quadratically strengthens an earlier bound that was implicit in [RSV13].

  - We show that any read-$\Delta$ degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot (k \Delta d)^{O(k)}$.

  - We establish a composition theorem which gives $L_{1,k}$ bounds on disjoint compositions of functions that are closed under restrictions and admit $L_{1,k}$ bounds.

  Finally, we apply the above structural results to obtain new unconditional pseudorandom generators and new correlation bounds for various classes of $\mathbb{F}_2$-polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/110"><span class="datestr">at July 25, 2021 11:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5566">Steven Weinberg (1933-2021): a personal view</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p></p>



<p></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://www.sciencenews.org/wp-content/uploads/2021/07/072421_weinberg_closecrop-1030x580.jpg" alt="Steven Weinberg sitting in front of a chalkboard covered in equations" /></figure></div>



<p>Steven Weinberg was, perhaps, the last truly towering figure of 20th-century physics.  In 1967, he wrote a <a href="https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.19.1264">3-page paper</a> saying in effect that as far as he could see, two of the four fundamental forces of the universe—namely, electromagnetism and the weak nuclear force—had actually been the same force until a tiny fraction of a second after the Big Bang, when a broken symmetry caused them to decouple.  Strangely, he had developed the math underlying this idea for the strong nuclear force, and it didn’t work there, but it <em>did</em> seem to work for the weak force and electromagnetism.  Steve noted that, if true, this would require the existence of two force-carrying particles that hadn’t yet been seen — the W and Z bosons — and would <em>also </em>require the existence of the famous Higgs boson.</p>



<p>By 1979, enough of this picture had been confirmed by experiment that Steve shared the Nobel Prize in Physics with Sheldon Glashow—Steve’s former high-school classmate—as well as with Abdus Salam, both of whom had separately developed pieces of the same puzzle.  As arguably the central architect of what we now call the Standard Model of elementary particles, Steve was in the ultra-rarefied class where, had he <em>not</em> won the Nobel Prize, it would’ve been a stain on the prize rather than on him.</p>



<p>Steve once recounted in my hearing that Richard Feynman initially heaped scorn on the electroweak proposal.  Late one night, however, Steve was woken up by a phone call.  It was Feynman.  “I believe your theory now,” Feynman announced.  “Why?” Steve asked.  Feynman, being Feynman, gave some idiosyncratic reason that he’d worked out for himself.</p>



<p>It used to happen more often that someone would put forward a bold new proposal about the most fundamental laws of nature … and then the experimentalists would <em>actually go out and confirm it</em>.  Besides with the Standard Model, though, there’s approximately <em>one</em> other time that that’s happened in the living memory of most of today’s physicists.  Namely, when astronomers discovered in 1998 that the expansion of the universe was accelerating, apparently due to a dark energy that behaved like Einstein’s long-ago-rejected cosmological constant.  Very few had expected such a result.  There was one prominent exception, though: Steve Weinberg had <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.2607">written in 1987</a> that he saw no reason why the cosmological constant shouldn’t take a nonzero value that was still tiny enough to be consistent with galaxy formation and so forth.</p>



<hr class="wp-block-separator" />



<p>In his long and illustrious career, one of the <em>least</em> important things Steve did, six years ago, was to play a major role in recruiting me and my wife Dana to UT Austin.  The first time I met Steve, his first question to me was “have we met before?  you look familiar.”  It turns out that he’d met my dad, Steve Aaronson, way back in the 1970s, when my dad (then a young science writer) had interviewed Weinberg for a magazine article.  I was astonished that Weinberg would remember such a thing across decades.</p>



<p>Steve was then gracious enough to take me, Dana, and both of my parents out to dinner in Austin as part of my and Dana’s recruiting trip.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><a href="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg"><img width="422" alt="" src="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg" class="wp-image-5618" height="317" /></a></figure></div>



<p>We talked, among other things, about Telluride House at Cornell, where Steve had lived as an undergrad in the early 1950s and where I’d lived as an undergrad almost half a century later.  Steve said that, while he loved the intellectual atmosphere at Telluride, he tried to have as little to do as possible with the “self-government” aspect, since he found the political squabbles that convulsed many of the humanities majors there to be a waste of time.  I burst out laughing, because … well, imagine you got to have dinner with James Clerk Maxwell, and he opened up about some ridiculously specific pet peeve from his college years, and it was <em>your</em> ridiculously specific pet peeve from <em>your</em> college years.</p>



<p>(Steve claimed to us, not entirely convincingly, that he was a mediocre student at Cornell, more interesting in “necking” with his fellow student and future wife Louise than in studying physics.)</p>



<p>After Dana and I came to Austin, Steve was kind enough to invite me to the high-energy theoretical physics lunches, where I chatted with him and the other members of his group every week (or better yet, simply listened).  I’d usually walk to the faculty club ten minutes early.  Steve, having arrived by car, would be sitting alone in an armchair, reading a newspaper, while he waited for the other physicists to arrive by foot.  No matter how scorching the Texas sun, Steve would <em>always</em> be wearing a suit (usually a tan one) and a necktie, his walking-cane by his side.  I, typically in ratty shorts and t-shirt, would sit in the armchair next to him, and we’d talk—about the latest developments in quantum computing and information (Steve, a perpetual student, would pepper me with questions), or his recent work on nonlinear modifications of quantum mechanics, or his memories of Cambridge, MA, or climate change or the anti-Israel protests in Austin or whatever else.  These conversations, brief and inconsequential as they probably were to him, were highlights of my week. </p>



<p>There was, of course, something a little melancholy about getting to know such a great man only in the twilight of his life.  To be clear, Steve Weinberg in his mid-to-late 80s was <em>far</em> more cogent, articulate, and quick to understand what was said to him than just about anyone you’d ever met in their prime.  But then, after a short conversation, he’d have to leave for a nap.  Steve was as clear-eyed and direct about his age and impending mortality as he was about everything else.  “Scott!” he once greeted me.  “I just saw the announcement for your physics colloquium about quantum supremacy.  I hope I’m still alive next month to attend it.”</p>



<p>(As it happens, the colloquium in question was on November 9, 2016, the day we learned that Trump would become president.  I offered to postpone the talk, since no one could concentrate on physics on such a day.  While several of the physicists agreed that that was the right call, Steve convinced me to go ahead with the following message: “I sympathize, but I do want to hear you … There is some virtue in just plowing on.”)</p>



<p>I sometimes felt, as well, like I was speaking with Steve across a cultural chasm even greater than the half-century that separated us in age.  Steve enjoyed nothing more than to discourse at length, in his booming New-York-accented baritone, about opera, or ballet, or obscure corners of 18th-century history.  It would be easy to feel like a total philistine by comparison … and I did.  Steve also told me that he never reads blogs or other social media, since he’s unable believe any written work is “real” unless it’s published, ideally on paper.  I could only envy such an attitude.</p>



<hr class="wp-block-separator" />



<p>If you <em>did</em> try to judge by the social media that he never read, you might conclude that Steve would be remembered by the wider world less for any of his epochal contributions to physics than for a single viral quote of his:</p>



<blockquote class="wp-block-quote"><p>With or without religion, good people can behave well and bad people can do evil; but for good people to do evil — that takes religion.</p></blockquote>



<p>I can testify that Steve fully lived his atheism.  Four years ago, I invited him (along with many other UT colleagues) to the <em>brit milah</em> of my newborn son Daniel.  Steve said he’d be happy to come over our house another time (and I’m happy to say that he did a year later), but not to witness any body parts being cut.</p>



<p>Despite his hostility to Judaism—along with every other religion—Steve was a vociferous supporter of the state of Israel, almost to the point of making me look like Edward Said or Noam Chomsky.  For Steve, Zionism was not in spite of his liberal, universalist Enlightenment ideals but because of them.</p>



<p>Anyway, there’s no need even to wonder whether Steve had any sort of deathbed conversion.  He’d laugh at the thought.</p>



<hr class="wp-block-separator" />



<p>In 2016, Steve published <a href="https://www.amazon.com/Explain-World-Discovery-Modern-Science/dp/0062346660"><em>To Explain the World</em></a>, a history of human progress in physics and astronomy from the ancient Greeks to Newton (when, Steve says, the scientific ethos reached the form that it still basically has today).  It’s unlike any other history-of-science book that I’ve read.  Of course I’d read other books about Aristarchus and Ptolemy and so forth, but I’d never read a modern writer treating them not as historical subjects, but as <em>professional colleagues merely separated in time.</em>  Again and again, Steve would redo ancient calculations, finding errors that had escaped historical notice; he’d remark on how Eratosthenes or Kepler could’ve done better with the data available to them; he’d grade the ancients by how much of modern physics and cosmology they’d correctly anticipated.</p>



<p><em>To Explain the World</em> was savaged in reviews by professional science historians.  Apparently, Steve had committed the unforgivable sin of “Whig history”: that is, judging past natural philosophers by the standards of today.  Steve clung to the naïve, debunked, scientistic notions that there’s such a thing as “actual right answers” about how the universe works; that we today are, at any rate, much closer to those right answers than the ancients were; and that we can <em>judge</em> the ancients by how close they got to the right answers that we now know.</p>



<p>As I read the sneering reviews, I kept thinking: so suppose Archimedes, Copernicus, and all the rest were brought back from the dead.  Who would they rather talk to: historians seeking to explore every facet of their misconceptions, like anthropologists with a paleolithic tribe; or Steve Weinberg, who’d want to bring them up to speed as quickly as possible so they could continue the joint quest?</p>



<hr class="wp-block-separator" />



<p>When it comes to the foundations of quantum mechanics, Steve <a href="https://www.nybooks.com/articles/2017/01/19/trouble-with-quantum-mechanics/">took the view</a> that <em>no</em> existing interpretation is satisfactory, although the Many-Worlds Interpretation is perhaps the least bad of the bunch.  Steve felt that our reaction to this state of affairs should be to <em>test quantum mechanics more precisely</em>—for example, by looking for tiny nonlinearities in the Schrödinger equation, or other signs that QM itself is only a limit of some more all-encompassing theory.  This is, to put it mildly, not a widely-held view among high-energy physicists—but it provided a fascinating glimpse into how Steve’s mind works.</p>



<p>Here was, empirically, the most successful theoretical physicist alive, and again and again, his response to conceptual confusion was not to ruminate more about basic principles but to <em>ask for more data</em> or <em>do a more detailed calculation</em>.  He never, ever let go of a short tether to the actual testable consequences of whatever was being talked about, or future experiments that might change the situation.</p>



<p>(Steve worked on string theory in the early 1980s, and he remained engaged with it for the rest of his life, for example by recruiting the string theorists Jacques Distler and Willy Fischler to UT Austin.  But he later soured on the prospects for getting testable consequences out of string theory within a reasonable timeframe.  And he once complained to me that the papers he’d read about “It from Qubit,” AdS/CFT, and the black hole information problem had had “too many words and not enough equations.”)</p>



<hr class="wp-block-separator" />



<p>Steve was, famously, about as hardcore a reductionist as has ever existed on earth.  He was a reductionist not just in the usual sense that he believed there <em>are</em> fundamental laws of physics, from which, together with the initial conditions, everything that happens in our universe can be calculated in principle (if not in practice), at least probabilistically.  He was a reductionist in the stronger sense that he thought the quest to discover the fundamental laws of the universe had a special pride of place among all human endeavors—a place not shared by the many sciences devoted to the study of complex emergent behavior, interesting and important though they might be.</p>



<p>This came through clearly in Steve’s <a href="https://www.nybooks.com/articles/2002/10/24/is-the-universe-a-computer/">critical review</a> of Stephen Wolfram’s <em>A New Kind of Science</em>, where Steve (Weinberg, that is) articulated his views of why “free-floating” theories of complex behavior can’t take the place of a reductionistic description of our actual universe.  (Of course, I was <em>also</em> highly critical of <em>A New Kind of Science</em> in <a href="https://arxiv.org/abs/quant-ph/0206089">my review</a>, but for somewhat different reasons than Steve was.)  Steve’s reductionism was also clearly expressed in his testimony to Congress in support of continued funding for the Superconducting Supercollider.  (Famously, Phil Anderson testified <em>against</em> the SSC, arguing that the money would better be spent on condensed-matter physics and other sciences of emergent behavior.  The result: Congress did cancel the SSC, and it redirected precisely zero of the money to other sciences.  But at least Steve lived to see the LHC dramatically confirm the existence of the Higgs boson, as the SSC would have.)</p>



<p>I, of course, have devoted my career to theoretical computer science, which you might broadly call a “science of emergent behavior”: it tries to figure out the ultimate possibilities and limits of computation, taking the underlying laws of physics as given.  Quantum computing, in particular, takes as its input a physical theory that was already known by 1926, and studies what can be done with it.  So you might expect me to disagree passionately with Weinberg on reductionism versus holism.</p>



<p>In reality, I have a hard time pinpointing any substantive difference.  Mostly I see a difference in <em>opportunities</em>: Steve saw a golden chance to contribute something to the millennia-old quest to discover the fundamental laws of nature, at the tail end of the heroic era of particle physics that culminated in what we now call the Standard Model.  He was brilliant enough to seize that chance.  I didn’t see a similar chance: possibly because it no longer existed; almost certainly because, even if it did, I wouldn’t have had the right mind for it.  I found a different chance, to work at the intersection of physics and computer science that was finally kicking into high gear at the end of the 20th century.  Interestingly, while I came to that intersection from the CS side, quite a few who were originally trained as high-energy physicists ended up there as well—including a star PhD student of Steve Weinberg’s named John Preskill.</p>



<p>Despite his reductionism, Steve was as curious and enthusiastic about quantum computation as he was about a hundred other topics beyond particle physics—he even ended his <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> with a chapter about Shor’s factoring algorithm.  Having said that, a central <em>reason</em> for his enthusiasm about QC was that he clearly saw how demanding a test it would be of quantum mechanics itself—and as I mentioned earlier, Steve was open to the possibility that quantum mechanics might not be exactly true.</p>



<hr class="wp-block-separator" />



<p>It would be an understatement to call Steve “left-of-center.”  He believed in higher taxes on rich people like himself to service a robust social safety net.  When Trump won, Steve remarked to me that most of the disgusting and outrageous things Trump would do could be reversed in a generation or so—but not the aggressive climate change denial; that actually <em>could</em> matter on the scale of centuries.  Steve made the news in Austin for <a href="https://www.texastribune.org/2016/01/27/the-brief/">openly defying</a> the Texas law forcing public universities to allow concealed carry on campus: he said that, regardless of what the law said, firearms would not be welcome in <em>his</em> classroom.  (Louise, Steve’s wife for 67 years and a professor at UT Austin’s law school, also wrote perhaps the <a href="https://law.utexas.edu/faculty/publications/2002-When-Courts-Decide-Elections-The-Constitutionality-of-Bush-v-Gore">definitive scholarly takedown</a> of the shameful <em>Bush vs. Gore</em> Supreme Court decision, which installed George W. Bush as president.)</p>



<p>All the same, during the “science wars” of the 1990s, Steve was <a href="https://www.nybooks.com/articles/1996/08/08/sokals-hoax/">scathing</a> about the academic left’s postmodernist streak and deeply sympathetic to what Alan Sokal had done with his <em>Social Text </em>hoax.  Steve also once told me that, when he (like other UT faculty) was required to write a statement about what he would do to advance Diversity, Equity, and Inclusion, he submitted just a single sentence: “I will seek the best candidates, without regard to race or sex.”  I remarked that he might be one of the only academics who could get away with that.</p>



<p>I confess that, for the past five years, knowing Steve was a greater source of psychological strength for me than, from a rational standpoint, it probably should have been.  Regular readers will know that I’ve spent months of my life agonizing over various nasty things people have said me about on Twitter and Reddit—that I’m a sexist white male douchebag, a clueless techbro STEMlord, a neoliberal Zionist shill, and I forget what else.</p>



<p>But I lately <em>have</em> had a secret emotional weapon that helped somewhat: namely, the certainty that Steven Weinberg had more intellectual power in a single toenail clipping than these Twitter-attackers had collectively experienced over the course of their lives.  It’s like, have you heard the joke where two rabbis are arguing some point of Talmud, and then God speaks from a booming thundercloud to declare that the first rabbi is right, and then the second rabbi says “OK fine, now it’s 2 against 1?”  For the W and Z bosons and Higgs boson that <em>you</em> predicted to turn up at the particle accelerator is not <em>exactly</em> God declaring from a thundercloud that the way your mind works is aligned with the way the world actually is—Steve, of course, would wince at the suggestion—but it’s about the closest thing available in this universe.  My secret emotional weapon was that I knew the man who’d experienced this, arguably more than any of the 7.6 billion other living humans, and not only did that man not sneer at me, but by some freakish coincidence, he seemed to have reached roughly the same views as I had on &gt;95% of controversial questions where we both had strong opinions.</p>



<hr class="wp-block-separator" />



<p>My final conversations with Steve Weinberg were about a laptop.  When covid started in March 2020, Steve and Louise, being in their late 80s, naturally didn’t want to take chances, and rigorously sheltered at home.  But an issue emerged: Steve couldn’t install Zoom on his Bronze Age computer, and so couldn’t participate in the virtual meetings of his own group, nor could he do Zoom calls with his daughter and granddaughter.  While as a <em>theoretical</em> computer scientist, I don’t normally volunteer myself as tech support staff, I decided that an exception was more than warranted in this case.  The quickest solution was to configure one of my own old laptops with everything Steve needed and bring it over to his house.</p>



<p>Later, Steve emailed me to say that, while the laptop had worked great and been a lifesaver, he’d finally bought his own laptop, so I should come by to pick mine up.  I delayed and delayed with that, but finally decided I should do it before leaving Austin at the beginning of this summer.  So I emailed Steve to tell him I’d be coming.  He replied to me asking Louise to leave the laptop on the porch — but the email was addressed only to me, not her.</p>



<p>At that moment, I knew something had changed: only a year before, incredibly, <em>I’d</em> been more senile and out-of-it as a 39-year-old than Steve had been as an 87-year-old.  What I didn’t know at the time was that Steve had sent that email from the hospital when he was close to death.  It was the last I heard from him.</p>



<p>(Once I learned what was going on, I did send a get-well note, which I hope Steve saw, saying that I hoped he appreciated that I <em>wasn’t</em> praying for him.)</p>



<hr class="wp-block-separator" />



<p>Besides the quote about good people, bad people, and religion, the other quote of Steve’s that he never managed to live down came from the last pages of <a href="https://www.amazon.com/First-Three-Minutes-Modern-Universe/dp/0465024378"><em>The First Three Minutes</em></a>, his classic 1970s popularization of big-bang cosmology:</p>



<blockquote class="wp-block-quote"><p>The more the universe seems comprehensible, the more it also seems pointless.</p></blockquote>



<p>In the 1993 epilogue, Steve tempered this with some more hopeful words, nearly as famous:</p>



<blockquote class="wp-block-quote"><p>The effort to understand the universe is one of the very few things which lifts human life a little above the level of farce and gives it some of the grace of tragedy.</p></blockquote>



<p>It’s not my purpose here to resolve the question of whether life or the universe have a point.  What I can say is that, even in his last years, Steve never for a nanosecond <em>acted</em> as if life was pointless.  He already had all the material comforts and academic renown anyone could possibly want.  He could have spent all day in his swimming pool, or listening to operas.  Instead, he continued publishing textbooks—a <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> in 2012, an <a href="https://www.amazon.com/Lectures-Astrophysics-Steven-Weinberg/dp/1108415075">astrophysics textbook</a> in 2019, and a <a href="https://www.amazon.com/dp/1108841767/?tag=pfamazon01-20">“Foundations of Modern Physics” textbook</a> in 2021 (!).  As recently as this year, he continued <a href="https://arxiv.org/search/hep-th?searchtype=author&amp;query=Weinberg%2C+Steven">writing papers</a>—and not just “great man reminiscing” papers, but hardcore technical papers.  He continued writing with nearly unmatched lucidity for a general audience, in the <em>New York Review of Books</em> and elsewhere.  And I can attest that he continued peppering visiting speakers with questions about stellar evolution or whatever else they were experts on—because, more likely than not, he had redone some calculation himself and gotten a subtly different result from what was in the textbooks.</p>



<p>If God exists, I can’t believe He or She would find nothing more interesting to do with Steve than to torture him for his unbelief.  More likely, I think, God is right now talking to Steve the same way Steve talked to Aristarchus in <em>To Explain the World</em>: “yes, you were close about the origin of neutrino masses, but here’s the part you were missing…”  While, of course, Steve is redoing God’s calculation to be sure.</p>



<hr class="wp-block-separator" />



<p>Feel free to use the comments as a place to share your own memories.</p>



<hr class="wp-block-separator" />



<p><strong>More Steven Weinberg memorial links (I’ll continue adding to this over the next few days):</strong></p>



<ul><li><a href="https://www.math.columbia.edu/~woit/wordpress/?p=12413">Peter Woit</a></li><li><a href="https://motls.blogspot.com/2021/07/steven-weinberg-1933-2021.html?m=1">He-Who-Must-Not-Be-Named</a></li><li><a href="https://mobile.twitter.com/johncarlosbaez/status/1418804320611364867?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet">John Baez</a> </li><li><a href="https://www.sciencenews.org/article/steven-weinberg-death-physics-electromagnetism-standard-model">Tom Siegfried</a></li><li><a href="https://mobile.twitter.com/seanmcarroll/status/1418804903871356930">Sean Carroll</a></li><li><a href="https://whyevolutionistrue.com/2021/07/24/steven-weinberg-died/">Jerry Coyne</a></li><li><a href="https://grahamfarmelo.com/remembering-steven-weinberg/">Graham Farmelo</a></li><li><a href="https://www.nytimes.com/2021/07/25/science/steven-weinberg-groundbreaking-nobelist-in-physics-dies-at-88.html">NYT</a></li><li><a href="https://physicstoday.scitation.org/do/10.1063/PT.6.4.20210803a/full/"><em>Physics Today</em> remembrances</a></li><li><a href="https://thebulletin.org/2021/07/steven-weinberg-nobel-laureate-in-physics-and-bulletin-board-member-died-at-88/"><em>Bulletin of the Atomic Scientists</em></a></li></ul>



<hr class="wp-block-separator" />



<p><strong>Miscellaneous Steven Weinberg links</strong></p>



<ul><li><a href="https://mobile.twitter.com/dashunwang/status/1419267004905754624">Steve’s advice to young scientists</a></li><li><a href="https://physicstoday.scitation.org/doi/full/10.1063/1.3397044">Lenny Susskind’s 2010 review</a> of Steve’s book <em>Lake Views</em></li><li><a href="https://www.closertotruth.com/contributor/steven-weinberg/profile">Steve’s interviews on “Closer to Truth”</a></li><li><a href="https://cerncourier.com/a/model-physicist/">Interview in <em>CERN Courier</em></a></li><li>Steve <a href="https://www.youtube.com/watch?v=ebuve4INdAU&amp;t=1454s">spars with another of my greatest friends and heroes</a>, Rebecca Goldstein, about the basis of morality—also featuring Richard Dawkins, Daniel Dennett, Sean Carroll, and more (YouTube video from a 2012 conference)</li></ul>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5566"><span class="datestr">at July 25, 2021 04:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/109">TR21-109 |  QRAT Polynomially Simulates Merge Resolution. | 

	Sravanthi Chede, 

	Anil Shukla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Merge Resolution (MRes [Beyersdorff et al. J. Autom. Reason.'2021] ) is a refutational proof system for quantified Boolean formulas (QBF). Each line of MRes consists of clauses with only existential literals, together with information of countermodels stored as merge maps. As a result, MRes has strategy extraction by design. The QRAT [Heule et al. J. Autom. Reason.'2017] proof system was designed to capture QBF preprocessing. QRAT can simulate both the expansion-based proof system $\forall$Exp+Res and CDCL-based QBF proof system LD-Q-Res. 

A family of false QBFs called SquaredEquality formulas were introduced in [Beyersdorff et al. J. Autom. Reason.'2021] and shown to be easy for MRes but need exponential size proofs in Q-Res, QU-Res, CP+$\forall$red, $\forall$Exp+Res, IR-calc and reductionless LD-Q-Res. As a result none of these systems can simulate MRes. In this paper, we show a short QRAT refutation of the SquaredEquality formulas. We further show that QRAT strictly p-simulates MRes. 
Besides highlighting the power of QRAT system, this work also presents the first simulation result for MRes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/109"><span class="datestr">at July 23, 2021 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">Postdoc at University of Vienna (apply by August 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two full-time 3-year postdoc positions in algorithms are available starting Jan 1, 2022 as part of the ERC Advanced Grant “Modern Dynamic Data Structures” to joint the algorithms research group headed by Monika Henzinger. Prior knowledge in fair algorithms and differential privacy is a plus. Please send your CV, a letter of motivation, and the names of 3 references to applications.taa@univie.ac.at</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br />
Email: applications.taa@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/"><span class="datestr">at July 23, 2021 06:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5690">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/">A Basic Question About Multiplicative Energy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As part of some research project, I got to a basic question about multiplicative energy. Embarrassingly , I wasn’t able to get any non-trivial bound for it. Here is the problem. Any information about it would be highly appreciated. Problem. Let . Let be a set of real numbers. How large can the multiplicative energy […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/"><span class="datestr">at July 23, 2021 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
