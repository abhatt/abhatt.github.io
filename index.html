<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 10, 2021 09:40 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=86">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/">Thursday March 18 — Tim Roughgarden  from Columbia University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, March 18</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Tim Roughgarden</strong> from <strong>Columbia Univeristy</strong> will speak about “<strong>Data-Driven Algorithm Design</strong>.”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: The best algorithm for a computational problem generally depends on the “relevant inputs”, a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem.</p>



<p class="has-text-align-justify">We adapt concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models are straightforward to understand, but also expressive enough to capture several existing approaches in the theoretical computer science and AI communities, ranging from self-improving algorithms to empirical performance models. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binary- and real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.</p>



<p>Joint work with Rishi Gupta.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/"><span class="datestr">at March 09, 2021 08:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1823">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/03/09/automated-design-of-error-correcting-codes-part-2/">Automated Design of Error-Correcting Codes, Part 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In our <a href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/">previous post</a>, we discussed the automation of error correcting codes and how formal methods are quite helpful toward this goal. In this post, we will discuss machine learning techniques as well as possible directions for future research.</p>



<p><strong>Machine Learning. </strong>The use of machine-learning techniques to study ECCs has increased exponentially in recent years. The main strength of machine learning methods is that they can adapt well to a variety of conditions (c.f., <a href="https://arxiv.org/pdf/1911.03038.pdf">this paper</a>), unlike the codes produced by formal methods which are often designed for a rigid application. On the other hand, it is much more difficult to give any formal guarantees for error-correcting codes designed with machine learning, although such an obstacle may be overcome in the future (see the conclusion). </p>



<p><strong>Error-correcting output codes. </strong>One interface between machine learning and error correcting codes which has been studied for decades is the construction of error correcting codes for multiclass learning. Multiclass learning is classifying a group of objects into a number of categories. Instead of trying to distinguish the categories all up front, one can instead try to do a binary classification of some subset of the categories from another subset of the categories.” If an error correcting code (called an error-correcting output code–ECOC) is used for the distinguishing, it can provide a more robust classification.</p>



<p>This technique was studied theoretically for a preset error-correcting code (e.g., <a href="https://dl.acm.org/doi/pdf/10.1145/307400.307429">[Guruswami, Sahai, 1999]</a>). There are also works which synthesize an ECOC for multiclass learning. For example, the work of <a href="https://ieeexplore.ieee.org/abstract/document/1624364">Pujol, Radeva, and Vitria [2006]</a> used a heuristic which tries to find tests which reveal the highest amount of information (subject to a prior that each class is a <a href="https://en.wikipedia.org/wiki/Mixture_of_gaussians#Gaussian_mixture_model">mixture of gaussians</a>).</p>



<p>A related recent work <a href="https://arxiv.org/pdf/1808.01942.pdf">[Xiang, Wang, Kitani, 2018]</a> uses the technique of <em>deep hashing</em> to hash images so that images of a similar class have hashes which are close in Hamming distance, while images of different classes have hashes which are far in edit distance.</p>



<p><strong>Deep Learning. </strong>The bulk of recent machine learning research on the automation of ECCs has been using <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> techniques; that is, training a constructed deep learning network to either encode–given a message as input, output a robust encoding–or decode–given a noisy transmission, recover the original message. As deep neural networks pass messages from earlier neurons to deeper neurons, the commonly referred to benchmark is that of <em>belief propagation</em>.</p>



<p>In brief (see also <a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>), the <a href="https://en.wikipedia.org/wiki/Belief_propagation">belief propagation</a> (BP) algorithm starts by taking the parity-check matrix defining a given linear code and converts it into a bipartite graph–one side of the vertices are the symbols of the code and the other vertices are the parity checks. Each vertex starts with a prior (i.e., probability that the vertex should be a 0 or 1) based on the received transmission, and then messages are passed back and forth. In odd-numbered rounds, the symbols give their confidence levels to the parity checks, which then update probabilities based on being satisfied or not. In even-numbered rounds the parity checks inform the symbols if they should change their prior to better satisfy the parity checks. After some number of rounds, a decoding is deduced.</p>



<p>Some of the earlier papers using deep learning to automate ECCs [<a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>, <a href="https://arxiv.org/pdf/1706.07043.pdf">2018</a>; <a href="https://arxiv.org/pdf/1702.06901.pdf">Crammerer, et.al., 2017</a>; <a href="https://arxiv.org/pdf/1701.07738.pdf">Gruber, et.al., 2017</a>] tried to generalize a belief propagation algorithm for decoding ECCs. At a high level, the works of Nachmani, et.al. “unroll” the belief propagation into a deep neural network with many layers. Unlike BP which has a fixed weight for the value of each message in each stage, they have trainable weights which allow one to obtain a lower BER than plain BP. Both Nachmani, et.al., and the works of Crammerer, et.al., and Gruber, et.al., consider more complex neural architectures which use BP as a subroutine. The latter two works show success in these methods for <a href="https://en.wikipedia.org/wiki/Polar_code_(coding_theory)">polar codes</a> and random linear codes. Note that all of these works are only training a decoder, as the encodings are fixed.</p>



<p>A later group of papers [<a href="https://arxiv.org/pdf/1807.00801.pdf">Kim, et.al. 2020</a>; <a href="https://arxiv.org/pdf/1903.02295.pdf">Jiang, et.al., 2019a</a>, <a href="https://arxiv.org/pdf/1911.03038.pdf">2019b</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9053254">2020</a>] (see also <a href="https://deepcomm.github.io/">their blog</a>) introduce what is known as DeepCode. Unlike previous work, the initial DeepCode paper [Kim, et.al., 2020] builds both a custom encoder and decoder. A particular tool which the authors utilized is that of <em>feedback</em>–after each bit is transmitted the noisy received bit is sent back (possibly with more noise added). Intuitively, feedback allows for the encoder to have an approximate understanding of what the decoder doesn’t know, allowing for on-the-fly adjustments to the encoding. The encoder sends the original message in plain-text and then incorporates the feedback with <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a> units to add two redundant bits per message bit (and thus the rate is one-third). The decoder incorporates the noisy plain text and these redundant bits using bidirectional-<a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a>s. Their methods can get a BER of as low as <img src="https://s0.wp.com/latex.php?latex=10%5E%7B-7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="10^{-7}" class="latex" /> for a block length of 50 bits.</p>



<p>Follow-up work [Jiang, et.al., 2019a, 2019b, 2020] consider variants of the model. In [Jiang, et.al., 2019a], the encoder is fixed to be a <a href="https://en.wikipedia.org/wiki/Turbo_code">turbo code</a> (a type of <a href="https://en.wikipedia.org/wiki/Convolutional_code">convolutional code</a>), a “standard” code in many applications, but the goal is to train a deep net to beat the <a href="https://en.wikipedia.org/wiki/BCJR_algorithm">standard decoder</a> in a “non-Gaussian noise” model. The work [Jiang, et.al., 2019b] tries to beat the turbo code at its own game by constructing a turbo code-like deep net where a commonly used finite automata is replaced by a deep net (and a corresponding decoder is constructed as well). Finally, the work [Jiang, et.al, 2020] generalized [Jiang, et.al., 2019b] by also using feedback, like in DeepCode.</p>



<p>Outside of deep learning, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> techniques have also been used to construct a decoder (e.g., <a href="https://arxiv.org/pdf/2009.09277.pdf">[Liao, et.al., 2020]</a>).</p>



<p><strong>Conclusion and Future Directions</strong>. Although the technology is still in its infancy, the automation of error correction codes is an exciting domain which could have a variety of applications in the future. As we have seen in this post, both formal methods and machine learning allow for powerful methods for obtaining automating various aspects of ECCs, whether it be constructing error correcting codes with provable guarantees or designing non-standard decodes which are robust to a variety of conditions. Looking ahead, there are a number of exciting directions which could be ripe for theoretical and practical contributions.</p>



<ol><li>A very active field of research currently is DNA storage–that is, synthesizing DNA molecules which encode data (rather than biological life). The primary theoretical challenge of DNA storage is that data loss is no longer a “bit flip,” rather nucleotides can be inserted, deleted, replicated, etc. Very recently, convolutional codes have proved successful in this setting <a href="https://www.biorxiv.org/content/10.1101/2019.12.20.871939v2.full">[Chandak, et.al., 2020]</a>, but an analogue to DeepCode does not currently exist in this setting. One barrier is that it is difficult for a fixed-architecture neural network to easily accommodate “index shifting” taking place during insertions and deletions. Automated techniques have been used to give lower bounds on how much redundancy is needed for such codes <a href="https://publish.illinois.edu/kiyavash/files/2015/06/Kulkarni_it_trans_2013.pdf">[Kulkarni, Kiyavash, 2013]</a>.</li><li>One potential theoretical contribution in this space is understanding how these automation techniques relate to <a href="https://en.wikipedia.org/wiki/Augmented_learning"><em>augmented learning</em></a>. An example of augmented learning is the problem of picking the “best” algorithm from a class of algorithms for a given problem. A theoretical framework for understanding such questions has recently been developed by <a href="https://theory.stanford.edu/~tim/papers/features.pdf">Gupta and Roughgarden</a>. Such a framework seems natural in the context of error-correcting codes, as there are many different kinds of codes, and even within one family of codes, such as Reed-Solomon codes, there are many parameter choices whose optimal values can vary significantly from application to application. </li><li>As a final thought, the use of “Formal Methods” in contrast to “Machine Learning” for automating ECCs has been largely separate. Recently, outside the context of ECCs, there have been a number of works (e.g., <a href="https://arxiv.org/pdf/1705.01320.pdf">[Ehlers, 2017]</a>, <a href="https://arxiv.org/abs/1811.01057">[Raghunathan, Steinhardt, Liang, 2018]</a>) which use formal methods to <em>provably verify</em> that a machine learning model such as a deepnet runs correctly under given conditions. It would be exciting if such methods could be extended to ECCs by showing that machine learning encoders/decoders like DeepCode can be utilized correctly. </li></ol>



<p>Are there other directions for which you would like to see the automation of ECCs extended? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. I would also like to thank Sivakanth Gopi and Sergey Yekhanin for insightful discussion on the relationship between automation of ECCs and DNA storage as well as Ofir Geri for discussion on augmented learning.</p></div>







<p class="date">
by Joshua Brakensiek <a href="https://theorydish.blog/2021/03/09/automated-design-of-error-correcting-codes-part-2/"><span class="datestr">at March 09, 2021 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/">Good-case Latency of Byzantine Broadcast: the Synchronous Case</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In our first post, we presented a summary of our good-case latency results for Byzantine broadcast (BB) and state machine replication (SMR), where the good case measures the latency to commit given that the broadcaster or leader is honest. In our second post, we discussed our results for partial synchrony,...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/"><span class="datestr">at March 09, 2021 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/034">TR21-034 |  Robust Self-Ordering versus Local Self-Ordering | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study two notions that refers to asymmetric graphs, which we view as graphs having a unique ordering that can be reconstructed by looking at an unlabeled version of the graph.

A {\em local self-ordering} procedure for a graph $G$ is given oracle access to an arbitrary isomorphic copy of $G$, denoted $G'$, and a vertex $v$ in $G'$, and is required to identify the name (or location) of $v$ in $G$, while making few (i.e., polylogarithmically many) queries to $G'$.
A graph $G=(V,E)$ is {\em robustly self-ordered} if the size of the symmetric difference between $E$ and the edge-set of the graph obtained by permuting $V$ using any permutation $\pi:V\to V$ is proportional to the number of non-fixed-points of $\pi$ and to the maximal degree of $G$; that is, any permutation of the vertices that displaces $t$ vertices must ``displace'' $\Omega(t\cdot d)$ edges, where $d$ is the maximal degree of the graph. 

We consider the relation between these two notions in two regimes: The bounded-degree graph regime, where oracle access to a graph means oracle access to its incidence function, and the dense graph regime, where oracle access to the graph means access to its adjacency predicate. 

We show that, {\em in the bounded-degree regime}, robustly self-ordering and local self-ordering are almost orthogonal; that is, even extremely strong versions of one notion do not imply very weak versions of the other notion. 
Specifically, we present very efficient local self-ordering procedures for graphs that possess derangements that are almost automorphisms (i.e., a single incidence is violated).  
One the other hand, we show robustly self-ordered graphs having no local self-ordering procedures even when allowing a number of queries that is a square root of the graph's size. 

{\em In the dense graph regime}, local self-ordering procedures are shown to yield a quantitatively weaker version of the robust self-ordering condition, in which the said proportion is off by a factor that is related to the query complexity of the local self-ordering procedure. Furthermore, we show that this quantitatively loss is inherent.
On the other hand, we show how to transform any robustly self-ordered graph 
into one having a local self-ordering procedure, while preserving the robustness condition. Combined with prior work, this yields explicit constructions of graphs that are both robustly and locally self-ordered, and an application to property testing.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/034"><span class="datestr">at March 09, 2021 09:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04543">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04543">Online Directed Spanners and Steiner Forests</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grigorescu:Elena.html">Elena Grigorescu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Young=San.html">Young-San Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quanrud:Kent.html">Kent Quanrud</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04543">PDF</a><br /><b>Abstract: </b>We present online algorithms for directed spanners and directed Steiner
forests. These problems fall under the unifying framework of online covering
linear programming formulations, developed by Buchbinder and Naor (MOR, 34,
2009), based on primal-dual techniques. Our results include the following:
</p>
<p>For the pairwise spanner problem, in which the pairs of vertices to be
spanned arrive online, we present an efficient randomized
$\tilde{O}(n^{4/5})$-competitive algorithm for graphs with general lengths,
where $n$ is the number of vertices of the given graph. For graphs with uniform
lengths, we give an efficient randomized
$\tilde{O}(n^{2/3+\epsilon})$-competitive algorithm, and an efficient
deterministic $\tilde{O}(k^{1/2+\epsilon})$-competitive algorithm, where $k$ is
the number of terminal pairs. These are the first online algorithms for
directed spanners.
</p>
<p>For the directed Steiner forest problem with uniform costs, in which the
pairs of vertices to be connected arrive online, we present an efficient
randomized $\tilde{O}(n^{2/3 + \epsilon})$-competitive algorithm. The
state-of-the-art online algorithm for general costs is due to Chakrabarty, Ene,
Krishnaswamy, and Panigrahi (SICOMP 2018) and is $\tilde{O}(k^{1/2 +
\epsilon})$-competitive.
</p>
<p>In the offline setting, the current best approximation ratio for pairwise
spanners with uniform lengths and directed Steiner forests with uniform costs
are both $\tilde{O}(n^{3/5 + \epsilon})$, due to Chlamtac, Dinitz, Kortsarz,
and Laekhanukit (TALG 2020).
</p>
<p>We observe that a small modification of the online covering framework by
Buchbinder and Naor implies a polynomial-time primal-dual approach with
separation oracles, which a priori might perform exponentially many calls to
the oracle. We convert the online spanner problem and the online Steiner forest
problem into online covering problems and round in a problem-specific fashion.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04543"><span class="datestr">at March 09, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04540">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04540">Sliding Window Persistence of Quasiperiodic Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gakhar:Hitesh.html">Hitesh Gakhar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perea:Jose_A=.html">Jose A. Perea</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04540">PDF</a><br /><b>Abstract: </b>A function is called quasiperiodic if its fundamental frequencies are
linearly independent over the rationals. With appropriate parameters, the
sliding window point clouds of such functions can be shown to be dense in tori
with dimension equal to the number of independent frequencies. In this paper,
we develop theoretical and computational techniques to study the persistent
homology of such sets. Specifically, we provide parameter optimization schemes
for sliding windows of quasiperiodic functions, and present theoretical lower
bounds on their Rips persistent homology. The latter leverages a recent
persistent K\"{u}nneth formula. The theory is illustrated via computational
examples and an application to dissonance detection in music audio samples.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04540"><span class="datestr">at March 09, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04519">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04519">Formal Verification of Authenticated, Append-Only Skip Lists in Agda: Extended Version</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miraldo:Victor_Cacciari.html">Victor Cacciari Miraldo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carr:Harold.html">Harold Carr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moir:Mark.html">Mark Moir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Lisandra.html">Lisandra Silva</a>, Guy L. Steele Jr <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04519">PDF</a><br /><b>Abstract: </b>Authenticated Append-Only Skiplists (AAOSLs) enable maintenance and querying
of an authenticated log (such as a blockchain) without requiring any single
party to store or verify the entire log, or to trust another party regarding
its contents. AAOSLs can help to enable efficient dynamic participation (e.g.,
in consensus) and reduce storage overhead.
</p>
<p>In this paper, we formalize an AAOSL originally described by Maniatis and
Baker, and prove its key correctness properties. Our model and proofs are
machine checked in Agda. Our proofs apply to a generalization of the original
construction and provide confidence that instances of this generalization can
be used in practice. Our formalization effort has also yielded some
simplifications and optimizations.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04519"><span class="datestr">at March 09, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04502">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04502">Quantum-accelerated constraint programming</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Booth:Kyle_E=_C=.html">Kyle E. C. Booth</a>, Bryan O'Gorman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marshall:Jeffrey.html">Jeffrey Marshall</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hadfield:Stuart.html">Stuart Hadfield</a>, Eleanor Rieffel <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04502">PDF</a><br /><b>Abstract: </b>Constraint programming (CP) is a paradigm used to model and solve constraint
satisfaction and combinatorial optimization problems. In CP, problems are
modeled with constraints that describe acceptable solutions and solved with
backtracking tree search augmented with logical inference. In this paper, we
show how quantum algorithms can accelerate CP, at both the levels of inference
and search. Leveraging existing quantum algorithms, we introduce a
quantum-accelerated filtering algorithm for the $\texttt{alldifferent}$ global
constraint and discuss its applicability to a broader family of global
constraints with similar structure. We propose frameworks for the integration
of quantum filtering algorithms within both classical and quantum backtracking
search schemes, including a novel hybrid classical-quantum backtracking search
method. This work suggests that CP is a promising candidate application for
early fault-tolerant quantum computers and beyond.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04502"><span class="datestr">at March 09, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04452">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04452">Strong Approximate Consensus Halving and the Borsuk-Ulam Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Eleni Batziou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hansen:Kristoffer_Arnsfelt.html">Kristoffer Arnsfelt Hansen</a>, Kasper Høgh <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04452">PDF</a><br /><b>Abstract: </b>In the consensus halving problem we are given n agents with valuations over
the interval $[0,1]$. The goal is to divide the interval into at most $n+1$
pieces (by placing at most n cuts), which may be combined to give a partition
of $[0,1]$ into two sets valued equally by all agents. The existence of a
solution may be established by the Borsuk-Ulam theorem. We consider the task of
computing an approximation of an exact solution of the consensus halving
problem, where the valuations are given by distribution functions computed by
algebraic circuits. Here approximation refers to computing a point that
$\varepsilon$-close to an exact solution, also called strong approximation. We
show that this task is polynomial time equivalent to computing an approximation
to an exact solution of the Borsuk-Ulam search problem defined by a continuous
function that is computed by an algebraic circuit.
</p>
<p>The Borsuk-Ulam search problem is the defining problem of the complexity
class BU. We introduce a new complexity class BBU to also capture an
alternative formulation of the Borsuk-Ulam theorem from a computational point
of view. We investigate their relationship and prove several structural results
for these classes as well as for the complexity class FIXP.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04452"><span class="datestr">at March 09, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04451">On the Termination of Some Biclique Operators on Multipartite Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crespelle:Christophe.html">Christophe Crespelle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Latapy:Matthieu.html">Matthieu Latapy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phan:Thi_Ha_Duong.html">Thi Ha Duong Phan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04451">PDF</a><br /><b>Abstract: </b>We define a new graph operator, called the weak-factor graph, which comes
from the context of complex network modelling. The weak-factor operator is
close to the well-known clique-graph operator but it rather operates in terms
of bicliques in a multipartite graph. We address the problem of the termination
of the series of graphs obtained by iteratively applying the weak-factor
operator starting from a given input graph. As for the clique-graph operator,
it turns out that some graphs give rise to series that do not terminate.
Therefore, we design a slight variation of the weak-factor operator, called
clean-factor, and prove that its associated series terminates for all input
graphs. In addition, we show that the multipartite graph on which the series
terminates has a very nice combinatorial structure: we exhibit a bijection
between its vertices and the chains of the inclusion order on the intersections
of the maximal cliques of the input graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04451"><span class="datestr">at March 09, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04447">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04447">Termination of Multipartite Graph Series Arising from Complex Network Modelling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Latapy:Matthieu.html">Matthieu Latapy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phan:Thi_Ha_Duong.html">Thi Ha Duong Phan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crespelle:Christophe.html">Christophe Crespelle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Thanh_Qui.html">Thanh Qui Nguyen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04447">PDF</a><br /><b>Abstract: </b>An intense activity is nowadays devoted to the definition of models capturing
the properties of complex networks. Among the most promising approaches, it has
been proposed to model these graphs via their clique incidence bipartite
graphs. However, this approach has, until now, severe limitations resulting
from its incapacity to reproduce a key property of this object: the overlapping
nature of cliques in complex networks. In order to get rid of these limitations
we propose to encode the structure of clique overlaps in a network thanks to a
process consisting in iteratively factorising the maximal bicliques between the
upper level and the other levels of a multipartite graph. We show that the most
natural definition of this factorising process leads to infinite series for
some instances. Our main result is to design a restriction of this process that
terminates for any arbitrary graph. Moreover, we show that the resulting
multipartite graph has remarkable combinatorial properties and is closely
related to another fundamental combinatorial object. Finally, we show that, in
practice, this multipartite graph is computationally tractable and has a size
that makes it suitable for complex network modelling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04447"><span class="datestr">at March 09, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04205">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04205">Matroid Secretary is Equivalent to Contention Resolution</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dughmi:Shaddin.html">Shaddin Dughmi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04205">PDF</a><br /><b>Abstract: </b>We show that the matroid secretary problem is equivalent to correlated
contention resolution in the online random-order model. Specifically, the
matroid secretary conjecture is true if and only if every matroid admits an
online random-order contention resolution scheme which, given an arbitrary
(possibly correlated) prior distribution over subsets of the ground set,
matches the balance ratio of the best offline scheme for that distribution up
to a constant. We refer to such a scheme as universal. Our result indicates
that the core challenge of the matroid secretary problem lies in resolving
contention for positively correlated inputs, in particular when the positive
correlation is benign in as much as offline contention resolution is concerned.
</p>
<p>Our result builds on our previous work which establishes one direction of
this equivalence, namely that the secretary conjecture implies universal
random-order contention resolution, as well as a weak converse, which derives a
matroid secretary algorithm from a random-order contention resolution scheme
with only partial knowledge of the distribution. It is this weak converse that
we strengthen in this paper: We show that universal random-order contention
resolution for matroids, in the usual setting of a fully known prior
distribution, suffices to resolve the matroid secretary conjecture in the
affirmative.
</p>
<p>Our proof is the composition of three reductions. First, we use duality
arguments to reduce the matroid secretary problem to the matroid prophet
secretary problem with arbitrarily correlated distributions. Second, we
introduce a generalization of contention resolution we term labeled contention
resolution, to which we reduce the correlated matroid prophet secretary
problem. Finally, we combine duplication of elements with limiting arguments to
reduce labeled contention resolution to classical contention resolution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04205"><span class="datestr">at March 09, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04183">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04183">HexDom: Polycube-Based Hexahedral-Dominant Mesh Generation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Yuxuan.html">Yuxuan Yu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Jialei_Ginny.html">Jialei Ginny Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yongjie_Jessica.html">Yongjie Jessica Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04183">PDF</a><br /><b>Abstract: </b>In this paper, we extend our earlier polycube-based all-hexahedral mesh
generation method to hexahedral-dominant mesh generation, and present the
HexDom software package. Given the boundary representation of a solid model,
HexDom creates a hex-dominant mesh by using a semi-automated polycube-based
mesh generation method. The resulting hexahedral dominant mesh includes
hexahedra, tetrahedra, and triangular prisms. By adding non-hexahedral
elements, we are able to generate better quality hexahedral elements than in
all-hexahedral meshes. We explain the underlying algorithms in four modules
including segmentation, polycube construction, hex-dominant mesh generation and
quality improvement, and use a rockerarm model to explain how to run the
software. We also apply our software to a number of other complex models to
test their robustness. The software package and all tested models are availabe
in github (https://github.com/CMU-CBML/HexDom).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04183"><span class="datestr">at March 09, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04049">New Separations Results for External Information</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Mark.html">Mark Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Minzer:Dor.html">Dor Minzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04049">PDF</a><br /><b>Abstract: </b>We obtain new separation results for the two-party external information
complexity of boolean functions. The external information complexity of a
function $f(x,y)$ is the minimum amount of information a two-party protocol
computing $f$ must reveal to an outside observer about the input. We obtain the
following results:
</p>
<p>1. We prove an exponential separation between external and internal
information complexity, which is the best possible; previously no separation
was known.
</p>
<p>2. We prove a near-quadratic separation between amortized zero-error
communication complexity and external information complexity for total
functions, disproving a conjecture of \cite{Bravermansurvey}.
</p>
<p>3. We prove a matching upper showing that our separation result is tight.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04049"><span class="datestr">at March 09, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.04046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.04046">Simplicial Complex Representation Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajij:Mustafa.html">Mustafa Hajij</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zamzmi:Ghada.html">Ghada Zamzmi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Xuanting.html">Xuanting Cai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.04046">PDF</a><br /><b>Abstract: </b>Simplicial complexes form an important class of topological spaces that are
frequently used to in many applications areas such as computer-aided design,
computer graphics, and simulation. The representation learning on graphs, which
are just 1-d simplicial complexes, has witnessed a great attention and success
in the past few years. Due to the additional complexity higher dimensional
simplicial hold, there has not been enough effort to extend representation
learning to these objects especially when it comes to learn entire-simplicial
complex representation. In this work, we propose a method for simplicial
complex-level representation learning that embeds a simplicial complex to a
universal embedding space in a way that complex-to-complex proximity is
preserved. Our method utilizes a simplex-level embedding induced by a
pre-trained simplicial autoencoder to learn an entire simplicial complex
representation. To the best of our knowledge, this work presents the first
method for learning simplicial complex-level representation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.04046"><span class="datestr">at March 09, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.03959">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.03959">Fine-Grained Complexity and Algorithms for the Schulze Voting Method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sornat:Krzysztof.html">Krzysztof Sornat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yinzhan.html">Yinzhan Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03959">PDF</a><br /><b>Abstract: </b>We study computational aspects of a well-known single-winner voting rule
called the Schulze method [Schulze, 2003] which is used broadly in practice. In
this method the voters give (weak) ordinal preference ballots which are used to
define the weighted majority graph (WMG) of direct comparisons between pairs of
candidates. The choice of the winner comes from indirect comparisons in the
graph, and more specifically from considering directed paths instead of direct
comparisons between candidates.
</p>
<p>When the input is the WMG, to our knowledge, the fastest algorithm for
computing all possible winners in the Schulze method uses a folklore reduction
to the All-Pairs Bottleneck Paths (APBP) problem and runs in $O(m^{2.69})$
time, where $m$ is the number of candidates. It is an interesting open question
whether this can be improved. Our first result is a combinatorial algorithm
with a nearly quadratic running time for computing all possible winners. If the
input to the possible winners problem is not the WMG but the preference
profile, then constructing the WMG is a bottleneck that increases the running
time significantly; in the special case when there are $O(m)$ voters and
candidates, the running time becomes $O(m^{2.69})$, or $O(m^{2.5})$ if there is
a nearly-linear time algorithm for multiplying dense square matrices. To
address this bottleneck, we prove a formal equivalence between the well-studied
Dominance Product problem and the problem of computing the WMG. We prove a
similar connection between the so called Dominating Pairs problem and the
problem of verifying whether a given candidate is a possible winner.
</p>
<p>Our paper is the first to bring fine-grained complexity into the field of
computational social choice. Using it we can identify voting protocols that are
unlikely to be practical for large numbers of candidates and/or voters, as
their complexity is likely, say at least cubic.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.03959"><span class="datestr">at March 09, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.03914">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.03914">Essentially Tight Kernels for (Weakly) Closed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koana:Tomohiro.html">Tomohiro Koana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sommer:Frank.html">Frank Sommer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03914">PDF</a><br /><b>Abstract: </b>We study kernelization of classic hard graph problems when the input graphs
fulfill triadic closure properties. More precisely, we consider the recently
introduced parameters closure number $c$ and the weak closure number $\gamma$
[Fox et al., SICOMP 2020] in addition to the standard parameter solution size
$k$. For Capacitated Vertex Cover, Connected Vertex Cover, and Induced Matching
we obtain the first kernels of size $k^{\mathcal{O}(\gamma)}$ and $(\gamma
k)^{\mathcal{O}(\gamma)}$, respectively, thus extending previous kernelization
results on degenerate graphs. The kernels are essentially tight, since these
problems are unlikely to admit kernels of size $k^{o(\gamma)}$ by previous
results on their kernelization complexity in degenerate graphs [Cygan et al.,
ACM TALG 2017]. In addition, we provide lower bounds for the kernelization of
Independent Set on graphs with constant closure number~$c$ and kernels for
Dominating Set on weakly closed split graphs and weakly closed bipartite
graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.03914"><span class="datestr">at March 09, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/03/08/more-mathematics-books">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html">More mathematics books by women</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A year ago, for International Women’s Day, I made <a href="https://11011110.github.io/blog/2020/03/08/mathematics-books-women.html">a list of mathematics books by women covered by then-new Wikipedia articles</a>. I thought it would be worthwhile to revisit the same topic and list several more mathematics books with at least one female author, at many different levels of audience, and again covered by new Wikipedia articles. They are (alphabetical by title):</p>

<ul>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Combinatorics_on_Partial_Words">Algorithmic Combinatorics on Partial Words</a></em> (2008), Francine Blanchet-Sadri. Partial words are strings with “don’t care” symbols; Blanchet-Sadri looks at the combinatorics of repeated patterns within these strings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Geometry">Algorithmic Geometry</a></em> (1995), Jean-Daniel Boissonnat and Mariette Yvinec. One of several standard computational geometry textbooks; this is the French one, but it has also been published in translation into English.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Puzzles">Algorithmic Puzzles</a></em> (2011), Anany and Maria Levitin. A nice collection of classic logic puzzles involving algorithmic thinking.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Braids,_Links,_and_Mapping_Class_Groups">Braids, Links, and Mapping Class Groups</a></em> (1975), Joan Birman. A classic research monograph on the topology of braid groups.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Code_of_the_Quipu">Code of the Quipu</a></em> (1981), Marcia and Robert Ascher. A general-audience book on how the Inca used knotted strings to record numbers and other information.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics:_The_Rota_Way">Combinatorics: The Rota Way</a></em> (2009), Joseph P. S. Kung, Catherine Yan, and (posthumously) Gian-Carlo Rota. A graduate textbook on algebraic combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics_of_Experimental_Design">Combinatorics of Experimental Design</a></em> (1987), Anne Penfold Street and her daughter Deborah Street. A textbook on the design of experiments, an area that crosses between statistics and combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Computability_in_Analysis_and_Physics">Computability in Analysis and Physics</a></em> (1989), Marian Pour-El and J. Ian Richards. A research monograph on problems involving differential equations including the wave equation whose initial conditions are continuous and computable, but that evolve to states whose values cannot be computed.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Diophantus_and_Diophantine_Equations">Diophantus and Diophantine Equations</a></em> (1972), Isabella Bashmakova. A somewhat idiosyncratic history based on the idea that Diophantus knew some very general techniques for finding rational-number solutions to equations, that can be inferred from the much more specific solutions to individual equations that have survived to us.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Elementary_Number_Theory,_Group_Theory_and_Ramanujan_Graphs">Elementary Number Theory, Group Theory, and Ramanujan Graphs</a></em> (2003), Giuliana Davidoff, Peter Sarnak, and Alain Valette. An attempt to make the construction of expander graphs accessible to undergraduate mathematics students.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Equivalents_of_the_Axiom_of_Choice">Equivalents of the Axiom of Choice</a></em> (1963, updated 1985), Herman and Jean Rubin. A large catalog of problems in mathematics whose solution is equivalent to the axiom of choice, from a time when the independence of choice from ZF set theory had not been proven.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Erd%C5%91s_on_Graphs">Erdős on Graphs: His Legacy of Unsolved Problems</a></em> (1998), 
Fan Chung and Ronald Graham. The open problems in graph theory from this book have been further collected and updated on a web site, <a href="http://www.math.ucsd.edu/~erdosproblems/">Erdős’s Problems on Graphs</a>, maintained by Chung.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Extensions_of_First_Order_Logic">Extensions of First Order Logic</a></em> (1996), María Manzano. Attempts to unify second-order logic, modal logic, and dynamic logic, by translating them all into many-sorted logic.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Fat_Chance:_Probability_from_0_to_1">Fat Chance: Probability from 0 to 1</a></em> (2019), Benedict Gross, Joe Harris, and Emily Riehl. A general-audience undergraduate textbook on probability theory based on a metaphor of games of chance.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Fractal_Dimension_of_Architecture">The Fractal Dimension of Architecture</a></em> (2016), Michael J. Ostwald and Josephine Vaughan. Studies the fractal dimension of floor plans as a way to model the changing demands on the complexity of housing structures and to classify buildings by architect and style.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Geometry_of_Numbers">The Geometry of Numbers</a></em> (2000), Carl D. Olds, Anneli Cahn Lax, and Giuliana Davidoff. A textbook on connections between number theory and integer grids, rescued twice from the posthumous works of its first two coauthors.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_History_of_Mathematical_Tables">The History of Mathematical Tables: from Sumer to Spreadsheets</a></em> (2003), Martin Campbell-Kelly, Mary Croarken, Raymond Flood, and Eleanor Robson. An edited volume with chapters on tables from many different periods in mathematical history.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Incidence_and_Symmetry_in_Design_and_Architecture">Incidence and Symmetry in Design and Architecture</a></em> (1983), Jenny Baglivo and Jack E. Graver. A textbook on graph theory and symmetry aimed at architecture students, also including interesting material on structural rigidity.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_the_Theory_of_Error-Correcting_Codes">Introduction to the Theory of Error-Correcting Codes</a></em> (1982, updated 1989 and 1998), Vera Pless. An advanced undergraduate textbook centered on algebraic constructions of linear block codes.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_3-Manifolds">Introduction to 3-Manifolds</a></em> (2014), Jennifer Schultens. An introductory graduate textbook on low-dimensional topology, leading up to the use of normal surfaces and Heegard splittings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Journey_into_Geometries">Journey into Geometries</a></em> (1991), Márta Svéd. A conversational Alice-in-wonderland-inspired tour of non-Euclidean geometry.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Knots_Unravelled">Knots Unravelled: From String to Mathematics</a></em> (2011), Meike Akveld and Andrew Jobbings. Knot theory for schoolchildren, centered on knot invariants.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Lectures_in_Geometric_Combinatorics">Lectures in Geometric Combinatorics</a></em> (2006), Rekha R. Thomas. An advanced undergraduate or introductory graduate textbook on the combinatorics of convex polytopes and their connections to abstract algebra through secondary polytopes and toric varieties.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Making_Mathematics_with_Needlework">Making Mathematics with Needlework: Ten Papers and Ten Projects</a></em> (2008), sarah-marie belcastro and Carolyn Yackel. The projects come from eight different contributors and include photos, instructions, mathematical analyses, and teaching activities.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematical_Excursions">Mathematical Excursions: Side Trips along Paths Not Generally Traveled in Elementary Courses in Mathematics</a></em> (1933), Helen Abbot Merrill. An early book on recreational mathematics, aimed at getting high school students interested in mathematics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematics_in_India">Mathematics in India: 500 BCE–1800 CE</a></em> (2009), Kim Plofker. Organized chronologically, this has become the standard overview of this large topic. It also includes material on the history of astronomy in India, which was often tied to the mathematics of its era.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Mathematics_of_Chip-Firing">The Mathematics of Chip-Firing</a></em> (2018), Caroline Klivans. A textbook on chip-firing games and abelian sandpile models.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Markov_Chains_and_Mixing_Times">Markov Chains and Mixing Times</a></em> (2009, 2017), David A. Levin and Yuval Peres, with contributions by Elizabeth Wilmer. A graduate-level text and research reference on how quickly random walks converge to their stable distributions.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mirrors_and_Reflections">Mirrors and Reflections: The Geometry of Finite Reflection Groups</a></em> (2009), Alexandre V. and Anna Borovik. An undergraduate textbook on the classification of finite reflection groups and their associated root systems.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Pioneering_Women_in_American_Mathematics">Pioneering Women in American Mathematics: The Pre-1940 PhD’s</a></em> (2009), Judy Green and Jeanne LaDuke. Biographical profiles of over 200 women who earned doctorates in mathematics in the US before 1940, with some background material on what it was like for women to work in mathematics in those times.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Playing_with_Infinity">Playing with Infinity: Mathematical Explorations and Excursions</a></em> (1955, translated into English 1961), Rózsa Péter. An attempt to explain the nature of mathematics and of the infinite in mathematics to non-mathematicians, based on a series of letters from Péter to a literary friend.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Point_Processes">Point Processes</a></em> (1980), David Cox and Valerie Isham. A research reference on processes that randomly place points on the real line or other geometric spaces.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Power_in_Numbers:_The_Rebel_Women_of_Mathematics">Power in Numbers: The Rebel Women of Mathematics</a></em> (2018), Talithia Williams. A selection of profiles of famous women mathematicians, aimed at motivating young women to become mathematicians.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Primality_Testing_for_Beginners">Primality Testing for Beginners</a></em> (2009, translated into English 2014), Lasse Rempe-Gillen and Rebecca Waldecker. An undergraduate text on primality testing algorithms, based on a course from a summer research program for undergraduates.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Quantum_Computing:_A_Gentle_Introduction">Quantum Computing: A Gentle Introduction</a></em> (2011), Eleanor Rieffel and Wolfgang Polak. One of many texts on this fast-moving subject.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Robust_Regression_and_Outlier_Detection">Robust Regression and Outlier Detection</a></em> (1987), Peter Rousseeuw and Annick M. Leroy. A monograph on statistical methods that can tolerate the total corruption of a large fraction of the data points that they analyze, and still produce meaningful results.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Two-Sided_Matching">Two-Sided Matching: A Study in Game-Theoretic Modeling and Analysis</a></em> (1990), Alvin E. Roth and Marilda Sotomayor. A survey of methods related to stable matching, aimed at economics practitioners and focused on applications.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/When_Topology_Meets_Chemistry">When Topology Meets Chemistry: A Topological Look At Molecular Chirality</a></em> (2000), Erica Flapan. Many biomolecules are different than their mirror images; classical examples include sugars, whose mirrored molecules may taste different and have different effects. This undergraduate-level text studies how to model this effect using a combination of graph theory and knot theory.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Women_in_Mathematics">Women in Mathematics</a></em> (1974), Lynn Osen. This is the one that based its coverage of Hypatia on an early-20th-century children’s book that gave her a made-up backstory and attributed made-up modern rationalist quotes to her. Not recommended, and included mainly as a warning not to use this as a reference.</p>
  </li>
</ul>

<p>To keep from ending on a sour note, I’ll add one more, that I found recently on Wikipedia (although the article there is very old) and I think is worthy of expansion: <em><a href="https://en.wikipedia.org/wiki/Logic_Made_Easy">Logic Made Easy: How to Know When Language Deceives You</a></em> (2004), Deborah J. Bennett, a popular-audience book on how to translate English phrases into logical formalisms and use that translation to understand more clearly what they mean.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105857580884627445">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html"><span class="datestr">at March 08, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/033">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/033">TR21-033 |  Automating Tree-Like Resolution in Time $n^{o(\log n)}$ Is ETH-Hard | 

	Susanna de Rezende</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that tree-like resolution is not automatable in time $n^{o(\log n)}$ unless ETH is false. This implies that, under ETH, the algorithm given by Beame and Pitassi (FOCS 1996) that automates tree-like resolution in time $n^{O(\log n)}$ is optimal. We also provide a simpler proof of the result of Alekhnovich and Razborov (FOCS 2001) that unless the fixed parameter hierarchy collapses, tree-like resolution is not automatable in polynomial time. The proof of our results builds on a joint work with Göös, Nordström, Pitassi, Robere and Sokolov (STOC 2021), which presents a simplification of the recent breakthrough of Atserias and Müller (FOCS 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/033"><span class="datestr">at March 08, 2021 05:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-61276742347305278">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html">When do I need to warn about Spoilers?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In a recent post <a href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html">here</a> I mentioned in passing a plot point from the last season of The Big Bang Theory. Note that the last season was in 2019.  WARNING- do not read that post if you are watching The Big Bang Theory and do not want a plot point revealed. </p><p>Someone who perhaps thinks Lance and I are the same person (are we? See <a href="https://blog.computationalcomplexity.org/2014/04/i-am-bill-gasarch.html">here</a>) left Lance a tweet complaining about the spoiler. At least I think they are complaining. The tweet is in Spanish and its <a href="https://twitter.com/deoxyt2/status/1366120364070338560">here</a>.</p><p>Either</p><p>1) Some country is two years behind America on showing The Big Bang Theory. </p><p>2) The person who tweeted has them on DVR (or something like that) and is watching them a few years after they air (I watched Firefly on a DVD I borrowed from a friend 10 years after it went off he air. Ask your grandparents what a DVD used to be.) </p><p>3) They are kidding us and making fun of the notion of spoilers.</p><p>This raises the question: When is it okay to post spoilers without warning? A few random thoughts:</p><p>1) ``Don't tell me who won the superb owl! I have it on tape and want to watch it without knowing who won!''  This always seemed odd to me.  Routing for events to happen that have already happened seems weird to me. When I was 10 years old I was in New York listening to a Knicks-Celtics Basketball game on the radio and during halftime I accidentally found a Boston radio station that had the game 30 minutes later (I did not realize that the channel I was on originally was 30 minutes behind). So I heard how the game ended, then switched back <i>listening to a game knowing how it would end. </i>I didn't route for my team (the Knicks, who lost) but it just felt very weird listening to it. If I had thought of it I might have noticed how the different broadcasts differ and got a paper out of the data, but as a 10 year old I was not thinking about how to pad my resume quite yet. </p><p>2) I like seeing a mystery twice- first time I don't know who did it, second time I do but can look for clues I missed the first time.</p><p>3) I would have thought 2 years after a show is off the air its fine to spoil. But... maybe not.</p><p>4) It also matters how important the plot point is. I didn't think the plot point I revealed was that important. </p><p>5) Many TV shows are predictable so I am not sure what `spoiler' even means. If I said to Darling:</p><p><i> The bad guy is an unimportant character we meet in the first 10 minutes.</i></p><p>that does not show I've seen it before. It shows that I am a master of TV-logic.</p><p>6) With Arc TV shows this is more of a problem. While it was possible to spoil an episode (Captain Kir will survive but Ensign Red Shirt will bite the dust) it was impossible to spoil a long-term arc. TV has gotten to complicated. And I say that without having watched Game of Thrones. </p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html"><span class="datestr">at March 08, 2021 02:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5371">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5371">Another axe swung at the Sycamore</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>So there’s an interesting new paper on the arXiv by Feng Pan and Pan Zhang, entitled <a href="https://arxiv.org/abs/2103.03074">“Simulating the Sycamore supremacy circuits.”</a>  It’s about a new tensor contraction strategy for classically simulating Google’s 53-qubit quantum supremacy experiment from Fall 2019.  Using their approach, and using just 60 GPUs running for a few days, the authors say they managed to generate a million <em>correlated</em> 53-bit strings—meaning, strings that all agree on a specific subset of 20 or so bits—that achieve a high linear cross-entropy score.</p>



<p>Alas, I haven’t had time this weekend to write a “proper” blog post about this, but several people have by now emailed to ask my opinion, so I thought I’d share the brief response I sent to a journalist.</p>



<p>This does look like a significant advance on simulating Sycamore-like random quantum circuits!  Since it’s based on tensor networks, you don’t need the literally largest supercomputer on the planet filling up tens of petabytes of hard disk space with amplitudes, as in the brute-force strategy <a href="https://arxiv.org/abs/1910.09534">proposed by IBM</a>.  Pan and Zhang’s strategy seems most similar to the strategy previously <a href="https://arxiv.org/pdf/2005.06787.pdf">proposed by Alibaba</a>, with the key difference being that the new approach generates millions of correlated samples rather than just one.</p>



<p>I guess my main thoughts for now are:</p>



<ol><li>Once you knew about this particular attack, you could evade it and get back to where we were before by switching to a more sophisticated verification test — namely, one where you not only computed a Linear XEB score for the observed samples, you <em>also</em> made sure that the samples didn’t share too many bits in common.  (Strangely, though, the paper never mentions this point.)</li><li>The other response, of course, would just be to redo random circuit sampling with a slightly bigger quantum computer, like the ~70-qubit devices that Google, IBM, and others are now building!</li></ol>



<p>Anyway, very happy for thoughts from anyone who knows more.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5371"><span class="datestr">at March 07, 2021 07:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18263">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/07/advancing-and-counting/">Advancing and Counting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Announcing tomorrow’s Women in Data Science workshop (global start tonight 8pm ET), plus a US State Department event for International Women’s Day (also March 8)</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/dr.png"><img width="145" alt="" src="https://rjlipton.files.wordpress.com/2021/03/dr.png?w=145&amp;h=175" class="alignright wp-image-18265" height="175" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Santa Fe Inst. external faculty <a href="https://www.scs.gatech.edu/news/610412/dana-randall-named-external-faculty-santa-fe-institute">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Dana Randall is an ADVANCE Professor of Computing and also is an Adjunct Professor of Mathematics at the Georgia Institute of Technology. She is a terrific <a href="https://www.youtube.com/watch?v=MhYpfBUjQFQ">speaker</a> and <a href="https://www.ratemyprofessors.com/ShowRatings.jsp?tid=1579140">teacher</a> and leader. Her class ratings are off the charts. See also <a href="http://www.ams.org/publicoutreach/students/mathgame/arl2009">AMS</a> for her past special talks.</p>
<p>
Today I thought we would discuss her research and its connections to complexity theory and to physics and to math in general.</p>
<p>
Dana does research into the boundary between math and physics. At the highest level Dana seeks to understand random processes, especially those connected to physical systems. The difficulty, in my opinion, is that sometimes the random system is not artificial. This means that we have no control over the system, and this makes the analysis of its behavior that much harder. </p>
<p>
Another way to say this is: we often fare better when we can control the exact random process. When someone else gets to decide on what the process is, we are often in trouble. The system might behave badly, or even worse, might be hard to understand. Nature is often that way—not always thinking about making the analysis of a system easy.</p>
<p>
</p><h2> Shuffling </h2><p></p>
<p>
Let’s make this concrete. Suppose that you or Dana were presented with three methods for shuffling a deck of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> cards—when we play cards <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{52}" class="latex" />. Imagine the methods are: </p>
<ol>
<li>This method selects <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> numbers in the range <img src="https://s0.wp.com/latex.php?latex=%7B%5B1%2Cn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{[1,n]}" class="latex" /> and checks for repeats. If there are, then try again. Use the permutation to shuffle.
</li><li>This method takes a deck of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> numbers and repeatedly shuffles them.
</li><li>This method executes the following code:
</li></ol>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/qscode.png"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2021/03/qscode.png?w=550&amp;h=100" class="aligncenter wp-image-18266" height="100" /></a></p>
<p>The task of understanding these methods is before us. The first (1) is slow, even for modest size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. The chance of getting a permutation on a given trial is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bn%21%7D%7Bn%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{n!}{n^n}}" class="latex" />, which for <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n= 52}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B4.7257911+%5Ctimes+10%5E%7B-22%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{4.7257911 \times 10^{-22}}" class="latex" />, which equals </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0.00000000000000000000047257911.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  0.00000000000000000000047257911. " class="latex" /></p>
<p>But it does generate a fair shuffle—all possible ones are equally likely. And the proof of this is easy. The second (2) is more complicated. The final shuffle depends on the number and manner we use to shuffle the deck. The final analysis is messy.</p>
<p>
The third one (3) is due to Ronald Fisher and Frank Yates, who discovered it in 1938. It has an elegant, but nontrivial, analysis. It is both exact in that all orderings are equally likely, and it takes time linear in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. The <a href="https://en.wikipedia.org/wiki/Fisher-Yates_shuffle">history</a> of it is:</p>
<blockquote><p><b> </b> <em> The modern version of the Fisher-Yates shuffle, designed for computer use, was introduced by Richard Durstenfeld in 1964 and popularized by Donald Knuth in The Art of Computer Programming as “Algorithm P (Shuffling)” […apparently unawares; Fisher and Yates were acknowledged in later editions of Knuth’s text…]</em>
</p></blockquote>
<p>
My guess is that Dana, if given these methods, would not be interested in (1): too slow on one hand and trivial on the other. Nor interested in (2): not elegant and messy. Perhaps (3) would accord with her work: it has a nice analysis and runs in linear time. </p>
<p>
</p><h2> Advancing </h2><p></p>
<p>
As we said earlier, Dana is part of <a href="http://www.advance.gatech.edu/team/gt-advance-professors">ADVANCE</a> at Georgia Tech: </p>
<blockquote><p><b> </b> <em> Georgia Tech’s ADVANCE Program seeks to develop systemic and institutional approaches that increase the representation, full participation, and advancement of women and minorities in academic STEM careers—thus contributing to a more diverse workforce, locally and nationally. </em>
</p></blockquote>
<p>
Dana has been and is a leader in helping advance these goals. It is especially relevant since this Monday, March 8th is special. It is <i>Celebrate International Women’s Day with WiDS</i>. See <a href="https://www.widsconference.org/conference.html">this</a> for details:</p>
<blockquote><p><b> </b> <em> Join us for the 24-hour virtual WiDS Worldwide Conference. We’ll follow the sun, bringing you speakers from around the world on International Women’s Day beginning at 1:00 am GMT March 8 (5:00 pm PST March 7). </em>
</p></blockquote>
<p>
In making a collage of their speaker <a href="https://www.widsconference.org/speakers.html">page</a>, we have compressed and rearranged it somewhat. And we have added the logo for the <a href="https://www.widsconference.org/regional-events-2021.html">regional events</a> happening around the globe (some already past) and one for their sponsors.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png?w=600&amp;h=590" class="aligncenter size-large wp-image-18268" height="590" /></a></p>
<p></p><p><br />
If you could not take time to follow all the talks, you might take a few for a sample. Maybe you would figure that taking the first four speakers in alphabetical order, or the last four, would be as random a sample as any—after all, what’s in a name?  Well, if you took the last four, you would actually get three of the four <a href="https://www.widsconference.org/conference.html">keynote</a> speakers. Sometimes procedures that we hope would give “random” samples in fact give special ones. That takes us back to one more topic in Randall’s work.</p>
<p>
</p><h2> Counting </h2><p></p>
<p>
One benefit of a random process is that under good conditions it can give us an accurate small sampling of a large and complex system. We have <a href="https://rjlipton.wordpress.com/2016/08/14/a-surprise-for-big-data-analytics/">mentioned</a> dimension reduction in this context. A simpler task is just to get an approximate count of entities in the system. Sometimes one can control the system, but sometimes not.</p>
<p>
One success is represented by a <a href="https://www.sciencedirect.com/science/article/pii/S0166218X15002255">paper</a> with Sarah Miracle of the University of St. Thomas. It is about counting colorings in multigraphs <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G = (V,E)}" class="latex" /> that do not violate simple constraints on the edges <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(u,v)}" class="latex" />. Each edge has a forbidden pair <img src="https://s0.wp.com/latex.php?latex=%7B%28c%2Cc%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(c,c')}" class="latex" /> of colors, and a coloring <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi}" class="latex" /> defined on <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V}" class="latex" /> is legal provided it does not have both <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28u%29+%3D+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi(u) = c}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28v%29+%3D+c%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\chi(v) = c'}" class="latex" />. Multiple edges between <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> can enforce multiple such constraints. Several natural problems can be represented via this one. The paper is one where their Markov Chain methods do well.</p>
<p>
A second recent <a href="https://arxiv.org/pdf/1611.03385.pdf">paper</a> uses Markov chains to count elements of a given rank in finite partially ordered sets. The chains should be biased according to the structure of the Hasse diagram of the poset. The trick in the paper is a way to balance the bias so as to prevent states that need to be counted from have too low frequency. This enables a direct analysis of the mixing time. The notable application was the first provably efficient ways to sample uniformly from certain kinds of partitions of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> quite large. </p>
<p>
The flip side—a phase change being a kind of a flip—is represented by other work with myriad collaborators that is summarized in a wonderful <a href="http://dimacs.rutgers.edu/events/details?eID=1409">talk</a> she gave at a <a href="http://dimacs.rutgers.edu/events/details?eID=1226&amp;loc=1409#1409">workshop</a> marking the 30th anniversary of DIMACS. As with an earlier <a href="https://drops.dagstuhl.de/opus/volltexte/2017/8021/pdf/LIPIcs-DISC-2017-3.pdf">version</a> at a Schloss Dagstuhl workshop, the talk was titled, “Phase Transitions and Emergent Phenomena in Algorithms and Applications”: </p>
<blockquote><p><b> </b> <em> Markov chain Monte Carlo methods have become ubiquitous across science and engineering as a means of exploring large configuration spaces. The idea is to walk among the configurations so that even though you explore a very small part of the space, samples will be drawn from a desirable distribution. Over the last 30 years there have been tremendous advances in the design and analysis of efficient sampling algorithms for this purpose, largely building on insights from statistical physics. One of the striking discoveries has been the realization that many natural Markov chains undergo a phase transition where they change from being efficient to inefficient as some parameter of the system is varied. </em>
</p></blockquote>
<p>
Here are a <a href="https://www.youtube.com/watch?v=IYkikVLkpoU">video</a> and <a href="http://dimacs.rutgers.edu/tools/fileman/Uploads/Documents/DIMACS-30/Randall_DIMACS30.pdf">slides</a>. The first main slide is about <em>programmable active matter</em> and it interests me especially to see DNA computing included. These systems can have <em>emergent behavior</em>, and while that can ruin randomized procedures that would bank on the system staying stable, it opens other opportunities. </p>
<p>
I, Dick, have run into this type of issue before. I have been on the wrong side, with theorems that were weak because we assumed the process could not be changed. Others who followed us changed the process—got stronger results with easier proofs. Is there a name for this?</p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>
Take a look at the <a href="https://www.widsconference.org/conference.html">talks</a> for the WiDS Worldwide Conference this Monday. </p>
<p>
Ken also notes another event happening tomorrow: a 10am <a href="https://www.state.gov/2021-international-women-of-courage-award-recipients-announced/">ceremony</a> for the International Women of Courage Award. His friend the Iranian chess arbiter Shohreh Bayat is among the honorees, after a story told in her own words <a href="https://www.washingtonpost.com/opinions/i-loosened-my-hijab-at-a-chess-championship-now-im-afraid-to-return-to-iran/2020/02/17/1a670f66-5194-11ea-9e47-59804be1dcfb_story.html">here</a> in the Washington Post. Ken was working with her, on statistical assurance against cheating in the championship match, at the time. The ceremony starts at 10am ET hosted by the US Department of State, with opening remarks by Dr. Jill Biden.</p>
<p>
We must add that there is something that is hard about the type of random processes that Dana studies. We tried to explain what makes her work deep, but perhaps we did not properly explain it. </p>
<p></p><p><br />
[added global start time of workshop to subtitle]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/03/07/advancing-and-counting/"><span class="datestr">at March 07, 2021 05:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=5711">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/self-concordant-analysis-for-logistic-regression/">Going beyond least-squares – II : Self-concordant analysis for logistic regression</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text"><a href="https://francisbach.com/self-concordant-analysis-newton/">Last month</a>, we saw that self-concordance is a key property in optimization, to use local quadratic approximations in the sharpest possible way. In particular it was an affine-invariant quantity leading to a simple and elegant analysis of Newton method. The key assumption was a link between third and second-order derivatives, which took the following form for one-dimensional functions, $$|f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ Alas, some of the most classical smooth functions appearing in machine learning are not self-concordant with this particular link between derivatives. The main example is the logistic loss, which is widely used across machine learning.</p>



<p class="justify-text">Indeed, if we take this logistic loss function \(f(x) = \log ( 1 + \exp(-x))\), it satisfies $$ f^\prime(x) =  \frac{ -\exp(-x)}{1+\exp(-x)} =\  – \frac{1}{1+\exp(x)} = \ – \sigma(-x),$$ where \(\sigma\) is the usual <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> defined as \(\sigma(x) = \frac{1}{1+\exp(-x)}\), and which is increasing from \(0\) to \(1\). We then have \(f^{\prime\prime}(x) = \sigma(x) ( 1- \sigma(x) )\) and \(f^{\prime \prime \prime}(x) = \sigma(x) ( 1- \sigma(x) )( 1 – 2 \sigma(x) )\) leading to $$|f^{\prime\prime\prime}(x)| \leqslant f^{\prime\prime}(x).$$ See below for plots of the logistic loss (left) and its derivatives (right).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="639" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/losses_logistic.png" class="wp-image-5795" height="267" /></figure></div>



<p class="justify-text">There is thus a link between third and second-order derivatives, but <em>without the power \(3/2\)</em>. Does this difference really matter? In this post, I will show how some properties from classical self-concordance can be extended to this slightly different notion. We will then present applications to stochastic gradient descent as well as the statistical analysis of generalized linear models, and in particular logistic regression.</p>



<p class="justify-text">I will describe applications to Newton method for large-scale logistic regression [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>] in later posts (you read well: Newton method for large-scale machine learning can be useful, in particular for severely ill-conditioned problems).</p>



<h2>\(\nu\)-self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said \(\nu\)-self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and there exists \(R &gt; 0\), such that $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant R f^{\prime\prime}(x)^{\nu\: \!  /2}.$$ </p>



<p class="justify-text">Note the difference with classical self-concordance (which corresponds to \(\nu=3\) and \(R=2\)). All positive powers are possible (see [<a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">1</a>]), but we will focus primarily on \(\nu=2\), for which most of the properties below were derived in [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>].</p>



<p class="justify-text">Note that the definition above in one dimension is still “affine-invariant” if the constant \(R\) is allowed to change (that is, if it is true for \(f\), it is true for \(x \mapsto f(ax)\) for any \(a\)). However, unless \(\nu = 3\), this will not be true in higher dimension, and therefore, the analysis of Newton method will be more complicated.</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), we need the same property along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime\prime}]= \sum_{i,j,k=1}^d h_i h_j^\prime h^{\prime\prime}_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the third-order tensor (with three different arguments, as needed below) and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the symmetric second-order one, then there exists \(R\) such that $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime}]| \leqslant R \| h\| \cdot f^{\prime\prime}(x)[h^\prime,h^{\prime}],$$ where \(\| h\|\) is the standard Euclidean norm of \(h\). Note here the difference with classical self-concordance where we could consider the symmetric third-order tensor (that is, no need for \(h^\prime\) and \(h^{\prime\prime}\)), and only the Euclidean norm based on the Hessian \(f^{\prime\prime}(x)\) was used.</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are \(2\)-self-concordant, then so is their average \(\frac{1}{2} ( f+g ) \) with the same constant \(R\) (this is one key advantage over \(3\)-self-concordance). Moreover, if \(f\) is \(2\)-self-concordant with constant \(R\), then \(g(x) = f(Ax)\) is also \(2\)-self concordant, with constant \(R \| A\|_{\rm op}\).</p>



<p class="justify-text">Classical examples are all linear and quadratic functions (with constant \(R = 0\)), the exponential function and the logistic loss \(f(x) = \log(1+\exp(-x))\), both with constant \(R=1\). This extends to the “log-sum-exp” function \(f(x) = \log\big( \sum_{i=1}^d \exp(x_i)\big)\), which is \(2\)-self-concordant with constant \(R = \sqrt{2}\). More generally, as shown at the end of the post, any log-partition function of the form $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big) $$ arising from <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear models</a> with bounded features, will be \(2\)-self-concordant, with constant the diameter of the set of features. Thus, self-concordance applies to all generalized linear models with the canonical link function. This includes <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">softmax regression</a> (for multiple classses), <a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random fields</a>, and of course logistic regression which I will focus on below.</p>



<p class="justify-text"><strong>Logistic regression.</strong> The most classical example is thus logistic regression, with $$f(x) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp( – x^\top a_i b_i ) ),$$ for observations \((a_i,b_i) \in \mathbb{R}^d \times \{-1,1\}\). See an example below in \(d=2\) dimensions.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img width="660" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/video_log_reg.gif" class="wp-image-5814" height="261" />Logistic regression in two dimensions: data space with \(a_i \in \mathbb{R}^2\) represented with a different color/mark depending on the label \(b_i\) (left), parameter space (right) with level sets of the objective function \(f\) and its minimizer (purple asterisk).</figure></div>



<p class="justify-text"><strong>Properties in one dimension.</strong>  Mimicking what was done <a href="https://francisbach.com/self-concordant-analysis-newton/">last month</a>, a nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( \! \log( f^{\prime\prime}(x)) \big) \big| = \big|   f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-1} \big| \leqslant R,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ – Rx \leqslant \log( f^{\prime\prime}(x))  \, – \log(f^{\prime\prime}(0)) \leqslant Rx,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3}  f^{\prime\prime}(0) \exp(\  – R x ) \leqslant f^{\prime\prime}(x) \leqslant f^{\prime\prime}(0) \exp( R x ).$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$  f^{\prime\prime}(0) \frac{1-\exp( \ – R x )}{R} \leqslant f^\prime(x)-f^\prime(0) \leqslant f^{\prime\prime}(0) \frac{\exp( R x )\  – 1}{R},$$ and  $$ \tag{4} \!\!\!\!\!\! f^{\prime\prime}(0) \frac{\exp( \ – R x ) + Rx \ – 1}{R^2}\leqslant f(x) \ – f(0) \ – f^\prime(0) x \leqslant  f^{\prime\prime}(0) \frac{\exp( R x ) \ – Rx \ – 1}{R^2}.$$ We thus get a bound $$f(x) \ – f(0) \ – f^\prime(0) x \in f^{\prime\prime}(0) \frac{x^2}{2} \cdot [ \rho(-Rx), \rho(Rx) ],$$ with \(\displaystyle \rho(u) =\ \frac{\exp( u ) \ – u\  – 1}{u^2 / 2 } \sim 1 \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. These upper and lower Taylor expansions are illustrated below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="292" alt="" src="https://francisbach.com/wp-content/uploads/2021/03/rho_log.png" class="wp-image-5792" height="227" /></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(\Delta \in \mathbb{R}^d\), we have upper and lower bounds for the Hessian, the gradient (not presented below) and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm $$\tag{5}\exp(\ – R\|\Delta\|) f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq \exp( R\|\Delta\|)  f^{\prime \prime}(x),$$ and $$\tag{6} \!\!\!\!\!\!\!\!\! \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(-R \|\Delta\|_2) \leqslant f(x+\Delta)\ -f(x) \ – f^\prime(x)^\top \Delta \leqslant \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(R \|\Delta\|_2).\! $$ These approximations are “second-order tight” at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact. These can be derived by considering \(g(t) = f(x+ t\Delta)\), which is \(2\)-self-concordant with constant \(R \|\Delta\|_2\), and applying the one-dimensional properties above in Eqs. (2) and (3) between \(0\) and \(1\).</p>



<p class="justify-text"><strong>Avoiding exponentially decaying constants.</strong> In this post, I will focus primarily on the use self-concordant functions in optimization (stochastic gradient descent in this post and Newton method in another post) as well as in statistics.</p>



<p class="justify-text">The main benefit of using self-concordance is to avoid exponential constants traditionally associated with the analysis of logistic regression. Indeed, for the logistic loss, the second-derivative at \(x\) is is equal to \(\sigma(x) ( 1 – \sigma(x) )\) and is equivalent to \(\exp(-|x|)\) when  \(| x| \) is large. Thus, if we are willing to only apply the logistic loss to small values of \(|x|\), let’s say less than \(M\), then the logistic loss is strongly-convex with constant greater than \(\exp(-M)\). Therefore, we can apply many results in optimization and statistics that apply to such losses. However, all of these results will be impacted by the constant \(\exp(-M)\), which is strictly positive but can be very small. With self-concordance, the analysis will get rid of these annoying constants and replace them by eigenvalues of Hessian matrices at the optimum, which are typically much larger (note that in the worst case, the exponential constants are unavoidable [<a href="http://proceedings.mlr.press/v35/hazan14a.pdf">3</a>]).</p>



<h2>Adaptivity of stochastic gradient descent</h2>



<p class="justify-text">I have not written about stochastic gradient descent for quite a while. Self-concordance gives me the occasion to talk about <em>adaptivity</em>.</p>



<p class="justify-text">It is well known that for smooth convex functions, gradient descent will converge exponentially fast if the function is also strongly-convex (essentially all eigenvalues of all Hessians being strictly positive). If the problem is ill-conditioned, then the exponential convergence rate turns into a rate of \(O(1/t)\) where \(t\) is the number of iterations. Gradient descent is <em>adaptive</em> as the exact same algorithm (with constant step-size or line-search) can be applied without the need to know the strong-convexity parameter. Moreover, if locally around the global optimum, the Hessians are better conditioned, gradient descent will also benefit from it. Therefore, gradient descent is great! What about stochastic gradient descent (SGD)?</p>



<p class="justify-text">It turns out that similar adaptivity exists for a well-defined version of SGD, and that self-concordance is one way to achieve simple non-asymptotic bounds (asymptotic bounds exist more generally [4]).</p>



<p class="justify-text"><strong>Logistic regression. </strong>We consider the logistic regression problem where we aim to minimize the expectation $$f(x) = \mathbb{E}_{a,b} \log( 1 + \exp(-b a^\top x) ) = \mathbb{E}_{a,b} g(x|a,b) ,$$ where \((a,b) \in \mathbb{R}^d \times \{-1,1\}\) is a pair of input \(a\) and output \(b\) (hopefully, the machine learning police will excuse my use of \(x\) as the parameter and not the input). We are given \(n\) independent and identically distributed observations \((a_1,b_1),\dots, (a_n,b_n)\) and we aim at finding the minimizer \(x_\ast\) of \(f\) (which is the logistic loss on unseen data), which we assume to exist. We assume that the feature norms \(\|a\|\) are almost surely bounded by \(R\).</p>



<p>Note here that we are not trying to minimize the empirical risk and by using a single pass, we obtain bounds on the generalization performance. This is one of the classical benefits of SGD.</p>



<p class="justify-text"><strong>Averaged stochastic gradient descent.</strong> We consider the stochastic gradient recursion: $$x_i = x_{i-1} – \gamma_i g^\prime(x_{i-1}|a_i,b_i),$$ for \(i=1,\dots,n\), with a single pass over the data. We also consider the average iterate \(\bar{x}_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i\). </p>



<p class="justify-text">Standard results from the stochastic gradient descent literature [<a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>, <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">6</a>] show that if \(\gamma_i = \frac{1}{R^2 \sqrt{i}}\), then, up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i)\  – f(x_\ast) \leqslant (1 + R^2 \| x_0 – x_\ast\|^2) \frac{\log n}{\sqrt{n}}.$$ If in addition, the function \(f\) is \(\mu\)-strongly-convex, then with the step-size \(\gamma_i = \frac{1}{\mu i}\), up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i) \ – f(x_\ast) \leqslant   \frac{R^2 \log n}{n\mu}.$$ The strongly convex result seems beneficial as we get a rate in \(O(( \log n) / n)\) instead of \(O(( \log n) / \sqrt{n})\), <em>but</em>, (1) it depends on \(\mu\), which can be very small in problems in high dimension \(d\), (2) it depends on this global strong-convexity constant \(\mu\), that is a lower bound on all Hessians, which is zero for logistic regression unless a projection step is used (and with exponentially small constant as explained above), and (3) the step-size has to be adapted. </p>



<p class="justify-text"><strong>Adaptivity. </strong>Wouldn’t it be great if these three problems could be solved at once? This is what I worked on a few years ago [<a href="http://">7</a>], where I showed that for constant step-size \(\gamma\) proportional to \(\frac{1}{R^2 \sqrt{n}}\) (thus dependent on the total number of gradient steps), we have. up to constants: $$ \mathbb{E} f(\bar{x}_i)\ – f(x_\ast) \leqslant (1 + R^2 \| x_0 – x_\ast\|^2) \frac{1}{\sqrt{n}},$$ <em>and</em> $$ \mathbb{E} f(\bar{x}_i)\ – f(x_\ast) \leqslant (1 + R^4 \| x_0 – x_\ast\|^4) \frac{R^2 }{\mu_\ast n},$$ where \(\mu_\ast\) is the smallest eigenvalue of the Hessian \(f^{\prime\prime}(x_\ast)\) <em>at the optimum</em>. The two bounds are always satisfied and one can be bigger than the other depending on \(n\) and the condition number \(R^2 / \mu_\ast\).</p>



<p class="justify-text">We thus get (almost) the best of all worlds! The proof relies strongly on self-concordance and applies to all generalized linear models. Note that (a) no new algorithm is proposed here, I am simply providing partial theoretical justifications why a classical algorithm works so well, (b) this is <em>only an upper-bound</em> on performance (more on this below).</p>



<h2>Generalization bounds for generalized linear models</h2>



<p class="justify-text">Beyond optimization, the use of self-concordant can make the non-asymptotic <em>statistical</em> analysis of logistic regression, and more generally all generalized linear models, sharper in the regularized unregularized setting [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], with \(\ell_1\)-norm [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], or with non-parametric kernel-based models [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>]. The first benefit is to avoid exponential constants associated with usual strong-convexity arguments of the loss (which can also be achieved with other tools, see [<a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">11</a>]). But there is another important benefit that requires some digression.</p>



<p class="justify-text"><strong>Asymptotic statistics is great…</strong> Supervised learning through empirical risk minimization is the workhorse of machine learning. It can be analyzed from different perspectives and with different tools. As shown in the great book by Aad Van der Vaart [12], asymptotics statistics is a very clean way of understanding the behavior of statistical estimators when the number of observations \(n\) goes to infinity. </p>



<p class="justify-text">Empirical risk minimization is indeed an example of M-estimation problems (estimators based on minimizing the empirical average of some loss functions), and it is known that under general conditions, the estimator has a known asymptotic mean and variance (with the traditional <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrices</a>), which leads to an asymptotic equivalent of the unseen population risk (e.g., the “test error”). We recover the usual \(d/n\) bound for unregularized problems as well as dimension-independent results when using regularization with squared Euclidean norms (then with worse dependence in \(n\)).</p>



<p class="justify-text">Because we deal with <em>limits</em>, one can formally compare two methods by favoring the one with the smallest asymptotic risk. This is not possible when non-asymptotic <em>upper bounds</em> are available: the fact that they are true for all \(n\) is a strong benefit, but since they are only bounds, they don’t say anything about which method is best.</p>



<p class="justify-text"><strong>… But it is only asymptotic.</strong> Of course, these comparisons are only true in the limit of large \(n\), and, in particular for high-dimensional problems (e.g., data and/or parameters in large dimensions), we are unlikely to be in the asymptotic regime. So we cannot really rely only on letting \(n\) go to infinity. Moreover, these asymptotic limits typically depend on some information which is not available at training time. But does this mean that we have to throw away all asymptotic results?</p>



<p class="justify-text"><strong>Self-concordance to the rescue.</strong> Since many of the asymptotic results are obtained by second-order Taylor expansions, we need non-asymptotic ways of dealing with these expansions, which is exactly what self-concordance allows you to do (with some extra effort of course). Therefore the best of both worlds can be achieved with such tools; see, e.g., [<a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], for examples of analysis, and [<a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">13</a>, <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">14</a>] for other tools that can achieve similar results. It is then possible to prove that some asymptotic expansions are valid non-asymptotically.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post I focused on two aspects of self-concordant analysis for logistic regression and its extensions, namely adaptivity of stochastic gradient descent and statistical generalization bounds.</p>



<p class="justify-text">In a later post, I will go back to Newton’s method, where the lack of affine invariance of \(2\)-self-concordance makes the analysis more complicated. However it will come with some interesting benefits for large-scale severely ill-conditioned problems [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>]. You may wonder why we should bother with Newton method for large-scale machine learning when stochastic gradient descent, with or without variance reduction, seems largely enough. Stay tuned!</p>



<h2>References</h2>



<p class="justify-text">[1] Tianxiao Sun, and Quoc Tran-Dinh. <a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">Generalized self-concordant functions: a recipe for Newton-type methods</a>. <em>Mathematical Programming</em> 178(1): 145-213, 2019.<br />[2] Francis Bach. <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010.<br />[3] Elad Hazan, Tomer Koren, and Kfir Y. Levy. <a href="http://proceedings.mlr.press/v35/hazan14a.pdf">Logistic regression: Tight bounds for stochastic and online optimization</a>. Proceedings of the International Conference on Learning Theory (COLT), 2014.<br />[4] Boris T. Polyak, and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.<br />[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. SIAM Journal on optimization, 19(4), 1574-1609, 2009.<br />[6] Francis Bach, and Eric Moulines. <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. Advances in Neural Information Processing Systems (NIPS), 2011.<br />[7] Francis Bach. <a href="http://jmlr.org/papers/volume15/bach14a/bach14a.pdf">Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression</a>. Journal of Machine Learning Research, 15(Feb):595−627, 2014.<br />[8] Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi. <a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">Globally convergent Newton methods for ill-conditioned generalized self-concordant Losses</a>. Advances in Neural Information Processing Systems (NeurIPS), 2019.<br />[9] Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, Alessandro Rudi. <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance</a>. Proceedings of the International Conference on Learning Theory (COLT), 2019<br />[10] Dmitrii Ostrovskii, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">Francis Bach. Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br />[11] Sham Kakade, Ohad Shamir, Karthik Sridharan, and Ambuj Tewari. <a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">Learning exponential families in high-dimensions: Strong convexity and sparsity</a>. In Proceedings of the international conference on artificial intelligence and statistics (AISTATS), 2010.<br />[12] Aad W. Van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.<br />[13] Vladimir Spokoiny. <a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">Parametric estimation. Finite sample theory</a>. The Annals of Statistics, 40(6), 2877-2909, 2012.<br />[14] Tomer Koren, and Kfir Y. Levy. <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">Fast Rates for Exp-concave Empirical Risk Minimization</a>. Advances in Neural Information Processing Systems (NIPS), 2015.<br /></p>



<h2>Self-concordance for generalized linear models</h2>



<p class="justify-text">We consider a probability distribution on some set \(\mathcal{A}\), with density $$\exp\big( \varphi(a)^\top x) \ – f(x) \big)$$ with respect to the positive measure \(d\mu\), with \(f(x)\) the log-partition function, defined so that the total mass is one, that is, $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big). $$ We assume the feature vector \(\varphi(a)\) and the parameter \(x\) are in \(\mathbb{R}^d\).</p>



<p class="justify-text">The theory of <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential families</a> tells us that the function \(f(x)\) is the “cumulant generating” function. That is, the cumulants of \(\varphi(a)\) for the probability distribution defined by \(x\), are exactly the derivatives of \(f\) taken at \(x\). More precisely, for the usual mean and covariance matrix, we get $$ \mathbb{E}_{a|x} \varphi(a) = f^\prime (x),$$ $$ \mathbb{E}_{a|x} \big(\varphi(a) \ –  f^\prime (x)\big) \otimes \big(\varphi(a) \ – f^\prime (x)\big) = f^{\prime\prime}(x).$$ For the third order cumulant, we get: $$ \mathbb{E}_{a|x} \big(\varphi(a)\  – f^\prime (x)\big)\otimes \big(\varphi(a)\  – f^\prime (x)\big) \otimes \big(\varphi(a) \ – f^\prime (x)\big) = f^{\prime\prime\prime}(x).$$ Thus, for any \(h \in \mathbb{R}^d\), \(\big(\varphi(a) \ – f^\prime (x)\big)^\top h \leqslant \| h\| D\), where \(D\) is the diameter of the set \(\{ \varphi(a), \ a \in \mathcal{A} \}\), which leads to the desired \(2\)-self-concordance property.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/self-concordant-analysis-for-logistic-regression/"><span class="datestr">at March 07, 2021 04:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=848">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/03/07/questions-on-the-future-of-lower-bounds/">Questions on the future of lower bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Will any of the yellow books be useful?</p>



<figure class="wp-block-image"><img src="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg" alt="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg" /></figure>



<p>Book 576 (not pictured) was saved just in time from the paper mill.  It was rumored that Lemma 76.7.(ii) could have applications to lower bounds.  Upon closer inspection, that lemma has a one-line proof by linearity of expectation if you change the constant 17 to 19.  This change does not affect the big-Oh.</p>



<p>Will the study of randomness lead to the answer to any of the questions that are open since before randomness became popular? I think it’s a coin-toss.</p>



<p>Will there be any substance to the belief that algebraic lower bounds must be proved <em>first</em>?</p>



<p>Will the people who were mocked for working on DLOGTIME uniformity, top fan-in k circuits, or ZFC independence have the last laugh?</p>



<p>Will someone switch the circuit breaker and lit up CRYPT, DERAND, and PCPOT, or will they remain unplugged amusement parks where you sit in the roller coaster, buckle up, and pretend?</p>



<p>Will diagonalization be forgotten, or will it continue to frustrate combinatorialists with lower bounds they can’t match for functions they don’t care about?</p>



<p>Will decisive progress be made tonight, or will it take centuries?</p>



<p>Only Ketan Mulmuley knows for sure.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/03/07/questions-on-the-future-of-lower-bounds/"><span class="datestr">at March 07, 2021 11:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1485">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1485">News for February 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random <em>graphlets</em>. </p>



<p><strong>Testing Hamiltonicity (and other problems) in Minor-Free Graphs</strong>, by Reut Levi and Nadav Shoshan (<a href="https://arxiv.org/abs/2102.11728">arXiv</a>). Graph Property Testing has been explored pretty well for dense graphs (and reasonably well for bounded degree graphs). However, testing properties in the general case still remains an elusive goal. This paper makes contributions in this direction and as a first result it gives an algorithm for testing Hamiltonicity <em>in minor free graphs</em> (with two sided error) with running time \(poly(1/\varepsilon)\). Let me begin by pointing out that Hamiltonicity is an irksome property to test in the following senses.</p>



<ul><li>It is neither monotone nor additive. So the partition oracle based algorithms do not immediately imply a tester (with running time depending only on \(\varepsilon\) for Hamiltonicity. This annoyance bugs you even in the bounded degree case.</li><li> Czumaj and Sohler characterized what graph properties are testable with one-sided error in general planar graphs. In particular, they show a property of general planar graphs is testable <em>iff</em> this property can be reduced to testing for a finite family of finite forbidden subgraphs. Again, Hamiltonicity does not budge to this result. </li><li>There are (concurrent) results by Goldreich and Adler-Kohler which show that with one-sided error, Hamiltonicity cannot be tested with \(o(n)\) queries. </li></ul>



<p>The paper shows that distance to Hamiltonicity can be exactly captured in terms of a certain combinatorial parameter. Thereafter, the paper tries to estimate this parameter after cleaning up the graph a little. This allows them to estimate the distance to Hamiltonicity and thus also implies a tolerant tester (restricted to mino-free graphs).</p>



<p><strong>Testing properties of signed graphs</strong>, by Florian Adriaens, Simon Apers (<a href="https://arxiv.org/abs/2102.07587">arXiv</a>). Suppose I give you a graph \(G=(V,E)\) where all edges come with a label: which is either “positive” or “negative”. Such signed graphs are used to model various scientific phenomena. Eg, you can use these to model interactions between individuals in social networks into two categories like friendly or antagonistic.</p>



<p>This paper considers property testing problems on signed graphs. The notion of farness from the property extends naturally to these graphs (both in the dense graph model and the bounded degree model). The paper contains explores three problems in both of these models: signed triangle freeness, balance and clusterability. Below I will zoom into the tester for clusterability in the bounded degree setting developed In the paper. A signed graph is considered clusterable if you can partition the vertex set into some number of components such that the edges within any component are all positive and the edges running across components are all negative.</p>



<p>The paper exploits a forbidden subgraph characterization of clusterability which shows that any cycle with exactly one negative edge is a certificate of non-clusterability of \(G\). The tester runs multiple random walks from a handful of start vertices to search for these “bad cycles” by building up on ideas in the seminal work of Goldreich and Ron for testing bipariteness. The authors put all of these ideas together and give a \(\widetilde{O}(\sqrt n)\) time one-sided tester for clusterability in signed graphs.</p>



<p></p>



<p><strong>Local Access to Random Walks</strong>, by Amartya Shankha Biswas, Edward Pyne, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2102.07740">arXiv</a>). Suppose I give you a gigantic graph (with bounded degree) which does not fit in your main memory and I want you to solve some computational problem which requires you to solve longish random walks of length \(t\). And lots of them. It would be convenient to not spend \(\Omega(t)\) units of time performing every single walk. Perhaps it would work just as well for you to have an oracle which provides query access to a \(Position(G,s,t)\) oracle which returns the position of a walk from \(s\) at time \(t\) of your choice. Of course, you would want the sequence of vertices returned to behave consistently with some actual random walk sampled from the distribution of random walks starting at \(s\). Question is: Can I build you this primitive? This paper answers this question in affirmative  and shows that for graphs with spectral gap \(\Delta\), this can be achieved with running time \(\widetilde{O}(\sqrt n/\Delta)\) per query. And you get the guarantee that the joint distribution of the vertices you return at queried times is \(1/poly(n)\) close to the uniform distribution over such walks in \(\ell_1\).  Thus, for a random \(d\)-regular graph, you get running times of the order \(\widetilde{O}(\sqrt n)\) per query. The authors also show tightness of this result by showing to get subconstant error in \(\ell_1\), you necessarily need \(\Omega(\sqrt n/\log n)\) queries in expectation.</p>



<p></p>



<p><strong>Efficient and near-optimal algorithms for sampling connected subgraphs</strong>, by Marco Bressan (<a href="https://arxiv.org/abs/2007.12102">arXiv</a>). As the title suggests, this paper considers efficient algorithms for sampling a uniformly random \(k\)-graphlet from a given graph \(G\) (for \(k \geq 3\)). Recall, a \(k\)-graphlet refers to a collection of \(k\)-vertices which induce a connected graph in \(G\). The algorithm considered in the paper is pretty simple. You just define a Markov Chain \(\mathcal{G}_k\) with all \(k\)-graphlets as its state space. Two states in \(\mathcal{G}_k\) are adjacent <em>iff</em> their intersection is a \((k-1)\)-graphlet. To obtain a uniformly random sample, a classical idea is to just run this Markov Chain and obtain an \(\varepsilon\)-uniform sample. However, the gap between upper and lower bounds on the mixing time of this walk is of the order \(\rho^{k-1}\) where \(\rho = \Delta/\delta\) (that is the ratio of maximum and minimum degrees to the power \(k-1\)). The paper closes this gap up to logarithmic factors and shows that the mixing time of the walk is at most \(t_{mix}(G) \rho^{k-1} \log(n/\varepsilon)\). It also proves an almost matching lower bound. Further, the paper also presents an algorithm with event better running time to return an almost uniform \(k\)-graphlet. This exploits a previous observation: sampling a uniformly random \(k\)-graphlet is equivalent to sampling a uniformly random edge in \(\mathcal{G}_{k-1}\). The paper then proves a lemma which upperbounds the relaxation time of walks in \(\mathcal{G}_k\) to walks in \(\mathcal{G}_{k-1}\). And then you upperbound the mixing time in terms of the relaxation time to get an improved expected running time of the order \(O(t_{mix}(G) \cdot \rho^{k-2} \cdot \log(n/\varepsilon)\).</p>



<p></p>



<p><strong>Toward Instance-Optimal State Certification With Incoherent Measurements</strong>, by Sitan Chen, Jerry Li, Ryan O’Donnell (<a href="https://arxiv.org/abs/2102.13098">arXiv</a>). The problem of quantum state certification has gathered interest over the last few years. Here is the setup: you are given a quantum state \(\sigma \in \mathbb{C}^{d \times d}\) and you are also given \(N\) copies of an unknown state \(\rho\). You want to distinguish between the following two cases: Does \(\rho = \sigma\) or is \(\sigma\) at least \(\varepsilon\)-far from \(\rho\) in trace norm? Badescu et al showed in a recent work that if entangled measurements are allowed, you can do this with a mere \(O(d/\varepsilon^2)\) copies of \(\rho\). But using entangled states comes with its own share of problems. On the other hand if you disallow entanglement, as Bubeck et al show, you need \(\Omega(d^{3/2}/\varepsilon^2)\) measurements. This paper asks: for which states \(\sigma\) can you improve upon this bound. The work takes inspirations from <em>a la</em> “instance optimal” bounds for identity testing. Authors show a fairly general result which (yet again) confirms that the quantum world is indeed weird. In particular, the main result of the paper implies that the copy complexity of (the quantum analog of) identity testing in the quantum world (with non-adaptive queries) grows as \(\Theta(d^{1/2}/\varepsilon^2)\). That is, the number of quantum measurements you need increases with \(d\) (which is the stark opposite of the behavior you get in the classical world).</p></div>







<p class="date">
by Akash <a href="https://ptreview.sublinear.info/?p=1485"><span class="datestr">at March 06, 2021 05:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/032">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/032">TR21-032 |  Fiat-Shamir via List-Recoverable Codes (or: Parallel Repetition of GMW is not Zero-Knowledge) | 

	Ron Rothblum, 

	Justin Holmgren, 

	Alex Lombardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Shortly after the introduction of zero-knowledge proofs, Goldreich, Micali and Wigderson (CRYPTO '86) demonstrated their wide applicability by constructing  zero-knowledge proofs for the NP-complete problem of graph 3-coloring. A long-standing open question has been whether parallel repetition of their protocol preserves zero knowledge. In this work, we answer this question in the negative, assuming a a standard cryptographic assumption (i.e., the hardness of learning with errors (LWE)).

Leveraging a connection observed by Dwork, Naor, Reingold, and Stockmeyer (FOCS '99), our negative result is obtained by making positive progress on a related fundamental problem in cryptography: securely instantiating the Fiat-Shamir heuristic for eliminating interaction in public-coin interactive protocols. A recent line of works has shown how to instantiate the heuristic securely, albeit only for a limited class of protocols.

Our main result shows how to instantiate Fiat-Shamir for parallel repetitions of much more general interactive proofs. In particular, we construct hash functions that, assuming LWE, securely realize the Fiat-Shamir transform for the following rich classes of protocols:

- The parallel repetition of any ``commit-and-open'' protocol (such as the GMW protocol mentioned above), when a specific (natural) commitment scheme is used.  Commit-and-open protocols are a ubiquitous paradigm for constructing general purpose public-coin zero knowledge proofs.

- The parallel repetition of any base protocol that (1) satisfies a stronger notion of soundness called round-by-round soundness, and (2) has an efficient procedure, using a suitable trapdoor, for recognizing ``bad verifier randomness'' that would allow the prover to cheat.

Our results are obtained by establishing a new connection between the Fiat-Shamir transform and  list-recoverable codes.  In contrast to the usual focus in coding theory, we focus on a parameter regime in which the input lists are extremely large, but the rate can be small.  We give a (probabilistic) construction based on Parvaresh-Vardy codes (FOCS '05) that suffices for our applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/032"><span class="datestr">at March 05, 2021 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/031">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/031">TR21-031 |  Upper Bound for Torus Polynomials | 

	Vaibhav Krishan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that all functions that have low degree torus polynomials approximating them with small error also have $MidBit^+$ circuits computing them. This serves as a partial converse to the result that all $ACC$ functions have low degree torus polynomials approximating them with small error, by Bhrushundi, Hosseini, Lovett and Rao (ITCS 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/031"><span class="datestr">at March 05, 2021 02:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/">WSJ Meets Group Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Our whole life is solving puzzles. — Ernő Rubik</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jf-1.png"><img width="163" alt="" src="https://rjlipton.files.wordpress.com/2021/03/jf-1.png?w=163&amp;h=120" class="alignright wp-image-18244" height="120" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="http://www.ws.binghamton.edu/fridrich/pressconnects_com%20%2009-11-03%20%20News%20Story.htm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a>. She has over 34,000 citations—impressive. A lot more than most of us. She also has <a href="http://www.ws.binghamton.edu/fridrich/cube.html">worked</a> on the famous Rubik’s cube.</p>
<p>
Today we look at her work on Rubik’s cube, the WSJ’s interest in Rubik’s cube, and what both say—and don’t say—about fundamental algorithms.<br />
<span id="more-18238"></span></p>
<p>
By the way, WSJ stands for the Wall Street Journal—the American <a href="https://en.wikipedia.org/wiki/The_Wall_Street_Journal">newspaper</a> of business. The WSJ has shown great interest in the Rubik’s cube puzzle and has run many articles over the years on it.</p>
<p>
Recall the cube puzzle was invented…of course you know all about Rubik’s cube. You probably have owned one at one time. Right. Just for a <a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube">refresher</a>: </p>
<blockquote><p><b> </b> <em> The Rubik’s Cube is a 3-D combination puzzle invented in 1974 by Hungarian sculptor and professor of architecture Ernő Rubik. As of January 2009, 350 million cubes had been sold worldwide, making it the world’s top-selling puzzle game. It is widely considered to be the world’s best-selling toy. </em>
</p></blockquote>
<p>
But you may not know all about Fridrich.</p>
<p></p><h2> Speed Solving </h2><p></p>
<p>
Fridrich was one of the progenitors of <em>speed cubing</em>. She took part in the First World Championship in 1982 in Budapest, next-door to her native Czechoslovakia. She finished in the middle of the pack with a time of <b>29.11</b> seconds from a randomly well-mixed starting cube position. Her thoughts on how the cubes could be better prepared for speed are recorded on her <a href="http://www.ws.binghamton.edu/fridrich/cubewrld.html">page</a> about the tournament.</p>
<p>
At the Second World Championship, she improved her average time to <b>20.48</b> seconds and placed <a href="https://www.worldcubeassociation.org/competitions/WC2003">2nd</a>. She had the two fastest solves in the finals but lost on average-of-median-three-of-five.  That championship took place in Toronto—in <b>2003</b>. She is at a loss to explain why there was such a gap. Usually an athlete—in this case a mathlete?—is on the downswing nearing age 40, but even as a self-described “<a href="http://ws2.binghamton.edu/fridrich/history.html">old-timer</a>,” she fended off all but one of a whole next generation. </p>
<p>
Much of the credit goes to her solving method. She originated the “O” and “P” parts of the <a href="https://en.wikipedia.org/wiki/CFOP_method">CFOP</a> method. CFOP stands for: Cross, First 2 Layers, Orient Last Layer, Permute Last Layer. Versions of this are used my most top “cubers” to this day, and her name is often affixed to the method. In a 2008 profile of her, the New York Times <a href="https://www.nytimes.com/2008/12/16/science/16prof.html?_r=1&amp;em">quoted</a> the 2003 winner as saying that Fridrich found the route up the mountain while the rest of the cubers optimize traversing ledges along it. And in 2012, the NYT <a href="https://london2012.blogs.nytimes.com/2012/06/25/master-of-the-shot-put-and-the-cube/?searchResultPosition=5">quoted</a> Olympic shot-putter Reese Hoffa as wanting “to learn the Fridrich Method of solving the puzzle, ‘which is what all of the best cubers use.'” </p>
<p>
At this point, knowing our interest in chess, you might expect a <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(novel)"><i>Queen’s</i></a> <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(miniseries)"><i>Gambit</i></a> reference. But what we have here is not a story of Beth Harmon coming back from a life <a href="https://www.thereviewgeek.com/thequeensgambit-e6review/">adjournment</a> or Roy Hobbs in <a href="https://en.wikipedia.org/wiki/The_Natural"><i>The</i></a> <a href="https://en.wikipedia.org/wiki/The_Natural_(film)"><i>Natural</i></a> rejoining baseball almost 20 years after being shot. It’s about going overseas, earning a PhD, getting two research positions, writing early papers (under the <a href="https://dblp.org/pid/29/4038.html">name</a> Jiri Fridrich), transitioning, then getting a faculty position leading to tenure while developing mathematical formulas and writing tons of code for systems to <a href="https://www.nytimes.com/2004/07/22/technology/what-s-next-for-doctored-photos-a-new-flavor-of-digital-truth-serum.html?searchResultPosition=16">source</a> photos and catch digital pirates and pornographers and other image fraudsters, then coming back to light up an <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{8 \times 8}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3 \times 3}" class="latex" /> universe. Not to mention doing her own stunning <a href="https://www.jessicafridrich.com/">photo art</a> of the American Southwest.</p>
<p></p><h2> Quicker Times and Cubes </h2><p></p>
<p>
Since 2003, the <a href="https://en.wikipedia.org/wiki/Speedcubing#Competitions">championships</a> have been held every other year, thought the 2021 championships set for the Netherlands are uncertain owing to the pandemic. The youngsters soon broke through en-masse, and it strikes me that the cube technology improved so that the cubes are springier and lighter. The winning time fell almost 5 seconds to <b>15.10</b> in 2005 and hit <b>6.74</b> in 2019. That was not the world record, however—an incredible <b>3.47</b> seconds in 2018 by Yusheng Du, beating the previous record of Feliks Zemdegs by a whopping 3/4 of a second.</p>
<p>
Fridrich, however, must claim a distinction no one may ever match. She learned how to solve the cube and traced out the performance of methods of doing so in 1981, months before she saw a cube, let alone owned one. Despite the “Bűvös Kocka” (“Magic Cube,” as Rubik called it) having been on shelves in neighboring Hungary for four years, with worldwide marketing by early 1980, they were hard to come by in her home city, Ostrava. </p>
<p>
She found an article on solving the cube in a Russian magazine. It laid out the concept of group theory and the role of group commutators, which she learned to apply creatively in order to streamline actions. The first time she touched a cube was to help a friend put his back the way it was. A family visiting from France let her keep one, and later in 1981 she was finally able to purchase a few more. This invites analogy to working out chess without a board on a bedroom ceiling as depicted in <em>The Queen’s Gambit</em>.</p>
<p>
We—Dick and Ken—must admit that neither of us has ever done this with the cubes we own, not fast, not slow. Yet we do understand the theory behind it. We believe we do. </p>
<p></p><h2> Group Theory of the Cube </h2><p></p>
<p>
I (Dick) plan on explaining the theory by using a new toy that I have invented: The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Bslider%7D%5E%7BTM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathit{slider}^{TM}}" class="latex" />. 	</p>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png"><img width="96" alt="" src="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png?w=96&amp;h=45" class="aligncenter wp-image-18247" height="45" /></a></p>
<p>
We will write the state as <img src="https://s0.wp.com/latex.php?latex=%7Bxyz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{xyz}" class="latex" /> where each of <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x,y,z}" class="latex" /> is one of <font color="red">1</font>, <font color="green">2</font>, or <font color="blue">3</font>. The operations allowed are the <i>cyclic shift</i> <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" />, which does 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+zxy%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow zxy, " class="latex" /></p>
<p>and the <i>flip</i> <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F}" class="latex" /> of the initial two elements: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+yxz.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow yxz. " class="latex" /></p>
<p>
Note there are 6 possible states. For the real Rubik’s cube, the number of states is just a little bit larger: <b>43,252,003,274,489,856,000</b>. But the basic concept is the same. Suppose we are given the state 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132. " class="latex" /></p>
<p>How fast can you get the initial state <img src="https://s0.wp.com/latex.php?latex=%7B123%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{123}" class="latex" />? Apply <img src="https://s0.wp.com/latex.php?latex=%7BFCC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{FCC}" class="latex" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132+%5Crightarrow+312+%5Crightarrow+231+%5Crightarrow+123+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132 \rightarrow 312 \rightarrow 231 \rightarrow 123 . " class="latex" /></p>
<p>
This is a special case of the general <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/genset.pdf">result</a> that any symmetric group is <a href="https://groupprops.subwiki.org/wiki/Symmetric_group_on_a_finite_set_is_2-generated">generated</a> by two operations: a full cycle and a single flip. The key with the actual Rubik’s cube is since the group is larger and it has more operations that can be applied finding the group operations may be more difficult. But there are algorithms that can find them. See <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/gpaction.pdf">this</a> for another article by Keith Conrad. </p>
<p>
There are many more pages like that on the cube. But Fridrich still shows the <a href="http://www.ws.binghamton.edu/fridrich/system.html">seminal page</a> she posted in “Winter 1996/97.” It links to other pages, ones that also credit other people, such as <a href="http://www.ws.binghamton.edu/fridrich/Mike/middle.html">this</a> explaining the algorithms in great pictorial detail. This was in the infancy of the Internet. Her pages are often credited with spurring the turn-of-the-millennium boom in Rubik’s cube which led to the revival of the championships in 2003. A 2016 New York Post <a href="https://nypost.com/2016/10/31/how-the-internet-brought-the-rubiks-cube-back-to-life/">article</a> whose URL is titled, “how the Internet brought the Rubik’s cube back to life,” says: </p>
<blockquote><p><b> </b> <em> The seeds for Rubik’s Cube’s rediscovery were sown on the internet. In the mid-1990s, a Rubik’s Cube champion-turned-computer-science professor at SUNY Binghamton posted her secrets of the Cube on a primitive Web 1.0 site on the university’s servers. Jessica Fridrich’s method spread and is today the most widely used technique to solve the puzzle. </em>
</p></blockquote>
<p>
See also this <a href="https://uncletyson.wordpress.com/tag/dan-knights/">telling</a> by the 2003 winner, Dan Knights. This shows how one person using spare time on the Internet can power up business.</p>
<p></p><h2> Enter the WSJ </h2><p></p>
<p>
The WSJ has had an interest in Rubik’s cube for years. They had a long feature <a href="https://www.wsj.com/articles/how-to-teach-professors-humility-hand-them-a-rubiks-cube-11614352261">article</a> last week titled, “How to Teach Professors Humility? Hand Them a Rubik’s Cube,” by Melissa Korn. It describes a faculty development challenge among several small colleges in which professors became students again. Last month they also had an <a href="https://www.wsj.com/articles/seeing-things-with-the-power-of-symmetry-11612461325">article</a> on symmetry by the mathematician Eugenia Cheng that mentioned the cube.</p>
<p>
I recall several features the WSJ has run on the cube and its solvers. The 2011 <a href="https://www.wsj.com/articles/SB10001424052970204319004577088513615125328">article</a>, “One Cube, Many Knockoffs, Quintillions of Possibilities,” led off with the Polish teenager Michal Pleskowicz winning the 2011 world championship with a time of <b>8.65</b> seconds, then discussed the performance of pirated cubes: “One reason Mr. Pleskowicz and a new generation of Rubik’s fanatics can solve the notoriously difficult puzzle in record time: They don’t use Rubik’s Cubes at all, instead substituting souped-up Chinese knockoffs engineered for speed…” Their 2014 <a href="https://www.wsj.com/articles/SB10001424052702304518704579523513594900696">article</a>, “Rubik’s Cube Proves It’s Hip to Be Square,” profiled both Rubik and speed-solvers. </p>
<p>
The <a href="https://www.wsj.com/articles/a-thinking-persons-guide-to-the-rubiks-cube-1517586702">feature</a> I recall best was in 2018. It was titled, “A Thinking Person’s Guide to the Rubik’s Cube,” and subtitled, “What’s the best solution method—theory, algorithms or chance?” It was also by Eugenia Cheng. She begins by confessing, “I have always loved playing with a Rubik’s Cube, which combines logic with a satisfying tactile activity. I can solve it—getting each of the six sides to be one color—but not particularly quickly or cleverly.” </p>
<p>
They also like its use for analogies. Scrolling through their advanced search—both Ken and I subscribe to the WSJ—we find:</p>
<ul>
<li><a href="https://www.wsj.com/articles/close-reopen-repeat-restaurants-dont-know-what-covid-19-will-dish-out-next-11613138412">2/12/21</a>: “Running restaurants is now ‘a bit of a Rubik’s Cube,’ said Mr. Mosier, who reopened his casual cafes in late January.”
</li><li><a href="https://www.wsj.com/articles/reopening-schools-is-so-complicated-new-york-struggles-to-schedule-classes-11597939473">8/20/20</a>, headline: “Reopening Schools Is So Complicated, New York Is Struggling to Schedule Classes Nation’s largest district is still hashing out basic details about the school day; ‘a multidimensional Rubik’s Cube’ of challenges.”
</li><li><a href="https://www.wsj.com/articles/new-u-s-rules-on-foreign-students-put-universitiesin-dilemma-11594149280">7/7/20</a>: “The new [pandemic] rules have created a Rubik’s Cube of decisions for schools, which face unique challenges with each of their international student populations.”
</li><li><a href="https://www.wsj.com/articles/an-l-a-home-asking-62-million-includes-a-playful-perk-a-model-racetrack-11590523214">5/26/20</a>, about a home selling for $62 million: “Designed by Seattle-based architecture firm Olson Kundig, the house has interlocking boxes and planes resembling a Rubik’s cube…”
</li><li><a href="https://www.wsj.com/articles/president-trump-announces-19-billion-relief-program-for-farmers-11587165759">4/17/20</a>, quoting Agriculture Secretary Sonny Perdue on the coronavirus relief program for farmers: “It will be a logistical Rubik’s Cube.”
</li><li><a href="https://www.wsj.com/articles/he-wanted-something-more-from-retirement-so-he-got-three-jobs-11573743922">11/14/19</a>, about a retiree who started teaching business classes, keeping books for a non-profit business, and working on a ferry dock: “My society consists of able-bodied seamen, boat captains, truckers hauling bait and lobsters, fishermen, islanders and wide-eyed vacationers,” says Mr. Marshall. It’s “a constant Rubik’s cube. You never know what you’ll find.”
</li></ul>
<p>
In all, using the WSJ advanced search, we find 239 hits for “Rubik” going back to 1980. We should mention in-passing that one of them is their 7/17/20 <a href="https://www.wsj.com/articles/ron-graham-dazzled-admirers-with-math-and-juggling-feats-11594994403">obituary</a> for Ron Graham. We also find 7 hits for “Fridrich” over the same span. But they are all about the housing market, involving the Nashville-based realty Fridrich and Clarke.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
I am happy to see that the WSJ has published multiple articles on a particular algorithmic task. I like that algorithms have been the center of articles. I wish they would talk more about important algorithms. Solving a Rubik’s cube is not an algorithm that is used every day: What about: </p>
<ul>
<li>Sorting
</li><li>Searching
</li><li>Dynamic Programming
</li><li>Fast Arithmetic
</li></ul>
<p>They do have Eugenia Cheng, who wrote a <a href="https://www.wsj.com/articles/algorithms-arent-just-for-computers-11557407055">column</a> comparing sorting algorithms. And they have written on algorithms used in <a href="https://www.wsj.com/graphics/journey-inside-a-real-life-trading-algorithm/">trading</a> and on <a href="https://www.wsj.com/articles/social-media-algorithms-rule-how-we-see-the-world-good-luck-trying-to-stop-them-11610884800">social</a>–<a href="https://www.wsj.com/articles/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753">media</a> <a href="https://www.wsj.com/articles/how-to-win-friends-and-influence-algorithms-11555246800">platforms</a> and for <a href="https://www.wsj.com/articles/algorithms-used-in-policing-face-policy-review-11591003801">policing</a> and <a href="https://www.wsj.com/articles/SB10001424052702304626104579121251595240852">parole</a> and <a href="https://www.wsj.com/articles/algorithm-helps-new-york-decide-who-goes-free-before-trial-11600610400">bail</a> decisions. But that tends away from <em>fundamental algorithms</em> where the math is the matter.</p>
<p>
A 2018 WSJ <a href="https://www.wsj.com/articles/dont-believe-the-algorithm-1536157620">article</a> by Hannah Fry titled “Don’t Believe the Algorithm,” which begins with flaws in using facial recognition to find wanted suspects, brings us back toward Fridrich’s research. Might this all also raise discussion of “algorithms” for what and whom to cover?</p>
<p>
[fixed name at end]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/"><span class="datestr">at March 05, 2021 12:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5359">The Zen Anti-Interpretation of Quantum Mechanics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell?  It feels therapeutic, I have tenure, and anyone who doesn’t like it can close their broswer tab.</p>



<p>So: although I’ve written tens of thousands <a href="https://www.pbs.org/wgbh/nova/article/can-quantum-computing-reveal-the-true-meaning-of-quantum-mechanics/">of</a> <a href="https://arxiv.org/abs/1306.0159">words</a>, <a href="https://www.scottaaronson.com/papers/philos.pdf">on</a> <a href="https://www.scottaaronson.com/blog/?p=1103">this</a> <a href="https://www.scottaaronson.com/blog/?p=3628">blog</a> <a href="https://www.scottaaronson.com/democritus/">and</a> <a href="https://www.scottaaronson.com/qclec.pdf">elsewhere</a>, about interpretations of quantum mechanics, again and again I’ve dodged the question of which interpretation (if any) I <em>really believe myself</em>.  Today, at last, I’ll emerge from the shadows and tell you precisely where I stand.</p>



<p>I hold that all interpretations of QM are just crutches that are better or worse at helping you along to the Zen realization that <strong>QM is what it is and doesn’t need an interpretation</strong>.  As Sidney Coleman <a href="https://arxiv.org/abs/2011.12671">famously argued</a>, what needs reinterpretation is not QM itself, but all our <em>pre</em>-quantum philosophical baggage—the baggage that leads us to demand, for example, that a wavefunction |ψ⟩ either be “real” like a stubbed toe or else “unreal” like a dream.  Crucially, because this philosophical baggage differs somewhat from person to person, the “best” interpretation—meaning, the one that leads most quickly to the desired Zen state—can also differ from person to person.  Meanwhile, though, thousands of physicists (and chemists, mathematicians, quantum computer scientists, etc.) have approached the Zen state merely by spending decades working with QM, never worrying much about interpretations at all.  This is probably the truest path; it’s just that most people lack the inclination, ability, or time.</p>



<p>Greg Kuperberg, one of the smartest people I know, once told me that the problem with the Many-Worlds Interpretation is not that it says anything wrong, but only that it’s “melodramatic” and “overwritten.”  Greg is far along the Zen path, probably further than me.</p>



<p>You shouldn’t confuse the Zen Anti-Interpretation with “Shut Up And Calculate.”  The latter phrase, mistakenly attributed to Feynman but really due to David Mermin, is something one might say at the <em>beginning</em> of the path, when one is as a baby.  I’m talking here only about the <em>endpoint</em> of path, which one can approach but never reach—the endpoint where you intuitively understand exactly what a Many-Worlder, Copenhagenist, or Bohmian would say about any given issue, and also how they’d respond to each other, and how they’d respond to the responses, etc. but after years of study and effort you’ve <em>returned</em> to the situation of the baby, who just sees the thing for what it is.</p>



<p>I don’t mean to say that the interpretations are all interchangeable, or equally good or bad.  If you had to, you could call even me a “Many-Worlder,” but <em>only</em> in the following limited sense: that in fifteen years of teaching quantum information, my experience has consistently been that for <em>most</em> students, <a href="https://en.wikipedia.org/wiki/Many-worlds_interpretation">Everett’s crutch</a> is the best one currently on the market.  At any rate, it’s the one that’s the most like a straightforward <em>picture</em> of the equations, and the least like a wobbly tower of words that might collapse if you utter any wrong ones.  Unlike Bohr, Everett will never make you feel stupid for asking the questions an inquisitive child would ask; he’ll simply give you answers that are as clear, logical, and internally consistent as they are metaphysically extravagant.  That’s a start.</p>



<p>The <a href="https://en.wikipedia.org/wiki/Copenhagen_interpretation">Copenhagen Interpretation</a> retains a place of honor as the <em>first</em> crutch, for decades the <em>only</em> crutch, and the one closest to the spirit of positivism.  Unfortunately, <em>wielding</em> the Copenhagen crutch requires mad philosophical skillz—which parts of the universe should you temporarily regard as “classical”?  which questions should be answered, and which deflected?—to the point where, if you’re capable of all that verbal footwork, then why do you even <em>need</em> a crutch in the first place?  In the hands of amateurs—meaning, alas, nearly everyone—Copenhagen often leads <em>away</em> <em>from</em> rather than toward the Zen state, as one sees with the generations of New-Age bastardizations about “observations creating reality.”</p>



<p>As for <a href="https://en.wikipedia.org/wiki/De_Broglie%E2%80%93Bohm_theory">deBroglie-Bohm</a>—well, that’s a weird, interesting, baroque crutch, one whose actual details (the preferred basis and the guiding equation) are historically contingent and tied to specific physical systems.  It’s probably the right crutch for <em>someone</em>—it gets eternal credit for having led Bell to discover the Bell inequality—but its quirks definitely need to be discarded along the way.</p>



<p>Note that, among those who approach the Zen state, many might still call themselves Many-Worlders or Copenhagenists or Bohmians or whatever—just as those far along in spiritual enlightenment might still call themselves Buddhists or Catholics or Muslims or Jews (or atheists or agnostics)—even though, by that point, they might have more in common with each other than they do with their supposed coreligionists or co-irreligionists.</p>



<p>Alright, but isn’t all this Zen stuff just a way to dodge the <em>actual, substantive</em> questions about QM, by cheaply claiming to have transcended them?  If that’s your charge, then please help yourself to the following FAQ about the details of the Zen Anti-Interpretation.</p>



<ol><li><strong>What is a quantum state?</strong>  It’s a unit vector of complex numbers (or if we’re talking about mixed states, then a trace-1, Hermitian, positive semidefinite matrix), which encodes everything there is to know about a physical system.<br /></li><li><strong>OK, but are the quantum states “ontic” (really out in the world), or “epistemic” (only in our heads)?</strong>  Dude.  Do “basketball games” really exist, or is that just a phrase we use to summarize our knowledge about certain large agglomerations of interacting quarks and leptons?  Do even the “quarks” and “leptons” exist, or are those just words for excitations of the more fundamental fields?  Does “jealousy” exist?  Pretty much<em> all</em> our concepts are complicated grab bags of “ontic” and “epistemic,” so it shouldn’t surprise us if quantum states are too.  Bad dichotomy.<br /></li><li><strong>Why are there probabilities in QM?</strong>  Because QM <em>is</em> a (the?) generalization of probability theory to involve complex numbers, whose squared absolute values are probabilities.  It <em>includes</em> probability as a special case.<br /></li><li><strong>But why do the probabilities obey the Born rule?</strong>  Because, once the unitary part of QM has picked out the 2-norm as being special, for the probabilities <em>also</em> to be governed by the 2-norm is pretty much the only possibility that makes mathematical sense; there are many nice theorems formalizing that intuition under reasonable assumptions.<br /></li><li><strong>What is an “observer”?</strong>  It’s exactly what modern decoherence theory says it is: a particular kind of quantum system that interacts with other quantum systems, becomes entangled with them, and thereby records information about them—reversibly in principle but irreversibly in practice.<br /></li><li><strong>Can observers be manipulated in coherent superposition, as in the <a href="https://en.wikipedia.org/wiki/Wigner%27s_friend">Wigner’s Friend</a> scenario?</strong>  If so, they’d be radically unlike any physical system we’ve ever had direct experience with.  So, are you asking whether such “observers” would be <em>conscious</em>, or if so what they’d be conscious of?  Who the hell knows?<br /></li><li><strong>Do “other” branches of the wavefunction—ones, for example, where my life took a different course—exist in the same sense this one does?</strong>  If you start with a quantum state for the early universe and then time-evolve it forward, then yes, you’ll get not only “our” branch but also a proliferation of other branches, in the overwhelming majority of which Donald Trump was never president and civilization didn’t grind to a halt because of a bat near Wuhan.  But how could we possibly know whether anything “breathes fire” into the other branches and makes them real, when we have no idea what breathes fire into <em>this</em> branch and makes <em>it</em> real?  This is not a dodge—it’s just that a simple “yes” or “no” would fail to do justice to the enormity of such a question, which is above the pay grade of physics as it currently exists. <br /></li><li><strong>Is this it?  Have you brought me to the end of the path of understanding QM?</strong>  No, I’ve just pointed the way toward the <em>beginning</em> of the path.  The most fundamental tenet of the Zen Anti-Interpretation is that there’s no shortcut to actually <a href="https://www.scottaaronson.com/qclec.pdf">working</a> <a href="https://www.amazon.com/Quantum-Mechanics-Theoretical-Leonard-Susskind/dp/0465062903/ref=asc_df_0465062903/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312014159412&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7852945785672685485&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9028280&amp;hvtargid=pla-435140302691&amp;psc=1">through</a> the Bell inequality, quantum teleportation, Shor’s algorithm, the Kochen-Specker and PBR theorems, possibly even a … <em>photon</em> or a <em>hydrogen atom</em>, so you can see quantum probability in action and be enlightened.  I’m further along the path than I was twenty years ago, but not as far along as some of my colleagues.  Even the greatest quantum Zen masters will be able to get further when new quantum phenomena and protocols are discovered in the future.  All the same, though—and this is another major teaching of the Zen Anti-Interpretation—there’s more to life than achieving greater and greater clarity about the foundations of QM.  And on that note…</li></ol>



<p>To those who asked me about Claus Peter Schnorr’s <a href="https://eprint.iacr.org/2021/232">claim</a> to have discovered a fast <em>classical</em> factoring algorithm, thereby “destroying” (in his words) the RSA cryptosystem, see (e.g.) <a href="https://twitter.com/inf_0_/status/1367376526300172288?fbclid=IwAR19Ip7XyoPjHfm9WBzqiUkQpxUVLGfVTgLGQmmncgrkUsvcLIrkzbOPw_U">this Twitter thread by Keegan Ryan</a>, which explains what certainly <em>looks</em> like a fatal error in Schnorr’s paper.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5359"><span class="datestr">at March 04, 2021 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/">2-round BFT SMR with n=4, f=1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Zhuolun Xiang In the previous post, we presented a summary of our good-case latency results for Byzantine broadcast and Byzantine fault tolerant state machine replication (BFT SMR), where the good case measures the latency to commit given that the leader/broadcaster is honest. In this post, we describe...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/"><span class="datestr">at March 03, 2021 11:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/030">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/030">TR21-030 |  Hardness of Constant-round Communication Complexity | 

	Rahul Ilango, 

	Shuichi Hirahara, 

	Bruno Loff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
How difficult is it to compute the communication complexity of a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix? In 2009, Kushilevitz and Weinreb showed that this problem is cryptographically hard, but it is still open whether it is NP-hard. 

In this work, we show that it is NP-hard to approximate the size (number of leaves) of the smallest constant-round protocol for a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix. Along the way to proving this, we show a new *deterministic* variant of the round elimination lemma, which may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/030"><span class="datestr">at March 02, 2021 09:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/">Faculty at Universidad Católica de Chile (apply by April 10, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br />
Email: pbarcelo@uc.cl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/"><span class="datestr">at March 02, 2021 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/">Automated Design of Error-Correcting Codes, Part 1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Introduction. </strong>For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from<a href="https://en.wikipedia.org/wiki/Multiclass_classification"> multiclass learning</a> to even <a href="https://arxiv.org/pdf/1002.3864.pdf">showing hardness of approximation</a>. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains.</p>



<p>In our setting, imagine Alice wants to send a message to Bob of length <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />, but the channel between them is corrupted by noise. To overcome this, Alice uses an <em>encoder</em> to turn her message into a longer, redundant string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />. Then, Bob receives this transmission and uses a <em>decoder</em> to (hopefully) recover the original message of length <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />. Two important properties are the <em>rate</em> <img src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k/n" class="latex" /> of the code (essentially what fraction of the transmission is “information”) and the <em>bit error rate (BER)</em> which is the (expected) number of decoding errors divided by <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />. Desirable properties are to make the rate as large as possible and the BER as small as possible.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img width="526" alt="" src="https://lh3.googleusercontent.com/9y96GkfFf6FBCUKoufmsqUGfXt7VmrDqAuCQI1IaOfy4DB-VmJWEvSwL9c1mj9QP9gVYq49haRBI96eNMx5qPpr3BFhhGuWvTv4wixNbLLuTuSnI3xv36xaVa64D3DvshMs_XwmT" height="353" /></figure></div>



<p class="has-text-align-center">Basic ECC paradigm.</p>



<p>Since the days of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, many error-correcting codes have been discovered, such as <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> and <a href="https://en.wikipedia.org/wiki/BCH_code">BCH codes</a>. Each error-correcting code has its own tradeoffs (e.g., some have higher rate, some are more resistant to special kinds of channel corruptions, etc.). With the large number of ECCs which have been discovered, it can sometimes be overwhelming what the proper error correcting code is for a given application. Further, if the application is sufficiently specialized there may be <em>no </em>known ECC which meets your needs. Such concerns motivate the <em>automation</em> <em>of error correcting codes</em>, which is the main topic of this blog post. </p>



<p>I’m using the word “automation” to cover a variety of tasks which various computational methods could assist with in the study of ECCs:</p>



<ol><li>Existence — Does the code I want even exist?</li><li>Encoding — What is the “best” way to convert my messages into a code?</li><li>Decoding — How do I recover from noisy transmissions?</li><li>Verification — Is the proposed ECC design provably correct?</li><li>Selection — Which ECC from a given class should I use for a given application?</li></ol>



<p>Each of these facets of the automation of ECCs is a whole field of research! In this and the subsequent post, I will discuss at a high level two types of techniques which have been used to approach these questions: “Formal Methods” and “Machine Learning.” We’ll cover formal methods in this post, and in the next post we will cover machine learning methods.</p>



<p><strong>Formal Methods. </strong>The field of Formal Methods strives to give <em>provable guarantees</em> for various computational questions by reducing them to formal logic. Although formal methods are mostly used for software and hardware verification (that is, making sure they are “bug free”), such tools are also used by mathematicians to show the validity of mathematical statements that would be difficult to prove by hand. For example, the <a href="https://en.wikipedia.org/wiki/Kepler%27s_conjecture">Kepler conjecture</a>, a question of what is the best way to pack spheres in three dimensions–essentially finding an optimal error-correcting code in Euclidean space–was only firmly proved by <a href="https://github.com/flyspeck/flyspeck">Thomas Hales and his team</a> through the use of automated theorem-proving tools.</p>



<p>A line of work for using formal methods to directly construct practical ECCs was initiated by <a href="https://ieeexplore.ieee.org/abstract/document/5699220">Shamshiri and Cheng</a> in 2010. In their work, they are motivated by designing error-correcting codes for static random-access memory (SRAM), the kind that is often used for CPU caches. When using SRAM (pictured below), a worry is that cosmic rays could hit some of the bits, causing them to flip. Further, it is not uncommon for a group of consecutive bits to flip. As such, it is desirable to esure that the error correcting code can correct either <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g" class="latex" /> <em>global </em>errors or <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l" class="latex" /> <em>local</em> errors. Correcting global errors is a common property of error correcting codes, such as BCH codes or the <a href="https://en.wikipedia.org/wiki/Binary_Golay_code">Golay code</a>. However, local error correction is a much less common property to guarantee. Thus, the authors use a <em>SAT solver</em> to construct error correcting codes with the properties they desire.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img width="529" alt="" src="https://lh4.googleusercontent.com/qQRzeu2F2XzrjAl6DyPPWAnBVOKOSODIBX4RPBuz0aAMqErYxC2ZyAbtWy3z8ORU4rvMT9UEMI8-C7boHeKEijFwrKY2pRaneEm1lsAoKBUHpQQ1rcMpaLBZs8zAwr2yviMGJPyG" height="318" /></figure></div>



<p class="has-text-align-center">Static random-access memory (source: <a href="https://en.wikipedia.org/wiki/File:Hyundai_RAM_HY6116AP-10.jpg" rel="prettyphoto">Wikipedia</a>)</p>



<p>Assuming that the code to be construction is linear (the encoding map is a linear function over the field <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+F_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb F_2" class="latex" />), then the error correcting code can be described by a <a href="https://en.wikipedia.org/wiki/Parity-check_matrix">parity check matrix</a> M in <img src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5E%7B%28n-k%29+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0,1}^{(n-k) \times n}" class="latex" />. The key observation the authors make is that for M to be a proper error correcting code, every error pattern <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> (i.e., vectors with hamming weight at most <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g" class="latex" /> or consecutive errors in a block of length <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l" class="latex" />) must have <img src="https://s0.wp.com/latex.php?latex=Mp&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Mp" class="latex" /> be a distinct vector. For instance  <img src="https://s0.wp.com/latex.php?latex=M%280%2C+1%2C+0%2C+0%2C+1%29+%5Cneq+M%281%2C1%2C1%2C0%2C0%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(0, 1, 0, 0, 1) \neq M(1,1,1,0,0)" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g = 2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=l+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l = 3" class="latex" />.</p>



<p>These Boolean constraints can be expressed in conjunctive normal form, i.e., a SAT instance. As such a SAT-solver can be used to determine if there exists a matrix M with the given properties for a given k and n. For instance, they are able to find an error correcting code with parameters <img src="https://s0.wp.com/latex.php?latex=k+%3D+16%2C+n+%3D+26%2C+g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k = 16, n = 26, g = 2" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=l+%3D+4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l = 4" class="latex" />. In their follow-up work [<a href="https://ieeexplore.ieee.org/abstract/document/6139156">Shamshi, Ghofrani, Cheng, 2011]</a>, they use this error-correcting code for modeling an “on-chip network” between CPU cores in a multi-core processor<strong>.</strong></p>



<p>Another line of work led by Ben Curtis (see the <a href="https://cs.uwaterloo.ca/~cbright/reports/cacm-preprint.pdf">survey by Curtis, Kotsireas, and Ganesh</a>) has been seeking to construct ECC-like combinatorial objects. An example of such an object is a Hadamard matrix: a square matrix with <img src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\pm 1" class="latex" /> entries such that every row and column is orthogonal in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb R^n" class="latex" />. In fact, the authors search for a special type of Hadamard matrix made up of a quartet Williamson matrices which have an intricate algebraic structure. They find these objects by using an algorithm which goes back-and-forth between a SAT solver with a CAS (computer algebraic system) to help narrow the search space. </p>



<p>Formal Methods have further applications in error-correcting codes for <a href="https://ieeexplore.ieee.org/abstract/document/6649704">distributed cloud storage</a> and <a href="https://arxiv.org/pdf/1804.02317.pdf">value-deviation-bounded codes</a>.</p>



<p>This concludes our first post. In the <a href="https://wordpress.com/post/theorydish.blog/1823">next post</a>, we discuss machine learning methods. </p>



<p>Are you aware of other examples or applications of automation to error-correcting codes? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. </p></div>







<p class="date">
by Joshua Brakensiek <a href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/"><span class="datestr">at March 02, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/">Associate professor at KTH Royal Institute of Technology (apply by April 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/</a><br />
Email: tenuretrack@eecs.kth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/"><span class="datestr">at March 01, 2021 09:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/029">TR21-029 |  Public-Coin Statistical Zero-Knowledge Batch Verification against Malicious Verifiers | 

	Inbar Kaslasi, 

	Ron Rothblum, 

	Prashant Nalini Vasudevan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose that a problem $\Pi$ has a statistical zero-knowledge (SZK) proof with communication complexity $m$. The question of batch verification for SZK asks whether one can prove that $k$ instances $x_1,\ldots,x_k$ all belong to $\Pi$ with a statistical zero-knowledge proof whose communication complexity is better than $k \cdot m$ (which is the complexity of the trivial solution of executing the original protocol independently on each input).

In a recent work, Kaslasi et al. (TCC, 2020) constructed such a batch verification protocol for any problem having a non-interactive SZK (NISZK) proof-system. Two drawbacks of their result are that their protocol is private-coin and is only zero-knowledge with respect to the honest verifier.

In this work, we eliminate these two drawbacks by constructing a public-coin malicious-verifier SZK protocol for batch verification of NISZK. Similarly to the aforementioned prior work, the communication complexity of our protocol is $\big(k+poly(m) \big) \cdot polylog(k,m)$</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/029"><span class="datestr">at March 01, 2021 04:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/03/01/beyondlogconcave2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/">Beyond log-concave sampling (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In our previous <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">blog post</a>, we introduced the challenges of sampling distributions beyond log-concavity. 
We first introduced the problem of sampling from a distibution $p(x) \propto e^{-f(x)}$ given value or gradient oracle access to $f$, as an analogous problem to black-box optimization with oracle access. We introduced the natural algorithm for sampling in this setup: Langevin Monte Carlo, a Markov Chain reminiscent of noisy gradient descent,</p>

\[x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>Finally, we laid out the challenges when $f$ is not convex; in particular, LMC can suffer from slow mixing.</p>

<p>In this and the coming post, we describe two of our recent works tackling this problem. We identify two kinds of structure beyond log-concavity under which we can design provably efficient algorithms:  <em>multi-modality</em> and <em>manifold structure in the level sets</em>. These structures commonly occur in practice, especially in problems involving statistical inference and posterior sampling in generative models.</p>

<p>In this post, we will focus on multimodality, covered by the paper <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski.</p>

<h1 id="sampling-multimodal-distributions-with-simulated-tempering">Sampling multimodal distributions with simulated tempering</h1>

<p>The classical scenario in which Langevin takes exponentially long to mix is when $p$ is a mixture of two well-separated gaussians. In broadest generality, this was considered by <a href="http://www.ems-ph.org/journals/show_abstract.php?issn=1435-9855%20&amp;vol=6&amp;iss=4&amp;rank=1">Bovier et al. 2004</a> who used tools from metastable processes to show that transitioning from one peak to another can take exponential time. Roughly speaking, they show the transition time is proportional to the “energy barrier” a particle has to cross. If the gaussians have unit variance and means at distance $2r$, then the probability density at a point midway in between is $\propto e^{-r^2/2}$, and this energy barrier is $\propto e^{r^2/2}$. Thus, the mixing time is exponential. Qualitatively, the intuition for this phenomenon is simple to describe: if started at point A, the drift (i.e. gradient) term will push the walk towards A, so long as it’s close to the basin around A; hence, to transition from A to B (through C) the Gaussian noise must persistenly counteract the gradient term.</p>

<center>
<img width="500" src="http://www.andrew.cmu.edu/user/aristesk/animation_bovier.gif" />
</center>

<p>Hence Langevin on its own will not work even in very simple multimodal settings.</p>

<p>In <a href="https://arxiv.org/abs/1812.00793">our paper</a>, we show that combining Langevin Monte Carlo with a temperature-based heuristic called <em>simulated tempering</em> can significantly speed up mixing for multimodal distributions, where the number of modes is not too large, and the modes “look similar.”</p>

<p>More precisely, we show:</p>

<blockquote>
  <p><strong>Theorem (Ge, Lee, Risteski ‘18, informal)</strong>: If $p(x)$ is a mixture of $k$ shifts of a strongly log-concave distribution in $d$ dimensions (e.g. Gaussian), an algorithm based on simulated tempering and Langevin Monte Carlo that runs in time poly($d,k, 1/\varepsilon$) produces samples from a distribution $\varepsilon$-close to $p$ in total variation distance.</p>
</blockquote>

<p>The main idea is to create a meta-Markov chain (the simulated tempering chain) which has two types of moves: change the current “temperature” of the sample, or move “within” a temperature. The main intuition behind this is that at higher temperatures, the distribution is flatter, so the chain explores the landscape faster (see the figure below).</p>

<center> 
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_tempering.gif" />
</center>

<p>More formally, the distribution at inverse temperature $\beta$ is given by $p_\beta(x) \propto e^{-\beta f(x)}$. The Langevin chain which corresponds to $\beta$ is given by</p>

\[x_{t+\eta} = x_t - \eta \beta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>As in the figure above, a high temperature (low $\beta&lt;1$) flattens out the distribution and causes the chain to mix faster (top distribution in figure). However, we can’t merely run Langevin at a higher temperature, because the stationary distribution of the high-temperature chain is wrong: it’s $p_\beta(x)$. The idea behind simulated tempering is to run Langevin chains at different temperatures, sometimes swapping to another temperature to help lower-temperature chains explore. To maintain the right stationary distributions at each temperature, we use a Metropolis-Hastings filtering step.</p>

<p>More formally, choosing a suitable sequence $0&lt; \beta_1&lt; \cdots &lt;\beta_L=1$, we define the simulated tempering chain as follows.</p>

<p><img width="300" style="float: right;" src="http://holdenlee.github.io/pics/stl.png" /></p>

<ul>
  <li>The <em>state space</em> is a pair of a temperature and location in space $(i, x), i \in [L], x \in \mathbb{R}^d$.<br />
</li>
  <li>The <em>transitions</em> are defined as follows.
    <ul>
      <li>If the current point is $(i,x)$, then <em>evolve</em> $x$ according to Langevin diffusion with inverse temperature $\beta_i$.</li>
      <li>Propose swaps with some rate $\lambda &gt;0$. Proposing a swap means attempting to move to a neighboring chain, i.e. change $i$ to $i’=i\pm 1$. With probability $\min{p_{i’}(x)/p_i(x), 1}$, the transition is accepted. Otherwise, stay at the same point. This is a <em>Metropolis-Hastings step</em>; its purpose is to preserve the stationary distribution.</li>
    </ul>
  </li>
</ul>

<p>Finally, it’s not too hard to see that at the stationary distribution, the samples at the $L$th level ($\beta_L=1$) are the desired samples.</p>

<h2 id="proof-idea-decomposition-theorem">Proof idea: decomposition theorem</h2>

<p>The main strategy is inspired by Madras and Randall’s <a href="https://www.jstor.org/stable/2699896">Markov chain decomposition theorem</a>, which gives a criterion for a Markov chain to mix rapidly: partition the state space into sets, and show that</p>

<ol>
  <li>The Markov chain mixes rapidly when restricted to each set of the partition.</li>
  <li>The <em>projected</em> Markov Chain, which we define momentarily, mixes rapidly. If there are $m$ sets, the projected chain $\overline M$ is defined on the state space ${1,\ldots, m}$, and transition probabilities are given by average probability flows between the corresponding sets.</li>
</ol>

<p>To implement this strategy, we first have to specify the partition. In fact, we roughly show that there is a partition of $[L] \times \mathbb{R}^d$ in which:</p>

<ol>
  <li>The simulated tempering Langevin chain mixes fast within each of the sets.</li>
  <li>The “volume” of the sets (under the stationary distribution of the tempering chain) is not too small.
</li>
</ol>

<p>In applying the Madras-Randall framework with this partition, it’s clear that point (1) above satisfies requirement (1) for the framework; point (2) ensures that the projected Markov chain has no “bottlenecks” and hence that it mixes rapidly (requirement (2)). More precisely, we can show rapid mixing either through the method of canonical paths or Cheeger’s inequality. To do this, we exhibit a “good-probability” path between any two sets in the partition, going through the highest temperature.</p>

<p>The intuition for why this path works is illustrated in the figure below: when transitioning from the set corresponding to the left mode at level $L$ to the right mode at level $L$, each of the steps up/down the temperatures are accepted with good probability if the neighboring temperatures are not too different; at the highest temperature, the chain mixes fast by point (1), and since each of the sets are not too small by point (2), there is a reasonable probability to end at the right mode at the highest temperature.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_conductance.gif" />
</center>



<p>Intuitively, the partition should track the “modes” of the distribution, but a technical hurdle in implementing this plan is in defining the partition when the modes overlap. One can either do this spectrally (i.e. showing that the Langevin chain has a spectral gap, and use theorems about <a href="https://arxiv.org/abs/1309.3223">spectral graph partitioning</a>, as we did in the <a href="https://arxiv.org/abs/1710.02736">first version</a> of the paper), or use a functional “soft decomposition theorem” which is a more flexible version of the classical decomposition theorem, which we use in a <a href="https://arxiv.org/abs/1812.00793">later version</a> of the paper.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/"><span class="datestr">at March 01, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/">PhD. Thesis at LAMSADE (Paris Dauphine) (apply by April 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs”</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf">https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf</a><br />
Email: florian.sikora@dauphine.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/"><span class="datestr">at March 01, 2021 10:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/">Good-case Latency of Byzantine Broadcast: a Complete Categorization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Zhuolun Xiang State Machine Replication and Broadcast Many existing permission blockchains are built using Byzantine fault-tolerant state machine replication (BFT SMR), which ensures all honest replicas agree on the same sequence of client inputs. Most of the practical solutions for BFT SMR are based on the Primary-Backup...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/"><span class="datestr">at February 28, 2021 06:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5663884325046890461">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html">Using number-of-PhD's as a measure of smartness is stupid.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In <i>Thor:Ragnorak</i> Bruce Banner mentions that he has 7 PhDs. Gee, I wonder how he managed to slip that into a conversation casually.  Later in the movie:</p><p><br /></p><p>Bruce: I don't know how to fly one of those (it an Alien Spacecraft)</p><p>Thor: You're a scientist. Use one of your PhD's </p><p>Bruce: None of them are for flying alien spaceships.</p><p><br /></p><p>On the episode <i>Double Date </i>of Archer (Season 11, Episode 6) Gabrielle notes that she has 2 PhD's whereas Lana only has 1 PhD. </p><p><br /></p><p>I am sure there are other examples of a work of fiction using <i>number of PhDs </i>as a way to say that someone is smart. In reality the number of PhD's one has is... not really a thing. </p><p>In reality if a scientist wants to do work in another field they... do work in that field.</p><p>Godel did research in Physics in the 1950's, but it would have been silly to go back and get a PhD in it.</p><p>Fortnow did research in Economics, but it would have been silly to go back and get a PhD in it. </p><p>Amy Farrah Fowler worked in neurobiology and then in Physics. Her Nobel prize in physics (with Sheldon Cooper) is impressive, getting a PhD in Physics would be ... odd. Imagine someone looking at here resume: <i>She has a Nobel Prize in Physics, but does she have a PhD? Did she pass her qualifying</i> <i>exams?</i>  This is the flip side of what I mentioned in a prior post about PhD's: <i>Not only does Dr. Doom want to take over the world, but his PhD is from The University of Latveria, which is not accredited. </i></p><p>There are other examples.</p><p>There ARE some people who get two PhDs for reasons of job market or other such things. That's absolutely fine of course. However, I wonder if in the real world they brag about it. I doubt it. </p><p>Is there anyone who has 3 PhDs? I would assume yes, but again, I wonder if they brag about it. Or should. </p><p>WHY do TV and movies use number-of-PhDs as a sign of genius? I do not know- especially since there are BETTER ways say someone is a genius in a way the audience can understand:  number-of-Nobel-prizes, number-of-times-mentioned-in-complexityblog,  number of Dundie's (see <a href="https://theoffice.fandom.com/wiki/Dundie">here</a>), etc. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html"><span class="datestr">at February 28, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/28/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/28/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="http://acm-stoc.org/stoc2021/accepted-papers.html">STOC 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105748480557219533">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/">Arranging invisible icons in quadratic time</a> (<a href="https://mathstodon.xyz/@11011110/105756917532905626">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26152335">via</a>). Yet another instance where using a too-slow algorithm causes a UI hang, with the twist that the better solution would not be to replace it with a faster algorithm, but instead to not do the useless thing that the bad algorithm does at all.</p>
  </li>
  <li>
    <p><a href="https://joshdata.me/iceberger.html">Fun with shapes: draw an iceberg and see which way up and how deep it would float</a> (<a href="https://mathstodon.xyz/@11011110/105768276511155377">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26201160">via</a>, <a href="https://www.metafilter.com/190533/Iceberger">via2</a>, <a href="https://boingboing.net/2021/02/20/make-your-own-iceberg-with-iceberger.html">via3</a>). Inspired by <a href="https://mobile.twitter.com/GlacialMeg/status/1362557149147058178">a twitter thread by Megan Thompson-Munson</a> pointing out that many supposed photos or illustrations of icebergs are fake and wrong.</p>
  </li>
  <li>
    <p>Draw an infinite subgraph of the 3d integer lattice in which each vertex has four co-planar neighbors, in a perpendicular plane to each of its neighbors (<a href="https://mathstodon.xyz/@11011110/105771494222747316">\(\mathbb{M}\)</a>). This completely determines the subgraph, which is 4-regular and highly symmetric. It is the graph of adjacencies of the cubes in the <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix structure</a>. Does this graph have a name and history?</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/3/250708-gender-trends-in-computer-science-authorship">Gender trends in computer science authorship</a> (<a href="https://mathstodon.xyz/@11011110/105781841287243050">\(\mathbb{M}\)</a>). Takeaways for me (mostly from the barely-readable Fig. 4) are:</p>

    <ul>
      <li>
        <p>Roughly one in four coauthors of CS research publications are currently female, up from a big dip of one in seven in the 1970s to 1990s.</p>
      </li>
      <li>
        <p>Mathematics started lower and is currently more or less the same.</p>
      </li>
      <li>
        <p>We are not on track to gender parity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>I’m sad that the only way to find a viewable version of the 1991 short film <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em> (on the hyperbolic geometry of knot complements) seems to be through pirate copies (<a href="https://mathstodon.xyz/@11011110/105785401264334824">\(\mathbb{M}\)</a>). Or you could pay $45 to Amazon for a copy on DVD. Do most people still have DVD players? At least they’re not still trying to sell it on VHS only.</p>
  </li>
  <li>
    <p><a href="https://cscresearchblog.wordpress.com/2018/11/16/karp-sipser-heuristic-and-reductions/">On the slow spread of knowledge of nice theorems</a> (<a href="https://mathstodon.xyz/@11011110/105793165233864617">\(\mathbb{M}\)</a>), an amusing cartoon at the end of a longer blog post on fast graph matching heuristics.</p>
  </li>
  <li>
    <p>Today’s LaTeX formatting tip (<a href="https://mathstodon.xyz/@11011110/105796107362586793">\(\mathbb{M}\)</a>): You know that bug where amsthm + hyperref, with one numbering for theorems and lemmas and corollaries and whatever, causes <code class="language-plaintext highlighter-rouge">\autoref</code> to call them theorems even when they’re really lemmas and corollaries and whatever? If you don’t, you’re lucky. Anyway, there’s a very simple workaround: after loading amsthm and hyperref, add one more package:</p>

    <p><code class="language-plaintext highlighter-rouge">\usepackage[capitalize,nameinlink]{cleveref}</code></p>

    <p>Then, just use <code class="language-plaintext highlighter-rouge">\cref</code> everywhere you were using <code class="language-plaintext highlighter-rouge">\autoref</code>. Problem solved!</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s algorithm</a> animated for 3d points (<a href="https://mathstodon.xyz/@tpfto/105553548210257285">\(\mathbb{M}\)</a>). See also <a href="https://mathstodon.xyz/@tpfto/105803635782297523">the spherical version</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/">Applications of the no-3-in-line problem and cap-sets to complexity theory</a> (<a href="https://mathstodon.xyz/@11011110/105807834096788492">\(\mathbb{M}\)</a>). “What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas [for linear algebraic circuits] it frustrates a better lower bound.”</p>
  </li>
  <li>
    <p><a href="https://www.bldgblog.com/2013/08/tensioned-suspension/">Tensioned suspension</a> (<a href="https://mathstodon.xyz/@11011110/105811049795181041">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=9093187">via</a>): sculptures by Dan Grayber in which the weight of mechanical linkages causes them to push out against the sides of their glass enclosures, seemingly causing them to hang suspended in air. More at <a href="http://www.dangrayber.com/">Grayber’s web site</a>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/28/linkage.html"><span class="datestr">at February 28, 2021 04:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
