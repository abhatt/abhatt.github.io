<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 11, 2021 11:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">STOC Test of time award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br /><br />The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br /><br />Please nominate the papers you think have most influenced our field!</li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/"><span class="datestr">at May 11, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04035">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04035">Knapsack and Subset Sum with Small Items</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Adam Polak, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, Karol Węgrzycki <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04035">PDF</a><br /><b>Abstract: </b>Knapsack and Subset Sum are fundamental NP-hard problems in combinatorial
optimization. Recently there has been a growing interest in understanding the
best possible pseudopolynomial running times for these problems with respect to
various parameters.
</p>
<p>In this paper we focus on the maximum item size $s$ and the maximum item
value $v$. We give algorithms that run in time $O(n + s^3)$ and $O(n + v^3)$
for the Knapsack problem, and in time $\tilde{O}(n + s^{5/3})$ for the Subset
Sum problem.
</p>
<p>Our algorithms work for the more general problem variants with
multiplicities, where each input item comes with a (binary encoded)
multiplicity, which succinctly describes how many times the item appears in the
instance. In these variants $n$ denotes the (possibly much smaller) number of
distinct items.
</p>
<p>Our results follow from combining and optimizing several diverse lines of
research, notably proximity arguments for integer programming due to Eisenbrand
and Weismantel (TALG 2019), fast structured $(\min,+)$-convolution by Kellerer
and Pferschy (J. Comb. Optim. 2004), and additive combinatorics methods
originating from Galil and Margalit (SICOMP 1991).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04035"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.04023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.04023">Fast and Error-Adaptive Influence Maximization based on Count-Distinct Sketches</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Gokhan Gokturk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaya:Kamer.html">Kamer Kaya</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.04023">PDF</a><br /><b>Abstract: </b>Influence maximization (IM) is the problem of finding a seed vertex set that
maximizes the expected number of vertices influenced under a given diffusion
model. Due to the NP-Hardness of finding an optimal seed set, approximation
algorithms are frequently used for IM. In this work, we describe a fast,
error-adaptive approach that leverages Count-Distinct sketches and hash-based
fused sampling. To estimate the number of influenced vertices throughout a
diffusion, we use per-vertex Flajolet-Martin sketches where each sketch
corresponds to a sampled subgraph. To efficiently simulate the diffusions, the
reach-set cardinalities of a single vertex are stored in memory in a
consecutive fashion. This allows the proposed algorithm to estimate the number
of influenced vertices in a single step for simulations at once. For a faster
IM kernel, we rebuild the sketches in parallel only after observing estimation
errors above a given threshold. Our experimental results show that the proposed
algorithm yields high-quality seed sets while being up to 119x faster than a
state-of-the-art approximation algorithm. In addition, it is up to 62x faster
than a sketch-based approach while producing seed sets with 3%-12% better
influence scores
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.04023"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03968">Fast $n$-fold Boolean Convolution via Additive Combinatorics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03968">PDF</a><br /><b>Abstract: </b>We consider the problem of computing the Boolean convolution (with
wraparound) of $n$~vectors of dimension $m$, or, equivalently, the problem of
computing the sumset $A_1+A_2+\ldots+A_n$ for $A_1,\ldots,A_n \subseteq
\mathbb{Z}_m$. Boolean convolution formalizes the frequent task of combining
two subproblems, where the whole problem has a solution of size $k$ if for some
$i$ the first subproblem has a solution of size~$i$ and the second subproblem
has a solution of size $k-i$. Our problem formalizes a natural generalization,
namely combining solutions of $n$ subproblems subject to a modular constraint.
This simultaneously generalises Modular Subset Sum and Boolean Convolution
(Sumset Computation). Although nearly optimal algorithms are known for special
cases of this problem, not even tiny improvements are known for the general
case.
</p>
<p>We almost resolve the computational complexity of this problem, shaving
essentially a factor of $n$ from the running time of previous algorithms.
Specifically, we present a \emph{deterministic} algorithm running in
\emph{almost} linear time with respect to the input plus output size $k$. We
also present a \emph{Las Vegas} algorithm running in \emph{nearly} linear
expected time with respect to the input plus output size $k$. Previously, no
deterministic or randomized $o(nk)$ algorithm was known.
</p>
<p>At the heart of our approach lies a careful usage of Kneser's theorem from
Additive Combinatorics, and a new deterministic almost linear output-sensitive
algorithm for non-negative sparse convolution. In total, our work builds a
solid toolbox that could be of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03968"><span class="datestr">at May 11, 2021 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03831">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03831">Super Solutions of the Model RB</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Guangyan.html">Guangyan Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Wei.html">Wei Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03831">PDF</a><br /><b>Abstract: </b>The concept of super solution is a special type of generalized solutions with
certain degree of robustness and stability. In this paper we consider the
$(1,1)$-super solutions of the model RB. Using the first moment method, we
establish a "threshold" such that as the constraint density crosses this value,
the expected number of $(1,1)$-super solutions goes from $0$ to infinity.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03831"><span class="datestr">at May 11, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03782">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03782">Construction of Sparse Suffix Trees and LCE Indexes in Optimal Time and Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kosolobov:Dmitry.html">Dmitry Kosolobov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sivukhin:Nikita.html">Nikita Sivukhin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03782">PDF</a><br /><b>Abstract: </b>The notions of synchronizing and partitioning sets are recently introduced
variants of locally consistent parsings with great potential in
problem-solving. In this paper we propose a deterministic algorithm that
constructs for a given readonly string of length $n$ over the alphabet
$\{0,1,\ldots,n^{\mathcal{O}(1)}\}$ a version of $\tau$-partitioning set with
size $\mathcal{O}(b)$ and $\tau = \frac{n}{b}$ using $\mathcal{O}(b)$ space and
$\mathcal{O}(\frac{1}{\epsilon}n)$ time provided $b \ge n^\epsilon$, for
$\epsilon &gt; 0$. As a corollary, for $b \ge n^\epsilon$ and constant $\epsilon &gt;
0$, we obtain linear construction algorithms with $\mathcal{O}(b)$ space on top
of the string for two major small-space indexes: a sparse suffix tree, which is
a compacted trie built on $b$ chosen suffixes of the string, and a longest
common extension (LCE) index, which occupies $\mathcal{O}(b)$ space and allows
us to compute the longest common prefix for any pair of substrings in
$\mathcal{O}(n/b)$ time. For both, the $\mathcal{O}(b)$ construction storage is
asymptotically optimal since the tree itself takes $\mathcal{O}(b)$ space and
any LCE index with $\mathcal{O}(n/b)$ query time must occupy at least
$\mathcal{O}(b)$ space by a known trade-off (at least for $b \ge \Omega(n /
\log n)$). In case of arbitrary $b \ge \Omega(\log^2 n)$, we present
construction algorithms for the partitioning set, sparse suffix tree, and LCE
index with $\mathcal{O}(n\log_b n)$ running time and $\mathcal{O}(b)$ space,
thus also improving the state of the art.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03782"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03773">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03773">Separations for Estimating Large Frequency Moments on Data Streams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03773">PDF</a><br /><b>Abstract: </b>We study the classical problem of moment estimation of an underlying vector
whose $n$ coordinates are implicitly defined through a series of updates in a
data stream. We show that if the updates to the vector arrive in the
random-order insertion-only model, then there exist space efficient algorithms
with improved dependencies on the approximation parameter $\varepsilon$. In
particular, for any real $p &gt; 2$, we first obtain an algorithm for $F_p$ moment
estimation using $\tilde{\mathcal{O}}\left(\frac{1}{\varepsilon^{4/p}}\cdot
n^{1-2/p}\right)$ bits of memory. Our techniques also give algorithms for $F_p$
moment estimation with $p&gt;2$ on arbitrary order insertion-only and turnstile
streams, using $\tilde{\mathcal{O}}\left(\frac{1}{\varepsilon^{4/p}}\cdot
n^{1-2/p}\right)$ bits of space and two passes, which is the first optimal
multi-pass $F_p$ estimation algorithm up to $\log n$ factors. Finally, we give
an improved lower bound of $\Omega\left(\frac{1}{\varepsilon^2}\cdot
n^{1-2/p}\right)$ for one-pass insertion-only streams. Our results separate the
complexity of this problem both between random and non-random orders, as well
as one-pass and multi-pass streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03773"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03753">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03753">Parameterized Complexity of Feature Selection for Categorical Data Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simonov:Kirill.html">Kirill Simonov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03753">PDF</a><br /><b>Abstract: </b>We develop new algorithmic methods with provable guarantees for feature
selection in regard to categorical data clustering. While feature selection is
one of the most common approaches to reduce dimensionality in practice, most of
the known feature selection methods are heuristics. We study the following
mathematical model. We assume that there are some inadvertent (or undesirable)
features of the input data that unnecessarily increase the cost of clustering.
Consequently, we want to select a subset of the original features from the data
such that there is a small-cost clustering on the selected features. More
precisely, for given integers $\ell$ (the number of irrelevant features) and
$k$ (the number of clusters), budget $B$, and a set of $n$ categorical data
points (represented by $m$-dimensional vectors whose elements belong to a
finite set of values $\Sigma$), we want to select $m-\ell$ relevant features
such that the cost of any optimal $k$-clustering on these features does not
exceed $B$. Here the cost of a cluster is the sum of Hamming distances
($\ell_0$-distances) between the selected features of the elements of the
cluster and its center. The clustering cost is the total sum of the costs of
the clusters. We use the framework of parameterized complexity to identify how
the complexity of the problem depends on parameters $k$, $B$, and $|\Sigma|$.
Our main result is an algorithm that solves the Feature Selection problem in
time $f(k,B,|\Sigma|)\cdot m^{g(k,|\Sigma|)}\cdot n^2$ for some functions $f$
and $g$. In other words, the problem is fixed-parameter tractable parameterized
by $B$ when $|\Sigma|$ and $k$ are constants. Our algorithm is based on a
solution to a more general problem, Constrained Clustering with Outliers. We
also complement our algorithmic findings with complexity lower bounds.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03753"><span class="datestr">at May 11, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03697">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03697">Quantum Proofs of Proximity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Marcel Dall'Agnol, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gur:Tom.html">Tom Gur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moulik:Subhayan_Roy.html">Subhayan Roy Moulik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thaler:Justin.html">Justin Thaler</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03697">PDF</a><br /><b>Abstract: </b>We initiate the systematic study of QMA algorithms in the setting of property
testing, to which we refer as QMA proofs of proximity (QMAPs). These are
quantum query algorithms that receive explicit access to a sublinear-size
untrusted proof and are required to accept inputs having a property $\Pi$ and
reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a
minuscule portion of their input. Our algorithmic results include a
general-purpose theorem that enables quantum speedups for testing an expressive
class of properties, namely, those that are succinctly decomposable.
Furthermore, we show quantum speedups for properties that lie outside of this
family, such as graph bipartitneness. We also investigate the complexity
landscape of this model, showing that QMAPs can be exponentially stronger than
both classical proofs of proximity and quantum testers. To this end, we extend
the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to
prove quantum property testing lower bounds via reductions from communication
complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory
of Computing, 2016).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03697"><span class="datestr">at May 11, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03594">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03594">Learning stochastic decision trees</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03594">PDF</a><br /><b>Abstract: </b>We give a quasipolynomial-time algorithm for learning stochastic decision
trees that is optimally resilient to adversarial noise. Given an
$\eta$-corrupted set of uniform random samples labeled by a size-$s$ stochastic
decision tree, our algorithm runs in time
$n^{O(\log(s/\varepsilon)/\varepsilon^2)}$ and returns a hypothesis with error
within an additive $2\eta + \varepsilon$ of the Bayes optimal. An additive
$2\eta$ is the information-theoretic minimum.
</p>
<p>Previously no non-trivial algorithm with a guarantee of $O(\eta) +
\varepsilon$ was known, even for weaker noise models. Our algorithm is
furthermore proper, returning a hypothesis that is itself a decision tree;
previously no such algorithm was known even in the noiseless setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03594"><span class="datestr">at May 11, 2021 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.03531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.03531">On the Complexity of Verification of Time-Sensitive Distributed Systems: Technical Report</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Max Kanovich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kirigin:Tajana_Ban.html">Tajana Ban Kirigin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nigam:Vivek.html">Vivek Nigam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scedrov:Andre.html">Andre Scedrov</a>, Carolyn Talcott <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.03531">PDF</a><br /><b>Abstract: </b>Time-Sensitive Distributed Systems (TSDS), such as applications using
autonomous drones, achieve goals under possible environment interference (e.g.,
winds). Goals are often specified using explicit time constraints, and,
moreover, goals must be satisfied by the system perpetually. For example,
drones carrying out the surveillance of some area must always have recent
pictures, i.e., at most M time units old, of some strategic locations. This
paper proposes a Multiset Rewriting language with explicit time for specifying
and analyzing TSDSes. We introduce new properties, such as realizability (there
exists a good trace), survivability (where, in addition, all admissible traces
are good), recoverability (all compliant traces do not reach
points-of-no-return), and reliability (system can always continue functioning
using a good trace). A good trace is an infinite trace in which goals are
perpetually satisfied. We propose a class of systems called Progressing Timed
Systems (PTS), where intuitively only a finite number of actions can be carried
out in a bounded time period. We prove that for this class of systems the
problems of realizability, recoverability, reliability, and survivability are
PSPACE-complete. Furthermore, if we impose a bound on time (as in bounded
model-checking), we show that for PTS, realizability becomes NP-complete, while
survivability and reliability problems are in the $\Delta_2^p$ class of the
polynomial hierarchy.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.03531"><span class="datestr">at May 11, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br />
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/"><span class="datestr">at May 10, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5486">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br /></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br /></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5486"><span class="datestr">at May 10, 2021 05:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8609684815037895352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html">Trump, Facebook, and ComplexityBlog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html"><span class="datestr">at May 10, 2021 12:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">Arc-triangle tilings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg" alt="Tiling by ordinary triangles" /></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg" alt="Tiling by arc-triangles with two curved sides" /></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg" alt="Tiling by arc-triangles with three curved sides" /></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pinwheels.svg" alt="Tiling by arc-triangles with two curved sides, without reflection" /></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html"><span class="datestr">at May 09, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1512">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1512">News for April 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper.<em> (We may have missed some; if so, please let us know in the comments!)</em></p>



<p><strong>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</strong>, by Sepehr Assadi and Vishvajeet N (<a href="https://arxiv.org/abs/2104.04908">arXiv</a>). This paper establishes space vs. pass trade-offs lower bounds for streaming algorithms, for a variety of graph tasks: that is, of the sort “any \(m\)-pass-streaming algorithm for task \(\mathcal{T}\) must use memory at least \(f(m)\).” The tasks considered include graph property estimation (size of the maximum matching, of the max cut, of the  weight of the MST) and property testing for sparse graphs (connectivity, bipartiteness, and cycle-freeness). The authors obtained exponentially improved lower bounds for those, via reductions to a relatively standard problem, (noisy) gap cycle counting, for which they establish their main lower bound. As a key component of their proof, they prove a general direct product result (XOR lemma) for the streaming setting, showing that the advantage for solving the XOR of \(\ell\) copies of a streaming predicate \(f\) decreases exponentially with \(\ell\). </p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1512"><span class="datestr">at May 08, 2021 01:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/068">TR21-068 |  Quantum Proofs of Proximity | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Subhayan Roy Moulik, 

	Justin Thaler</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate the systematic study of QMA algorithms in the setting of property testing, to which we refer as QMA proofs of proximity (QMAPs). These are quantum query algorithms that receive explicit access to a sublinear-size untrusted proof and are required to accept inputs having a property $\Pi$ and reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a minuscule portion of their input.

Our algorithmic results include a general-purpose theorem that enables quantum speedups for testing an expressive class of properties, namely, those that are succinctly decomposable. Furthermore, we show quantum speedups for properties that lie outside of this family, such as graph bipartitneness.

We also investigate the complexity landscape of this model, showing that QMAPs can be exponentially stronger than both classical proofs of proximity and quantum testers. To this end, we extend the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to prove quantum property testing lower bounds via reductions from communication complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory of Computing, 2016).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/068"><span class="datestr">at May 08, 2021 11:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/067">TR21-067 |  Variety Evasive Subspace Families | 

	Zeyu Guo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of constructing explicit variety evasive subspace families. Given a family $\mathcal{F}$ of subvarieties of a projective or affine space, a collection $\mathcal{H}$ of projective or affine $k$-subspaces is $(\mathcal{F},\epsilon)$-evasive if for every $\mathcal{V}\in\mathcal{F}$, all but at most $\epsilon$-fraction of $W\in\mathcal{H}$ intersect every irreducible component of $\mathcal{V}$ with (at most) the expected dimension. The problem of constructing such an explicit subspace family generalizes both deterministic black-box polynomial identity testing (PIT) and the problem of constructing explicit (weak) lossless rank condensers. 

Using Chow forms, we construct explicit $k$-subspace families of polynomial size that are evasive for all varieties of bounded degree in a projective or affine $n$-space. As one application, we obtain a complete derandomization of Noether's normalization lemma for varieties of bounded degree in a projective or affine $n$-space. In another application, we obtain a simple polynomial-time black-box PIT algorithm for depth-4 arithmetic circuits with bounded top fan-in and bottom fan-in that are not in the Sylvester-Gallai configuration, improving and simplifying a result of Gupta (ECCC TR 14-130).

As a complement of our explicit construction, we prove a lower bound for the size of $k$-subspace families that are evasive for degree-$d$ varieties in a projective $n$-space. When $n-k=\Omega(n)$, the lower bound is superpolynomial unless $d$ is bounded. The proof uses a dimension-counting argument on Chow varieties that parametrize projective subvarieties.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/067"><span class="datestr">at May 08, 2021 06:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html">Congratulations, Dr. Matias!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://pmatias.com/">Pedro Ascensao Ferreira Matias</a>, one of the students working with Mike Goodrich in the UC Irvine <a href="https://www.ics.uci.edu/~theory/">Center for Algorithms and Theory of Computation</a>, passed his Ph.D. defense today.</p>

<p>Pedro is Portuguese, and came to UCI after a bachelor’s degree from the University of Coimbra in Portugal and a master’s degree from Chalmers University of Technology in Sweden.</p>

<p>The general topic of Pedro’s research is “exact learning”, the inference of structured information from queries or other smaller pieces of data. I’ve written here before about my work with Matias on <a href="https://11011110.github.io/blog/2019/02/21/mutual-nearest-neighbors.html">nearest-neighbor chains</a> and on <a href="https://11011110.github.io/blog/2019/08/17/footprints-in-snow.html">tracking paths in planar graphs</a>, the problem of placing sensors on a small subset of vertices so that, by detecting the order in which a path reaches each sensor, you can uniquely determine the whole path. His dissertation combines the tracking paths work with a second paper on tracking paths (“How to Catch Marathon Cheaters: New Approximation Algorithms for Tracking Paths”, <a href="https://arxiv.org/abs/2104.12337">arXiv:2104.12337</a>, to appear at WADS 2021), and a paper on reconstructing periodic and near-periodic strings from sublinear numbers of queries (“Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors and Jumbled-Index Queries in String Reconstruction”, <a href="https://arxiv.org/abs/2007.08787">arXiv:2007.08787</a>, in SPIRE 2020). He also has recent papers on reconstructing trees in SPAA 2020 and ESA 2020.</p>

<p>After finishing his doctorate, Pedro’s next position will be working for Facebook.</p>

<p>Congratulations, Pedro!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106196168129163033">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html"><span class="datestr">at May 07, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/">Faculty Full Professor at University of Bamberg, Germany (apply by June 18, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems.</p>
<p>Website: <a href="https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/">https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/</a><br />
Email: michael.mendler@uni-bamberg.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/"><span class="datestr">at May 07, 2021 02:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1659">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/">Fair Clustering with Probabilistic Group Membership</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br /><br />Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h \in {\cal H}" class="latex" /> to designate its group membership and we were to consider the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="l_h" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="u_h" class="latex" /> are the lower and upper pre-set proportionality bounds for color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C_i" class="latex" /> denotes the points in cluster <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="i" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="C^h_i" class="latex" /> denotes the subset of those points with color <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="h" class="latex" />. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br /></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" class="wp-image-1735" height="151" />Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" /></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br /></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img width="800" alt="" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" class="wp-image-1737" height="210" />Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-center, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-median, and <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p></p></div>







<p class="date">
by seyed2357 <a href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/"><span class="datestr">at May 07, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/">Ph.D. Student at IDSIA, USI-SUPSI, Lugano, Switzerland (apply by June 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms.<br />
The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas.<br />
The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and contact details of 2-3 references.</p>
<p>Website: <a href="https://people.idsia.ch//~grandoni/">https://people.idsia.ch//~grandoni/</a><br />
Email: fabrizio@idsia.ch</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/"><span class="datestr">at May 07, 2021 09:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21716">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/">Alef Corner: ICM2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img width="2100" alt="icm2022" src="https://gilkalai.files.wordpress.com/2021/05/icm2022.jpg" class="alignnone size-full wp-image-21717" height="2100" /><strong><span style="color: #ff0000;">Alef’s new piece for ICM 2022 will surely cheer you up!</span> </strong></p>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/"><span class="datestr">at May 06, 2021 07:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=51">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/">ALT Highlights – A Report on the First ALT Mentoring Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fifth post in the series, coverage of the first <a href="https://www.let-all.com/alt.html">ALT Mentoring Workshop</a> organized by the Learning Theory Alliance, written by <a href="https://www.let-all.com/blog/feed/knaggita@ttic.edu">Keziah Naggita</a> and <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>.</p>



<hr class="wp-block-separator" />



<h2><strong>1 Introduction</strong></h2>



<p class="has-text-align-left has-text-align-justify">The Learning Theory Alliance (Let-All) is an online initiative aimed at developing a supportive learning theory community, founded by (1) <a href="https://www.cs.utexas.edu/~surbhi/" target="_blank" rel="noopener noreferrer">Surbhi Goel</a>, a postdoctoral researcher at Microsoft Research New York, (2) <a href="https://people.eecs.berkeley.edu/~nika/" target="_blank" rel="noopener noreferrer">Nika Haghtalab</a>, an assistant professor at UC Berkeley EECS, (3) and <a href="https://vitercik.github.io" target="_blank" rel="noopener noreferrer">Ellen Vitercik</a>, a Ph.D. Student at CMU; and advised by <a href="https://www.stat.berkeley.edu/~bartlett/" target="_blank" rel="noopener noreferrer">Peter Bartlett</a>, <a href="https://home.ttic.edu/~avrim/" target="_blank" rel="noopener noreferrer">Avrim Blum</a>, <a href="https://people.csail.mit.edu/stefje/" target="_blank" rel="noopener noreferrer">Stefanie Jegelka</a>, <a href="https://www.dpmms.cam.ac.uk/%7Epll28/" target="_blank" rel="noopener noreferrer">Po-Ling Loh</a>, and <a href="http://www.jennwv.com" target="_blank" rel="noopener noreferrer">Jenn Wortman Vaughan</a>. The goal of the alliance is to ensure healthy community growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers. Let-All’s efforts towards realizing these goals include a series of ongoing and future activities, such as the first ALT mentoring workshop, coordinating the ALT Highlights blog series, and other upcoming community initiatives. This article reports on  Let-All’s <a href="https://let-all.com/alt.html" target="_blank" rel="noopener noreferrer">first Mentoring Workshop</a>, which was affiliated with the 32<sup>nd</sup> International Conference on Algorithmic Learning Theory.</p>



<p class="has-text-align-left has-text-align-justify">The workshop had two main sessions to cater to the time zone differences of the participants.  These sessions had three main components: an academic program, which included how-to-talks, Ask Me Anythings (AMAs), and presentation dissections; a technical program, which included research talks; and a social program, which included discussion tables and other activities.</p>



<p class="has-text-align-left has-text-align-justify">The workshop participants included students, researchers, and industry professionals, all at different levels of familiarity with learning theory. Because of the ongoing COVID-19 pandemic, the workshop was virtual. It was held on the online platforms Zoom and Gather town, a virtual interactive environment that mimics an in-person workshop setting. For accessibility, the workshop organizers opened up the workshop free of cost to all registered participants. </p>



<h2><strong>2 Program Highlights</strong></h2>



<h3><strong>2.1 Academic Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">To kick off the workshop, one of the organizers began with a welcome lecture: Surbhi in session one and Nika in session two.  They read out the code of conduct and who to contact in case of issues, outlined the workshop’s purpose, and gave attendees demographic information. They explained how participants could navigate the workshop-themed Gather town workspace and then ended the introduction with encouragement for participants to mingle. </p>



<p class="has-text-align-left has-text-align-justify">The <strong><em>How-to-Talks</em></strong> sessions covered writing papers, giving talks, and networking. In Session 1, <a href="https://www.cs.cmu.edu/~praveshk/" target="_blank" rel="noopener noreferrer">Pravesh Kothari</a> talked in great detail about the dos and don’ts of what to add in the abstract, overview, introduction, and appendix when advising participants on how to best structure research papers. He told attendees to always put effort into understanding their intended reader or talk audience.  Pravesh encouraged attendees to consider the expertise and interests of the reader or listener to capture their attention since these highly determine the attention span and interest in the information presented to them.  He strongly recommended attendees watch the <em><a href="https://www.youtube.com/watch?v=vtIzMaLkCaM" target="_blank" rel="noopener noreferrer">Leadership Lab: The Craft of Writing Effectively</a></em> by Larry McEnerney , Director of the University of Chicago Writing Program.<em> </em>In session 2 of the workshop, <a href="https://home.cs.colorado.edu/~raf/" target="_blank" rel="noopener noreferrer">Rafael Frongillo</a>, similar to Pravesh, discussed how to capture the intended audience when one writes a paper, reviews, and talks.</p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>networking session</em></strong>, <a href="https://www.cc.gatech.edu/~jabernethy9/" target="_blank" rel="noopener noreferrer">Jacob Abernethy</a> encouraged participants to seek out horizontal and vertical networking, for example, through collaborations, talks, and reach outs. He said that currently, in academia, Ph.D. admissions, faculty hiring, and tenure appointments are heavily risk-averse. Therefore, people seek out candidates based on their network. For this reason, it is crucial for students to network from early on in their careers. He gave great examples of how junior researchers can reach out and forge relationships with other researchers. For example, when you meet academics, faculty/postdocs at events, ask to give a talk at their lab. Jacob also candidly talked about his earlier failures at MIT and how they shaped his journey. He talked about luck and how <a href="https://www.microsoft.com/en-us/research/people/jcl/" target="_blank" rel="noopener noreferrer">John Langford</a>, who was at Toyota Technological Institute at Chicago at the time, took a chance on him that forever changed his life. Jacob, therefore, advised academics to take chances on people as this would change the course of the field. </p>



<p class="has-text-align-left has-text-align-justify"><a href="https://jamiemorgenstern.com" target="_blank" rel="noopener noreferrer">Jamie Morgenstern</a> discussed different networking methods in the <strong><em>second How-to-talks session</em></strong>. She emphasized that for junior researchers, it’s important to attend conferences and to network with others, to advertise their research through talks, and to reach out to faculty for collaboration. To introduce oneself and capture the listener’s attention, Jamie said, for conferences, prepare to do so in two minutes, for social four minutes, bar 12 minutes, and faculty interview 25 minutes. Senior grad students may help introduce the juniors during lunch/poster sessions. Finally, when emailing faculty about research, she said one should avoid discussion about other people’s work and instead should stick to the recipient’s work – “showing deep understanding and possibly open questions which might lead to collaboration.” </p>



<p class="has-text-align-left has-text-align-justify">In both workshop sessions, there were<strong><em> two parallel talk dissections, </em></strong>in which senior faculty members gave both positive and constructive feedback on talks junior researchers presented. In the first session, <a href="https://www.cs.cornell.edu/~rdk/" target="_blank" rel="noopener noreferrer">Bobby Kleinberg</a> discussed <a href="https://www.emilyruthdiana.com" target="_blank" rel="noopener noreferrer">Emily Diana</a>‘s talk titled “Minimax and Lexicographically Fair Learning: Algorithms, Experiments, and Generalization”. He highlighted parts that were impressive, those that needed improvement, and gave general advice on structuring an audience-based presentation. When Bobby suggested including more diagrams than text, a few people made suggestions of free tools including tikz, matcha.io, PowerPoint, and draw.io. In parallel, <a href="http://cseweb.ucsd.edu/~kamalika/" target="_blank" rel="noopener noreferrer">Kamalika Chaudhuri</a> dissected a talk on “Efficient, Noise-Tolerant, and Private Learning via Boosting” by <a href="https://marco.ntime.org" target="_blank" rel="noopener noreferrer">Marco Carmosino</a>. Two main takeaways of this talk dissection were the balance of technical and nontechnical content (e.g., explaining ideas with fun pictures, etc.) and having one main and clear idea as the talk’s takeaway. </p>



<p class="has-text-align-left has-text-align-justify">In the second session, <a href="https://sites.google.com/site/acmonsterqiao/" target="_blank" rel="noopener noreferrer">Mingda Qiao</a> gave a talk titled: “Stronger Calibration Lower Bounds via Sidestepping” which <a href="https://praneethnetrapalli.org" target="_blank" rel="noopener noreferrer">Praneeth Netrapalli</a> dissected. Praneeth remarked that theory folks often jump into the problem straight away without covering much background. In conferences, this might be fine due to time pressure and specific interests. However, in broader settings such as departmental seminars, he advised the speaker to allocate more time to introduce the problem lucidly and concisely. In parallel, <a href="https://sites.google.com/site/marywootters/" target="_blank" rel="noopener noreferrer">Mary Wootters</a> dissected a talk titled “List-Decodable Subspace Recovery: Dimension Independent Error in Polynomial Time” that <a href="http://aineshbakshi.com" target="_blank" rel="noopener noreferrer">Ainesh Bakshi</a> presented. </p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>AMA session</em></strong> moderated by <a href="https://www.stat.cmu.edu/~aramdas/" target="_blank" rel="noopener noreferrer">Aaditya Ramdas</a>, <a href="https://web.stanford.edu/~lmackey/" target="_blank" rel="noopener noreferrer">Lester Mackey</a> refreshingly answered several of the attendees’ well-curated questions about what makes strong collaborations, how to get into grad school, and whether or not he ever felt like quitting his Ph.D., among others.  He encouraged students to take classes with professors they are interested in as it makes it easy to ask for a mentorship opportunity. Lester talked about collaborations and imposter syndrome and encouraged attendees to look on the brighter side of things, to remember that we all are working towards one big goal, creating positive changes in the world. Therefore if someone discovers a result before us, we should applaud them, collaborate if possible, and move onto new problems. He said he did not necessarily plan to do a Ph.D. but got into it towards the end of his undergraduate degree due to an internship that made him fall in love with doing research. </p>



<p class="has-text-align-left has-text-align-justify">In the evening, there was an AMA session with <a href="http://people.csail.mit.edu/shafi/" target="_blank" rel="noopener noreferrer">Shafi Goldwasser</a> moderated by Nika. Shafi gave thoughtful and candid answers to attendees’ captivating questions about research, life in academia, collaborations, among others. Shafi told attendees that healthy competition, trust, and overlap of research interest, is crucial for successful research in the early stage of the career. She also asserted that fundamental science is always impactful. She mentioned that the high points of her career were working on problems she was curious about: cryptography, pseudo-randomness, and zero-knowledge proofs. Finally, when asked about what advice she wished she had during the early stage of her career, interestingly Shafi replied: “having good colleagues, good friends at work, very important, most important – having a listening, promoting and supportive cohort of friends rather than an individualistic path as a scholar is priceless.” </p>



<h3><strong>2.2 Technical Program</strong></h3>



<p class="has-text-align-left has-text-align-justify"><strong><em>Two research talks</em></strong> happened in the first session. First, Po-Ling Loh gave a talk titled “Mean estimation for entangled single-sample distributions.” Then <a href="https://web.stanford.edu/~vsharan/" target="_blank" rel="noopener noreferrer">Vatsal Sharan</a> talked about “Sample Amplification: Increasing Dataset Size even when Learning is Impossible”.<br />Similarly in the second session, <a href="https://www.cohennadav.com" target="_blank" rel="noopener noreferrer">Nadav Cohen</a> gave the first talk about tensor and matrix completion problems and the importance of understanding the theory behind deep learning from theoretical and practical perspectives. After, <a href="https://i.cs.hku.hk/~zhiyi/" target="_blank" rel="noopener noreferrer">Zhiyi Huang</a> gave a talk titled “Setting the Sample Complexity of Single-parameter Revenue Maximization.” </p>



<h3><strong>2.3 Social Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">In both sessions, during the social hours, <a href="https://sites.google.com/view/sumegha-garg/home" target="_blank" rel="noopener noreferrer">Sumegha Garg</a>, <a href="http://sgunasekar.github.io" target="_blank" rel="noopener noreferrer">Suriya Gunasekar</a>, and <a href="https://teddlyk.github.io" target="_blank" rel="noopener noreferrer">Thodoris Lykouris</a> organized the <strong><em>table topics</em></strong> to help attendees meet and interact with senior researchers and professors on different topics. The table topics included the following; starting on ML research, research agendas, ML+X: multidisciplinary research, advisor-advisee relationships, collaborators, communicating research and networking, beyond your institution: internships and research visits, planning after grad school: academia versus industry, Grad school applications, Work ethics, and Open research discussion. </p>



<p class="has-text-align-left has-text-align-justify">The table topics were chaired by; Jacob Abernethy, <a href="https://www.shivani-agarwal.net" target="_blank" rel="noopener noreferrer">Shivani Agarwal</a>, <a href="https://ericbalkanski.com" target="_blank" rel="noopener noreferrer">Eric Balkanski</a>, Peter Bartlett, Avrim Blum, <a href="http://sbubeck.com/" target="_blank" rel="noreferrer noopener">Sébastien Bubeck</a>, Kamalika Chaudhuri, Nadav Cohen, Sumegha Garg, Surbhi Goel, Suriya Gunasekar, Nika Haghtalab,  <a href="https://www.cs.columbia.edu/~djhsu/" target="_blank" rel="noopener noreferrer">Daniel Hsu</a>, <a href="https://www.prateekjain.org" target="_blank" rel="noopener noreferrer">Prateek Jain</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html" target="_blank" rel="noopener noreferrer">Mike Jordan</a>, <a href="https://homes.cs.washington.edu/~sham/" target="_blank" rel="noopener noreferrer">Sham Kakade</a>, <a href="https://www.microsoft.com/en-us/research/people/adum/" target="_blank" rel="noopener noreferrer">Adam Kalai</a>, Pravesh Kothari, <a href="https://people.cs.umass.edu/~akshay/" target="_blank" rel="noopener noreferrer">Akshay Krishnamurthy</a>, <a href="https://jerryzli.github.io" target="_blank" rel="noopener noreferrer">Jerry Li</a>, Po-Ling Loh, Thodoris Lykouris, <a href="https://www.tau.ac.il/~mansour/" target="_blank" rel="noopener noreferrer">Yishay Mansour</a>, <a href="https://pasin30055.github.io" target="_blank" rel="noopener noreferrer">Pasin Manurangsi</a>, <a href="https://vmuthukumar.ece.gatech.edu" target="_blank" rel="noopener noreferrer">Vidya Muthukumar</a>, Praneeth Netrapalli, <a href="https://wensun.github.io" target="_blank" rel="noopener noreferrer">Wen Sun</a>, <a href="https://www.bowaggoner.com" target="_blank" rel="noopener noreferrer">Bo Waggoner</a>, <a href="https://mzampet.com" target="_blank" rel="noopener noreferrer">Manolis Zampetakis</a>, and <a href="https://cyrilzhang.com/">Cyril Zhang</a>. </p>



<p class="has-text-align-left has-text-align-justify">Lastly, at the end of the two general research talks in sessions one and two of the workshop, attendees assembled on Gather town to close the workshop. The social event included 1:1 social interactions with other attendees, an attempt at forging relationships, and activities like dancing.</p>



<h2><strong>3 Attendance Statistics, Testimonials and Feedback</strong></h2>



<h3><strong>3.1 Participants Statistics</strong></h3>



<p class="has-text-align-left has-text-align-justify">ALT mentoring workshop welcomed talented academics, researchers, and professionals from a wide array of backgrounds. Of the 438 registered to attend the workshop, 197 were new to the learning theory community, 37 attended at least one ALT/COLT conference in the past, and 146 hadn’t attended ALT/COLT but had attended machine learning conferences (STOC, NeurIPS, etc.) as shown in Figure 1. </p>



<p class="has-text-align-left has-text-align-justify">The workshop participants came from different parts of the world, were of different genders, races, and seniority levels. We use Figures 2, 3b, 4a, and 4b to highlight this demographic information about the participants. Some participants chose session one, and others chose session two, a choice driven by their schedules and time zones. The attendance composition is as shown in figure 3a.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="515" alt="" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.28-AM.png?resize=515%2C310&amp;ssl=1" class="wp-image-58" height="310" />Figure 1: Registrants familiarity with the community</figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.11-AM-2.png?resize=678%2C401&amp;ssl=1" class="wp-image-61" height="401" />Figure 2: Career stages of participants</figure></div>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.33.54-AM-1.png?resize=678%2C414&amp;ssl=1" class="wp-image-70" height="414" /></figure></li><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.25-AM-1.png?resize=678%2C406&amp;ssl=1" class="wp-image-71" height="406" /></figure></li></ul>Figure 3: Session preferences (a) and locations of participants (b)</figure>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.57-AM.png?resize=678%2C404&amp;ssl=1" class="wp-image-75" height="404" /></figure></li><li class="blocks-gallery-item"><figure><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.42-AM-1.png?resize=678%2C404&amp;ssl=1" class="wp-image-77" height="404" /></figure></li></ul>Figure 4: Race (a) and Gender (b) distribution of participants</figure>



<p></p>



<h3><strong>3.2 Testimonials and Feedback</strong></h3>



<p class="has-text-align-left has-text-align-justify">In this section, we give a recount of testimonials from participants who we interviewed after the workshop. We also highlight some of the common themes in feedback from participants.</p>



<p class="has-text-align-left has-text-align-justify">In general, participants loved the content delivered in the sessions. They said it was informative, intuitive, and rare to find. Several participants loved interacting with peers and senior members and wished they had more time and activities to do it. The How-to-talks session (focused on networking skills, structuring papers, talks, and reviews) was the most popular session among attendees. 76.7% of the survey respondents said the session helped them gain new technical skills or hone existing skills, see opportunities in academia and how to use them, and see barriers in academia and ways to overcome them. Figure 5 highlights attendees’ ratings of the skills acquired from the workshop. </p>



<figure class="wp-block-image size-large is-resized is-style-default"><img width="678" alt="" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.11-AM.png?resize=678%2C404&amp;ssl=1" class="wp-image-80" height="404" />Figure 5: Usefulness ratings of skills gained from the workshop</figure>



<p><strong>Below is a recount of the workshop experiences of the interviewed attendees.</strong></p>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlight was the How-to-Talks since they provided a lot of more personal information/inputs that you cannot easily find online and which was very valuable. The event helped me to remember and reflect upon which qualities are crucial to becoming a good researcher. I even made a list in a place that I see every day to keep them in mind.” – <em><a href="https://www.michaelaerni.com" target="_blank" rel="noopener noreferrer">Michael Aerni</a>, MSc student at ETH Zurich.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“The workshop highlights for me were the How-to-talks, the AMA session with Lester, and the social tables. The How-to-talks were extremely valuable as they discussed topics such as structuring papers and networking in the community. These are subtle aspects that are not often explicitly talked about in the community. I, therefore, learned a lot from them. The AMA session was refreshingly honest and open. Finally, the social tables were also great as I got to meet and talk to some well-established senior community members like Sebastien Bubeck, Shivani Agarwal, and Akshay Krishnamurthy.” – <em><a href="https://people.eecs.berkeley.edu/~tgautam23/" target="_blank" rel="noopener noreferrer">Tanmay Gautam</a>, a second-year Ph.D. student at UC Berkeley. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“It was awesome to have Lester Mackey answer my questions, indubitably. I learned about staying true to the research questions I genuinely believe in regardless of external opinions and rewards. As an NYU AI School organizer, I can appreciate how much effort went into organizing the workshop. The organizers did a stellar job! I think a version of the same event again would be perfect.” – <em><a href="https://swapneelm.github.io" target="_blank" rel="noopener noreferrer">Swapneel Mehta</a>, a Data Science Ph.D. student at New York University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlights were getting to talk with senior members of the community. These opportunities rarely come by for someone who lives in a foreign country. For a timid person like myself, I am also thankful for the senior members for helping me (and other participants) breaking the ice and easing us into the conversations. Thanks to this event, I am now more confident in engaging with other researchers.”- <em><a href="http://www.donlapark.cmustat.com" target="_blank" rel="noopener noreferrer">Donlapark Ponnoprat</a>, a Statistics lecturer at Chiang Mai University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main highlights were Dr. Goldwasser’s session with Dr. Haghtalab and the socials. In the socials, I was able to ask professors and senior researchers for advice on varied topics based on the tables. Insights from Dr. Cohen’s lecture on tensor rank and implicit regularisation gave me several pointers to ideas in the literature that I was not aware of as a junior researcher from a slightly different AI specialty. These ideas might be beneficial for my research in the long term. </p><p class="has-text-align-left has-text-align-justify"> I send a sincere thank you to all the organizers. The mentorship workshop was a great event, and it models concrete actions, what it means to foster a welcoming community. It is clear how kind and dedicated folks are here as some researchers even stayed beyond midnight in their time zones to answer questions that attendees had. If an event like this happens again, I am most definitely signing up to come.” – <em><a href="https://github.com/esraa-saleh" target="_blank" rel="noopener noreferrer">Esra’a Saleh</a>, a Masters in Computer Science student at the University of Alberta, affiliated with AMII and RLAI.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main takeaway from the event was an inside look at academia. As an undergrad, my only experience in academia has been the little experience I have with my advisory professors. While this is an invaluable experience, this event was nice as it was one of the very few that cater to students, including undergrads, with the intent of bringing them into the academia fold. Getting to know new people and talking to them was extremely interesting, especially during lockdown when connecting with others is a much more valuable commodity.” – <em><a href="https://pages.cs.wisc.edu/~shrey/" target="_blank" rel="noopener noreferrer">Shrey Shah</a>, a penultimate year undergraduate student at the University of Wisconsin-Madison. 
</em></p></blockquote>



<p class="has-text-align-left has-text-align-justify">Several participants enjoyed the workshop sessions and hoped that Let-All holds more similar themed workshops in conferences. Attendees suggested ways for attendees to interact more with each other. Some of the suggestions included the following: ‘beginner-friendly open problems sessions where attendees can collaborate’ – Esra’a Saleh, ‘an icebreaker session at the beginning that encourages attendees to mingle’ – Michael Aerni, and ‘a poster session for participants to present their work’ – Shrey Shah. </p>



<h2><strong>4 Conclusion</strong></h2>



<p class="has-text-align-left has-text-align-justify">The ALT mentorship workshop organized by the Learning Theory Alliance brought together many academics and researchers. It was, and we hope it continues to be, an opportunity for the budding researchers to learn about research and meta-research, forge collaborations, and be inspired. Kudos to the organizers and the Alliance in general for dreaming such a positive vision and then striving to make it a great success! </p>



<p class="has-text-align-left has-text-align-justify"><em>Thanks to <a href="http://www.gautamkamath.com" target="_blank" rel="noreferrer noopener">Gautam Kamath</a>, <a href="https://web.stanford.edu/~mglasgow/" target="_blank" rel="noreferrer noopener">Margalit Glasgow</a>, Surbhi Goel, Nika Haghtalab</em> <em>and Ellen Vitercik for helpful conversations and comments.</em></p></div>







<p class="date">
by Keziah <a href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/"><span class="datestr">at May 06, 2021 03:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-21129445.post-4927313116100787313">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/pizza.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mysliceofpizza.blogspot.com/2021/05/scaling-research-training.html">Scaling Research Training</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>There is a lot of need in the Industry for engineers who know the art (edited to <i>craft</i>)of research (pursue and find the right literature and algorithms, understand prior art, adopt and modify for a specific context with domain awareness, interpret results,  dive deeper into the unique insights), or researchers who can apply this art with engineering finesse. I wondered yesterday in a meeting if our technological lessons from COVID times have helped us identify a way to train many more in the ways of research, more than what we produce as PhDs in universities (the MS programs seem to train advanced engineers more than padawan researchers). <br /></p></div>







<p class="date">
by metoo (noreply@blogger.com) <a href="http://mysliceofpizza.blogspot.com/2021/05/scaling-research-training.html"><span class="datestr">at May 06, 2021 01:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6832816500979478899">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/negotiations.html">Negotiations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>So you got an offer to be an assistant professor in the computer science department at Prestigious U. Congratulations! </p><p>Time to negotiate your offer with the chair. Don't be nervous. This shouldn't be adversarial. Both of you have the same goal in mind--for you to come to Prestigious and be successful. </p><p>Let's discuss the different aspects of each package.</p><p><b>Research </b></p><p>Funds for supporting your research such as equipment, graduate student support, travel and postdocs. Here you should explain what you need to be successful. This will vary by subdiscipline, a systems researcher will need more equipment and students than a theorist. Keep in mind the university is giving you funds for 2-4 years to start your research, after which you are expected to fund your own research via grants.</p><p>I don't recommend taking on a postdoc right at the start of your first academic appointment. Postdocs require good mentoring while you need to spend the first year getting your research up and running. If you do ask for postdoc money, ask to have a flexible start time.</p><p>Many departments give course reductions to get your research going. I'd suggest asking to spend your first semester teaching a graduate topics course based on your thesis research to pick up some PhD students followed by a semester with no classes to get you research program going.</p><p><b>Salary</b></p><p>This includes actual salary, which is also the base for future raises, and summer salary in the first couple of years. Feel free to ask for more salary, but often these numbers are fixed for new assistant professors. There is more give if you take an academic job later in your career. You could also say something like, "Well if you can't give me more salary maybe you could give me another semester of grad student support?"</p><p><b>Partner</b></p><p>It seems 80% of the time, a job candidate has a partner that needs accommodating. Don't wait until the end of negotiations, bring it up early. The more time we have, the better we can help. Doesn't matter what job they want--we know people and we know people who know people.</p><p><b>Thesis</b></p><p>Many schools won't hire you as an assistant professor if you haven't finished your thesis. Has to do with college rankings work. Don't worry--they will generally give you some other role with the same package until you finish. This might delay your tenure clock though.</p><p><b>Delayed start time</b></p><p>A January start is usually fine with good reason but if you weren't planning to start until the fall of 2022 why are you on the market this year? If you do get the department to hold a position for you, remember you are also making a commitment--this is not an opportunity to try again for something better.</p><p><b>Overall</b></p><p>You may not get all that you want after a negotiation--don't take it personally. You shouldn't necessarily choose the place that gives you the biggest package. It's far more important in the long run that you pick a place where you can best succeed both professionally and personally, and the package is just a small piece of that puzzle.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/negotiations.html"><span class="datestr">at May 06, 2021 01:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1628">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/">Self-fulfilling and self-negating predictions: a short tale of performativity in machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<blockquote class="wp-block-quote has-text-align-center"><p><em>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo.</em></p></blockquote>



<p>In 1998, Michel Callon wrote what would be the first in an ongoing series of controversial publications in economic sociology [1]. He was the first to propose the idea that “the economy is not embedded in society but in economics”. With this, he challenged the conventional view that economic theories and models passively observe markets and infer their behavior, just like laws of physics passively describe the principles governing natural phenomena. Instead, Callon argued that economic theories are <em>performative:</em> they induce the economy, creating the phenomena they aim to describe.</p>



<p>One example that is often cited in support of Callon’s claims is the impact of the celebrated Black-Scholes-Merton options pricing model [2, 3]. MacKenzie and Millo [4] investigated the role of this model in the economy and found that it “made itself true”. In their words,</p>



<p class="has-text-align-center"><em>“Black, Scholes, and Merton’s model did not describe an already existing world: when first formulated, its assumptions were quite unrealistic, and empirical prices differed systematically from the model. Gradually, though, the financial markets changed in a way that fitted the model”.</em></p>



<p>Indeed, participants in the market started making decisions assuming the market obeys the mathematical laws implied by the Black-Scholes-Merton model. As MacKenzie and Millo put it, “pricing models came to shape the very way participants thought and talked about options”.</p>



<p>This phenomenon — whereby models and predictions inform decision-making and thus alter the target of prediction itself — is by no means special to economic forecasts.</p>



<p>Predictive policing, for example, develops algorithms that use historical data to estimate the likelihood of crime at a given location. Those locations where criminal behavior is deemed likely by the system typically get more police patrols and surveillance in general. In a kind of self-fulfilling prophecy [5], these actions resulting from prediction might further increase the <em>perceived</em> crime rate at the patrolled locations, thus biasing the data used for future decisions.</p>



<p>A similar feedback loop arises in traffic predictions, when drivers decide which route to take based on the estimated time of arrival (ETA) calculated by a traffic prediction system. If the predictive system estimates low ETA for a given route, many drivers take the route, potentially leading to an overflow of traffic and making the ETA prediction inaccurate as a result. Contrary to the previous example, traffic predictions arguably exhibit a self-negating prophecy: low ETA might imply a longer travel time, and vice versa.</p>



<p>While the previous examples deal with qualitatively different feedback mechanisms, the interplay of predictions and decision-making is similar. First, one uses historical data to build a predictive model. Then, the predictions of the model feed into and inform consequential decisions. Finally, these decisions trigger changes in the environment, making future observations differ from those in the initial dataset.</p>



<p class="has-text-align-center"><img width="624" src="https://lh3.googleusercontent.com/C3qD-SK6AD4gDpkJznllugJS8OZaJwGvXSNi96qYKPu2ALr5vSxPMhteCScLq-CCdKOvh8bZ8KB-gvTRVkbZALVEuML0WhSV0pX8YM1MHupTLPjZ8PKEydiNu_h3iUdlVAYUcFDT" height="81" /></p>



<p>We refer to prediction problems that exhibit this feedback-loop behavior as <em>performative prediction</em> problems.</p>



<p>In the language of machine learning, such a change in patterns would often be called <em>distribution shift</em>. Notably, however, performative distribution shift is not due to external factors independent of the model, such as, say, when traffic patterns change due to seasonal effects. Rather, the distribution shift is triggered directly by the choice of predictive model. (Of course, distribution shifts can also be caused by a combination of external factors and model choice.)</p>



<p>To formalize performative prediction mathematically, it is instructive to contrast performative prediction problems with supervised learning problems. In supervised learning, the decision-maker observes pairs of features and outcomes <img width="72" src="https://lh5.googleusercontent.com/2U92MhGBO6DJqe607HL2T4uWicPXKFgrU2PoSaeymNLxbOteL6r_dhkuuFo91W2AXoAzrhxt8Ndg1jfhf8KVqZMbID1koi01cpGcXz-CTACfH_b54DohHMqpO2hJ5bEtfdE6v6Ag" height="15" /> drawn from a <em>fixed</em> distribution <img width="14" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" height="13" />. The key difference in performative prediction is that there is no longer an unknown static distribution generating observations; rather, data is drawn from a <em>model-dependent distribution</em> <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="17" />, where <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="12" /> is a parameter vector specifying the deployed model. For example, <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" /> could be the weights of a neural network, or a vector of linear regression coefficients. For a given choice of parameters <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />, <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="17" /> should be thought of as the distribution over features and outcomes that results from making decisions according to the model specified by <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />. In the context of the traffic prediction example, <img width="32" src="https://lh4.googleusercontent.com/HDm0VWvPNAUIdPfMSLc0ptEDS7Tph1p00WbC6TlSbQ8oYjK2nJSuSMiLuKN8IAA89L2H8rDLRPf9GWwc37Zb_0_qLxN-UuVVsz_hkBB43hQcTOtV4HUg30BtsHel_hpCFHfYJa6E" height="17" /> could be a distribution over traffic conditions and travel times, given that drivers make routing decisions in response to ETA forecasts by model <img width="8" src="https://lh3.googleusercontent.com/TkPLvHJ9t0soCQkx43reFppYsx4b0sJrNUBWd_Yx_rarIU4nS92GpBFBn-D_jhvDNAU04Aee4NMsVzHEozOLxXN11DMl4h1O2TvHfNih9IqHi0R3wM1otXYEp1MbkZlbI0faSV1i" height="13" />.</p>



<p>In supervised learning, the quality of a model <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" /> is typically measured by its <em>risk</em>, namely, the expected loss of the model on instances from distribution <img width="14" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" height="13" /> as measured via a loss function <img width="9" src="https://lh5.googleusercontent.com/iLwCF0E2qkddKEmiLhziUoOB171Na3z6aLjqWSYTPykfGKIx5PBtNT9qsZXIMPLc3hVXkkupbpqILbC4Fx9p_h5G8a1GmlYVL3rcx787SVO-24HKrL39LxAwPGaQ2i8wuPURBoyS" height="16" />:</p>



<p class="has-text-align-center"><img width="153" src="https://lh5.googleusercontent.com/vP7Ll-u4VopJmW24L2lMvqwf8N9bOrKaoLFDDlmcZgmML4CM8bM53_lMEG--5Om2mTYOQ5uNqUBkzlp2Hgd9i3EO8Y8bzC8kvkFgQKSfqOgHoWiMpK2MVasGJmdFTdERwIUg0RzK" height="26" /></p>



<p>Since performative prediction does not admit one true data-generating distribution, but rather a family of distributions <img width="58" src="https://lh6.googleusercontent.com/un2s0qA36MFKtNGHoDb7sQdPfToUoZjYy45nbwmSy2jQhSDJbiouNjATQWsSwDlpUF-PfOk1ElFPDb8Rsr_TAAB0GUemVV5Y_bG1EISEUD7M9N5RGxug1gGdvpXa-D70ATQSafzT" height="18" />, evaluating a model <img width="8" src="https://lh4.googleusercontent.com/zJsapVnRoEAmwRciJFyPDFM3pc3YOuB_6gM71GJX1Aa_9Tm09RLD4mj_DOEkH0CzKHFPV93LesWVf1AUH9QlwQoU0x07xusBHiT8-EeAApf2qMsufNc8Vuzc77LWu4bxEuYwgxXp" height="13" /> calls for a new risk concept. Arguably the most natural counterpart of the risk in supervised learning is the expected loss on the distribution that arises once the model is deployed and feeds into consequential decisions. This leads to the notion of <em>performative risk</em>, defined as:</p>



<p class="has-text-align-center"><img width="181" src="https://lh5.googleusercontent.com/zWCmnf2uG6j8LZG7w68p8y4I2wHcKc82ZULfh_MUmGc3YI6UByQP8dB3nBDugFmg8t0VybMz6RV4tzviUsIyHmVY1uUH1Xjv2XKTSEQGpFW3-o0e50-rxKeouEula7UNdqC6HYHr" height="30" /></p>



<p>Adopting the performative risk as the single overarching measure of quality, a model would be optimal if it minimizes the performative risk. While an appealing solution concept, performative optimality is difficult to achieve, seeing the double dependence of the risk function on <img width="8" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" height="13" />. One of the main computational difficulties is the fact that, even if the loss <img width="9" src="https://lh5.googleusercontent.com/WZWkgjmj0CvaBREmhCbwBw1i5JfL1Tviq96JbXG1SePOG-BXJ8uv_fUUE2KKJCw2qvd5zpA_GUhHZ9azV6AUENm0qVsKnTPMg6WzLvT5QGGLNwcLyhRU0iOhYtRy6Fylx66uMGRS" height="16" /> is convex in <img width="8" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" height="13" />, <img width="43" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" height="17" /> need not be convex. Prior work on strategic classification [6] implies a set sufficient conditions for <img width="43" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" height="17" /> to be convex in a binary classification context, and in recent work [7] we identified a complementary set of conditions when the family of distributions <img width="58" src="https://lh4.googleusercontent.com/Ti5C4ibszTKUqTM5evbcuaHojWHAN_Rq20XcnDlunJNG1M4lBKsjB7nJOKEN4lTSdWNmgKkcxqAwPPRahQcZ4uL8kCZHn_AuaB_Z9T35eztMk2CZzMcK50GMGnH69Jdjd0euwDtI" height="18" /> forms an appropriate location-scale family. That said, in many practical settings convexity might be an unrealistic and unnecessarily strong guarantee to aim for. As we know from present-day machine learning, even non-convex problems can sometimes be amenable to simple optimization algorithms. Understanding the optimization landscape of the performative risk beyond convex settings is a fruitful direction going forward.</p>



<p>A seemingly less ambitious target is to find a model that is <em>locally</em> optimal in some appropriate sense. For example, one could optimize for models that are optimal on the distribution that they induce:</p>



<p class="has-text-align-center"><img width="233" src="https://lh6.googleusercontent.com/ENuLNHEI7mNEDONR7rUA2FzwbkvqbINkwHJfO7jiiODrZ3_Ko9NZmJAt4Qv0cPNF4-_C68yI3CE4UMcFjzP8v5UHdu_LgBkdrCnUj4A-E6uccXlclWWZef2onVzsd5X2sHq5tTlh" height="22" /></p>



<p>We call a model that satisfies the fixed-point equation above <em>performatively stable</em>. Performative stability arises naturally when the decision-maker applies the heuristic of myopically updating the model based on the distribution resulting from the previous deployment:</p>



<p class="has-text-align-center"><img width="248" src="https://lh4.googleusercontent.com/RlZ0ZmF3E0VHyj_ktWb3QtAxsXTY_WjoGS29Jss3v_5cXnDVsWbeEvaTAA4l4tUYa5ccYvJPWcPXy5oLa3BnBT0Yqws89mNind-2UT8IA2ip3_pt3PbFhiNVcgIhgMS1omjfgzI1" height="20" /></p>



<p>If this retraining strategy converges, then it necessarily converges to a performatively stable solution. This is an appealing property, since it says that stability eliminates the need for retraining. Several existing works [8, 9, 10] have identified necessary and sufficient conditions for the above retraining heuristic, and some of its efficient approximations, to converge to a stable point. Roughly speaking, retraining converges to a stable solution if the loss is well-behaved and the performative feedback effects are not too strong. If either of those two conditions is violated, there is no guarantee of convergence.</p>



<p>In the language of game theory, one can think of performative prediction as a two-player game between a decision-maker, who decides which predictive model to deploy, and the model’s environment, which generates observations according to <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="16" />. If <img width="32" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" height="16" /> is thought of as the “best response” (according to some underlying utility) of the model’s environment to the deployment of model <img width="8" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" height="13" />, then a performatively stable solution corresponds to a <em>Nash</em> equilibrium, while a performatively optimal solution corresponds to a <em>Stackelberg</em> equilibrium with the decision-maker acting as the leader.</p>



<p>Only in special cases, such as in well-behaved zero-sum games, it is known that Nash equilibria coincide with Stackelberg equilibria. Therefore, whenever performative prediction is a well-behaved zero-sum game, all stable solutions are also performatively optimal. However, <em>performative prediction is typically not a zero-sum game</em>. For example, if the decision-maker’s loss simply measures predictive accuracy, it seems odd that the environment’s primary objective is to hurt the model’s accuracy. Indeed, a typical performative prediction problem is a general-sum game without much structure. This implies that stable solutions and performative optima can be <em>very different</em>. And, since naive retraining strategies only converge to stability, this means that such myopic updates can be an inadequate method of overcoming performative distribution shifts and achieving low performative risk. This observation further motivates understanding the optimization landscape of the performative risk, as well as developing efficient algorithms for optimizing it. Recent work has explored several algorithmic solutions [11, 7], appropriate in convex settings.</p>



<p>Performative prediction relates to many other areas beyond game theory, including bandits, reinforcement learning, control theory. These frameworks are flexible enough to capture performative prediction as a special case, however performativity arises via distinctive feedback mechanisms and as such deserves its own specialized analysis. There is a long way to go in understanding the properties of performative distribution shifts, how they connect to feedback mechanisms in other disciplines, and how to tackle these shifts in practice. Furthermore, it is unclear whether a single distribution <img width="32" src="https://lh6.googleusercontent.com/a-1tKaIQT9pJwi6WQ4eV4ZO3xcFy1e_joEh0tz1QdG-cj7x-60nDwfRyyztRSzjEaL-0xbImihmV5LDB8njPUhSlsIeyJABtowIBOktz79sRW7hAY8B75CePuDxp47xEFOZ68gSA" height="16" /> is expressive enough to describe the observations after model deployment; in practice there are different kinds of memory effects [12] and self-reinforcing loops that make the data distribution evolve with time, even when the model is kept fixed. Finally, to make the existing theoretical insights actionable, going forward we need to think about what is the right solution concept — both statistically and ethically — to optimize for in performative settings.</p>



<p><br />[1] M. Callon. Introduction: the embeddedness of economic markets in economics. <em>The Sociological Review</em>, 1998<br />[2] F. Black, M. Scholes. The pricing of options and corporate liabilities. <em>The Journal of Political Economy</em>, 1973<br />[3] R. C. Merton. Theory of rational option pricing. <em>The Bell Journal of Economics and Management Science</em>, 1973<br />[4] D. MacKenzie, Y. Millo. Constructing a market, performing theory: The historical sociology of a financial derivatives exchange. <em>American Journal of Sociology</em>, 2003<br />[5] D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, S. Venkatasubramanian. Runaway feedback loops in predictive policing. <em>ACM Conference on Fairness, Accountability and Transparency</em>, 2018<br />[6] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, Z. S. Wu. Strategic classification from revealed preferences. <em>ACM Conference on Economics and Computation</em>, 2018<br />[7] J. Miller, J. C. Perdomo, T. Zrnic. Outside the echo chamber: Optimizing the performative risk. <em>arXiv preprint</em>, 2021<br />[8] J. C. Perdomo, T. Zrnic, C. Mendler-Dünner, M. Hardt. Performative prediction. <em>International Conference on Machine Learning</em>, 2020<br />[9] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, M. Hardt. Stochastic optimization for performative prediction. <em>Conference on Neural Information Processing Systems</em>, 2020<br />[10] D. Drusvyatskiy, L. Xiao. Stochastic optimization with decision-dependent distributions. <em>arXiv preprint</em>, 2020<br />[11] Z. Izzo, L. Ying, J. Zou. How to learn when data reacts to your model: Performative gradient descent. <em>arXiv preprint</em>, 2021<br />[12] G. Brown, S. Hod, I. Kalemaj. Performative prediction in a stateful world. <em>arXiv preprint</em>, 2020</p></div>







<p class="date">
by tijanazrnic <a href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/"><span class="datestr">at May 06, 2021 12:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/066">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/066">TR21-066 |  Dimension-free Bounds and Structural Results in Communication Complexity | 

	Lianna Hambardzumyan, 

	Hamed Hatami, 

	Pooya Hatami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The purpose of this article is to initiate a systematic study of dimension-free relations between basic communication and query complexity measures  and various  matrix norms.  In other words, our goal is to obtain    inequalities that bound a parameter   solely as a function of another parameter. This is in contrast to perhaps the more common framework in communication complexity  where  poly-logarithmic dependencies on the number of input bits are   tolerated. 


Dimension-free bounds are also closely related to structural results, where one seeks to describe the structure of Boolean matrices and functions that have low complexity.  We prove such  theorems for several communication and query complexity measures as well as various matrix and operator norms. In several other cases we show that such bounds do not exist. 


We propose several conjectures, and establish that, in addition to applications in complexity theory, these   problems are central to  characterization of the idempotents of the algebra of Schur multipliers, and could lead to new extensions of  Cohen's celebrated idempotent theorem regarding the Fourier algebra.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/066"><span class="datestr">at May 05, 2021 06:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/tpdp21-cfp/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/tpdp21-cfp/">Call for Papers - Workshop on the Theory and Practice of Differential Privacy (TPDP 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Work on differential privacy spans a number of different research communities, including theoretical computer science, machine learning, statistics, security, law, databases, cryptography, programming languages, social sciences, and more.
Each of these communities may choose to publish their work in their own community’s venues, which could result in small groups of differential privacy researchers becoming isolated.
To alleviate these issues, we have the Workshop on the <a href="https://tpdp.journalprivacyconfidentiality.org/">Theory and Practice of Differential Privacy</a> (TPDP), which is intended to bring these subcommunities together under one roof (well, a virtual one at least for 2020 and 2021).</p>

<p>We have just posted the <a href="https://tpdp.journalprivacyconfidentiality.org/2021/TPDP2021CfP.pdf">Call for Papers</a> for <a href="https://tpdp.journalprivacyconfidentiality.org/2021/">TPDP 2021</a>, which will be a workshop affiliated with <a href="https://icml.cc/Conferences/2021/">ICML 2021</a>.
The submission deadline is Friday, May 28, 2021, Anywhere on Earth (conveniently, two days after the deadline for NeurIPS 2021).
Submissions are extended abstracts of up to four pages in length, and will undergo a lightweight review process, based mostly on relevance and interest to the differential privacy community.
The workshop is non-archival, so feel free to submit recent work at any stage of publication.
Submissions will be on <a href="https://openreview.net/group?id=ICML.cc/2021/Workshop/TPDP">OpenReview</a>, but since submitted work may be preliminary, the process will be “closed” similar to traditional review processes.
One goal of the workshop is to be inclusive and welcoming to newcomers to the differential privacy community, so please consider participating even if you are new to the field.</p>

<p>Most papers will be presented as posters at a (virtual) poster session, while a few papers will be selected for spotlight talks.
There will also be plenary talks by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a> (Hebrew University of Jerusalem) and <a href="https://www2.math.upenn.edu/~ryrogers/">Ryan Rogers</a> (LinkedIn).
The program co-chairs are <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a> and <a href="http://www.gautamkamath.com/">myself</a>.
Please submit your best work on differential privacy, and hope to see you there!</p>
<p align="center">
  <img src="https://differentialprivacy.org/images/Ligett.png" />
  <img src="https://differentialprivacy.org/images/Rogers.png" /> <br />
    <i>Invited speakers Katrina Ligett and Ryan Rogers</i>
</p></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/tpdp21-cfp/"><span class="datestr">at May 05, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18701">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/">How to Teach Math?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>The only way to learn mathematics is to do mathematics—Paul Halmos</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/an/" rel="attachment wp-att-18704"><img width="150" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/an-150x150.jpg?resize=150%2C150&amp;ssl=1" class="alignright size-thumbnail wp-image-18704" height="150" /></a></p>
<p>
Angie Hodge is an Associate Professor of mathematics at Northern Arizona University. Her interests as stated on her <a href="https://directory.nau.edu/person/amh952">website</a> are focused mainly on education, mentoring, and equity in the STEM disciplines. </p>
<p>
Today I thought we would discuss the issue of teaching and learning math and complexity theory.</p>
<p>
I am now retired from Georgia Tech. But I continue to be interested in how we can teach math. Dually how we can learn math. I still try to learn math—not for formal classes, nor in formal classes—but to help advance my own research. Even now I find that I need to learn some topics to help write a post or to solve a problem. </p>
<p>
On this blog with Ken I am also interested in still helping others learn. Some call that teaching. This is therefore the topic for today.</p>
<p>
</p><p></p><h2> First An Issue </h2><p></p>
<p></p><p>
One thing that drives me nuts about learning new math is what I will call the </p>
<blockquote><p><b> </b> <em> <i>“It is too obvious to state” principle.</i> </em>
</p></blockquote>
<p>What I mean is that when you start to learn material from some corner of math you get the key definitions and the main results. What I do not always get is some totally obvious ideas. Does this make any sense?</p>
<p>
Here is an example. Consider the class of sequences that are defined by linear <a href="https://en.wikipedia.org/wiki/Recurrence_relation">recurrences</a>. That is: a recursion that defines a sequence as a linear combination of earlier terms. One of the most famous is the Fibonacci numbers: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F_n+%3D+F_%7Bn-1%7D+%2B+F_%7Bn-2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  F_n = F_{n-1} + F_{n-2}. " class="latex" /></p>
<p>And <img src="https://s0.wp.com/latex.php?latex=%7BF_0%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F_0=0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF_1%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F_1=1}" class="latex" />.</p>
<p>
The property we are interested in is: </p>
<blockquote><p><b> </b> <em> <i>Is a product of two such sequences also of the same form?</i> </em>
</p></blockquote>
<p>This is a basic question, but it is not trivial to find the <a href="https://reader.elsevier.com/reader/sd/pii/0012365X79901869?token=C0E36B1EE6735F9AB6140E86924EB1A19F236C9507028E02CB912D21952F2438487C78DDBB68CC1541D2E9CF5754B8C1&amp;originRegion=us-east-1&amp;originCreation=20210415000540">answer</a>. </p>
<p>
</p><p></p><h2> How To Teach? </h2><p></p>
<p></p><p>
This is an ancient question that we all probably have thought about from time to time. It is complicated by the recent issue of most classes being done via online. Hodge has some nice stuff on teaching, especially on Inquiry-Based Learning (IBL). </p>
<ul>
<li>
See her <a href="https://www.artofmathematics.org/blogs/cvonrenesse/guest-blog-by-angie-hodge">blog</a>. <p></p>
</li><li>
See <a href="http://math.uchicago.edu/~boller/IBL/">this</a> for her comments on <em>Inquiry-Based Learning</em>. <p></p>
</li><li>
See her thoughts on <a href="https://www.maa.org/sites/default/files/Programs/MathFest2018_IPS_Hodge2.pdf">IBL</a>.
</li></ul>
<p>
</p><p></p><h2> Ken’s Similar Question </h2><p></p>
<p></p><p>
Ken writes: I am interested in manipulating logistic curves. Such curves not only undergird the chess rating system and the theory of standardized tests, they relate directly to the design of chess programs which I use to take my data. This decade’s neural chess-playing programs, following on from AlphaZero, express values directly in terms of the likelihood of winning, as numbers between 0 and 1, rather than traditional evaluation schemes built on counting 1.00 for a pawn, 3–3.5 for a knight or bishop, and so on. </p>
<p>
The new likelihood numbers follow a logistic curve. I especially want to convert from the evaluation numbers to them. After doing this conversion for various major chess programs, I would like to average their values as input to my predictive model. This involves taking averages of logistic curves. The curves can be generalized to the <a href="https://en.wikipedia.org/wiki/Generalised_logistic_function">form</a> named for Francis Richards: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+A+%2B+%5Cfrac%7BK-A%7D%7B%28C+%2B+Qe%5E%7B-Bx%7D%29%5E%7B1%2F%5Cnu%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x) = A + \frac{K-A}{(C + Qe^{-Bx})^{1/\nu}} " class="latex" /></p>
<p>for constant parameters <img src="https://s0.wp.com/latex.php?latex=%7BA%2CK%2CB%2CC%2CQ%2C%5Cnu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A,K,B,C,Q,\nu}" class="latex" />. The standard family has <img src="https://s0.wp.com/latex.php?latex=%7BA%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A=0}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BK%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K=1}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BC%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C=1}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cnu%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\nu=1}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7BQ%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q=1}" class="latex" />, with <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B}" class="latex" /> the central parameter determining the slope of the curve <img src="https://s0.wp.com/latex.php?latex=%7Bf%28t%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Bx%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(t) = \frac{1}{1+e^{-Bx}}}" class="latex" /> at <img src="https://s0.wp.com/latex.php?latex=%7Bx%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x=0}" class="latex" />. We can ask the basic question about intermediate families between the standard family and the most general kind:</p>
<blockquote><p><b> </b> <em> When does a linear combination of logistic curves belong to the same family? And if it doesn’t belong, how close to a member of the family does it come? </em>
</p></blockquote>
<p></p><p>
My point is that I have not been able to find an easy source for answers. This strikes me as exactly the kind of question for which sites like MathOverflow and StackExchange exist. But it is also a nice instructional exercise for training students in both the grit and adventure of mathematical research. </p>
<p>
One can also pose literally the same as Dick’s question above: how about a product of two curves <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%28x%29%2Cf_2%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_1(x),f_2(x)}" class="latex" /> from the family? The product still has values that run from <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> goes from <img src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-\infty}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%2B%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{+\infty}" class="latex" />. If we want the curves all to have value <img src="https://s0.wp.com/latex.php?latex=%7Bf%280%29+%3D+0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(0) = 0.5}" class="latex" /> then we can compose the product with a square root, viz. <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+%5Csqrt%7Bf_1%28x%29+f_2%28x%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(x) = \sqrt{f_1(x) f_2(x)}}" class="latex" />, thus taking a geometric rather than arithmetic mean. I mentioned other nuts-and-bolts problems about logistic curves in this <a href="https://rjlipton.wpcomstaging.com/2018/08/25/do-you-want-to-know-a-secret/">post</a> and its longer <a href="https://rjlipton.wpcomstaging.com/2018/09/07/sliding-scale-problems/">followup</a>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
The following video shows how not to teach math: <a href="https://www.youtube.com/watch?v=vU5LoCLGMdQ&amp;t=109s">The Kettles</a> do math.</p>
<p></p><p><br />
[some format and word tweaks]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/05/05/how-to-teach-math/"><span class="datestr">at May 05, 2021 01:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/065">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/065">TR21-065 |  One-way communication complexity and non-adaptive decision trees | 

	Nikhil Mande, 

	Swagato Sanyal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the relationship between various one-way communication complexity measures of a composed function with the analogous decision tree complexity of the outer function. We consider two gadgets: the AND function on 2 inputs, and the Inner Product on a constant number of inputs. Let $IP$ denote Inner Product on $2b$ bits.

1) If $f$ is a total Boolean function that depends on all of its inputs, the bounded-error one-way quantum communication complexity of $f \circ IP$ equals $\Omega(n(b-1))$.

2) If $f$ is a partial Boolean function, the deterministic one-way communication complexity of $f \circ IP$ is at least $\Omega(b \cdot D_{dt}^{\rightarrow}(f))$, where $D_{dt}^{\rightarrow}(f)$ denotes the non-adaptive decision tree complexity of $f$.

For our quantum lower bound, we show a lower bound on the VC-dimension of $f \circ IP$, and then appeal to a result of Klauck [STOC'00].
Our deterministic lower bound relies on a combinatorial result due to Frankl and Tokushige [Comb.'99].

It is known due to a result of Montanaro and Osborne [arXiv'09] that the deterministic one-way communication complexity of $f \circ XOR_2$ equals the non-adaptive parity decision tree complexity of $f$.
In contrast, we show the following with the gadget $AND_2$.

1) There exists a function for which even the randomized non-adaptive AND decision tree complexity of $f$ is exponentially large in the deterministic one-way communication complexity of $f \circ AND_2$.

2) For symmetric functions $f$, the non-adaptive AND decision tree complexity of $f$ is at most quadratic in the (even two-way) communication complexity of $f \circ AND_2$.

In view of the first point, a lower bound on non-adaptive AND decision tree complexity of $f$ does not lift to a lower bound on one-way communication complexity of $f \circ AND_2$.
The proof of the first point above uses the well-studied Odd-Max-Bit function.
For the second bullet, we first observe a connection between the one-way communication complexity of $f$ and the M\"obius sparsity of $f$, and then use a known lower bound on the M\"obius sparsity of symmetric functions. An upper bound on the non-adaptive AND decision tree complexity of symmetric functions follows implicitly from prior work on combinatorial group testing; for the sake of completeness, we include a proof of this result.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/065"><span class="datestr">at May 05, 2021 01:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3524">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2021/05/05/netecon-2021/">NetEcon 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>NetEcon’21,</strong> the 16th Workshop on the Economics of Networks, Systems and Computation, will take place on July 23, 2021. NetEcon’21 is a workshop of EC’21, the 22nd ACM Conference on Economics and Computation, which will be held on July 19-23, 2021, co-located with the 6th World Congress of the Game Theory Society. The aim of NetEcon is to foster discussions on the application of economic and game-theoretic models and principles to address challenges in the development of networks and network-based applications and services.</p>



<p>Details regarding submission rules and dates can be found at <a href="https://netecon21.gametheory.online/" target="_blank" rel="noreferrer noopener">https://netecon21.gametheory.online/</a>. A novelty compared with prior editions of the workshop is that papers that were already formatted for and submitted to EC’21 or SIGMETRICS’21 may retain this format (for submission) if submitted together with all the reviews (see submission guidelines for details).</p></div>







<p class="date">
by Kevin Leyton-Brown <a href="https://agtb.wordpress.com/2021/05/05/netecon-2021/"><span class="datestr">at May 05, 2021 01:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/064">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/064">TR21-064 |  Streaming approximation resistance of every ordering CSP | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Madhu Sudan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An ordering constraint satisfaction problem (OCSP) is given by a positive integer $k$ and a constraint predicate $\Pi$ mapping permutations on $\{1,\ldots,k\}$ to $\{0,1\}$. Given an instance of OCSP$(\Pi)$ on $n$ variables and $m$ constraints, the goal is to find an ordering of the $n$ variables that maximizes the number of constraints that are satisfied, where a constraint specifies a sequence of $k$ distinct variables and the constraint is satisfied by an ordering on the $n$ variables if the ordering induced on the $k$ variables in the constraint satisfies $\Pi$. Ordering constraint satisfaction problems capture natural problems including ''Maximum acyclic subgraph (MAS)'' and ''Betweenness''. 

In this work we consider the task of approximating the maximum number of satisfiable constraints in the (single-pass) streaming setting, where an instance is presented as a stream of constraints. We show that for every $\Pi$, OCSP$(\Pi)$ is approximation-resistant to $o(\sqrt{n})$-space streaming algorithms, i.e., algorithms using $o(\sqrt{n})$ space cannot distinguish streams where almost every constraint is satisfiable from streams where no ordering beats the random ordering by a noticeable amount. In the case of MAS our result shows that for every $\epsilon&gt;0$, MAS is not $1/2+\epsilon$-approximable. The previous best inapproximability result only ruled out a $3/4$ approximation.

Our results build on a recent work of Chou, Golovnev, Sudan, and Velusamy who show tight inapproximability results for some constraint satisfaction problems over arbitrary (finite) alphabets. We show that the hard instances from this earlier work have the following ''small-set expansion'' property: in every partition of the hypergraph formed by the constraints into small blocks, most of the hyperedges are incident on vertices from distinct blocks. By exploiting this combinatorial property, in combination with a natural reduction from CSPs over large finite alphabets to OCSPs, we give optimal inapproximability results for all OCSPs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/064"><span class="datestr">at May 04, 2021 09:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.let-all.com/blog/?p=55">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/letall.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/">ALT Highlights – An Interview with the PC Chairs of ALT 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fourth post in the series, an interview with ALT 2021 PC Chairs <a href="http://vtaly.net/">Vitaly Feldman</a> and <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>, written by <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a> and <a href="https://sites.google.com/view/michal-moshkovitz">Michal Moshkovitz</a>.</p>



<hr class="wp-block-separator" />



<p>We had the great opportunity to attend <em>The 32nd International Conference on Algorithmic Learning Theory</em>, held online between March 16-19, 2021, and co-chaired by Vitaly Feldman and Katrina Ligett. Vitaly is a research scientist at Apple AI Research and has done foundational works in machine learning and privacy-preserving data analysis. Katrina is an Associate Professor of Computer Science at the Hebrew University of Jerusalem and has done pivotal works in data privacy, algorithmic fairness, algorithmic game theory, and online algorithms. We asked for an interview with them about their experiences and perspectives as co-chairs, to which they kindly agreed. We are happy to share with the readers the excerpts of this interview.</p>



<p class="has-text-align-center"><img src="https://lh4.googleusercontent.com/7RUOfC-v06amphwIhfCYsQNH4jUg82AVsePZcbWO0D40DhSRk-Cf_wnXx9y5RI-qVYz6D-IRAxRzsQ9lWBpraofuggrTYC_sG40GehOCvSrQyZfj1khlMTVqK23NKOZueGcbK_xa" style="width: 225px;" />           <img width="194" src="https://lh4.googleusercontent.com/lUMmpb6Bcfq3bPljS7jYaF2TFaXRks64--IeslcdR3WzqnCtUETu-ymoOSxm7ys_6eyRFb2mGmd1mCe1boNHdxii0d1_UCthAEJy_eTsWsGQsFKpsV5snZe3Nm9g-QDy0JTnzPKx" height="252" /></p>



<p><strong>How it started</strong></p>



<p>How are chairs and program committees chosen? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: The chairs are selected by the Association for Algorithmic Learning Theory (AALT) Steering Committee (<a href="http://algorithmiclearningtheory.org/alt-steering-committee/">http://algorithmiclearningtheory.org/alt-steering-committee/</a>), and the chairs then select the program committee members. Vitaly and I brainstormed potential PC member names, solicited additional suggestions, and also considered the lists of people who have served on recent ALT and COLT PCs. In building the PC, we had many considerations in mind, including coverage of research areas, and various metrics of diversity.</p>



<p><strong>The chairs’ role</strong></p>



<p>What are the different tasks a chair has? What is the most difficult task?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: At a high level, the roles of the PC chairs are to build the PC, oversee the reviewing process and create the conference program. Practically, though, there are a lot of decisions that need to be discussed, emails to be sent, and a lot of organizational aspects to tend to—configuring the reviewing platform, sending reminders, chasing down late reviews, and so on. Vitaly and I have a very, very long joint “to do” list—and luckily, it’s now almost all crossed off! We also had additional responsibilities this year because of the move to the virtual conference format, including selecting the technologies, overseeing the pre-recording process, and much more.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I think the hardest and probably the most time-consuming is making the accept/reject decisions on papers.  For a large fraction of the papers arriving at the decision requires getting a sense of the results; understanding the main points in reviews, author responses and discussion (while calibrating them to the PC members and reviewers); ensuring that each paper is properly discussed by chasing reviewers, asking questions and often soliciting additional opinions. We also needed to come up with a set of criteria for deciding on borderline cases and make sure that these criteria are applied as consistently as possible. At the end it is a rather long and iterative process that luckily for us has converged to a program we are happy with.</p>



<p>How much time do you spend doing chair tasks? How do you balance chairing a conference (a massive amount of work) with all your other commitments? Do you turn down other service items you would generally accept, etc.?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: It’s difficult to estimate the number of hours, but I think we have been meeting regularly since early June 2020, and we’re only wrapping up our work now, in late March 2021. It’s a much longer-timeframe commitment than serving as a PC member. I actually am chairing a second conference this year, FORC, and together it makes for a pretty serious load. As a result, I have been declining all other conference-related service. I also have a couple of other pretty substantial service commitments, as well, so I just don’t have bandwidth this year for additional PC and Area Chair-type roles.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s hard to tell how much time we spent in total. My rough estimate is that it’s about a month of full-time work. I also had to decline most other service commitments during that period some of which I’d normally accept. Naturally, it also slows down other work so I definitely had to lean more on my collaborators in some of the ongoing projects <img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /></p>



<p>Can chairs bring their own personality into the conference? How? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: One area where the chairs enjoy freedom is in selecting the keynote and tutorial speakers. I’m biased, of course, but I think we chose very well, and all of these speakers (Joelle Pineau, Shay Moran, and Costis Daskalakis) gave excellent talks (they were recorded—check them out if you missed them)! We also were fortunate to be able to work with amazing partners who organized the mentoring workshop (Surbhi Goel, Nika Hagtalab, and Ellen Vitercik) and the Women in ML Theory event (Tosca Lechner and Ruth Urner). These aspects of the conference beyond the papers are a way for the chairs to express their priorities.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: The chairs have a lot of freedom in choosing how to run the review process, design the conference program and who else will be involved. So, inevitably, the chairs’ personalities and tastes end up being reflected in the final results. </p>



<p>Does the online conference impact the chair job? How? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: Typically, the PC chairs build the program, and then many details of organizing and running the conference get handed off to local organization chairs. But this year, since ALT was held virtually, there were many unusual tasks that fell to the PC chairs—not just the obvious ones like choosing the technologies and format and negotiating those contracts, but smaller things like chasing down authors who failed to upload their recordings, and developing instructions for people in various roles to interact with the conference platform.</p>



<p style="color: #0000ff;" class="has-text-color">In addition, COVID times placed strains on many people, which made it more challenging to recruit PC members, and resulted in a higher than usual rate of late reviews and PC drop-outs, which of course left us scrambling.</p>



<p>What motivates you to spend time on a conference service?</p>



<p style="color: #0000ff;" class="has-text-color">Katrina: We all rely on the conference system for our personal professional advancement, and for the advancement of our field as a whole. So we all owe that system our service, of course, according to our abilities, availability, and seniority. Also, it’s fun to get this different perspective on the conference review process and on the field. And it’s an honor to be entrusted with shepherding a conference for a year and hopefully nurturing its growth.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that it’s a mix of (1) contribution to the community I’m a member of and, perhaps, an opportunity to improve some of its processes (2) a learning experience that gives one a higher-level view of the research that is happening and people who do it (3) honor and recognition that come with the job.</p>



<p><strong>Awards </strong></p>



<p>How do you decide which papers were chosen as awards? </p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: A necessary condition for a paper to receive an award is that at least one of the PC members/reviewers assigned to the paper is excited about the results.  So we start by looking at papers with the highest scores (typically all papers that received at least one “strong accept”) and reading their reviews. This allowed us to narrow down the list to a set of 5-6 candidates. From those we selected the winners by learning more about the results and selecting those, we found the most significant and interesting for the community.</p>



<p><strong>The review process</strong></p>



<p>What are your thoughts about the current peer review process in ALT? What are the downsides and advantages? Do you have suggestions for improvement?</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: It is common that confidence in correctness of the results in a conference submission is based on higher-level sanity checks and general intuition of the reviewers. Naturally, the more interesting and important result the more likely it is to be scrutinized. At this ALT we did not run into a situation where the authors’ reputation affected our confidence in the correctness of the results. In case of concerns about correctness of an interesting result we would ask either an expert on the PC or an external expert to try to verify the result.</p>



<p class="has-luminous-vivid-orange-color has-text-color">ALT currently relies on a traditional theory conference model of reviewing and for a typical submission has several PC members who are experts in the subarea. The reviewing load is also relatively light (8 papers per PC member). So I think that the overall reviewing quality is pretty much as good as it gets in ML (and is similar to COLT). Naturally, the model is not perfect and there is still variation in the quality of individual reviews. This year many more reviewers and PC members were under unusual time pressure due to the pandemic so perhaps the variation was higher than usual.</p>



<p><strong>The future</strong></p>



<p>What are your suggestions for the next chair? </p>



<p style="color: #0000ff;" class="has-text-color">Katrina: Make sure you have a good co-chair. <img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> Vitaly has been a great partner for this process—fun to work with, reliable, always willing to pitch in even on the less-fun tasks, and I have great respect for his technical perspective.</p>



<p class="has-luminous-vivid-orange-color has-text-color">Vitaly: I agree that diversity of perspectives and expertise is useful in several ways. Most notably, it gives the chairs a wider network of people to select the PC from. But I completely agree with Katrina, that the most important thing is the ability of co-chairs to work well together: after all, it’s a lot of work and complicated decisions that need to be made jointly. Here, I couldn’t have asked for more: Katrina is amazing both professionally and personally. Working with her was definitely the highlight of being the ALT co-chair and learned a lot from her in the process as well.</p></div>







<p class="date">
by Gautam Kamath <a href="https://www.let-all.com/blog/2021/05/04/alt-highlights-an-interview-with-the-pc-chairs-of-alt-2021/"><span class="datestr">at May 04, 2021 04:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=559">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/">TCS+ talk: Wednesday, May 12 — Santhoshini Velusamy, Harvard University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://scholar.harvard.edu/santhoshiniv/home"><strong>Santhoshini Velusamy</strong></a> from Harvard University will speak about “<em>Classification of the approximability of all finite Max-CSPs in the dynamic streaming setting</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: A maximum constraint satisfaction problem, Max-CSP(F), is specified by a finite family of constraints F, where each constraint is of arity k. An instance of the problem on n variables is given by m applications of constraints from F to length-k subsequences of the n variables, and the goal is to find an assignment to the n variables that satisfies the maximum number of constraints. The class of Max-CSP(F) includes optimization problems such as Max-CUT, Max-DICUT, Max-3SAT, Max-q-Coloring, Unique Games, etc.</p>
<p>In this talk, I will present our recent dichotomy theorem on the approximability of Max-CSP(F) for every finite family F, in the single-pass dynamic streaming setting. In this setting, at each time step, a constraint is either added to or deleted from the stream. In the end, the streaming algorithm must estimate the maximum number of constraints that can be satisfied using space that is only polylogarithmic in n. No background in streaming algorithms or constraint satisfaction problems will be needed to enjoy this talk!</p>
<p>The talk will be based on <a href="https://eccc.weizmann.ac.il/report/2021/011/">this paper</a>, and <a href="https://eccc.weizmann.ac.il/report/2021/063/">this paper</a> with Chi-Ning Chou, Alexander Golovnev, and Madhu Sudan.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/05/04/tcs-talk-wednesday-may-12-santhoshini-velusamy-harvard-university/"><span class="datestr">at May 04, 2021 06:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/">Postdoc at UC Irvine (apply by June 4, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One Post-doctoral position is available under the guidance of Ioannis Panageas. The appointment is for one year and may be renewable if funding permits. Requirement is a Ph.D. in TCS/Theory of ML, or related field. Expertise can be demonstrated by 3 top-tier publications in venues like ICML, NeurIPS, AISTATS, STOC, FOCS, SODA, ICALP, EC. Anticipated starting date is October 1 2021 (negotiable).</p>
<p>Website: <a href="https://recruit.ap.uci.edu/JPF06615">https://recruit.ap.uci.edu/JPF06615</a><br />
Email: ipanagea@ics.uci.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/03/postdoc-at-uc-irvine-apply-by-june-4-2021/"><span class="datestr">at May 03, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/063">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/063">TR21-063 |  Approximability of all finite CSPs in the dynamic streaming setting | 

	Chi-Ning  Chou, 

	Alexander Golovnev, 

	Madhu Sudan, 

	Santhoshini Velusamy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A constraint satisfaction problem (CSP), Max-CSP$({\cal F})$, is specified by a finite set of constraints ${\cal F} \subseteq \{[q]^k \to \{0,1\}\}$ for positive integers $q$ and $k$. An instance of the problem on $n$ variables is given by $m$ applications of constraints from ${\cal F}$ to subsequences of the $n$ variables, and the goal is to find an assignment to the variables that satisfies the maximum number of constraints. In the $(\gamma,\beta)$-approximation version of the problem for parameters $0 \leq \beta &lt; \gamma \leq 1$, the goal is to distinguish instances where at least $\gamma$ fraction of the constraints can be satisfied from instances where at most $\beta$ fraction of the constraints can be satisfied. 

In this work we consider the approximability of this problem in the context of streaming algorithms and give a dichotomy result in the dynamic setting, where constraints can be inserted or deleted. Specifically,  for every family ${\cal F}$ and every $\beta &lt; \gamma$,  we show that either the approximation problem is solvable with polylogarithmic space in the dynamic setting, or not solvable with $o(\sqrt{n})$ space. We also establish tight inapproximability results for a broad subclass in the streaming insertion-only setting. Our work builds on, and significantly extends previous work by the authors who consider the special case of Boolean variables ($q=2$), singleton families ($|{\cal F}| = 1$) and where constraints may be placed on variables or their negations. Our framework extends non-trivially the previous work allowing us to appeal to richer norm estimation algorithms to get our algorithmic results. For our negative results we introduce new variants of the communication problems studied in the previous work, build new reductions for these problems, and extend the technical parts of previous works. In particular, previous works used Fourier analysis over the Boolean cube to prove their results and the results seemed particularly tailored to functions on Boolean literals (i.e., with negations). Our techniques surprisingly allow us to get to general $q$-ary CSPs without negations by appealing to the same Fourier analytic starting point over Boolean hypercubes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/063"><span class="datestr">at May 03, 2021 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7474459772601125760">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html">The Mythical Man-Month, Hen-Day, and Cat-Minute (Fred Brooks Turned 90)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><i> The Mythical Man-Month </i>is a great book which talks about the (obvious in retrospect) fact that putting more people on a project may slow it down. It was by Fred Brooks who turned 90 in April (he is still alive). It's a good read. I actually read it many years ago when I exchanged books with a Software Engineer I was dating- She lent me <i>The Mythical Man Month </i>which I found interesting, and I lent her <i>What is the name of this book by Smullyan </i>which she found amusing. Did this exchange of books help our relationship? We have now been married for many years, though its not clear if we can trace this to the exchange of books OR to the fact that she had KNUTH Volumes 1 and 3, and I had KNUTH Volume 2. </p><p> Fred Brooks: You have my thanks and of course Happy Birthday!</p><p>When I read The Mythical Man-Month  I was reminded of a math problem I heard as a kid: </p><p>If a hen-and-half lays an egg-and-a-half in a day-and-a-half then how many eggs can seven hen lay in seven days? </p><p>My answer: if (3/2) hens lay (3/2) eggs in (3/2) days then that's 2/3 of an egg per hen-day, so the answer is </p><p>49* 2/3 = 32 and 2/3 eggs.</p><p>It did not bother me one whit that (1) you can't have 2/3 of an egg, and (2) Just like adding more people might slow down a project, adding more hens might end up being a bad idea-- especially if they are all crowded into the same chicken-coop and hence don't feel much like laying eggs.</p><p>Who was the first person to note that adding <i>more</i> people or hens might be a bad idea? I do not know, but here is an amusing, yet realistic, article by Mark Twain on what I would call <i>The mythical</i> <i>cat-minute. </i>My advisor Harry Lewis send it to me in the midst of an email exchange about <i>The Mythical</i> <i>Man-Month.</i> He got it from a student of his, Larry Denenberg. Here it is: </p><p><br /></p><p>CATS AND RATS</p><pre>The following piece first appeared in ``The Monthly Packet'' of February
1880 and is reprinted in _The_Magic_of_Lewis_Carroll_, edited by John
Fisher, Bramhall House, 1973.


   If 6 cats kill 6 rats in 6 minutes, how many will be needed to kill
   100 rats in 50 minutes?

   This is a good example of a phenomenon that often occurs in working
   problems in double proportion; the answer looks all right at first, but,
   when we come to test it, we find that, owing to peculiar circumstances in
   the case, the solution is either impossible or else indefinite, and needing
   further data.  The 'peculiar circumstance' here is that fractional cats or
   rats are excluded from consideration, and in consequence of this the
   solution is, as we shall see, indefinite.

   The solution, by the ordinary rules of Double Proportion, is 12 cats.
   [Steps of Carroll's solution, in the notation of his time, omitted.]

   But when we come to trace the history of this sanguinary scene through all
   its horrid details, we find that at the end of 48 minutes 96 rats are dead,
   and that there remain 4 live rats and 2 minutes to kill them in: the
   question is, can this be done?

   Now there are at least *four* different ways in which the original feat,
   of 6 cats killing 6 rats in 6 minutes, may be achieved.  For the sake of
   clearness let us tabulate them:
      A.  All 6 cats are needed to kill a rat; and this they do in one minute,
          the other rats standing meekly by, waiting for their turn.
      B.  3 cats are needed to kill a rat, and they do it in 2 minutes.
      C.  2 cats are needed, and do it in 3 minutes.
      D.  Each cat kills a rat all by itself, and takes 6 minutes to do it.

   In cases A and B it is clear that the 12 cats (who are assumed to come
   quite fresh from their 48 minutes of slaughter) can finish the affair in
   the required time; but, in case C, it can only be done by supposing that 2
   cats could kill two-thirds of a rat in 2 minutes; and in case D, by
   supposing that a cat could kill one-third of a rat in two minutes.  Neither
   supposition is warranted by the data; nor could the fractional rats (even
   if endowed with equal vitality) be fairly assigned to the different cats.
   For my part, if I were a cat in case D, and did not find my claws in good
   working order, I should certainly prefer to have my one-third-rat cut off
   from the tail end.

   In cases C and D, then, it is clear that we must provide extra cat-power.
   In case C *less* than 2 extra cats would be of no use.  If 2 were supplied,
   and if they began killing their 4 rats at the beginning of the time, they
   would finish them in 12 minutes, and have 36 minutes to spare, during which
   they might weep, like Alexander, because there were not 12 more rats to
   kill.  In case D, one extra cat would suffice; it would kill its 4 rats in
   24 minutes, and have 26 minutes to spare, during which it could have killed
   another 4.  But in neither case could any use be made of the last 2
   minutes, except to half-kill rats---a barbarity we need not take into
   consideration.

   To sum up our results.  If the 6 cats kill the 6 rats by method A or B,
   the answer is 12; if by method C, 14; if by method D, 13.

   This, then, is an instance of a solution made `indefinite' by the
   circumstances of the case.  If an instance of the `impossible' be desired,
   take the following: `If a cat can kill a rat in a minute, how many would be
   needed to kill it in the thousandth part of a second?'  The *mathematical*
   answer, of course, is `60,000,' and no doubt less than this would *not*
   suffice; but would 60,000 suffice?  I doubt it very much.  I fancy that at
   least 50,000 of the cats would never even see the rat, or have any idea of
   what was going on.

   Or take this: `If a cat can kill a rat in a minute, how long would it be
   killing 60,000 rats?'  Ah, how long, indeed!  My private opinion is that
   the rats would kill the cat.
</pre><div><br /></div><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/the-mythical-man-month-hen-day-and-cat.html"><span class="datestr">at May 02, 2021 07:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/062">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/062">TR21-062 |  Improved Hitting Set for Orbit of ROABPs | 

	Vishwas Bhargava, 

	Sumanta Ghosh</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The orbit of an $n$-variate polynomial $f(\mathbf{x})$ over a field $\mathbb{F}$ is the set $\{f(A \mathbf{x} +  b)\,\mid\, A\in \mathrm{GL}({n,\mathbb{F}})\mbox{ and }\mathbf{b} \in \mathbb{F}^n\}$, and the orbit of a polynomial class is the union of orbits of all the polynomials in it. In this paper, we give improved constructions of hitting-sets for the orbit of read-once oblivious algebraic branching programs (ROABPs) and a related model. Over field with characteristic zero or greater than $d$, we construct a hitting set of size  $(ndw)^{O(w^2\log n\cdot \min\{w^2, d\log w\})}$  for the orbit of ROABPs in unknown variable order where $d$ is the individual degree and $w$ is the width of ROABPs. We also give hitting set of size $(ndw)^{O(\min\{w^2,d\log w\})}$ for the orbit of polynomials  computed by $w$-width ROABPs in any variable order. Our hitting sets improve upon the results of Saha and Thankey \cite{Saha-Thankey'21} who gave an $(ndw)^{O(d\log w)}$ size hitting set for the orbit of commutative ROABPs (a subclass of \textit{any-order} ROABPs) and $(nw)^{O(w^6\log n)}$ size hitting set for the orbit of multilinear ROABPs. Designing better hitting sets in large individual degree regime, for instance $d&gt;n$, was asked as an open problem by \cite{Saha-Thankey'21} and this work solves it in  small width setting. 

We prove some new rank concentration results by establishing \emph{low-cone concentration} for the polynomials over vector spaces, and they strengthen some previously known \emph{low-support} based rank concentrations shown in \cite{FSS14}. These new low-cone concentration results are crucial in our hitting set construction, and may be of independent interest. To the best of our knowledge, this is the first time when low-cone rank concentration has been used for designing hitting sets.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/062"><span class="datestr">at May 02, 2021 07:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18674">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/">Test of Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Time is the ultimate critic. What future generations think of us and our work ultimately determines our standing or lack of it— Stewart Stafford</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/bk/" rel="attachment wp-att-18676"><img width="150" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/bk-150x150.png?resize=150%2C150&amp;ssl=1" class="alignright size-thumbnail wp-image-18676" height="150" /></a></p>
<p>
Bobby Kleinberg just reached out to those of us who post from time to time. He wanted some help in announcing a new STOC Test of Time Award. </p>
<p>
So today, Ken and I put this together. </p>
<p>Bobby said: </p>
<blockquote><p><b> </b> <em> As with FOCS, three awards will be given: one for papers from approximately 10 years ago, one for approximately 20 years ago, and one for approximately 30 years ago. The selection committee for this year’s award will be Joe Halpern, Mihalis Yannakakis, and Salil Vadhan. </em>
</p></blockquote>
<p></p><p>
I would suggest that one of Bobby’s papers could fit this award: </p>
<p>Group-theoretic algorithms for matrix multiplication <br />
Henry Cohn, Robert Kleinberg, Balazs Szegedy, Christopher Umans </p>
<p>
Well, I can say that without being out of order <em>here</em> because that paper was in FOCS, not STOC.</p>
<p>
</p><p></p><h2> Criteria </h2><p></p>
<p>
</p><li>
<i>Area</i>: Opening up a new area of research <p></p>
</li><li>
<i>Proof</i>: Introducing new proof techniques <p></p>
</li><li>
<i>Use</i>: Solving a problem of lasting importance <p></p>
</li><li>
<i>Else</i>: Stimulating advances in other areas of computer science or in other disciplines. <p></p>
<p>
Go here for details on how to <a href="https://sigact.org/prizes/stoc_tot.html">nominate</a>. By the way: Another test of Time is that the nominations are due relatively soon—May 24. So if you wish to nominate some paper please act soon. </p>
<p>
Ken notes that the first criterion could also be called <i>Leadership</i>, the second always comes with an element of <i>Surprise</i>, and the last two have aspects of <i>Applicability</i> and <i>Practicality</i>.  Adding those to my terms makes a double acronym <i>APPLAUSE</i>.</p>
<p>
</p><p></p><h2> Early Early Years </h2><p></p>
<p></p><p>
I have been around long enough to fit the a 30++ years category, and Ken almost <a href="https://rjlipton.wpcomstaging.com/2021/04/22/ken-turns-40/">ditto</a>. Here are some opinions on the early days. Those papers with<br />
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1" alt="" class="alignleft  wp-image-18682" /></a><br />
are an absolute must include—I hope you agree.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc81.html">1981</a>:</p>
<p>
Space-Bounded Probabilistic Turing Machine Complexity Classes Are Closed under Complement <br />
<i>Use</i>.</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc82.html">1982</a>:</p>
<p>
Shafi Goldwasser, Silvio Micali <br />
Probabilistic Encryption and How to Play Mental Poker Keeping Secret All Partial Information <br />
<i>Area</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800061">1983</a>:</p>
<p>
Miklós Ajtai, Janos Komlós, and Endre Szemerédi <br />
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1" alt="" class="alignleft  wp-image-18682" /></a>An <img src="https://s0.wp.com/latex.php?latex=%7B0%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0(n \log n)}" class="latex" /> sorting network <br />
<i>Use</i> and <i>Proof</i>.</p>
<p>
Larry Stockmeyer <br />
The complexity of approximate counting <br />
<i>Use</i>.</p>
<p>
<a href="https://dl.acm.org/doi/proceedings/10.1145/800057">1984</a>:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1" alt="" class="alignleft  wp-image-18682" /></a>Les Valiant <br />
A theory of the learnable <br />
<i>Area</i> and <i>Else</i>.</p>
<p>
Narendra Karmarkar <br />
<a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/ll/" rel="attachment wp-att-18682"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/ll-150x150.png?w=40&amp;ssl=1" alt="" class="alignleft  wp-image-18682" /></a>A new polynomial-time algorithm for linear programming <br />
<i>Use</i> and <i>Proof</i>.</p>
<p>
Of course, these are my own opinions (with concurrence from Ken) and do not reflect those of organizations we belong to.</p>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Ken thinks that one way not to be asked here about my own papers is to mention one, so here goes:</p>
<p>
<a href="https://dblp.org/db/conf/stoc/stoc80.html">1980</a></p>
<p>
Ravindran Kannan, Richard Lipton <br />
The Orbit Problem is Decidable <br />
<i>Proof</i>.  Ken adds: Could also be <i>Surprise</i> because we not only showed decidable but in polynomial time.  But the real test of time here may be whether (despite some technical limitations) it proves useful to the great recent interest in adjacent kinds of orbit problems in <a href="https://rjlipton.wpcomstaging.com/2018/06/06/princeton-is-invariant/">invariant</a> and <a href="https://rjlipton.wpcomstaging.com/2015/07/12/the-long-reach-of-reachability/">reachability</a> theory.</p></li></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/04/30/test-of-time/"><span class="datestr">at April 30, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
