<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at February 24, 2021 03:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/027">TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/027"><span class="datestr">at February 24, 2021 02:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5350">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5350">Stop emailing my utexas address</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, you <span class="has-inline-color has-vivid-red-color">MUST</span> send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve the problem.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5350"><span class="datestr">at February 23, 2021 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/026">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/026">TR21-026 |  Conditional Dichotomy of Boolean Ordered Promise CSPs | 

	Joshua Brakensiek, 

	Venkatesan Guruswami, 

	Sai Sandeep</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Promise Constraint Satisfaction Problems (PCSPs) are a generalization of Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a weak form and given a CSP instance, the objective is to distinguish if the strong form can be satisfied vs. even the weak form cannot be satisfied. Since their formal introduction by Austrin, Guruswami, and Håstad, there has been a flurry of works on PCSPs, including recent breakthroughs in approximate graph coloring. The key tool in studying PCSPs is the algebraic framework developed in the context of CSPs where the closure properties of the satisfying solutions known as *polymorphisms* are analyzed. 
    
    The polymorphisms of PCSPs are significantly richer than CSPs---this is illustrated by the fact that even in the Boolean case, we still do not know if there exists a dichotomy result for PCSPs analogous to Schaefer's dichotomy result for CSPs. In this paper, we study a special case of Boolean PCSPs, namely Boolean *Ordered* PCSPs where the Boolean PCSPs have the predicate $x \leq y$. In the algebraic framework, this is the special case of Boolean PCSPs when the polymorphisms are *monotone* functions. We prove that Boolean Ordered PCSPs exhibit a computational dichotomy assuming the Rich $2$-to-$1$ Conjecture due to Braverman, Khot, and Minzer, which is a perfect completeness surrogate of the Unique Games Conjecture. 
    
    In particular, assuming the Rich $2$-to-$1$ Conjecture, we prove that a Boolean Ordered PCSP can be solved in polynomial time if for every $\epsilon &gt;0$, it has polymorphisms where each coordinate has *Shapley value* at most $\epsilon$, else it is NP-hard. The algorithmic part of our dichotomy result is based on a structural lemma showing that Boolean monotone functions with each coordinate having low Shapley value have arbitrarily large threshold functions as minors. The hardness part proceeds by showing that the Shapley value is consistent under a uniformly random $2$-to-$1$ minor. As a structural result of independent interest, we construct an example to show that the Shapley value can be inconsistent under an adversarial $2$-to-$1$ minor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/026"><span class="datestr">at February 23, 2021 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6398537358110172858">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html">Good Names and Bad Names of Game Shows and theorems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In my post on Alex Trebek, see <a href="https://blog.computationalcomplexity.org/2020/11/alex-trebekwhat-is-todays-post-about.html">here</a>, I noted that <i>Jeopardy!</i> is not a good name for the game show since it doesn't tell you much about the show. Perhaps <i>Answers and Questions </i>is a better name.</p><p>The following game shows have names that tell you something about the game and hence have better names: </p><p>Wheel of Fortune, The Price is Right, Lets make a Deal, Beautiful women have suitcases full of money (the original name for Deal-No Deal), Win Ben Stein's Money, Beat the Geeks. </p><p>In Math we often name a concept  after a person. While this may be a good way to honor someone, the name does not tell us much about the concept and it leads to statements like:</p><p><br /></p><p><i>A Calabi-Yau manifold is a compact complex Kahler manifold with a trivial first Chern class. </i></p><p><i>A Kahler manifold is a Hermitian manifold for which the Hermitian form is closed.</i></p><p><i>A Hermitian manifold is the complex analog of the Riemann manifold. </i></p><p>(These examples are from an article I will point to later---I do not understand <i>any </i>of these terms, though I once knew what a <i>Riemann manifold</i> was. I heard the term <i>Kahler Manifold </i>in the song <a href="https://www.youtube.com/watch?v=2rjbtsX7twc">Bohemian Gravity</a>.  It's at about the 4 minute 30 second place.) </p><p>While I am amused by the name <i>Victoria Delfino Problems</i> (probably the only realtor who has problems in math named after her, see my post <a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">here</a>) it's not a descriptive way to name open problems in descriptive set theory. </p><p><br /></p><p>Sometimes  a name becomes SO connected to a concept that it IS descriptive, e.g.:</p><p><i>The first proof of VDW's theorem yields ACKERMAN-LIKE bounds. </i></p><p>but you cannot count on that happening AND it is only descriptive to people already somewhat in the field. </p><p><br /></p><p>What to do? <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">This</a> article makes the  ballian point that we should   STOP DOING THIS and that the person who first proves the theorem should name it in a way that tells you something about the concept. I would agree. But this can still be hard to really do.</p><p><br /></p><p>In my book on Muffin Mathematics (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>) I have a sequence of methods called</p><p>Floor Ceiling, Half, Mid, Interval, Easy-Buddy-Match, Hard-Buddy-Match, Gap, Train. </p><p>There was one more method that I didn't quite name, but I used the phrase `Scott Muffin Problem' to honors Scott Huddleton who came up with the method, in my description of it. </p><p>All but the last concept were given ballian names.  Even so, you would need to read the book to see why the names make sense. Still, that would be easier than trying to figure out what a Calabi-Yau manifold is. </p><p><br /></p><p></p><br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html"><span class="datestr">at February 23, 2021 05:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10689">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10689">Mining EL Bases with Adaptable Role Depth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ricardo Guimarães, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ozaki:Ana.html">Ana Ozaki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Persia:Cosimo.html">Cosimo Persia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sertkaya:Baris.html">Baris Sertkaya</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10689">PDF</a><br /><b>Abstract: </b>In Formal Concept Analysis, a base for a finite structure is a set of
implications that characterizes all valid implications of the structure. This
notion can be adapted to the context of Description Logic, where the base
consists of a set of concept inclusions instead of implications. In this
setting, concept expressions can be arbitrarily large. Thus, it is not clear
whether a finite base exists and, if so, how large concept expressions may need
to be. We first revisit results in the literature for mining EL bases from
finite interpretations. Those mainly focus on finding a finite base or on
fixing the role depth but potentially losing some of the valid concept
inclusions with higher role depth. We then present a new strategy for mining EL
bases which is adaptable in the sense that it can bound the role depth of
concepts depending on the local structure of the interpretation. Our strategy
guarantees to capture all EL concept inclusions holding in the interpretation,
not only the ones up to a fixed role depth.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10689"><span class="datestr">at February 23, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10568">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10568">TS-Reconfiguration of Dominating Sets in circle and circular-arc graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bousquet:Nicolas.html">Nicolas Bousquet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Joffard:Alice.html">Alice Joffard</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10568">PDF</a><br /><b>Abstract: </b>We study the dominating set reconfiguration problem with the token sliding
rule. It consists, given a graph G=(V,E) and two dominating sets D_s and D_t of
G, in determining if there exists a sequence S=&lt;D_1:=D_s,...,D_l:=D_t&gt; of
dominating sets of G such that for any two consecutive dominating sets D_r and
D_{r+1} with r&lt;t, D_{r+1}=(D_r\ u) U v, where uv is an edge of G.
</p>
<p>In a recent paper, Bonamy et al studied this problem and raised the following
questions: what is the complexity of this problem on circular arc graphs? On
circle graphs? In this paper, we answer both questions by proving that the
problem is polynomial on circular-arc graphs and PSPACE-complete on circle
graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10568"><span class="datestr">at February 23, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10509">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10509">An Optimal Inverse Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Alex.html">Alex Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Guy.html">Guy Moshkovitz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10509">PDF</a><br /><b>Abstract: </b>We prove that the partition rank and the analytic rank of tensors are equal
up to a constant, over any large enough finite field. The proof constructs
rational maps computing a partition rank decomposition for successive
derivatives of the tensor, on an open subset of the kernel variety associated
with the tensor. This largely settles the main question in the "bias implies
low rank" line of work in higher-order Fourier analysis, which was reiterated
by Kazhdan and Ziegler, Lovett, and others.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10509"><span class="datestr">at February 23, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10474">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10474">Towards the k-server conjecture: A unifying potential, pushing the frontier to the circle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coester:Christian.html">Christian Coester</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koutsoupias:Elias.html">Elias Koutsoupias</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10474">PDF</a><br /><b>Abstract: </b>The $k$-server conjecture, first posed by Manasse, McGeoch and Sleator in
1988, states that a $k$-competitive deterministic algorithm for the $k$-server
problem exists. It is conjectured that the work function algorithm (WFA)
achieves this guarantee, a multi-purpose algorithm with applications to various
online problems. This has been shown for several special cases: $k=2$,
$(k+1)$-point metrics, $(k+2)$-point metrics, the line metric, weighted star
metrics, and $k=3$ in the Manhattan plane.
</p>
<p>The known proofs of these results are based on potential functions tied to
each particular special case, thus requiring six different potential functions
for the six cases. We present a single potential function proving
$k$-competitiveness of WFA for all these cases. We also use this potential to
show $k$-competitiveness of WFA on multiray spaces and for $k=3$ on trees.
While the DoubleCoverage algorithm was known to be $k$-competitive for these
latter cases, it has been open for WFA. Our potential captures a type of lazy
adversary and thus shows that in all settled cases, the worst-case adversary is
lazy. Chrobak and Larmore conjectured in 1992 that a potential capturing the
lazy adversary would resolve the $k$-server conjecture.
</p>
<p>To our major surprise, this is not the case, as we show (using connections to
the $k$-taxi problem) that our potential fails for three servers on the circle.
Thus, our potential highlights laziness of the adversary as a fundamental
property that is shared by all settled cases but violated in general. On the
one hand, this weakens our confidence in the validity of the $k$-server
conjecture. On the other hand, if the $k$-server conjecture holds, then we
believe it can be proved by a variant of our potential.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10474"><span class="datestr">at February 23, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10317">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10317">On guarding polygons with holes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alipour:Sharareh.html">Sharareh Alipour</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10317">PDF</a><br /><b>Abstract: </b>There is an old conjecture by Shermer \cite{sher} that in a polygon with $n$
vertices and $h$ holes, $\lfloor \dfrac{n+h}{3} \rfloor$ vertex guards are
sufficient to guard the entire polygon. The conjecture is proved for $h=1$ by
Shermer \cite{sher} and Aggarwal \cite{aga} seperately.
</p>
<p>In this paper, we prove a theorem similar to the Shermer's conjecture for a
special case where the goal is to guard the vertices of the polygon (not the
entire polygon) which is equivalent to finding a dominating set for the
visibility graph of the polygon. Our proof also guarantees that the selected
vertex guards also cover the entire outer boundary (outer perimeter of the
polygon) as well.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10317"><span class="datestr">at February 23, 2021 10:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10261">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10261">Bayesian Online Matching: Approximating the Optimal Online Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadimitriou:Christos.html">Christos Papadimitriou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pollner:Tristan.html">Tristan Pollner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saberi:Amin.html">Amin Saberi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wajc:David.html">David Wajc</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10261">PDF</a><br /><b>Abstract: </b>The rich literature on online Bayesian selection problems has long focused on
so-called prophet inequalities, which compare the gain of an online algorithm
to that of a "prophet" who knows the future. An equally-natural, though
significantly less well-studied benchmark is the optimum online algorithm,
which may be omnipotent (i.e., computationally-unbounded), but not omniscient.
What is the computational complexity of the optimum online? How well can a
polynomial-time algorithm approximate it?
</p>
<p>Motivated by applications in ride hailing, we study the above questions for
the online stochastic maximum-weight matching problem under vertex arrivals.
This problem was recently introduced by Ezra, Feldman, Gravin and Tang (EC'20),
who gave a $1/2$-competitive algorithm for it. This is the best possible ratio,
as this problem is a generalization of the original single-item prophet
inequality.
</p>
<p>We present a polynomial-time algorithm which approximates optimal online
within a factor of $0.51$, beating the best-possible prophet inequality. At the
core of our result are a new linear program formulation, an algorithm that
tries to match the arriving vertices in two attempts, and an analysis that
bounds the correlation resulting from the second attempts. In contrast, we show
that it is PSPACE-hard to approximate this problem within some constant $\alpha
&lt; 1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10261"><span class="datestr">at February 23, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10245">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10245">ALTO: Adaptive Linearized Storage of Sparse Tensors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Helal:Ahmed_E=.html">Ahmed E. Helal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laukemann:Jan.html">Jan Laukemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Checconi:Fabio.html">Fabio Checconi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tithi:Jesmin_Jahan.html">Jesmin Jahan Tithi</a>, Teresa Ranadive, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Petrini:Fabrizio.html">Fabrizio Petrini</a>, Jeewhan Choi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10245">PDF</a><br /><b>Abstract: </b>The analysis of high-dimensional sparse data is becoming increasingly popular
in many important domains. However, real-world sparse tensors are challenging
to process due to their irregular shapes and data distributions. We propose the
Adaptive Linearized Tensor Order (ALTO) format, a novel mode-agnostic (general)
representation that keeps neighboring nonzero elements in the multi-dimensional
space close to each other in memory. To generate the indexing metadata, ALTO
uses an adaptive bit encoding scheme that trades off index computations for
lower memory usage and more effective use of memory bandwidth. Moreover, by
decoupling its sparse representation from the irregular spatial distribution of
nonzero elements, ALTO eliminates the workload imbalance and greatly reduces
the synchronization overhead of tensor computations. As a result, the parallel
performance of ALTO-based tensor operations becomes a function of their
inherent data reuse. On a gamut of tensor datasets, ALTO outperforms an oracle
that selects the best state-of-the-art format for each dataset, when used in
key tensor decomposition operations. Specifically, ALTO achieves a geometric
mean speedup of 8X over the best mode-agnostic format, while delivering a
geometric mean compression ratio of more than 4X relative to the best
mode-specific format.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10245"><span class="datestr">at February 23, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10196">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10196">Approximating the Log-Partition Function</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cosson:Romain.html">Romain Cosson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Devavrat.html">Devavrat Shah</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10196">PDF</a><br /><b>Abstract: </b>Variational approximation, such as mean-field (MF) and tree-reweighted (TRW),
provide a computationally efficient approximation of the log-partition function
for a generic graphical model. TRW provably provides an upper bound, but the
approximation ratio is generally not quantified.
</p>
<p>As the primary contribution of this work, we provide an approach to quantify
the approximation ratio through the property of the underlying graph structure.
Specifically, we argue that (a variant of) TRW produces an estimate that is
within factor $\frac{1}{\sqrt{\kappa(G)}}$ of the true log-partition function
for any discrete pairwise graphical model over graph $G$, where $\kappa(G) \in
(0,1]$ captures how far $G$ is from tree structure with $\kappa(G) = 1$ for
trees and $2/N$ for the complete graph over $N$ vertices. As a consequence, the
approximation ratio is $1$ for trees, $\sqrt{(d+1)/2}$ for any graph with
maximum average degree $d$, and $\stackrel{\beta\to\infty}{\approx}
1+1/(2\beta)$ for graphs with girth (shortest cycle) at least $\beta \log N$.
In general, $\kappa(G)$ is the solution of a max-min problem associated with
$G$ that can be evaluated in polynomial time for any graph.
</p>
<p>Using samples from the uniform distribution over the spanning trees of G, we
provide a near linear-time variant that achieves an approximation ratio equal
to the inverse of square-root of minimal (across edges) effective resistance of
the graph. We connect our results to the graph partition-based approximation
method and thus provide a unified perspective.
</p>
<p>Keywords: variational inference, log-partition function, spanning tree
polytope, minimum effective resistance, min-max spanning tree, local inference
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10196"><span class="datestr">at February 23, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10174">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10174">Restorable Shortest Path Tiebreaking for Edge-Faulty Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodwin:Greg.html">Greg Bodwin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parter:Merav.html">Merav Parter</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10174">PDF</a><br /><b>Abstract: </b>The restoration lemma by Afek, Bremler-Barr, Kaplan, Cohen, and Merritt
[Dist. Comp. '02] proves that, in an undirected unweighted graph, any
replacement shortest path avoiding a failing edge can be expressed as the
concatenation of two original shortest paths. However, the lemma is
tiebreaking-sensitive: if one selects a particular canonical shortest path for
each node pair, it is no longer guaranteed that one can build replacement paths
by concatenating two selected shortest paths. They left as an open problem
whether a method of shortest path tiebreaking with this desirable property is
generally possible.
</p>
<p>We settle this question affirmatively with the first general construction of
restorable tiebreaking schemes. We then show applications to various problems
in fault-tolerant network design. These include a faster algorithm for subset
replacement paths, more efficient fault-tolerant (exact) distance labeling
schemes, fault-tolerant subset distance preservers and $+4$ additive spanners
with improved sparsity, and fast distributed algorithms that construct these
objects. For example, an almost immediate corollary of our restorable
tiebreaking scheme is the first nontrivial distributed construction of sparse
fault-tolerant distance preservers resilient to three faults.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10174"><span class="datestr">at February 23, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2102.10169">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2102.10169">Co-clustering Vertices and Hyperedges via Spectral Hypergraph Partitioning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Yu.html">Yu Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Boning.html">Boning Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Segarra:Santiago.html">Santiago Segarra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2102.10169">PDF</a><br /><b>Abstract: </b>We propose a novel method to co-cluster the vertices and hyperedges of
hypergraphs with edge-dependent vertex weights (EDVWs). In this hypergraph
model, the contribution of every vertex to each of its incident hyperedges is
represented through an edge-dependent weight, conferring the model higher
expressivity than the classical hypergraph. In our method, we leverage random
walks with EDVWs to construct a hypergraph Laplacian and use its spectral
properties to embed vertices and hyperedges in a common space. We then cluster
these embeddings to obtain our proposed co-clustering method, of particular
relevance in applications requiring the simultaneous clustering of data
entities and features. Numerical experiments using real-world data demonstrate
the effectiveness of our proposed approach in comparison with state-of-the-art
alternatives.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2102.10169"><span class="datestr">at February 23, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18152">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/">Riemann Hypothesis—Why So Hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/voroninsteklov/" rel="attachment wp-att-18177"><img width="142" alt="" src="https://rjlipton.files.wordpress.com/2021/02/voroninsteklov.jpg?w=142&amp;h=190" class="alignright wp-image-18177" height="190" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Steklov Institute memorial <a href="http://www.mi-ras.ru/index.php?c=inmemoria&amp;l=1">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his amazing 1975 result about the Riemann zeta function <a href="https://rjlipton.wordpress.com/2012/12/04/the-amazing-zeta-code/">here</a>. Others call the result the amazing theorem. I (Dick) am getting old—I almost forgot that we did a <a href="https://rjlipton.wordpress.com/2016/12/20/hunting-complexity-in-zeta/">post</a> on his theorem again over four years ago. </p>
<p>
Today I thought we would recall his theorem, sketch why the theorem is true, and then discuss some extensions.</p>
<p>
Starting with Alan Turing we have been interested in universal objects. Turing famously <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">proved</a> that there are universal machines: these can simulate any other machine on any input. Martin Davis has an entire <a href="https://www.amazon.com/Universal-Computer-Road-Leibniz-Turing/dp/0393047857">book</a> on this subject. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/davis/" rel="attachment wp-att-18156"><img width="235" alt="" src="https://rjlipton.files.wordpress.com/2021/02/davis.jpg?w=235&amp;h=375" class="aligncenter wp-image-18156" height="375" /></a>
</td>
</tr>
</tbody></table>
<p>
Universal objects are basic to complexity theory. Besides Turing’s notion, a universal property is key to the definition of NP-complete. A set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> in NP is NP-complete provided all other sets in NP can be reduced to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> in polynomial time. Michael Nielsen once began a <a href="https://www.quantamagazine.org/the-physical-origin-of-universal-computing-20151027/">discussion</a> of universality in this amusing fashion:</p>
<blockquote><p><b> </b> <em> Imagine you’re shopping for a new car, and the salesperson says, “Did you know, this car doesn’t just drive on the road.” “Oh?” you reply. “Yeah, you can also use it to do other things. For instance, it folds up to make a pretty good bicycle. And it folds out to make a first-rate airplane. Oh, and when submerged it works as a submarine. And it’s a spaceship too!” </em>
</p></blockquote>
<p></p><h2> Voronin’s Insight </h2><p></p>
<p>
In 1975 Voronin had the brilliant insight that the Riemann zeta <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function has an interesting universality property.  Roughly speaking, it says that a wide class of analytic functions can be approximated by shifts <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%2Bit%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s+it)}" class="latex" title="{\zeta(s+it)}" /> with real <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" />. Recall 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " class="latex" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt;1}" class="latex" title="{\Re(s) &gt;1}" />, and it has an analytic extension for all other values but <img src="https://s0.wp.com/latex.php?latex=%7Bs%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s=1}" class="latex" title="{s=1}" />.</p>
<p>
The intense interest in the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function started in 1859 with Bernhard Riemann’s breakthrough <a href="https://en.wikipedia.org/wiki/On_the_Number_of_Primes_Less_Than_a_Given_Magnitude">article</a>. This was the first statement of what we call the Riemann Hypothesis (RH).</p>
<p>
In over a century of research on RH before Voronin’s theorem, many identities, many results, many theorems were proved about the zeta function. But none saw that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function was universal before Voronin. Given the zeta function’s importance in understanding the structure of prime numbers this seems to be surprising. </p>
<p>
Before we define the universal property I thought it might be useful to state a related <a href="https://arxiv.org/pdf/1305.3933.pdf">property</a> that the <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function has:</p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> is a polynomial so that for all <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" title="{s}" />, 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%5Cleft%28%5Czeta%28s%29%2C+%5Czeta%7B%27%7D%28s%29%2C%5Cdots%2C%5Czeta%5E%7B%28m%29%7D%28s%29+%5Cright%29+%3D+0.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " class="latex" title="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " /></p>
</em><p><em>Then <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" title="{P}" /> is identically zero. </em>
</p></blockquote>
<p>Since <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{s}" class="latex" title="{s}" /> is a single variable, this says that <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> and its derivatives <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta'(s)}" class="latex" title="{\zeta'(s)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%27%28s%29+%5Cdots+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta''(s) \dots }" class="latex" title="{\zeta''(s) \dots }" /> do not satisfy any polynomial relationship. This means intuitively that <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> must be <a href="https://en.wikipedia.org/wiki/Hypertranscendental_function">hypertranscendental</a>. Let’s now make this formal.</p>
<p></p><h2> Voronin’s Theorem </h2><p></p>
<p>
Here is his <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">theorem</a>: </p>
<blockquote><p><b>Theorem 2</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7B0%3Cr%3C1%2F4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0&lt;r&lt;1/4}" class="latex" title="{0&lt;r&lt;1/4}" />. Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> be an analytic function that never is zero for <img src="https://s0.wp.com/latex.php?latex=%7B%7Cs%7C+%5Cle+r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|s| \le r}" class="latex" title="{|s| \le r}" />. Then for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> there is a real <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" /> so that 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7B%5Cleft+%7C+s+%5Cright+%7C+%5Cleq+r%7D+%5Cleft+%7C+%5Czeta%28s+%2B+%5Cfrac%7B3%7D%7B4%7D+%2B+i+t%29+-+f%28s%29+%5Cright+%7C+%3C+%5Cepsilon.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " class="latex" title="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
See the <a href="https://www.researchgate.net/profile/Renata-Macaitiene/publication/321139128_Zeros_of_the_Riemann_zeta-function_and_its_universality/links/5c7f7b5092851c695058d6fe/Zeros-of-the-Riemann-zeta-function-and-its-universality.pdf">paper</a> “Zeroes of the Riemann zeta-function and its universality,” by Ramunas Garunkstis, Antanas Laurincikas, and Renata Macaitiene, for a detailed modern discussion of his theorem. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/thm/" rel="attachment wp-att-18158"><img width="320" alt="" src="https://rjlipton.files.wordpress.com/2021/02/thm.png?w=320&amp;h=275" class="aligncenter wp-image-18158" height="275" /></a>
</td>
</tr>
</tbody></table>
<p></p><p><br />
Note that the theorem is not constructive. However, the values of <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" title="{t}" /> that work have a positive density—there are lots of them. Also note the restriction that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> is never zero is critical. Otherwise one would be able to show that the Riemann Hypothesis is false. In 2003, Garunkstis et al. did prove a constructive version, in a <a href="https://www.jstor.org/stable/43736941?seq=1">paper</a> titled, “Effective Uniform Approximation By The Riemann Zeta-Function.”</p>
<p></p><h2> Voronin’s Proof </h2><p></p>
<p>
The key insight is to combine two properties of the zeta <img src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\zeta(s)}" class="latex" title="{\zeta(s)}" /> function: The usual definition with the Euler product. Recall the Riemann zeta-function has an Euler product expression 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Cprod_p+%5Cfrac%7B1%7D%7B1-p%5E%7B-s%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " class="latex" title="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" title="{p}" /> runs over prime numbers. This is valid only in the region <img src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Re(s) &gt; 1}" class="latex" title="{\Re(s) &gt; 1}" />, but it makes sense in a approximate sense in the critical strip: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F2+%3C+%5CRe%28s%29+%3C+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " class="latex" title="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " /></p>
<p>Then take logarithms and since <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log(p)}" class="latex" title="{\log(p)}" /> are linearly independent over <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Q}" class="latex" title="{Q}" />, we can apply the Kronecker approximation theorem to obtain that any target function <img src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(s)}" class="latex" title="{f(s)}" /> can be approximated by the above finite truncation. This is the basic structure of the <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">proof</a>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Voronin’s insight was immediately interesting to number theorists. Many found new methods for proving universality and for extending it to other functions. Some methods work for all zeta-functions defined by Euler products. See this <a href="https://arxiv.org/pdf/1407.4216.pdf">survey</a> by Kohji Matsumoto and a recent <a href="https://www.semanticscholar.org/paper/Quantized-Number-Theory%2C-Fractal-Strings-and-the-to-Herichi-Lapidus/37dbcf9c28c316b8cfcfe74394f3a1f2d709235d">paper</a><br />
by Hafedh Herichi and Michel Lapidus, the latter titled “Quantized Number Theory, Fractal Strings and the Riemann Hypothesis: From Spectral Operators to Phase Transitions and Universality.”</p>
<p>
Perhaps the most interesting question is: </p>
<p><i>Can universality be used to finally unravel the RH?</i> </p>
<p>See Paul Gauthier’s 2014 IAS <a href="http://www.math.kent.edu/~zvavitch/informal/Informal_Analysis_Seminar/Slides,_April_2014_files/IAS2014_Gauthier_1.pdf">talk</a>, “Universality and the Riemann Hypothesis,” for some ideas.</p>
<p>
[fixed missing line at end]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/"><span class="datestr">at February 21, 2021 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-1990979806491986425">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html">Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel's memory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://www.strath.ac.uk/staff/kitaevsergeydr/" target="_blank">Sergey Kitaev</a> just shared with me <a href="http://ecajournal.haifa.ac.il/Volume2021/ECA2021_S1H2.pdf" target="_blank">an article</a> he wrote with Anthony Mendes in <a href="https://senate.universityofcalifornia.edu/in-memoriam/files/jeffrey-b-remmel.html" target="_blank">Jeff Remmel's memory</a>. Jeff Remmel was a distinguished mathematician with a very successful career in both logic and combinatorics. </p><p>The short biography at the start of the article paints a vivid picture of Jeff Remmel's  personality, and will be of interest and inspiration to many readers. His hiring as "an Assistant Professor in the Department of Mathematics at UC San Diego at age 25, without officially finishing his Ph.D. and without having published a single paper" was, in Jeff Remmel's own words, a "fluke that will never happen again."<br /></p><p>I had the pleasure of making Jeff Remmel's acquaintance when he visited Sergey in Reykjavik and thoroughly enjoyed talking to him about a variety of subjects. He was truly a larger-than-life academic. <br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html"><span class="datestr">at February 21, 2021 11:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/025">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/025">TR21-025 |  Improved Maximally Recoverable LRCs using Skew Polynomials | 

	Sivakanth Gopi, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over $\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$ local groups each of size $r$. Each local group satisfies `$a$' local parity checks to recover from `$a$' erasures in that local group and there are further $h$ global parity checks to provide fault tolerance from more global erasure patterns. Such an LRC is Maximally Recoverable (MR), if it offers the best blend of locality and global erasure resilience---namely it can correct all erasure patterns whose recovery is information-theoretically feasible given the locality structure (these are precisely patterns with up to `$a$' erasures in each local group and an additional $h$ erasures anywhere in the codeword).

Random constructions can easily show the existence of MR LRCs over very large fields, but a major algebraic challenge is to construct MR LRCs, or even show their existence, over smaller fields, as well as understand inherent lower bounds on their field size. We give an explicit construction of $(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by $\left(O\left(\max\{r,n/r\}\right)\right)^{\min\{h,r-a\}}$. This improves upon known constructions in many relevant parameter ranges. Moreover, it matches the lower bound from Gopi et al. (2020) in an interesting range of parameters where $r=\Theta(\sqrt{n})$, $r-a=\Theta(\sqrt{n})$ and $h$ is a fixed constant with $h\le a+2$, achieving the optimal field size of $\Theta_{h}(n^{h/2}).$

Our construction is based on the theory of skew polynomials.  We believe skew polynomials should have further applications in coding and complexity theory; as a small illustration we show how to capture algebraic results underlying list decoding folded Reed-Solomon and multiplicity codes in a unified way within this theory.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/025"><span class="datestr">at February 21, 2021 10:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/024">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/024">TR21-024 |  A Majority Lemma for Randomised Query Complexity | 

	Mika Göös, 

	Gilbert Maystre</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that computing the majority of $n$ copies of a boolean function $g$ has randomised query complexity $\mathrm{R}(\mathrm{Maj} \circ g^n) = \Theta(n\cdot \bar{\mathrm{R}}_{1/n}(g))$. In fact, we show that to obtain a similar result for any composed function $f\circ g^n$, it suffices to prove a sufficiently strong form of the result only in the special case $g=\mathrm{GapOr}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/024"><span class="datestr">at February 21, 2021 10:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/023">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/023">TR21-023 |  $3.1n - o(n)$ Circuit Lower Bounds for Explicit Functions | 

	Tianqi Yang, 

	Jiatu Li</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proving circuit lower bounds has been an important but extremely hard problem for decades. Although one may show that almost every function $f:\mathbb{F}_2^n\to\mathbb{F}_2$ requires circuit of size $\Omega(2^n/n)$ by a simple counting argument, it remains unknown whether there is an explicit function (for example, a function in $NP$) not computable by circuits of size $10n$. In fact, a $3n-o(n)$ explicit lower bound by Blum (TCS, 1984) was unbeaten for over 30 years until a recent breakthrough by Find et al. (FOCS, 2016), which proved a $(3+\frac{1}{86})n-o(n)$ lower bound for affine dispersers, a class of functions known to be constructible in $P$.

In this paper, we prove a stronger lower bound $3.1n - o(n)$ for affine dispersers. To get this result, we strengthen the gate elimination approach for $(3+\frac{1}{86})n$ lower bound, by a more sophisticated case analysis that significantly decreases the number of bottleneck structures introduced during the elimination procedure. Intuitively, our improvement relies on three observations: adjacent bottleneck structures becomes less troubled; the gates eliminated are usually connected; and the hardest cases during gate elimination have nice local properties to prevent the introduction of new bottleneck structures.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/023"><span class="datestr">at February 21, 2021 06:59 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/022">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/022">TR21-022 |  Depth lower bounds in Stabbing Planes for combinatorial principles | 

	Stefan Dantchev, 

	Nicola  Galesi, 

	Abdul Ghani, 

	Barnaby Martin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove logarithmic depth lower bounds in Stabbing Planes for the classes of  combinatorial principles known as  the Pigeonhole principle and the Tseitin contradictions. The depth lower bounds are new, obtained by giving almost linear length lower bounds which do not depend on the bit-size of the inequalities and in the case of  the Pigeonhole principle are tight. 

The technique known so far to prove depth lower bounds for Stabbing Planes is a generalization of that used for the Cutting Planes proof system.  In this work  we  introduce two  new approaches to prove length/depth lower bounds in Stabbing Planes: one relying on Sperner's Theorem which works for the Pigeonhole principle and Tseitin contradictions over the complete graph; a second proving the lower bound for Tseitin contradictions over a grid graph, which uses a result on essential  coverings of the boolean cube by linear polynomials, which in turn relies on Alon's combinatorial Nullenstellensatz</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/022"><span class="datestr">at February 20, 2021 08:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/021">TR21-021 |  Average-Case Perfect Matching Lower Bounds from Hardness of Tseitin Formulas | 

	Kilian Risse, 

	Per Austrin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the complexity of proving that a sparse random regular graph on an odd number of vertices does not have a perfect matching, and related problems involving each vertex being matched some pre-specified number of times. We show that this requires proofs of degree $\Omega(n/\log n)$ in the Polynomial Calculus (over fields of characteristic $\ne 2$) and Sum-of-Squares proof systems, and exponential size in the bounded-depth Frege proof system. This resolves a question by Razborov asking whether the Lovász-Schrijver proof system requires $n^\delta$ rounds to refute these formulas for some $\delta &gt; 0$. The results are obtained by a worst-case to average-case reduction of these formulas relying on a topological embedding theorem which may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/021"><span class="datestr">at February 20, 2021 06:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/020">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/020">TR21-020 |  Error Reduction For Weighted PRGs Against Read Once Branching Programs | 

	Gil Cohen, 

	Dean Doron, 

	Amnon Ta-Shma, 

	Ori Sberlo, 

	Oren Renard</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Weighted pseudorandom generators (WPRGs), introduced by Braverman, Cohen and Garg [BCG20], is a generalization of pseudorandom generators (PRGs) in which arbitrary real weights are considered rather than a probability mass. Braverman et al. constructed WPRGs against read once branching programs (ROBPs) with near-optimal dependence on the error parameter. Chattopadhyay and Liao [CL20] somewhat simplified the technically involved BCG construction, also obtaining some improvement in parameters.

In this work we devise an error reduction procedure for PRGs against ROBPs. More precisely, our procedure transforms any PRG against length n width w ROBP with error 1/poly(n) having seed length s to a WPRG with seed length s + O(log(w/?)loglog(1/?)). By instantiating our procedure with Nisan’s PRG [Nis92] we obtain a WPRG with seed length O(log(n)log(nw) + log(w/?)loglog(1/?)). This improves upon [BCG20] and is incomparable with [CL20].

Our construction is significantly simpler on the technical side and is conceptually cleaner. Another advantage of our construction is its low space complexity O(log nw)+ poly(loglog(1/?)) which is logarithmic in n for interesting values of the error parameter ?. Previous constructions (like [BCG20, CL20]) specify the seed length but not the space complexity, though it is plausible they can also achieve such (or close) space complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/020"><span class="datestr">at February 20, 2021 06:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/019">TR21-019 |  Pseudodistributions That Beat All Pseudorandom Generators | 

	Edward Pyne, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A recent paper of Braverman, Cohen, and Garg (STOC 2018) introduced the concept of a pseudorandom pseudodistribution generator (PRPG), which amounts to a pseudorandom generator (PRG) whose outputs are accompanied with real coefficients that scale the acceptance probabilities of any potential distinguisher. They gave an explicit construction of PRPGs for ordered branching programs whose seed length has a better dependence on the error parameter $\epsilon$ than the classic PRG construction of Nisan (STOC 1990 and Combinatorica 1992). 
    
    In this work, we give an explicit construction of PRPGs that achieve parameters that are impossible to achieve by a PRG.  In particular, we construct a PRPG for ordered permutation branching programs of unbounded width with a single accept state that has seed length $\tilde{O}(\log^{3/2} n)$ for error parameter $\epsilon=1/\text{poly}(n)$, where $n$ is the input length.  In contrast, recent work of Hoza et al. (ITCS 2021) shows that any PRG for this model requires seed length $\Omega(\log^2 n)$ to achieve error $\epsilon=1/\text{poly}(n)$.
    
    As a corollary, we obtain explicit PRPGs with seed length $\tilde{O}(\log^{3/2} n)$ and error $\epsilon=1/\text{poly}(n)$ for ordered permutation branching programs of width $w=\text{poly}(n)$ with an arbitrary number of accept states.  Previously, seed length $o(\log^2 n)$ was only known when both the width and the reciprocal of the error are subpolynomial, i.e. $w=n^{o(1)}$ and $\epsilon=1/n^{o(1)}$ (Braverman, Rao, Raz, Yehudayoff, FOCS 2010 and SICOMP 2014).
    
    The starting point for our results are the recent space-efficient algorithms for estimating random-walk probabilities in directed graphs by Ahmadenijad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020), which are based on spectral graph theory and space-efficient Laplacian solvers.  We interpret these algorithms as giving PRPGs with large seed length, which we then derandomize to obtain our results.  We also note that this approach gives a simpler proof of the original result of Braverman, Cohen, and Garg, as independently discovered by Cohen, Doron, Renard, Sberlo, and Ta-Shma (personal communication, January 2021).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/019"><span class="datestr">at February 20, 2021 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/018">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/018">TR21-018 |  Monotone Branching Programs: Pseudorandomness and Circuit Complexity | 

	Dean Doron, 

	Raghu Meka, 

	Omer Reingold, 

	Avishay Tal, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study monotone branching programs, wherein the states at each time step can be ordered so that edges with the same labels never cross each other. Equivalently, for each fixed input, the transition functions are a monotone function of the state. 

We prove that constant-width monotone branching programs of polynomial size are equivalent in power to $AC^{0}$ circuits. This complements the celebrated theorem of Barrington, which states that constant-width branching programs, without the monotonicity constraint, are equivalent in power to $NC^{1}$ circuits.

Next we turn to read-once monotone branching programs of constant width, which we note are strictly more powerful than read-once $AC^0$.  Our main result is an explicit pseudorandom generator that $\varepsilon$-fools length $n$ programs with seed length $\widetilde{O}(\log(n/\varepsilon))$. This extends the families of constant-width read-once branching programs for which we have an explicit pseudorandom generator with near-logarithmic seed length. 

Our pseudorandom generator construction follows Ajtai and Wigderson's approach of iterated pseudorandom restrictions [AW89,GMRTV12]. We give a randomness-efficient width-reduction process which allows us to simplify the branching program after only $O(\log\log n)$ independent applications of the Forbes--Kelley pseudorandom restrictions [FK18].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/018"><span class="datestr">at February 20, 2021 06:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/017">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/017">TR21-017 |  Mixing in non-quasirandom groups | 

	Timothy Gowers, 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate a systematic study of mixing in non-quasirandom groups.
Let $A$ and $B$ be two independent, high-entropy distributions over
a group $G$. We show that the product distribution $AB$ is statistically
close to the distribution $F(AB)$ for several choices of $G$ and
$F$, including:

(1) $G$ is the affine group of $2\times2$ matrices, and $F$ sets
the top-right matrix entry to a uniform value,

(2) $G$ is the lamplighter group, that is the wreath product of $\Z_{2}$
and $\Z_{n}$, and $F$ is multiplication by a certain subgroup,

(3) $G$ is $H^{n}$ where $H$ is non-abelian, and $F$ selects a
uniform coordinate and takes a uniform conjugate of it.

The obtained bounds for (1) and (2) are tight.

This work is motivated by and applied to problems in communication
complexity. We consider the 3-party communication problem of deciding
if the product of three group elements multiplies to the identity.
We prove lower bounds for the groups above, which are tight for the
affine and the lamplighter groups.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/017"><span class="datestr">at February 19, 2021 07:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html">Loops, degrees, and matchings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A student in my graph algorithms class asked how <a href="https://11011110.github.io/blog/2021/02/19/Loop (graph theory)">self-loops</a> in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</p>

<p>However, this turns out not to make much difference to many matching problems, because the following simple transformation turns a problem with self-loops (allowed in matchings in this way) into a problem with no self-loops (so it doesn’t matter whether they are allowed or not). Simply form a <a href="https://en.wikipedia.org/wiki/Covering_graph">double cover</a>\(^*\) of the given graph (let’s call it the “loopless double cover”) by making two copies of the graph and replacing all corresponding pairs of loops by simple edges from one copy to the other. In weighted matching problems, give the replacement edges for the loops the sum of the weights of the two loops they replace; all other edges keep their original weights.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/loopless-double-cover.svg" alt="The loopless double cover of a graph and of one of its loopy matchings" /></p>

<p>Then (unlike the <a href="https://en.wikipedia.org/wiki/Bipartite_double_cover">bipartite double cover</a>, which also eliminates loops) the cardinality or optimal weight of a matching in the loopy graph can be read off from the corresponding solution in its loopless double cover. Any matching of the original loopy graph can be translated into a matching of the loopless cover by applying the same loopless cover translation to the matching instead of to the whole graph; this doubles the total weight of the matching and the total number of matched vertices. And among matchings on the loopless cover, when trying to optimize weight or matched vertices, it is never helpful to match the two copies differently, so there is an optimal solution that can be translated back to the original graph without changing its optimality.</p>

<p>This doesn’t quite work for the problem of finding a matching that maximizes the total number of matched edges, rather than the total number of matched vertices. These two problems are the same in simple graphs, but different in loopy graphs. However, in a loopy graph, if you are trying to maximize matched edges, you might as well include all loops in the matching, and then search for a maximum matching of the simple graph induced by the remaining unmatched vertices. Again, in this case, you don’t get a problem that requires any new algorithms to solve it.</p>

<p>In the case of my student, I only provided the conventional answer, because really all they wanted to know was whether these issues affected how they answered one of the homework questions, and the answer was that the question didn’t involve and didn’t need loops. However it seems that the more-complicated answer is that even if you allow loops to count only one unit towards degree, and to be included in matchings, they don’t change the matching problem much.</p>

<p>\(^*\) This is only actually a covering graph under the convention that the degree of a loop is one. For the usual degree-2 convention for loops, you would need to replace each loop by a pair of parallel edges, forming a multigraph, to preserve the degrees of the vertices.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105762400402127534">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html"><span class="datestr">at February 19, 2021 06:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21277">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/">Nostalgia corner: John Riordan’s referee report of my first paper</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  “Combinatorial Identities” and “Combinatorial Analysis”.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan1.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan1.png?w=640&amp;h=834" class="alignnone size-full wp-image-21279" height="834" /></a></p>
<p>I was surely very happy to read the sentence <span style="color: #0000ff;">“I think you have had a splendid idea”</span>.  Here is part of Riordan’s remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep1.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan3.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan3.png?w=640&amp;h=827" class="alignnone size-full wp-image-21280" height="827" /></a></p>
<p>It took me some time to revise the paper and get it printed. And here is the report for the second version.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan2.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan2.png?w=640&amp;h=839" class="alignnone size-full wp-image-21281" height="839" /></a></p>
<p>And here is part of Riordan’s second round of remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep2.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan4.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2021/02/riordan4.png?w=640&amp;h=843" class="alignnone size-full wp-image-21282" height="843" /></a></p>
<p>I was certainly happy to read the following sentence: <span style="color: #0000ff;">“I would remark that the result for  <em>p = -1 </em> is new and perhaps <span style="color: #ff0000;">the simplest derivation of Abel’s result</span>.”</span></p>
<p>In 1978 I actually visited John Riordan in his office at Rockefeller University, NYC. I remember him as very cheerful and he told me that when his first book appeared he was working at Bell Labs and his managers wanted to talk to him. He was a bit worried that they would not approve of him spending time and effort to write a book in pure mathematics. But actually, they gave him a salary raise!</p>
<p>(If you have a picture of John Riordan, please send me.)</p>
<p>In 1979 the paper <a href="https://gilkalai.files.wordpress.com/2021/02/1-s2.0-0097316579900475-main-1.pdf">appeared</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan5.png"><img src="https://gilkalai.files.wordpress.com/2021/02/riordan5.png?w=640" alt="" class="alignnone size-full wp-image-21286" /></a></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/"><span class="datestr">at February 19, 2021 09:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18125">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/">Computing’s Role In The Pandemic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>How can we help?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/unknown-145/" rel="attachment wp-att-18132"><img src="https://rjlipton.files.wordpress.com/2021/02/unknown.jpeg?w=600" alt="" class="alignright size-full wp-image-18132" /></a></p>
<p>
Joe Biden is the 46th president of the USA. Note <img src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{46}" class="latex" title="{46}" /> is called a <a href="https://en.wikipedia.org/wiki/Centered_triangular_number">centered triangular number</a>. These numbers obey the formula: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B3n%5E2+%2B+3n+%2B+2%7D%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac{3n^2 + 3n + 2}{2} " class="latex" title="\displaystyle  \frac{3n^2 + 3n + 2}{2} " /></p>
<p>and start with <img src="https://s0.wp.com/latex.php?latex=%7B1%2C4%2C10%2C19%2C31%2C46%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1,4,10,19,31,46,\dots}" class="latex" title="{1,4,10,19,31,46,\dots}" /> The previous one, the <img src="https://s0.wp.com/latex.php?latex=%7B31%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{31^{st}}" class="latex" title="{31^{st}}" />, was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic">pandemic</a> one of his top priorities. </p>
<p>
Today I thought we would discuss how he might use computer technology to help get the virus under control. </p>
<p>
First, we thank the drug companies since we now have <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Vaccines">vaccines</a> that work against the virus. Without these we would have little chance to bring the pandemic under control at all. </p>
<p>
Second, we must state that we are worried that the virus is mutating and this may render the current vaccines less useful, if not useless. We hope this is not happening, or that the drug companies will be able to respond with vaccine boosters. Today there seems to be <a href="https://www.usnews.com/news/health-news/articles/2021-02-18/pfizer-coronavirus-vaccine-protects-against-uk-south-africa-variants-study-shows">good news</a> and <a href="https://www.reuters.com/article/us-health-coronavirus-vaccines-variants/pfizer-says-south-african-variant-could-significantly-reduce-protective-antibodies-idUSKBN2AH2VG">bad news</a>.  </p>
<p>
Results will fluctuate, but in any case, vaccines will definitely play a key role in defeating the pandemic. We want to ask the same about computing technology.</p>
<p>
</p><p></p><h2> Computing’s Role—I </h2><p></p>
<p></p><p>
There are many web sites that discuss how computing technology can play a role in defeating the pandemic. Here are some of the main points:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Tracking People:</i> Many places are interested in tracking who are sick. Tracking can by itself help stop the spreading of the virus, and thus help save lives. For example, <a href="https://www.computer.org/publications/tech-news/five-ways-tech-is-being-used-to-fight-covid-19">IEEE</a> says: </p>
<blockquote><p><b> </b> <em> “We believe software can help combat this global pandemic, and that’s why we’re launching the Code Against COVID-19 initiative…,” said Weiting Liu, founder and CEO of Arc. “From tracking outbreaks and reducing the spread to scaling testing and supporting healthcare, teams around the world are using software to flatten the curve. The eMask app (real-time mask inventory in Taiwan) and TraceTogether (contact tracing in Singapore) are just two of the many examples.” </em>
</p></blockquote>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Changing Behavior:</i> A powerful idea is to avoid human to human contact and thus stop the spread of the virus. For example, here are <a href="https://www.weforum.org/agenda/2020/04/10-technology-trends-coronavirus-covid19-pandemic-robotics-telehealth">examples</a> from a longer list of ideas: </p>
<ul>
<li>
Robot Deliveries; <p></p>
</li><li>
Digital and Contactless Payments; <p></p>
</li><li>
Remote Work and Remote Learning and more.
</li></ul>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>Changing Health Delivery:</i> An important idea is how can we reduce the risk of health delivery. A paradox is that health care may need to be avoided, since traditional delivery requires human contact. There are many examples of ways to make health care online, and therefore safer. Shwetak Patel won the 2018 ACM Prize in Computing for contributions to creative and practical sensing systems for sustainability and health. He outlined here <a href="https://cccblog.org/2020/09/24/what-role-can-computing-play-in-battling-the-covid-19-pandemic/">CCC blog</a> how health care could be made more online.</p>
<p>
</p><p></p><h2> Computing’s Role—II </h2><p></p>
<p></p><p>
The above ideas are fine but I believe the real role for computing is simple: </p>
<blockquote><p><b> </b> <em> <i>Make signing up and obtaining an appointment for a vaccine easier, fairer, and sooner.</i> </em>
</p></blockquote>
<p></p><p>
In the US each state is in charge of running web sites that allow people to try and get an appointment for a vaccine shot. <i>Try</i> is the key word. Almost all sites require an appointment to get a shot—walk-ins are mostly not allowed. </p>
<p>
I cannot speak for all states and all web sites, but my direct experience is that the sites are terrible. Signing up for a vaccination shot is a disaster. The web sites that I have seen are poorly written, clumsy, and difficult to use. They are some of the worst sites I have ever needed to use, for anything. Some of the top issues: </p>
<ol>
<li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites require you to sign in each time from scratch. <p></p>
</li><li>
The sites rules are confusing and unclear. <p></p>
</li><li>
You may need to search for particular vaccine locations, rather than for any locations. <p></p>
</li><li>
And more <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" />
</li></ol>
<p>Repeating (1,2,3) is a poor joke, but one that reflects reality. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
If Amazon, Google, Apple had sites that worked this way, they would be out of business quickly. Perhaps this is the key: <i>Can our top companies help build the state sites?</i> Is it too late to help? See <a href="https://www.nytimes.com/2021/01/12/technology/the-problem-with-vaccine-websites.html">here</a> for a New York Times article on this issue: </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/game/" rel="attachment wp-att-18129"><img width="300" alt="" class="aligncenter wp-image-18129" src="https://rjlipton.files.wordpress.com/2021/02/game.png?w=300" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<blockquote><p><b> </b> <em> When you start to pull your hair out because you can’t register for a vaccine on a local website, remember that it’s not (only) the fault of a bad tech company or misguided choices by government leaders today. It’s a systematic failure years in the making. </em>
</p></blockquote>
<p></p><p>
Also is the issue of <a href="https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709">algorithmic fairness</a> relevant here? We know that it is unfortunately easy to have web sites that are unfair—that assign vaccine sign up dates unfairly, that favor one class of people over another. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/"><span class="datestr">at February 19, 2021 03:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5347">Brief thoughts on the Texas catastrophe</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or heat or clean water; the UT campus a short walk from me converted into a giant refugee camp.</p>



<p>For all those who asked: my family and I are fine.  While many we know were without power for days (or are <em>still</em> without power), we lucked out by living close to a hospital, which means that they can’t shut off the electricity to our block.  We <em>are</em> now on a boil-water notice, like all of Austin, and we can’t take deliveries or easily go anywhere, and the university and schools and daycares are all closed (even for remote learning).  Which means: we’re simply holed up in our house, eating through our stockpiled food, the kids running around being crazy, Dana and I watching them with one eye and our laptops with the other.  Could be worse.</p>



<p>In some sense, it’s not surprising that the Texas infrastructure would buckle under weather stresses outside the envelope of anything it was designed for or saw for decades.  The central problem is that our elected leaders have shown zero indication of understanding the urgent need, for Texas’ economic viability, to do whatever it takes to make sure nothing like this ever happens again.  Ted Cruz, as everyone now knows, left for Cancun; the mayor of Colorado City angrily told everyone to fend for themselves (and then resigned); and Governor Abbott has been blaming frozen wind turbines, a tiny percentage of the problem (frozen gas pipes are a much bigger issue) but one that plays with the base.  The bare minimum of a sane response might be, I dunno,</p>



<ul><li>acknowledging the reality that climate change means that “once-per-century” weather events will be every couple years from now on,</li><li>building spare capacity (nuclear would be ideal … well, I can dream),</li><li>winterizing what we have now, and</li><li>connecting the Texas grid to the rest of the US.</li></ul>



<p>If I were a Texas Democrat, I’d consider making Republican incompetence on infrastructure, utilities, and public health my <em>only</em> campaign issues.</p>



<p>Alright, now back to watching the Mars lander, which is apparently easier to build and deploy than a reliable electric grid.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5347"><span class="datestr">at February 18, 2021 09:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4494">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/">This year, for Lent, we realized it has been Lent all along</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give something up as a penance, such as giving up eating meat.</p>



<p>The period that immediately precedes Lent is known as Carnival, and, perhaps incongruously, it is a time for having fun, playing pranks, and eating special sweets, often deep-fried ones. Traditionally kids, and also grownups, dress up in costumes and attend costume parties. The idea being, let’s have fun and eat now, because soon we are “entirely voluntarily”  going to fast and to reflect on sin and death, and stuff like that. The day before Ash Wednesday, indeed, is called “Fat Tuesday”.</p>



<p>In Milan, however, the tradition is to power through Ash Wednesday and to continue the Carnival festivities until the following Sunday. There are a number of legends that explain this unique tradition, that is apparently ancient. One such legend is that a plague epidemic had been ravaging Milan in the IV century around the time that should have been Carnival, and life was beginning to go back to normal right around Ash Wednesday. So people rebelled against Lent, and were like, haven’t we suffered enough, what more penance do we need, and celebrated Carnival later.</p>



<p>It has now been nearly a year since the first lockdown, and we still cannot travel between regions (for example, we cannot travel from Milan to Bologna, or to Venice), cannot eat dinner in a restaurant, cannot go see a movie, a play or a sporting event, cannot ski, and so on.</p>



<p>My proposal is that when (if?) we go back to a normal life, we shorten Lent to three days (start with “Ash Thursday” the day before Good Friday), and that we make Carnival  start on Easter Monday and last for 361 days. Not because we have had it worse than a IV century plague epidemic: indeed, even in the best of times, IV century people in Milan did not usually eat in restaurants, travel to Venice, see movies, or ski. We, however, are spoiled XXI century people, we are not used to inconveniences, and when (if?) this is over we will need a lot of self-care, especially the eating-deep-fried-sweets-and-partying kind of self-care.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/"><span class="datestr">at February 18, 2021 01:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">Postdoc at Imperial College London in Complexity  (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based at the South Kensington campus at the heart of London.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br />
Email: iddo.tzameret@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/"><span class="datestr">at February 18, 2021 11:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/">PhD positions at Imperial College London (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by Iddo Tzameret.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br />
Email: iddo.tzameret@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/"><span class="datestr">at February 18, 2021 11:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7992">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do deep networks learn and when do they learn it</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://manosth.github.io/">Manos Theodosis</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through statistical learning theory</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c518b9e4-5f63-4278-871d-acc2017b8984">Lecture video</a> – <a href="https://boazbk.github.io/mltheoryseminar/lectures/seminar_lecture2.pdf">Slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_2.pptx">Slides (powerpoint with ink and animation)</a></p>



<p>In this lecture, we talk about <em>what</em> neural networks end up learning (in terms of their weights) and <em>when</em>, during training, they learn it.</p>



<p>In particular, we’re going to discuss</p>



<ul><li><strong>Simplicity bias</strong>: how networks favor “simple” features first.</li><li><strong>Learning dynamics</strong>: what is learned early in training.</li><li><strong>Different layers</strong>: do the different layers learn the same features?</li></ul>



<p>The type of results we will discuss are:</p>



<ul><li>Gradient-based deep learning algorithms have a bias toward learning simple classifiers. In particular this often holds when the optimization problem they are trying to solve is “underconstrained/overparameterized”, in the sense that there are exponentially many different models that fit the data.</li><li>Simplicity also affects the <em>timing</em> of learning. Deep learning algorithms tend to learn simple (but still predictive!) features first.</li><li>Such “simple predictive features” tend to be in lower (closer to input) levels of the network. Hence deep learning also tends to learn lower levels earlier.</li><li>On the other side, the above means that distributions that do not have “simple predictive features” pose significant challenges for deep learning. Even if there is a small neural network that works very well for the distribution, gradient-based algorithms will not “get off the ground” in such cases. We will see a lower bound for <em>learning parities</em> that makes this intuition formal.</li></ul>



<h2>What do neural networks learn, and when do they learn it?</h2>



<p>As a first example to showcase what is learned by neural networks, we’ll consider the following data distribution where we sample points <img src="https://s0.wp.com/latex.php?latex=%28X%2C+Y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(X, Y)" class="latex" title="(X, Y)" />, with <img src="https://s0.wp.com/latex.php?latex=Y+%5Cin+%7B1%2C+-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y \in {1, -1}" class="latex" title="Y \in {1, -1}" /> (<img src="https://s0.wp.com/latex.php?latex=Y+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y = 1" class="latex" title="Y = 1" /> corresponding to orange points and <img src="https://s0.wp.com/latex.php?latex=Y%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y=-1" class="latex" title="Y=-1" /> corresponding to blue points).<br /><img src="https://i.imgur.com/xiXoqDj.png" alt="" /></p>



<p>If we <a href="http://tfmeter.icsi.berkeley.edu/#activation=tanh&amp;batchSize=10&amp;dataset=byod&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;trueLearningRate=0&amp;regularizationRate=0&amp;noise=35&amp;networkShape=10,6,4,2&amp;seed=0.36834921&amp;showTestData=false&amp;discretize=false&amp;percTrainData=74&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">train</a> a neural network to fit this distribution, we can see below that the neurons that are closest to the input data end up learning features that are highly correlated with the input (mostly linear subspaces at 45-degree angle, which correspond to one of the stripes). In the subsequent layers, the features learned are more sophisticated and have increased complexity.<br /><img src="https://i.imgur.com/p2FQdUd.png" alt="" /><br /></p>



<h3>Neural networks have simpler but useful features in lower layers</h3>



<p>Some people have spent a lot of time trying to understand what is learned by different layers. In a <a href="https://distill.pub/2020/circuits/early-vision/">recent</a> work, Olah et al. dig deep into a particular architecture for computer vision, trying to interpret the features learned by neurons at different layers.</p>



<p>They found that earlier layers learn features that resemble edge detectors.<br /><img src="https://i.imgur.com/I8veBVf.png" alt="" /><br />However, as we go deeper, the neurons at those layers start learning more convoluted (for example, these features from layer <img src="https://s0.wp.com/latex.php?latex=3b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="3b" class="latex" title="3b" /> resemble heads).<br /><img src="https://i.imgur.com/rJsmKHR.png" alt="" /></p>



<h3>SGD learns simple (but still predictive) features earlier.</h3>



<p>There is evidence that <a href="https://arxiv.org/abs/1905.11604">SGD learns simpler classifiers first</a>. The following figure tracks how much of a learned classifier’s performance can be accounted for by a linear classifier. We see that up to a certain point in training <em>all</em> of the performance of the neural network learned by SGD (measured as mutual information with the label or as accuracy) can be ascribed to the linear classifier. They diverge only very near the point where the linear classifier “saturates,” in the sense that the classifier reachers the best possible accuracy for linear models. (We use the quantity <img src="https://s0.wp.com/latex.php?latex=I%28f%28x%29%3By+%7CL%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I(f(x);y |L(x))" class="latex" title="I(f(x);y |L(x))" /> – the mutual information of <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> conditioned on the prediction of the linear classifier <img src="https://s0.wp.com/latex.php?latex=L%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(x)" class="latex" title="L(x)" /> – to measure how much of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />‘s performance <em>cannot</em> be accounted for by <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" />.)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/pGxap3e.png" alt="" /></figure>



<h3>The benefits and pitfalls of simplicity bias</h3>



<p>In general, simplicity bias is a very good thing. For example, the most “complex” function is a random function. However, if given some observed data <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29%7D_%7Bi%5Cin+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i)}_{i\in [n]}" class="latex" title="{ (x_i,y_i)}_{i\in [n]}" />, SFD were to find a random function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that perfectly fits it, then it would never generalize (since for every fresh <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, the value of <img src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)" class="latex" title="f(x)" /> would be random).</p>



<p>At the same time, simplicity bias means that our algorithms might focus too much on simple solutions and miss more complex ones. Sometimes the complex solutions actually do perform better. In the following cartoon a person could go to the low-hanging fruit tree on the right-hand side and miss the bigger rewards on the left-hand side.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/eijQfpl.png" alt="" /></figure>



<p>This <a href="https://arxiv.org/abs/2006.07710">can actually happen</a> in neural networks. We also saw a simple example in class:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/LvONKsw.png" alt="" /></figure>



<p>The two datasets are equally easy to represent, but on the righthand side, there is a very strong “simple classifier” (the 45-degree halfspace) that SGD will “latch onto.” Once it gets stuck with that classifier, it is hard for SGD to get “unstuck.” As a result, SGD has a much harder time learning the righthand dataset than the lefthand dataset.</p>



<h2>Analysing SGD for over-parameterized linear regression</h2>



<p>So, what can we prove about the dynamics of gradient descent? Often we can gain insights by studying <em>linear regression</em>.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/T9VGkXj.png" alt="" /></figure>



<p>Formally, given <img src="https://s0.wp.com/latex.php?latex=%28x_i%2C+y_i%29_%7Bi%3D1%7D%5En+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" class="latex" title="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" /> with <img src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d\gg n" class="latex" title="d\gg n" /> we would like to find a vector <img src="https://s0.wp.com/latex.php?latex=w%5Cin%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w\in\mathbb{R}^d" class="latex" title="w\in\mathbb{R}^d" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+w%2C+x_i%5Crangle%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle w, x_i\rangle\approx y_i" class="latex" title="\langle w, x_i\rangle\approx y_i" />.</p>



<p>In this setting, we can prove that running SGD (from zero or tiny initialization) on the loss <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%29+%3D%5ClVert+Xw+-y+%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(w) =\lVert Xw -y \rVert^2" class="latex" title="\mathcal{L}(w) =\lVert Xw -y \rVert^2" /> will converge to solution <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" /> of minimum norm. To see whym note that SGD performs updates of the form<br /><img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta+x_i%5ET%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" class="latex" title="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" />.<br />However note that <img src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta(\langle x_i, w\rangle - y_i)" class="latex" title="\eta(\langle x_i, w\rangle - y_i)" /> is a scalar. Therefore all of the updates keep the updated vector <img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1}" class="latex" title="w_{t+1}" /> within <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" />. This implies that the converging solution <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" /> will also lie in <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" />.<br /><img src="https://i.imgur.com/G8tFMrF.png" alt="" /><br />Geometrically this translates into <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" /> being the projection of <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> onto the the subspace <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" title="\mathrm{span}(x_1^T, \ldots, x_n^T)" /> which results in the least norm solution.</p>



<p>Analyzing the dynamics of descent, we can write the distance between consecutive weight updates and the converging solution as<br /><img src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+-+w_%7B%5Cinfty%7D+%3D+%28I+-+%5Ceta+X%5ETX%29%28w_t+-+w_%7B%5Cinfty%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" class="latex" title="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" />.<br />We see that we are applying the linear operator <img src="https://s0.wp.com/latex.php?latex=%28I+-+%5Ceta+X%5ETX%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(I - \eta X^TX)" class="latex" title="(I - \eta X^TX)" /> at every step we take. As long as this operator is contractive, we will continue to progress and converge to <img src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{\infty}" class="latex" title="w_{\infty}" />. Formally, to make progress, we require<br /><img src="https://s0.wp.com/latex.php?latex=0+%5Cprec+I+-%5Ceta+X%5ETX%5Cprec+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \prec I -\eta X^TX\prec 1" class="latex" title="0 \prec I -\eta X^TX\prec 1" />.<br />This directly translates into <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+%5Cfrac%7B1%7D%7B%5Clambda_1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta &lt; \frac{1}{\lambda_1}" class="latex" title="\eta &lt; \frac{1}{\lambda_1}" /> and then the progress we make is approximately <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda_d%7D%7B%5Clambda_1%7D%3D%5Cfrac%7B1%7D%7B%5Ckappa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" class="latex" title="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" />, where <img src="https://s0.wp.com/latex.php?latex=%5Ckappa&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\kappa" class="latex" title="\kappa" /> is the <em>condition number</em> of <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" />.</p>



<p>What happens now if the matrix <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> is random? Then, results from random matrix theory (specifically the <a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko-Pastur distribution</a>) state that</p>



<ul><li>if <img src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &lt; n" class="latex" title="d &lt; n" />, then the matrix <img src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X^\top X" class="latex" title="X^\top X" /> has <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Brank%7D%28X%5E%5Ctop+X%29%3Dd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{rank}(X^\top X)=d" class="latex" title="\mathrm{rank}(X^\top X)=d" /> and the eigenvalues are bounded away from <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" />. This means that the matrix is well conditioned.</li><li>if <img src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \approx n" class="latex" title="d \approx n" />, then the spectrum of <img src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X^\top X" class="latex" title="X^\top X" /> starts shifting towards <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" />, with some eigenvalues being equal to zero, resulting in an ill-conditioned matrix.</li><li>if <img src="https://s0.wp.com/latex.php?latex=d+%3E+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &gt; n" class="latex" title="d &gt; n" />, then the spectrum has some zero eigenvalues, but is otherwise bounded away from zero. If we restrict to the subspace of positive eigenvalues, we achieve again a good condition number.</li></ul>



<figure class="wp-block-image"><img src="https://i.imgur.com/hLZtfz5.png" alt="" /></figure>



<p></p>



<h2>Deep linear networks</h2>



<p>We now want to go beyond linear regression and talk about deep networks. As deep networks are very hard to understand, we will first start analyzing a depth <img src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2" class="latex" title="2" /> network. We will also consider a <em>linear</em> network and omit the nonlinearity. This might seem strange, as we could consider the corresponding linear model, which has exactly the same expressiveness. However, note that these two models have a different parameter space. This means that gradient-based algorithms will travel on different paths when optimizing these two models.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ekh9k8M.png" alt="" /></figure>



<p>Specifically, we can see that the minimum loss attained by the two models will coincide, i.e., <img src="https://s0.wp.com/latex.php?latex=%5Cmin+%5Cmathcal%7BL%7D%28A_1%2C+A_2%29+%3D+%5Cmin+%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" class="latex" title="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" />, but the SGD path and the solution will be different.</p>



<p>We will analyze the gradient flow on these two networks (which is gradient descent with the learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta \rightarrow 0" class="latex" title="\eta \rightarrow 0" />). We will make the simplifying assumption that <img src="https://s0.wp.com/latex.php?latex=A_1+%3D+A_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_1 = A_2" class="latex" title="A_1 = A_2" /> and symmetric. Then, we can see that <img src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2+%5CRightarrow+A+%3D+%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B = A^2 \Rightarrow A = \sqrt{B}" class="latex" title="B = A^2 \Rightarrow A = \sqrt{B}" />. We will try and compare the gradient flow of two different loss functions: <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}(B)" class="latex" title="\mathcal{L}(B)" /> (doing gradient flow on a linear model) and <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Cmathcal%7BL%7D%28A%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" class="latex" title="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" /> (doing gradient flow on a depth linear model).</p>



<p>Gradient flow on the linear model simply gives <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" class="latex" title="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" />, whereas for the deep linear network we have (using the chain rule)<br /><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%28t%29%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29A+%3D+A%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," class="latex" title="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," /><br />since <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> is symmetric.</p>



<p>For simplicity, let’s denote <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Cmathcal%7BL%7D%28B%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29+%3D+%5Cnabla&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" class="latex" title="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Ctilde%7B%5Cnabla%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" class="latex" title="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" />. We then have<br /><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D%3D%5Cfrac%7BdA%28t%29%7D%7Bdt%7DA%3D-%5Ctilde%7B%5Cnabla%7DA+%3D+-A+%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" class="latex" title="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" />.</p>



<p>Another way to view the comparison between the models of interest, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dA^2(t)}{dt}" class="latex" title="\frac{dA^2(t)}{dt}" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt}" class="latex" title="\frac{dB(t)}{dt}" /> is as follows: let <img src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B = A^2" class="latex" title="B = A^2" />, then <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-A%5Cnabla+A+%3D+-%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" title="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" />.<br />We can view this as follows: when we multiply the gradient with <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{B}" class="latex" title="\sqrt{B}" /> we end up making the “big bigger and the small smaller”. Basically, this accenuates the differences between the eigenvalues and is biasing <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> to become a low-rank matrix. </p>



<p>To see why, you can think of a low rank matrix has one that has few large eigenvalues and the others small. If <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" /> is already close low rank, then replacing a gradient by <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" title="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" /> encourages the gradient steps to mostly happen in the top eigenspace of <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" />. This result <a href="https://arxiv.org/abs/1910.05505">generalizes</a> to networks of greater depth, and the gradient evolves as <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-%5Cpsi_%7BB%28t%29%7D%28%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" class="latex" title="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" />, with <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D%28%5Cnabla%29+%3D+%5Csum+B%5E%7B%5Calpha%7D%5Cnabla+B%5E%7B1-%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" class="latex" title="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" />.</p>



<p>This means that we end up doing gradient flow on a <em>Riemannian manifold</em>. An interesting result is that the flow induced by the operator <img src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\psi_{B}" class="latex" title="\psi_{B}" /> is provably not equivalent to a regularized minimization problem <img src="https://s0.wp.com/latex.php?latex=%5Cmin%5Cmathcal%7BL%7D+%2B+%5Clambda+R%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min\mathcal{L} + \lambda R(B)" class="latex" title="\min\mathcal{L} + \lambda R(B)" /> for any <img src="https://s0.wp.com/latex.php?latex=R%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R(\cdot)" class="latex" title="R(\cdot)" />.</p>



<h2>What is learned at different layers?</h2>



<p>Finally, let’s discuss what is learned by the different layers in a neural network. Some intuition people have is that learning proceeds roughly like the following cartoon:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/rteesY5.png" alt="" /></figure>



<p>We can think of our data as being “built up” as a sequence of choices from higher level to lower level features. For example, the data is generated by first deciding that it would be a photo of a dog, then that it would be on the beach, and finally low-level details such as the type of fur and light. This is also how a human would describe this photo. In contrast, a neural network builds up the features in the opposite direction. It starts from the simplest (lowest-level) features in the image (edges, textures, etc.) and gradually builds up complexity until it finally classifies the image.</p>



<h2>How neural networks learn features?</h2>



<p>To build a bit of intuition, consider an example of combining different simple features. We can see that if we try to combine two good edge detectors with different orientations, the end result will hardly be an edge detector.<br /><img src="https://i.imgur.com/CsTB2LV.png" alt="" /></p>



<p>So the intuition is that there is competitive/evolutionary pressure on neurons to “specialize” and recognize useful features. Initially, all the neurons are random features, which can be thought of as random linear combination of the various detectors. However, after training, the symmetry will break between the neurons, and they will specialize (in this simple example, they will either become vertical or horizontal edge detectors).</p>



<p><a href="https://arxiv.org/abs/1706.05806">Raghu, Gilmer, Yosinski, and Sohl-Dickstein</a> tracked the speed at which features learned by different layers reach their final learned state. In the figure below the diagonal elements denote the similarity of the current state of a layer to its final one, where lighter color means that the state is more similar. We can see that earlier layer (more to the left) reach their final state earlier (with th exception of the 2 layers closest to the output that also converge very early).</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/qHTIAzz.png" alt="" /></figure>



<p>The “symmetry breaking” intuition is explored by a recent work of <a href="https://arxiv.org/abs/1912.05671">Frankle, Dziugaite, Roy, and Carbin</a>. Intuitively, because the average of two good features is generally <em>not</em> a good feature, averaging the weights of two neural networks with small loss will likely result in a network with large loss. That is, if we start from two random initializations <img src="https://s0.wp.com/latex.php?latex=w_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_0" class="latex" title="w_0" />, <img src="https://s0.wp.com/latex.php?latex=w%27_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_0" class="latex" title="w'_0" /> and train two networks until we reach weights <img src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_\infty" class="latex" title="w_\infty" /> and <img src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_\infty" class="latex" title="w'_\infty" /> with small loss, then we expect the average of <img src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_\infty" class="latex" title="w_\infty" /> and <img src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w'_\infty" class="latex" title="w'_\infty" /> to result in a network with poor loss:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/uLPBPpN.png" alt="" /></figure>



<p>In contrast, Frankle et al showed that sometimes, when we start from the same initialization (especially after pruning) and use random SGD noise (obtained by randomly shuffling the training set) then we reach a “linear plateu” of the loss function in which averaging two networks yields a network with similar loss:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/owtRGUL.png" alt="" /></figure>



<h2>The contrapositive of simplicity: lower bounds for learning parities</h2>



<p>If we believe that networks learn simple features first, and learn them in the early layers, then this has an interesting consequence. If the data has the form that simple features (e.g. linear or low degree) are completely uninformative (have no correlation with the label) then we may expect that learning cannot “get off the ground”. That is, even if there exists a small neural network that can learn the class, gradient based algorithms such as SGD will never find it. (In fact, it is possible that <em>no</em> efficient algorithm could find it.) There are some settings where we can prove such conjectures. (For gradient-based algorithms that is; proving this for all efficient algorithms would require settling the P vs NP question.)</p>



<p>We discuss one of the canonical “hard” examples for neural networks: parities. Formally, for <img src="https://s0.wp.com/latex.php?latex=I%5Csubset+%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I\subset [d]" class="latex" title="I\subset [d]" />, the distribution <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" /> is the distribution over <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+%7B+%5Cpm+1+%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \in { \pm 1 }^{d+1}" class="latex" title="(x,y) \in { \pm 1 }^{d+1}" /> defined as follows: <img src="https://s0.wp.com/latex.php?latex=x%5Csim+%7B%5Cpm+1%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim {\pm 1}^d" class="latex" title="x\sim {\pm 1}^d" /> and <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y = \prod_{i\in I}x_i" class="latex" title="y = \prod_{i\in I}x_i" />. The “learning parity” problem is as follows: given <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" /> samples <img src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ (x_i,y_i) }_{i=1..n}" class="latex" title="{ (x_i,y_i) }_{i=1..n}" /> drawn from <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />, either recover <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> or do the weaker task of finding a predictor <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> such that <img src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(x)=y" class="latex" title="f(x)=y" /> with high probability over future samples <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \sim D_I" class="latex" title="(x,y) \sim D_I" />.</p>



<p>It turns out that if we don’t restrict ourselves to deep learning, given <img src="https://s0.wp.com/latex.php?latex=2d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2d" class="latex" title="2d" /> samples we can recover <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />. Consider the transformations <img src="https://s0.wp.com/latex.php?latex=Z_%7Bi%2Cj%7D+%3D+%281+-+x_%7Bi%2Cj%7D%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z_{i,j} = (1 - x_{i,j})/2" class="latex" title="Z_{i,j} = (1 - x_{i,j})/2" /> and <img src="https://s0.wp.com/latex.php?latex=b_i+%3D+%281+-+y_i%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="b_i = (1 - y_i)/2" class="latex" title="b_i = (1 - y_i)/2" />. If we let <img src="https://s0.wp.com/latex.php?latex=s_i%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_i=1" class="latex" title="s_i=1" /> if <img src="https://s0.wp.com/latex.php?latex=i%5Cin+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\in I" class="latex" title="i\in I" /> and <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> otherwise, we can write <img src="https://s0.wp.com/latex.php?latex=%5Csum_j+Z_%7Bi%2C+j%7Ds_j+%3D+b_i+%28%5Ctext%7Bmod+%7D+2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" class="latex" title="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" />. Basically, we transformed the problem of parity to a problem of counting if we have an odd or an even number of <img src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-1" class="latex" title="-1" />. In this setting, we can think of every sample <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \in D_I" class="latex" title="(x,y) \in D_I" /> as providing a <em>linear equation</em> moudlo 2 over the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> unknown variables <img src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1,\ldots,s_d" class="latex" title="s_1,\ldots,s_d" />. When <img src="https://s0.wp.com/latex.php?latex=n%3Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n&gt;d" class="latex" title="n&gt;d" />, these linear equations will be very likely to be of full rank, and hence we can use Gaussian elimination to find <img src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s_1,\ldots,s_d" class="latex" title="s_1,\ldots,s_d" /> and hence <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />.</p>



<p>Switching to the learning setting, we can express parities by using few ReLUs. In particular, we’ve shown that we can create a step function using <img src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="4" class="latex" title="4" /> ReLUs. Therefore for every <img src="https://s0.wp.com/latex.php?latex=k+%5Cin+%7B0%2C1%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k \in {0,1,\ldots, d }" class="latex" title="k \in {0,1,\ldots, d }" />, there is a combination of four ReLUs that computes the function <img src="https://s0.wp.com/latex.php?latex=f_k%3A%5Cmathbb%7BR%7D+%3A%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k:\mathbb{R} :\rightarrow \mathbb{R}" class="latex" title="f_k:\mathbb{R} :\rightarrow \mathbb{R}" /> such that <img src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k(s)" class="latex" title="f_k(s)" /> outputs <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> for <img src="https://s0.wp.com/latex.php?latex=s%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="s=k" class="latex" title="s=k" />, and <img src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_k(s)" class="latex" title="f_k(s)" /> outputs <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> if <img src="https://s0.wp.com/latex.php?latex=%7Cx-k%7C%3E0.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|x-k|&gt;0.5" class="latex" title="|x-k|&gt;0.5" />. We can then write the parity function (for example for <img src="https://s0.wp.com/latex.php?latex=I%3D%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I=[d]" class="latex" title="I=[d]" />) as <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Ctext%7B+odd+%7D+%5Cin+%5Bd%5D%7D+f_k%28%5Csum_%7Bi%3D1%7D%5Ed+%281-x_i%29%2F2+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" class="latex" title="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" />. This will be a linear combination of at most <img src="https://s0.wp.com/latex.php?latex=4d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="4d" class="latex" title="4d" /> ReLUs.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/NwQ31Jy.png" alt="" /></figure>



<p>Parities are an example of a case where simple feature are uninformative. For example, if <img src="https://s0.wp.com/latex.php?latex=%7CI%7C%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|I|&gt;1" class="latex" title="|I|&gt;1" /> then for every linear function <img src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" title="L:\mathbb{R}^d \rightarrow \mathbb{R}" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%28x%2Cy%29+%5Csim+D_I%7D%5B+L%28x%29y%5D+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" class="latex" title="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" /></p>



<p>in other words, there is no correlation between the linear function and the label.<br />To see why this is true, write <img src="https://s0.wp.com/latex.php?latex=L%28x%29+%3D+%5Csum+L_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(x) = \sum L_i x_i" class="latex" title="L(x) = \sum L_i x_i" />. By linearity of expectation, it suffices to show that $latex \mathbb{E}<em>{(x,y) \sim D_I}[ L_ix_i y] = L_i \mathbb{E}</em>{(x,y) \sim D_I}[ x_i y] = 0&amp;bg=ffffff$. Both <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y_i" class="latex" title="y_i" /> are just values in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{\pm 1 }" class="latex" title="{\pm 1 }" />. To evaluate the expectation <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+y%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[x_i y]" class="latex" title="\mathbb{E}[x_i y]" /> we simply need to know the marginal distribution that <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" /> induces on <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }^2" class="latex" title="{ \pm 1 }^2" /> when we restrict it to these two coordinates. This distribution is just the uniform distribution. To see why this is the case, consider a coordinate <img src="https://s0.wp.com/latex.php?latex=j%5Cin+I+%5Csetminus+%7B+i+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j\in I \setminus { i }" class="latex" title="j\in I \setminus { i }" /> and let’s condition on the values of all coordinates other than <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j" class="latex" title="j" />. After conditioning on these values, <img src="https://s0.wp.com/latex.php?latex=y+%3D+%5Csigma+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y = \sigma x_i x_j" class="latex" title="y = \sigma x_i x_j" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Csigma+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma \in { \pm 1 }" class="latex" title="\sigma \in { \pm 1 }" /> and <img src="https://s0.wp.com/latex.php?latex=x_i%2Cx_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i,x_j" class="latex" title="x_i,x_j" /> are chosen uniformly and independently from <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \pm 1 }" class="latex" title="{ \pm 1 }" />. For every choice of <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" />, if we flip <img src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_j" class="latex" title="x_j" /> then that would flip the value of <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, and hence the marginal distribution on <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> and <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> will be uniform.</p>



<p>This lack of correlation turns out to be a real obstacle for gradient-based algorithms. While small neural networks for parities exist, and Gaussian elimination can find them, it turns out that gradient-based algorithms such as SGD will <em>fail</em> to do so. Parities are hard to learn, and even if the capacity of the network is such that it can memorize the input, it will still perform poorly in a test set. Indeed, we can prove that for <em>every</em> neural network architecture <img src="https://s0.wp.com/latex.php?latex=f_w%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_w(x)" class="latex" title="f_w(x)" />, running SGD on <img src="https://s0.wp.com/latex.php?latex=%5Cmin%5ClVert+f_w%28x%29+-%5Cprod_%7Bi%5Cin+I%7D+x_i%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" class="latex" title="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" /> will require <img src="https://s0.wp.com/latex.php?latex=e%5E%7B%5COmega%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="e^{\Omega(d)}" class="latex" title="e^{\Omega(d)}" /> steps. (Note that if we add <em>noise</em> to parities, then Gaussian elimination will fail and it is believed that <em>no efficient algorithm</em> can learn the distribution in this case. This is known as the <a href="https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/Cryptography_from_learning_parity_with_noise.pdf">learning parity with noise</a> problem, which is also related to the <a href="https://en.wikipedia.org/wiki/Learning_with_errors">learning with errors</a> problem that is the foundation of modern lattice-based cryptography.)</p>



<p>We now sketch the proof that gradient-based algorithms require exponentially many steps to learn parities, following Theorem 1 of <a href="https://arxiv.org/abs/1703.07950">Shalev-Shwartz,Shamir and Shammah</a>. We think of an idealized setting where we have an unlimited number of samples and only use a sample <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y) \sim D_I" class="latex" title="(x,y) \sim D_I" /> only once (this should only make learning easier). We will show that we make very little progress in learning <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />, by showing that for any given <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />, the expected gradient over <img src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x,y)" class="latex" title="(x,y)" /> will be exponentially small, and hence we make very little progress toward learning <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />. Specifically, using the notation <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%3D%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x)=\prod_{i\in I}x_i" class="latex" title="\chi_I(x)=\prod_{i\in I}x_i" />, for any <img src="https://s0.wp.com/latex.php?latex=w%2Cx%2CI&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w,x,I" class="latex" title="w,x,I" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla_w+%5Cparallel+f_w%28x%29+-+%5Cchi_I%28x%29+%5Cparallel%5E2+%3D+2%5Csum_%7Bi%3D1%7D%5Ed+%5Cleft+%5B+f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29-+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" class="latex" title="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" /></p>



<p>The term <img src="https://s0.wp.com/latex.php?latex=f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" class="latex" title="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" /> is independent of <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and so does not contribute toward learning <img src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D_I" class="latex" title="D_I" />. Hence intuitively to show we make exponentially small progress, it suffices to show that typically for every <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" />, <img src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" class="latex" title="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" /> will be exponentially small. (That is, even if for a fixed <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> we make a large step, these all cancel out and give us exponentially small progress toward actually learning <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />.)</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/lWCuan2.png" alt="" /></figure>



<p>Formally, we will prove the following lemma:</p>



<p><strong>Lemma:</strong> For every <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" title="w" />, <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_I+%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2+%5Cleq+%5Ctfrac%7Bpoly%28d%2Cn%29%5Cmathbb%7BE%7D_x+%5Cfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5E2%7D%7B2%5Ed%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" class="latex" title="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" /></p>



<p><strong>Proof:</strong> Let us fix <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and define <img src="https://s0.wp.com/latex.php?latex=g%28x%29+%3D+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x) = \tfrac{d}{d x_i}f_{w}(x)" class="latex" title="g(x) = \tfrac{d}{d x_i}f_{w}(x)" />. The quantity <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" class="latex" title="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" /> can be written as <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2Cg+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \chi_I,g \rangle" class="latex" title="\langle \chi_I,g \rangle" /> with respect to the inner product <img src="https://s0.wp.com/latex.php?latex=%5Clangle+f%2Cg+%5Crangle+%3D+%5Cmathbb%7BE%7D_x+f%28x%29g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" class="latex" title="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" />. However, <img src="https://s0.wp.com/latex.php?latex=%7B+%5Cchi_I+%7D%7BI%5Csubseteq+%5Bd%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ \chi_I }{I\subseteq [d]}" class="latex" title="{ \chi_I }{I\subseteq [d]}" /> is an orhtonormal basis with respect to this inner product. To see this note that since <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x) \in { \pm 1 }" class="latex" title="\chi_I(x) \in { \pm 1 }" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+%5Cchi_I%28x%29%5E2+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x \chi_I(x)^2 = 1" class="latex" title="\mathbb{E}_x \chi_I(x)^2 = 1" /> for every <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" />, and for <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />, <img src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%5Cchi_J%28x%29+%3D+%28%5Cprod%7Bi+%5Cin+I%7D+x_i%29%28%5Cprod_%7Bj+%5Cin+J%7D+x_j%29+%3D+%5Cprod_%7Bk+%5Cin+I+%5Coplus+H%7D+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" class="latex" title="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" /> where <img src="https://s0.wp.com/latex.php?latex=I%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I\oplus J" class="latex" title="I\oplus J" /> is the symmetric difference of <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" />. The reason is that <img src="https://s0.wp.com/latex.php?latex=x_i%5E2+%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i^2 =1" class="latex" title="x_i^2 =1" /> for all <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" title="i" /> and so elements that appear in both <img src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I" class="latex" title="I" /> and <img src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J" class="latex" title="J" /> “cancel out”. Since the coordinates of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> are distributed independently and uniformly, the expectation of the product is the product of expectations. This means that as long as <img src="https://s0.wp.com/latex.php?latex=I+%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \oplus J" class="latex" title="I \oplus J" /> is not empty (i.e., <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />) this will be a product of one or more terms of the form <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BE%7D+x_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mathbb{E} x_k)" class="latex" title="(\mathbb{E} x_k)" />. Since <img src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_k" class="latex" title="x_k" /> is uniform over <img src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cpm+1+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\{ \pm 1 \}" class="latex" title="\{ \pm 1 \}" />, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x_k+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} x_k = 0" class="latex" title="\mathbb{E} x_k = 0" /> and so we get that if <img src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="I \neq J" class="latex" title="I \neq J" />, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2C%5Cchi_J+%5Crangle+%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle \chi_I,\chi_J \rangle =0" class="latex" title="\langle \chi_I,\chi_J \rangle =0" />.</p>



<p>Given the above</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+g%28x%29%5E2+%3D+%5Clangle+g%2Cg%5Crangle+%3D+%5Csum_%7BI+%5Csubseteq+%5Bd%5D%7D+%5Clangle+g+%2C+%5Cchi_I+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" class="latex" title="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" /></p>



<p>which means that (since there are <img src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^d" class="latex" title="2^d" /> subsets of <img src="https://s0.wp.com/latex.php?latex=%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[d]" class="latex" title="[d]" />) on average <img src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C+%5Cchi_I+%5Crangle+%3D+%5Cparallel+g+%5Cparallel%5E2+%2F+2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" class="latex" title="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" />. In other words, <img src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C%5Cchi_I+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle g,\chi_I \rangle" class="latex" title="\langle g,\chi_I \rangle" /> is typically exponentially small which is what we wanted to prove.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/"><span class="datestr">at February 17, 2021 09:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/">Alan Selman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>A special journal issue in his honor</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/all-9/" rel="attachment wp-att-18136"><img width="200" alt="" class="alignright  wp-image-18136" src="https://rjlipton.files.wordpress.com/2021/02/all.png?w=200" /></a></p>
<p>
Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal <a href="https://www.springer.com/journal/224">Theory of Computing Systems</a> dedicated to Alan Selman. Alan passed away this January 2021. </p>
<p>
Today we circulate their request for contributions.</p>
<p>
The details of the <a href="https://www.springer.com/journal/224/updates/18863610">call</a> say:  This special issue celebrates Alan’s life and commemorate his extraordinary contributions to the field. The topics of interest include but are not limited to: </p>
<ul>
<li>
average-case complexity <p></p>
</li><li>
circuit complexity <p></p>
</li><li>
comparison of reducibilities <p></p>
</li><li>
complexity theoretic characterizations of models <p></p>
</li><li>
function complexity <p></p>
</li><li>
hierarchy theorems <p></p>
</li><li>
	parameterized complexity <p></p>
</li><li>
promise problems and disjoint NP-pairs <p></p>
</li><li>
public-key cryptography <p></p>
</li><li>
relativization <p></p>
</li><li>
semi-feasible algorithms <p></p>
</li><li>
sparse sets <p></p>
</li><li>
structure of complete sets
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>Please look at <a href="https://www.springer.com/journal/224/updates/18863610">this</a> for details—the deadline for submission is 31st July 2021. You have 164 days to write your paper. Which is 3936 hours or 236160 minutes.</p>
<p>
Please send a contribution. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/"><span class="datestr">at February 17, 2021 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2021/02/17/stoc-2021-workshops/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2021/02/17/stoc-2021-workshops/">STOC 2021 workshops</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 21-25, 2021 Online http://acm-stoc.org/stoc2021/callforworkshops.html Submission deadline: March 15, 2021 STOC 2021 will hold workshops during the conference week, June 21–25, 2021. We invite groups of interested researchers to submit workshop proposals. The due date for proposals is March 15.</div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2021/02/17/stoc-2021-workshops/"><span class="datestr">at February 17, 2021 04:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings.html">Lattice Borromean rings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A lot of topology is finding ways to prove things that are really obvious but where explaining why they’re obvious can be difficult. So I want to do this for a discrete analogue of <a href="https://en.wikipedia.org/wiki/Ropelength">ropelength</a>, the length of the shortest lattice representation, for the <a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean rings</a>. You can find several pretty lattice (and non-lattice) representations of the Borromean rings in a paper by Verhoeff &amp; Verhoeff, “<a href="https://archive.bridgesmathart.org/2015/bridges2015-53.pdf">Three families of mitered Borromean ring sculptures</a>” [<em>Bridges</em>, 2015]; the one in the middle of their figure 2, thinned down to use only lattice edges and not thick solid components, is the one I have in mind. It is formed by three \(2\times 4\) rectangles, shown below next to <a href="https://en.wikipedia.org/wiki/Jessen%27s_icosahedron">Jessen’s icosahedron</a> which has the same vertex coordinates. (You can do the same thing with a regular icosahedron but then you get non-lattice golden rectangles.)</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/Borromean-Jessen.svg" alt="Lattice Borromean rings and Jessen's icosahedron" /></p>

<p>Each of the three rectangles has perimeter \(12\), so the total length of the whole link is \(36\). Why should this be the minimum possible? One could plausibly run a brute force search over all small-enough realizations, but this would be tedious and some effort would be needed to prune the search enough to get it to run at all. Instead, I found an argument based on the lengths of the individual components of the link, allowing me to analyze them (mostly) separately.</p>

<p>Each component is unknotted, so it can be the boundary of a disk in space. Importantly, for the Borromean rings, every disk spanned by one of the components must be crossed at least twice by other components. If we could find a disk spanned by one component that was not crossed at all by other components, then we could shrink the first component topologically within its disk down to a size so small that it could easily be pulled apart from the other two components, something that is not possible with the Borromean rings. And if we could find a disk that was only crossed once by another component, then the <a href="https://en.wikipedia.org/wiki/Linking_number">linking number</a> of the two components would be one, something that doesn’t happen for the Borromean rings.</p>

<p>If you travel in some consistent direction around a cycle in a 3d lattice, every step in one direction along a coordinate axis must be cancelled by a step in the opposite direction elsewhere along the ring. So if a lattice cycle has length \(\ell\), there must be \(\ell/2\) pairs of opposite steps, partitioned somehow among the three dimensions. If the bounding box of the cycle has size \(a\times b\times c\), then we must have \(a+b+c\le\ell/2\), and we can classify the possible shapes of lattice cycles of length \(\ell\) by the possible shapes of their bounding boxes. This gives us the following cases:</p>

<ul>
  <li>
    <p>A lattice cycle of length \(\ell=4\) can only be a square, with bounding box dimensions \(1\times 1\times 0\) (the zero means that it lies in a single plane in 3d, not that it doesn’t exist at all). The square itself is a disk not crossed by any other lattice path, unusable as a component of the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=6\) can be a rectangle with bounding box \(2\times 1\times 0\), or fully 3-dimensional with bounding box \(1\times 1\times 1\). There are two fully 3-dimensional cases, one that avoids two opposite vertices of the bounding box and one that avoids two adjacent vertices. The rectangle can be its own spanning disk, and in the 3-dimensional cases we can use a spanning disk connecting the center of the bounding cube by a line segment to each point along the ring. Neither of these types of disks is crossed by any other lattice path.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/grid-6-cycles.svg" alt="Spanning disks for three grid 6-cycles" /></p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=8\) can be a rectangle with bounding box \(3\times 1\times 0\), a square with bounding box \(2\times 2\times 0\) or fully 3-dimensional with bounding box \(2\times 1\times 1\). It can also double back on itself and cover all vertices of a cube, with bounding box \(1\times 1\times 1\). All cases except the \(2\times 2\times 0\) square can be handled as in the length \(6\) cases; for instance, for the \(2\times 1\times 1\) bounding box we form a disk at the center of the box, connected by a line segment to all points of the ring. These cases cannot be crossed by any other lattice path. The \(2\times 2\times 0\) square can be crossed by a lattice path, through its center point, but only by one path. We can see from this that the shortest lattice representation of the <a href="https://en.wikipedia.org/wiki/Hopf_link">Hopf link</a> (two linked circles) is the obvious one formed from two length-\(8\) squares. However, these squares are still too small to be used in the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=10\) can be a rectangle with bounding box \(4\times 1\times 0\) or \(3\times 2\times 0\), or fully 3-dimensional with bounding box \(3\times 1\times 1\) or \(2\times 2\times 1\). The \(4\times 1\times 0\) rectangle and the center-point spanning disk of the \(3\times 1\times 1\) box cannot be crossed by any other lattice path, and the center-point spanning disk of the \(2\times 2\times 1\) can be crossed only by one, through its center edge. Using even-smaller bounding boxes doesn’t help.</p>
  </li>
</ul>

<p>That leaves only one problematic case, the \(3\times 2\times 0\) rectangle, of perimeter \(10\), which is shorter than the rectangles of the optimal representation but can nevertheless be crossed by two other lattice paths. In fact, this rectangle can be used as a component in a representation of the Borromean rings. It is even possible to use two of them! (I’ll leave this as an exercise.) So we need some other argument to prove that, when we use one or two of these short rectangles, we have to make up for it elsewhere by making something else extra-long.</p>

<p>If a \(3\times 2\times 0\) rectangle is a component of the Borromean rings, it must be twice by one of the other components, because if its two crossings were from different components it would have nonzero linking number with both of them, different from what happens in the Borromean rings. And the crossings must happen at the two interior lattice points of the rectangle, through paths that (to avoid each other and the boundary of the rectangle) must pass straight across the rectangle, at least for one unit on each side. The component that crosses the rectangle in this way consists of two loops connecting the pairs of ends of these two straight paths; any other connection pattern would lead to linking number \(2\), not zero. We can think of these two loops as being separate cycles, shortcut by the lattice edges between the endpoints of the two straight paths. And any disk that spans either of these two loops must itself be crossed by another component of the Borromean rings, because if one of the loops had an uncrossed spanning disk then we could wrap a spanning disk for the rectangle around it (like a glove around a hand) and create an uncrossed spanning disk for the rectangle as well.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/glove.png" alt="Spanning disk wrapping around a loop like a glove around a hand, adapted from https://commons.wikimedia.org/wiki/File:Disposable_nitrile_glove_with_transparent_background.png" /></p>

<p>By the analysis above, in order to be crossed by something else, both of the two shortcut loops of the component that crosses the \(3\times 2\times 0\) rectangle must have length at least \(8\). Adding in the two straight paths (and removing the two shortcut edges) shows that the component itself must have length at least \(18\). And if we have one component of length \(18\) and two more of length \(10\), we get total length at least \(38\), more than the length of the minimal representation. Since all representations that use components of length less than \(12\) are too long, the representation in which all three component lengths are exactly \(12\) must be the optimal one, QED.</p>

<p>Researching all this led me to an interesting paper by Dai, Ernst, Por, and Ziegler,  “<a href="https://doi.org/10.1142/S0218216519500858">The ropelengths of knots are almost linear in terms of their crossing numbers</a>” [<em>J. Knot Theory and its Ramifications</em>, 2019]. Ropelength is the minimum length of a 3d representation that can be thickened to a radius-1 tube without self-intersections. (Some sources use diameter in place of radius; this changes the numeric values by a factor of two but does not change the optimizing representations.) Doubling the dimensions of a lattice representation gives you such a representation, and on the other hand one can find short lattice representations by following the thickened tubes of a ropelength representation, so ropelength and lattice length are within constant factors of each other. Dai et al. use this to show that knots that can be drawn in the plane with few crossings also have small ropelength. It doesn’t work to use the plane embedding directly, adding a third coordinate to handle the crossings, because some planar graphs (like the <a href="https://en.wikipedia.org/wiki/Nested_triangles_graph">nested triangles graph</a>) have nonlinear total edge length in any planar lattice drawing. Instead, Dai et al show how to crumple up a planar drawing of any degree-four planar graph into a 3d integer lattice embedding of the graph, with near-linear total edge length, so that the faces of the drawing can also be embedded as disks that are not crossed by each other or the graph edges. One can then modify the lifted drawing to turn the degree-four vertices into crossings in the lifted topologically-planar surface formed by these faces, giving a grid representation of the original knot with near-linear total length.</p>

<p>The ropelength of the Borromean rings has also been the subject of some study. Doubling the grid rectangles and rounding off their corners produces three <a href="https://en.wikipedia.org/wiki/Stadium_(geometry)">stadia</a> with total perimeter \(12\pi+24\approx 61.7\). The same argument as above shows that each curve must be at least long enough for all its spanning disks to be crossable by two disjoint radius-1 tubes. Intuitively the smallest curve that can surround two tubes is a smaller stadium corresponding to the \(2\times 3\) rectangle, with length \(4\pi+4\). If so, this would give a lower bound of \(12\pi+12\approx 49.7\) for the total ropelength of the Borromean rings. The conjectured-optimal configuration, <a href="https://archive.bridgesmathart.org/2008/bridges2008-63.html">used for the logo of the International Mathematical Union</a>, uses three copies of a complicated two-lobed planar curve in roughly the same positions as the three rectangles or stadia; it is described carefully by Cantarella, Fu, Kusner, Sullivan, and Wrinkle, “<a href="http://dx.doi.org/10.2140/gt.2006.10.2055">Criticality for the Gehring link problem</a>” [<em>Geometry &amp; Topology</em> 2006] (section 10), and has length \(\approx 58.006\). The intuition that the \(2\times 3\) stadium is the shortest curve that can surround two others also appears to be stated as proven in this paper, in section 7.1. But they state that the best lower bound for the Borromean ropelength is \(12\pi\) so maybe the \(12\pi+12\) argument above is new?</p>

<p><strong>Update, February 17:</strong> In email, John Sullivan pointed me to a paper by Uberti, Janse van Rensburg, Orlandinit, Tesi, and Whittington, “<a href="https://doi.org/10.1007/978-1-4612-1712-1_9">Minimal links in the cubic lattice</a>” [<em>Topology and Geometry in Polymer Science</em>, 1998; see table 2, p. 97], which does the tedious computer search and comes up with the same result, that the shortest length for a lattice representation of the Borromean rings is 36. (I had searched for papers on lattice representations of the Borromean rings but didn’t find this one, probably failing because it identifies the Borromean rings only by the <a href="https://en.wikipedia.org/wiki/Alexander%E2%80%93Briggs_notation">Alexander–Briggs notation</a> \(6_2^3\), which is hard to search for.) John also tells me that the proof of ropelength-minimality of the \(2\times 3\) stadium is only for links in which it is linked with two other components, different enough from the situation here in which every spanning disk is crossed twice that the same proof doesn’t apply. So the question of whether this stadium really is the ropelength minimizer for components satisfying this crossed-twice condition seems to fall into the category of obvious topological facts that are difficult to prove, rather than being already known.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105743724683948185">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings.html"><span class="datestr">at February 16, 2021 04:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5330">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5330">On standing up sans backbone</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<blockquote class="wp-block-quote"><p><strong>Note:</strong> To get myself into the spirit of writing this post, tonight I watched the 2019 movie <a href="https://www.amazon.com/Mr-Jones-James-Norton/dp/B089XVJB9S/ref=sr_1_1?dchild=1&amp;keywords=Mr.+Jones+%282019%29&amp;qid=1613446265&amp;s=instant-video&amp;sr=1-1">Mr. Jones</a>, about the true story of the coverup of Stalin’s 1932-3 mass famine by <em>New York Times</em> journalist <a href="https://en.wikipedia.org/wiki/Walter_Duranty">Walter Duranty</a>.  Recommended!</p></blockquote>



<p>In my <a href="https://www.scottaaronson.com/blog/?p=5310">last post</a>, I wrote that despite all my problems with Cade Metz’s <em>New York Times</em> hit piece on Scott Alexander, I’d continue talking to journalists—even Metz himself, I added, assuming he’d still talk to me after my public disparagement of his work.  Over the past few days, though, the many counterarguments in my comments section and elsewhere gradually caused me to change my mind.  I now feel like to work with Metz again, even just on some quantum computing piece, would be to reward—and to be seen as rewarding—journalistic practices that are making the world worse, and that this consideration overrides even my extreme commitment to openness.</p>



<p>At the least, before I could talk to Metz again, I’d need a better understanding of how the hit piece happened.  What was the role of the editors?  How did the original hook—namely, the rationalist community’s early rightness about covid-19—disappear entirely from the article?  How did the piece manage to evince so little <em>curiosity</em> about such an unusual subculture and such a widely-admired writer?  How did it fail so completely to engage with the rationalists’ <em>ideas</em>, instead jumping immediately to “six degrees of Peter Thiel” and other reductive games?  How did an angry SneerClubber, David Gerard, end up (according to <a href="https://twitter.com/davidgerard/status/1360735880466604040">his own boast</a>) basically dictating the NYT piece’s content?</p>



<p>It’s always ripping-off-a-bandage painful to admit when trust in another person was wildly misplaced—for then who<em> else</em> can we not trust?  But sometimes that’s the truth of it.</p>



<p>I continue to believe passionately in the centrality of good journalism to a free society.  I’ll continue to talk to journalists often, about quantum computing or whatever else.  I also recognize that the NYT is a large, heterogeneous institution (I myself <a href="https://www.nytimes.com/2011/12/06/science/scott-aaronson-quantum-computing-promises-new-insights.html">published</a> in it <a href="https://www.nytimes.com/2019/10/30/opinion/google-quantum-computer-sycamore.html">twice</a>); it’s not hard to imagine that many of its own staff take issue with the SSC piece.</p>



<p>But let’s be clear about the stakes here.  In the discussion of my last post, I <a href="https://www.scottaaronson.com/blog/?p=5310#comment-1878641">described</a> the NYT as “still the main vessel of consensus reality in <s>human civilization</s>” [alright, alright, American civilization!].  What’s really at issue, beyond the treatment of a single blogger, is whether the NYT can continue serving that central role in a world reshaped by social media, resurgent fascism, and entitled wokery.</p>



<p>Sure, we all know that the NYT has been disastrously wrong before: it ridiculed Goddard’s dream of spaceflight, denied the Holodomor, relegated the Holocaust to the back pages while it was happening, published the fabricated justifications for the Iraq War.  But the NYT and a few other publications were still the blockchain of reality, the engine of the consensus of all that is, the last bulwark against the conspiracists and the anti-vaxxers and the empowered fabulists and the horned insurrectionists storming the Capitol, because there was no ability to coordinate around any serious alternative.  I’m <em>still</em> skeptical that there’s a serious alternative, but I now look more positively than I did just a few days ago on attempts to create one.</p>



<p>To all those who called me naïve or a coward for having cooperated with the NYT: believe me, I’m well aware that I wasn’t born with much backbone.  (I am, after all, that guy on the Internet who famously once planned on a life of celibate asceticism, or more likely suicide, rather than asking women out and thereby risking eternal condemnation as a misogynistic sexual harasser by the normal, the popular, the socially adept, the … <em>humanities grads</em> and the <em>journalists</em>.)  But whenever I need a pick-me-up, I tell myself that rather than being ashamed about my lack of a backbone, I can take pride in having occasionally managed to stand even without one.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5330"><span class="datestr">at February 16, 2021 05:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://cacm.acm.org/opinion/articles/250078-lets-not-dumb-down-the-history-of-computer-science/fulltext">Let’s not dumb down the history of computer science</a> (<a href="https://mathstodon.xyz/@11011110/105663355639241434">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/190214/Lets-Not-Dumb-Down-the-History-of-Computer-Science">via</a>). A 2014 plea from Knuth to historians of computer science to stop ignoring the technical parts of the history, reprinted this month in CACM.</p>
  </li>
  <li>
    <p><a href="https://www.newscientist.com/article/2266041-tom-gaulds-runaway-lobster-telephone-problem/">Studies in ethical surrealism: the runaway lobster telephone problem</a> (<a href="https://mathstodon.xyz/@11011110/105664424099324982">\(\mathbb{M}\)</a>). I was pleased to learn that <a href="https://en.wikipedia.org/wiki/Lobster_Telephone">the lobster telephone depicted in this cartoon is a real objet d’art</a>.</p>
  </li>
  <li>
    <p><a href="https://www.archim.org.uk/eureka/archive/">Archive of back issues of Eureka</a> (<a href="https://mathstodon.xyz/@11011110/105671433589477405">\(\mathbb{M}\)</a>, <a href="https://aperiodical.com/2021/02/aperiodical-news-roundup-january-2021/">via</a>), the recreational mathematics journal of the Cambridge Archimedeans, now online for open access. On Wikipedia, the popular articles from Eureka appear to be Dyson’s work on ranks of partitions, in #8, Haselgrove &amp; Haselgrove on polyominoes, in #23, Penrose on pentaplexity, in #39, and Leinster on his eponymous groups, in #55.</p>
  </li>
  <li>
    <p><a href="https://philpapers.org/rec/BOBFPT">In a new book chapter, Susanne Bobzien claims that famous philosopher of logic Gottlob Frege plagiarized extensively from the Stoic logicians</a> (<a href="https://mathstodon.xyz/@11011110/105680228594244342">\(\mathbb{M}\)</a>, <a href="https://dailynous.com/2021/02/03/frege-plagiarize-stoics/">via</a>, <a href="https://www.metafilter.com/190330/Frege-plagiarized-the-Stoics">via2</a>, <a href="https://handlingideas.blog/2021/02/05/the-stoic-foundations-of-analytic-philosophy-on-susanne-bobziens-groundbreaking-discovery-in-frege-and-prantl/">see also</a>).</p>
  </li>
  <li>
    <p>How did I not know about the <a href="https://civs.cs.cornell.edu/">Condorcet Internet Voting Service</a> before (<a href="https://mathstodon.xyz/@11011110/105683306227631389">\(\mathbb{M}\)</a>)? Set up public or private polls and collate the results with your favorite Condorcet rank aggregation method (at least, if your favorite is one of the five they implement, which it probably is). Their public polls are kind of insipid, though, and in comments David Bremner brings up their past history of enabling online abusers.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@RefurioAnachro/105684468712016832">Perkel’s graph and the 57-cell</a>, multi-post sequence on an abstract 4-polytope and associated distance-regular graph, by Refurio Anachro.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">The Victoria Delfino Problems</a> (<a href="https://mathstodon.xyz/@11011110/105694384937282737">\(\mathbb{M}\)</a>). Bill Gasarch blogs about mathematics problems named after non-mathematicians, in this case a Los Angeles based real estate agent.</p>
  </li>
  <li>
    <p>The speech recognition system Zoom and/or my university are using to auto-caption my recorded lectures (whatever it is) really doesn’t like the word “bipartite”, heavily used in my lecture on matching (<a href="https://mathstodon.xyz/@11011110/105700305493373685">\(\mathbb{M}\)</a>). It came out “bipartisan”, “invite part tight”, “by party”, “by protect”, “by apartheid”, “by part aight”, and “by partnership”. Also “spanning forest” is now “Hispanic forest”, but mysteriously it got “spanning tree” right.</p>
  </li>
  <li>
    <p><a href="https://retractionwatch.com/2021/02/09/20-ways-to-spot-the-work-of-paper-mills/">20 ways to spot the work of paper mills</a> (<a href="https://mathstodon.xyz/@11011110/105702699189961130">\(\mathbb{M}\)</a>). However one, using a non-institutional email address, is not “a bad global habit”, but deliberate. I have no thought of moving but do not want my entire professional life tied by email to my employer. My UCI address keeps student emails private but I tend to use gmail for off-campus concerns such as publishers. And not all scholars have institutions who can provide emails. If they refuse my email, I refuse to publish with them.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.01543">Ben Green presents super-polynomial lower bounds for off-diagonal van der Waerden numbers \(W(3,k)\)</a> (<a href="https://mathstodon.xyz/@11011110/105713896982428000">\(\mathbb{M}\)</a>, <a href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/">via</a>). \(W(3,k)\) is the smallest \(N\) such that a 2-coloring of \([N]\) has a 3-term arithmetic progression of one color or a \(k\)-term progression of the other. It was previously known to be subexponential and thought to be only quadratic.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/382940/440">The compound of an 11-simplex in an 11-hypercube (as a subset of its vertices) has the Mathieu group M11 as its symmetries</a> (<a href="https://mathstodon.xyz/@11011110/105717258433892969">\(\mathbb{M}\)</a>, <a href="https://cp4space.hatsya.com/2021/02/08/a-curious-construction-of-the-mathieu-group-m11/">via</a>). The via link goes on to describe how to find two dual 11-simplices in the same hypercube from the perfect ternary Golay code, much like the two simplices in a 3-cube that form the stella octangula.</p>
  </li>
  <li>
    <p><a href="https://distill.pub/selforg/2021/textures/">Self-organizing textures</a> (<a href="https://mathstodon.xyz/@11011110/105719504474157451">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26112959">via</a>). A small input image + “neural cellular automata” magic leads to organic-looking image textures.</p>
  </li>
  <li>
    <p><a href="https://cse.buffalo.edu/socg21/accepted.html">Accepted papers for the Symposium on Computational Geometry</a> (SoCG; <a href="https://mathstodon.xyz/@11011110/105727235598338446">\(\mathbb{M}\)</a>). Decisions are out for the Symposium on Theory of Computing (STOC) but I haven’t seen a public list yet. Upcoming submission deadlines include the <a href="https://projects.cs.dal.ca/wads2021/">Algorithms and Data Structures Symposium</a> (WADS, Feb. 20), <a href="https://wg2021.mimuw.edu.pl/">Graph-Theoretic Concepts in Computer Science</a> (WG, Mar. 3), and the new <a href="https://www.siam.org/conferences/cm/conference/acda21">SIAM Conference on Applied and Computational Discrete Algorithms</a> (ACDA21, Mar. 1).</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/ode-to-hippasus/">A new contribution of Hypatia to mathematics</a> (<a href="https://mathstodon.xyz/@11011110/105732195870260348">\(\mathbb{M}\)</a>). Not the ancient Hypatia, but Hypatia Hamkins, and her parents, philosopher Barbara Gail Montero and logician Joel David Hamkins; the contribution is a verse proof of the irrationality of \(\sqrt{2}\).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/channel/UC8bRNi3tJX-tfR_RMtyWR7w">Computational Geometry YouTube channel</a> (<a href="https://mathstodon.xyz/@11011110/105739350560629546">\(\mathbb{M}\)</a>). This has been set up by Sariel Har-Peled and Sándor Fekete, and is recording talks from the New York Geometry Seminar. So far there are eleven, of roughly an hour length each.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/15/linkage.html"><span class="datestr">at February 15, 2021 09:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/">Pigenhole Principle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Mathematics is based on the application of simple ideas over and over: From tiny nuts do big trees grow. </em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/jv/" rel="attachment wp-att-18113"><img width="150" alt="" class="alignright wp-image-18113" src="https://rjlipton.files.wordpress.com/2021/02/jv.png?w=150" /></a></p>
<p>
Jorgen Veisdal is an assistant professor at the Norwegian University of Science and Technology. He is also the editor in chief at <a href="https://medium.com/cantors-paradise">Cantor’s Paradise</a>, which is a publication of math and science essays. </p>
<p>
Today I thought we would discuss a <a href="https://medium.com/cantors-paradise/the-pigeonhole-principle-e4c637940619">post</a> of his on the famous Pigenhole Princeiple (PP).</p>
<p>
Recall the PP states that if <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> items are put into <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m}" class="latex" title="{m}" /> boxes, with <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3E+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n &gt; m}" class="latex" title="{n &gt; m}" />, then at least one box must contain more than one item.</p>
<p>
The paradox in my opinion is that this idea has any power at all. I wonder if I could explain why it was stated as an explicit <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle">principle</a> by the famous Peter Dirichlet under the name Schubfachprinzip (“drawer principle” or “shelf principle”) in 1834. </p>
<p>
Parts of mathematics not only use PP, but could not live without it. Other parts of mathematics—I believe—are almost untouched by it. Am I right about this? Number theory and combinatorics especially Ramsey theory could not survive without it. What happens in your favorite area? Is there some area of math that is almost untouched by PP?</p>
<p>
</p><table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/pigeons/" rel="attachment wp-att-18114"><img width="400" alt="" class="aligncenter  wp-image-18114" src="https://rjlipton.files.wordpress.com/2021/02/pigeons.png?w=400" /></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p></p><h2> An Example of PP </h2><p></p>
<p>
The main issue is why is PP so indispensable to some areas of math. But I though it might be fun to give a sample type of proof that uses PP.</p>
<p>
Prove that however one selects 55 integers 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%5Cle+x_%7B1%7D+%3C+x_%7B2%7D+%3C+x_%7B3%7D+%3C+%5Cdots+%3C+x_%7B55%7D+%5Cle+100%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100," class="latex" title="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100," /></p>
<p>there will be some two that differ by 9, some two that differ by 10, a pair that differ by 12, and a pair that differ by 13. Surprisingly, there need not be a pair of numbers that differ by 11. </p>
<p></p><h2> Proof </h2><p></p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28y%2Cx%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f(y,x)}" class="latex" title="{f(y,x)}" /> be the number of collisions when placing <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{y}" class="latex" title="{y}" /> into <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" />. Claim:	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Bd%2Cx%29+%5Cge+f%28x%2B1%2Cx%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x+d,x) \ge f(x+1,x), " class="latex" title="\displaystyle  f(x+d,x) \ge f(x+1,x), " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \ge 1}" class="latex" title="{x \ge 1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d \ge 1}" class="latex" title="{d \ge 1}" /> and 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2B1%2Cx%29+%5Cge+f%28x%2Cx-1%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  f(x+1,x) \ge f(x,x-1), " class="latex" title="\displaystyle  f(x+1,x) \ge f(x,x-1), " /></p>
<p>for <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x \ge 2}" class="latex" title="{x \ge 2}" />. </p>
<p>
Note the first is really simple. Consider the first <img src="https://s0.wp.com/latex.php?latex=%7Bx%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x+1}" class="latex" title="{x+1}" /> pigeons. They are placed into <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> places and the inequality follows. The second is about the same. Consider the first <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" title="{x}" /> pigeons. There are two cases. They are all placed in <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" /> places. Then we are done. So there must have been some placed into the last place. But if two are there then we are also done. So <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" /> are placed into <img src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x-1}" class="latex" title="{x-1}" />. But where does the last one go? In either case we are done.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p>Are there areas that almost never use the PP? I would like to hear about areas that just do not use PP. </p>
<p></p><p><br />
[some word fixes]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/"><span class="datestr">at February 15, 2021 05:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/">Postdoc at National University of Singapore (apply by March 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One post-doctoral position is available. The goal of the intended projects is to develop new frameworks and techniques for testing properties of functions and distributions on high-dimensional spaces.</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~arnab/testing-postdoc.html">https://www.comp.nus.edu.sg/~arnab/testing-postdoc.html</a><br />
Email: arnabb@nus.edu.sg</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/15/postdoc-at-national-university-of-singapore-apply-by-march-31-2021-2/"><span class="datestr">at February 15, 2021 11:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
