<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.blogger.com/feeds/25562705/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://benjamin-recht.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://www.blogger.com/feeds/21224994/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at September 09, 2020 11:27 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/135">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/135">TR20-135 |  Estimation of Graph Isomorphism Distance in the Query World | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Arijit Ghosh, 

	Gopinath Mishra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The graph isomorphism distance between two graphs $G_u$ and $G_k$ is the fraction of entries in the adjacency matrix that has to be changed to make $G_u$ isomorphic to $G_k$. We study the problem of estimating, up to a constant additive factor, the graph isomorphism distance between two graphs in the query model. In other words, if $G_k$ is a known graph and $G_u$ is an unknown graph whose adjacency matrix has to be accessed by querying the entries, what is the query complexity for testing whether the graph isomorphism distance between $G_u$ and $G_k$ is less than $\gamma_1$ or more than $\gamma_2$, where $\gamma_1$ and $\gamma_2$ are two constants with $0\leq \gamma_1 &lt; \gamma_2 \leq 1$. It is also called the tolerant property testing of graph isomorphism in the dense graph model. The non-tolerant version (where $\gamma_1$ is $0$) has been studied by Fischer and Matsliah (SICOMP'08). 


In this paper, we study both the upper and lower bounds of tolerant graph isomorphism testing. We prove an upper bound of $\widetilde{{\cal O}}(n)$ for this problem. Our upper bound algorithm crucially uses the tolerant testing of the well studied Earth Mover Distance (EMD), as the main subroutine, in a slightly different setting from what is generally studied in property testing literature.


Testing tolerant EMD between two probability distributions is equivalent to testing EMD between two multi-sets, where the multiplicity of each element is taken appropriately, and we sample elements from the unknown multi-set with replacement. In this paper, our (main conceptual) contribution is to introduce the problem of tolerant EMD testing between multi-sets (over Hamming cube) when we get samples from the unknown multi-set without replacement and to show that this variant of tolerant testing of EMD is as hard as tolerant testing of graph isomorphism between two graphs. Thus, while testing of equivalence between distributions is at the heart of the non-tolerant testing of graph isomorphism, we are showing that the estimation of the EMD over a Hamming cube (when we are allowed to sample without replacement) is at the heart of 
tolerant graph isomorphism. We believe that the introduction of the problem of testing EMD between multi-sets (when we get samples without replacement) opens an entirely new direction in the world of testing properties of distributions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/135"><span class="datestr">at September 09, 2020 08:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1387">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1387">News for August 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report.</p>



<p><strong>Testing asymmetry in bounded degree graphs</strong>, by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/118/">ECCC</a>). This paper studies a natural graph property hitherto not considered in the property testing literature. Namely, the question of testing whether a graph is <em>asymmetric</em> or whether it is far from being asymmetric. A graph is said to be asymmetric if its automorphism group is trivial (that is, it only contains the identity permutation). One of the results in the paper says that this problem is easy in the dense graph model – which is a side result of the paper. This is because all dense graphs are \(O(\log n/n)\)-close to being asymmetric. To see this, the paper points out that a simple randomized process which takes \(G\) as input and returns an asymmetric graph by changing very few edges. This process asks you to do the following: Take a set \(S \subseteq V\) with \(|S| = O(\log n)\) nodes and replace the subgraph they induce with a random graph. Moreover,  randomize all the edges between \(S\) and \(V \setminus S\). What you can show is that in this modified graph, any automorphism (whp) will map \(S\) to itself. And all the remaining vertices behave (whp) in a unique manner which is peculiar to it. In particular, this means that any automorphism better not map a vertex \(v\) in \(V \setminus S\) to any other vertex in \(V \setminus S\). And this finishes the argument. The main result explores the bounded degree model. By a simple argument, you can show that testing asymmetry is easy if all the connected components have size \(s(n) \leq o\left(\frac{\log n}{\log {\log n}}\right)\). Thus, the challenging case is when you have some connected components of a larger size. In this case, what Goldreich shows is the following: If all the components have size at most \(s(n)\) where \(s(n) \geq \Omega\left(\frac{\log n}{\log {\log n}}\right)\), then you can test asymmetry (with one-sided-error) in \(O(n^{1/2} \cdot s(n)/\epsilon)\) queries.  Moreover, the paper also shows a two-sided-lower bound of \(\Omega\left(n/s(n)\right)^{1/2}\) queries which holds as long as \(\epsilon \leq O(1/s(n)\). This leaves open the bounded degree question of determining the query complexity of testing asymmetry in the general case as the paper also points out.</p>



<p></p>



<p><strong>On testability of first order properties in Bounded degree graphs</strong>, by Isolde Adler, Noleen Köhler and Pan Peng (<a href="https://arxiv.org/pdf/2008.05800.pdf">arXiv</a>). One of the well understood motivations for property testing begins in the following manner. Decision problems are typically hard to solve if they involve a universal quantifier — either \(\exists\) or  \(\forall\). One way around this hardness is to do the following ritual: relax the problem by dropping the universal quantifier, define a notion of distance between objects in your universe and ask a promise problem instead. Indeed, if you take your favorite property testing problem, you will note that it precisely fits in the template above. How about making this business more rigorous in bounded degree graph property model? This is precisely the content of this work which considers the face off between (First Order) logic and property testing in the bounded degree model. The authors show some interesting results. They begin by showing that in the bounded degree model, you can show that all first order graph properties which involve a single quantifier \(Q \in \{\forall, \exists\}\) are testable with constant query complexity.<br />If you find this baffling, it would be good to remind yourself that not all graph properties can be expressed in the language of first order logic with a single quantifier! So, you can rest easy. The graph properties you know are not constant time testable are most assuredly not expressible with a single quantifier in First Order Logic. However, this work shows more. It turns out, that “any FO property that is defined by a formula with quantifier prefix \(\exists^* \forall^*\) is testable”. Moreover, there do exist FO graph properties defined by the quantifier prefix \(\forall^* \exists^*\) which are not testable. Thus, this work achieves results in the bounded degree model which are kind of analogous to the results in the dense graph model by Alon et al [1]. On a final note, I find the following one sentence summary of the techniques used to prove the lower bound rather intriguing: “[the paper] obtains the lower bound by a first-order formula that defines a class of bounded-degree expanders, based on zig-zag products of graphs.”  </p>



<p></p>



<p><strong>On graphs of bounded degree that are far from being Hamiltonian</strong>, by Isolde Adler and Noleen Köhler (<a href="https://arxiv.org/abs/2008.05801">arXiv</a>). This paper explores the question of testing Hamiltonicity in the bounded degree model. The main result of the paper is that Hamiltonicity is not testable with one-sided error with \(o(n)\) queries. PTReview readers might recall from our <a href="https://ptreview.sublinear.info/?p=1371">July Post</a> a concurrent paper by Goldriech [2] which achieves the same lower bound on query complexity in the two-sided error model (the authors call attention to [2] as well). One of the interesting feature of this result is that the lower bounds are obtained by an explicit deterministic reduction as opposed to the usual randomized reduction. Like the authors point out, this offers more insights into structural complexity of instances that are far from being Hamiltonian. We point out that this also differs from how the lower bound is derived in [2] — which is via local hardness reductions to a promise problem of 3 CNF satisfiability.</p>



<p></p>



<p><strong>An optimal tester for \(k\)-linear</strong>, by Nader Bshouty (<a href="https://eccc.weizmann.ac.il/report/2020/123/">ECCC</a>). This paper explores two related questions. We call a function \(f \colon \{0,1\}^n \to \{0,1\}\) \(k\)-linear if it equals the \(\sum_{i \in S} x_i\) for some \(S \subseteq [n]\) of size exactly \(k\). A boolean function is said to be \(k\)-linear<strong>*</strong> if it is \(j\) linear for a fixed  \(j\) where \(j \in \{0,1,2, \cdots, k\}\). The paper proves the following theorems.</p>



<ol><li>There exists a non-adaptive <em>one-sided</em> distribution free tester for \(k\)-linear<strong>*</strong> with query complexity being \(O\left(k \log k + \frac{1}{\varepsilon}\right)\). This matches the two-sided lower bound (where the underlying distribution is uniform) by Blais et al [3].</li><li>Using a reduction from \(k\)-linear<strong>*</strong> to \(k\)-linear, the paper shows one can obtain a non-adpative <em>two-sided</em> distribution free tester for \(k\)-linear with same query complexity as the above result. The lower bound from Blais et al applies here also (in fact, they prove a lower bound on \(k\)-linearity).</li><li>Next up, the paper has a couple of lower bound results to accompany this. One of these results reveals the price you pay for being <em>one-sided</em> and <em>exact</em> (that is, you insist on the function being exactly \(k\)-linear). Turns out, now you have a non-adaptive one-sided uniform distribution lower bound of \(\widetilde{\Omega}(k) \log n + \Omega(1/\varepsilon)\).  If you allow adaptivity instead, the paper shows a lower bound of \(\widetilde{\Omega}(\sqrt k)\log n + \Omega(1/\varepsilon)\).</li></ol>



<p></p>



<p><strong>Amortized Edge Sampling</strong>, by Talya Eden, Saleet Mossel and Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2008.08032">arXiv</a>). Consider the following setup. You are given query access to adjacency list of a graph \(G\) with \(n\) vertices and \(m\) edges. You can make degree queries and neighbor queries. Suppose I ask you to sample a single edge from this graph from a distribution that is pointwise \(\varepsilon\) close to the uniform distribution. Eden and Rosenbaum already showed how you can achieve this with a budget of \(\widetilde{\Theta}(n/\sqrt m)\) queries. Starting from this jump off point, the authors ask whether you can circumvent this lower bound if you want to return multiple samples from a distribution which is again pointwise close to uniform. The paper answers this question in the affirmative and shows that if you know the number of samples, \(q\), in advance you can get away with an amortized bound of \(O*(\sqrt q n/\sqrt m)\) on the total number of queries needed.</p>



<p></p>



<p><strong>On the High Accuracy Limitation of Adaptive Property Estimation</strong>, by Yanjun Han (<a href="https://arxiv.org/abs/2008.11964">arXiv</a>). Take a discrete distribution \(\mathcal{P}\) with support size \(k\) and consider the task of estimating some symmetric property of \(\mathcal{P}\) to a small \(\pm \varepsilon\) additive error. Here, a symmetric property refers to a “nice” functional defined over the probability simplex, i.e., it refers to functions \(F \colon \Delta_k \to \mathbb{R}\) where \(F(p) = \sum_{i=1}^{k} f(p_i)\) where \(f \colon (0,1) \to \mathbb{R}\). A naive attack to these estimation tasks goes through the following ritual: you get your hands on the empirical distribution, you plug it in \(F\) and you hope for the best. Turns out, you are out of luck if the function \(f\) is non-smooth and in these cases you end up with a suboptimal estimator. Previous works have also looked at more sophisticated estimators (like the <em>local moment matching or LMM and profile maximum likelihood or PML</em> estimator). Turns out, using the LMM or PML estimator leads to optimal sample complexity for a handful of symmetric properties (as long as \(\varepsilon \geq n^{-1/3}\)). This paper considers the question of what can you say for supersmall values of \(\varepsilon\) where \(n^{-1/2} \leq \varepsilon \leq n^{-1/3}\). (The \(n^{-1/2}\) appears because there are estimators that use the knowledge of \(f\) and \(\varepsilon\) can be driven all the way down to \(n^{-1/2}\) for these estimators). The paper focuses on estimators which do not exploit any structure in \(f\). In particular, the paper specializes this question to PML and shows a fundamental limitation on PML which means that the PML approach fails to be sample optimal for the entire range of \(\varepsilon\) and is sample optimal only for \(\varepsilon &gt;&gt; n^{-1/3}\) — which also confirms a conjecture of Han and Shiragur (and refutes a conjecture of Acharya et al. who postulated this is sample optimal for the entire range of \(\varepsilon\)).</p>



<p></p>



<p><strong>\(k\)-Forrelation Optimally Separates Quantum and Classical Query<br />Complexity</strong>, by Nikhil Bansal and Makrand Sinha (<a href="https://arxiv.org/abs/2008.07003">arXiv</a>). Understanding the power of quantum query over classical queries is a well motivated problem with a rich history. One of the biggest questions in this area asks for the largest separation between classical and quantum query complexities. In a breakthrough, Aaronson and Ambainis [4] showed a fundamental simulation result which confirmed that you can simulate \(q\) quantum queries with \(O(N^{1 – 1/2q})\) classical queries in the randomized decision tree model of computation as long as \(q = O(1)\). In the same paper, the authors also showed that the standard <em>forrelation</em>* problem exhibits a \(1\) versus \(\widetilde{\Omega}(\sqrt n)\) separation. This means that for \(q = 1\), you essentially have optimal separation. But what about \(q &gt; 1\)? To this end, Aaronson and Ambainis conjectured that a certain problem which they called \(k\)-forrelation — which can be computed with \(q = k/2\) queries requires at least \(\widetilde{\Omega}(n^{1-1/k})\) classical queries. The current work precisely confirms this conjecture.</p>



<p>(*) The forrelation problem asks you to decide whether one Boolean function is highly correlated with the Fourier transform of a second function.</p>



<p><em><strong>(Edit:</strong> Added Later)</em>  Simon Apers points out a paper by Shrestov, Storozhenko and Wu that we missed. (Thanks Simon)! Here is a brief report on that paper.<br /></p>



<p><strong>An optimal separation of randomized and quantum query complexity</strong> (by Alexander Shrestov, Andrey Storozhenko and Pei Wu)(<a href="https://arxiv.org/abs/2008.10223">arXiv</a>) Similar to the paper by Bansal and Sinha [BS20] mentioned above, this paper also resolves the conjecture by Aaronson and Ambainis proving the same result. Like the paper also notes, the techniques in both of these works are completely different and incomparable. On the one hand [BS20] proves the separation for an explicit function as opposed to a function chosen uniformly at random from a certain set as considered in this work. On the other hand,  the separation result shown in [BS20] only applies when the query algorithm returns the correct answer with probability at least \(1/2 + 1/poly(\log n)\) — in contrast the results in this paper apply even when the query algorithm is required to have probability of correctness be a constant at least \(1/2\). In addition, this work also proves the \(\ell\)-Fourier weight conjecture of Tal which is of independent interest beyond quantum computing.</p>



<p></p>



<p>So, it looks like all in all we had a great month with two concurrent papers both resolving Aaronson Ambainis conjecture (yet again after two concurrent papers on testing Hamiltonicity)!</p>



<p><strong>References</strong>:</p>



<p>[1] Noga Alon, Eldar Fischer, Michael Krivelevich, and Mario Szegedy. Efficient testing of<br />large graphs. Combinatorica, 20(4):451–476, 2000.</p>



<p><br />[2] Oded Goldreich. On testing hamiltonicity in the bounded degree graph model. Electronic Colloquium on Computational Complexity (ECCC), (18), 2020</p>



<p>[3] Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication complexity. <em>CCC 2011</em></p>



<p>[4] Scott Aaronson and Andris Ambainis. Forrelation: A problem that optimally separates quantum from classical computing. SIAM J. Comput., 47(3):982–1038, 2018</p></div>







<p class="date">
by akumar <a href="https://ptreview.sublinear.info/?p=1387"><span class="datestr">at September 09, 2020 03:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03788">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03788">Tight List-Sizes for Oblivious AVCs under Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yihan.html">Yihan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaggi:Sidharth.html">Sidharth Jaggi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Budkuley:Amitalok_J=.html">Amitalok J. Budkuley</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03788">PDF</a><br /><b>Abstract: </b>We study list-decoding over adversarial channels governed by oblivious
adversaries (a.k.a. oblivious Arbitrarily Varying Channels (AVCs)). This type
of adversaries aims to maliciously corrupt the communication without knowing
the actual transmission from the sender. For any oblivious AVCs potentially
with constraints on the sender's transmitted sequence and the adversary's noise
sequence, we determine the exact value of the minimum list-size that can
support a reliable communication at positive rate. This generalizes a classical
result by Hughes (IEEE Transactions on Information Theory, 1997) and answers an
open question posed by Sarwate and Gastpar (IEEE Transactions on Information
Theory, 2012). A lower bound on the list-decoding capacity (whenever positive)
is presented. Under a certain combinatorial conjecture, we also prove a
matching upper bound. En route to a tight characterization of the list-decoding
capacity, we propose a method for subcode construction towards the resolution
of the combinatorial conjecture.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03788"><span class="datestr">at September 09, 2020 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03687">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03687">Finding Diverse Trees, Paths, and More</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurita:Kazuhiro.html">Kazuhiro Kurita</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03687">PDF</a><br /><b>Abstract: </b>Mathematical modeling is a standard approach to solve many real-world
problems and {\em diversity} of solutions is an important issue, emerging in
applying solutions obtained from mathematical models to real-world problems.
Many studies have been devoted to finding diverse solutions. Baste et al.
(Algorithms 2019, IJCAI 2020) recently initiated the study of computing diverse
solutions of combinatorial problems from the perspective of fixed-parameter
tractability. They considered problems of finding $r$ solutions that maximize
some diversity measures (the minimum or sum of the pairwise Hamming distances
among them) and gave some fixed-parameter tractable algorithms for the diverse
version of several well-known problems, such as {\sc Vertex Cover}, {\sc
Feedback Vertex Set}, {\sc $d$-Hitting Set}, and problems on bounded-treewidth
graphs. In this work, we investigate the (fixed-parameter) tractability of
problems of finding diverse spanning trees, paths, and several subgraphs. In
particular, we show that, given a graph $G$ and an integer $r$, the problem of
computing $r$ spanning trees of $G$ maximizing the sum of the pairwise Hamming
distances among them can be solved in polynomial time. To the best of the
authors' knowledge, this is the first polynomial-time solvable case for finding
diverse solutions of unbounded size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03687"><span class="datestr">at September 09, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03675">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03675">Space efficient merging of de Bruijn graphs and Wheeler graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Egidi:Lavinia.html">Lavinia Egidi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Louza:Felipe_A=.html">Felipe A. Louza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03675">PDF</a><br /><b>Abstract: </b>The merging of succinct data structures is a well established technique for
the space efficient construction of large succinct indexes. In the first part
of the paper we propose a new algorithm for merging succinct representations of
de Bruijn graphs. Our algorithm has the same asymptotic cost of the state of
the art algorithm for the same problem but it uses less than half of its
working space. A novel important feature of our algorithm, not found in any of
the existing tools, is that it can compute the Variable Order succinct
representation of the union graph within the same asymptotic time/space bounds.
In the second part of the paper we consider the more general problem of merging
succinct representations of Wheeler graphs, a recently introduced graph family
which includes as special cases de Bruijn graphs and many other known succinct
indexes based on the BWT or one of its variants. We show that Wheeler graphs
merging is in general a much more difficult problem, and we provide a space
efficient algorithm for the slightly simplified problem of determining whether
the union graph has an ordering that satisfies the Wheeler conditions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03675"><span class="datestr">at September 09, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03416">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03416">Probabilistic analysis of algorithms for cost constrained minimum weighted combinatorial objects</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Alan Frieze, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tkocz:Tomasz.html">Tomasz Tkocz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03416">PDF</a><br /><b>Abstract: </b>We consider cost constrained versions of the minimum spanning tree problem
and the assignment problem. We assume edge weights are independent copies of a
continuous random variable $Z$ that satisfies $F(x)=\Pr(Z\leq x)\approx
x^\alpha$ as $x\to0$, where $\alpha\geq 1$. Also, there are $r=O(1)$ budget
constraints with edge costs chosen from the same distribution. We use
Lagrangean duality to construct polynomial time algorithms that produce
asymptotically optimal solutions. For the spanning tree problem, we allow
$r&gt;1$, but for the assignment problem we can only analyse the case $r=1$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03416"><span class="datestr">at September 09, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03358">A Lightweight Algorithm to Uncover Deep Relationships in Data Tables</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Jin.html">Jin Cao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Yibo.html">Yibo Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Linjun.html">Linjun Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03358">PDF</a><br /><b>Abstract: </b>Many data we collect today are in tabular form, with rows as records and
columns as attributes associated with each record. Understanding the structural
relationship in tabular data can greatly facilitate the data science process.
Traditionally, much of this relational information is stored in table schema
and maintained by its creators, usually domain experts. In this paper, we
develop automated methods to uncover deep relationships in a single data table
without expert or domain knowledge. Our method can decompose a data table into
layers of smaller tables, revealing its deep structure. The key to our approach
is a computationally lightweight forward addition algorithm that we developed
to recursively extract the functional dependencies between table columns that
are scalable to tables with many columns. With our solution, data scientists
will be provided with automatically generated, data-driven insights when
exploring new data sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03358"><span class="datestr">at September 09, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2009.03352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2009.03352">A Fast Randomized Algorithm for Finding the Maximal Common Subsequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Jin.html">Jin Cao</a>, Dewei Zhong <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2009.03352">PDF</a><br /><b>Abstract: </b>Finding the common subsequences of $L$ multiple strings has many applications
in the area of bioinformatics, computational linguistics, and information
retrieval. A well-known result states that finding a Longest Common Subsequence
(LCS) for $L$ strings is NP-hard, e.g., the computational complexity is
exponential in $L$. In this paper, we develop a randomized algorithm, referred
to as {\em Random-MCS}, for finding a random instance of Maximal Common
Subsequence ($MCS$) of multiple strings. A common subsequence is {\em maximal}
if inserting any character into the subsequence no longer yields a common
subsequence. A special case of MCS is LCS where the length is the longest. We
show the complexity of our algorithm is linear in $L$, and therefore is
suitable for large $L$. Furthermore, we study the occurrence probability for a
single instance of MCS and demonstrate via both theoretical and experimental
studies that the longest subsequence from multiple runs of {\em Random-MCS}
often yields a solution to $LCS$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2009.03352"><span class="datestr">at September 09, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/134">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/134">TR20-134 |  Tight Bounds on Sensitivity and Block Sensitivity of Some Classes of Transitive Functions | 

	Anna Gal, 

	Siddhesh Chaubal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Nisan and Szegedy conjectured that block sensitivity is at most
polynomial in sensitivity for any Boolean function.
Until a recent breakthrough of Huang, the conjecture had been
wide open in the general case,
and was proved only for a few special classes
of Boolean functions.
Huang's result implies that block sensitivity is at most
the 4th power of sensitivity for any Boolean function.
It remains open if a tighter relationship between
sensitivity and block sensitivity holds for arbitrary Boolean functions;
the largest known gap between these measures is quadratic.

We prove tighter bounds showing that block sensitivity is at most
3rd power, and in some cases at most square of sensitivity for
subclasses of transitive functions,
defined by various properties of their DNF (or CNF) representation.
Our results improve and extend previous results regarding
transitive functions. We obtain these results by
proving tight (up to constant factors) lower bounds on the
smallest possible sensitivity of functions in these classes.

In another line of research, it has also been examined what is the
smallest possible block sensitivity of transitive functions.
Our results yield tight (up to constant factors) lower bounds
on the block sensitivity of the classes we consider.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/134"><span class="datestr">at September 08, 2020 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/133">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/133">TR20-133 |  Query complexity lower bounds for local list-decoding and hard-core predicates (even for small rate and huge lists) | 

	Noga Ron-Zewi, 

	Ronen Shaltiel, 

	Nithin Varma</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A binary code $\text{Enc}:\{0,1\}^k \rightarrow \{0,1\}^n$ is $(\frac{1}{2}-\varepsilon,L)$-list decodable if for every $w \in \{0,1\}^n$, there exists a set $\text{List}(w)$ of size at most $L$, containing all messages $m \in \{0,1\}^k$ such that the relative Hamming distance between $\text{Enc}(m)$ and $w$ is at most $\frac{1}{2}-\varepsilon$.
A $q$-query local list-decoder is a randomized procedure that when given oracle access to a string $w$, makes at most $q$ oracle calls, and for every message $m \in \text{List}(w)$, with high probability, there exists $j \in [L]$ such that for every $i \in [k]$, with high probability, $\text{Dec}^w(i,j)=m_i$.

We prove lower bounds on $q$, that apply even if $L$ is huge (say $L=2^{k^{0.9}}$) and the rate of $\text{Enc}$ is small (meaning that $n \ge 2^{k}$):

* For $\varepsilon = 1/k^{\nu}$ for some constant $\nu&gt;0$, we prove a lower bound of $q=\Omega(\frac{\log(1/\delta)}{\varepsilon^2})$, where $\delta$ is the error probability of the local list-decoder. This bound is tight as there is a matching upper bound by Goldreich and Levin (STOC 1989) of $q=O(\frac{\log(1/\delta)}{\varepsilon^2})$ for the Hadamard code (which has $n=2^k$). This bound extends an earlier work of Grinberg, Shaltiel and Viola (FOCS 2018) which only works if $n \le 2^{k^{\nu}}$ and the number of coins tossed by $\text{Dec}$ is small (and therefore does not apply to the Hadamard code, or other codes with low rate).

* For smaller $\varepsilon$, we prove a lower bound of roughly $q = \Omega(\frac{1}{\sqrt{\varepsilon}})$. To the best of our knowledge, this is the first lower bound on the number of queries of local list-decoders that gives $q \ge k$ for small $\varepsilon$.

Local list-decoders with small $\varepsilon$ form the key component in the celebrated theorem of Goldreich and Levin that extracts a hard-core predicate from a one-way function.
We show that black-box proofs cannot improve the Goldreich-Levin theorem and produce a hard-core predicate that is hard to predict with probability $\frac{1}{2}+\frac{1}{\ell^{\omega(1)}}$ when provided with a one-way function $f:\{0,1\}^{\ell} \rightarrow \{0,1\}^{\ell}$, such that circuits of size $\text{poly}(\ell)$ cannot invert $f$ with probability $\rho=1/2^{\sqrt{\ell}}$ (or even $\rho=1/2^{\Omega(\ell)}$). This limitation applies to any proof by black-box reduction (even if the reduction is allowed to use nonuniformity and has oracle access to $f$).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/133"><span class="datestr">at September 08, 2020 05:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">Eberhard’s theorem for bipartite polyhedra with one big face</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://en.wikipedia.org/wiki/Eberhard%27s_theorem">Eberhard’s theorem</a> is a topic in the combinatorial theory of convex polyhedra that once saw a lot of research, but has faded from more recent interest. It’s named after <a href="https://en.wikipedia.org/wiki/Victor_Eberhard">Victor Eberhard</a>, a German mathematician from the late 19th and early 20th century who worked in geometry despite becoming blind at age 12 or 13. I find this hard to imagine, as my own research in geometry is based very heavily on visual thinking, but he was far from the only successful blind mathematician; <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a>, <a href="https://en.wikipedia.org/wiki/Lev_Pontryagin">Lev Pontryagin</a>, and <a href="https://en.wikipedia.org/wiki/Bernard_Morin">Bernard Morin</a> also come to mind, and there are more.</p>

<p>Anyway, Eberhard’s theorem concerns the following question. Suppose I tell you that a polyhedron has a certain number of faces of certain types. For instance, after Archimedes’ work on polytopes was lost, all we knew about the <a href="https://en.wikipedia.org/wiki/Archimedean_solid">Archimedean solids</a> until their rediscovery in the Renaissance was a brief listing from <a href="https://en.wikipedia.org/wiki/Pappus_of_Alexandria">Pappus of Alexandria</a> giving this information: there is one with 8 triangles and 6 squares, etc. How can we tell that these counts of faces actually determine a polyhedron?</p>

<p>The given information for Eberhard’s theorem, then, is just a collection of counts of face types (triangles, quadrilaterals, etc.), without specifying the exact shapes of these faces. The goal is to use these faces to build a simple polyhedron, one for which three edges meet at every vertex (like a cube, unlike an octahedron). One necessary condition for this to be possible is that the polyhedron must obey Euler’s polyhedral formula \(v-e+f=2\). And it’s easy to calculate the numbers of vertices, edges, and faces appearing in this formula, from the face counts. Plugging these numbers into Euler’s formula leads to a linear equation that the face counts must obey. Crucially, this linear equation omits the count of hexagons: adding or removing hexagons will not change whether Euler’s formula holds. What Eberhard’s theorem states is that, as long as the face counts obey Euler’s formula in this way, there is always some number of hexagons that can be added or removed so that the remaining faces will form a polyhedron.</p>

<p>However, calculating the fewest number of hexagons needed, or even determining whether a given number of faces of all types (including hexagons) can be put together into a polyhedron, remains somewhat mysterious. So I thought I’d play with a case that would be both simple enough to solve and still interesting: the bipartite simple polyhedra (famous from <a href="https://en.wikipedia.org/wiki/Barnette%27s_conjecture">Barnette’s conjecture</a>), with one big face (a \(2n\)-gon for some \(n&gt;3\)), many small faces (\(n+3\) quadrilaterals, the number needed to make Euler’s formula hold), and a mysterious number of hexagons. What is the smallest number of hexagons that will allow the construction of a simple polyhedron with these face counts? The answer turns out to be \(\lfloor (3n-6)/2\rfloor\), achieved with polyhedra (or polyhedral graphs) in which the outer \(2n\)-gon surrounds a <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus tree</a> of 6-vertex cycles (and possibly one 4-vertex cycle), connected to each other by bridge edges:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/eberhard.svg" alt="Three hexagon-minimal bipartite simple polyhedra" /></p>

<p>The central cactus tree can be rearranged, as long as no two bridge edges have adjacent endpoints. For instance, in the graph with the dodecagon outer face, at the bottom of the figure, it’s possible for the middle six-vertex loop to have three connections to the outside polygon on one side and only one connection on the other side, or to have the four-vertex loop in the middle. But I can prove that all optimal solutions have the same overall central cactus tree structure.</p>

<p>I find it easier to think about the following equivalent rephrasing of the optimization problem: instead of finding a minimum number of hexagons that will allow us to build a polyhedron with those face counts, let’s build a polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons, and concentrate on minimizing the number of vertices in this polyhedron. The number of quadrilaterals will automatically come out right, and the number of hexagons will be minimized if the number of vertices is minimized.</p>

<p>Now suppose that we have any simple polyhedron with one \(2n\)-gon face and the rest quadrilaterals and hexagons. Remove the outer \(2n\)-gon from the graph, leaving a conncted subgraph, and look at the biconnected components of this subgraph. For any one component, its outer face in its induced planar embedding must be a simple cycle, with some vertices having degree two in the component (the endpoints of edges connecting the component to the rest of the graph) and some having degree three. If the component is a 4-cycle or 6-cycle, then all of its vertices have degree two. But if not, then at most four consecutive vertices of its outer cycle can have degree two, because they and the two vertices connected to them on both sides form part of the boundary of a face interior to the component, which can have at most six vertices. And the degree-three vertices of the outer cycle must come in consecutive pairs, which cannot be adjacent to the endpoints of bridge edges connecting to other biconnected components, because a degree-three vertex next to a bridge edge or next to two other degree-three vertices would combine with part of the outer \(2n\)-gon to form a face with seven or more vertices, and a degree-three vertex by itself would form a pentagon, neither of which is allowed.</p>

<p>So in a component that is not a 4-cycle or 6-cycle, the degree-two and degree-three vertices alternate around the outer cycle of the component in consecutive sequences of at most four and exactly two vertices. This implies that the number of degree-two vertices is even (because the whole cycle is even by bipartiteness) and that the number of degree-three vertices in the component (even just counting the ones on its boundary) is at least half of the number of degree-two vertices on its boundary. For the cactus trees that we’ve been using, on the other hand, the number of degree-three vertices in each cactus tree is strictly less than half of the number of degree-two vertices. So if we replace a whole non-cycle component by a cactus tree, we can get a graph with the same number of exposed degree-2 vertices, but fewer total vertices. After repeated replacement of biconnected components, at each step reducing the number of vertices, we would reach a state where the subgraph inside the \(2n\)-gon is a cactus tree. It might not meet the requirement that its bridge edges have nonadjacent endpoints, but it could always be rearranged to do so. And it might not be a cactus with at most one 4-cycle, but if not we could replace two 4-cycles by one 6-cycle and make it even smaller. So the only graphs that cannot be made smaller are the ones we started with, the cactus trees of 6-cycles and at most one 4-cycle, surrounded by an outer \(2n\)-gon.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104827671950147352">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html"><span class="datestr">at September 07, 2020 10:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1108">IGAFIT Algorithmic Colloquium</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We are excited to announce a new online seminar - IGAFIT Algorithmic Colloquium. This new event aims to integrate the European algorithmic community and keep it connected during the times of the pandemic. This online seminar will take place biweekly on Thursday at 14:00 CET, with the talks lasting for 45 minutes. Each talk will be followed by a networking and discussion session on topics related to the talk. We cordially invite all participants to this session. The meeting will be run on <a href="https://www.airmeet.com/e/55923fa0-eee9-11ea-8530-b3eab1e75816" target="_blank" rel="noreferrer noopener">Airmeet</a>. More details on the event can be found on <a href="http://igafit.mimuw.edu.pl/?page_id=483786" target="_blank" rel="noreferrer noopener">IGAFIT web page</a>.</p>



<p>The first talk will be held on the 1st of October 2020.</p>



<p>October 1, 2020<br />Vera Traub, University of Bonn<br />Title: An improved approximation algorithm for ATSP<br />Abstract: In a recent breakthrough, Svensson, Tarnawski, and Végh gave the first constant-factor approximation algorithm for the asymmetric traveling salesman problem (ATSP). In this work we revisit their algorithm. While following their overall framework, we improve on each part of it.</p>



<p>Svensson, Tarnawski, and Végh perform several steps of reducing ATSP to more and more structured instances. We avoid one of their reduction steps (to irreducible instances) and thus obtain a simpler and much better reduction to vertebrate pairs. Moreover, we show that a slight variant of their algorithm for vertebrate pairs has a much smaller approximation ratio.</p>



<p>Overall we improve the approximation ratio from 506 to 22 + ε for any ε &gt; 0. We also improve the upper bound on the integrality ratio of the standard LP relaxation from 319 to 22.</p>



<p>This is joint work with Jens Vygen.</p>



<p>Other upcoming talks include:</p>



<p>October 15, 2020<br />Thatchaphol Saranurak, Toyota Technological Institute at Chicago<br />Title: An almost-linear time deterministic algorithm for expander decomposition</p>



<p>October 29, 2020<br />Nathan Klein, University of Bonn<br />Title: A (Slightly) Improved Approximation Algorithm for Metric TSP</p>



<p>For more details please contact the Organization Committee:<br />Nikhil Bansal<br />Artur Czumaj<br />Andreas Feldmann<br />Adi Rosén<br />Eva Rotenberg<br />Piotr Sankowski<br />Christian Sohler <br /></p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1108"><span class="datestr">at September 07, 2020 08:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=2727">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/">The many faces of integration by parts – II : Randomized smoothing and score functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="556" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" class="wp-image-4633" height="254" />Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="357" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" class="wp-image-4598" height="292" />Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="491" alt="" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" class="wp-image-4545" height="322" />Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="446" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" class="wp-image-4605" height="225" />Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img width="432" alt="" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" class="wp-image-4624" height="288" />Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br />[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br />[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br />[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br />[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br />[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br />[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br />[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br />[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br />[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br />[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br />[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br />[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br />[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br />[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br />[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br />[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br />[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br />[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br />[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br />[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br />[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br />[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br />[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/"><span class="datestr">at September 07, 2020 07:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/132">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/132">TR20-132 |  Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture | 

	Arkadev Chattopadhyay, 

	Ankit Garg, 

	Suhail Sherif</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on $n$ input bits, each of which has approximate Fourier sparsity at most $O(n^3)$ and randomized parity decision tree complexity $\Theta(n)$. This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/132"><span class="datestr">at September 07, 2020 03:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-83517349672236531">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">Two Math Problems of interest (at least to me)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I will give two math problems that are of interest to me.</p><p>These are not new problems, however you will have more fun if you work on them yourself and leave comments on what you find. So if you want to work on it without hints, don't read the comments.</p><p><br /></p><p>I will post about the answers (not sure I will post THE answers) on Thursday.</p><p><br /></p><p>1) Let x(1)&gt;0. Let x(n+1) = (  1 + (1/x(n))  )^n. </p><p><br /></p><p>For how many values of x(1) does this sequence go to infinity?</p><p><br /></p><p>2) Find all (x,y) \in N \times N such that x^2+3y and y^2+3x are both squares. </p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html"><span class="datestr">at September 06, 2020 09:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/131">TR20-131 |  A Direct Product Theorem for One-Way Quantum Communication | 

	Srijita Kundu, 

	Rahul  Jain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a direct product theorem for the one-way entanglement-assisted quantum communication complexity of a general relation $f\subseteq\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$. For any $\varepsilon, \zeta &gt; 0$ and any $k\geq1$, we show that
\[ \mathrm{Q}^1_{1-(1-\varepsilon)^{\Omega(\zeta^6k/\log|\mathcal{Z}|)}}(f^k) = \Omega\left(k\left(\zeta^5\cdot\mathrm{Q}^1_{\varepsilon + 12\zeta}(f) - \log\log(1/\zeta)\right)\right),\]
where $\mathrm{Q}^1_{\varepsilon}(f)$ represents the one-way entanglement-assisted quantum communication complexity of $f$ with worst-case error $\varepsilon$ and $f^k$ denotes $k$ parallel instances of $f$.

As far as we are aware, this is the first direct product theorem for quantum communication -- direct sum theorems were previously known for one-way quantum protocols. Our techniques are inspired by the parallel repetition theorems for the entangled value of two-player non-local games, under product distributions due to Jain, Pereszl\'{e}nyi and Yao, and under anchored distributions due to Bavarian, Vidick and Yuen, as well as message-compression for quantum protocols due to Jain, Radhakrishnan and Sen. In particular, we show that a direct product theorem holds for the distributional one-way quantum communication complexity of $f$ under any distribution $q$ on $\mathcal{X}\times\mathcal{Y}$ that is anchored on one side, i.e., there exists a $y^*$ such that $q(y^*)$ is constant and $q(x|y^*) = q(x)$ for all $x$. This allows us to show a direct product theorem for general distributions, since for any relation $f$ and any distribution $p$ on its inputs, we can define a modified relation $\tilde{f}$ which has an anchored distribution $q$ close to $p$, such that a protocol that fails with probability at most $\varepsilon$ for $\tilde{f}$ under $q$ can be used to give  a protocol that fails with probability at most $\varepsilon + \zeta$ for $f$ under $p$.

Our techniques also work for entangled non-local games which have input distributions anchored on any one side, i.e., either there exists a $y^*$ as previously specified, or there exists an $x^*$ such that $q(x^*)$ is constant and $q(y|x^*) = q(y)$ for all $y$. In particular, we show that for any game $G = (q, \mathcal{X}\times\mathcal{Y}, \mathcal{A}\times\mathcal{B}, V)$ where $q$ is a distribution on $\mathcal{X}\times\mathcal{Y}$ anchored on any one side with anchoring probability $\zeta$, then
\[ \omega^*(G^k) = \left(1 - (1-\omega^*(G))^5\right)^{\Omega\left(\frac{\zeta^2 k}{\log(|\mathcal{A}|\cdot|\mathcal{B}|)}\right)}\]
where $\omega^*(G)$ represents the entangled value of the game $G$. This is a generalization of the result of Bavarian, Vidick and Yuen, who proved a parallel repetition theorem for games anchored on both sides, i.e., where both a special $x^*$ and a special $y^*$ exist, and potentially a simplification of their proof.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/131"><span class="datestr">at September 06, 2020 05:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/130">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/130">TR20-130 |  Optimal Inapproximability of Satisfiable k-LIN over Non-Abelian Groups | 

	Amey Bhangale, 

	Subhash Khot</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A seminal result of H\r{a}stad [J. ACM, 48(4):798–859, 2001]  shows that it is NP-hard to find an assignment that satisfies $\frac{1}{|G|}+\varepsilon$ fraction of the constraints of a given $k$-LIN instance over an abelian group, even if there is an assignment that satisfies $(1-\varepsilon)$ fraction of the constraints, for any constant $\varepsilon&gt;0$.  Engebretsen et al. [Theoretical Computer Science, 312(1):17–45, 2004] later showed that the same hardness result holds for $k$-LIN instances over any finite non-abelian group.

Unlike the abelian case, where we can efficiently find a solution if the instance is satisfiable, in the non-abelian case, it is NP-complete to decide if a given system of linear equations is satisfiable or not, as shown by Russell and Goldmann [Information and Computation, 178(1):253–262, 2002].  

Surprisingly, for certain non-abelian groups $G$, given a satisfiable $k$-LIN instance over $G$, one can in fact do better than just outputting a random assignment using a simple but clever algorithm. The approximation factor achieved by this algorithm varies with the underlying group. In this paper, we show that this algorithm is {\em optimal} by proving a  tight hardness of approximation of satisfiable $k$-LIN instance over {\em any} non-abelian $G$, assuming $P \neq NP$.

As a corollary, we also get $3$-query probabilistically checkable proofs with perfect completeness over large alphabets with improved soundness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/130"><span class="datestr">at September 06, 2020 04:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/">Closing An Open Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Crawl, then walk, then run.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/bg/" rel="attachment wp-att-17512"><img width="150" alt="" class="alignright  wp-image-17512" src="https://rjlipton.files.wordpress.com/2020/09/bg.png?w=150" /></a></p>
<p>Bogdan Grechuk is a lecturer in the math department at the University of Leicester. His office is in the Michael Atiyah Building. Pretty cool. He works in risk analysis, but is more broadly interested in math of all kinds. See his wonderful book <a href="https://link.springer.com/book/10.1007%2F978-3-030-19096-5">Theorems of the 21st Century</a>. Or go to his web <a href="https://theorems.home.blog/theorems-list/">site</a>.</p>
<p>
Today Ken and I want to talk about solving open problems.</p>
<p>
Grechuk’s site got us thinking about results that solve open problems. Most of us like to think our research solves an open problem. Personally I can say that I have tried to solve problems for the first time, but did not always succeed. </p>
<p>
Open problems usually mean something stronger. To be an open problem, a problem must be known to some community for some time. Advances in math and in complexity theory roughly fall into two categories: </p>
<ol>
<li>
Results that prove or disprove something that was explicitly stated before. The more who knew the problem the better. The longer the problem was known the better, too. <p></p>
</li><li>
Results that prove something that is new. Something that no one had explicitly asked before.
</li></ol>
<p>Both type of results are important. The latter kind may ultimately be more important. They raise new questions, often contain new methods, and move the field ahead in a new direction. See our<br />
<a href="https://rjlipton.wordpress.com/2011/02/01/godel’s-lost-letter-is-two-years-old/">discussion</a> of Freeman Dyson and frogs and birds.</p>
<p>
Think of how Kurt Gödel’s incompleteness theorem was unsuspected, or how Alan Turing’s proof of undecidability of the halting problem came in tandem with settling the criterion for computability, or years latter the definition of public-key crypto-systems. But we will focus on open problems in the sense (1). </p>
<p></p><h2> Claiming A Solution </h2><p></p>
<p></p><p>
Ken and I are amazed that when an open problem is claimed, especially for P versus NP, the claim swallows it whole. That is the claim is that the full problem is solved. We do not recall once when the claim was: </p>
<ul>
<li>
We are able to prove that SAT requires quadratic time, or <p></p>
</li><li>
We can show that SAT is in co-NP.
</li></ul>
<p>Either of these would be a “stop the press” result. </p>
<p>
For a more concrete example, suppose you claim to have a polynomial-time algorithm for finding a maximum clique in an undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Of course this implies P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP. Your algorithm may require a chain of difficult lemmas that obscure its workings. Can you perhaps analyze its effectiveness more easily on <em>random</em> graphs? Here are two relevant facts:</p>
<ul>
<li>
In 1976, David Matula <a href="https://s2.smu.edu/~matula/Tech-Report76.pdf">proved</a> that with high probability for a random <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph of edge probability <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" />, the size of the maximum clique is one of the two integers flanking <img src="https://s0.wp.com/latex.php?latex=%7B2%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\log_{1/p}(n)}" class="latex" title="{2\log_{1/p}(n)}" />. <p></p>
</li><li>
As observed in 1976 by Dick Karp in his <a href="https://www.semanticscholar.org/paper/The-probabilistic-analysis-of-some-combinatorial-Karp/9a9558d79b93fd884354f1ae27463be2836d2ec0">paper</a>, “The Probabilistic Analysis of Some Combinatorial Search Algorithms,” no polynomial time algorithm is known to achieve size <img src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cepsilon%29%5Clog_%7B1%2Fp%7D%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1+\epsilon)\log_{1/p}(n)}" class="latex" title="{(1+\epsilon)\log_{1/p}(n)}" />, for any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0}" class="latex" title="{\epsilon &gt; 0}" /> and sufficiently large <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.
</li></ul>
<p>
You only need to close a gap of a factor of <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, not to hit the maximum value exactly, and you do not need to succeed for all graphs. The behavior of random graphs should help your analysis. A more-recent mention of Karp’s open problem is in these 2005 <a href="https://www.math.cmu.edu/~af1p/MAA2005/L7.pdf">slides</a>.</p>
<p>
</p><p></p><h2> Some Examples </h2><p></p>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Collatz Conjecture </b>: Terence Tao made important <a href="https://terrytao.wordpress.com/2019/09/10/almost-all-collatz-orbits-attain-almost-bounded-values/">progress</a> on this notorious <a href="https://terrytao.files.wordpress.com/2020/02/collatz.pdf">problem</a>. He said: </p>
<blockquote><p><b> </b> <em> In mathematics, when we cannot solve a problem completely, we look for partial results. Even if they do not lead to a complete solution, they often reveal insights about the problem. </em>
</p></blockquote>
<p>Recall this is also called the <img src="https://s0.wp.com/latex.php?latex=%7B3n%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3n+1}" class="latex" title="{3n+1}" /> problem. It asks for the long-term behavior of the function: <img src="https://s0.wp.com/latex.php?latex=f%28n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f(n) " class="latex" title="f(n) " /> which is equal to <img src="https://s0.wp.com/latex.php?latex=n%2F2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n/2 " class="latex" title="n/2 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> even and <img src="https://s0.wp.com/latex.php?latex=3n%2B1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="3n+1 " class="latex" title="3n+1 " /> for <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="n " class="latex" title="n " /> odd. </p>
<p>The conjecture is that for every <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, iterating the function eventually hits <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, i.e., <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n) = 1}" class="latex" title="{f^i(n) = 1}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />.</p>
<p>
There are two ways the conjecture can fail:</p>
<ul>
<li>
There is a finite cycle besides the trivial cycle <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Crightarrow+4+%5Crightarrow+2+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" class="latex" title="{1 \rightarrow 4 \rightarrow 2 \rightarrow 1}" />. <p></p>
</li><li>
For some <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />, the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bf%5Ei%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f^i(n)}" class="latex" title="{f^i(n)}" /> goes off to infinity.
</li></ul>
<p>
What Tao proved is that “many” values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> achieve: <img src="https://s0.wp.com/latex.php?latex=f%5Ei%28n%29+%3C+%5Clog%5E%2A+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="f^i(n) &lt; \log^* n " class="latex" title="f^i(n) &lt; \log^* n " /> for some <img src="https://s0.wp.com/latex.php?latex=i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i " class="latex" title="i " />.</p>
<p>Grechuk's <a href="https://theorems.home.blog/2020/04/29/almost-all-orbits-of-the-collatz-map-attain-almost-bounded-value/">page</a> includes the definition of “many,” which turns out to be <em>weaker</em> than saying a <img src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-\epsilon)}" class="latex" title="{(1-\epsilon)}" /> proportion of values <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> “swing low” in this sense. Moreover, Tao proved this for any unbounded function <img src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(n)}" class="latex" title="{g(n)}" /> in place of the iterated logarithm, such as the inverse Ackermann function. Note this is a case where randomized analysis worked—in the hands of a master.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Twin Prime Conjecture </b>: Yitang Zhang made tremendous progress on this long standing <a href="https://en.wikipedia.org/wiki/Twin_prime">conjecture</a>. He proved that infinitely often is a prime <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p}" class="latex" title="{p}" /> so there is another prime <img src="https://s0.wp.com/latex.php?latex=%7Bq%3Ep%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q&gt;p}" class="latex" title="{q&gt;p}" /> bounded above by <img src="https://s0.wp.com/latex.php?latex=%7Bp+%2BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p +C}" class="latex" title="{p +C}" />. His <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was <img src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{70}" class="latex" title="{70}" /> million, but this was still a breakthrough. Previously no bounded <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> was known. We discussed this before <a href="https://rjlipton.wordpress.com/2013/05/21/twin-primes-are-useful/">here</a>. </p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <b> Sensitivity Conjecture </b>: Hao Huang is given as a <a href="http://mentalfloss.com/article/52698/how-does-exception-prove-rule">counterexample</a> to partial progress—it is the exception that proves the rule. He solved the full <a href="https://arxiv.org/pdf/1907.00847.pdf">conjecture</a>. He proved that every <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{n}-1}" class="latex" title="{2^{n}-1}" /> vertex induced subgraph of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-dimensional cube graph has maximum degree at least <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{n}}" class="latex" title="{\sqrt{n}}" />. The previous best was only order logarithm in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. But there does remain some slack: He proved a fourth-power bound, but can it be closed to cubic or even quadratic? We <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">discussed</a> this last year.</p>
<p>
By the <a href="https://www.mentalfloss.com/article/52698/how-does-exception-prove-rule">way</a>: </p>
<blockquote><p><b> </b> <em> The expression comes from the Latin legal principle exceptio probat regulam (the exception proves the rule), also rendered as exceptio firmat regulam (the exception establishes the rule) and exceptio confirmat regulam (the exception confirms the rule). The principle provides legal cover for inferences such as the following: if I see a sign reading “no swimming allowed after 10 pm,” I can assume swimming is allowed before that time. </em>
</p></blockquote>
<p>
</p><p></p><h2> Advice To Claimers </h2><p></p>
<p></p><p>
Our general advice to claimers: </p>
<blockquote><p><b> </b> <em> <i>Okay you are sure you have solved the big problem. Write up the weakest new result that you can.</i> </em>
</p></blockquote>
<p></p><p>
Use your methods, your insights, to minimize the work needed for someone to be 99.99% convinced that you have proved something <em>new</em>, rather than a lower confidence of your having proved something <em>huge</em>. For P<img src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{=}" class="latex" title="{=}" />NP show that you have a exponential algorithm that is better than known. Or for P<img src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{&lt;}" class="latex" title="{&lt;}" />NP give a non-linear lower bound. </p>
<p>
The rationale is: You are more likely to get someone to read your paper if you make a weaker claim. A paper titled: <i>A New SAT Algorithm that Runs in Sub-exponential Time</i> is more likely to get readers than a paper tiled <i>P=NP</i>. This shows that readership is non-monotone. </p>
<p>
This is consequence of two phenomena: One is believability. The weaker paper is more likely to be correct. One is human. The stronger paper, if correct, may not be easy to improve. A weaker paper could have results that the reader could improve and write a follow-on paper: </p>
<blockquote><p><b> </b> <em> In Carol Fletcher’s recent paper an <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%5E%7B1%2F3%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{2^{n^{1/3}}}" class="latex" title="{2^{n^{1/3}}}" /> algorithm is found for SAT. She required the full Riemann Hypothesis. We remove that requirement and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\dots}" class="latex" title="{\dots}" /> </em>
</p></blockquote>
<p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What about our advice: what would you do if you solved a major open problem? Note that the examples we highlighted all have slack for improvement short of the optimum statements.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/09/05/closing-an-open-problem/"><span class="datestr">at September 05, 2020 05:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/129">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/129">TR20-129 |  A Lower Bound on Determinantal Complexity | 

	Mrinal Kumar, 

	Ben Lee Volk</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The determinantal complexity of a polynomial $P \in \mathbb{F}[x_1,  \ldots, x_n]$ over a field $\mathbb{F}$ is the dimension of the smallest matrix $M$ whose entries are affine functions in $\mathbb{F}[x_1,  \ldots, x_n]$ such that $P = Det(M)$. We prove that the determinantal complexity of the polynomial $\sum_{i = 1}^n x_i^n$ is at least $1.5n - 3$. 

For every $n$-variate polynomial of degree $d$, the determinantal complexity is trivially at least $d$, and it is a long standing open problem to prove a lower bound which is super linear in $\max\{n,d\}$. Our result is the first lower bound for any explicit polynomial which is bigger by a constant factor than $\max\{n,d\}$, and improves upon the prior best bound of $n + 1$, proved by Alper, Bogart and Velasco [ABV17] for the same polynomial.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/129"><span class="datestr">at September 05, 2020 03:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5519">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/">Mathematics for Human Flourishing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/"><span class="datestr">at September 04, 2020 09:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20205">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/">Alef Corner: Math Collaboration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Another artistic view by Alef on mathematical collaboration.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/09/mathcoll.jpg?w=640&amp;h=640" class="alignnone size-full wp-image-20207" height="640" /></a></p>
<p> </p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/09/04/alef-corner-math-collaboration/"><span class="datestr">at September 04, 2020 07:17 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=52">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Friday, Sept 11 — Bin Yu from UC Berkeley</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/"><span class="datestr">at September 04, 2020 12:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsmath.wordpress.com/?p=2298">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/jrl.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/">ITCS 2021 Call For Papers (deadline extension)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>NOTE</strong>:  The deadline has been extended to Tuesday, September 8th to account for the Labor Day Holiday in the US.</p>



<p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>. The <strong>submission deadline</strong> is now <strong>September 8, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way! See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong>September 8, 2020 (05:59PM PDT)</li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul></div>







<p class="date">
by James <a href="https://tcsmath.wordpress.com/2020/09/03/itcs-2021-call-for-papers-deadline-extension/"><span class="datestr">at September 03, 2020 09:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4949">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4949">My Utility+ podcast with Matthew Putman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Another Update (Sep. 8):</span></strong> A reader wrote to let me know about a <a href="https://www.gofundme.com/f/help-denis-smirnov">fundraiser for Denys Smirnov</a>, a 2015 IMO gold medalist from Ukraine who needs an expensive bone marrow transplant to survive Hodgkin’s lymphoma.  I just donated and I hope you’ll consider it too!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep. 5):</span></strong> <a href="https://dunctank.podbean.com/e/scott-aaronson-infinite-universes/">Here’s another quantum computing podcast I did</a>, “Dunc Tank” with Duncan Gammie.  Enjoy!</p>



<p></p><hr />
<p><br />Thanks so much to <em>Shtetl-Optimized</em> readers, so far we’ve raised $1,371 for the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> and $225 for the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, which I intend to match for $3,192 total.  If you’d like to donate by tonight (Thursday night), there’s still $404 to go!</p><p></p>



<p>Meanwhile, a mere three days after declaring my <a href="https://www.scottaaronson.com/blog/?p=4942">“new motto,”</a> I’ve come up with a <em>new</em> new motto for this blog, hopefully a more cheerful one:</p>



<blockquote class="wp-block-quote"><p>When civilization seems on the brink of collapse, sometimes there’s nothing left to talk about but maximal separations between randomized and quantum query complexity.</p></blockquote>



<p>On that note, please enjoy my new <a href="https://nanotronics.co/thinkspace/24-scott-aaronson-before-and-after-the-machine/">one-hour podcast on Spotify</a> (if that link doesn’t work, <a href="https://overcast.fm/+UZFvS_CT8">try this one</a>) with <a href="https://en.wikipedia.org/wiki/Matthew_Putman_(scientist)">Matthew Putman</a> of Utility+.  Alas, my umming and ahhing were more frequent than I now aim for, but that’s partly compensated for by Matthew’s excellent decision to speed up the audio.  This was an unusually wide-ranging interview, covering everything from SlateStarCodex to quantum gravity to interdisciplinary conferences to the challenges of teaching quantum computing to 7-year-olds.  I hope you like it!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4949"><span class="datestr">at September 03, 2020 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/128">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/128">TR20-128 |  An Optimal Separation of Randomized and Quantum Query Complexity | 

	Alexander A. Sherstov, 

	Andrey Storozhenko, 

	Pei Wu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every decision tree, the absolute values of the Fourier coefficients of given order $\ell\geq1$ sum to at most $c^{\ell}\sqrt{{d\choose\ell}(1+\log n)^{\ell-1}},$ where $n$ is the number of variables, $d$ is the tree depth, and $c&gt;0$ is an absolute constant. This bound is essentially tight and settles a conjecture due to Tal (arxiv 2019; FOCS 2020). The bounds prior to our work degraded rapidly with $\ell,$ becoming trivial already at $\ell=\sqrt{d}.$

As an application, we obtain, for any positive integer $k,$ a partial Boolean function on $n$ bits that has bounded-error quantum query complexity at most $\lceil k/2\rceil$ and randomized query complexity $\tilde{\Omega}(n^{1-1/k}).$ This separation of bounded-error quantum versus randomized query complexity is best possible, by the results of Aaronson and Ambainis (STOC 2015). Prior to our work, the best known separation was polynomially weaker: $O(1)$ versus $n^{2/3-\epsilon}$ for any $\epsilon&gt;0$ (Tal, FOCS 2020).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/128"><span class="datestr">at September 03, 2020 01:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/">Post-doc at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by October 26, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a postdoc employed at IT University of Copenhagen (ITU).</p>
<p>The ideal candidate will have proven their research ability through an outstanding PhD thesis, and by publishing in leading international venues for algorithms theory.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181207&amp;DepartmentId=3439&amp;MediaId=5</a><br />
Email: pagh@itu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/03/post-doc-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-october-26-2020/"><span class="datestr">at September 03, 2020 08:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/">PhD student at Basic Algorithms Research Copenhagen at IT University of Copenhagen (apply by September 28, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The BARC Center for Basic Algorithms Research Copenhagen (barc.ku.dk) is seeking a PhD student, to be employed at IT University of Copenhagen.</p>
<p>The ideal candidate will have proven potential through e.g.: 1) an outstanding MSc thesis, 2) publishing in leading international venues for algorithms theory, or 3) successful participation in international competitions in informatics or mathematics.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=119&amp;ProjectId=181206&amp;DepartmentId=3439&amp;MediaId=1282</a><br />
Email: pagh@itu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/03/phd-student-at-basic-algorithms-research-copenhagen-at-it-university-of-copenhagen-apply-by-september-28-2020/"><span class="datestr">at September 03, 2020 08:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-4174113397967272312">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html">Broad Testing Thank You</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I have a loose association with the Broad Institute, an institute created so that "complementary expertise of the genomic scientists and the chemical biologists across MIT and Harvard be brought together in one place to drive the transformation of medicine with molecular knowledge."  (See <a href="https://www.broadinstitute.org/history">https://www.broadinstitute.org/history</a> )</p><p>They recently passed an amazing milestone, having performed over 1 million Covid tests.  They weren't set up to be a Covid testing lab, but the converted their institute space to respond to the Covid crisis.  (See <a href="https://covid19-testing.broadinstitute.org/">https://covid19-testing.broadinstitute.org/</a> )  </p><p>In short, they stepped up.  They certainly didn't have to, but they did.  Maybe it will help them do good science, now and in the future.  But my understanding is that they saw a clear societal need (lots of <a href="https://www.amazon.com/gp/product/B07DFZ12KV/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=michaelmitzen-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=B07DFZ12KV&amp;linkId=5a5c250c33b2ba71012ff577c69e1a77">outbreak specialists there</a> ) and they realized they had the expertise and equipment to do good that went beyond science.  I just wanted to give a shout out to the Broad for their good works, scientific and societal.  </p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2020/09/broad-testing-thank-you.html"><span class="datestr">at September 03, 2020 12:32 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20194">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/">Alef’s Corner: Math Collaboration 2</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://gilkalai.files.wordpress.com/2020/09/alef.jpg"><img src="https://gilkalai.files.wordpress.com/2020/09/alef-math-collaboration.jpg?w=640" alt="alef-math-collaboration" class="alignnone size-full wp-image-20202" /></a></p>
<p>Other <a href="https://gilkalai.wordpress.com/tag/alefs-corner/">Alef’s corner</a> posts</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/09/02/alef-corner-math-collaboration-2/"><span class="datestr">at September 02, 2020 01:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html">Isosceles polyhedra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest arXiv preprint is “On polyhedral realization with isosceles triangles”, <a href="http://arxiv.org/abs/2009.00116">arXiv:2009.00116</a>. As the title suggests, it studies polyhedra whose faces are all isosceles triangles. Despite several new results in it, there’s a lot I still don’t know. The paper finds a sort-of-new<sup id="fnref:1"><a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:1" class="footnote">1</a></sup> infinite family of polyhedra with congruent isosceles faces, shown below, but I don’t know if there are any more such families.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/twisted.svg" alt="Twisted augmented bipyramid with isosceles-triangle faces" /></p>

<p>One of the other previously known families, shown below, has two integer parameters (the numbers of sides on the two half-bipyramids it combines), but I don’t know whether the same double parameterization is possible for the new family.</p>

<p style="text-align: center;"><img width="80%" alt="Combination of two half-bipyramids with isosceles-triangle faces" src="https://11011110.github.io/blog/assets/2020/biarc.svg" /></p>

<p>In 2001, Branko Grünbaum published an example of a polyhedron that could not be realized with congruent faces, even non-convexly,<sup id="fnref:2"><a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fn:2" class="footnote">2</a></sup> but it can be realized as a convex polyhedron with all faces isosceles (or equilateral), as shown below. I don’t know whether there are polyhedra that cannot be realized with all faces isosceles, if one allows the realization to be non-convex (but non-self-crossing and combinatorially equivalent to a convex polyhedron) and the faces to be non-congruent.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/grunbaum.svg" alt="Grünbaum's example of a polyhedron that cannot be realized with congruent faces, realized convexly with isosceles and equilateral triangle faces" /></p>

<p>My new preprint proves that there exist polyhedra (iterated Kleetopes) that cannot be realized as convex polyhedra with isosceles faces. But the construction is a little non-explicit and I don’t know how complicated these polyhedra need to be. For instance, I don’t know whether there is a convex isosceles-face realization of the double Kleetope of the octahedron, shown below.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/kkoct.svg" alt="Double Kleetope of an octahedron" /></p>

<p>Grünbaum’s example can be realized convexly with only two edge lengths, and my non-isosceles-faced polyhedra require at least three edge lengths in any convex realization. I don’t know whether the number of required edge lengths can be unbounded, or whether non-convex realizations ever require three lengths (although certain stacked polyhedra require at least two).</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The family of polyhedra from the first image is only “sort-of-new” because the same combinatorial structure was previously described as a triangulation of the sphere by congruent spherical isosceles triangles: Dawson, Robert J. MacG. (2005), “<a href="https://archive.bridgesmathart.org/2005/bridges2005-489.html">Some new tilings of the sphere with congruent triangles</a>”, Renaissance Banff. In exchange for re-purposing Dawson’s triangulation, my paper describes another infinite family of spherical triangulations by congruent spherical isosceles triangles, not given by Dawson, based on applying a similar \(2\pi/3\) twist to an infinite family of non-convex bipyramids with congruent isosceles faces like the one below. Again, I don’t know whether there are other such families of spherical triangulations.</p>

      <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/nwb.svg" alt="Non-convex polyhedron with congruent isosceles-triangle faces" /> <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Grünbaum, Branko (2001), “<a href="https://sites.math.washington.edu/~grunbaum/Nonequifacettablesphere.pdf">A convex polyhedron which is not equifacettable</a>”, <em>Geombinatorics</em> 10: 165–171. I don’t know how to access old papers on this journal in general, but fortunately Grünbaum made his one available on his web site. <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html#fnref:2" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/104794000861684463">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/09/01/isosceles-polyhedra.html"><span class="datestr">at September 01, 2020 11:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4407">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/">Time Flies When the World is Falling Apart</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>It has been exactly one year since I moved to Italy.</p>
<p>Coming here a year ago, I expected that I would face some challenges and that there would be major life changes. Obviously, I did not know the half of it.</p>
<p><span id="more-4407"></span></p>
<p>In March and April, being in the hottest hot spot of a global pandemic was not ideal. Something rather unexpected, however, has happened. Italy is not known for very effective governance and Italians are not known as a rule-following people. Yet the government implemented a strict lockdown, and held it long enough that the “reopening” took place with a safely low circulation of the virus. People have been mostly following the rules after the reopening, and for most of the summer Italy has been doing quite well. (There has been a recent surge of new cases  in the last few weeks, so it may be too soon to declare victory, but the numbers of intensive care hospitalizations and of deaths are still low).</p>
<p>Meanwhile, there have been “second waves” in several other European countries and the situation in the United States, as the President memorably said, is what it is.</p>
<p>This picture by Noah Berger for AP has been circulating as the perfect description of what is going on in Northern California this summer</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/img_0077.jpeg?w=584" alt="IMG_0077" class="alignnone size-full wp-image-4409" /></p>
<p>This New York Times headline also does a good job of packing at least five things that are wrong in California right now (the spread of covid-19, the overcrowding of prisons because of excessive sentencing, the budget cuts affecting firefighting efforts, the exploitation of prisoners for underpaid work, and the spread of wildfires) in a single sentence:</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/california-inmates.png?w=584" alt="california-inmates" class="alignnone size-full wp-image-4410" /></p>
<p>At the cost of resorting to banalities and cliches, Italians often do badly when confronted with good opportunities and lucky breaks, that are often wasted, but we do unexpectedly well in times of crisis. This can even be seen in the national football team, that is known for epic come-from-behind victories and for missing penalty kicks. The American spirit is to be ambitious and optimistic, and to pursue high-risk high-reward opportunities when they present themselves. The flip side is that adversity is often met in America with either despair or anger, typically counterproductively. All things considered, I am happy that I am getting to spend 2020 in Italy. Earlier this summer, the president and the rector of Bocconi announced the new long-term strategic plan for the university, which involves creating a new computer science department. If/when such plans come to fruition, there will be several faculty positions in computer science, at internationally competitive salaries, with advantageous taxation for people moving to Italy from other countries, and with English as the language of instructions, so maybe you too will consider such a move.</p>
<p>Bocconi is preparing to restart in-person teaching in a couple of weeks. The plan is to record and post lectures, to allow students to attend lectures remotely if they prefer, and to use classrooms at most at half capacity. If a class has to be scheduled in a classroom whose capacity is less than twice the class enrollment, students will be split in two groups. Each group will (be allowed to) physically attend in alternating weeks, and will (be required to) attend online at other times.</p>
<p>The space between campus buildings has been marked with well-spaced walking paths, and there are even pedestrian roundabouts.</p>
<p><img src="https://lucatrevisan.files.wordpress.com/2020/09/img_0122.jpeg?w=584" alt="IMG_0122" class="alignnone size-full wp-image-4412" /></p>
<p>Are we heading for a second-wave disaster? Will students really follow the rules? On Sunday we were in Rome and we took a bus to the city center. Every other seat in the bus was marked with a “do not sit” sign to create distancing between sitting people. All marked seats were empty even as the bus was filling up and several people were standing. Then a group of German tourists got in, and they sat in all the forbidden seats. This is the upside-down world of 2020, all bets are off.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2020/09/01/time-flies-when-the-world-is-falling-apart/"><span class="datestr">at September 01, 2020 05:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4647242799583898034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html">A well known theorem that has not been written down- so I wrote it down- CLIQ is #P-complete</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(The two proofs that CLIQ is #P-complete that I discuss in this blog are written up by Lance and myself and are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/sharpclique.pdf">here</a>. I think both are well known but I have not been able to find a writeup,so Lance and I did one.)</p><p><br /></p><p> I have been looking at #P (see my last blog on it it, <a href="https://blog.computationalcomplexity.org/2020/08/sharp-p-and-issue-of-natural-problems.html">here</a>, for a refresher on this topic) since I will be teaching this topic in Spring 2021.  Recall</p><p>#SAT(\phi) = the number of satisfying assignments for phi</p><p>#CLIQ((G,k))= the number of cliques of size \ge k of G</p><p>#SAT is #P-complete by a cursory look at the proof of the Cook-Levin theorem.</p><p>A function is #P-complete if everything else in #P is Turing-Poly red to it. To show for some set A </p><p>in NP, #A is #P-complete one usually uses pars reductions. </p><p>I wanted a proof that #CLIQ is #P-complete, so I wanted a parsimonious reduction from SAT to CLIQ (Thats a reduction f: SAT--&gt; CLIQ such that the number of satisfying assignments of phi equals the number of cliques of size \ge k of G.)</p><p>I was sure there is such a reduction and that it was well known and would be on the web someplace. So I tried to find it.</p><p>ONE:</p><p>I tracked some references to a paper by Janos Simon (<a href="https://link.springer.com/content/pdf/10.1007%2F3-540-08342-1_37.pdf">see here</a>)  where he claims that the reduction of SAT to CLIQ  by Karp <a href="https://blog.computationalcomplexity.org/feeds/posts/{http://cgi.di.uoa.gr/~sgk/teaching/grad/handouts/karp.pdf">(see here)</a> was pars.  I had already considered that and decided that Karps reduction was NOT pars.  I looked at both Simon's paper and Karp's paper to make sure I wasn't missing something (e.g., I misunderstood what Simon Says or what Karp ... darn, I can't think of anything as good as `Simon Says'). It seemed to me that Simon was incorrect. If I am wrong let me know.</p><p>TWO</p><p>Someone told me that it was in Valiant's  paper (see <a href="https://epubs.siam.org/doi/10.1137/0208032">here</a>). Nope. Valiant's paper shows that counting the number of maximal cliques is #P-complete. Same Deal Here- if I am wrong let me know. One can modify Valiant's argument to get #CLIQ #P-complete, and I do so in the write up. The proof is a string of reductions and starts with PERM is #P-complete. This does prove that# CLIQ is  #P-complete, but is rather complicated. Also, the reduction is not pars.</p><p>THREE <br />Lance told me an easy pars reduction that is similar to Karp's non-pars reduction, but it really is NOT Karp's reduction. Its in my write up. I think it is well known since I recall hearing it about 10 years ago. From Lance. Hmmm, maybe its just well known to Lance.</p><p><br /></p><p>But here is my question: I am surprised I didn't find it on the web. If someone can point to a place on the web where it is, please let me know. In any case, its on the web NOW (my write up) so hopefully in the future someone else looking for it can find it.</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/09/a-well-known-theorem-that-has-not-been.html"><span class="datestr">at September 01, 2020 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/">QuICS Hartree Postdoctoral Fellowships at Joint Center for Quantum Information and Computer Science (apply by December 1, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Joint Center for Quantum Information and Computer Science (QuICS, <a href="http://quics.umd.edu">http://quics.umd.edu</a>) is seeking exceptional candidates for the QuICS Hartree Postdoctoral Fellowships in Quantum Information and Computer Science.<br />
Applications should be submitted through AcademicJobsOnline at <a href="https://academicjobsonline.org/ajo/jobs/16778.">https://academicjobsonline.org/ajo/jobs/16778.</a></p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/16778">https://academicjobsonline.org/ajo/jobs/16778</a><br />
Email: quics-coordinator@umiacs.umd.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/01/quics-hartree-postdoctoral-fellowships-at-joint-center-for-quantum-information-and-computer-science-apply-by-december-1-2020/"><span class="datestr">at September 01, 2020 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-7958692547156288391">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html">No Regret Algorithms from the Min Max Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The existence of no-regret learning algorithms can be used to prove <a href="https://en.wikipedia.org/wiki/Minimax_theorem">Von-Neumann's min-max theorem</a>. This argument is originally due to <a href="https://www.cs.princeton.edu/~schapire/papers/FreundScYY.pdf">Freund and Schapire</a>, and I <a href="https://www.cis.upenn.edu/~aaroth/courses/slides/agt20/lect08.pdf">teach it to my undergraduates </a>in my algorithmic game theory class. The min-max theorem also can be used to prove the existence of no-regret learning algorithms. Here is a constructive version of the argument (Constructive in that in the resulting algorithm, you only need to solve polynomially sized zero-sum games, so you can do it via linear programming).<div><br /></div><div>Recall the setting. Play proceeds in rounds $t \in \{1,\ldots,T\}$. At each day $t$, the learner chooses one of $k$ actions $i_t \in \{1,\ldots,k\}$, and the adversary chooses a loss vector $\ell^t \in [0,1]^k$. The learner incurs loss $\ell^t_{i_t}$, corresponding to the action he chose. At the end of the interaction, the regret of the learner is defined to be the difference between the cumulative loss he incurred and the cumulative loss of the best fixed action (played consistently)  in hindsight:</div><div>$$\textrm{Regret}_T = \max_j \left(\sum_{t=1}^T \ell^t_{i_t} - \ell^t_j\right)$$ </div><div>A classical and remarkable result is that there exist algorithms that can guarantee that regret grows only sublinearly with time: $\textrm{Regret}_T = O(\sqrt{T})$. Lets prove this. </div><div><br /></div><div><br /></div><div>Define the non-negative portion of our cumulative regret with respect to action $j$ up until day $d$ as:</div><div>$$V_d^j = \left(\sum_{t=1}^d\left(\ell^t_{i_t} - \ell^t_j\right)\right)^+$$</div><div>and our additional regret at day $d+1$ with respect to action $j$ as:</div><div>$$r_{j}^{d+1} = \ell_{i_{d+1}}^{d+1} - \ell^{d+1}_j$$</div><div>Observe that if $V_{d}^j \geq 1$  then $V_{d+1}^j = V_d^j + r_j^{d+1}$. </div><div><br /></div><div><br /></div><div>Define a surrogate loss function as our squared cumulative regrets, summed over all actions: </div><div>$$L_d = \sum_{j=1}^k (V_d^j)^2$$</div><div>Observe that we can write the expected gain in our loss on day $d+1$, conditioned on the history thus far:</div><div>$$\mathbb{E}[L_{d+1} - L_d] \leq \sum_{j : V_d^j \geq 1} \mathbb{E}[(V_d^j+r_j^{d+1})^2 - (V_d^j)^2) ] + 3k$$</div><div>$$= \sum_{j : V_d^j \geq 1} \left(2V_d^j \mathbb{E}[r_{j}^{d+1}] + \mathbb{E}[(r_{j}^{d+1})^2]\right) + 3k $$</div><div>$$\leq \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) + 4k$$</div><div>where the expectations are taken over the randomness of both the learner and the adversary in round $d+1$. </div><div><br /></div><div>Now consider a zero-sum game played between the learner and the adversary in which the learner is the minimization player, the adversary is the maximization player, and the utility function is $$u(i_{d+1}, \ell^{d+1}) = \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right)$$ The min-max theorem says that the learner can guarantee the same payoff for herself in the following two scenarios:</div><div><br /></div><div><ol style="text-align: left;"><li>The learner first has to commit to playing a distribution $p_{d+1}$ over actions $i$, and then the adversary gets to best respond by picking the worst possible loss vectors, or</li><li>The adversary has to first commit to a distribution over loss vectors $\ell$ and then the learner gets the benefit of picking the best action $i_{d+1}$ to respond with. </li></ol>Scenario 1) is the scenario our learner finds herself in, when playing against an adaptive adversary. But 2) is much easier to analyze. If the adversary first commits to a distribution over loss vectors $\ell^{d+1}$, the learner can always choose action $i_{d+1} = \arg\min_j \mathbb{E}[\ell^{d+1}_j]$, which guarantees that $\mathbb{E}[r_{j}^{d+1}] \leq 0$, which in turn guarantees that the value of the game $ \sum_{j=1}^k \left(2V_d^j \mathbb{E}[r_{j}^{d+1}]\right) \leq 0$.  Hence, the min-max theorem tells us that the learner always has a distribution over actions $p_{d+1}$ that guarantees that $\mathbb{E}[L_{d+1} - L_d] \leq 4k$, <i>even in the worst case over loss functions</i>. If the learner always plays according to this distribution, then by a telescoping sum, we have that:</div><div>$$\mathbb{E}[L_T] \leq 4kT$$.</div><div>We therefore have by Jensen's inequality that:</div><div>$$\mathbb{E}[\max_j (V^j_T)] \leq \sqrt{\mathbb{E}[\max_j (V^j_T)^2]}\leq \sqrt{\mathbb{E}[\sum_{j=1}^k (V_j^T)^2]} \leq 2\sqrt{kT}$$.</div></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2020/07/the-existence-of-no-regret-learning.html"><span class="datestr">at September 01, 2020 02:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/">postdoc at TU Hamburg (apply by September 30, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Algorithms and Complexity at TUHH: Hamburg University of Technology invites applications for a 5-year postdoc position (attached to DASHH: Data Science in Hamburg). The focus is on algorithms, applied to combinatorial optimization, operations research, and discrete mathematics in the natural sciences, particularly on big data obtained at Germany’s largest particle accelerator.</p>
<p>Website: <a href="https://www.tuhh.de/algo/jobs.html">https://www.tuhh.de/algo/jobs.html</a><br />
Email: matthias.mnich@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/09/01/postdoc-at-tu-hamburg-apply-by-september-30-2020/"><span class="datestr">at September 01, 2020 08:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/icml2020/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/icml2020/">Conference Digest - ICML 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a href="https://icml.cc/virtual/2020">ICML 2020</a> is one of the premiere venues in machine learning, and generally features a lot of great work in differentially private machine learning.
This year is no exception: the relevant papers are listed below to the best of our ability, including links to the full versions of papers, as well as the conference pages (which contain slides and 15 minute videos for each paper).
As always, please inform us if we overlooked any papers on differential privacy.</p>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1909.12732">Alleviating Privacy Attacks via Causal Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6346">page</a>)<br />
<a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, <a href="http://www.amitsharma.in/">Amit Sharma</a>, <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1805.10341">An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm</a> (<a href="https://icml.cc/virtual/2020/poster/6240">page</a>)<br />
<a href="https://github.com/dpeng817">Chris DeCarolis</a>, <a href="https://twitter.com/exsidius">Mukul Ram</a>, <a href="https://www.cs.umd.edu/people/sesmaeil">Seyed Esmaeili</a>, <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a>, <a href="http://furong-huang.com/">Furong Huang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.09697">Bayesian Differential Privacy for Machine Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6547">page</a>)<br />
<a href="https://scholar.google.com/citations?user=BCWx7iQAAAAJ">Aleksei Triastcyn</a>, <a href="https://people.epfl.ch/boi.faltings">Boi Faltings</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.03030">Certified Data Removal from Machine Learning Models</a> (<a href="https://icml.cc/virtual/2020/poster/5895">page</a>)<br />
<a href="https://sites.google.com/view/chuanguo">Chuan Guo</a>, <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a>, <a href="https://awnihannun.com/">Awni Hannun</a>, <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.00038">Context Aware Local Differential Privacy</a> (<a href="https://icml.cc/virtual/2020/poster/5775">page</a>)<br />
<a href="https://people.ece.cornell.edu/acharya/">Jayadev Acharya</a>, <a href="https://research.google/people/105175/">Kallista Bonawitz</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://research.google/people/106777/">Daniel Ramage</a>, <a href="http://www.zitengsun.com/">Ziteng Sun</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.12813">Data-Dependent Differentially Private Parameter Learning for Directed Graphical Models</a> (<a href="https://icml.cc/virtual/2020/poster/6262">page</a>)<br />
<a href="https://scholar.google.com/citations?user=lWWAZ4YAAAAJ">Amrita Roy Chowdhury</a>, <a href="http://pages.cs.wisc.edu/~thodrek/">Theodoros Rekatsinas</a>, <a href="http://pages.cs.wisc.edu/~jha/">Somesh Jha</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09745">Differentially Private Set Union</a> (<a href="https://icml.cc/virtual/2020/poster/6541">page</a>)<br />
<a href="https://www.microsoft.com/en-us/research/people/sigopi/">Sivakanth Gopi</a>, <a href="https://www.linkedin.com/in/pankajgulhane/">Pankaj Gulhane</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://heyyjudes.github.io/">Judy Hanwen Shen</a>, <a href="https://www.microsoft.com/en-us/research/people/milads/">Milad Shokouhi</a>, <a href="http://www.yekhanin.org/">Sergey Yekhanin</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.11651">Fair Learning with Private Demographic Data</a> (<a href="https://icml.cc/virtual/2020/poster/6499">page</a>)<br />
<a href="https://husseinmozannar.github.io/">Hussein Mozannar</a>, <a href="https://sites.google.com/site/mesrob/home/">Mesrob Ohannessian</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15744">Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraint</a> (<a href="https://icml.cc/virtual/2020/poster/6365">page</a>)<br />
<a href="https://dblp.org/pid/166/1694.html">Akbar Rafiey</a>, <a href="http://research.nii.ac.jp/~yyoshida/">Yuichi Yoshida</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.00706">(Locally) Differentially Private Combinatorial Semi-Bandits</a> (<a href="https://icml.cc/virtual/2020/poster/6315">page</a>)<br />
<a href="https://scholar.google.com/citations?user=sioumZAAAAAJ">Xiaoyu Chen</a>, <a href="https://scholar.google.com/citations?user=Bw-WdyUAAAAJ">Kai Zheng</a>, <a href="https://twitter.com/zixinjackzhou">Zixin Zhou</a>, <a href="https://scholar.google.com/citations?user=m8m9nD0AAAAJ">Yunchang Yang</a>, <a href="https://www.microsoft.com/en-us/research/people/weic/">Wei Chan</a>, <a href="http://www.liweiwang-pku.com/">Liwei Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05453">New Oracle-Efficient Algorithms for Private Synthetic Data Release</a> (<a href="https://icml.cc/virtual/2020/poster/5814">page</a>)<br />
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://scholar.google.com/citations?user=dDVIyEQAAAAJ">Grace Tian</a>, <a href="https://cs-people.bu.edu/mbun/">Mark Bun</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1190-Paper.pdf">On Differentially Private Stochastic Convex Optimization with Heavy-tailed Data</a> (<a href="https://icml.cc/virtual/2020/poster/5948">page</a>)<br />
<a href="http://www.acsu.buffalo.edu/~dwang45/">Di Wang</a>, <a href="https://scholar.google.com/citations?user=e3ZhEDEAAAAJ">Hanshen Xiao</a>, <a href="https://people.csail.mit.edu/devadas/">Srinivas Devadas</a>, <a href="https://cse.buffalo.edu/~jinhui/">Jinhui Xu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.13830">Optimal Differential Privacy Composition for Exponential Mechanisms</a> (<a href="https://icml.cc/virtual/2020/poster/6687">page</a>)<br />
<a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://dblp.org/pid/155/9794.html">David Durfee</a>, <a href="https://scholar.google.com/citations?user=jr7gGB4AAAAJ">Ryan Rogers</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.01783">Oracle Efficient Private Non-Convex Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/5815">page</a>)<br />
<a href="https://sethneel.com/">Seth Neel</a>, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, <a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2341-Paper.pdf">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</a> (<a href="https://icml.cc/virtual/2020/poster/6134">page</a>)<br />
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, <a href="https://www.itu.dk/people/pagh/">Rasmus Pagh</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6298-Paper.pdf">Private Outsourced Bayesian Optimization</a> (<a href="https://icml.cc/virtual/2020/poster/6783">page</a>)<br />
<a href="https://scholar.google.com/citations?user=7_2XTQ8AAAAJ">Dmitrii Kharkovskii</a>, <a href="https://daizhongxiang.github.io/">Zhongxiang Dai</a>, <a href="https://www.comp.nus.edu.sg/~lowkh/research.html">Bryan Kian Hsiang Low</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.10941">Private Query Release Assisted by Public Data</a> (<a href="https://icml.cc/virtual/2020/poster/6329">page</a>)<br />
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="https://www.ccs.neu.edu/home/albertcheu/">Albert Cheu</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2453-Paper.pdf">Private Reinforcement Learning with PAC and Regret Guarantees</a> (<a href="https://icml.cc/virtual/2020/poster/6152">page</a>)<br />
<a href="https://sites.google.com/umn.edu/giuseppe-vietri/home">Giuseppe Vietri</a>, <a href="https://borjaballe.github.io/">Borja Balle</a>, <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.01327">Privately Detecting Changes in Unknown Distributions</a> (<a href="https://icml.cc/virtual/2020/poster/5854">page</a>)<br />
<a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a>, <a href="https://sites.google.com/view/skrehbiel/home">Sara Krehbiel</a>, <a href="https://scholar.google.com/citations?user=ayasb_wAAAAJ">Yuliia Lut</a>, <a href="https://wanrongz.github.io/">Wanrong Zhang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09463">Privately Learning Markov Random Fields</a> (<a href="https://icml.cc/virtual/2020/poster/5776">page</a>)<br />
<a href="https://huanyuzhang.github.io/">Huanyu Zhang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://www.microsoft.com/en-us/research/people/jakul/">Janardhan Kulkarni</a>, <a href="https://zstevenwu.com/">Zhiwei Steven Wu</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1903.09822">Scalable Differential Privacy with Certified Robustness in Adversarial Learning</a> (<a href="https://icml.cc/virtual/2020/poster/6401">page</a>)<br />
<a href="https://sites.google.com/site/ihaiphan/">NhatHai Phan</a>, <a href="https://www.cise.ufl.edu/~mythai/">My T. Thai</a>, <a href="https://scholar.google.com/citations?user=OgXtPDIAAAAJ">Han Hu</a>, <a href="http://www.cs.kent.edu/~jin/">Ruoming Jin</a>, <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>, <a href="https://ix.cs.uoregon.edu/~dou/">Dejing Dou</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.04493">Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion</a> (<a href="https://icml.cc/virtual/2020/poster/6734">page</a>)<br />
<a href="https://enosair.github.io/">Qinqing Zheng</a>, <a href="https://www.math.upenn.edu/~jinshuo/">Jinshuo Dong</a>, <a href="https://www.med.upenn.edu/apps/faculty/index.php/g275/p8939931">Qi Long</a>, <a href="http://www-stat.wharton.upenn.edu/~suw/">Weijie J. Su</a></p>
  </li>
</ul></div>







<p class="date">
by Gautam Kamath <a href="https://differentialprivacy.org/icml2020/"><span class="datestr">at August 31, 2020 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=17496">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2020/08/31/mea-culpa/">Mea Culpa</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Plus more on the separating word problem</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/08/31/mea-culpa/mea/" rel="attachment wp-att-17499"><img width="110" alt="" src="https://rjlipton.files.wordpress.com/2020/08/mea.jpg?w=110&amp;h=147" class="alignright wp-image-17499" height="147" /></a></p>
<p>
Mea Culpa is not someone we introduced on the blog before. She does not come from the same world as Lofa Polir or Neil L. As in the French <a href="https://en.wikipedia.org/wiki/Mea_Culpa_(film)">movie</a> poster at right, the meaning is “my fault.”</p>
<p></p><p>
Today we apologize for some oversights and hasty omissions regarding the last <a href="https://rjlipton.wordpress.com/2020/08/29/20000-comments-and-more/">post</a>. Then we will explain Dick’s “laws with errors”.<br />
<span id="more-17496"></span></p>
<p>
Let’s be clear. The paper we discussed in the last post claiming an order <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log n}" class="latex" title="{\log n}" /> upper bound is wrong. The best upper bound is still due to Zachary Chase and is order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{1/3}}" class="latex" title="{n^{1/3}}" />, with some log’s. </p>
<p>
At GLL we try to be fair and upbeat, especially with proof attempts. We know how hard it is to put yourself out there when you claim a new result. Whether for a famous open problem or not, it is always difficult. Mistakes are possible, miss understandings are possible. </p>
<p>
But perhaps this time we have gone too far, and for that we apologize. We just love the SWP, and want to reiterate the idea the post was supposed to be (only) about, an approach to SWP. </p>
<p>
There is also Nostra Culpa—“our fault,” Ken says too.  Not only is there a short <a href="https://www.imdb.com/title/tt8513100/">film</a> with that title, there is a 2013 one-singer <a href="https://www.youtube.com/watch?v=t2paj1mrCQI">opera</a> of that title with libretto drawn from a 2012 Twitter <a href="https://news.err.ee/106193/nostra-culpa-an-interview-with-the-writers-of-a-financial-opera">feud</a> between the economist Paul Krugman and the president of Estonia. At least our issue did not break out on Twitter. We were instead contacted privately by some friends who knew more about the erroneous paper highlighted in our post. We could have contacted them first—that was our omission.</p>
<p>
Again sorry for any confusion we created. Let’s turn to the approach we want to share about SWP.</p>
<p>
</p><p></p><h2> Laws with Errors </h2><p></p>
<p></p><p>
I think there is hope that positive laws must be large, especially if we extend what it mean by a law. Consider a positive law 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++U+%3D+V%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  U = V, " class="latex" title="\displaystyle  U = V, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> are words over the letters <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. Recall we can tell if <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> are different provided there is a finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> so that for some <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B}" class="latex" title="{A,B}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> the value of <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> are different. This can be computed by a FSA with at most order <img src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|G|}" class="latex" title="{|G|}" /> states. So the smaller <img src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|G|}" class="latex" title="{|G|}" /> is the better. </p>
<p>
The trouble is it does not seem to the case that strong enough lower bounds are known for positive laws. This suggest that we make it harder for a law to work. </p>
<p>
The idea is to use a FST, that is a finite state transducer. This is a finite state device that reads a symbol, updates its state, and outputs one or more symbols. If <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is the FST and <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> is a string, let <img src="https://s0.wp.com/latex.php?latex=%7BT%28U%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T(U)}" class="latex" title="{T(U)}" /> be the result of applying <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />. For example, suppose that <img src="https://s0.wp.com/latex.php?latex=%7BABBBAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{ABBBAB}" class="latex" title="{ABBBAB}" /> is an input <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" />. Let the transducer add a <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> after each <img src="https://s0.wp.com/latex.php?latex=%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{AB}" class="latex" title="{AB}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> maps to: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ABCBBABC.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  ABCBBABC. " class="latex" title="\displaystyle  ABCBBABC. " /></p>
<p>	 Then the following is true: </p>
<blockquote><p><b>Lemma 1</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> be group and let <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> be a transducer with <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> states. Then we can separate <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> from <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> by a FSA with order <img src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C+%5Ccdot+S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{|G| \cdot S}" class="latex" title="{|G| \cdot S}" /> states provided 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++T%28U%29+%3D+T%28V%29+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  T(U) = T(V) " class="latex" title="\displaystyle  T(U) = T(V) " /></p>
</em><p><em>is not a law in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. </em>
</p></blockquote>
<p></p><p>
<em>Proof:</em>  The key is to simulate <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> as we read the input, and at the same time update the group element. This can be done by a FSA with the product <img src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|G|}" class="latex" title="{|G|}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> states. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
The hope is that having a law that is invariant for all FST, even only those with few states, is difficult. </p>
<blockquote><p><b> </b> <em> <b>A conjecture</b>: Positive laws that can handle all FST transducers with <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Cepsilon%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n^{\epsilon}}" class="latex" title="{n^{\epsilon}}" /> states must be large. </em>
</p></blockquote>
<p>That is are large enough to advance our best bounds on SWP. What do you think?</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2020/08/31/mea-culpa/"><span class="datestr">at August 31, 2020 01:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=99">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2020/08/31/algorithms-for-convex-optimization-book/">Algorithms for Convex Optimization Book</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am excited to announce that a pre-publication draft of my book <strong>Algorithms for Convex Optimization</strong> (to be published by Cambridge University Press) is now available for download here:</p>
<p><a href="https://convex-optimization.github.io/">Algorithms for Convex Optimization Book</a></p>
<p>The goal of this book is to enable a reader to gain an in-depth understanding of algorithms for convex optimization. The emphasis is to derive key algorithms for convex optimization from first principles and to establish precise running time bounds in terms of the input length.</p>
<p>The intended audience includes advanced undergraduate students, graduate students and researches from theoretical computer science, discrete optimization, and machine learning.</p>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
<p>The book has four parts:</p>
</div>
</div>
</div>
<ol>
<li class="p1">Chapters 3,4, and 5: Introduction to convexity, models of computation and notions of efficiency in convex optimization, Lagrangian duality, Legendre-Fenchel duality, and KKT conditions.</li>
<li class="p1">Chapters 6,7, and 8: First-order methods such as gradient descent, mirror descent and the multiplicative weights update method, and accelerated gradient descent.</li>
<li class="p1">Chapters 9,10, and 11: Newton’s method, path-following interior point methods for linear programming, and self-concordant barrier functions.</li>
<li class="p1">Chapter 11 and 12: Cutting plane methods such as the ellipsoid method for linear and general convex programs.</li>
</ol>
<p class="p1">Chapter 1 summarizes the book via a brief history of the interplay between continuous and discrete optimization: how the search for fast algorithms for discrete problems is leading to improvements in algorithms for convex optimization.</p>
<p class="p1">Many chapters contain applications ranging from finding maximum flows, minimum cuts, and perfect matchings in graphs, to linear optimization over 0-1-polytopes, to submodular function minimization, to computing maximum entropy distributions over combinatorial polytopes.</p>
<p class="p1">The book is self-contained and starts with a review of calculus, linear algebra, geometry, dynamical systems, and graph theory in Chapter 2. Exercises posed in this book not only play an important role in checking one’s understanding, but sometimes important methods and concepts are introduced and developed entirely through them. Examples include the Frank-Wolfe method, coordinate descent, stochastic gradient descent, online convex optimization, the min-max theorem for zero-sum games, the Winnow algorithm for classification, the conjugate gradient method, primal-dual interior point method, and matrix scaling.</p>
<p>Feedback, corrections, and comments are welcome.</p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2020/08/31/algorithms-for-convex-optimization-book/"><span class="datestr">at August 31, 2020 01:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/08/31/postdoc-at-epfl-apply-by-september-13-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/08/31/postdoc-at-epfl-apply-by-september-13-2020/">postdoc at EPFL (apply by September 13, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are solicited for postdoctoral positions in all aspects of Theoretical Computer Science at EPFL. Each position will be for a period of up to two years and comes with a competitive salary and generous travel support.</p>
<p>Website: <a href="https://theory.epfl.ch/postdoc.html">https://theory.epfl.ch/postdoc.html</a><br />
Email: tcs-postdoc@groupes.epfl.ch</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/08/31/postdoc-at-epfl-apply-by-september-13-2020/"><span class="datestr">at August 31, 2020 08:29 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4942">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4942">My new motto</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong><span class="has-inline-color has-vivid-red-color">Update (Sep 1):</span></strong> Thanks for the comments, everyone!  As you can see, I further revised this blog’s header based on the feedback and on further reflection.<br /><br /><strong>The Right could only kill me and everyone I know.<br />The Left is scarier; it could convince me that it was my fault</strong>!</p>



<p>(In case you missed it on the blog’s revised header, right below “Quantum computers aren’t just nondeterministic Turing machines” and “Hold the November US election by mail.”  I added an exclamation point at the end to suggest a <em>slightly</em> comic delivery.)</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> A friend expressed concern that, because my new motto appears to “blame both sides,” it might generate confusion about my sympathies or what I want to happen in November.  So to eliminate all ambiguity: I hereby announce that I will match all reader donations made in the next 72 hours to either the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> or the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>, up to a limit of $2,000.  Honor system; just tell me in the comments what you donated.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4942"><span class="datestr">at August 30, 2020 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
