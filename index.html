<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 14, 2021 04:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06399">Frequent Pattern Mining in Continuous-time Temporal Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jazayeri:Ali.html">Ali Jazayeri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Christopher_C=.html">Christopher C. Yang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06399">PDF</a><br /><b>Abstract: </b>Networks are used as highly expressive tools in different disciplines. In
recent years, the analysis and mining of temporal networks have attracted
substantial attention. Frequent pattern mining is considered an essential task
in the network science literature. In addition to the numerous applications,
the investigation of frequent pattern mining in networks directly impacts other
analytical approaches, such as clustering, quasi-clique and clique mining, and
link prediction. In nearly all the algorithms proposed for frequent pattern
mining in temporal networks, the networks are represented as sequences of
static networks. Then, the inter- or intra-network patterns are mined. This
type of representation imposes a computation-expressiveness trade-off to the
mining problem. In this paper, we propose a novel representation that can
preserve the temporal aspects of the network losslessly. Then, we introduce the
concept of constrained interval graphs (CIGs). Next, we develop a series of
algorithms for mining the complete set of frequent temporal patterns in a
temporal network data set. We also consider four different definitions of
isomorphism to allow noise tolerance in temporal data collection. Implementing
the algorithm for three real-world data sets proves the practicality of the
proposed algorithm and its capability to discover unknown patterns in various
settings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06399"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06357">On Minimizing the Number of Running Buffers for Tabletop Rearrangement</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Kai.html">Kai Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Si_Wei.html">Si Wei Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Jingjin.html">Jingjin Yu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06357">PDF</a><br /><b>Abstract: </b>For tabletop rearrangement problems with overhand grasps, storage space
outside the tabletop workspace, or buffers, can temporarily hold objects which
greatly facilitates the resolution of a given rearrangement task. This brings
forth the natural question of how many running buffers are required so that
certain classes of tabletop rearrangement problems are feasible. In this work,
we examine the problem for both the labeled (where each object has a specific
goal pose) and the unlabeled (where goal poses of objects are interchangeable)
settings. On the structural side, we observe that finding the minimum number of
running buffers (MRB) can be carried out on a dependency graph abstracted from
a problem instance, and show that computing MRB on dependency graphs is
NP-hard. We then prove that under both labeled and unlabeled settings, even for
uniform cylindrical objects, the number of required running buffers may grow
unbounded as the number of objects to be rearranged increases; we further show
that the bound for the unlabeled case is tight. On the algorithmic side, we
develop highly effective algorithms for finding MRB for both labeled and
unlabeled tabletop rearrangement problems, scalable to over a hundred objects
under very high object density. Employing these algorithms, empirical
evaluations show that random labeled and unlabeled instances, which more
closely mimics real-world setups, have much smaller MRBs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06357"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06349">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06349">Disjoint Paths and Connected Subgraphs for H-Free Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kern:Walter.html">Walter Kern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paulusma:Dani=euml=l.html">Daniël Paulusma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Siani.html">Siani Smith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leeuwen:Erik_Jan_van.html">Erik Jan van Leeuwen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06349">PDF</a><br /><b>Abstract: </b>The well-known Disjoint Paths problem is to decide if a graph contains k
pairwise disjoint paths, each connecting a different terminal pair from a set
of k distinct pairs. We determine, with an exception of two cases, the
complexity of the Disjoint Paths problem for $H$-free graphs. If $k$ is fixed,
we obtain the $k$-Disjoint Paths problem, which is known to be polynomial-time
solvable on the class of all graphs for every $k \geq 1$. The latter does no
longer hold if we need to connect vertices from terminal sets instead of
terminal pairs. We completely classify the complexity of $k$-Disjoint Connected
Subgraphs for $H$-free graphs, and give the same almost-complete classification
for Disjoint Connected Subgraphs for $H$-free graphs as for Disjoint Paths.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06349"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06322">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06322">Hedging Against Sore Loser Attacks in Cross-Chain Transactions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Yingjie.html">Yingjie Xue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Herlihy:Maurice.html">Maurice Herlihy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06322">PDF</a><br /><b>Abstract: </b>A *sore loser attack* in cross-blockchain commerce rises when one party
decides to halt participation partway through, leaving other parties' assets
locked up for a long duration. Although vulnerability to sore loser attacks
cannot be entirely eliminated, it can be reduced to an arbitrarily low level.
This paper proposes new distributed protocols for hedging a range of
cross-chain transactions in a synchronous communication model, such as
two-party swaps, $n$-party swaps, brokered transactions, and auctions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06322"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06287">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06287">Analysis of Busy-Time Scheduling on Heterogeneous Machines</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Mozhengfu Liu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Xueyan.html">Xueyan Tang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06287">PDF</a><br /><b>Abstract: </b>This paper studies a generalized busy-time scheduling model on heterogeneous
machines. The input to the model includes a set of jobs and a set of machine
types. Each job has a size and a time interval during which it should be
processed. Each job is to be placed on a machine for execution. Different types
of machines have distinct capacities and cost rates. The total size of the jobs
running on a machine must always be kept within the machine's capacity, giving
rise to placement restrictions for jobs of various sizes among the machine
types. Each machine used is charged according to the time duration in which it
is busy, i.e., it is processing jobs. The objective is to schedule the jobs
onto machines to minimize the total cost of all the machines used. We develop
an $O(1)$-approximation algorithm in the offline setting and an
$O(\mu)$-competitive algorithm in the online setting (where $\mu$ is the
max/min job length ratio), both of which are asymptotically optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06287"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06166">The Dynamic k-Mismatch Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clifford:Rapha=euml=l.html">Raphaël Clifford</a>, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a>, Daniel P. Martin, Przemysław Uznański <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06166">PDF</a><br /><b>Abstract: </b>The text-to-pattern Hamming distances problem asks to compute the Hamming
distances between a given pattern of length $m$ and all length-$m$ substrings
of a given text of length $n\ge m$. We focus on the $k$-mismatch version of the
problem, where a distance needs to be returned only if it does not exceed a
threshold $k$. We assume $n\le 2m$ (in general, one can partition the text into
overlapping blocks). In this work, we show data structures for the dynamic
version of this problem supporting two operations: An update performs a
single-letter substitution in the pattern or the text, and a query, given an
index $i$, returns the Hamming distance between the pattern and the text
substring starting at position $i$, or reports that it exceeds $k$.
</p>
<p>First, we show a data structure with $\tilde{O}(1)$ update and $\tilde{O}(k)$
query time. Then we show that $\tilde{O}(k)$ update and $\tilde{O}(1)$ query
time is also possible. These two provide an optimal trade-off for the dynamic
$k$-mismatch problem with $k \le \sqrt{n}$: we prove that, conditioned on the
strong 3SUM conjecture, one cannot simultaneously achieve $k^{1-\Omega(1)}$
time for all operations.
</p>
<p>For $k\ge \sqrt{n}$, we give another lower bound, conditioned on the Online
Matrix-Vector conjecture, that excludes algorithms taking $n^{1/2-\Omega(1)}$
time per operation. This is tight for constant-sized alphabets: Clifford et al.
(STACS 2018) achieved $\tilde{O}(\sqrt{n})$ time per operation in that case,
but with $\tilde{O}(n^{3/4})$ time per operation for large alphabets. We
improve and extend this result with an algorithm that, given $1\le x\le k$,
achieves update time $\tilde{O}(\frac{n}{k} +\sqrt{\frac{nk}{x}})$ and query
time $\tilde{O}(x)$. In particular, for $k\ge \sqrt{n}$, an appropriate choice
of $x$ yields $\tilde{O}(\sqrt[3]{nk})$ time per operation, which is
$\tilde{O}(n^{2/3})$ when no threshold $k$ is provided.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06166"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06145">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06145">Efficient Stepping Algorithms and Implementations for Parallel Shortest Paths</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Xiaojun.html">Xiaojun Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yan.html">Yan Gu</a>, Yihan Sun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yunming.html">Yunming Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06145">PDF</a><br /><b>Abstract: </b>In this paper, we study the single-source shortest-path (SSSP) problem with
positive edge weights, which is a notoriously hard problem in the parallel
context. In practice, the $\Delta$-stepping algorithm proposed by Meyer and
Sanders has been widely adopted. However, $\Delta$-stepping has no known
worst-case bounds for general graphs. The performance of $\Delta$-stepping also
highly relies on the parameter $\Delta$. There have also been lots of
algorithms with theoretical bounds, such as Radius-stepping, but they either
have no implementations available or are much slower than $\Delta$-stepping in
practice.
</p>
<p>We propose a stepping algorithm framework that generalizes existing
algorithms such as $\Delta$-stepping and Radius-stepping. The framework allows
for similar analysis and implementations of all stepping algorithms. We also
propose a new ADT, lazy-batched priority queue (LaB-PQ), that abstracts the
semantics of the priority queue needed by the stepping algorithms. We provide
two data structures for LaB-PQ, focusing on theoretical and practical
efficiency, respectively. Based on the new framework and LaB-PQ, we show two
new stepping algorithms, $\rho$-stepping and $\Delta^*$-stepping, that are
simple, with non-trivial worst-case bounds, and fast in practice.
</p>
<p>The stepping algorithm framework also provides almost identical
implementations for three algorithms: Bellman-Ford, $\Delta^*$-stepping, and
$\rho$-stepping. We compare our code with four state-of-the-art
implementations. On five social and web graphs, $\rho$-stepping is 1.3--2.5x
faster than all the existing implementations. On two road graphs, our
$\Delta^*$-stepping is at least 14\% faster than existing implementations,
while $\rho$-stepping is also competitive. The almost identical implementations
for stepping algorithms also allow for in-depth analyses and comparisons among
the stepping algorithms in practice.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06145"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.06131">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.06131">A Fast Algorithm for SAT in Terms of Formula Length</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Junqiang.html">Junqiang Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Mingyu.html">Mingyu Xiao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06131">PDF</a><br /><b>Abstract: </b>In this paper, we prove that the general CNF satisfiability problem can be
solved in $O^*(1.0646^L)$ time, where $L$ is the length of the input
CNF-formula (i.e., the total number of literals in the formula), which improves
the current bound $O^*(1.0652^L)$ given by Chen and Liu 12 years ago. Our
algorithm is a standard branch-and-search algorithm analyzed by using the
measure-and-conquer method. We avoid the bottleneck in Chen and Liu's algorithm
by simplifying the branching operation for 4-degree variables and carefully
analyzing the branching operation for 5-degree variables. To simplify
case-analyses, we also introduce a general framework for analysis, which may be
able to be used in other problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.06131"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05984">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05984">Sparse Nonnegative Convolution Is Equivalent to Dense Nonnegative Convolution</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Nick.html">Nick Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05984">PDF</a><br /><b>Abstract: </b>Computing the convolution $A\star B$ of two length-$n$ vectors $A,B$ is an
ubiquitous computational primitive. Applications range from string problems to
Knapsack-type problems, and from 3SUM to All-Pairs Shortest Paths. These
applications often come in the form of nonnegative convolution, where the
entries of $A,B$ are nonnegative integers. The classical algorithm to compute
$A\star B$ uses the Fast Fourier Transform and runs in time $O(n\log n)$.
</p>
<p>However, often $A$ and $B$ satisfy sparsity conditions, and hence one could
hope for significant improvements. The ideal goal is an $O(k\log k)$-time
algorithm, where $k$ is the number of non-zero elements in the output, i.e.,
the size of the support of $A\star B$. This problem is referred to as sparse
nonnegative convolution, and has received considerable attention in the
literature; the fastest algorithms to date run in time $O(k\log^2 n)$.
</p>
<p>The main result of this paper is the first $O(k\log k)$-time algorithm for
sparse nonnegative convolution. Our algorithm is randomized and assumes that
the length $n$ and the largest entry of $A$ and $B$ are subexponential in $k$.
Surprisingly, we can phrase our algorithm as a reduction from the sparse case
to the dense case of nonnegative convolution, showing that, under some mild
assumptions, sparse nonnegative convolution is equivalent to dense nonnegative
convolution for constant-error randomized algorithms. Specifically, if $D(n)$
is the time to convolve two nonnegative length-$n$ vectors with success
probability $2/3$, and $S(k)$ is the time to convolve two nonnegative vectors
with output size $k$ with success probability $2/3$, then
$S(k)=O(D(k)+k(\log\log k)^2)$.
</p>
<p>Our approach uses a variety of new techniques in combination with some old
machinery from linear sketching and structured linear algebra, as well as new
insights on linear hashing, the most classical hash function.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05984"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05923">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05923">Open-end bin packing: new and old analysis approaches</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Leah.html">Leah Epstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05923">PDF</a><br /><b>Abstract: </b>We analyze a recently introduced concept, called the price of clustering, for
variants of bin packing called open-end bin packing problems (OEBP). Input
items have sizes, and they also belong to a certain number of types. The new
concept deals with the comparison of optimal solutions for the cases where
items of distinct types can and cannot be packed together, respectively. The
problem is related to greedy bin packing algorithms and to batched bin packing,
and we discuss some of those concepts as well. We analyze max-OEBP, where a
packed bin is valid if by excluding its largest item, the total size of items
is below 1. For this variant, we study the case of general item sizes, and the
parametric case with bounded item sizes, which shows the effect of small items.
Finally, we briefly discuss min-OEBP, where a bin is valid if the total size of
its items excluding the smallest item is below 1, which is known to be an
entirely different problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05923"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05911">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05911">The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Christopher Morris, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fey:Matthias.html">Matthias Fey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kriege:Nils_M=.html">Nils M. Kriege</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05911">PDF</a><br /><b>Abstract: </b>In recent years, algorithms and neural architectures based on the
Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism
problem, emerged as a powerful tool for (supervised) machine learning with
graphs and relational data. Here, we give a comprehensive overview of the
algorithm's use in a machine learning setting. We discuss the theoretical
background, show how to use it for supervised graph- and node classification,
discuss recent extensions, and its connection to neural architectures.
Moreover, we give an overview of current applications and future directions to
stimulate research.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05911"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05784">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05784">Particle-Based Assembly Using Precise Global Control</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jakob Keller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rieck:Christian.html">Christian Rieck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheffer:Christian.html">Christian Scheffer</a>, Arne Schmidt <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05784">PDF</a><br /><b>Abstract: </b>In micro- and nano-scale systems, particles can be moved by using an external
force like gravity or a magnetic field. In the presence of adhesive particles
that can attach to each other, the challenge is to decide whether a shape is
constructible. Previous work provides a class of shapes for which
constructibility can be decided efficiently, when particles move maximally into
the same direction on actuation.
</p>
<p>In this paper, we consider a stronger model. On actuation, each particle
moves one unit step into the given direction. We prove that deciding
constructibility is NP-hard for three-dimensional shapes, and that a maximum
constructible shape can be approximated. The same approximation algorithm
applies for 2D. We further present linear-time algorithms to decide whether a
tree-shape in 2D or 3D is constructible. If scaling is allowed, we show that
the $c$-scaled copy of every non-degenerate polyomino is constructible, for
every $c \geq 2$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05784"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05782">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05782">How to Design Robust Algorithms using Noisy Comparison Oracle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Addanki:Raghavendra.html">Raghavendra Addanki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galhotra:Sainyam.html">Sainyam Galhotra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saha:Barna.html">Barna Saha</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05782">PDF</a><br /><b>Abstract: </b>Metric based comparison operations such as finding maximum, nearest and
farthest neighbor are fundamental to studying various clustering techniques
such as $k$-center clustering and agglomerative hierarchical clustering. These
techniques crucially rely on accurate estimation of pairwise distance between
records. However, computing exact features of the records, and their pairwise
distances is often challenging, and sometimes not possible. We circumvent this
challenge by leveraging weak supervision in the form of a comparison oracle
that compares the relative distance between the queried points such as `Is
point u closer to v or w closer to x?'.
</p>
<p>However, it is possible that some queries are easier to answer than others
using a comparison oracle. We capture this by introducing two different noise
models called adversarial and probabilistic noise. In this paper, we study
various problems that include finding maximum, nearest/farthest neighbor search
under these noise models. Building upon the techniques we develop for these
comparison operations, we give robust algorithms for k-center clustering and
agglomerative hierarchical clustering. We prove that our algorithms achieve
good approximation guarantees with a high probability and analyze their query
complexity. We evaluate the effectiveness and efficiency of our techniques
empirically on various real-world datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05782"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05761">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05761">From Average Embeddings To Nearest Neighbor Search</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Andoni:Alexandr.html">Alexandr Andoni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheikhi:David.html">David Cheikhi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05761">PDF</a><br /><b>Abstract: </b>In this note, we show that one can use average embeddings, introduced
recently in [Naor'20, <a href="http://export.arxiv.org/abs/1905.01280">arXiv:1905.01280</a>], to obtain efficient algorithms for
approximate nearest neighbor search. In particular, a metric $X$ embeds into
$\ell_2$ on average, with distortion $D$, if, for any distribution $\mu$ on
$X$, the embedding is $D$ Lipschitz and the (square of) distance does not
decrease on average (wrt $\mu$). In particular existence of such an embedding
(assuming it is efficient) implies a $O(D^3)$ approximate nearest neighbor
search under $X$. This can be seen as a strengthening of the classic
(bi-Lipschitz) embedding approach to nearest neighbor search, and is another
application of data-dependent hashing paradigm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05761"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05725">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05725">On (Coalitional) Exchange-Stable Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jiehua Chen, Adrian Chmurovic, Fabian Jogl, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sorge:Manuel.html">Manuel Sorge</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05725">PDF</a><br /><b>Abstract: </b>We study (coalitional) exchange stability, which Alcalde [Review of Economic
Design, 1995] introduced as an alternative solution concept for matching
markets involving property rights, such as assigning persons to two-bed rooms.
Here, a matching of a given Stable Marriage or Stable Roommates instance is
called coalitional exchange-stable if it does not admit any exchange-blocking
coalition, that is, a subset S of agents in which everyone prefers the partner
of some other agent in S. The matching is exchange-stable if it does not admit
any exchange-blocking pair, that is,an exchange-blocking coalition of size two.
We investigate the computational and parameterized complexity of the
Coalitional Exchange-Stable Marriage (resp. Coalitional Exchange Roommates)
problem, which is to decide whether a Stable Marriage (resp. Stable Roommates)
instance admits a coalitional exchange-stable matching. Our findings resolve an
open question and confirm the conjecture of Cechl\'arov\'a and Manlove
[Discrete Applied Mathematics, 2005] that Coalitional Exchange-Stable Marriage
is NP-hard even for complete preferences without ties. We also study
bounded-length preference lists and a local-search variant of deciding whether
a given matching can reach an exchange-stable one after at most k swaps, where
a swap is defined as exchanging the partners of the two agents in an
exchange-blocking pair.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05725"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05685">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05685">Isomorphic unordered labeled trees up to substitution ciphering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ingels:Florian.html">Florian Ingels</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aza=iuml=s:Romain.html">Romain Azaïs</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05685">PDF</a><br /><b>Abstract: </b>Given two messages - as linear sequences of letters, it is immediate to
determine whether one can be transformed into the other by simple substitution
cipher of the letters. On the other hand, if the letters are carried as labels
on nodes of topologically isomorphic unordered trees, determining if a
substitution exists is referred to as marked tree isomorphism problem in the
literature and has been show to be as hard as graph isomorphism. While the
left-to-right direction provides the cipher of letters in the case of linear
messages, if the messages are carried by unordered trees, the cipher is given
by a tree isomorphism. The number of isomorphisms between two trees is roughly
exponential in the size of the trees, which makes the problem of finding a
cipher difficult by exhaustive search. This paper presents a method that aims
to break the combinatorics of the isomorphisms search space. We show that in a
linear time (in the size of the trees), we reduce the cardinality of this space
by an exponential factor on average.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05685"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05637">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05637">A Mathematical Definition of Particle Methods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Johannes Bamme, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sbalzarini:Ivo_F=.html">Ivo F. Sbalzarini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05637">PDF</a><br /><b>Abstract: </b>We provide a formal definition for a class of algorithms known as "particle
methods". Particle methods are used in scientific computing. They include
popular simulation methods, such as Discrete Element Methods (DEM), Molecular
Dynamics (MD), Particle Strength Exchange (PSE), and Smoothed Particle
Hydrodynamics (SPH), but also particle-based image processing methods,
point-based computer graphics, and computational optimization algorithms using
point samples. All of these rest on a common concept, which we here formally
define. The presented definition of particle methods makes it possible to
distinguish what formally constitutes a particle method, and what not. It also
enables us to define different sub-classes of particle methods that differ with
respect to their computational complexity and power. Our definition is purely
formal, independent of any application. After stating the definition, we
therefore illustrate how several well-known particle methods can be formalized
in our framework, and we show how the formal definition can be used to
formulate novel particle methods for non-canonical problems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05637"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05626">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05626">Reversify any sequential algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gurevich:Yuri.html">Yuri Gurevich</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05626">PDF</a><br /><b>Abstract: </b>To reversify an arbitrary sequential algorithm $A$, we gently instrument $A$
with bookkeeping machinery. The result is a step-for-step reversible algorithm
that mimics $A$ step-for-step and stops exactly when $A$ does.
</p>
<p>Without loss of generality, we presume that algorithm $A$ is presented as an
abstract state machine that is behaviorally identical to $A$. The existence of
such representation has been proven theoretically, and the practicality of such
representation has been amply demonstrated.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05626"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05574">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05574">Locally Checkable Labelings with Small Messages</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balliu:Alkida.html">Alkida Balliu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olivetti:Dennis.html">Dennis Olivetti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suomela:Jukka.html">Jukka Suomela</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05574">PDF</a><br /><b>Abstract: </b>A rich line of work has been addressing the computational complexity of
locally checkable labelings (\LCL{}s), illustrating the landscape of possible
complexities. In this paper, we study the landscape of \LCL complexities under
bandwidth restrictions. Our main results are twofold. First, we show that on
trees, the \CONGEST complexity of an \LCL problem is asymptotically equal to
its complexity in the \LOCAL model. An analog statement for general (non-\LCL)
problems is known to be false. Second, we show that for general graphs this
equivalence does not hold, by providing an \LCL problem for which we show that
it can be solved in $O(\log n)$ rounds in the \LOCAL model, but requires
$\tilde{\Omega}(n^{1/2})$ rounds in the \CONGEST model.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05574"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05555">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05555">Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Yu.html">Yu Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Honghao.html">Honghao Lin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05555">PDF</a><br /><b>Abstract: </b>We study the problem of learning Bayesian networks where an
$\epsilon$-fraction of the samples are adversarially corrupted. We focus on the
fully-observable case where the underlying graph structure is known. In this
work, we present the first nearly-linear time algorithm for this problem with a
dimension-independent error guarantee. Previous robust algorithms with
comparable error guarantees are slower by at least a factor of $(d/\epsilon)$,
where $d$ is the number of variables in the Bayesian network and $\epsilon$ is
the fraction of corrupted samples.
</p>
<p>Our algorithm and analysis are considerably simpler than those in previous
work. We achieve this by establishing a direct connection between robust
learning of Bayesian networks and robust mean estimation. As a subroutine in
our algorithm, we develop a robust mean estimation algorithm whose runtime is
nearly-linear in the number of nonzeros in the input samples, which may be of
independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05555"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05503">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05503">kMatrix: A Space Efficient Streaming Graph Summarization Technique</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Oshan Mudannayake, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ranasinghe:Nalin.html">Nalin Ranasinghe</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05503">PDF</a><br /><b>Abstract: </b>The amount of collected information on data repositories has vastly increased
with the advent of the internet. It has become increasingly complex to deal
with these massive data streams due to their sheer volume and the throughput of
incoming data. Many of these data streams are mapped into graphs, which helps
discover some of their properties. However, due to the difficulty in processing
massive streaming graphs, they are summarized such that their properties can be
approximately evaluated using the summaries. gSketch, TCM, and gMatrix are some
of the major streaming graph summarization techniques. Our primary contribution
is devising kMatrix, which is much more memory efficient than existing
streaming graph summarization techniques. We achieved this by partitioning the
allocated memory using a sample of the original graph stream. Through the
experiments, we show that kMatrix can achieve a significantly less error for
the queries using the same space as that of TCM and gMatrix.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05503"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05495">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05495">LipBaB: Computing exact Lipschitz constant of ReLU networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Aritra Bhowmick, Meenakshi D'Souza, G. Srinivasa Raghavan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05495">PDF</a><br /><b>Abstract: </b>The Lipschitz constant of neural networks plays an important role in several
contexts of deep learning ranging from robustness certification and
regularization to stability analysis of systems with neural network
controllers. Obtaining tight bounds of the Lipschitz constant is therefore
important. We introduce LipBaB, a branch and bound framework to compute
certified bounds of the local Lipschitz constant of deep neural networks with
ReLU activation functions up to any desired precision. We achieve this by
bounding the norm of the Jacobians, corresponding to different activation
patterns of the network caused within the input domain. Our algorithm can
provide provably exact computation of the Lipschitz constant for any p-norm.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05495"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05275">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05275">Hermitian Symmetric Spaces for Graph Embeddings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=oacute=pez:Federico.html">Federico López</a>, Beatrice Pozzetti, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trettel:Steve.html">Steve Trettel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wienhard:Anna.html">Anna Wienhard</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05275">PDF</a><br /><b>Abstract: </b>Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. The quality of the embeddings is usually determined by how well
the geometry of the target space matches the structure of the data. In this
work we learn continuous representations of graphs in spaces of symmetric
matrices over C. These spaces offer a rich geometry that simultaneously admits
hyperbolic and Euclidean subspaces, and are amenable to analysis and explicit
computations. We implement an efficient method to learn embeddings and compute
distances, and develop the tools to operate with such spaces. The proposed
models are able to automatically adapt to very dissimilar arrangements without
any apriori estimates of graph features. On various datasets with very diverse
structural properties and reconstruction measures our model ties the results of
competitive baselines for geometrically pure graphs and outperforms them for
graphs with mixed geometric features, showcasing the versatility of our
approach.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05275"><span class="datestr">at May 14, 2021 12:01 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2012.12347">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2012.12347">Beating Random Assignment for Approximating Quantum 2-Local Hamiltonian Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parekh:Ojas.html">Ojas Parekh</a>, Kevin Thompson <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12347">PDF</a><br /><b>Abstract: </b>The quantum k-Local Hamiltonian problem is a natural generalization of
classical constraint satisfaction problems (k-CSP) and is complete for QMA, a
quantum analog of NP. Although the complexity of k-Local Hamiltonian problems
has been well studied, only a handful of approximation results are known. For
Max 2-Local Hamiltonian where each term is a rank 3 projector, a natural
quantum generalization of classical Max 2-SAT, the best known approximation
algorithm was the trivial random assignment, yielding a 0.75-approximation. We
present the first approximation algorithm beating this bound, a classical
polynomial-time 0.764-approximation. For strictly quadratic instances, which
are maximally entangled instances, we provide a 0.801 approximation algorithm,
and numerically demonstrate that our algorithm is likely a 0.821-approximation.
We conjecture these are the hardest instances to approximate. We also give
improved approximations for quantum generalizations of other related classical
2-CSPs. Finally, we exploit quantum connections to a generalization of the
Grothendieck problem to obtain a classical constant-factor approximation for
the physically relevant special case of strictly quadratic traceless 2-Local
Hamiltonians on bipartite interaction graphs, where a inverse logarithmic
approximation was the best previously known (for general interaction graphs).
Our work employs recently developed techniques for analyzing classical
approximations of CSPs and is intended to be accessible to both quantum
information scientists and classical computer scientists.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2012.12347"><span class="datestr">at May 14, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/070">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/070">TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/070"><span class="datestr">at May 13, 2021 05:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/">Matrix—The Meeting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img width="192" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" class="alignright wp-image-18739" height="128" /></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{20-60}" class="latex" /> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img width="384" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" class="alignright wp-image-18742" height="216" /></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br />
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p></p>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p></p><h2> The Question </h2><p></p>
<p></p><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  Ax = b " class="latex" /></p>
<p>Alice could go off and return the <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> that solves the system. Or she could say there is no such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. Or she could say there are many such <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> I have. <p></p>
</li><li>
I plan to see what the norm of <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> is. <p></p>
</li><li>
Or, I plan to see what <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_1}" class="latex" /> is. <p></p>
</li><li>
Or, I could be just happy to know that there is some <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" />. <p></p>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p></p><h2> One View </h2><p></p>
<p></p><p>
What does it mean to solve the equation <img src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{Ax=b}" class="latex" />, for an invertible matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" />? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br />
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br />
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/"><span class="datestr">at May 13, 2021 12:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1929264998273205739">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html">Cryptocurrency, Blockchains and NFTs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p></p><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br /></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html"><span class="datestr">at May 13, 2021 11:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05673">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05673">Breaking O(nr) for Matroid Intersection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blikstad:Joakim.html">Joakim Blikstad</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05673">PDF</a><br /><b>Abstract: </b>We present algorithms that break the $\tilde O(nr)$-independence-query bound
for the Matroid Intersection problem for the full range of $r$; where $n$ is
the size of the ground set and $r\leq n$ is the size of the largest common
independent set. The $\tilde O(nr)$ bound was due to the efficient
implementations [CLSSW FOCS'19; Nguyen 2019] of the classic algorithm of
Cunningham [SICOMP'86]. It was recently broken for large $r$
($r=\omega(\sqrt{n})$), first by the $\tilde O(n^{1.5}/\epsilon^{1.5})$-query
$(1-\epsilon)$-approximation algorithm of CLSSW [FOCS'19], and subsequently by
the $\tilde O(n^{6/5}r^{3/5})$-query exact algorithm of BvdBMN [STOC'21]. No
algorithm, even an approximation one, was known to break the $\tilde O(nr)$
bound for the full range of $r$. We present an $\tilde
O(n\sqrt{r}/\epsilon)$-query $(1-\epsilon)$-approximation algorithm and an
$\tilde O(nr^{3/4})$-query exact algorithm. Our algorithms improve the $\tilde
O(nr)$ bound and also the bounds by CLSSW and BvdBMN for the full range of $r$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05673"><span class="datestr">at May 13, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05500">Test of Quantumness with Small-Depth Quantum Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirahara:Shuichi.html">Shuichi Hirahara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gall:Fran=ccedil=ois_Le.html">François Le Gall</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05500">PDF</a><br /><b>Abstract: </b>Recently Brakerski, Christiano, Mahadev, Vazirani and Vidick (FOCS 2018) have
shown how to construct a test of quantumness based on the learning with errors
(LWE) assumption: a test that can be solved efficiently by a quantum computer
but cannot be solved by a classical polynomial-time computer under the LWE
assumption. This test has lead to several cryptographic applications. In
particular, it has been applied to producing certifiable randomness from a
single untrusted quantum device, self-testing a single quantum device and
device-independent quantum key distribution.
</p>
<p>In this paper, we show that this test of quantumness, and essentially all the
above applications, can actually be implemented by a very weak class of quantum
circuits: constant-depth quantum circuits combined with logarithmic-depth
classical computation. This reveals novel complexity-theoretic properties of
this fundamental test of quantumness and gives new concrete evidence of the
superiority of small-depth quantum circuits over classical computation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05500"><span class="datestr">at May 13, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05431">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05431">Proving Regulatory Compliance: A Computational Complexity Analysis of Elementary Variants</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tosatto:Silvano_Colombo.html">Silvano Colombo Tosatto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Governatori:Guido.html">Guido Governatori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beest:Nick_van.html">Nick van Beest</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05431">PDF</a><br /><b>Abstract: </b>Organisations model their processes using so-called business process models,
to allow for verification of their correctness with respect to regulatory
requirements and business rules. Automated methods for checking compliance,
however, have to deal with the high complexity of the requirements as well as
the significant size and quantity of process models in an organisation, which
may prevent process models from being checked efficiently and timely. This
paper provides a computational complexity analysis of the problem of proving
regulatory compliance of process models. We investigate the computational
complexity of each variant of the problem resulting from a combination of three
binary properties associated to the regulatory framework, determining the
regulatory requirements that a process model needs to follow to be compliant.
These binary properties are whether the framework contains one or multiple
obligations, whether the obligations are global or conditional, and whether
only literals or formulae can be used to describe the obligations. For each
variant of the problem we study the computational complexity of proving full
compliance, partial compliance, and non-compliance. This analysis allows to
understand the specific features of the problem leading to intractability
issues, thus potentially guiding future research towards developing feasible
solutions for the problem in practical settings.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05431"><span class="datestr">at May 13, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2105.05383">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2105.05383">On the probability of generating a primitive matrix</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Jingwei.html">Jingwei Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Yong.html">Yong Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang.html">Yang Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Wenyuan.html">Wenyuan Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05383">PDF</a><br /><b>Abstract: </b>Given a $k\times n$ integer primitive matrix $A$ (i.e., a matrix can be
extended to an $n\times n$ unimodular matrix over the integers) with size of
entries bounded by $\lambda$, we study the probability that the $m\times n$
matrix extended from $A$ by choosing other $m-k$ vectors uniformly at random
from $\{0, 1, \ldots, \lambda-1\}$ is still primitive. We present a complete
and rigorous proof that the probability is at least a constant for the case of
$m\le n-4$. Previously, only the limit case for $\lambda\rightarrow\infty$ with
$k=0$ was analysed in Maze et al. (2011), known as the natural density. As an
application, we prove that there exists a fast Las Vegas algorithm that
completes a $k\times n$ primitive matrix $A$ to an $n\times n$ unimodular
matrix within expected $\tilde{O}(n^{\omega}\log \|A\|)$ bit operations, where
$\tilde{O}$ is big-$O$ but without log factors, $\omega$ is the exponent on the
arithmetic operations of matrix multiplication and $\|A\|$ is the maximal
absolute value of entries of $A$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2105.05383"><span class="datestr">at May 13, 2021 11:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html">The constructive solid geometry of piecewise-linear functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/minmax.svg" alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" /></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg" alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" /></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html"><span class="datestr">at May 12, 2021 05:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/069">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/069">TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/069"><span class="datestr">at May 12, 2021 08:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/glm_saga/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/glm_saga/">Debuggable Deep Networks: Sparse Linear Models (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/intro.png" alt="Overview" /></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png" alt="Examples of feature visualization" /></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png" alt="Examples of word cloud visualization" /></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png" alt="Feature visualization dump" /></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas width="400" id="ablation_dense_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img src="https://gradientscience.org/feed.xml" id="reg" />
        <div id="reg_slider"></div>
        <div class="quarterblock"> </div>
        <div style="text-align: center;" class="quarterblock">Accuracy: <span id="reg_accuracy"></span>%</div>
        <div style="text-align: center;" class="quarterblock">Non-zero: <span id="reg_sparsity"></span>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas width="400" id="ablation_chart" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span style="text-align: center;" class="glm_weight"></span>
                <img src="https://gradientscience.org/feed.xml" class="smallimg" />
            </div>
        </div>
    </div>
    <div id="zoom">
        <img src="https://gradientscience.org/feed.xml" id="feature_big" />
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>







<p class="date">
<a href="https://gradientscience.org/glm_saga/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/debugging/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/debugging/">Debuggable Deep Networks: Usage and Evaluation (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2105.04857" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/DebuggableDeepNetworks" class="bbutton">
<i class="fab fa-github"></i>
   Code
</a>

<br /></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png" height="350" /></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png" alt="Wordcloud visualization of feature used in unbiased BERT" /></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" value="" id="toxic_">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="christianity" id="toxic_christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="African" id="toxic_African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" value="Catholic" id="toxic_Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"></span>
        <canvas width="400" id="toxic" height="200"></canvas>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png" height="350" /></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"></div>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>

</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th colspan="3" class="reg_header">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas width="400" id="sst_canvas" height="200"></canvas>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png" alt="ImageNet counterfactuals" /></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img src="https://gradientscience.org/assets/glm_saga/figures/problematic.png" alt="Problematic features" /></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png" height="350" /></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"></div>
  <div style="float: none;" class="blocktxt" id="mislabels"> </div>
  <div style="clear: both;" id="misimages"> </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"></span><br /><span style="color: red;" class="choices_info_text"><b></b></span><br /><span style="color: green;" class="choices_info_text"><b></b></span><br /><hr /><h3 style="text-align: center;"><h3><div style="text-align: center; font-weight: 300; margin: 0.75em auto;" class="sp_txt"></div><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png" /><hr style="margin: 0.75em auto;" /><div style="text-align: center; font-weight: 200;" class="sp_txt"><span></span></div><hr style="margin: 0.3em auto;" /><h3 style="text-align: center;"><h3><div style="text-align: center;" class="sp_txt"><span style="font-weight: 200;"></span></div><br /><span></span></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"></span><span class="choices_info_text"></span><br /><span class="choices_info_text"></span><div class="mis_txt blocktxt thirdblock"><br /><span class="widgetheading"></span></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;" /></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png" /></div></div></h3></h3></div>







<p class="date">
<a href="https://gradientscience.org/debugging/"><span class="datestr">at May 12, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8117">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">STOC Test of time award</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br /><br />The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br /><br />Please nominate the papers you think have most influenced our field!</li></ul>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/"><span class="datestr">at May 11, 2021 06:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/">Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br />
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/"><span class="datestr">at May 10, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5486">Three updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br /></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br /></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5486"><span class="datestr">at May 10, 2021 05:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8609684815037895352">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html">Trump, Facebook, and ComplexityBlog</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html"><span class="datestr">at May 10, 2021 12:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">Arc-triangle tilings</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg" alt="Tiling by ordinary triangles" /></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg" alt="Tiling by arc-triangles with two curved sides" /></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg" alt="Tiling by arc-triangles with three curved sides" /></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/pinwheels.svg" alt="Tiling by arc-triangles with two curved sides, without reflection" /></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html"><span class="datestr">at May 09, 2021 04:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
