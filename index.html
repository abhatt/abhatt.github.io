<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 03, 2019 10:24 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/?p=116">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2019/04/15/reinforcement-learning-without-rewards/">Reinforcement learning without rewards</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>by <a href="http://www.abbyvansoest.com/">Abby van Soest</a> and Elad Hazan, based on <a href="https://arxiv.org/abs/1812.02690">this paper</a> </em></p>
<p>When humans interact with the environment, we receive a series of signals that indicate the value of our actions. We know that chocolate tastes good, sunburns feel bad, and certain actions generate praise or disapproval from others. Generally speaking, we learn from these signals and adapt our behavior in order to get more positive “rewards” and fewer negative ones.</p>
<p><i>Reinforcement learning</i> (RL) is a sub-field of machine learning that formally models this setting of learning through interaction in a reactive environment. In RL, we have an <b>agent</b> and an <b>environment</b>. The agent observes its position (or “state”) in the environment and takes actions that transition it to a new state. The environment looks at an agent’s state and hands out rewards based on a hidden set of criteria.</p>
<p style="text-align: center;"><img src="https://minimizingregret.files.wordpress.com/2019/04/null.png?w=360&amp;h=145" alt="" title="" width="360" height="145" /></p>
<p><i>Source: <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Reinforcement Learning: An Introduction</a>. Sutton &amp; Barto, 2017.</i></p>
<p>Typically, the goal of RL is for the agent to learn behavior that maximizes<i> </i>the total reward it receives from the environment. This methodology has led to some notable successes: machines have learned how to<a href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/"> play Atari games</a>, how to <a href="https://sigmoidal.io/alphago-how-it-uses-reinforcement-learning-to-beat-go-masters/">beat human masters</a> of Go, and how to write <a href="https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2">long-form responses</a> to an essay prompt.</p>
<h2>But what can we learn without an external reward signal?</h2>
<p>It seems like a paradoxical question to ask, given that RL is all about rewards. But even though the reward paradigm is fundamentally flexible in many ways, it is also brittle and limits the agent’s ability to learn about its environment. This is due to several reasons. First, a reward signal directs the agents towards a single specific goal that may not generalize. Second, the reward signal may be sparse and uninformative, as we illustrate below.</p>
<p>Imagine that you want a robot to learn to navigate through the following maze.</p>
<p><b>Case 1: Sparse Rewards.</b> The agent gets a reward of +1 when it exits the maze, and a reward of 0 everywhere else. The agent doesn’t learn anything until it stumbles upon the exit.</p>
<p style="text-align: center;">⇒ Clear reward signals are not always available.</p>
<p><b>Case 2: Misleading Rewards.</b> The agent gets a reward of +1 at the entrance and a reward of +10 at the exit. The agent incorrectly learns to sit at the entrance because it hasn’t explored its environment sufficiently.</p>
<p style="text-align: center;">⇒ Rewards can <i>prevent</i> discovery of the full environment.</p>
<p>These issues are easy to overcome in the small maze on the left. But what about the maze on the right? As the size of the environment grows, it’ll get harder and harder to find the correct solution — the intractability of the problem scales exponentially.</p>
<p style="text-align: center;"><img src="https://minimizingregret.files.wordpress.com/2019/04/image.png?w=244&amp;h=238" alt="" title="" width="244" height="238" /><img src="https://minimizingregret.files.wordpress.com/2019/04/pasted-image-0.png?w=1100" alt="pasted image 0.png" class="alignnone size-full wp-image-130" /></p>
<p>So what we find is that there is power<i> </i>in being able to learn effectively in the <b>absence</b> of rewards. This intuition is supported by a body of research that shows learning fails when rewards aren’t dense or are <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf">poorly shaped</a>; and fixing these problems can require substantial engineering effort.</p>
<p>By enabling agents to discover the environment without the requirement of a reward signal, we create a more<b> flexible and generalizable</b> form of reinforcement learning. This framework can be considered a form of “unsupervised” RL. Rather than relying on explicit and inherently limited signals (or “labels”), we can deal with a broad, unlabelled pool of data. Learning from this pool facilitates a more general extraction of knowledge from the environment.</p>
<h2>Our new approach: Maximum Entropy</h2>
<p>In <a href="https://arxiv.org/abs/1812.02690">recent work</a>, we propose finding a policy that maximizes entropy (which we refer to as a MaxEnt policy), or another related and concave function of the distribution. This objective is reward-independent and favors exploration.</p>
<p>In the video below, a two-dimensional cheetah robot learns to run backwards and forwards, move its legs fast and in all different directions, and even do flips. The cheetah doesn’t have access to any external rewards; it only uses signals from the MaxEnt policy.</p>
<p> </p>
<div style="width: 1100px; height: 620px;" class="video-player" id="v-ro0BDWO4-1">
</div>
<p>Entropy is a function of the distribution over states. A high entropy distribution visits all states with near-equal frequency — it’s a <i>uniform</i> distribution. On the other hand, a low entropy distribution is biased toward visiting some states more frequently than others. (In the maze example, a low entropy distribution would result from the agent sitting at the entrance of the maze forever.)</p>
<p><img src="https://minimizingregret.files.wordpress.com/2019/04/sladx3i4fcsebwejbiohb_g.png?w=1100" alt="slAdX3I4FcseBWeJbioHb_g.png" class=" size-full wp-image-129 aligncenter" /></p>
<p>So given that policy <img src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\pi" class="latex" title="\pi" /> creates a distribution <img src="https://s0.wp.com/latex.php?latex=d_%5Cpi&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="d_\pi" class="latex" title="d_\pi" /> over states, the problem we are hoping to solve is:</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=%5Cpi%5E%2A+%3D+%5Carg+%5Cmax_%7B%5Cpi%7D+%5Ctext%7Bentropy%7D%28d_%5Cpi%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\pi^* = \arg \max_{\pi} \text{entropy}(d_\pi) " class="latex" title="\pi^* = \arg \max_{\pi} \text{entropy}(d_\pi) " /></p>
<p>When we know all the states, actions, and dynamics of a given environment, finding the policy with maximum entropy is a concave optimization problem. This type of problem can be easily and exactly solved by convex programming.</p>
<p>But we very rarely have all that knowledge available to use. In practice, one of several complications usually arise:</p>
<ol>
<li>The states are <b>non-linearly approximated </b>by a neural network or some other function approximator.</li>
<li>The transition dynamics are unknown.</li>
<li>The state space is intractably large. (As an interesting example, the game of Go has more than<b> one googol or <img src="https://s0.wp.com/latex.php?latex=10%5E%7B100%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="10^{100}" class="latex" title="10^{100}" /></b> possible states. That’s more than the number of atoms in the universe, according to <a href="https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html">this blog</a> from 2016, and definitely more than can fit on a computer.)</li>
</ol>
<p>In such cases, the problem of finding a max-entropy policy becomes non-convex and computationally hard.</p>
<p>So what to do? If we look at many practical RL problems (Atari, OpenAI Gym), we see that there are many known, efficient solvers that can construct an optimal (or nearly-optimal) policy when they are <i>given a reward signal</i>.</p>
<p>We thus consider an <i>oracle model</i>: let’s assume that we have access to one of these solvers, so that we can pass it an explicit reward vector and receive an optimal policy in return. Can we now maximize entropy in a provably efficient way? In other words, is it possible to reduce this high complexity optimization problem to that of “standard” RL?</p>
<p>Our approach does exactly that! It is based on the Frank-Wolfe method. This is a gradient-based optimization algorithm that is particularly suited for oracle-based optimization. Instead of moving in the direction of the steepest decline of the objective function, the Frank-Wolfe method iteratively moves in the direction of the optimal point in the direction of the gradient. This is depicted below (and deserves a separate post…). The Frank-Wolfe method is a projection-free algorithm, see <a href="https://drive.google.com/file/d/1GIDnw7T-NT4Do3eC0B5kYJlzwOs6nzIO/view">this exposition</a> about its theoretical properties.</p>
<p><img src="https://minimizingregret.files.wordpress.com/2019/04/unnamed-file.png?w=616&amp;h=345" alt="" title="" width="616" height="345" /></p>
<p>For the exact specification of the algorithm and its performance guarantee, see<a href="https://arxiv.org/abs/1812.02690"> our paper</a>.</p>
<h2>Experiments</h2>
<p>To complement the theory, we also created some experiments to test the MaxEnt algorithm on simulated robotic locomotion tasks (<a href="https://github.com/abbyvansoest/maxent/tree/refactor">open source code available here</a>). We used test environments from <a href="https://gym.openai.com/">OpenAI Gym</a> and <a href="https://github.com/openai/mujoco-py">Mujoco</a> and trained MaxEnt experts for various environments.</p>
<p>These are some results from the Humanoid experiment, where the agent is a human-like bipedal robot. The behavior of the MaxEnt agent (blue) is baselined against a random agent (orange), who explores by sampling randomly from the environment. This random approach is often used in practice for epsilon-greedy RL exploration.</p>
<p style="text-align: center;"><img src="https://minimizingregret.files.wordpress.com/2019/04/null-1.png?w=442&amp;h=332" alt="" title="" width="442" height="332" /></p>
<p>In this figure, we see that over the course of 25 epochs, the MaxEnt agent progressively increases the total entropy over the state space.</p>
<p style="text-align: center;"><img src="https://minimizingregret.files.wordpress.com/2019/04/null-2.png?w=460&amp;h=345" alt="" title="" width="460" height="345" /></p>
<p>Here, we see a visualization of the Humanoid’s coverage of the $xy$-plane, where the shown plane is of size 40-by-40. After one epoch, there is <b>minimal</b> coverage of the area. But by the 5th, 10th, and 15th epoch, we see that the agent has learned to visit all the different states in the plane, obtaining full and nearly uniform coverage of the grid!</p>
<div><a href="https://minimizingregret.wordpress.com/2019/04/15/reinforcement-learning-without-rewards/"><img src="https://videos.files.wordpress.com/ro0BDWO4/cheetah-pretty_std.original.jpg" alt="cheetah-pretty" width="160" height="120" /></a></div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2019/04/15/reinforcement-learning-without-rewards/"><span class="datestr">at April 15, 2019 01:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1229">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">Randomness and interaction? Entanglement ups the game!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>[05/25/19 Update: Kevin Hartnett has a nice article at Quanta explaining Natarajan &amp; Wright’s result in slightly more layman terms than I’d be able to…see here: <a href="https://www.quantamagazine.org/computer-scientists-expand-the-frontier-of-verifiable-knowledge-20190523/">Computer Scientists Expand the Frontier of Verifiable Knowledge</a>]</em></p>
<p>The study of entanglement through the length of interactive proof systems has been one of the most productive applications of complexity theory to the physical sciences that I know of. Last week Anand Natarajan and John Wright, postdoctoral scholars at Caltech and MIT respectively, added a <a href="https://arxiv.org/abs/1904.05870">major stone</a> to this line of work. Anand &amp; John (hereafter “NW”) establish the following wild claim: it is possible for a classical polynomial-time verifier to decide membership in any language in <em>non-deterministic doubly exponential time</em> by asking questions to two infinitely powerful, but untrusted, provers sharing entanglement. In symbols, NEEXP <img src="https://s0.wp.com/latex.php?latex=%7B%5Csubseteq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\subseteq}" class="latex" title="{\subseteq}" /> MIP<img src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^\star}" class="latex" title="{^\star}" />! (The last symbol is for emphasis — no, we don’t have an MIP<img src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^\star}" class="latex" title="{^\star}" />! class — yet.)</p>
<p>What is amazing about this result is the formidable gap between the complexity of the verifier and the complexity of the language being verified. We know since the 90s that the use of interaction and randomness can greatly expand the power of polynomial-time verifiers, from NP to <a href="https://dl.acm.org/citation.cfm?id=146609">PSPACE</a> (with a single prover) and <a href="https://dl.acm.org/citation.cfm?id=2794945">NEXP</a> (with two provers). As a result of the work of Natarajan and Wright, we now know that yet an additional ingredient, the use of <em>entanglement</em> between the provers, can be leveraged by the verifier — the same verifier as in the previous results, a classical randomized polynomial-time machine — to obtain an exponential increase in its verification power. Randomness and interaction brought us one exponential; entanglement gives us another.</p>
<p>To gain intuition for the result consider first the structure of a classical two-prover one-round interactive proof system for non-deterministic doubly exponential time, with exponential-time verifier. Cutting some corners, such a protocol can be obtained by “scaling up” a standard two-prover protocol for non-deterministic singly exponential time. In the protocol, the verifier would sample a pair of exponential-length questions <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y)}" class="latex" title="{(X,Y)}" />, send <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> to each prover, receive answers <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, and perform an exponential-time computation that verifies some predicate about <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%2CA%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y,A,B)}" class="latex" title="{(X,Y,A,B)}" />.</p>
<p>How can entanglement help design an <em>exponentially more efficient</em> protocol? At first it may seem like a polynomial-time verifier has no way to even get started: if it can only communicate polynomial-length messages with the provers, how can it leverage their power? And indeed, if the provers are classical, it can’t: it is known that even with a polynomial number of provers, and polynomially many rounds of interaction, a polynomial-time verifier cannot decide any language beyond NEXP.</p>
<p>But the provers in the NW protocol are not classical. They can share entanglement. How can the verifier exploit this to its advantage? The key property that is needed is know as the <em>rigidity</em> of entanglement. In words, rigidity is the idea that by verifying the presence of certain statistical correlations between the provers’ questions and answers the verifier can determine precisely (up to a local change of basis) the quantum state and measurements that the provers must have been using to generate their answers. The most famous example of rigidity is the <em>CHSH game</em>: as already shown by <a href="http://www.numdam.org/item/AIHPA_1988__49_2_215_0/">Werner and Summers</a> in 1982, the CHSH game can only be optimally, or even near-optimally, won by measuring a maximally entangled state using two mutually unbiased bases for each player. No other state or measurements will do, unless they trivially imply an EPR pair and mutually unbiased bases (such as a state that is the tensor product of an EPR pair with an additional entangled state).</p>
<p>Rigidity gives the verifier control over the provers’ use of their entanglement. The simplest use of this is for the verifier to force the provers to share a certain number <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> of EPR pairs and measure them to obtain identical uniformly distributed <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />-bit strings. Such a test for <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> EPR pairs can be constructed from <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> CHSH games. In a <a href="https://arxiv.org/abs/1801.03821">paper</a> with Natarajan we give a more efficient test that only requires questions and answers of length that is poly-logarithmic in <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />. Interestingly, the test is built on classical machinery — the low-degree test — that plays a central role in the analysis of some classical multi-prover proof systems for NEXP.</p>
<p>At this point we have made an inch of progress: it is possible for a polynomial-time (in <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D%5Clog+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=\log N}" class="latex" title="{n=\log N}" />) verifier to “command” two quantum provers sharing entanglement to share <img src="https://s0.wp.com/latex.php?latex=%7BN%3D2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N=2^n}" class="latex" title="{N=2^n}" /> EPR pairs, and measure them in identical bases to obtain identical uniformly random <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />-bit strings. What is this useful for? Not much — yet. But here comes the main insight in NW: suppose we could similarly force the provers to generate, not identical uniformly random strings, but a pair of <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" />-bit strings <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y)}" class="latex" title="{(X,Y)}" /> that is distributed as a pair of questions from the verifier in the aforementioned interactive proof system for NEEXP with exponential-time (in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />) verifier. Then we could use a polynomial-time (in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />) verifier to “command” the provers to generate their exponentially-long questions <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y)}" class="latex" title="{(X,Y)}" /> by themselves. The provers would then compute answers <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(A,B)}" class="latex" title="{(A,B)}" /> as in the NEEXP protocol. Finally, they would prove to the verifier, using a polynomial interaction, that <img src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(A,B)}" class="latex" title="{(A,B)}" /> is a valid pair of answers to the pair of questions <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y)}" class="latex" title="{(X,Y)}" /> — indeed, the latter verification is an NEXP problem, hence can be verified using a protocol with polynomial-time verifier.</p>
<p>Sounds crazy? Yes. But they did it! Of course there are many issues with the brief summary above — for example, how does the verifier even know the questions <img src="https://s0.wp.com/latex.php?latex=%7BX%2CY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X,Y}" class="latex" title="{X,Y}" /> sampled by the provers? The answer is that it doesn’t need to know the entire question; only that it was sampled correctly, and that the quadruple <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%2CA%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y,A,B)}" class="latex" title="{(X,Y,A,B)}" /> satisfies the verification predicate of the exponential-time verifier. This can be verified using a polynomial-time interactive proof.</p>
<p>Diving in, the most interesting insight in the NW construction is what they call “introspection”. What makes multi-prover proof systems powerful is the ability for the verifier to send correlated questions to the provers, in a way such that each prover has only partial information about the other’s question — informally, the verifier plays a variant of prisonner’s dilemma with the provers. In particular, any interesting distribution <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,Y)}" class="latex" title="{(X,Y)}" /> will have the property that <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> are not fully correlated. For a concrete example think of the “planes-vs-lines” distribution, where <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> is a uniformly random plane and <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> a uniformly random line in <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. The aforementioned test for <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> EPR pairs can be used to force both provers to sample the same uniformly random plane <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />. But how does the verifier ensure that one of the provers “forgets” parts of the plane, to only remember a uniformly random line <img src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Y}" class="latex" title="{Y}" /> that is contained in it? NW’s insight is that the information present in a quantum state — such as the prover’s half-EPR pairs — can be “erased” by commanding the prover to perform a measurement in the <em>wrong</em> basis — a basis that is mutually unbiased with the basis used by the other prover to obtain its share of the query. Building on this idea, NW develop a battery of delicate tests that provide the verifier the ability to control precisely what information gets distributed to each prover. This allows a polynomial-time verifier to perfectly simulate the local environment that the exponential-time verifier would have created for the provers in a protocol for NEEXP, thus simulating the latter protocol with exponentially less resources.</p>
<p>One of the aspects of the NW result I like best is that they showed how the “history state barrier” could be overcome. Previous works attempting to establish strong lower bounds on the class MIP<img src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^\star}" class="latex" title="{^\star}" />, such as the <a href="https://arxiv.org/abs/1805.12166">paper</a> by Yuen et al., relies on a compression technique that requires the provers to share a history state of the computation performed by a larger protocol. Unfortunately, history states are very non-robust, and as a result such works only succeeded in developing protocols with vanishing completeness-soundness gap. NW entirely bypass the use of history states, and this allows them to maintain a constant gap.</p>
<p>Seven years ago Tsuyoshi Ito and I showed that <a href="https://arxiv.org/abs/1207.0550">MIP<img src="https://s0.wp.com/latex.php?latex=%7B%7D%5E%5Cstar&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{}^\star" class="latex" title="{}^\star" /> contains NEXP</a>. At the time, we thought this may be the end of the story — although it seemed challenging, surely someone would eventually prove a matching upper bound. Natarajan and Wright have defeated this expectation by showing that MIP<img src="https://s0.wp.com/latex.php?latex=%7B%5E%5Cstar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^\star}" class="latex" title="{^\star}" /> contains NEEXP. What next? NEEEXP? The halting problem? I hope to make this the topic of a future post.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/"><span class="datestr">at April 14, 2019 04:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=78">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2018/09/19/the-dynamics-of-lagrange-and-hamilton/">The dynamics of Lagrange and Hamilton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In 1788, Lagrange presented a set of equations of motion that, unlike Newtonian mechanics, are independent of the choice of coordinates of the physical system, and ultimately led to the formulation of <a href="https://en.wikipedia.org/wiki/Einstein%E2%80%93Hilbert_action">general relativity</a>. Hamilton came up with a different set of equations of motion in 1833 that arguably led to the development of <a href="https://en.wikipedia.org/wiki/Hamiltonian_(quantum_mechanics)">quantum mechanics</a>. Remarkably, in classical mechanics, these sets of equations turn out to be <em>equivalent</em> via a beautiful duality due to <strong>Legendre</strong>.</p>
<p><img src="https://nisheethvishnoi.files.wordpress.com/2018/09/aduc_057_legendre_l-_1756-1797.jpg?w=1008" alt="AduC_057_Legendre_(L.,_1756-1797)" class="alignnone size-full wp-image-82" /></p>
<p><span style="color: #999999;"><em>A portrait of Legendre by H. Rousseau, E. Thomas, Augustin Challamel, and Desire Lacroix via Wikimedia Commons</em></span></p>
<p>Lagrangian and Hamiltonian dynamics have inspired several promising optimization and sampling algorithms such as <a href="https://arxiv.org/pdf/1603.04245.pdf">first-order methods in optimization</a>,  <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">Hamiltonian Monte Carlo</a> (see also this <a href="https://arxiv.org/abs/1802.08898">paper</a> that will appear in NIPS 2018). Legendre duality also appears in convex optimization as <a href="https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem">Fenchel</a> <a href="https://nisheethvishnoi.wordpress.com/convex-optimization/">duality</a>.  <a href="https://nisheethvishnoi.files.wordpress.com/2018/09/lagrangehamiltonian.pdf" title="LagrangeHamiltonian">This</a> note, written primarily for optimization folks, introduces Lagrangian dynamics, Hamiltonian dynamics, and proves the duality that connects them.</p>
<p>I hope that these fundamental ideas inspire you as well to think about optimization from a physics perspective!</p>
<p> </p>
<p> </p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2018/09/19/the-dynamics-of-lagrange-and-hamilton/"><span class="datestr">at September 19, 2018 10:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=63">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2018/09/16/fair-elections/">Fair Elections</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: center;"><em><a href="https://theory.epfl.ch/celis/HOME.html">Elisa Celis</a> and Nisheeth Vishnoi</em></p>
<p>Elections are the nervous system of a democracy and, hence, issues related to their transparency and fairness are central to healthy societies. However, the number of <a href="https://en.wikipedia.org/wiki/List_of_controversial_elections">controversial elections</a> has exploded since 2000, and the more recent ones involve the use of the Internet and algorithms — a good excuse for computer scientists to get involved in elections to save democracy!</p>
<p>Last year, the two of us and Lingxao Huang revisited the question of <em>fairness</em> in <em>multi-winner</em> elections<em>. </em>A multi-winner election is one where the objective is to select a committee from a set of candidates based on voter preferences. However, election systems for multi-winner elections have been shown to be <a href="https://www.representwomen.org/voting_rules_pose_barrier_for_women">biased against minorities</a> by giving preference to committees that under-represent their already-small numbers. When the candidates have certain sensitive attributes (such as gender), and different types (e.g., male, female and non-binary) we may instead wish to impose additional requirement so that the winning committee is <em>fair</em>. We considered the problem of finding the committee with the maximum number of votes, subject to such additional fairness requirements.</p>
<p style="text-align: center;"><em>But what does it mean to be fair?</em></p>
<p>Is a committee fair when types are equally represented? Or <a href="https://en.wikipedia.org/wiki/Proportional_representation">proportionally represented</a>? These questions have received more than a century’s worth of attention in social choice theory and are deeply entangled with culture and politics. In short, fairness means different things in different contexts and rather than addressing a specific instance of this question, we asked ourselves — <em>can we provide a framework in which a user can specify fairness constraints according to their needs and design algorithms that can compute the winning committee accordingly?</em></p>
<p>Because such a framework must handle general classes of constraints, the algorithmic task became more challenging than the usual unconstrained case. If you are interested in understanding our algorithmic and complexity results, they appear in our recent <a href="https://arxiv.org/pdf/1710.10057.pdf">paper</a> that was presented at IJCAI-ECAI this year.</p>
<p>Clearly, however, there are real merits to providing such generality — earlier this year, our paper caught the eye of members of a <a href="https://appelcitoyen.ch/">people’s movement</a> from the <a href="https://en.wikipedia.org/wiki/Canton_of_Valais">Valais</a> canton in Switzerland, who were in the process of rethinking elections in their region. They were happy to know that we had solved the exact problem they needed for a primary election and asked us if we would be willing to help them conduct their elections. We were, of course, thrilled!</p>
<p>After an intense collaboration for more than four months, 8 elections that used our framework concluded on September 9, 2018. For us, there were several eye-opening aspects of taking our algorithmic framework from theory to practice. Perhaps the most interesting one was coming up with an answer to:</p>
<p style="text-align: center;"><em>Who decides what is fair?</em></p>
<p>In this case, it was the voters! Before voting on the committee, a vote to determine the fairness constraints was conducted in each of the eight districts. This vote was in mid-June and also checked if the voters agree to use our algorithmic framework. Since transparency was the key motivation, prior to this vote,  we were invited to a press conference to educate the people about the whole process that resulted in coverage in newspapers <a href="https://www.letemps.ch/suisse/listes-ideales-constituante-valaisanne?itm_source=homepage&amp;itm_medium=position-12">Le Temps</a>,  <a href="https://www.lenouvelliste.ch/articles/valais/canton/appel-citoyen-lance-sa-primaire-digitale-en-vue-de-la-constituante-les-passionnes-de-democratie-ont-jusqu-a-jeudi-pour-s-inscrire-763669">Le Nouvelliste</a>, TV channel <a href="https://www.google.com/url?q=http://canal9.ch/appel-citoyen-va-choisir-ses-candidats-pour-la-lassemblee-constituante-en-deux-etapes-et-par-internet/&amp;source=gmail&amp;ust=1537194868256000&amp;usg=AFQjCNGQkC-CmcI2n0iBfAAzxP4GWbiBzQ">Canal 9</a>, and radio channels <a href="http://www.rhonefm.ch/fr/podcasts/journal-du-soir-une-election-primaire-digitale--c-est-l-aventure-dans-laquelle-se-lance-le-mouvement-apolitique-appel-citoyen-en-premiere-mondiale-1107827">Rhone FM</a> and  <a href="http://radiochablais.ch//podcast/mp3/info_12h_13062018.mp3">Radio Chablais</a>.</p>
<p><img src="https://nisheethvishnoi.files.wordpress.com/2018/09/20180612_142435.jpg?w=1008" alt="20180612_142435" class="alignnone size-full wp-image-65" /></p>
<p><em>An image from the press conference in Sion on June 12, 2018. Elisa Celis (center), accompanied by members of Appel Citoyen.</em></p>
<p>In the end, the voters decided to place constraints on the gender balance, age demographics, and regions. While all districts approved these same three attributes, the specific constraints varied by district according to their demographics and pre-defined regions. This was a truly open and democratic way to determine the criteria by which the committee was to be balanced. The constraints not only ensured that the outcome would be fair (as defined by the voters) but also encouraged many more women to run for election than the party originally anticipated — in fact, in several districts there were more women than men contesting the election!</p>
<p><img src="https://nisheethvishnoi.files.wordpress.com/2018/09/20180909_153659.jpg?w=1008" alt="20180909_153659" class="alignnone size-full wp-image-64" /></p>
<p style="text-align: left;"><em>Computing the winning committees in Sion on September 9, 2018. From L to R: Lingxiao Huang, Vijay Keswani, Elisa Celis, Florian Evequoz, Bernadette Morand-Aymon.</em></p>
<p>Everything about these elections, from votes, candidates, constraints, and code, to the output, is open for all to <a href="https://appelcitoyen.ch/blog/on-ouvre-les-urnes-donnees-brutes-de-la-primaire/">verify</a>. We have also made available a <a href="http://valais-elections.herokuapp.com/">demo</a> with pre-loaded constraints, candidate information, and votes for anyone to test and compute the results. To know more about the use of our algorithm in these elections, take a look at <a href="https://www.youtube.com/watch?v=X6M1fpcEBQE">this video</a>. A few links to the coverage of these elections can be found here: <a href="http://canal9.ch/constituante-appel-citoyen-a-designe-ses-96-candidats-une-primaire-digitale-realisee-grace-a-une-methode-concue-par-lepfl/">Canal 9</a>,  <a href="https://www.letemps.ch/suisse/valais-appel-citoyen-lance-primaire-constituante">Le Temps</a>, <a href="https://www.lenouvelliste.ch/dossiers/tout-savoir-sur-la-constituante/articles/constituante-la-primaire-numerique-d-appel-citoyen-a-rendu-son-verdict-782906">Le Nouvelliste</a>.</p>
<p>We feel fortunate that we were given this unique opportunity to take tools from computer science to the heart of a democratic process. It was an unparalleled learning experience for all of us and something that we increasingly look forward to in the future.</p>
<p><img src="https://nisheethvishnoi.files.wordpress.com/2018/09/constraints.gif?w=1008" alt="constraints" class="alignnone size-full wp-image-70" /></p>
<p><img src="https://nisheethvishnoi.files.wordpress.com/2018/09/candidates.gif?w=1008" alt="candidates" class="alignnone size-full wp-image-69" /></p>
<p>You can also use our <a href="https://theory.epfl.ch/bias/elections/">demo</a> to design your own elections and play around with our algorithms. This demo was developed with the help of Abhibhav Garg and Vijay Keswani and has limited functionality. If you are interested in using our full framework and/or working with us,  please feel free to <a href="https://theory.epfl.ch/bias/contact.html">contact us</a>.</p>
<p> </p>
<p> </p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2018/09/16/fair-elections/"><span class="datestr">at September 16, 2018 08:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1226">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2018/08/06/the-cryptographic-leash/">The cryptographic leash</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This post is meant as a companion to an introductory post I wrote on the <a href="https://quantumfrontiers.com/">blog</a> of Caltech’s IQIM (<a href="http://iqim.caltech.edu/">Institute for Quantum Information and Matter</a>), of which I am a member. The post describes a “summer cluster” on quantum computation that I co-organized with Andrew Childs, Ignacio Cirac, and Umesh Vazirani at the Simons Institute in Berkeley over the past couple months. The IQIM post also describes one of the highlights of the workshop we organized as part of this program: the recent result by Mahadev on <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html">classical verification of quantum computation</a>. The present post is a continuation of <a href="https://quantumfrontiers.com/2018/08/05/the-quantum-wave-in-computing/">that one</a>, so that I would encourage you to read it first. In this post my goal is to give additional detail on Mahadev’s result. For the real thing you should of course read the <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html">paper</a> (you may want to start by watching the author’s <a href="https://simons.berkeley.edu/talks/urmila-mahadev-06-15-18">beautiful talk</a> at our workshop). What follows is my attempt at an introduction, in great part written for the sake of clarifying my own understanding. I am indebted to Urmila for multiple conversations in which she indefatigably answered my questions and cleared my confusions — of course, any remaining inaccuracies in this post are entirely mine.</p>
<p><b>The result </b></p>
<p><a name="the-result"></a></p>
<p>Let’s start by recalling Mahadev’s result. She shows that from any quantum computation, specified by a polynomial-size quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, it is possible to efficiently compute a <em>classical-verifier quantum-prover protocol</em>, i.e.~a prescription for the actions of a classical probabilistic polynomial-time verifier interacting with a quantum prover, that has the following properties. For simplicity, assume that <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> produces a deterministic outcome <img src="https://s0.wp.com/latex.php?latex=%7Bo%28C%29%5Cin%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{o(C)\in\{0,1\}}" class="latex" title="{o(C)\in\{0,1\}}" /> when it is executed on qubits initialized in the state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+0+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| 0 \rangle}" class="latex" title="{| 0 \rangle}" /> (any input can be hard-coded in the circuit). At the end of the protocol, the verifier always makes one of three possible decisions: “reject”; “accept, 0”; “accept, 1”. The <em>completeness</em> property states that for any circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> there is a “honest” behavior for the prover that can be implemented by a polynomial-time quantum device and that will result in the verifier making the decision “accept, <img src="https://s0.wp.com/latex.php?latex=%7Bo%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{o(C)}" class="latex" title="{o(C)}" />”, where <img src="https://s0.wp.com/latex.php?latex=%7Bo%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{o(C)}" class="latex" title="{o(C)}" /> is the correct outcome, with probability <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. The <em>soundness</em> property states that for any behavior of the quantum prover in the protocol, either the probability that the verifier returns the outcome “accept, <img src="https://s0.wp.com/latex.php?latex=%7B1-o%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-o(C)}" class="latex" title="{1-o(C)}" />” is negligibly small, or the quantum prover has the ability to break a post-quantum cryptographic scheme with non-negligible advantage. Specifically, the proof of the soundness property demonstrates that a prover that manages to mislead the verifier into making the wrong decision (for any circuit) can be turned into an efficient attack on the learning with errors (LWE) problem (with superpolynomial noise ratio).</p>
<p>The fact that the protocol is only sound against computationally bounded provers sets it apart from previous approaches, which increased the power of the verifier by allowing her to dispose of a miniature quantum computer, but established soundness against computationally unbounded provers. The magic of Mahadev’s result is that she manages to leverage this sole assumption, computational boundnedness of the prover, to tie a very tight “leash” around its neck, by purely classical means. My use of the word “leash” is <a href="https://arxiv.org/abs/1209.0448">not innocent</a>: informally, it seems that the cryptographic assumption allows Mahadev to achieve the kind of feats that were previously known, for classical verifiers, in the model where there are two quantum provers sharing entanglement. I am not sure how far the analogy extends, and would like to explore it further; this has already started with a collaboration with Brakerski, Christiano, Mahadev and Vazirani that led to <a href="https://arxiv.org/abs/1804.00640">a single-prover protocol for certifiable randomness expansion</a>. Nevertheless, the main open question left open by Mahadev’s work remains whether the computational assumption is even necessary: could a similar result hold, where the honest prover can perform the required actions in quantum polynomial-time, but the protocol remains sound against arbitrarily powerful provers? (Experts will have recognized that the existence of a protocol where the honest prover is as powerful as PSPACE follows from the classical results that BQP is in PSPACE, and that PSPACE=IP. Unfortunately, we currently don’t expect even a supercharged AWS cloud to be able to implement PSPACE-complete computations.)</p>
<p><b> Encoding computation in ground states </b></p>
<p><a name="encoding-computation-in-ground-states"></a></p>
<p>Let’s get to business: how does this work? Fix a quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> that the verifier is interested in. Assume the description of <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> known to both the verifier and the prover. As earlier, assume further that when <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> is executed on a state initialized to <img src="https://s0.wp.com/latex.php?latex=%7B%7C+0+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| 0 \rangle}" class="latex" title="{| 0 \rangle}" /> a measurement of the output qubit of the circuit returns either the outcome <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> or the outcome <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, deterministically. The verifier wishes to determine which case holds.</p>
<p>The first step that the verifier performs is a classical polynomial-time reduction from this <em>circuit output decision problem</em> to the following <em>Hamiltonian energy decision problem</em>. In the Hamiltonian energy decision problem the input is the description of a pair of classical polynomial-time randomized circuits. The first circuit, <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, takes as input a random string <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />, and returns a string <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%5Cin%5C%7BX%2CZ%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta\in\{X,Z\}^n}" class="latex" title="{\theta\in\{X,Z\}^n}" />. The second circuit, <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" />, takes as input a string <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%5Cin%5C%7BX%2CZ%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta\in\{X,Z\}^n}" class="latex" title="{\theta\in\{X,Z\}^n}" /> of the kind returned by the first circuit, as well as an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit string <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\in\{0,1\}^n}" class="latex" title="{a\in\{0,1\}^n}" />, and returns a “decision bit” <img src="https://s0.wp.com/latex.php?latex=%7Bb%5Cin+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b\in \{0,1\}}" class="latex" title="{b\in \{0,1\}}" />. The goal of the verifier is to distinguish between the following two cases. Either there exists an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit state <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" /> such that, when a string <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta}" class="latex" title="{\theta}" /> is sampled according to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> (choosing a uniformly random <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> as input), the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> qubits of <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" /> are measured in the bases specified by <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta}" class="latex" title="{\theta}" /> (i.e.~the <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />-th qubit is measured in the computational basis in case <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta_i%3DZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta_i=Z}" class="latex" title="{\theta_i=Z}" />, and in the Hadamard basis in case <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta_i%3DX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\theta_i=X}" class="latex" title="{\theta_i=X}" />), the resulting <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit outcome <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%7BV%28%5Ctheta%2Ca%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V(\theta,a)=1}" class="latex" title="{V(\theta,a)=1}" /> with probability at least <img src="https://s0.wp.com/latex.php?latex=%7B3%2F4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3/4}" class="latex" title="{3/4}" />. Or, for any state <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" />, the same procedure results in <img src="https://s0.wp.com/latex.php?latex=%7BV%28%5Ctheta%2Ca%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V(\theta,a)=0}" class="latex" title="{V(\theta,a)=0}" /> with probability at most <img src="https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2/3}" class="latex" title="{2/3}" />.</p>
<p>I called this problem the Hamiltonian energy decision problem because the circuits <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> implicitly specify a Hamiltonian, whose minimal energy the verifier aims to approximate. Note that the Hamiltonian is not required to be local, and furthermore it may involve an average of exponentially many terms (as many as there are random strings <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />). The problem is still in QMA, because the verifier is efficient. It is not hard to show that the problem is QMA-hard. What the formulation above buys us, compared to using the usual QMA-complete formulation of the local Hamiltonian problem, is the constant energy gap — which comes at the cost of exponentially many terms and loss of locality. (Open question: I would like to know if it is possible to achieve a constant gap with only one of these caveats: local with exponentially many terms, or nonlocal with polynomially terms.) Of course here we only care that the problem is BQP-hard, and that the witness can be computed by a BQP prover; this is indeed the case. We also don’t really care that there is a constant gap – the soundness of the final protocol could be amplified by other means – but it is convenient that we are able to assume it.</p>
<p>The reduction that achieves this is a combination of Kitaev’s history state construction with some gadgetry from perturbation theory and an amplification trick. The first step reduces the verification that <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> returns outcome <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> (resp. <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />) on input <img src="https://s0.wp.com/latex.php?latex=%7B%7C+0+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| 0 \rangle}" class="latex" title="{| 0 \rangle}" /> to the verification that a local Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> (computed from <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />) has ground state energy exponentially close to <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> (resp. at least some positive inverse polynomial). The second step consists in applying perturbation theory to reduce to the case where <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is a weighted linear combination of terms of the form <img src="https://s0.wp.com/latex.php?latex=%7BX_iX_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_iX_j}" class="latex" title="{X_iX_j}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ_iZ_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_iZ_j}" class="latex" title="{Z_iZ_j}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X_i}" class="latex" title="{X_i}" />, <img src="https://s0.wp.com/latex.php?latex=%7BZ_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_j}" class="latex" title="{Z_j}" /> are the Pauli <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> operators on the <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />-th and <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" />-th qubit respectively. The final step is an amplification trick, that produces a nonlocal Hamiltonian whose each term is a tensor product of single-qubit <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> observables and has ground state energy either less than <img src="https://s0.wp.com/latex.php?latex=%7B1%2F4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/4}" class="latex" title="{1/4}" /> or larger than <img src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/3}" class="latex" title="{1/3}" /> (when the Hamiltonian is scaled to be non-negative with norm at most <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />).</p>
<p>These steps are fairly standard. The first two are combined in a <a href="https://arxiv.org/abs/1603.06046">paper</a> by Fitzsimons and Morimae to obtain a protocol for “post-hoc” verification of quantum computation: the prover prepares the ground state of an <img src="https://s0.wp.com/latex.php?latex=%7BXZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{XZ}" class="latex" title="{XZ}" /> local Hamiltonian whose energy encodes the outcome of the computation, and sends it to the verifier one qubit at a time; the verifier only needs to perform single-qubit <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> measurements to estimate the energy. The last step, amplification, is described in a <a href="https://arxiv.org/abs/1610.03574">paper</a> with Natarajan, where we use it to obtain a multi-prover interactive proof system for QMA.</p>
<p>For the remainder of this post, I take the reduction for granted and focus on the core of Mahadev’s result, a verification protocol for the following problem: given a Hamiltonian of the form described in the previous paragraph, decide whether the ground state energy of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is smaller than <img src="https://s0.wp.com/latex.php?latex=%7B1%2F4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/4}" class="latex" title="{1/4}" />, or larger than <img src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/3}" class="latex" title="{1/3}" />.</p>
<p><b> Stitching distributions into a qubit </b></p>
<p><a name="stitching-distributions-into-a-qubit"></a></p>
<p>In fact, for the sake of presentation I’ll make one further drastic simplification, which is that the verifier’s goal has been reduced to verifying the existence of a <em>single-qubit</em> state <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" />, whose existence is claimed by the prover. Specifically, suppose that the prover claims that it has the ability to prepare a state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+%5Cpsi+%7CX%7C+%5Cpsi+%5Crangle+%3D+E_X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle \psi |X| \psi \rangle = E_X}" class="latex" title="{\langle \psi |X| \psi \rangle = E_X}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+%5Cpsi+%7CZ%7C+%5Cpsi+%5Crangle%3DE_Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle \psi |Z| \psi \rangle=E_Z}" class="latex" title="{\langle \psi |Z| \psi \rangle=E_Z}" />, for real parameters <img src="https://s0.wp.com/latex.php?latex=%7BE_X%2CE_Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_X,E_Z}" class="latex" title="{E_X,E_Z}" />. In other words, that the Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%7BH+%3D+%5Cfrac%7B1%7D%7B2%7D%28X%2BZ%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H = \frac{1}{2}(X+Z)}" class="latex" title="{H = \frac{1}{2}(X+Z)}" /> has minimal energy at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%28E_X%2BE_Z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}(E_X+E_Z)}" class="latex" title="{\frac{1}{2}(E_X+E_Z)}" />. How can one verify this claim? (Of course we could do it analytically\ldots{}but that approach would break apart as soon as expectation values on larger sets of qubits are considered.)</p>
<p>We could ask the prover to measure in the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> basis, or the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> basis, repeatedly on identical copies of <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" />, and report the outcomes. But how do we know that all these measurements were performed on the same state, and that the prover didn’t choose e.g. <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%3D%7C+1+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle=| 1 \rangle}" class="latex" title="{| \psi \rangle=| 1 \rangle}" /> to report the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-basis outcomes, and <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%3D%7C+-+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle=| - \rangle}" class="latex" title="{| \psi \rangle=| - \rangle}" /> to report the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />-basis outcomes? We need to find a way to prevent the prover from measuring a different state depending on the basis it is asked for — as well as to ensure the measurement is performed in the right basis.</p>
<p><b> Committing to a qubit </b></p>
<p><a name="committing-to-a-qubit"></a></p>
<p>The key idea in Mahadev’s protocol is to use cryptographic techniques to force the prover to “commit” to the state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" /> in a way that, once the commitment has been performed, the prover no longer has the liberty to “decide” which measurement it performs on the commited qubit (unless it breaks the cryptographic assumption).</p>
<p>I described the commitment scheme in the companion post <a href="https://quantumfrontiers.com/2018/08/05/the-quantum-wave-in-computing/">here</a>. For convenience, let me quote from that post. Recall that the scheme is based on a pair of trapdoor permutations <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%2Cf_1%3A%5C%7B0%2C1%5C%7D%5En+%5Crightarrow+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0,f_1:\{0,1\}^n \rightarrow \{0,1\}^n}" class="latex" title="{f_0,f_1:\{0,1\}^n \rightarrow \{0,1\}^n}" /> that is <em>claw-free</em>. Informally, this means that it is hard to produce any pair <img src="https://s0.wp.com/latex.php?latex=%7B%28x_0%2Cx_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(x_0,x_1)}" class="latex" title="{(x_0,x_1)}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%28x_0%29%3Df_1%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0(x_0)=f_1(x_1)}" class="latex" title="{f_0(x_0)=f_1(x_1)}" />.</p>
<p>The commitment phase of the protocol works as follows. Starting from a state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%3D%5Calpha%7C+0+%5Crangle%2B%5Cbeta%7C+1+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle=\alpha| 0 \rangle+\beta| 1 \rangle}" class="latex" title="{| \psi \rangle=\alpha| 0 \rangle+\beta| 1 \rangle}" /> of its choice, the prover is supposed to perform the following steps. First, the prover creates a uniform superposition over the common domain of <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1}" class="latex" title="{f_1}" />. Then it evaluates either function, <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1}" class="latex" title="{f_1}" />, in an additional register, by controlling on the qubit of <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" />. Finally, the prover measures the register that contains the image of <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1}" class="latex" title="{f_1}" />. This achieves the following sequence of transformations:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%5Calpha%7C+0+%5Crangle%2B%5Cbeta%7C+1+%5Crangle+%26%5Cmapsto%26+%28%5Calpha%7C+0+%5Crangle+%2B+%5Cbeta%7C+1+%5Crangle%29+%5Cotimes+%5CBig%282%5E%7B-n%2F2%7D+%5Csum_%7Bx%5Cin%5C%7B0%2C1%5C%7D%5En%7D+%7C+x+%5Crangle%5CBig%29+%5C%5C+%26%5Cmapsto+%26+2%5E%7B-n%2F2%7D+%5Csum_x+%5Calpha+%7C+0+%5Crangle%7C+x+%5Crangle%7C+f_0%28x%29+%5Crangle+%2B+%5Cbeta+%7C+1+%5Crangle%7C+f_1%28x%29+%5Crangle%5C%5C+%26%5Cmapsto+%26+%5Cbig%28%5Calpha%7C+0+%5Crangle%7C+x_0+%5Crangle%2B%5Cbeta%7C+1+%5Crangle%7C+x_1+%5Crangle%5Cbig%29%7C+y+%5Crangle%5C%3B%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} \alpha| 0 \rangle+\beta| 1 \rangle &amp;\mapsto&amp; (\alpha| 0 \rangle + \beta| 1 \rangle) \otimes \Big(2^{-n/2} \sum_{x\in\{0,1\}^n} | x \rangle\Big) \\ &amp;\mapsto &amp; 2^{-n/2} \sum_x \alpha | 0 \rangle| x \rangle| f_0(x) \rangle + \beta | 1 \rangle| f_1(x) \rangle\\ &amp;\mapsto &amp; \big(\alpha| 0 \rangle| x_0 \rangle+\beta| 1 \rangle| x_1 \rangle\big)| y \rangle\;, \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} \alpha| 0 \rangle+\beta| 1 \rangle &amp;\mapsto&amp; (\alpha| 0 \rangle + \beta| 1 \rangle) \otimes \Big(2^{-n/2} \sum_{x\in\{0,1\}^n} | x \rangle\Big) \\ &amp;\mapsto &amp; 2^{-n/2} \sum_x \alpha | 0 \rangle| x \rangle| f_0(x) \rangle + \beta | 1 \rangle| f_1(x) \rangle\\ &amp;\mapsto &amp; \big(\alpha| 0 \rangle| x_0 \rangle+\beta| 1 \rangle| x_1 \rangle\big)| y \rangle\;, \end{array} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7By%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y\in\{0,1\}^n}" class="latex" title="{y\in\{0,1\}^n}" /> is the measured image. The string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> is the prover’s <em>commitment string</em>, that it reports the verifier.</p>
<p>The intuition for this commitment procedure is that it introduces asymmetry between prover and verifier: the prover knows <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> (it had to report it to the verifier) but not <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> (this is the claw-free assumption on the pair <img src="https://s0.wp.com/latex.php?latex=%7B%28f_0%2Cf_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(f_0,f_1)}" class="latex" title="{(f_0,f_1)}" />), which seems to prevent it from recovering the original state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" />, since it does not have the ability to “uncompute” <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" />. In contrast, the verifier can use the trapdoor information to recover both preimages.</p>
<p>In a little more detail, how is this used? Note that at this point, from the verifier’s point of view the only information that has been received is the prover’s commitment string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. In general there are multiple ways a prover could have come up with a value <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />: for example, by selecting an <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in\{0,1\}^n}" class="latex" title="{x\in\{0,1\}^n}" /> and returning <img src="https://s0.wp.com/latex.php?latex=%7By%3Df_0%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y=f_0(x)}" class="latex" title="{y=f_0(x)}" />. Or, by directly selecting an arbitrary string <img src="https://s0.wp.com/latex.php?latex=%7By%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y\in\{0,1\}^n}" class="latex" title="{y\in\{0,1\}^n}" />. At this stage of the protocol, any of these strategies look fine.</p>
<p>Let’s modify the commitment phase by adding a little test. With some probability, the verifier, upon receiving the commitment string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, decides to challenge the prover by asking it to report a valid preimage of <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, either under <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> or under <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1}" class="latex" title="{f_1}" /> (to the prover’s choice). Since both <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_1}" class="latex" title="{f_1}" /> are presumed to be hard to invert, the only way the prover can answer this challenge is if it already “knows” a valid preimage — or at a minimum, if it has a superposition on preimages that it can measure when tested. Thus the fact that the prover is required to succeed in the commitment test, when it is performed, guarantees that after the prover has returned the commitment string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> we may without loss of generality assume that the prover’s state can be written as</p>
<p><a name="eqdef-psi"></a></p>
<p align="center"><a name="eqdef-psi"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%3D%5Ctilde%7B%5Calpha%7D%7C+0%2Cx_0+%5Crangle%7C+%5Cphi_0+%5Crangle%2B%5Ctilde%7B%5Cbeta%7D%7C+1%2Cx_1+%5Crangle%7C+%5Cphi_1+%5Crangle%5C%3B%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle | \tilde{\psi} \rangle=\tilde{\alpha}| 0,x_0 \rangle| \phi_0 \rangle+\tilde{\beta}| 1,x_1 \rangle| \phi_1 \rangle\;, \ \ \ \ \ (1)" class="latex" title="\displaystyle | \tilde{\psi} \rangle=\tilde{\alpha}| 0,x_0 \rangle| \phi_0 \rangle+\tilde{\beta}| 1,x_1 \rangle| \phi_1 \rangle\;, \ \ \ \ \ (1)" /></p>
<p><a name="eqdef-psi"></a></p>
<p>where we have purposefully spelled out the two possible preimages that the prover could return if challenged. Note that aside from the fact that it gives the ability to obtain <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" />, this format does not make any assumption on <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />; in particular the register containing the preimage can be entangled with other private registers of the prover.</p>
<p>We have defined a four-message commitment protocol: the verifier sends the security parameters to prover; the prover sends a commitment string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> back; an optional one-round preimage test is executed. Now is the time to give a first definition for the single qubit to which the prover has “committed” by returning <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. This <em>committed qubit</em> is the state <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> that we ultimately aim to show has the claimed expectation values under <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> measurements.</p>
<p>Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> be the qubit obtained from <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> by erasing <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> (which is possible given knowledge of <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />) and returning the first qubit of the resulting state. (Later we will slightly modify this definition, but it is a good placeholder to get us started.) Note that the verifier does not know the state <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />; in fact, strictly speaking <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> is not present on the prover’s workspace either. The point is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> exists, and this is all we need. Our remaining task is to find a way for the verifier to extract from the prover measurement outcomes that are distributed as would be a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> basis, without the prover having the ability to deviate. If the verifier can do this, for a basis of her choice, she can choose a basis at random, estimate the expectation value, and check the prover’s claim (the values <img src="https://s0.wp.com/latex.php?latex=%7BE_X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_X}" class="latex" title="{E_X}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BE_Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_Z}" class="latex" title="{E_Z}" />).</p>
<p>As already mentioned, the key point that we’ll use in order to achieve this is that at the end of the commitment phase, the verifier has obtained some leverage over the prover: given <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> and the trapdoor information, the verifier can recover both <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" />. In contrast, the prover, while it holds the state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />, is not able to freely operate on it. Without the trapdoor, it can no longer uncompute <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> to recover the initial state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" />, and so it can’t obviously apply, say, the unitary on <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> that would amount to performing a single-qubit rotation on <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cpsi+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \psi \rangle}" class="latex" title="{| \psi \rangle}" />.</p>
<p><b> Measuring in the computational basis </b></p>
<p><a name="measuring-in-the-computational-basis"></a></p>
<p>We need to explain how the verifier extracts measurement outcomes in the X (Hadamard) or Z (computational) basis from the prover. For each basis there is a small sub-protocol. At the end of the sub-protocol the verifier records a single bit, that it considers is the outcome obtained by a measurement of the committed qubit, <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />, in the corresponding basis. We call this bit the verifier’s “decoded bit” for that basis.</p>
<p>The protocol for extracting the outcome of a measurement in the computational basis is straightforward. Recall that by definition the prover’s state after the commitment phase has ended is the state <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> in <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqdef-psi">(1)</a>. Moreover, recall that we made a choice of basis for the provers’ space such that when the prover is challenged for a preimage of <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />, it measures the first <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" /> qubits of <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> in the computational basis and returns the outcome. Now observe that the first bit of this outcome is <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Ctilde%7B%5Calpha%7D%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\tilde{\alpha}|^2}" class="latex" title="{|\tilde{\alpha}|^2}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> with probability <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Ctilde%7B%5Cbeta%7D%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\tilde{\beta}|^2}" class="latex" title="{|\tilde{\beta}|^2}" />. This is exactly the distribution of the outcome of a measurement of the committed qubit <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the computational basis, by definition! Thus to extract a measurement outcome in the computational basis the verifier simply executes the preimage test and records the first bit returned by the prover as the decoded bit.</p>
<p><b> Measuring in the Hadamard basis </b></p>
<p><a name="measuring-in-the-hadamard-basis"></a></p>
<p>Extracting a measurement outcome in the Hadamard basis is more delicate. Recall the form of the prover’s state in <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqdef-psi">(1)</a>. Given our definition of the committed qubit <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />, the natural way to obtain a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the Hadamard basis, starting from <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />, is to first erase the register containing <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" />, and then perform a Hadamard measurement of the first qubit. But even an honest prover cannot accomplish this, as it does not have the trapdoor information that would allow to erase <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> (of course we purposefully set things up this way). What the prover <em>can</em> do, however, is measure all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> qubits of the register containing <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0}" class="latex" title="{x_0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1}" class="latex" title="{x_1}" /> in the Hadamard basis. The result of this measurement is an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit string <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\in\{0,1\}^n}" class="latex" title="{d\in\{0,1\}^n}" />. The corresponding post-measurement state is, up to global phase,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctilde%7B%5Calpha%7D%7C+0+%5Crangle%7C+%5Cphi_0+%5Crangle%2B%28-1%29%5E%7Bd%5Ccdot%28x_0%2B+x_1%29%7D%5Ctilde%7B%5Cbeta%7D%7C+1+%5Crangle%7C+%5Cphi_1+%5Crangle%5C%3B%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \tilde{\alpha}| 0 \rangle| \phi_0 \rangle+(-1)^{d\cdot(x_0+ x_1)}\tilde{\beta}| 1 \rangle| \phi_1 \rangle\;," class="latex" title="\displaystyle \tilde{\alpha}| 0 \rangle| \phi_0 \rangle+(-1)^{d\cdot(x_0+ x_1)}\tilde{\beta}| 1 \rangle| \phi_1 \rangle\;," /></p>
<p>where the addition <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0+x_1}" class="latex" title="{x_0+x_1}" /> is taken bitwise, modulo <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. Observe that this state is almost exactly the committed qubit <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> — except for a phase flip, <img src="https://s0.wp.com/latex.php?latex=%7BZ%5E%7Bd%5Ccdot%28x_0%5Coplus+x_1%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z^{d\cdot(x_0\oplus x_1)}}" class="latex" title="{Z^{d\cdot(x_0\oplus x_1)}}" />, applied on the first qubit. If the prover measures the remaining qubit in the Hadamard basis, the phase flip leads to a bit flip on the outcome <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> of the measurement. So the verifier can ask the prover to report both <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" />; if she recourds the decoded bit <img src="https://s0.wp.com/latex.php?latex=%7Bb%3Dm%5Coplus+d%5Ccdot+%28x_0%5Coplus+x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b=m\oplus d\cdot (x_0\oplus x_1)}" class="latex" title="{b=m\oplus d\cdot (x_0\oplus x_1)}" /> then this bit matches the outcome of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the Hadamard basis.</p>
<p>This completes the description of the measurement sub-protocol for the Hadamard basis. It is clear that a honest prover, performing the actions described above, will induce the verifier into recording the correct outcome. Now of course in general the prover may act in an arbitrary way! It could report any values for <img src="https://s0.wp.com/latex.php?latex=%7B%28m%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(m,d)}" class="latex" title="{(m,d)}" />: the verifier accepts any outcomes on faith. How could this possibly work out? There is magic in Mahadev’s proof.</p>
<p><b> Malicious provers </b></p>
<p><a name="malicious-provers"></a></p>
<p>Let’s assume, as we already have, that the prover is arbitrary but that, if tested in the commitment phase, it succeeds with certainty. According to the discussion around <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqdef-psi">(1)</a> this implies that at the end of the commitment phase the prover holds a state of the form <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />. Moreover, by definition, when asked for a computational basis measurement the prover measures the first <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" /> qubits of <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> in the computational basis and reports the outcome; the verifier records the first bit as its decoded bit.</p>
<p>As we already argued, our earlier definition of the committed qubit <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> ensures that the verifier’s decoded bit for the case of a computational basis measurement matches the outcome of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the computational basis. Unfortunately for the case of a Hadamard basis measurement we are in trouble. Since the prover may in principle report an arbitrary pair <img src="https://s0.wp.com/latex.php?latex=%7B%28m%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(m,d)}" class="latex" title="{(m,d)}" /> there is no chance to argue that this matches (in distribution) the outcome of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> in the Hadamard basis. To find a state that is consistent with the verifier’s decoded bit in both bases we need to change our definition of the committed qubit to take into account the prover’s action in the case it is asked for a Hadamard measurement.</p>
<p>Recall that the main leverage that the verifier has over the prover is that, while the prover does have the possibility of reporting arbitrary outcomes <img src="https://s0.wp.com/latex.php?latex=%7B%28m%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(m,d)}" class="latex" title="{(m,d)}" />, it <em>does not</em> have control over the verifier’s decoding, i.e.~the operation <img src="https://s0.wp.com/latex.php?latex=%7Bb%5Cleftarrow+m%5Coplus+%28d%5Ccdot%28x_0%2B+x_1%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b\leftarrow m\oplus (d\cdot(x_0+ x_1))}" class="latex" title="{b\leftarrow m\oplus (d\cdot(x_0+ x_1))}" />. Let’s work a little bit and spell out the distribution of the verifier’s Hadamard basis decoded bit, <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />. Towards this it is convenient to think of the prover in the following way: the prover first applies an arbitrary unitary “attack” <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> on <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />, then “honestly” measures the first <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" /> qubits in the Hadamard basis, and finally reports the <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" />-bit outcome <img src="https://s0.wp.com/latex.php?latex=%7B%28m%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(m,d)}" class="latex" title="{(m,d)}" />. An arbitrary <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" />-bit-outcome measurement can always be expressed in this way. With this setup we can write the probability that the decoded bit is some value <img src="https://s0.wp.com/latex.php?latex=%7Bb%5Cin%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b\in\{0,1\}}" class="latex" title="{b\in\{0,1\}}" /> as</p>
<p><a name="eqprb-1"></a></p>
<p align="center"><a name="eqprb-1"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%28b%29+%5C%2C%3D%5C%2C+%5Csum_%7Bd%5Cin%5C%7B0%2C1%5C%7D%5En%7D+%5Clangle+%5Ctilde%7B%5Cpsi%7D+%7C+U%5E%5Cdagger+H+%5Cbig%28%28X%5E%7Bd%5Ccdot%28x_0%2Bx_1%29%7D+%7C+b+%5Crangle%5C%21%5Clangle+b+%7CX%5E%7Bd%5Ccdot%28x_0%2Bx_1%29%7D%29+%5Cotimes+%7C+d+%5Crangle%5C%21%5Clangle+d+%7C%5Cbig%29HU%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%5C%3B.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Pr(b) \,=\, \sum_{d\in\{0,1\}^n} \langle \tilde{\psi} | U^\dagger H \big((X^{d\cdot(x_0+x_1)} | b \rangle\!\langle b |X^{d\cdot(x_0+x_1)}) \otimes | d \rangle\!\langle d |\big)HU| \tilde{\psi} \rangle\;. \ \ \ \ \ (2)" class="latex" title="\displaystyle \Pr(b) \,=\, \sum_{d\in\{0,1\}^n} \langle \tilde{\psi} | U^\dagger H \big((X^{d\cdot(x_0+x_1)} | b \rangle\!\langle b |X^{d\cdot(x_0+x_1)}) \otimes | d \rangle\!\langle d |\big)HU| \tilde{\psi} \rangle\;. \ \ \ \ \ (2)" /></p>
<p><a name="eqprb-1"></a></p>
<p>Before we can proceed we should say a little more about the computational assumptions that are placed on the pair of functions <img src="https://s0.wp.com/latex.php?latex=%7B%28f_0%2Cf_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(f_0,f_1)}" class="latex" title="{(f_0,f_1)}" />. Earlier we mentioned this pair of functions should be claw-free, but in fact a little more is needed — though all requirements can ultimately be met by a construction based on the Learning With Errors problem. Rather than state the exact assumptions, I will mention two important consequences. The first is that the pair of functions is “collapsing”, a notion introduced by Unruh in his investigations of <a href="https://link.springer.com/chapter/10.1007/978-3-662-49896-5_18">collision-resistance against quantum attacks</a>. In our context this property implies that it is computationally hard to distinguish between an arbitrary superposition over preimages, as in <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" />, and the “collapsed” state obtained by measuring the control register (the first qubit). The second is that for any <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit string <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> that can be obtained as the outcome of an arbitrary, but computationally efficient, measurement on the collapsed state, the bit <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ccdot%28x_0%2Bx_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\cdot(x_0+x_1)}" class="latex" title="{d\cdot(x_0+x_1)}" /> is computationally indistinguishable from uniform. (This is analogous to a “hardcore bit” property, since <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0+x_1}" class="latex" title="{x_0+x_1}" /> encodes information about both preimages simultaneously, and such information should not be accessible if the pair <img src="https://s0.wp.com/latex.php?latex=%7B%28f_0%2Cf_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(f_0,f_1)}" class="latex" title="{(f_0,f_1)}" /> is claw-free.)</p>
<p>These two assumptions taken together justify the following two modifications to the expression for <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Pr(b)}" class="latex" title="{\Pr(b)}" /> in <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqprb-1">(2)</a>, that will lead to a computationally indistinguishable distribution. First, we can “collapse” the first <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" /> qubits of <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \tilde{\psi} \rangle}" class="latex" title="{| \tilde{\psi} \rangle}" /> by measuring them in the computational basis. Second, we can replace the bit <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ccdot%28x_0%2Bx_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\cdot(x_0+x_1)}" class="latex" title="{d\cdot(x_0+x_1)}" /> by a uniformly random bit <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />. Using that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_d+%7C+d+%5Crangle%5C%21%5Clangle+d+%7C%3D%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_d | d \rangle\!\langle d |=\ensuremath{\mathop{\rm Id}\nolimits}}" class="latex" title="{\sum_d | d \rangle\!\langle d |=\ensuremath{\mathop{\rm Id}\nolimits}}" />, the expression simplifies to</p>
<p><a name="eqprb-3"></a></p>
<p align="center"><a name="eqprb-3"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7B%5CPr%7D%5E%7B%27%7D%28b%29+%5C%2C%3D%5C%2C+%5Cfrac%7B1%7D%7B4%7D%5Csum_%7Br%5Cin%5C%7B0%2C1%5C%7D%7D+%5Clangle+%5Ctilde%7B%5Cpsi%7D+%7C+%28Z%5Er+%5Cotimes+%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%29+%28U%5E%5Cdagger%29+%28Z%5Er+%5Cotimes+%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%29+H+%5Cbig%28+%7C+b+%5Crangle%5C%21%5Clangle+b+%7C%29+%5Cotimes+%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%5Cbig%29+%28Z%5E%7Br%7D+%5Cotimes+%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%29+HU+%28Z%5E%7Br%7D+%5Cotimes+%5Censuremath%7B%5Cmathop%7B%5Crm+Id%7D%5Cnolimits%7D%29+%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%5C%3B%2C+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle {\Pr}^{'}(b) \,=\, \frac{1}{4}\sum_{r\in\{0,1\}} \langle \tilde{\psi} | (Z^r \otimes \ensuremath{\mathop{\rm Id}\nolimits}) (U^\dagger) (Z^r \otimes \ensuremath{\mathop{\rm Id}\nolimits}) H \big( | b \rangle\!\langle b |) \otimes \ensuremath{\mathop{\rm Id}\nolimits}\big) (Z^{r} \otimes \ensuremath{\mathop{\rm Id}\nolimits}) HU (Z^{r} \otimes \ensuremath{\mathop{\rm Id}\nolimits}) | \tilde{\psi} \rangle\;, \ \ \ \ \ (3)" class="latex" title="\displaystyle {\Pr}^{'}(b) \,=\, \frac{1}{4}\sum_{r\in\{0,1\}} \langle \tilde{\psi} | (Z^r \otimes \ensuremath{\mathop{\rm Id}\nolimits}) (U^\dagger) (Z^r \otimes \ensuremath{\mathop{\rm Id}\nolimits}) H \big( | b \rangle\!\langle b |) \otimes \ensuremath{\mathop{\rm Id}\nolimits}\big) (Z^{r} \otimes \ensuremath{\mathop{\rm Id}\nolimits}) HU (Z^{r} \otimes \ensuremath{\mathop{\rm Id}\nolimits}) | \tilde{\psi} \rangle\;, \ \ \ \ \ (3)" /></p>
<p><a name="eqprb-3"></a></p>
<p>where the outermost <img src="https://s0.wp.com/latex.php?latex=%7BZ%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z^r}" class="latex" title="{Z^r}" /> were inserted thanks to the first assumption (the collapsing property), and the innermost <img src="https://s0.wp.com/latex.php?latex=%7BZ%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z^r}" class="latex" title="{Z^r}" /> come from commuting <img src="https://s0.wp.com/latex.php?latex=%7BX%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X^r}" class="latex" title="{X^r}" /> past the Hadamard. I should clarify that obtaining <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqprb-3">(3)</a> formally requires more care. In particular, I made use of computational indistinguishability in an expression that involves a quantity that is hard to compute (the parity <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_0+x_1}" class="latex" title="{x_0+x_1}" />). This is illegal, and to work around the difficulty Mahadev has to introduce some additional ingenious manipulations that I am skipping here.</p>
<p>Note the key effect that the random <img src="https://s0.wp.com/latex.php?latex=%7BZ%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z^r}" class="latex" title="{Z^r}" /> operator has in <a href="https://mycqstate.wordpress.com/Users/Thomas/Dropbox/www/myCQstate/verification2-clean.html#eqprb-3">(3)</a>: it effectively trivializes the action of the prover’s “attack” <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> on the first qubit <em>with respect to the computational basis</em>. Thus the result of this argument is that we have managed to argue that the verifier’s decoded bit <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> associated with the Hadamard basis is <em>computationally indistinguishable</em> from the outcome of a Hadamard measurement on the state</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csigma%27+%3D+%5Cmbox%7B%5Crm+Tr%7D_E%5Cbig%28%28I+%5Cotimes+U_I%29+%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%5Clangle+%5Ctilde%7B%5Cpsi%7D+%7C+%28I%5Cotimes+U_I%29%5E%5Cdagger%2B+%28X%5Cotimes+U_X%29+%7C+%5Ctilde%7B%5Cpsi%7D+%5Crangle%5Clangle+%5Ctilde%7B%5Cpsi%7D+%7C+%28X+%5Cotimes+U_X%29%5E%5Cdagger+%5Cbig%29+%5C%3B%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \sigma' = \mbox{\rm Tr}_E\big((I \otimes U_I) | \tilde{\psi} \rangle\langle \tilde{\psi} | (I\otimes U_I)^\dagger+ (X\otimes U_X) | \tilde{\psi} \rangle\langle \tilde{\psi} | (X \otimes U_X)^\dagger \big) \;," class="latex" title="\displaystyle \sigma' = \mbox{\rm Tr}_E\big((I \otimes U_I) | \tilde{\psi} \rangle\langle \tilde{\psi} | (I\otimes U_I)^\dagger+ (X\otimes U_X) | \tilde{\psi} \rangle\langle \tilde{\psi} | (X \otimes U_X)^\dagger \big) \;," /></p>
<p>where we expanded the first qubit of the unitary <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> as <img src="https://s0.wp.com/latex.php?latex=%7BU+%3D+I%5Cotimes+U_I+%2B+X+%5Cotimes+U_X+%2B+Z%5Cotimes+U_Z+%2B+XZ+%5Cotimes+U_%7BXZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U = I\otimes U_I + X \otimes U_X + Z\otimes U_Z + XZ \otimes U_{XZ}}" class="latex" title="{U = I\otimes U_I + X \otimes U_X + Z\otimes U_Z + XZ \otimes U_{XZ}}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E}" class="latex" title="{E}" /> represents all registers except the first qubit. Note that the second term involves an <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> on the first qubit, which has no effect on a measurement in the Hadamard basis. Thus, <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma'}" class="latex" title="{\sigma'}" /> can be updated to a state <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma''}" class="latex" title="{\sigma''}" /> where we have “erased” the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> operator on the first qubit. Moreover, by definition, a measurement of the first (and only) qubit of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma''}" class="latex" title="{\sigma''}" /> in the computational basis yields an outcome distributed exactly as it would on <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />. In particular, it is consistent with the verifier’s decoded bit in the computational basis measurement protocol.</p>
<p>We are done! The state <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma'}" class="latex" title="{\sigma'}" /> is a well-defined single-qubit state such that the distribution of decoded bits recorded by the verifier for either basis is computationally indistinguishable from the distribution of outcomes of a measurement of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma'}" class="latex" title="{\sigma'}" /> in the same basis. Note that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma'}" class="latex" title="{\sigma'}" /> may not “exist” at any point of the protocol. But this is besides the point: as long as <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma'}" class="latex" title="{\sigma'}" /> is a well-defined quantum state, and the verifier correctly records decoded measurement outcomes, this eventually leads to a valid certificate for the prover’s claim that the XZ Hamiltonian that encodes the computation has low enough energy.</p>
<p>Phew. Catch your breath, read this post again (and please do ask for clarifications as needed), and then move on to the beautiful <a href="https://arxiv.org/abs/1804.01082">paper</a>, whose introduction already has more depth than I could provide here, and whose body fills in all the remaining gaps. (This includes how to deal with states that are more than a single qubit, an issue that my presentation of the single-qubit case may make seem more thorny than it is — in fact, it is possible to express the argument given here in a way that makes it relatively straightforward to extend to multiple qubits, though there are some technical issues, explained in Mahadev’s paper.) And then – use the idea to prove something!</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2018/08/06/the-cryptographic-leash/"><span class="datestr">at August 06, 2018 01:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=55">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2018/06/06/an-exposition-on-geodesic-convexity/">An Exposition on Geodesic Convexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I arrived at the Institute for Advanced Study to attend a workshop on <a href="https://www.math.ias.edu/ocit2018/agenda">Optimization, Complexity and Invariant Theory</a> organized by Avi Wigderson. The workshop has an incredibly diverse agenda, also reflected in the participants.</p>
<p>I will talk about <em>Geodesic Convexity: </em>Sometimes, functions that are non-convex in the Euclidean space turn out to be convex if one introduces a suitable metric (or differential structure) and redefines convexity with respect to the induced straight lines — <em>geodesics</em>. Such a function is called <em>geodesically convex. </em>Unlike standard convexity,<em>  w</em>hen does a function have this property and how to optimize it, is not well-understood. My talk will focus on introducing geodesic convexity and <a href="https://arxiv.org/abs/1804.04051">show</a> that the problem of computing the Brascamp-Lieb constant has a succinct geodesically convex formulation.</p>
<p>And, accompanying my talk are <a href="https://nisheethvishnoi.files.wordpress.com/2018/06/geodesicconvexity.pdf" title="GeodesicConvexity">these</a> self-contained and expository notes on differentiation on manifolds, geodesics, and geodesic convexity that are prepared with the help of my student Ozan Yildiz.</p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2018/06/06/an-exposition-on-geodesic-convexity/"><span class="datestr">at June 06, 2018 10:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2018/05/29/taking-control-by-convex-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2018/05/29/taking-control-by-convex-optimization/">Taking control by convex optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The picture to the right depicts Rudolf Kalman, a pioneer of modern control, receiving the presidential medal of freedom from president Barack Obama.<img src="https://minimizingregret.files.wordpress.com/2018/05/download.jpeg?w=1100" alt="download.jpeg" class=" size-full wp-image-181 alignright" /></p>
<p>The setting which Kalman considered, and has been a cornerstone of modern control, is that of stateful time-series. More precisely, consider a sequence of inputs and outputs, or measurements, coming from some real-world machine<br />
<img src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C...%2Cx_T%2C...%C2%A0+%C2%A0%5C+%5C+%5CRightarrow%C2%A0+%5C+%5C+%5C+y_1%2Cy_2%2C+y_3+%2C+...%2C+y_T+%2C+...+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_1,x_2,...,x_T,...   \ \ \Rightarrow  \ \ \ y_1,y_2, y_3 , ..., y_T , ... " class="latex" title="x_1,x_2,...,x_T,...   \ \ \Rightarrow  \ \ \ y_1,y_2, y_3 , ..., y_T , ... " /><br />
Examples include:</p>
<ul>
<li><img src="https://s0.wp.com/latex.php?latex=y_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y_t" class="latex" title="y_t" /> are measurements of ocean temperature, in a study of global warming. <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_t" class="latex" title="x_t" /> are various environmental knowns, such as time of the year, water acidity, etc.</li>
<li>Language translation, <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_t" class="latex" title="x_t" /> are words in Hebrew, and <img src="https://s0.wp.com/latex.php?latex=y_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y_t" class="latex" title="y_t" /> is their English translation</li>
<li><img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x_t" class="latex" title="x_t" /> are physical controls of a robot, i.e. force in various directions, and <img src="https://s0.wp.com/latex.php?latex=y_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y_t" class="latex" title="y_t" /> are the location to which it moves</li>
</ul>
<p>Such systems are generally called dynamical systems, and one of the simplest and most useful mathematical models to capture a subset of them is called  linear dynamical systems (LDS), in which the output is generated according to the following equations:</p>
<p style="text-align: center;"><img src="https://s0.wp.com/latex.php?latex=h_%7Bt%2B1%7D%C2%A0+%3D+A+h_t%C2%A0%2B+B+x_t+%2B+%5Ceta_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_{t+1}  = A h_t + B x_t + \eta_t " class="latex" title="h_{t+1}  = A h_t + B x_t + \eta_t " /><br />
<img src="https://s0.wp.com/latex.php?latex=y_%7Bt%2B1%7D%C2%A0+%C2%A0%3D+C+h_t%C2%A0%2B+D+x_t%C2%A0%2B+%5Czeta_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y_{t+1}   = C h_t + D x_t + \zeta_t " class="latex" title="y_{t+1}   = C h_t + D x_t + \zeta_t " /></p>
<p>Here <img src="https://s0.wp.com/latex.php?latex=h_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="h_t" class="latex" title="h_t" /> is a hidden state, which can have very large or even infinite dimensionality, <img src="https://s0.wp.com/latex.php?latex=A%2CB%2CC%2CD&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A,B,C,D" class="latex" title="A,B,C,D" /> are linear transformations and <img src="https://s0.wp.com/latex.php?latex=%5Ceta_t%2C%5Czeta_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\eta_t,\zeta_t" class="latex" title="\eta_t,\zeta_t" /> are noise vectors.</p>
<p>This setting is general enough to capture a variety of machine learning models previously considered in isolation, such as HMM (hidden Markov models), principle and independent component analysis, mixture of Gaussian clusters and many more, see this <a href="https://cs.nyu.edu/~roweis/papers/NC110201.pdf">survey by Roweis and Ghahramani</a>.</p>
<p>Alas – without any further assumptions the Kalman filtering model is non-convex and computationally hard, thus general polynomial time algorithms for efficient worst-case prediction were not known, till some exciting recent developments I’ll survey below.</p>
<p>If the linear transformations <img src="https://s0.wp.com/latex.php?latex=A%2CB%2CC%2CD&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="A,B,C,D" class="latex" title="A,B,C,D" /> are known, then it is possible to efficiently predict <img src="https://s0.wp.com/latex.php?latex=y_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="y_t" class="latex" title="y_t" /> given all previous inputs and outputs of the system using convex optimization. Indeed, this is what the Kalman filter famously achieves, and in an optimal way. Similarly, if there is no hidden state, the problem becomes trivially convex, and thus easily solvable (see recent blog posts and papers by the Recht group further analyzing this case).</p>
<p>However, if the system given by the transformations A,B,C,D is unknown, recovering them from observations is called “system identification”, a notoriously hard problem. Even for HMMs, which can be seen as a special case of LDS (see the <a href="http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf">RG</a> survey), the identification problem is known to be <a href="http://www.math.ru.nl/~terwijn/publications/icgiFinal.pdf">NP-hard</a> without additional assumptions. (Aside: HMMs <b>are</b> identifiable with further distributional assumption, see the breakthrough paper of<a href="http://www.cs.columbia.edu/~djhsu/papers/hmm.pdf"> Hsu, Kakade and Zhang</a>, which started the recent tensor revolution in theoretical ML).</p>
<p>In the face of NP-hardness, state-of-the-art reduces to a set of heuristics. Amongst them are:</p>
<ul>
<li>By far the most common is the use of EM, or expectation maximization, to alternately solve for the system and the hidden states. Since this is a heuristic for non-convex optimization, global optimality is not guaranteed, and indeed EM can settle on a local optimum, as shown <a href="https://arxiv.org/abs/1711.00946">here</a>.</li>
<li>Gradient descent: this wonderful heuristic can be applied for the non-convex problem of finding both the system and hidden states simultaneously. In a striking recent result, <a href="https://arxiv.org/abs/1609.05191">Hardt and Ma</a> showed that under some generative assumptions, gradient descent converges to the global minimum.</li>
</ul>
<div></div>
<p>This is where we go back to the convex path: using the failsafe methodology of convex relaxation and regret minimization in games, in a <a href="https://arxiv.org/abs/1711.00946">recent</a> set of <a href="https://arxiv.org/abs/1802.03981">results</a> our group has found assumption-free, efficient algorithms for predicting in general LDS, as well as some other control extensions. At the heart of these results is a new method of spectral filtering. This method requires some mathematical exposition, we’ll defer it to a future longer post. For the rest of this post, we’ll describe the autoregressive (a.k.a. regression) methodology for LDS, which similar to our result, is a worst-case and convex-relaxation based method.</p>
<p>To see how this works, consider the LDS equations above, and notice that for zero-mean noise, the expected output at time <img src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="t+1" class="latex" title="t+1" /> is given by: (we take <img src="https://s0.wp.com/latex.php?latex=D%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="D=0" class="latex" title="D=0" /> for simplicity)<br />
<img src="https://s0.wp.com/latex.php?latex=E%5By_%7Bt%2B1%7D+%5D+%3D%C2%A0+%5Csum_%7Bi%3D1%7D%5Et%C2%A0+C+A%5Ei+B+x_%7Bt-i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="E[y_{t+1} ] =  \sum_{i=1}^t  C A^i B x_{t-i} " class="latex" title="E[y_{t+1} ] =  \sum_{i=1}^t  C A^i B x_{t-i} " /><br />
This has few parameters to learn, namely only three matrices A,B,C, but has a very visible non-convex component of raising A to high powers.</p>
<p>The obvious convex relaxation takes <img src="https://s0.wp.com/latex.php?latex=M_i+%3D+C+A%5Ei+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="M_i = C A^i B" class="latex" title="M_i = C A^i B" /> as a set of matrices for <img src="https://s0.wp.com/latex.php?latex=i%3D1%2C...%2Ct&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="i=1,...,t" class="latex" title="i=1,...,t" />. We now have a set of linear equalities of the form:</p>
<p>$latex   y_{t+1}  = \sum_{i=1}^t M_i x_{t-1}  $</p>
<p>which can be solved by your favorite convex linear-regression optimizer. The fallacy is also clear: we have parameters that scale as the number of observations, and thus cannot hope to generalize.</p>
<p>Luckily, many systems are well behaved in the following way. First, notice that the very definition of an LDS requires the spectral norm of A to be bounded by one, otherwise we have signal explosion – it’s magnitude grows indefinitely with time. It is thus not too unreasonable to assume a strict bound away from one, say <img src="https://s0.wp.com/latex.php?latex=%7CA%7C+%5Cleq+1-+%5Cdelta&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="|A| \leq 1- \delta" class="latex" title="|A| \leq 1- \delta" />, for some small constant <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\delta &gt;0" class="latex" title="\delta &gt;0" />. This allows us to bound the magnitude of the i’th matrix <img src="https://s0.wp.com/latex.php?latex=M_i&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="M_i" class="latex" title="M_i" />, that is supposed to represent <img src="https://s0.wp.com/latex.php?latex=C+A%5Ei+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="C A^i B" class="latex" title="C A^i B" />, by <img src="https://s0.wp.com/latex.php?latex=%281-%5Cdelta%29%5Ei&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="(1-\delta)^i" class="latex" title="(1-\delta)^i" />.</p>
<p>With this assumption, we can then take only <img src="https://s0.wp.com/latex.php?latex=k+%3D+O%28%5Cfrac%7B1%7D%7B%5Cdelta%7D+%5Clog+%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="k = O(\frac{1}{\delta} \log \frac{1}{\epsilon})" class="latex" title="k = O(\frac{1}{\delta} \log \frac{1}{\epsilon})" /> matrices, and write<br />
$latex  y_{t+1} = \sum_{i=1}^k M_i x_{t-1} + O(\epsilon) $<br />
This folklore method, called an autoregressive model or simply learning LDS by regression, is very powerful despite its simplicity. It has the usual wonderful merits of online learning by convex relaxation: is worst-case (no generative assumptions), agnostic, and online. Caveats? we have a linear dependence on the eigengap of the system, which can be very large for many interesting systems.</p>
<p>However, it is possible to remove this gap completely, and learn arbitrarily ill-defined LDS without dependence on the condition number. This method is also online, agnostic, and worst case, and is based on a new tool: wave-filtering. In essence it applies the online regression method on a specially-designed fixed filter of the input. To see how these filters are designed, and their theoretical properties, check out our recent <a href="https://arxiv.org/abs/1711.00946">papers</a>!</p>
<p> </p></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2018/05/29/taking-control-by-convex-optimization/"><span class="datestr">at May 29, 2018 06:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://nisheethvishnoi.wordpress.com/?p=30">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/nisheeth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://nisheethvishnoi.wordpress.com/2018/05/26/algorithms-for-convex-optimization/">Lecture Notes on Algorithms for Convex Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I have been teaching convex optimization methods to students of computer science and machine learning over four years in various forms and styles. Finally, the notes are online <a href="https://nisheethvishnoi.wordpress.com/convex-optimization/">here</a>!</p></div>







<p class="date">
by nisheethvishnoi <a href="https://nisheethvishnoi.wordpress.com/2018/05/26/algorithms-for-convex-optimization/"><span class="datestr">at May 26, 2018 11:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2018/03/01/unsupervised-learning-iii-worst-case-compression-based-metrics-that-generalize/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2018/03/01/unsupervised-learning-iii-worst-case-compression-based-metrics-that-generalize/">unsupervised learning III: worst-case compression-based metrics that generalize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">
<p>This post is a long-overdue sequel to parts I and II of previous posts on unsupervised learning.  The long time it took me to publish this one is not by chance – I am still ambivalent on the merit, or lack of, for the notion of a worst-case PAC/statistical learnability (or, god forbid, regret minimization) without supervision.</p>
<p>And yet,<br />
1. I owe a followup to the stubborn few readers of the blog who have made it thus far.<br />
2. The algorithms we’ll discuss are mathematically correct, and even if completely useless in practice, can serve as amusement to us, fans of regret minimization in convex games.<br />
3. I really want to get on to more exciting recent developments in control that have kept me busy at night…</p>
<p>So, here goes:</p>
<p>The idea is to define unsupervised learning as PAC learning in the usual sense, with a real-valued objective function, and specially-structured hypothesis classes.</p>
<ul>
<li>A hypothesis is a pair of functions, one from domain to range and the other vice versa, called encoder-decoder pair.</li>
<li>The real-valued loss is also composed of two parts, one of which is the reconstruction error of the encoding-decoding scheme, and the other proportional to encoding length.</li>
</ul>
<p>Et-voila, all notions of generalizability carry over from statistical learning theory together with dimensionality concepts such as Rademacher complexity.</p>
<p>More significantly – these worst-case definitions allow improper learning of NP-hard concepts. For the fully-precise definitions, see  the <a href="https://arxiv.org/abs/1610.01132">paper with Tengyu Ma</a>. The latter also gives a striking example of a very well-studied NP-hard problem, namely dictionary learning, showing it can be learned in polynomial time using convex relaxations.</p>
<p>Henceforth, I’ll give a much simpler example that perhaps illustrates the concepts in an easier way, one that arose in conversations with our post-docs <a href="http://www.cs.princeton.edu/~rlivni/">Roi Livni</a> and <a href="http://www.cs.princeton.edu/~kothari/">Pravesh Kothari</a>.</p>
<p>This example is on learning the Mixture-Of-Gaussians (MOG) unsupervised learning model, one of the most well-studied problems in machine learning. In this setting, a given distribution over data is assumed to be from a mixture distribution over normal random variables, and the goal is to associate each data point with the corresponding Gaussian, as well as to identify these Gaussians.</p>
<p>The MOG model has long served as a tool for scientific discovery, see e.g. this <a href="http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html">blog post by Moritz Hardt</a> and links therein. However, computationally it is not well behaved in terms of worst-case complexity. Even the simpler “k-means” model is known to be NP-hard.</p>
<p>Here is where things get interesting: using reconstruction error and compression as a metric, we can learn MOG by a seemingly different unsupervised learning method – Principle Component Analysis (PCA)! The latter admits very efficient linear-algebra-based algorithms.</p>
<p>More formally: let <img src="https://s0.wp.com/latex.php?latex=X+%5Cin+R%5E%7Bm+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="X \in R^{m \times d}" class="latex" title="X \in R^{m \times d}" /> be a data matrix sampled i.i.d from some arbitrary distribution <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="x \sim D" class="latex" title="x \sim D" />. Assume w.l.o.g. that all norms are at most one. Given a set of k centers of Gaussians, <img src="https://s0.wp.com/latex.php?latex=%5Cmu_1%2C...%2C%5Cmu_k&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\mu_1,...,\mu_k" class="latex" title="\mu_1,...,\mu_k" />, minimize reconstruction error given by<br />
<img src="https://s0.wp.com/latex.php?latex=E_%7Bx+%5Csim+D%7D+%5B+%5Cmin_i+%5C%7C+%5Cmu_i+-+x+%5C%7C%5E2%C2%A0+%5D%C2%A0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="E_{x \sim D} [ \min_i \| \mu_i - x \|^2  ]  " class="latex" title="E_{x \sim D} [ \min_i \| \mu_i - x \|^2  ]  " /><br />
A more general formulation, allowing several means to contribute to the reconstruction error, is<br />
<img src="https://s0.wp.com/latex.php?latex=E_%7Bx+%5Csim+D%7D+%5B+%5Cmin_%7B+%5C%7C+%5Calpha_x%5C%7C_2+%5Cleq+1%C2%A0+%7D%C2%A0+%5C%7C+%5Csum_%7Bj%7D+%5Calpha_j%C2%A0+%5Cmu_j+-+x+%5C%7C%5E2%C2%A0+%5D%C2%A0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="E_{x \sim D} [ \min_{ \| \alpha_x\|_2 \leq 1  }  \| \sum_{j} \alpha_j  \mu_j - x \|^2  ]  " class="latex" title="E_{x \sim D} [ \min_{ \| \alpha_x\|_2 \leq 1  }  \| \sum_{j} \alpha_j  \mu_j - x \|^2  ]  " /><br />
Let <img src="https://s0.wp.com/latex.php?latex=M+%5Cin+R%5E%7Bd+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="M \in R^{d \times k}" class="latex" title="M \in R^{d \times k}" /> be the matrix of all $\mu$ vectors. Then we can write the optimization problem as<br />
$latex  \min_{M} E_{x \sim D} [ \min_{ \| \alpha_x\|_2 \leq 1  }  \|   x  – M \alpha_x \|^2  ]  $<br />
This corresponds to an <b>encoding by vector $\alpha$ rather than by a single coordinate</b>. We can write the closed form solution to $\alpha_x$ as:<br />
$latex  \arg \min_{\alpha_x }    \|   x  – M \alpha_x \|^2   = M^{-1} x $<br />
and the objective becomes<br />
$latex  \min_{\alpha_x }    \|   x  – M \alpha_x \|^2   = \| x – M M^{-1} x\|_\star^2 $<br />
for <img src="https://s0.wp.com/latex.php?latex=%5C%7C+%5C%7C_%5Cstar&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\| \|_\star" class="latex" title="\| \|_\star" /> being the spectral norm.<br />
Thus, we’re left with the optimization problem of<br />
$latex  \min_{M} E_{x \sim D} [  \|   x  – M M^{\dagger} x \|_\star^2  ]  $<br />
which amounts to PCA.</p>
<p>We have just semi-formally seen how MOG can be learned by PCA!</p>
<p>A more general theorem applies also to MOG that are not spherical, some details appear in the paper with Tengyu, in the section on spectral auto-encoders.</p>
<p>The keen readers will notice we lost something in compression along the way: the encoding is now a k-dimensional vector as opposed to a 1-hot k-dimensional vector, and thus takes $k \log \frac{1}{\epsilon}$ bits to represent in binary, as opposed to $O(\log k)$. We leave it as an open question to come up with an <i>efficient</i> poly-log(k) compression that matches MOG. The solver gets a free humus at Sayeed’s, my treat!</p>
</div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2018/03/01/unsupervised-learning-iii-worst-case-compression-based-metrics-that-generalize/"><span class="datestr">at March 01, 2018 07:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1224">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2018/01/23/ucsd-spring-school-on-quantum-computation/">UCSD Spring school on Quantum Computation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A couple months from now <a href="http://www.cs.huji.ac.il/~doria/">Dorit Aharonov</a>, <a href="http://www.davidgosset.com/">David Gosset</a> and myself will be giving a short 3.5-day “Spring School” that is meant to be an introduction to recent topics in quantum computing, directed at young researchers in theoretical computer science at large. The <a href="http://cseweb.ucsd.edu/~slovett/workshops/quantum-computation-2018/">school</a> is organized by Shachar Lovett at the University of California in San Diego, from March 19th to 22nd (these dates coincide with Spring break at many universities). Shachar has been organizing such schools successfully for multiple years now (<a href="http://cseweb.ucsd.edu/~slovett/workshops/sum-of-squares-2017/">last year’s school</a> was taught by Boaz Barak and David Steurer on Sums of Squares), and we hope that this year’s will be just as fun (and instructive). The (free) registration deadline is on February 1st, and we have limited funds available to support travel for a few needy students apply <a href="http://cseweb.ucsd.edu/~slovett/workshops/quantum-computation-2018/">here</a> by February 1st.</p>
<p>The past two years, and possibly even more so the coming couple years, may well be remembered as the moment when quantum computing entered the mainstream. Most of us have heard of IBM’s <a href="https://quantumexperience.ng.bluemix.net/qx">quantum computer in the cloud</a>, of Google’s effort in <a href="https://research.google.com/pubs/QuantumAI.html">, and of Microsoft’s naturally fault-tolerant \href</a>. Some of us might also have encountered a few of the <a href="https://en.wikipedia.org/wiki/List_of_companies_involved_in_quantum_computing_or_communication">dozens of startups</a> promising everything from quantum hardware to quantum learning, that seem to be appearing out of nowhere, raising capital in just a few months.</p>
<p>It is an interesting question, better left for wiser times, whether these events will be remembered as the initial sparks of a revolution in computing, or as the height of a “quantum bubble”. Bubble or no bubble, quantum information science is here to stay: while current developments make topics such as the computational power of small-scale quantum computers, the possibilities for testing quantum mechanics, all the more exciting, quantum cryptography, the theory of quantum error-correction, the ever-increasing applications of “quantum techniques” to problems from theoretical computer science, do not hinge on the success of current experiments.</p>
<p>In guise of teaser, our plan for the school is roughly as follows. Each day will have about 6 hours of lecture, a couple hours of informal “TA sessions” (to learn a language, one needs to practice it!), and some time for social interaction. This is a fairly heavy schedule, but if these are the 3.5 days you are going to spend learning about quantum information in your career, we want them to be useful. What this means is that we’ll simultaneously aim to cover the basics, so as to establish a common language, while quickly zooming in to a selection of the most interesting questions, such as the power of alternate models of quantum computation, the theory of quantum error-correction and fault-tolerance, or problems in quantum testing and quantum delegation.</p>
<p>In a little more detail, and although you should not treat this as contractual information, here is a sketch of our program for the school:</p>
<p><b>Day 1:</b> Introduction to quantum information: one qubit, <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />qubits, the quantum circuit model, simple algorithms and computational speed-ups in the query model. Introduction to quantum complexity: the class QMA and the local Hamiltonian problem.</p>
<p><b>Day 2:</b> Protocols for delegating quantum computations. The adiabatic model of computation and its equivalence to the circuit model. Quantum error-correction, stabilizer codes, and fault tolerance.</p>
<p><b>Day 3:</b> Restricted models of computation (shallow circuits, commuting circuits). Testing quantum systems. The quantum PCP conjecture and connection to many-body entanglement. Multi-prover interactive proofs with multiple provers sharing entanglement. Quantum linearity testing.</p>
<p><b>Day 4:</b> More restricted models of computation. Quantum optimization algorithms. Stoquastic Hamiltonians. Quantum Monte-Carlo and simulation.</p>
<p>If you’re not an expert in quantum information, a lot of these topics might not make much sense a priori. This is why you should come! Our goal in these 3.5 days is to summarize what we believe ought to be the highlights of a couple semesters’ worth of graduate courses in quantum information. Aside from the basics in the first day, each lecture will cover a topic of current interest, giving you the ability to understand the importance of recent progress, and start thinking about some of the more TCS-friendly problems. Towards this, we’ll highlight as many open problems as we can think of (and fit in the alloted time), and allow ample time for questions, discussions, and hands-on exercise sessions. Join us: register <a href="http://cseweb.ucsd.edu/~slovett/workshops/quantum-computation-2018/">here</a>!</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2018/01/23/ucsd-spring-school-on-quantum-computation/"><span class="datestr">at January 23, 2018 04:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1218">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2017/08/07/a-beginners-guide-to-pc-chairing/">A beginner’s guide to PC chairing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I recently had the privilege to serve as program committee (PC) chair for the yearly conference on quantum cryptography, <a href="http://2017.qcrypt.net/">QCRYPT’17</a> (note: for obvious public-relations reasons all names of conferences, PC members and authors in this post have been replaced by <em>entirely</em> fictional aliases). Although I had served on the PC of QCRYPT (and other conferences and workshops in quantum information and theoretical computer science) before, this was my first experience as chair. Since I am not aware of any publicly available resources that would help prepare one for this task, I thought I would share selected aspects of my experience here.</p>
<p>It is easiest to organize the discussion in chronological order: I will go through all the steps (and missteps), from the initial invitation email to the final notification to authors (probably safer to stop there – the next step would drag me into a discussion of the authors’ reaction, the consequences of which even the use of aliases may not save me from).</p>
<h2><b>Lights:</b></h2>
<p>You just received that glowing email — “Dear XX, would you be interested to serve as PC chair for ConfY’18?”. Followed by the obligatory series of flattering comments. Such an honor… who would refuse? But don’t jump on that reply-send button just now. Here are a few points to take into consideration before making the decision.</p>
<p>First off, carefully consider the reviewing schedule. The dates of the conference are likely decided already, giving you a good sense of when the submission and notification deadlines will fall. The period in-between represents two to four months of your working life. Are you ready to give them up? I estimate that most days within that period you will have to allocate one to two hours’ work to the PC reviewing process (the load is not evenly spread: during the reviewing phase, it depends how many papers you assign yourself to review; during the discussion phase, it depends on how active you are, whether there is an in-person meeting, etc.). This is a serious commitment, comparable in load to taking on an additional 12-week teaching job. So if you’re already planning on teaching two courses during the same period – think twice.</p>
<p>A second point to consider discussing upfront with the steering committee (SC) is your “mission”. The SC probably has its own idea of the scope of the conference (there might even be a charter), how many papers they would like to be accepted, what justifies a “ConfY-worthy paper”, etc. How rigid are they going to be regarding these points? How much interference can you expect — do you have full latitude in deciding final acceptances (should be)? How flexible is the final number of accepts?</p>
<p>Last but not least, make sure this is something you <em>want</em> to do. How good is ConfY? Does it serve a specific purpose that you value? How often have you attended, or served on the PC? Do you feel competent to make decisions across all areas covered by the conference? Check the past couple years’ accepts. Many conferences are broader than we think, just because when we attend we tend to unconsciously apply a selective bias towards those talks for which we can at least parse the title. This time you’ll have to understand the contents of every single one of the submitted (let alone accepted) papers. So again, is this something you {\textit want} to do?</p>
<h2><b>Camera,</b></h2>
<p><b>Selecting the PC.</b> Now that the fatal decision has been made, my first piece of advice is all too simple: <em>seek advice</em>. Your first task is to form a PC. This is clearly the most influential decision you will make, both in terms of the quality and content of the final program, as well as the ease with which you and your “team” will get there. Choosing a PC is a delicate balancing act. A good mix of seniority and young blood is needed: seniority for the experience, the perspective, and the credibility; young blood for the energy, the taste for novelty, the muscle power. It is a good idea to involve a PC member from the previous installment of the conference; this may in particular help with the more difficult cases of resubmission.</p>
<p>I was fortunate to receive multiple recommendations from the SC, past conference chairs, and colleagues. While you obviously want to favor diversity and broad representation of topic areas, I also recommend selecting PC members with whom one has a personal connection. My experience has been that the amount of effort any one person is willing to put into the PC process varies hugely. It is inevitable that some PC members will eventually drift away. The more connection you have to them the easier it will be to handle irresponsiveness or divergences of opinion.</p>
<p>The more important comment I will make, one which I wish I had been more keenly aware of, is to <em>know your PC</em>. You will eventually select a team of researchers with complementary qualities, not only in terms of the areas that they are familiar with but also in more human terms: some will be good at responding to “quick opinion” calls on difficult papers, while others will have insightful comments about the overall balance of papers in the emerging list of accepted papers, or generate thoughtful advice on the handling of the more tricky cases, etc. At multiple points in the process you will need help; it is crucial to know the right person to turn to, lest you waste precious days or make ill-informed decisions.</p>
<p>With a list of names in hand, you are ready to send out invitations. (Before doing so, consider forming a rough schedule for the reviewing process. This will be needed for PC members to decide whether they will be sufficiently available during the requisite periods.) In my experience this part went smoothly. About <img src="https://s0.wp.com/latex.php?latex=%7B75%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{75\%}" class="latex" title="{75\%}" /> of those on my initial list accepted the invitation (thanks!!). Filling in the remaining slots took a little more time. A small tip: if a researcher does not respond to your invitation within a reasonable delay, or is slow to decide whether to join or not, don’t push too hard: while you need a strong PC, you also need a responsive PC. It is not a good idea to start off in a situation where someone is “doing you a favor” by accepting the invitation as a result of some heavy arm-twisting.</p>
<p><b>Drafting a CFP.</b> The second main item on your pre-submission agenda is the drafting of a call for papers (CFP). This may be done in cooperation with the SC. CFP from previous years can serve as a starting point. Check with last year’s PC chair if they were happy with the wording used, or if they have a posteriori recommendations: did they omit an important area of interest? Were the submission instructions, including formatting guidelines, clear?</p>
<p>A good CFP balances out two conflicting desirata: first, it should make your life easier by ensuring that submissions follow a reasonably homogeneous format, and are presented in a way that facilitates the reviewing process; second, it should not place an unreasonable burden on the authors who, as we all know, have better things to do (and will read the instructions, if they ever read them, no earlier than 23:59 in any timezone – making an overly precise CFP a sure recipe for disaster).</p>
<p>One place where precision <em>is</em> needed is in the formulation of the requirements for rigor and completeness. Are full proofs expected, or will a short 3-page abstract suffice? Or should it be both – a short abstract clearly presenting the main ideas, together with a full paper providing precise complete proofs? Be warned that, whatever the guidelines, they will be stretched, forcing you into quick judgment calls as to whether a submission fits the CFP guidelines.</p>
<p>You should also pay attention to the part of the CFP that concerns the scope of the conference: although for all I know this is all but ignored by most authors, and varies little from year to year, it does play an important role in carving out an inch of originality and specificity for the conference.</p>
<p>Another item on the CFP is the “key dates” that will bound the time available for the reviewing process: the submission deadline and the notification date. Here again there are conflicting requirements: the submission date should be as late as possible (to ensure accepted papers are as fresh as possible by the time the conference takes place), the reviewing phase as long as possible (you’re going to need it…), and the notification as early as possible (so there is time to compile proceedings, when they exist, and for authors to make travel arrangements). In my experience as PC member the time allocated for reviewing almost invariably felt too long – yes, I did write <em>too long</em>. However much time is allocated for the reviewing phase invariably ends up divided into <img src="https://s0.wp.com/latex.php?latex=%7B%5Csim+90%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sim 90\%}" class="latex" title="{\sim 90\%}" /> procrastination and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csim+20%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sim 20\%}" class="latex" title="{\sim 20\%}" /> actual reviewing effort (obviously the actual reviewing gets under way too late for it to be completed by the reviewing deadline, which typically gets overstretched by some <img src="https://s0.wp.com/latex.php?latex=%7B%5Csim+10%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sim 10\%}" class="latex" title="{\sim 10\%}" />). I suggest that a good calendar should allocate a month for collecting reviews, and a month for discussion. This is tight but sufficient, and will ensure that everyone remains engaged throughout. A month for reviewing allows a week for going through papers and identifying those for which external help should be sought; 2-3 weeks for actual reviewing; and a week for collecting reviews, putting the scores together, and scrambling through the last-minute calls for help. Similarly, a month of discussion would allow a week for score homogenization, two weeks to narrow down on the <img src="https://s0.wp.com/latex.php?latex=%7B20%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{20\%}" class="latex" title="{20\%}" /> (say) borderline papers, and a final week to make those tough ultimate decisions. Tight, but feasible. Remember: however much time you allocate, will be taken up!</p>
<p>Now, as good a calendar you may have come up with, <em>plan for delays</em>. In my case I typically informed PC members that “reviews have to be completed by April 29th” and “the discussion phase will start on May 1st”. The “hidden” three days in-between the two dates were more than needed to track down missing reviews. Don’t ask PC members to complete a task the day you need the task completed, as it simply won’t happen: people have busy schedules, operate in different (and sometimes shifting) timezones, and have other deadlines to deal with. To respect your PC you ought to give them a precise calendar that you will follow, so they are able to plan ahead; but you also need to allow for the unavoidable time conflicts, last-minute no-shows, and other unpredictable events.</p>
<p>One last item before you break off. To set up the submissions webpage you’ll need to decide on a reviewing management software. I (quite mistakenly) didn’t give much thought to this. As PC member I had had a decent experience with <a href="http://easychair.org/">easychair</a>, and was under the impression that it was the most commonly used software – and would therefore be easiest to work with for the PC. Even though things went, on the whole, fairly smoothly, I had more than one occasion to regret the decision. The topic would deserve a blog post in itself, and I won’t expand here. Just make sure you carefully consider how easy the software will make different parts of the reviewing process, such as computing statistics, tracking missing reviews, ordering papers based on various criteria, allowing an efficient tagging system to keep track of memos or tentative decisions, handling communication with authors (including possibly the compilation of proceedings), etc.</p>
<h2><b>Action!</b></h2>
<p>Alright, so you went for a stroll and enjoyed your most leisurely conference submission deadline ever – as PC chair, you’re probably not allowed to submit – but the bell has rung, the submission sever closed…now it’s your turn!</p>
<p><b>The last few hours.</b> Actually, maybe this wasn’t quite your most leisurely submission deadline after all. I was advised to elect a “midnight <a href="https://www.timeanddate.com/time/zones/aoe">anywhere on earth</a>” deadline, as this supposedly made the guideline easier to comprehend for everyone. Not only do I now have strong evidence that I am not the only one to find this denomination absurdly confusing – where on earth is this place anyways, anywhere on earth?? – I would in any case strongly suggest setting a deadline that falls at a reasonable time in the PC chair’s timezone. You <em>will</em> get email from people unable to access the submission server (for whatever reason), unsure whether their submission fits the guidelines, asking whether they can get an extension, etc. It is more helpful if you can deal with such email as they arrive, rather than the next day.</p>
<p><b>Paper bidding.</b> Before reviewing can get under way you need to assign papers to PC members. And before you can assign papers, PC members need to express their preferences. The resulting allocation is critical. It determines how many headaches you will face later on: how many papers will have low confidence reviews, how many closely related papers will have been reviewed by disjoint sets of PC members, how many papers will live on the enthusiastic scores of expert <em>sub</em>reviewers. I found this phase challenging. An automatic assignment can be completed in milliseconds, but doesn’t take into account related submissions or expertise of PC members aside from their declared preference, which is a single noisy bit of information. I highly recommend (I realize I am “highly recommending” a lot of things for a first-timer – I only wish I had been told some of these ahead of time!) taking the process very seriously, and spending enough time to review, and tweak, the automatic assignment before it is made final.</p>
<p><b>Refereeing.</b> Each PC member now has a healthy batch of papers assigned, and a deadline by which to submit reviews. What kind of guidelines can you give to make the process as smooth as possible? Discrepancies in scores are always an issue: whichever reviewing software you use, it is bound to produce some kind of score-based ranking; this initial ranking, although it will change during the discussion phase, induces a huge bias in final decisions (this effect is exacerbated for conferences, such as QCRYPT, where there is no in-person meeting). I don’t have a magic solution to this, but establishing clear guidelines in terms of the significance and expected proportion for each numerical score helps. I eventually found it necessary to prod outliers to modify their scores. This is one of the things easychair did not make particularly easy, forcing me to download data in Excel format and run some basic home-made scripts on the spreadsheet.</p>
<p>Aside from scoring, it is useful to include precise guidelines on the use of sub-referees and conflicts of interest (COIs). I allowed sub-refereeing but insisted that the final opinion should be the PC member’s. (It is not ok to copy-paste a sub-review while barely having gone through it!) Unfortunately sub-reviewers tend to be experts, and experts tend to be overly enthusiastic: watch out for papers that received three high scores, each of which with high confidence: easychair will rank those right at the top, but they may well be worth a second look.</p>
<p>Regarding COIs, I did not set overly strict rules (with the idea that “everyone knows when it is appropriate to declare a COI”), and regretted it. It is simply too uncomfortable to realize at a late stage that this very enthusiastic review was written by a PC member who happens to be a close collaborator of one of the authors, but chose not to disclose the COI. Do you discard the review? I don’t know. It depends: maybe the source of the COI played a role in the PC member’s vocal defense of the paper, and maybe not. Better not let it happen. It is not necessarily that even weak COI should forbid reviewing, but rather that COIs should be made explicit. As long as everyone states their position, things are in the open and can be taken into account.</p>
<p><b>Discussion.</b> With all the reviews in (dream on… some reasonable fraction of the reviews in) begins the second phase of the reviewing process, the discussion phase. Success of this phase rests almost entirely on engagement of the PC chair and a few dedicated, dynamic PC members. Among PCs I have sat on the most satisfying were ones where the chair visibly spent large amounts of energy in the stimulation of online discussion. This is no trivial task: we all lead busy lives, and it is easy to let things slip; papers with high scores get in, low scores get out; a few days to discuss the few in the middle and we’ll be done…not so! Unfortunately, the initial ranking is bound to be <em>abysmal</em>. It is necessary to work to straighten things up. Some basic tricks apply: search for papers with high discrepancies in scores, low confidence, missing, very short, or uninformative reviews, etc. It is useful to individually prod PC members to keep the discussion going. This is a place where the “know your PC” recommendation comes in: for each submission, you need to be able to identify who will be able to clarify the arguments in favor and against the paper; who will have the technical expertise to clarify the relationship between papers X and Y, etc. It’s an exhausting, but highly rewarding process: I learned a lot by listening to my colleagues and trying to grasp at times rather subtle – and opinionated – arguments that could reach quite far from my expertise.</p>
<p><b>Decisions!</b> The discussion has been going on for a couple weeks, and you already only have little time left: it is time to start making decisions. Proceeding in phases seems popular, and effective. It helps to progressively sharpen the acceptance threshold. As long as there are too many papers in play it is very hard to get a sense of where the boundary will lie; typically, far too many papers will have positive scores and enthusiastic proponents than can ultimately be accepted.</p>
<p>However much ahead of time you get started, the real decisions will take place in the last few days. I found it helpful to set a clear calendar for the process, marking days when decisions would be made, identifying clear categories (accept, accept?, discuss!, etc.), and setting explicit targets for each phase (accept X/reject Y many more papers, etc.), even if I wasn’t always able to meet them. It is also important that the PC as a whole be aware of the target number of papers that is to be accepted. I have frequently been on PC where the chair gave us the information that “we will accept all great papers”, only to learn later that a hard limit had (of course) been set. Conversely, I’ve also been extremely annoyed at last-minute decisions along the lines of, well, we accepted about as much we could, but there’s 4 left undecided cases, and, well, they’re all really good, so why don’t we just stretch the program a bit and accept all 4 at the last minute. To me this is the PC not doing its job… be prepared to make difficult decisions! Make it clear to the PC (and to yourself) what your goal is. Is it to serve the authors, the conference attendees, the advancement of science – all of the above (good luck)?</p>
<h2><b>Post-mortem</b></h2>
<p>This was fun. Exhausting, but fun. Of course not all authors (or PC members) were happy. There will be complaints. And some of them will be justified: there is no perfect allocation. Mistakes happen. We did our best!</p>
<p>Some tasks lie down the road. Put a program together. Help select a best (student) paper. Gather statistics for the business meeting. But first things first: take a deep breath. This was fun.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2017/08/07/a-beginners-guide-to-pc-chairing/"><span class="datestr">at August 07, 2017 11:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1209">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2017/06/28/pauli-braiding/">Pauli braiding</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #ff0000;">[<strong>7/9/17 Update</strong>: Following a suggestion by Oded Regev I upgraded Section 1 from “probabilistic functions” to “matrix-valued functions”. This hopefully makes it a more useful, and interesting, mid-point between the classical analysis of BLR and the non-abelian extension discussed afterwards. I also fixed a bunch of typos — I apologize for the many remaining ones. The <a href="http://users.cms.caltech.edu/~vidick/pauli_braiding_1.pdf">pdf</a> has also been fixed.]</span></p>
<p>Last week Anand Natarajan from MIT presented our joint work on “<a href="https://arxiv.org/abs/1610.03574">A Quantum Linearity Test for Robustly Verifying Entanglement</a>” at the <a href="http://acm-stoc.org/stoc2017/">STOC’17</a> conference in Montreal. Since we first posted our paper on the quant-ph arXiv, Anand and I discovered that the test and its analysis could be reformulated in a more general framework of tests for group relations, and rounding of approximate group representations to exact group representations. This reformulation is stimulated by a beautiful paper by Gowers and Hatami on “<a href="https://arxiv.org/abs/1510.04085">Inverse and stability theorems for approximate representations of finite groups</a>”, which was first pointed to me by William Slofstra. The purpose of this post is to present the Gowers-Hatami result as a natural extension of the Blum-Luby-Rubinfeld linearity test to the non-abelian setting, with application to entanglement testing. (Of course Gowers and Hatami are well aware of this — though maybe not of the application to entanglement tests!) My hope in doing so is to make our result more accessible, and hopefully draw some of my readers from theoretical computer science into a beautiful area.</p>
<p>I will strive to make the post self-contained and accessible, with no quantum information background required — indeed, most of the content is purely — dare I say elegantly — mathematical. In the interest of being precise (and working out better parameters for our result than appear in our paper) I include essentially full proofs, though I may allow myself to skip a line or two in some of the calculations.</p>
<p>Given the post remains rather equation-heavy, here is a <a href="http://users.cms.caltech.edu/~vidick/pauli_braiding_1.pdf">pdf</a> with the same contents; it may be more convenient to read.</p>
<p>I am grateful to Anand, and Oded Regev and John Wright, for helpful comments on a preliminary version of this post.</p>
<p><b>1. Linearity testing</b></p>
<p>The Blum-Luby-Rubinfeld linearity test provides a means to certify that a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A%7B%5Cmathbb+Z%7D_2%5En%5Crightarrow%5C%7B%5Cpm1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:{\mathbb Z}_2^n\rightarrow\{\pm1 \}}" class="latex" title="{f:{\mathbb Z}_2^n\rightarrow\{\pm1 \}}" /> is close to a linear function. The test can be formulated as a two-player game:</p>
<p><b>BLR linearity test:</b></p>
<ul>
<li>(a) The referee selects <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%5Cin%7B%5Cmathbb+Z%7D_2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b\in{\mathbb Z}_2^n}" class="latex" title="{a,b\in{\mathbb Z}_2^n}" /> uniformly at random. He sends the pair <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a,b)}" class="latex" title="{(a,b)}" /> to one player, and either <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />, or <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a+b}" class="latex" title="{a+b}" /> (chosen uniformly at random) to the other.</li>
<li>(b) The first player replies with two bits, and the second player with a single bit. The referee accepts if and only if the player’s answers satisfy the natural consistency constraint.</li>
</ul>
<p>This test, as all others considered here, treats both players symmetrically. This allows us to restrict our attention to the case of players who both apply the same strategy, an assumption I will systematically make from now on.</p>
<p>Blum et al.’s result states that any strategy for the players in the linearity test must provide answers chosen according to a function that is close to linear. In this section I will provide a slight “matrix-valued” extension of the BLR result, that follows almost directly from the usual Fourier-analytic proof but will help clarify the extension to the non-abelian case.</p>
<p><b>1.1. Matrix-valued strategies</b></p>
<p>The “classical” analysis of the BLR test starts by modeling an arbitrary strategy for the players as a pair of functions <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A%7B%5Cmathbb+Z%7D_2%5En%5Crightarrow+%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:{\mathbb Z}_2^n\rightarrow \{\pm 1\}}" class="latex" title="{f:{\mathbb Z}_2^n\rightarrow \{\pm 1\}}" /> (for the second player, who receives a single string as query) and <img src="https://s0.wp.com/latex.php?latex=%7Bf%27%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+%5C%7B%5Cpm+1%5C%7D%5Ctimes%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow \{\pm 1\}\times\{\pm 1\}}" class="latex" title="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow \{\pm 1\}\times\{\pm 1\}}" /> (for the first player, who receives a pair of strings as query). In doing so we are making an assumption: that the players are deterministic. More generally, we should allow “probabilistic strategies”, which can be modeled via “probabilistic functions” <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%5COmega+%5Crightarrow+%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:{\mathbb Z}_2^n \times \Omega \rightarrow \{\pm 1\}}" class="latex" title="{f:{\mathbb Z}_2^n \times \Omega \rightarrow \{\pm 1\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bf%27%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes%5COmega%5Crightarrow+%5C%7B%5Cpm+1%5C%7D%5Ctimes%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \times\Omega\rightarrow \{\pm 1\}\times\{\pm 1\}}" class="latex" title="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \times\Omega\rightarrow \{\pm 1\}\times\{\pm 1\}}" /> respectively, where <img src="https://s0.wp.com/latex.php?latex=%7B%28%5COmega%2C%5Cmu%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\Omega,\mu)}" class="latex" title="{(\Omega,\mu)}" /> is an arbitrary probability space which plays the role of shared randomness between the players. Note that the usual claim that “probabilistic strategies are irrelevant because they can succeed no better than deterministic strategies” is somewhat moot here: the point is not to investigate success probabilities — it is easy to pass the BLR test with probability <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> — but rather derive structural consequences from the assumption that a certain strategy passes the test. In this respect, enlarging the kinds of strategies we consider valid can shed new light on the strengths, and weaknesses, of the test.</p>
<p>Thus, and with an eye towards the “quantum” analysis to come, let us consider an even broader set of strategies, which I’ll refer to as “matrix-valued” strategies. A natural matrix-valued analogue of a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:{\mathbb Z}_2^n \rightarrow \{\pm 1\}}" class="latex" title="{f:{\mathbb Z}_2^n \rightarrow \{\pm 1\}}" /> is <img src="https://s0.wp.com/latex.php?latex=%7BF%3A%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F:{\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" class="latex" title="{F:{\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BO_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O_d({\mathbb C})}" class="latex" title="{O_d({\mathbb C})}" /> is the set of <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\times d}" class="latex" title="{d\times d}" /> Hermitian matrices that square to identity (equivalently, have all eigenvalues in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}}" class="latex" title="{\{\pm 1\}}" />); these matrices are called “observables” in quantum mechanics. Similarly, we may generalize a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%27%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+%5C%7B%5Cpm+1+%5C%7D+%5Ctimes+%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow \{\pm 1 \} \times \{\pm 1\}}" class="latex" title="{f':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow \{\pm 1 \} \times \{\pm 1\}}" /> to a function <img src="https://s0.wp.com/latex.php?latex=%7BF%27%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29+%5Ctimes+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow O_d({\mathbb C}) \times O_d({\mathbb C})}" class="latex" title="{F':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow O_d({\mathbb C}) \times O_d({\mathbb C})}" />. Here we’ll impose an additional requirement: any pair <img src="https://s0.wp.com/latex.php?latex=%7B%28B%2CC%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(B,C)}" class="latex" title="{(B,C)}" /> in the range of <img src="https://s0.wp.com/latex.php?latex=%7BF%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F'}" class="latex" title="{F'}" /> should be such that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> commute. The latter condition is important so that we can make sense of the function as a strategy for the provers: we should be able to ascribe a probability distribution on outcomes <img src="https://s0.wp.com/latex.php?latex=%7B%28a%2C%28b%2Cc%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a,(b,c))}" class="latex" title="{(a,(b,c))}" /> to any query <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2C%28y%2Cz%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(x,(y,z))}" class="latex" title="{(x,(y,z))}" /> sent to the players. This is achieved by defining<a name="eqmatrixprob"></a></p>
<p align="center"><a name="eqmatrixprob"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5Cbig%28%28F%28x%29%2C+F%27%28y%2Cz%29%29%3D%28a%2C%28b%2Cc%29%29%5Cbig%29%5C%2C%3D%5C%2C%5Cfrac%7B1%7D%7Bd%7D%5C%2C%5Cmathrm%7BTr%7D%5Cbig%28+F%28x%29%5EaF%27%28y%2Cz%29_1%5Eb+F%27%28y%2Cz%29_2%5Ec%5Cbig%29%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Pr\big((F(x), F'(y,z))=(a,(b,c))\big)\,=\,\frac{1}{d}\,\mathrm{Tr}\big( F(x)^aF'(y,z)_1^b F'(y,z)_2^c\big), \ \ \ \ \ (1)" class="latex" title="\displaystyle \Pr\big((F(x), F'(y,z))=(a,(b,c))\big)\,=\,\frac{1}{d}\,\mathrm{Tr}\big( F(x)^aF'(y,z)_1^b F'(y,z)_2^c\big), \ \ \ \ \ (1)" /></p>
<p><a name="eqmatrixprob"></a>where for any observable <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" /> we denote <img src="https://s0.wp.com/latex.php?latex=%7BO%5E%7B%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O^{+1}}" class="latex" title="{O^{+1}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BO%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O^{-1}}" class="latex" title="{O^{-1}}" /> the projections on the <img src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+1}" class="latex" title="{+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-1}" class="latex" title="{-1}" /> eigenspaces of <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" />, respectively (so <img src="https://s0.wp.com/latex.php?latex=%7BO%3DO%5E%7B%2B1%7D-O%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O=O^{+1}-O^{-1}}" class="latex" title="{O=O^{+1}-O^{-1}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BO%5E%7B%2B1%7D%2BO%5E%7B-1%7D%3DI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O^{+1}+O^{-1}=I}" class="latex" title="{O^{+1}+O^{-1}=I}" />). The condition that <img src="https://s0.wp.com/latex.php?latex=%7BF%27%28y%2Cz%29_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F'(y,z)_1}" class="latex" title="{F'(y,z)_1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF%27%28y%2Cz%29_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F'(y,z)_2}" class="latex" title="{F'(y,z)_2}" /> commute ensures that this expression is always non-negative; moreover it is easy to check that for all <img src="https://s0.wp.com/latex.php?latex=%7B%28x%2C%28y%2Cz%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(x,(y,z))}" class="latex" title="{(x,(y,z))}" /> it specifies a well-defined probability distribution on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%5Ctimes+%28%5C%7B%5Cpm1%5C%7D%5Ctimes+%5C%7B%5Cpm1%5C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}\times (\{\pm1\}\times \{\pm1\})}" class="latex" title="{\{\pm 1\}\times (\{\pm1\}\times \{\pm1\})}" /> . Observe also that in case <img src="https://s0.wp.com/latex.php?latex=%7Bd%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=1}" class="latex" title="{d=1}" /> we recover the classical deterministic case, for which with our notation <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%5Ea+%3D+1_%7Bf%28x%29%3Da%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x)^a = 1_{f(x)=a}}" class="latex" title="{f(x)^a = 1_{f(x)=a}}" />. If all <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x)}" class="latex" title="{F(x)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF%27%28y%2Cz%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F'(y,z)}" class="latex" title="{F'(y,z)}" /> are simultaneously diagonal matrices we recover the probabilistic case, with the role of <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega}" class="latex" title="{\Omega}" /> (the shared randomness) played by the rows of the matrices (hence the normalization of <img src="https://s0.wp.com/latex.php?latex=%7B1%2Fd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/d}" class="latex" title="{1/d}" />; we will see later how to incorporate the use of non-uniform weights).</p>
<p>With these notions in place we establish the following simple lemma, which states the only consequence of the BLR test we will need.</p>
<blockquote><p><b>Lemma 1</b> <em><a name="lemblr-test"></a>Let <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> be an integer, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon\geq 0}" class="latex" title="{\varepsilon\geq 0}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BF%3A%7B%5Cmathbb+Z%7D_2%5En%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F:{\mathbb Z}_2^n\rightarrow O_d({\mathbb C})}" class="latex" title="{F:{\mathbb Z}_2^n\rightarrow O_d({\mathbb C})}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BF%27%3A%7B%5Cmathbb+Z%7D_2%5En+%5Ctimes+%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29%5Ctimes+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow O_d({\mathbb C})\times O_d({\mathbb C})}" class="latex" title="{F':{\mathbb Z}_2^n \times {\mathbb Z}_2^n \rightarrow O_d({\mathbb C})\times O_d({\mathbb C})}" /> a matrix strategy for the BLR test, such that players determining their answers according to this strategy (specifically, according to <a href="https://mycqstate.wordpress.com/feed/#eqmatrixprob">(1)</a>) succeed in the test with probability at least <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" />. Then</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D%5C%2C%5Cfrac%7B1%7D%7Bd%7D%5C%2C+%5CRe%5C%2C%5Cmathrm%7BTr%7D%5Cbig%28+F%28x%29F%28y%29F%28x%2By%29%5Cbig%29+%5C%2C%5Cgeq%5C%2C+1-O%28%5Cvarepsilon%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n}\,\frac{1}{d}\, \Re\,\mathrm{Tr}\big( F(x)F(y)F(x+y)\big) \,\geq\, 1-O(\varepsilon). " class="latex" title="\displaystyle \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n}\,\frac{1}{d}\, \Re\,\mathrm{Tr}\big( F(x)F(y)F(x+y)\big) \,\geq\, 1-O(\varepsilon). " /></p>
</blockquote>
<p>Introducing a normalized inner product <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+A%2CB%5Crangle_f+%3D+d%5E%7B-1%7D+%5Cmathrm%7BTr%7D%28AB%5E%2A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle A,B\rangle_f = d^{-1} \mathrm{Tr}(AB^*)}" class="latex" title="{\langle A,B\rangle_f = d^{-1} \mathrm{Tr}(AB^*)}" /> on the space of <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\times d}" class="latex" title="{d\times d}" /> matrices with complex entries (the <img src="https://s0.wp.com/latex.php?latex=%7B%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^*}" class="latex" title="{^*}" /> designates the conjugate-transpose), the conclusion of the lemma is that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5Clangle+F%28x%29F%28y%29%2C%5C%2CF%28x%2By%29%5Crangle_f+%5C%2C%3D%5C%2C+1-O%28%5Cvarepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \langle F(x)F(y),\,F(x+y)\rangle_f \,=\, 1-O(\varepsilon)}" class="latex" title="{\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \langle F(x)F(y),\,F(x+y)\rangle_f \,=\, 1-O(\varepsilon)}" />.</p>
<p><em>Proof:</em> Success with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" /> in the test implies the three conditions</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Clangle+F%27%28x%2Cy%29_1%2CF%28x%29%5Crangle_f+%5C%2C%5Cgeq%5C%2C+1-3%5Cvarepsilon%2C%5C%5C+%26%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Clangle+F%27%28x%2Cy%29_2%2CF%28y%29%5Crangle_f+%5C%2C%5Cgeq%5C%2C+1-3%5Cvarepsilon%2C%5C%5C+%26%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Clangle+F%27%28x%2Cy%29_1F%27%28x%2Cy%29_2%2CF%28x%2By%29%5Crangle_f+%5C%2C%5Cgeq%5C%2C+1-3%5Cvarepsilon.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_1,F(x)\rangle_f \,\geq\, 1-3\varepsilon,\\ &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_2,F(y)\rangle_f \,\geq\, 1-3\varepsilon,\\ &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_1F'(x,y)_2,F(x+y)\rangle_f \,\geq\, 1-3\varepsilon. \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_1,F(x)\rangle_f \,\geq\, 1-3\varepsilon,\\ &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_2,F(y)\rangle_f \,\geq\, 1-3\varepsilon,\\ &amp;&amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \langle F'(x,y)_1F'(x,y)_2,F(x+y)\rangle_f \,\geq\, 1-3\varepsilon. \end{array} " /></p>
<p>To conclude, use the triangle inequality as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%26+%5Cbig%5C%7CF%28x%29F%28y%29-F%28x%2By%29+%5Cbig%5C%7C_f%5E2+%5C%5C+%26+%26%5Cqquad%5Cleq+%5C%2C3%5CBig%28%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Cbig%5C%7C%28F%28x%29-F%27%28x%2Cy%29_1%29F%28y%29+%5Cbig%5C%7C_f%5E2%5C%5C+%26%26+%5Cqquad%5Cqquad+%2B+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Cbig%5C%7C%28F%28y%29-F%27%28x%2Cy%29_2%29F%27%28x%2Cy%29_1+%5Cbig%5C%7C_f%5E2%5C%5C+%26%26%5Cqquad%5Cqquad%2B%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Cbig%5C%7CF%27%28x%2Cy%29_1F%27%28x%2Cy%29_2-F%28x%2By%29+%5Cbig%5C%7C_f%5E2%5CBig%29%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} &amp; \big\|F(x)F(y)-F(x+y) \big\|_f^2 \\ &amp; &amp;\qquad\leq \,3\Big(\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|(F(x)-F'(x,y)_1)F(y) \big\|_f^2\\ &amp;&amp; \qquad\qquad + \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|(F(y)-F'(x,y)_2)F'(x,y)_1 \big\|_f^2\\ &amp;&amp;\qquad\qquad+\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|F'(x,y)_1F'(x,y)_2-F(x+y) \big\|_f^2\Big), \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} &amp; \big\|F(x)F(y)-F(x+y) \big\|_f^2 \\ &amp; &amp;\qquad\leq \,3\Big(\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|(F(x)-F'(x,y)_1)F(y) \big\|_f^2\\ &amp;&amp; \qquad\qquad + \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|(F(y)-F'(x,y)_2)F'(x,y)_1 \big\|_f^2\\ &amp;&amp;\qquad\qquad+\mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \big\|F'(x,y)_1F'(x,y)_2-F(x+y) \big\|_f^2\Big), \end{array} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7CA%5C%7C_f%5E2+%3D+%5Clangle+A%2CA%5Crangle_f%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\|A\|_f^2 = \langle A,A\rangle_f}" class="latex" title="{\|A\|_f^2 = \langle A,A\rangle_f}" /> denotes the dimension-normalized Frobenius norm. Expanding each squared norm and using the preceding conditions and <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%5E2%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x)^2=1}" class="latex" title="{F(x)^2=1}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> proves the lemma. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p><b>1.2. The BLR theorem for matrix-valued strategies</b></p>
<p>Before stating a BLR theorem for matrix-valued strategies we need to define what it means for such a function <img src="https://s0.wp.com/latex.php?latex=%7BG%3A+%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G: {\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" class="latex" title="{G: {\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" /> to be <em>linear</em>. Consider first the case of probabilistic functions, i.e. <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> such that all <img src="https://s0.wp.com/latex.php?latex=%7BG%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(x)}" class="latex" title="{G(x)}" /> are diagonal, in the same basis. Any such <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> whose every diagonal entry is of the form <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7BS%7D%28x%29+%3D+%28-1%29%5E%7BS+%5Ccdot+x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\chi_{S}(x) = (-1)^{S \cdot x}}" class="latex" title="{\chi_{S}(x) = (-1)^{S \cdot x}}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7BS%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S\in\{0,1\}^n}" class="latex" title="{S\in\{0,1\}^n}" /> <em>which may depend on the row/column number</em> will pass the BLR test. This shows that we cannot hope to force <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> to be a single linear function, we must allow “mixtures”. Formally, call <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> linear if <img src="https://s0.wp.com/latex.php?latex=%7BG%28x%29+%3D+%5Csum_S+%5Cchi_S%28x%29+P_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(x) = \sum_S \chi_S(x) P_S}" class="latex" title="{G(x) = \sum_S \chi_S(x) P_S}" /> for some decomposition <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BP_S%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{P_S\}}" class="latex" title="{\{P_S\}}" /> of the identity, i.e. the <img src="https://s0.wp.com/latex.php?latex=%7BP_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_S}" class="latex" title="{P_S}" /> are pairwsie orthogonal projections such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_S+P_S%3DI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_S P_S=I}" class="latex" title="{\sum_S P_S=I}" />. Note that this indeed captures the probabilistic case; in fact, up to a basis change it is essentially equivalent to it. Thus the following may come as a surprise.</p>
<blockquote><p><b>Theorem 2</b> <em><a name="thmblr"></a>Let <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> be an integer, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon\geq 0}" class="latex" title="{\varepsilon\geq 0}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BF%3A%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F:{\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" class="latex" title="{F:{\mathbb Z}_2^n \rightarrow O_d({\mathbb C})}" /> such that<a name="eqapprox-linear"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C+%5Cfrac%7B1%7D%7Bd%7D%5C%2C%5CRe%5C%2C%5Clangle+F%28x%29F%28y%29%2CF%28x%2By%29%5Crangle_f+%5C%2C%5Cgeq%5C%2C+1-%5Cvarepsilon.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \frac{1}{d}\,\Re\,\langle F(x)F(y),F(x+y)\rangle_f \,\geq\, 1-\varepsilon. \ \ \ \ \ (2)" class="latex" title="\displaystyle \mathop{\mathbb E}_{x,y\in {\mathbb Z}_2^n} \, \frac{1}{d}\,\Re\,\langle F(x)F(y),F(x+y)\rangle_f \,\geq\, 1-\varepsilon. \ \ \ \ \ (2)" /></p>
<p><em><a name="eqapprox-linear"></a>Then there exists a <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%5Cgeq+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'\geq d}" class="latex" title="{d'\geq d}" />, an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%3A%7B%5Cmathbb+C%7D%5Ed%5Crightarrow%7B%5Cmathbb+C%7D%5E%7Bd%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V:{\mathbb C}^d\rightarrow{\mathbb C}^{d'}}" class="latex" title="{V:{\mathbb C}^d\rightarrow{\mathbb C}^{d'}}" />, and a linear <img src="https://s0.wp.com/latex.php?latex=%7BG%3A%7B%5Cmathbb+Z%7D_2%5En+%5Crightarrow+O_%7Bd%27%7D%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G:{\mathbb Z}_2^n \rightarrow O_{d'}({\mathbb C})}" class="latex" title="{G:{\mathbb Z}_2^n \rightarrow O_{d'}({\mathbb C})}" /> such that</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%5Cin%7B%5Cmathbb+Z%7D_2%5En%7D+%5C%2C%5Cbig%5C%7C+F%28x%29+-+V%5E%2A+G%28x%29V%5Cbig%5C%7C_f%5E2+%5C%2C%5Cleq%5C%2C+2%5C%2C%5Cvarepsilon.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x\in{\mathbb Z}_2^n} \,\big\| F(x) - V^* G(x)V\big\|_f^2 \,\leq\, 2\,\varepsilon." class="latex" title="\displaystyle \mathop{\mathbb E}_{x\in{\mathbb Z}_2^n} \,\big\| F(x) - V^* G(x)V\big\|_f^2 \,\leq\, 2\,\varepsilon." /></p>
</blockquote>
<p>Note the role of <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> here, and the lack of control on <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> (more on both aspects later). Even if <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is a deterministic function <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" />, i.e. <img src="https://s0.wp.com/latex.php?latex=%7Bd%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=1}" class="latex" title="{d=1}" />, the function <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> returned by the theorem may be matrix-valued. In this case the isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> is simply a unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bv%5Cin+%7B%5Cmathbb+C%7D%5E%7Bd%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v\in {\mathbb C}^{d'}}" class="latex" title="{v\in {\mathbb C}^{d'}}" />, and expanding out the squared norm in the conclusion of the theorem yields the equivalent conclusion</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_S+%28v%5E%2A+P_S+v%29%5C%2C%5CBig%28%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%7D+f%28x%29%5C%2C+%5Cchi_S%28x%29+%5CBig%29+%5C%2C%5Cgeq%5C%2C+1-%5Cvarepsilon%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \sum_S (v^* P_S v)\,\Big(\mathop{\mathbb E}_{x} f(x)\, \chi_S(x) \Big) \,\geq\, 1-\varepsilon," class="latex" title="\displaystyle \sum_S (v^* P_S v)\,\Big(\mathop{\mathbb E}_{x} f(x)\, \chi_S(x) \Big) \,\geq\, 1-\varepsilon," /></p>
<p>where we expanded <img src="https://s0.wp.com/latex.php?latex=%7BG%28x%29+%3D+%5Csum_S+%5Cchi_S%28x%29+P_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(x) = \sum_S \chi_S(x) P_S}" class="latex" title="{G(x) = \sum_S \chi_S(x) P_S}" /> using our definition of a linear matrix-valued function. Note that <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+v%5E%2A+P_S+v%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ v^* P_S v\}}" class="latex" title="{\{ v^* P_S v\}}" /> defines a probability distribution on <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{0,1\}^n}" class="latex" title="{\{0,1\}^n}" />. Thus by an averaging argument there must exist an <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3D%5Cchi_S%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x)=\chi_S(x)}" class="latex" title="{f(x)=\chi_S(x)}" /> for a fraction at least <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon/2}" class="latex" title="{1-\varepsilon/2}" /> of all <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%7B%5Cmathbb+Z%7D_2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in{\mathbb Z}_2^n}" class="latex" title="{x\in{\mathbb Z}_2^n}" />: the usual conclusion of the BLR theorem is recovered.</p>
<p><em>Proof:</em> The proof of the theorem follows the classic <a href="http://ieeexplore.ieee.org/document/556674/">Fourier-analytic proof</a> of Bellare et al. Our first step is to define the isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" />. For a vector <img src="https://s0.wp.com/latex.php?latex=%7Bu%5Cin+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u\in {\mathbb C}^d}" class="latex" title="{u\in {\mathbb C}^d}" />, define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+V+u+%3D+%5Csum_S+%5Chat%7BF%7D%28S%29+u+%5Cotimes+e_S+%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5E%7B2%5En%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle V u = \sum_S \hat{F}(S) u \otimes e_S \in {\mathbb C}^d \otimes {\mathbb C}^{2^n}," class="latex" title="\displaystyle V u = \sum_S \hat{F}(S) u \otimes e_S \in {\mathbb C}^d \otimes {\mathbb C}^{2^n}," /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat%7BF%7D%28S%29+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%7D+%5Cchi_S%28x%29+F%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat{F}(S) = \mathop{\mathbb E}_{x} \chi_S(x) F(x)}" class="latex" title="{\hat{F}(S) = \mathop{\mathbb E}_{x} \chi_S(x) F(x)}" /> is the matrix-valued Fourier coefficient of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> at <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Be_S%5C%7D_%7BS%5Cin%5C%7B0%2C1%5C%7D%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{e_S\}_{S\in\{0,1\}^n}}" class="latex" title="{\{e_S\}_{S\in\{0,1\}^n}}" /> an arbitrary orthonormal basis of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5E%7B2%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^{2^n}}" class="latex" title="{{\mathbb C}^{2^n}}" />. An easily verified extension of Parseval’s formula shows <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_S+%5Chat%7BF%7D%28S%29%5E2+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_S \hat{F}(S)^2 = I}" class="latex" title="{\sum_S \hat{F}(S)^2 = I}" /> (recall <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%29%5E2%3DI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x)^2=I}" class="latex" title="{F(x)^2=I}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />), so that <img src="https://s0.wp.com/latex.php?latex=%7BV%5E%2AV+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V^*V = I}" class="latex" title="{V^*V = I}" />: <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> is indeed an isometry.</p>
<p>Next, define the linear probabilistic function <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> by <img src="https://s0.wp.com/latex.php?latex=%7BG%28x%29+%3D+%5Csum_S+%5Cchi_S%28x%29+P_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G(x) = \sum_S \chi_S(x) P_S}" class="latex" title="{G(x) = \sum_S \chi_S(x) P_S}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BP_S+%3D+I+%5Cotimes+e_Se_S%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_S = I \otimes e_Se_S^*}" class="latex" title="{P_S = I \otimes e_Se_S^*}" /> forms a partition of identity. We can evaluate</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%7D+%5C%2C%5Cfrac%7B1%7D%7Bd%7D%5C%2C%5Clangle+F%28x%29%2CV%5E%2AG%28x%29V+%5Crangle_f+%26%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%7D+%5Csum_%7BS%7D%5C%2C%5Cfrac%7B1%7D%7Bd%7D%5C%2C%5Clangle+F%28x%29%2C%5C%2C+%5Cchi_S%28x%29+%5Chat%7BF%7D%28S%29%5E2+%5Crangle_f+%5C%5C+%26%26%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%7D+%5C%2C%5Cfrac%7B1%7D%7Bd%7D%5C%2C%5Clangle+F%28x%2By%29%2C%5C%2CF%28x%29F%28y%29+%5Crangle_f%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x} \,\frac{1}{d}\,\langle F(x),V^*G(x)V \rangle_f &amp;= \mathop{\mathbb E}_{x} \sum_{S}\,\frac{1}{d}\,\langle F(x),\, \chi_S(x) \hat{F}(S)^2 \rangle_f \\ &amp;&amp;= \mathop{\mathbb E}_{x,y} \,\frac{1}{d}\,\langle F(x+y),\,F(x)F(y) \rangle_f, \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x} \,\frac{1}{d}\,\langle F(x),V^*G(x)V \rangle_f &amp;= \mathop{\mathbb E}_{x} \sum_{S}\,\frac{1}{d}\,\langle F(x),\, \chi_S(x) \hat{F}(S)^2 \rangle_f \\ &amp;&amp;= \mathop{\mathbb E}_{x,y} \,\frac{1}{d}\,\langle F(x+y),\,F(x)F(y) \rangle_f, \end{array} " /></p>
<p>where the last equality follows by expanding the Fourier coefficients and noticing the appropriate cancellation. Together with <a href="https://mycqstate.wordpress.com/feed/#eqapprox-linear">(2)</a>, this proves the theorem. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>At the risk of sounding yet more pedantic, it might be useful to comment on the relation between this proof and the usual argument. The main observation in Bellare et al.’s proof is that approximate linearity, expressed by <a href="https://mycqstate.wordpress.com/feed/#eqapprox-linear">(2)</a>, implies a lower bound on the sum of the <em>cubes</em> of the Fourier coefficients of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" />. Together with Parseval’s formula, this bound implies the existence of a large Fourier coefficient, which identifies a close-by linear function.</p>
<p>The proof I gave decouples the argument. Its first step, the construction of the isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> depends on <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, but does not use anything regarding approximate linearity. It only uses Parseval’s formula to argue that the isometry is well-defined. A noteworthy feature of this step is that the function <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> on the extended space is always well-defined as well: given a function <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, it is always possible to consider the linear matrix-valued function which “samples <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> according to <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat%7BF%7D%28S%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat{F}(S)^2}" class="latex" title="{\hat{F}(S)^2}" />” and then returns <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_S%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\chi_S(x)}" class="latex" title="{\chi_S(x)}" />. The second step of the proof evaluates the correlation of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> with the “pull-back” of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, and observes that this correlation is precisely our measure of “approximate linearity” of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, concluding the proof without having had to explicitly notice that there existed a large Fourier coefficient.</p>
<p><b>1.3. The group-theoretic perspective</b></p>
<p>Let’s re-interpret the proof we just gave using group-theoretic language. A linear function <img src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%7B%5Cmathbb+Z%7D_2%5En%5Crightarrow%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g: {\mathbb Z}_2^n\rightarrow\{\pm 1\}}" class="latex" title="{g: {\mathbb Z}_2^n\rightarrow\{\pm 1\}}" /> is, by definition, a mapping which respects the additive group structure on <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Z%7D_2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb Z}_2^n}" class="latex" title="{{\mathbb Z}_2^n}" />, namely it is a representation. Since <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28%7B%5Cmathbb+Z%7D_2%5En%2C%2B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=({\mathbb Z}_2^n,+)}" class="latex" title="{G=({\mathbb Z}_2^n,+)}" /> is an abelian group, it has <img src="https://s0.wp.com/latex.php?latex=%7B%7CG%7C%3D2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|G|=2^n}" class="latex" title="{|G|=2^n}" /> irreducible <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />-dimensional representations, given by the characters <img src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\chi_S}" class="latex" title="{\chi_S}" />. As such, the linear function defined in the proof of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmblr">2</a> is nothing but a list of all irreducible representations of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.</p>
<p>The condition <a href="https://mycqstate.wordpress.com/feed/#eqapprox-linear">(2)</a> derived in the proof of the theorem can be interpreted as the condition that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is an “approximate representation” of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Let’s make this a general definition. For <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />-dimensional matrices <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B}" class="latex" title="{A,B}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> is positive semidefinite, write</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clangle+A%2CB%5Crangle_%5Csigma+%3D+%5Cmathrm%7BTr%7D%28AB%5E%2A+%5Csigma%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \langle A,B\rangle_\sigma = \mathrm{Tr}(AB^* \sigma)," class="latex" title="\displaystyle \langle A,B\rangle_\sigma = \mathrm{Tr}(AB^* \sigma)," /></p>
<p>where we use <img src="https://s0.wp.com/latex.php?latex=%7BB%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B^*}" class="latex" title="{B^*}" /> to denote the conjugate-transpose. The following definition considers arbitrary finite groups (not necessarily abelian).</p>
<blockquote><p><b>Definition 3</b> <em>Given a finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, an integer <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\geq 1}" class="latex" title="{d\geq 1}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon\geq 0}" class="latex" title="{\varepsilon\geq 0}" />, and a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />-dimensional positive semidefinite matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> with trace <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />, an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cvarepsilon%2C%5Csigma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\varepsilon,\sigma)}" class="latex" title="{(\varepsilon,\sigma)}" />-representation of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+G+%5Crightarrow+U_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f: G \rightarrow U_d({\mathbb C})}" class="latex" title="{f: G \rightarrow U_d({\mathbb C})}" />, the unitary group of <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\times d}" class="latex" title="{d\times d}" /> matrices, such that<a name="eqgh-condition"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%5Cin+G%7D+%5C%2C%5CRe%5Cbig%28%5Cbig%5Clangle+f%28x%29%5E%2Af%28y%29+%2Cf%28x%5E%7B-1%7Dy%29+%5Cbig%5Crangle_%5Csigma%5Cbig%29+%5C%2C%5Cgeq%5C%2C+1-%5Cvarepsilon%2C+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x,y\in G} \,\Re\big(\big\langle f(x)^*f(y) ,f(x^{-1}y) \big\rangle_\sigma\big) \,\geq\, 1-\varepsilon, \ \ \ \ \ (3)" class="latex" title="\displaystyle \mathop{\mathbb E}_{x,y\in G} \,\Re\big(\big\langle f(x)^*f(y) ,f(x^{-1}y) \big\rangle_\sigma\big) \,\geq\, 1-\varepsilon, \ \ \ \ \ (3)" /></p>
<p><em><a name="eqgh-condition"></a>where the expectation is taken under the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.</em></p></blockquote>
<p>The condition <a href="https://mycqstate.wordpress.com/feed/#eqgh-condition">(3)</a> in the definition is very closely related to Gowers’s <img src="https://s0.wp.com/latex.php?latex=%7BU%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U^2}" class="latex" title="{U^2}" /> norm</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7Cf%5C%7C_%7BU%5E2%7D%5E4+%5C%2C%3D%5C%2C+%5Cmathop%7B%5Cmathbb+E%7D_%7Bxy%5E%7B-1%7D%3Dzw%5E%7B-1%7D%7D%5C%2C+%5Cbig%5Clangle+f%28x%29f%28y%29%5E%2A+%2Cf%28z%29f%28w%29%5E%2A+%5Cbig%5Crangle_%5Csigma.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \|f\|_{U^2}^4 \,=\, \mathop{\mathbb E}_{xy^{-1}=zw^{-1}}\, \big\langle f(x)f(y)^* ,f(z)f(w)^* \big\rangle_\sigma." class="latex" title="\displaystyle \|f\|_{U^2}^4 \,=\, \mathop{\mathbb E}_{xy^{-1}=zw^{-1}}\, \big\langle f(x)f(y)^* ,f(z)f(w)^* \big\rangle_\sigma." /></p>
<p>While a large Gowers norm implies closeness to an affine function, we are interested in testing linear functions, and the condition <a href="https://mycqstate.wordpress.com/feed/#eqgh-condition">(3)</a> will arise naturally from our calculations in the next section.</p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28%7B%5Cmathbb+Z%7D_2%5En%2C%2B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=({\mathbb Z}_2^n,+)}" class="latex" title="{G=({\mathbb Z}_2^n,+)}" />, the product <img src="https://s0.wp.com/latex.php?latex=%7Bxy%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{xy^{-1}}" class="latex" title="{xy^{-1}}" /> should be written additively as <img src="https://s0.wp.com/latex.php?latex=%7Bx-y%3Dx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x-y=x+y}" class="latex" title="{x-y=x+y}" />, so that the condition <a href="https://mycqstate.wordpress.com/feed/#eqapprox-linear">(2)</a> is precisely that <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cvarepsilon%2C%5Csigma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\varepsilon,\sigma)}" class="latex" title="{(\varepsilon,\sigma)}" />-representation of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%3D+d%5E%7B-1%7DI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma = d^{-1}I}" class="latex" title="{\sigma = d^{-1}I}" />. Theorem <a href="https://mycqstate.wordpress.com/feed/#thmblr">2</a> can thus be reformulated as stating that for any <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cvarepsilon%2C%5Csigma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\varepsilon,\sigma)}" class="latex" title="{(\varepsilon,\sigma)}" />-approximate representation of the abelian group <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%28%7B%5Cmathbb+Z%7D_2%5En%2C%2B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G=({\mathbb Z}_2^n,+)}" class="latex" title="{G=({\mathbb Z}_2^n,+)}" /> there exists an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%3A%7B%5Cmathbb+C%7D%5Ed+%5Crightarrow+%7B%5Cmathbb+C%7D%5Ed%5Cotimes+%7B%5Cmathbb+C%7D%5E%7B2%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V:{\mathbb C}^d \rightarrow {\mathbb C}^d\otimes {\mathbb C}^{2^n}}" class="latex" title="{V:{\mathbb C}^d \rightarrow {\mathbb C}^d\otimes {\mathbb C}^{2^n}}" /> and an exact representation <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> on <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5E%7B2%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^d \otimes {\mathbb C}^{2^n}}" class="latex" title="{{\mathbb C}^d \otimes {\mathbb C}^{2^n}}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is well-approximated by the “pull-back” <img src="https://s0.wp.com/latex.php?latex=%7BV%5E%2AgV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V^*gV}" class="latex" title="{V^*gV}" /> of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^d}" class="latex" title="{{\mathbb C}^d}" />. In the next section I will make the words in quotes precise and generalize the result to the case of arbitrary finite groups.</p>
<p><b>2. Approximate representations of non-abelian groups</b></p>
<p><b>2.1. The Gowers-Hatami theorem</b></p>
<p>In their paper Gowers and Hatami consider the problem of “rounding” approximate group representations to exact representations. I highly recommend the paper, which gives a thorough introduction to the topic, including multiple motivations. Here I will state and prove a slightly more general, but quantitatively weaker, variant of their result inspired by the somewhat convoluted analysis of the BLR test given in the previous section.</p>
<blockquote><p><b>Theorem 4 (Gowers-Hatami)</b> <em><a name="thmgh"></a>Let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> be a finite group, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon\geq 0}" class="latex" title="{\varepsilon\geq 0}" />, and <img src="https://s0.wp.com/latex.php?latex=%7Bf%3AG%5Crightarrow+U_d%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:G\rightarrow U_d({\mathbb C})}" class="latex" title="{f:G\rightarrow U_d({\mathbb C})}" /> an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cvarepsilon%2C%5Csigma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\varepsilon,\sigma)}" class="latex" title="{(\varepsilon,\sigma)}" />-representation of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Then there exists a <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%5Cgeq+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'\geq d}" class="latex" title="{d'\geq d}" />, an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%3A%7B%5Cmathbb+C%7D%5Ed%5Crightarrow+%7B%5Cmathbb+C%7D%5E%7Bd%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V:{\mathbb C}^d\rightarrow {\mathbb C}^{d'}}" class="latex" title="{V:{\mathbb C}^d\rightarrow {\mathbb C}^{d'}}" />, and a representation <img src="https://s0.wp.com/latex.php?latex=%7Bg%3AG%5Crightarrow+U_%7Bd%27%7D%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g:G\rightarrow U_{d'}({\mathbb C})}" class="latex" title="{g:G\rightarrow U_{d'}({\mathbb C})}" /> such that</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%5Cin+G%7D%5C%2C+%5Cbig%5C%7C+f%28x%29+-+V%5E%2Ag%28x%29V+%5Cbig%5C%7C_%5Csigma%5E2%5C%2C+%5Cleq%5C%2C+2%5C%2C%5Cvarepsilon.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x\in G}\, \big\| f(x) - V^*g(x)V \big\|_\sigma^2\, \leq\, 2\,\varepsilon." class="latex" title="\displaystyle \mathop{\mathbb E}_{x\in G}\, \big\| f(x) - V^*g(x)V \big\|_\sigma^2\, \leq\, 2\,\varepsilon." /></p>
</blockquote>
<p>Gowers and Hatami limit themselves to the case of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%3D+d%5E%7B-1%7DI_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma = d^{-1}I_d}" class="latex" title="{\sigma = d^{-1}I_d}" />, which corresponds to the dimension-normalized Frobenius norm. In this scenario they in addition obtain a tight control of the dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" />, and show that one can always take <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%5C+%3D+%281%2BO%28%5Cvarepsilon%29%29d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'\ = (1+O(\varepsilon))d}" class="latex" title="{d'\ = (1+O(\varepsilon))d}" /> in the theorem. I will give a much shorter proof than theirs (the proof is implicit in their argument) that does not seem to allow to recover this estimate. (It is possible to adapt their proof to keep a control of <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> even in the case of general <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />, but I will not explain this here.) Essentially the same proof as the one sketched below has been extended to some classes of infinite groups by De Chiffre, Ozawa and Thom in a <a href="https://arxiv.org/pdf/1706.04544.pdf">recent preprint</a>.</p>
<p>Note that, contrary to the BLR theorem, where the “embedding” is not strictly necessary (if <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" /> is small enough we can identify a single close-by linear function), as noted by Gowers and Hatami Theorem <a href="https://mycqstate.wordpress.com/feed/#thmgh">4</a> does not in general hold with <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%3Dd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'=d}" class="latex" title="{d'=d}" />. The reason is that it is possible for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> to have an approximate representation in some dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />, but no exact representation of the same dimension: to obtain an example of this, take any group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> that has all non-trivial irreducible representations of large enough dimension, and create an approximate representation in e.g. dimension one less by “cutting off” one row and column from an exact representation. The dimension normalization induced by the norm <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7C%5Ccdot%5C%7C_%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\|\cdot\|_\sigma}" class="latex" title="{\|\cdot\|_\sigma}" /> will barely notice this, but it will be impossible to “round” the approximate representation obtained to an exact one without modifying the dimension.</p>
<p>The necessity for the embedding helps distinguish the Gowers-Hatami result from other extensions of the linearity test to the non-abelian setting, such as the work by Ben-Or et al. on <a href="https://eccc.weizmann.ac.il/report/2004/052/">non-Abelian homomorphism testing</a> (I thank Oded Regev for pointing me to the paper). In that paper the authors show that a function <img src="https://s0.wp.com/latex.php?latex=%7Bf%3AG%5Crightarrow+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f:G\rightarrow H}" class="latex" title="{f:G\rightarrow H}" />, where <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> are finite non-abelian groups, which satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%5CPr%28+f%28x%29f%28y%29%3Df%28xy%29+%29+%5Cgeq+1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Pr( f(x)f(y)=f(xy) ) \geq 1-\varepsilon}" class="latex" title="{\Pr( f(x)f(y)=f(xy) ) \geq 1-\varepsilon}" />, is <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cvarepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\varepsilon)}" class="latex" title="{O(\varepsilon)}" />-close to a homomorphism <img src="https://s0.wp.com/latex.php?latex=%7Bg%3AG%5Crightarrow+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g:G\rightarrow H}" class="latex" title="{g:G\rightarrow H}" />. The main difference with the setting for the Gowers-Hatami result is that since <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> is finite, Ben-Or et al. use the Kronecker <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> function as distance on <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />. This allows them to employ combinatorial arguments, and provide a rounding procedure that does not need to modify the range space (<img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />). In contrast, here the unitary group is infinite.</p>
<p>The main ingredient needed to extend the analysis of the BLR test is an appropriate notion of Fourier transform over non-abelian groups. Given an irreducible representation <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%3A+G+%5Crightarrow+U_%7Bd_%5Crho%7D%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho: G \rightarrow U_{d_\rho}({\mathbb C})}" class="latex" title="{\rho: G \rightarrow U_{d_\rho}({\mathbb C})}" />, define<a name="eqfourier"></a></p>
<p align="center"><a name="eqfourier"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%28%5Crho%29+%5C%2C%3D%5C%2C+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%5Cin+G%7D+%5C%2Cf%28x%29+%5Cotimes+%5Coverline%7B%5Crho%28x%29%7D.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \hat{f}(\rho) \,=\, \mathop{\mathbb E}_{x\in G} \,f(x) \otimes \overline{\rho(x)}. \ \ \ \ \ (4)" class="latex" title="\displaystyle \hat{f}(\rho) \,=\, \mathop{\mathbb E}_{x\in G} \,f(x) \otimes \overline{\rho(x)}. \ \ \ \ \ (4)" /></p>
<p><a name="eqfourier"></a>In case <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is abelian, we always have <img src="https://s0.wp.com/latex.php?latex=%7Bd_%5Crho%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_\rho=1}" class="latex" title="{d_\rho=1}" />, the tensor product is a product, and <a href="https://mycqstate.wordpress.com/feed/#eqfourier">(4)</a> reduces to the usual definition of Fourier coefficient. The only properties we will need of irreducible representations is that they satisfy the relation<a name="eqortho"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%5Crho+%5C%2Cd_%5Crho%5C%2C%5Cmathrm%7BTr%7D%28%5Crho%28x%29%29+%5C%2C%3D%5C%2C+%7CG%7C%5Cdelta_%7Bxe%7D%5C%3B%2C+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \sum_\rho \,d_\rho\,\mathrm{Tr}(\rho(x)) \,=\, |G|\delta_{xe}\;, \ \ \ \ \ (5)" class="latex" title="\displaystyle \sum_\rho \,d_\rho\,\mathrm{Tr}(\rho(x)) \,=\, |G|\delta_{xe}\;, \ \ \ \ \ (5)" /></p>
<p><a name="eqortho"></a>for any <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in G}" class="latex" title="{x\in G}" />. Note that plugging in <img src="https://s0.wp.com/latex.php?latex=%7Bx%3De%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x=e}" class="latex" title="{x=e}" /> (the identity element in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />) yields <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%5Crho+d_%5Crho%5E2%3D+%7CG%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_\rho d_\rho^2= |G|}" class="latex" title="{\sum_\rho d_\rho^2= |G|}" />.</p>
<p><em>Proof:</em> } As in the proof of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmblr">2</a> our first step is to define an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%3A%7B%5Cmathbb+C%7D%5Ed+%5Crightarrow+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%28%5Coplus_%5Crho+%7B%5Cmathbb+C%7D%5E%7Bd_%5Crho%7D+%5Cotimes+%7B%5Cmathbb+C%7D%5E%7Bd_%5Crho%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V:{\mathbb C}^d \rightarrow {\mathbb C}^d \otimes (\oplus_\rho {\mathbb C}^{d_\rho} \otimes {\mathbb C}^{d_\rho})}" class="latex" title="{V:{\mathbb C}^d \rightarrow {\mathbb C}^d \otimes (\oplus_\rho {\mathbb C}^{d_\rho} \otimes {\mathbb C}^{d_\rho})}" /> by</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+V+%3A%5C%3Bu+%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5C%2C%5Cmapsto%5C%2C+%5Cbigoplus_%5Crho+%5C%2Cd_%5Crho%5E%7B1%2F2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bd_%5Crho%7D+%5C%2C%5Cbig%28%5Chat%7Bf%7D%28%5Crho%29+%28u%5Cotimes+e_i%29%5Cbig%29+%5Cotimes+e_i%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle V :\;u \in {\mathbb C}^d \,\mapsto\, \bigoplus_\rho \,d_\rho^{1/2} \sum_{i=1}^{d_\rho} \,\big(\hat{f}(\rho) (u\otimes e_i)\big) \otimes e_i," class="latex" title="\displaystyle V :\;u \in {\mathbb C}^d \,\mapsto\, \bigoplus_\rho \,d_\rho^{1/2} \sum_{i=1}^{d_\rho} \,\big(\hat{f}(\rho) (u\otimes e_i)\big) \otimes e_i," /></p>
<p>where the direct sum ranges over all irreducible representations <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Be_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{e_i\}}" class="latex" title="{\{e_i\}}" /> is the canonical basis. Note what <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> does: it “embeds” any vector <img src="https://s0.wp.com/latex.php?latex=%7Bu%5Cin+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u\in {\mathbb C}^d}" class="latex" title="{u\in {\mathbb C}^d}" /> into a direct sum, over irreducible representations <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" />, of a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />-dimensional vector of <img src="https://s0.wp.com/latex.php?latex=%7Bd_%5Crho%5Ctimes+d_%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_\rho\times d_\rho}" class="latex" title="{d_\rho\times d_\rho}" /> matrices. Each (matrix) entry of this vector can be thought of as the Fourier coefficient of the corresponding entry of the vector <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29u%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x)u}" class="latex" title="{f(x)u}" /> associated with <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho}" class="latex" title="{\rho}" />. If <img src="https://s0.wp.com/latex.php?latex=%7BG%3D%7B%5Cmathbb+Z%7D_2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G={\mathbb Z}_2^n}" class="latex" title="{G={\mathbb Z}_2^n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> ranges over <img src="https://s0.wp.com/latex.php?latex=%7BO_%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O_({\mathbb C})}" class="latex" title="{O_({\mathbb C})}" /> this recovers the isometry defined in the proof of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmblr">2</a>. And indeed, the fact that <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> is an isometry again follows from the appropriate extension of Parseval’s formula:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26+V%5E%2A+V+%26%3D+%5Csum_%5Crho+d_%5Crho+%5Csum_i+%28I%5Cotimes+e_i%5E%2A%29+%5Chat%7Bf%7D%28%5Crho%29%5E%2A%5Chat%7Bf%7D%28%5Crho%29+%28I%5Cotimes+e_i%29%5C%5C+%26%26%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%7D%5C%2C+f%28x%29%5E%2Af%28y%29+%5Csum_%5Crho+d_%5Crho+%5Csum_i+%28e_i%5E%2A+%5Crho%28x%29%5ET+%5Coverline%7B%5Crho%28y%29%7D+e_i%29%5C%5C+%26%26%3D+%5Csum_%5Crho+%5Cfrac%7Bd_%5Crho%5E2%7D%7B%7CG%7C%7DI+%3D+I%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp; V^* V &amp;= \sum_\rho d_\rho \sum_i (I\otimes e_i^*) \hat{f}(\rho)^*\hat{f}(\rho) (I\otimes e_i)\\ &amp;&amp;= \mathop{\mathbb E}_{x,y}\, f(x)^*f(y) \sum_\rho d_\rho \sum_i (e_i^* \rho(x)^T \overline{\rho(y)} e_i)\\ &amp;&amp;= \sum_\rho \frac{d_\rho^2}{|G|}I = I, \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp; V^* V &amp;= \sum_\rho d_\rho \sum_i (I\otimes e_i^*) \hat{f}(\rho)^*\hat{f}(\rho) (I\otimes e_i)\\ &amp;&amp;= \mathop{\mathbb E}_{x,y}\, f(x)^*f(y) \sum_\rho d_\rho \sum_i (e_i^* \rho(x)^T \overline{\rho(y)} e_i)\\ &amp;&amp;= \sum_\rho \frac{d_\rho^2}{|G|}I = I, \end{array} " /></p>
<p>where for the second line we used the definition <a href="https://mycqstate.wordpress.com/feed/#eqfourier">(4)</a> of <img src="https://s0.wp.com/latex.php?latex=%7B%5Chat%7Bf%7D%28%5Crho%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\hat{f}(\rho)}" class="latex" title="{\hat{f}(\rho)}" /> and for the third we used <a href="https://mycqstate.wordpress.com/feed/#eqortho">(5)</a> and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> takes values in the unitary group.</p>
<p>Following the same steps as in the proof of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmblr">2</a>, we next define</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+g%28x%29+%3D+%5Cbigoplus_%5Crho+%5C%2C%5Cbig%28I_d+%5Cotimes+I_%7Bd_%5Crho%7D+%5Cotimes+%5Crho%28x%29%5Cbig%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle g(x) = \bigoplus_\rho \,\big(I_d \otimes I_{d_\rho} \otimes \rho(x)\big), " class="latex" title="\displaystyle g(x) = \bigoplus_\rho \,\big(I_d \otimes I_{d_\rho} \otimes \rho(x)\big), " /></p>
<p>a direct sum over all irreducible representations of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> (hence itself a representation). Lets’ first compute the “pull-back” of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> by <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" />: following a similar calculation as above, for any <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x\in G}" class="latex" title="{x\in G}" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26+V%5E%2Ag%28x%29+V+%26%3D+%5Csum_%7B%5Crho%7D+d_%5Crho+%5Csum_%7Bi%2Cj%7D+%28I%5Cotimes+e_i%5E%2A%29%5Chat%7Bf%7D%28%5Crho%29%5E%2A+%5Chat%7Bf%7D%28%5Crho%29%28I%5Cotimes+e_j%29+%5Cotimes+e_i%5E%2A+%5Crho%28x%29+e_j+%29+%5C%5C+%26%26+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bz%2Cy%7D%5C%2C+f%28z%29%5E%2Af%28y%29+%5Csum_%7B%5Crho%7D+d_%5Crho+%5Csum_%7Bi%2Cj%7D+%28e_i%5E%2A+%5Crho%28z%29%5ET+%5Coverline%7B%5Crho%28y%29%7D+e_j%29+%5Cbig%28+e_i%5E%2A+%5Crho%28x%29+e_j+%5Cbig%29+%5C%5C+%26%26+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bz%2Cy%7D%5C%2C+f%28z%29%5E%2Af%28y%29+%5Csum_%7B%5Crho%7D+d_%5Crho+%5Cmathrm%7BTr%7D%5Cbig%28+%5Crho%28z%29%5ET+%5Coverline%7B%5Crho%28y%29%7D+%7B%5Crho%28x%29%5ET%7D+%5Cbig%29+%5C%5C+%26%26+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bz%2Cy%7D%5C%2C+f%28z%29%5E%2Af%28y%29+%5Csum_%7B%5Crho%7D+d_%5Crho+%5Cmathrm%7BTr%7D%5Cbig%28+%5Crho%28z%5E%7B-1%7Dy+x%5E%7B-1%7D%29+%5Cbig%29+%5C%5C+%26%26+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bz%7D%5C%2C+f%28z%29%5E%2Af%28zx%29+%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp; V^*g(x) V &amp;= \sum_{\rho} d_\rho \sum_{i,j} (I\otimes e_i^*)\hat{f}(\rho)^* \hat{f}(\rho)(I\otimes e_j) \otimes e_i^* \rho(x) e_j ) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \sum_{i,j} (e_i^* \rho(z)^T \overline{\rho(y)} e_j) \big( e_i^* \rho(x) e_j \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \mathrm{Tr}\big( \rho(z)^T \overline{\rho(y)} {\rho(x)^T} \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \mathrm{Tr}\big( \rho(z^{-1}y x^{-1}) \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z}\, f(z)^*f(zx) , \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp; V^*g(x) V &amp;= \sum_{\rho} d_\rho \sum_{i,j} (I\otimes e_i^*)\hat{f}(\rho)^* \hat{f}(\rho)(I\otimes e_j) \otimes e_i^* \rho(x) e_j ) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \sum_{i,j} (e_i^* \rho(z)^T \overline{\rho(y)} e_j) \big( e_i^* \rho(x) e_j \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \mathrm{Tr}\big( \rho(z)^T \overline{\rho(y)} {\rho(x)^T} \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z,y}\, f(z)^*f(y) \sum_{\rho} d_\rho \mathrm{Tr}\big( \rho(z^{-1}y x^{-1}) \big) \\ &amp;&amp; = \mathop{\mathbb E}_{z}\, f(z)^*f(zx) , \end{array} " /></p>
<p>where the last equality uses <a href="https://mycqstate.wordpress.com/feed/#eqortho">(5)</a>. It then follows that</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%7D%5C%2C+%5Cbig%5Clangle+f%28x%29%2C+V%5E%2Ag%28x%29+V+%5Cbig%5Crangle_%5Csigma+%26%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cz%7D+%5Cmathrm%7BTr%7D%5Cbig%28+f%28x%29+f%28zx%29%5E%2A+f%28z%29%5Csigma%5Cbig%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x}\, \big\langle f(x), V^*g(x) V \big\rangle_\sigma &amp;= \mathop{\mathbb E}_{x,z} \mathrm{Tr}\big( f(x) f(zx)^* f(z)\sigma\big). \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp;\mathop{\mathbb E}_{x}\, \big\langle f(x), V^*g(x) V \big\rangle_\sigma &amp;= \mathop{\mathbb E}_{x,z} \mathrm{Tr}\big( f(x) f(zx)^* f(z)\sigma\big). \end{array} " /></p>
<p>This relates correlation of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> with <img src="https://s0.wp.com/latex.php?latex=%7BV%5E%2AgV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V^*gV}" class="latex" title="{V^*gV}" /> to the quality of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> as an approximate representation and proves the theorem. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p><b>2.2. Application: the Weyl-Heisenberg group</b></p>
<p>In quantum information we care a lot about the <a href="https://en.wikipedia.org/wiki/Pauli_group">Pauli group</a>. For our purposes it will be be sufficient (and much more convenient, allowing us to avoid some trouble with complex conjugation) to consider the Weyl-Heisenberg group <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />, or “Pauli group modulo complex conjugation”, which is the <img src="https://s0.wp.com/latex.php?latex=%7B8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8}" class="latex" title="{8}" />-element group <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+%5Csigma_I%2C%5Cpm+%5Csigma_X%2C%5Cpm+%5Csigma_Z%2C%5Cpm+%5Csigma_W%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm \sigma_I,\pm \sigma_X,\pm \sigma_Z,\pm \sigma_W\}}" class="latex" title="{\{\pm \sigma_I,\pm \sigma_X,\pm \sigma_Z,\pm \sigma_W\}}" /> whose multiplication table matches that of the <img src="https://s0.wp.com/latex.php?latex=%7B2%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\times 2}" class="latex" title="{2\times 2}" /> matrices<a name="eqdef-pauli"></a></p>
<p align="center"><a name="eqdef-pauli"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csigma_X+%3D+%5Cbegin%7Bpmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bpmatrix%7D%2C%5Cqquad+%5Csigma_Z%3D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bpmatrix%7D%2C+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \sigma_X = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix},\qquad \sigma_Z= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}, \ \ \ \ \ (6)" class="latex" title="\displaystyle \sigma_X = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix},\qquad \sigma_Z= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}, \ \ \ \ \ (6)" /></p>
<p><a name="eqdef-pauli"></a><img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_I+%3D+%5Csigma_X%5E2+%3D+%5Csigma_Z%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_I = \sigma_X^2 = \sigma_Z^2}" class="latex" title="{\sigma_I = \sigma_X^2 = \sigma_Z^2}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_W%3D%5Csigma_X%5Csigma_Z%3D-%5Csigma_Z%5Csigma_X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_W=\sigma_X\sigma_Z=-\sigma_Z\sigma_X}" class="latex" title="{\sigma_W=\sigma_X\sigma_Z=-\sigma_Z\sigma_X}" />. This group has four <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />-dimensional representations, uniquely specified by the image of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_X}" class="latex" title="{\sigma_X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_Z}" class="latex" title="{\sigma_Z}" /> in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}}" class="latex" title="{\{\pm 1\}}" />, and a single irreducible <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />-dimensional representation, given by the matrices defined above. We can also consider the “<img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit Weyl-Heisenberg group” <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" />, the matrix group generated by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-fold tensor products of the <img src="https://s0.wp.com/latex.php?latex=%7B8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8}" class="latex" title="{8}" /> matrices identified above. The irreducible representations of <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" /> are easily computed from those of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" />; for us the only thing that matters is that the only irreducible representation which satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%5Crho%28-I%29%3D-%5Crho%28I%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\rho(-I)=-\rho(I)}" class="latex" title="{\rho(-I)=-\rho(I)}" /> has dimension <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" /> and is given by the defining matrix representation (in fact, it is the only irreducible representation in dimension larger than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />).</p>
<p>With the upcoming application to entanglement testing in mind, I will state a version of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmgh">4</a> tailored to the group <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" /> and a specific choice of presentation for the group relations. Towards this we first need to recall the notion of <em>Schmidt decomposition</em> of a bipartite state (i.e. unit vector) <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi+%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" />. The Schmidt decomposition states that any such vector can be written as<a name="eqschmidt"></a></p>
<p align="center"><a name="eqschmidt"></a><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cpsi+%5C%2C%3D%5C%2C+%5Csum_i+%5C%2C%5Csqrt%7B%5Clambda_i%7D%5C%2C+u_i+%5Cotimes+v_i%2C+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \psi \,=\, \sum_i \,\sqrt{\lambda_i}\, u_i \otimes v_i, \ \ \ \ \ (7)" class="latex" title="\displaystyle \psi \,=\, \sum_i \,\sqrt{\lambda_i}\, u_i \otimes v_i, \ \ \ \ \ (7)" /></p>
<p><a name="eqschmidt"></a>for some orthonomal bases <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bu_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{u_i\}}" class="latex" title="{\{u_i\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bv_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{v_i\}}" class="latex" title="{\{v_i\}}" /> of <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^d}" class="latex" title="{{\mathbb C}^d}" /> (the “Schmidt vectors”) and non-negative coefficients <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Clambda_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{\lambda_i}}" class="latex" title="{\sqrt{\lambda_i}}" /> (the “Schmidt coefficients”). The decomposition can be obtained by “reshaping” <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi+%3D+%5Csum_%7Bi%2Cj%7D+%5Cpsi_%7Bi%2Cj%7D+e_i+%5Cotimes+e_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi = \sum_{i,j} \psi_{i,j} e_i \otimes e_j}" class="latex" title="{\psi = \sum_{i,j} \psi_{i,j} e_i \otimes e_j}" /> into a <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\times d}" class="latex" title="{d\times d}" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7BK%3D%28%5Cpsi_%7Bi%2Cj%7D%29_%7B1%5Cleq+i%2Cj%5Cleq+d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K=(\psi_{i,j})_{1\leq i,j\leq d}}" class="latex" title="{K=(\psi_{i,j})_{1\leq i,j\leq d}}" /> and performing the singular value decomposition. To <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> we associate the (uniquely defined) positive semidefinite matrix<a name="eqsigma"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csigma+%5C%2C%3D%5C%2C+KK%5E%2A+%5C%2C%3D%5C%2C+%5Csum_i+%5Clambda_i%5C%2Cu_iu_i%5E%2A%5C%3B+%3B+%5C+%5C+%5C+%5C+%5C+%288%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \sigma \,=\, KK^* \,=\, \sum_i \lambda_i\,u_iu_i^*\; ; \ \ \ \ \ (8)" class="latex" title="\displaystyle \sigma \,=\, KK^* \,=\, \sum_i \lambda_i\,u_iu_i^*\; ; \ \ \ \ \ (8)" /></p>
<p><a name="eqsigma"></a>note that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> has trace <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. The matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> is called the <em>reduced density</em> of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> (on the first system).</p>
<blockquote><p><b>Corollary 5</b> <em><a name="corgh"></a>Let <img src="https://s0.wp.com/latex.php?latex=%7Bn%2Cd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n,d}" class="latex" title="{n,d}" /> be integer, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon \geq 0}" class="latex" title="{\varepsilon \geq 0}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi+%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" /> a unit vector, <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> the positive semidefinite matrix associated to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> as in <a href="https://mycqstate.wordpress.com/feed/#eqsigma">(8)</a>, and <img src="https://s0.wp.com/latex.php?latex=%7Bf%3A+%5C%7BX%2CZ%5C%7D%5Ctimes+%5C%7B0%2C1%5C%7D%5En+%5Crightarrow+U%28%7B%5Cmathbb+C%7D%5Ed%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f: \{X,Z\}\times \{0,1\}^n \rightarrow U({\mathbb C}^d)}" class="latex" title="{f: \{X,Z\}\times \{0,1\}^n \rightarrow U({\mathbb C}^d)}" />. For <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b\in\{0,1\}^n}" class="latex" title="{a,b\in\{0,1\}^n}" /> let <img src="https://s0.wp.com/latex.php?latex=%7BX%28a%29%3Df%28X%2Ca%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(a)=f(X,a)}" class="latex" title="{X(a)=f(X,a)}" />, <img src="https://s0.wp.com/latex.php?latex=%7BZ%28b%29%3Df%28Z%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z(b)=f(Z,b)}" class="latex" title="{Z(b)=f(Z,b)}" />, and assume <img src="https://s0.wp.com/latex.php?latex=%7BX%28a%29%5E2%3DZ%28b%29%5E2%3DI_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(a)^2=Z(b)^2=I_d}" class="latex" title="{X(a)^2=Z(b)^2=I_d}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b}" class="latex" title="{a,b}" /> (we call such operators, unitaries with eigenvalues in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%5Cpm+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{\pm 1\}}" class="latex" title="{\{\pm 1\}}" />, observables). Suppose that the following inequalities hold: consistency<a name="eqgh-cons"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_a+%5C%2C+%5Cpsi%5E%2A+%5Cbig%28X%28a%29+%5Cotimes+X%28a%29%5Cbig%29+%5Cpsi+%5C%2C%5Cgeq%5C%2C1-%5Cvarepsilon%2C%5Cqquad+%5Cmathop%7B%5Cmathbb+E%7D_b+%5C%2C+%5Cpsi%5E%2A+%5Cbig%28Z%28b%29+%5Cotimes+Z%28b%29+%5Cbig%29%5Cpsi%5C%2C%5Cgeq%5C%2C1-%5Cvarepsilon%2C+%5C+%5C+%5C+%5C+%5C+%289%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_a \, \psi^* \big(X(a) \otimes X(a)\big) \psi \,\geq\,1-\varepsilon,\qquad \mathop{\mathbb E}_b \, \psi^* \big(Z(b) \otimes Z(b) \big)\psi\,\geq\,1-\varepsilon, \ \ \ \ \ (9)" class="latex" title="\displaystyle \mathop{\mathbb E}_a \, \psi^* \big(X(a) \otimes X(a)\big) \psi \,\geq\,1-\varepsilon,\qquad \mathop{\mathbb E}_b \, \psi^* \big(Z(b) \otimes Z(b) \big)\psi\,\geq\,1-\varepsilon, \ \ \ \ \ (9)" /></p>
<p><em><a name="eqgh-cons"></a>linearity<a name="eqgh-commute"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Ca%27%7D+%5C%2C%5Cbig%5C%7CX%28a%29X%28a%27%29-X%28a%2Ba%27%29%5Cbig%5C%7C_%5Csigma%5E2+%5Cleq+%5Cvarepsilon%2C%5Cqquad%5Cmathop%7B%5Cmathbb+E%7D_%7Bb%2Cb%27%7D%5C%2C+%5Cbig%5C%7CZ%28b%29Z%28b%27%29-Z%28b%2Bb%27%29%5Cbig%5C%7C_%5Csigma%5E2+%5Cleq+%5Cvarepsilon%2C+%5C+%5C+%5C+%5C+%5C+%2810%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,a'} \,\big\|X(a)X(a')-X(a+a')\big\|_\sigma^2 \leq \varepsilon,\qquad\mathop{\mathbb E}_{b,b'}\, \big\|Z(b)Z(b')-Z(b+b')\big\|_\sigma^2 \leq \varepsilon, \ \ \ \ \ (10)" class="latex" title="\displaystyle \mathop{\mathbb E}_{a,a'} \,\big\|X(a)X(a')-X(a+a')\big\|_\sigma^2 \leq \varepsilon,\qquad\mathop{\mathbb E}_{b,b'}\, \big\|Z(b)Z(b')-Z(b+b')\big\|_\sigma^2 \leq \varepsilon, \ \ \ \ \ (10)" /></p>
<p><em><a name="eqgh-commute"></a>and anti-commutation<a name="eqgh-ac"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D+%5C%2C%5Cbig%5C%7C+X%28a%29Z%28b%29-%28-1%29%5E%7Ba%5Ccdot+b%7D+X%28a%29Z%28b%29%5Cbig%5C%7C_%5Csigma%5E2%5C%2C%5Cleq%5C%2C%5Cvarepsilon.+%5C+%5C+%5C+%5C+%5C+%2811%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b} \,\big\| X(a)Z(b)-(-1)^{a\cdot b} X(a)Z(b)\big\|_\sigma^2\,\leq\,\varepsilon. \ \ \ \ \ (11)" class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b} \,\big\| X(a)Z(b)-(-1)^{a\cdot b} X(a)Z(b)\big\|_\sigma^2\,\leq\,\varepsilon. \ \ \ \ \ (11)" /></p>
<p><em><a name="eqgh-ac"></a>Then there exists a <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%5Cgeq+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'\geq d}" class="latex" title="{d'\geq d}" />, an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%3A%7B%5Cmathbb+C%7D%5Ed%5Crightarrow+%7B%5Cmathbb+C%7D%5E%7Bd%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V:{\mathbb C}^d\rightarrow {\mathbb C}^{d'}}" class="latex" title="{V:{\mathbb C}^d\rightarrow {\mathbb C}^{d'}}" />, and a representation <img src="https://s0.wp.com/latex.php?latex=%7Bg%3AH%5E%7B%28n%29%7D%5Crightarrow+U_%7Bd%27%7D%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g:H^{(n)}\rightarrow U_{d'}({\mathbb C})}" class="latex" title="{g:H^{(n)}\rightarrow U_{d'}({\mathbb C})}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bg%28-I%29%3D-I_%7Bd%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(-I)=-I_{d'}}" class="latex" title="{g(-I)=-I_{d'}}" /> and</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D%5C%2C+%5Cbig%5C%7C+X%28a%29Z%28b%29+-+V%5E%2Ag%28%5Csigma_X%28a%29%5Csigma_Z%28b%29%29V+%5Cbig%5C%7C_%5Csigma%5E2+%5C%2C%3D%5C%2C+O%28%5Cvarepsilon%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b}\, \big\| X(a)Z(b) - V^*g(\sigma_X(a)\sigma_Z(b))V \big\|_\sigma^2 \,=\, O(\varepsilon)." class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b}\, \big\| X(a)Z(b) - V^*g(\sigma_X(a)\sigma_Z(b))V \big\|_\sigma^2 \,=\, O(\varepsilon)." /></p>
</blockquote>
<p>Note that the conditions <a href="https://mycqstate.wordpress.com/feed/#eqgh-commute">(10)</a> and <a href="https://mycqstate.wordpress.com/feed/#eqgh-ac">(11)</a> in the corollary are very similar to the conditions required of an approximate representation of the group <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" />; in fact it is easy to convince oneself that their exact analogue suffice to imply all the group relations. The reason for including only those relations is that they are the ones that it will be possible to test; see the next section for this. Condition <a href="https://mycqstate.wordpress.com/feed/#eqgh-cons">(9)</a> is necessary to derive the conditions of Theorem <a href="https://mycqstate.wordpress.com/feed/#thmgh">4</a> from <a href="https://mycqstate.wordpress.com/feed/#eqgh-commute">(10)</a> and <a href="https://mycqstate.wordpress.com/feed/#eqgh-ac">(11)</a>, and is also testable; see the proof.</p>
<p><em>Proof:</em> To apply Theorem <a href="https://mycqstate.wordpress.com/feed/#thmgh">4</a> we need to construct an <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cvarepsilon%2C%5Csigma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\varepsilon,\sigma)}" class="latex" title="{(\varepsilon,\sigma)}" />-representation <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> of the group <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" />. Using that any element of <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" /> has a unique representative of the form <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Csigma_X%28a%29%5Csigma_Z%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm \sigma_X(a)\sigma_Z(b)}" class="latex" title="{\pm \sigma_X(a)\sigma_Z(b)}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b\in\{0,1\}^n}" class="latex" title="{a,b\in\{0,1\}^n}" />, we define <img src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Cpm+%5Csigma_X%28a%29%5Csigma_Z%28b%29%29+%3D+%5Cpm+X%28a%29Z%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(\pm \sigma_X(a)\sigma_Z(b)) = \pm X(a)Z(b)}" class="latex" title="{f(\pm \sigma_X(a)\sigma_Z(b)) = \pm X(a)Z(b)}" />. Next we need to verify <a href="https://mycqstate.wordpress.com/feed/#eqgh-condition">(3)</a>. Let <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%5Cin+H%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x,y\in H^{(n)}}" class="latex" title="{x,y\in H^{(n)}}" /> be such that <img src="https://s0.wp.com/latex.php?latex=%7Bx%3D%5Csigma_X%28a_x%29%5Csigma_Z%28b_x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x=\sigma_X(a_x)\sigma_Z(b_x)}" class="latex" title="{x=\sigma_X(a_x)\sigma_Z(b_x)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%3D%5Csigma_X%28a_y%29%5Csigma_Z%28b_y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y=\sigma_X(a_y)\sigma_Z(b_y)}" class="latex" title="{y=\sigma_X(a_y)\sigma_Z(b_y)}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit strings <img src="https://s0.wp.com/latex.php?latex=%7B%28a_x%2Cb_x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a_x,b_x)}" class="latex" title="{(a_x,b_x)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%28a_y%2Cb_y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(a_y,b_y)}" class="latex" title="{(a_y,b_y)}" /> respectively. Up to phase, we can exploit successive cancellations to decompose <img src="https://s0.wp.com/latex.php?latex=%7B%28f%28x%29f%28y%29%5E%2A-f%28xy%5E%7B-1%7D%29%29%5Cotimes+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(f(x)f(y)^*-f(xy^{-1}))\otimes I}" class="latex" title="{(f(x)f(y)^*-f(xy^{-1}))\otimes I}" /> as</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Brcl%7D+%26%26%5Cbig%28+X%28a_x%29Z%28b_x%29X%28a_y%29Z%28b_y%29+-%28-1%29%5E%7Ba_y%5Ccdot+b_x%7D+X%28a_x%2Ba_y%29+Z%28b_x%2Bb_y%29%5Cbig%29%5Cotimes+I+%5C%5C+%26%26%5Cqquad+%3D+X%28a_x%29Z%28b_x%29X%28a_y%29%5Cbig+%28Z%28b_y%29%5Cotimes+I+-+I%5Cotimes+Z%28b_y%29%5Cbig%29%5C%5C+%26%26+%5Cqquad%5Cqquad%2B+X%28a_x%29%5Cbig%28Z%28b_x%29X%28a_y%29+-+%28-1%29%5E%7Ba_y%5Ccdot+b_x%7D+X%28a_y%29Z%28b_x%29%5Cbig%29%5Cotimes+Z%28b_y%29%5C%5C+%26%26+%5Cqquad%5Cqquad%2B%28-1%29%5E%7Ba_y%5Ccdot+b_x%7D+%5Cbig%28+X%28a_x%29X%28a_y%29%5Cotimes+Z%28b_y%29%5Cbig%29+%5Cbig%28+Z%28b_x%29%5Cotimes+I+-+I%5Cotimes+Z%28b_x%29%5Cbig%29%5C%5C+%26%26+%5Cqquad%5Cqquad%2B+%28-1%29%5E%7Ba_y%5Ccdot+b_x%7D+%5Cbig%28+X%28a_x%29X%28a_y%29%5Cotimes+Z%28b_y%29Z%28b_x%29+-+X%28a_x%2Ba_y%29%5Cotimes+Z%28b_x%2Bb_y%29%5Cbig%29%5C%5C+%26%26+%5Cqquad%5Cqquad%2B+%28-1%29%5E%7Ba_y%5Ccdot+b_x%7D+%5Cbig%28+X%28a_x%2Ba_y%29%5Cotimes+I+%5Cbig%29%5Cbig%28I%5Cotimes+Z%28b_x%2Bb_y%29+-+Z%28b_x%2Bb_y%29%5Cotimes+I%5Cbig%29.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \begin{array}{rcl} &amp;&amp;\big( X(a_x)Z(b_x)X(a_y)Z(b_y) -(-1)^{a_y\cdot b_x} X(a_x+a_y) Z(b_x+b_y)\big)\otimes I \\ &amp;&amp;\qquad = X(a_x)Z(b_x)X(a_y)\big (Z(b_y)\otimes I - I\otimes Z(b_y)\big)\\ &amp;&amp; \qquad\qquad+ X(a_x)\big(Z(b_x)X(a_y) - (-1)^{a_y\cdot b_x} X(a_y)Z(b_x)\big)\otimes Z(b_y)\\ &amp;&amp; \qquad\qquad+(-1)^{a_y\cdot b_x} \big( X(a_x)X(a_y)\otimes Z(b_y)\big) \big( Z(b_x)\otimes I - I\otimes Z(b_x)\big)\\ &amp;&amp; \qquad\qquad+ (-1)^{a_y\cdot b_x} \big( X(a_x)X(a_y)\otimes Z(b_y)Z(b_x) - X(a_x+a_y)\otimes Z(b_x+b_y)\big)\\ &amp;&amp; \qquad\qquad+ (-1)^{a_y\cdot b_x} \big( X(a_x+a_y)\otimes I \big)\big(I\otimes Z(b_x+b_y) - Z(b_x+b_y)\otimes I\big). \end{array} " class="latex" title="\displaystyle \begin{array}{rcl} &amp;&amp;\big( X(a_x)Z(b_x)X(a_y)Z(b_y) -(-1)^{a_y\cdot b_x} X(a_x+a_y) Z(b_x+b_y)\big)\otimes I \\ &amp;&amp;\qquad = X(a_x)Z(b_x)X(a_y)\big (Z(b_y)\otimes I - I\otimes Z(b_y)\big)\\ &amp;&amp; \qquad\qquad+ X(a_x)\big(Z(b_x)X(a_y) - (-1)^{a_y\cdot b_x} X(a_y)Z(b_x)\big)\otimes Z(b_y)\\ &amp;&amp; \qquad\qquad+(-1)^{a_y\cdot b_x} \big( X(a_x)X(a_y)\otimes Z(b_y)\big) \big( Z(b_x)\otimes I - I\otimes Z(b_x)\big)\\ &amp;&amp; \qquad\qquad+ (-1)^{a_y\cdot b_x} \big( X(a_x)X(a_y)\otimes Z(b_y)Z(b_x) - X(a_x+a_y)\otimes Z(b_x+b_y)\big)\\ &amp;&amp; \qquad\qquad+ (-1)^{a_y\cdot b_x} \big( X(a_x+a_y)\otimes I \big)\big(I\otimes Z(b_x+b_y) - Z(b_x+b_y)\otimes I\big). \end{array} " /></p>
<p>(It is worth staring at this sequence of equations for a little bit. In particular, note the “player-switching” that takes place in the 2nd, 4th and 6th lines; this is used as a means to “commute” the appropriate unitaries, and is the reason for including <a href="https://mycqstate.wordpress.com/feed/#eqgh-cons">(9)</a> among the assumptions of the corollary.) Evaluating each term on the vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" />, taking the squared Euclidean norm, and then the expectation over uniformly random <img src="https://s0.wp.com/latex.php?latex=%7Ba_x%2Ca_y%2Cb_x%2Cb_y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a_x,a_y,b_x,b_y}" class="latex" title="{a_x,a_y,b_x,b_y}" />, the inequality <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7C+AB%5Cpsi%5C%7C+%5Cleq+%5C%7CA%5C%7C%5C%7CB%5Cpsi%5C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\| AB\psi\| \leq \|A\|\|B\psi\|}" class="latex" title="{\| AB\psi\| \leq \|A\|\|B\psi\|}" /> and the assumptions of the theorem let us bound the overlap of each term in the resulting summation by <img src="https://s0.wp.com/latex.php?latex=%7BO%28%7B%5Cvarepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O({\varepsilon})}" class="latex" title="{O({\varepsilon})}" />. Using <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7C+%28A%5Cotimes+I%29+%5Cpsi%5C%7C+%3D+%5C%7CA%5C%7C_%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\| (A\otimes I) \psi\| = \|A\|_\sigma}" class="latex" title="{\| (A\otimes I) \psi\| = \|A\|_\sigma}" /> by definition, we obtain the bound</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%2Cy%7D%5C%2C%5Cbig%5C%7Cf%28x%29f%28y%29%5E%2A+-+f%28xy%5E%7B-1%7D%29%5Cbig%5C%7C_%5Csigma%5E2+%5C%2C%3D%5C%2C+O%28%7B%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{x,y}\,\big\|f(x)f(y)^* - f(xy^{-1})\big\|_\sigma^2 \,=\, O({\varepsilon})." class="latex" title="\displaystyle \mathop{\mathbb E}_{x,y}\,\big\|f(x)f(y)^* - f(xy^{-1})\big\|_\sigma^2 \,=\, O({\varepsilon})." /></p>
<p>We are thus in a position to apply Theorem <a href="https://mycqstate.wordpress.com/feed/#thmgh">4</a>, which gives an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> and exact representation <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> such that<a name="eqgi"></a></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D%5C%2C%5CBig%5C%7C+X%28a%29Z%28b%29-+%5Cfrac%7B1%7D%7B2%7DV%5E%2A%5Cbig%28+g%28%5Csigma_X%28a%29%5Csigma_Z%28b%29%29+-+g%28-%5Csigma_X%28a%29%5Csigma_Z%28b%29%29%5Cbig%29V%5CBig%5C%7C_%5Csigma%5E2+%5C%2C%3D%5C%2C+O%28%7B%5Cvarepsilon%7D%29.+%5C+%5C+%5C+%5C+%5C+%2812%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b}\,\Big\| X(a)Z(b)- \frac{1}{2}V^*\big( g(\sigma_X(a)\sigma_Z(b)) - g(-\sigma_X(a)\sigma_Z(b))\big)V\Big\|_\sigma^2 \,=\, O({\varepsilon}). \ \ \ \ \ (12)" class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b}\,\Big\| X(a)Z(b)- \frac{1}{2}V^*\big( g(\sigma_X(a)\sigma_Z(b)) - g(-\sigma_X(a)\sigma_Z(b))\big)V\Big\|_\sigma^2 \,=\, O({\varepsilon}). \ \ \ \ \ (12)" /></p>
<p><a name="eqgi"></a>Using that <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" /> is a representation, <img src="https://s0.wp.com/latex.php?latex=%7Bg%28-%5Csigma_X%28a%29%5Csigma_Z%28b%29%29+%3D+g%28-I%29g%28%5Csigma_X%28a%29%5Csigma_Z%28b%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(-\sigma_X(a)\sigma_Z(b)) = g(-I)g(\sigma_X(a)\sigma_Z(b))}" class="latex" title="{g(-\sigma_X(a)\sigma_Z(b)) = g(-I)g(\sigma_X(a)\sigma_Z(b))}" />. It follows from <a href="https://mycqstate.wordpress.com/feed/#eqgi">(12)</a> that <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cg%28-I%29+%2B+I+%5C%7C_%5Csigma%5E2+%3D+O%28%7B%5Cvarepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\|g(-I) + I \|_\sigma^2 = O({\varepsilon})}" class="latex" title="{\|g(-I) + I \|_\sigma^2 = O({\varepsilon})}" />, so we may restrict the range of <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> to the subspace where <img src="https://s0.wp.com/latex.php?latex=%7Bg%28-I%29%3D-I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g(-I)=-I}" class="latex" title="{g(-I)=-I}" /> without introducing much additional error. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p><b>3. Entanglement testing</b></p>
<p>Our discussion so far has barely touched upon the notion of entanglement. Recall the Schmidt decopmosition <a href="https://mycqstate.wordpress.com/feed/#eqschmidt">(7)</a> of a unit vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi+%5Cin+%7B%5Cmathbb+C%7D%5Ed%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi \in {\mathbb C}^d\otimes {\mathbb C}^d}" class="latex" title="{\psi \in {\mathbb C}^d\otimes {\mathbb C}^d}" />, and the associated reduced density matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" /> defined in <a href="https://mycqstate.wordpress.com/feed/#eqsigma">(8)</a>. The state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> is called <em>entangled</em> if this matrix has rank larger than <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />; equivalently, if there is more than one non-zero coefficient <img src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\lambda_i}" class="latex" title="{\lambda_i}" /> in <a href="https://mycqstate.wordpress.com/feed/#eqschmidt">(7)</a>. The <em>Schmidt rank</em> of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> is the rank of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma}" class="latex" title="{\sigma}" />, the number of non-zero terms in <a href="https://mycqstate.wordpress.com/feed/#eqschmidt">(7)</a>. It is a crude, but convenient, measure of entanglement; in particular it provides a lower bound on the local dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />. A useful observation is that the Schmidt rank is invariant under local unitary operations: these may affect the Schmidt vectors <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bu_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{u_i\}}" class="latex" title="{\{u_i\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bv_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{v_i\}}" class="latex" title="{\{v_i\}}" />, but not the number of non-zero terms.</p>
<p><b>3.1. A certificate for high-dimensional entanglement</b></p>
<p>Among all entangled states in dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />, the <em>maximally entangled state</em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi_d}" class="latex" title="{\phi_d}" /> is the one which maximizes entanglement entropy, defined as the Shannon entropy of the distribution induced by the squares of the Schmidt coefficients:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cphi_d+%5C%2C%3D%5C%2C+%5Cfrac%7B1%7D%7B%5Csqrt%7Bd%7D%7D+%5Csum_%7Bi%3D1%7D%5Ed%5C%2C+e_i%5Cotimes+e_i%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \phi_d \,=\, \frac{1}{\sqrt{d}} \sum_{i=1}^d\, e_i\otimes e_i," class="latex" title="\displaystyle \phi_d \,=\, \frac{1}{\sqrt{d}} \sum_{i=1}^d\, e_i\otimes e_i," /></p>
<p>with entropy <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log d}" class="latex" title="{\log d}" />. The following lemma gives a “robust” characterization of the maximally entangled state in dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%3D2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=2^n}" class="latex" title="{d=2^n}" /> as the unique common eigenvalue-<img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> eigenvector of all operators of the form <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_P+%5Cotimes+%5Csigma_P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_P \otimes \sigma_P}" class="latex" title="{\sigma_P \otimes \sigma_P}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_P}" class="latex" title="{\sigma_P}" /> ranges over the elements of the unique <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" />-dimensional irreducible representation of the Weyl-Heisenberg group <img src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H^{(n)}}" class="latex" title="{H^{(n)}}" />, i.e. the Pauli matrices (taken modulo <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{-1}}" class="latex" title="{\sqrt{-1}}" />).</p>
<blockquote><p><b>Lemma 6</b> <em><a name="lemsr"></a>Let <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon\geq 0}" class="latex" title="{\varepsilon\geq 0}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> an integer, <img src="https://s0.wp.com/latex.php?latex=%7Bd%3D2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=2^n}" class="latex" title="{d=2^n}" />, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in {\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{\psi\in {\mathbb C}^d \otimes {\mathbb C}^d}" /> a unit vector such that<a name="eqlem-ass"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D%5C%2C+%5Cpsi%5E%2A+%5Cbig%28%5Csigma_X%28a%29+%5Csigma_Z%28b%29%5Cotimes+%5Csigma_X%28a%29+%5Csigma_Z%28b%29+%5Cbig%29+%5Cpsi+%5Cgeq+1-%5Cvarepsilon.+%5C+%5C+%5C+%5C+%5C+%2813%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b}\, \psi^* \big(\sigma_X(a) \sigma_Z(b)\otimes \sigma_X(a) \sigma_Z(b) \big) \psi \geq 1-\varepsilon. \ \ \ \ \ (13)" class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b}\, \psi^* \big(\sigma_X(a) \sigma_Z(b)\otimes \sigma_X(a) \sigma_Z(b) \big) \psi \geq 1-\varepsilon. \ \ \ \ \ (13)" /></p>
<p><em><a name="eqlem-ass"></a>Then <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5E%2A%5Cphi_%7B2%5En%7D%7C%5E2+%5Cgeq+1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi^*\phi_{2^n}|^2 \geq 1-\varepsilon}" class="latex" title="{|\psi^*\phi_{2^n}|^2 \geq 1-\varepsilon}" />. In particular, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> has Schmidt rank at least <img src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cvarepsilon%29+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-\varepsilon) 2^n}" class="latex" title="{(1-\varepsilon) 2^n}" />.</em></p></blockquote>
<p><em>Proof:</em> Consider the case <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=1}" class="latex" title="{n=1}" />. The “swap” matrix</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+S+%3D+%5Cfrac%7B1%7D%7B4%7D%5Cbig%28%5Csigma_I+%5Cotimes+%5Csigma_I+%2B+%5Csigma_X+%5Cotimes+%5Csigma_X+%2B+%5Csigma_Z+%5Cotimes+%5Csigma_Z+%2B+%5Csigma_W+%5Cotimes+%5Csigma_W%5Cbig%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle S = \frac{1}{4}\big(\sigma_I \otimes \sigma_I + \sigma_X \otimes \sigma_X + \sigma_Z \otimes \sigma_Z + \sigma_W \otimes \sigma_W\big)" class="latex" title="\displaystyle S = \frac{1}{4}\big(\sigma_I \otimes \sigma_I + \sigma_X \otimes \sigma_X + \sigma_Z \otimes \sigma_Z + \sigma_W \otimes \sigma_W\big)" /></p>
<p>squares to identity and has a unique eigenvalue-<img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> eigenvector, the vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_2+%3D+%28e_1%5Cotimes+e_1+%2B+e_2%5Cotimes+e_2%29%2F%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\phi_2 = (e_1\otimes e_1 + e_2\otimes e_2)/\sqrt{2}}" class="latex" title="{\phi_2 = (e_1\otimes e_1 + e_2\otimes e_2)/\sqrt{2}}" /> (a.k.a. “EPR pair”). Thus <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5E%2A+S+%5Cpsi+%5Cgeq+1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi^* S \psi \geq 1-\varepsilon}" class="latex" title="{\psi^* S \psi \geq 1-\varepsilon}" /> implies <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5E%2A+%5Cphi%7C%5E2+%5Cgeq+1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi^* \phi|^2 \geq 1-\varepsilon}" class="latex" title="{|\psi^* \phi|^2 \geq 1-\varepsilon}" />. The same argument for general <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> shows <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5E%2A+%5Cphi_%7B2%5En%7D%7C%5E2+%5Cgeq+1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi^* \phi_{2^n}|^2 \geq 1-\varepsilon}" class="latex" title="{|\psi^* \phi_{2^n}|^2 \geq 1-\varepsilon}" />. Any unit vector <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> of Schmidt rank at most <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%7Cu%5E%2A+%5Cphi_%7B2%5En%7D%7C%5E2+%5Cleq+r2%5E%7B-n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|u^* \phi_{2^n}|^2 \leq r2^{-n}}" class="latex" title="{|u^* \phi_{2^n}|^2 \leq r2^{-n}}" />, concluding the proof. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>Lemma <a href="https://mycqstate.wordpress.com/feed/#lemsr">6</a> provides an “experimental road-map” for establishing that a bipartite system is in a highly entangled state:</p>
<ul>
<li>(i) Select a random <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_P+%3D+%5Cpm%5Csigma_X%28a%29%5Csigma_Z%28b%29+%5Cin+H%5E%7B%28n%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_P = \pm\sigma_X(a)\sigma_Z(b) \in H^{(n)}}" class="latex" title="{\sigma_P = \pm\sigma_X(a)\sigma_Z(b) \in H^{(n)}}" />;</li>
<li>(ii) Measure both halves of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> using <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_P}" class="latex" title="{\sigma_P}" />;</li>
<li>(iii) Check that the outcomes agree.</li>
</ul>
<p>To explain the connection between the above “operational test” and the lemma I should review what a measurement in quantum mechanics is. For our purposes it is enough to talk about binary measurements (i.e. measurements with two outcomes, <img src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+1}" class="latex" title="{+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{-1}" class="latex" title="{-1}" />). Any such measurement is specified by a pair of orthogonal projections, <img src="https://s0.wp.com/latex.php?latex=%7BM_%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M_+}" class="latex" title="{M_+}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BM_-%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M_-}" class="latex" title="{M_-}" />, on <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^d}" class="latex" title="{{\mathbb C}^d}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BM_%2B%2BM_-+%3D+I_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M_++M_- = I_d}" class="latex" title="{M_++M_- = I_d}" />. The probability of obtaining outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\pm}" class="latex" title="{\pm}" /> when measuring <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7CM_%5Cpm+%5Cpsi%5C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\|M_\pm \psi\|^2}" class="latex" title="{\|M_\pm \psi\|^2}" />. We can represent a binary measurement succinctly through the <em>observable</em> <img src="https://s0.wp.com/latex.php?latex=%7BM%3DM_%2B-M_-%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M=M_+-M_-}" class="latex" title="{M=M_+-M_-}" />. (In general, an observable is a Hermitian matrix which squares to identity.) It is then the case that if an observable <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> is applied on the first half of a state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin%7B%5Cmathbb+C%7D%5Ed%5Cotimes%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in{\mathbb C}^d\otimes{\mathbb C}^d}" class="latex" title="{\psi\in{\mathbb C}^d\otimes{\mathbb C}^d}" />, and another observable <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> is applied on the second half, then the probability of agreement, minus the probability of disagreement, between the outcomes obtained is precisely <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5E%2A%28M%5Cotimes+N%29%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi^*(M\otimes N)\psi}" class="latex" title="{\psi^*(M\otimes N)\psi}" />, a number which lies in <img src="https://s0.wp.com/latex.php?latex=%7B%5B-1%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{[-1,1]}" class="latex" title="{[-1,1]}" />. Thus the condition that the test described above accepts with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" /> when performed on a state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> is precisely equivalent to the assumption <a href="https://mycqstate.wordpress.com/feed/#eqlem-ass">(13)</a> of Lemma <a href="https://mycqstate.wordpress.com/feed/#lemsr">6</a>.</p>
<p>Even though this provides a perfectly fine test for entanglement in principle, practitioners in the foundations of quantum mechanics know all too well that their opponents — e.g. “quantum-skeptics” — will not be satisfied with such an experiment. In particular, who is to guarantee that the measurement performed in step (ii) is really <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_P%5Cotimes%5Csigma_P%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_P\otimes\sigma_P}" class="latex" title="{\sigma_P\otimes\sigma_P}" />, as claimed? To the least, doesn’t this already implicitly assume that the measured system has dimension <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" />?</p>
<p>This is where the notion of <em>device independence</em> comes in. Briefly, in this context the idea is to obtain the same conclusion (a certificate of high-dimensional entanglement) <em>without</em> any assumption on the measurement performed: the only information to be trusted is classical data (statistics generated by the experiment), but not the operational details of the experiment itself.</p>
<p>This is where Corollary <a href="https://mycqstate.wordpress.com/feed/#corgh">5</a> enters the picture. Reformulated in the present context, the corollary provides a means to <em>verify</em> that arbitrary measurements “all but behave” as Pauli measurements, provided they generate the right statistics. To explain how this can be done we need to provide additional “operational tests” that can be used to certify the assumptions of the corollary.</p>
<p><b>3.2. Testing the Weyl-Heisenberg group relations</b></p>
<p>Corollary <a href="https://mycqstate.wordpress.com/feed/#corgh">5</a> makes three assumptions about the observables <img src="https://s0.wp.com/latex.php?latex=%7BX%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(a)}" class="latex" title="{X(a)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z(b)}" class="latex" title="{Z(b)}" />: that they satisfy approximate consistency <a href="https://mycqstate.wordpress.com/feed/#eqgh-cons">(9)</a>, linearity <a href="https://mycqstate.wordpress.com/feed/#eqgh-commute">(10)</a>, and anti-commutation <a href="https://mycqstate.wordpress.com/feed/#eqgh-ac">(11)</a>. In this section I will describe two (somewhat well-known) tests that allow to certify these relations based only on the fact that the measurements generate statistics which pass the tests.</p>
<p><b>Linearity test:</b></p>
<ul>
<li>(a) The referee selects <img src="https://s0.wp.com/latex.php?latex=%7BW%5Cin%5C%7BX%2CZ%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W\in\{X,Z\}}" class="latex" title="{W\in\{X,Z\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Ca%27%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,a'\in\{0,1\}^n}" class="latex" title="{a,a'\in\{0,1\}^n}" /> uniformly at random. He sends <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%2Ca%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a,a')}" class="latex" title="{(W,a,a')}" /> to one player and <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a)}" class="latex" title="{(W,a)}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a')}" class="latex" title="{(W,a')}" />, or <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%2Ba%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a+a')}" class="latex" title="{(W,a+a')}" /> to the other.</li>
<li>(b) The first player replies with two bits, and the second with a single bit. The referee accepts if and only if the player’s answers are consistent.</li>
</ul>
<p>As always in this note, the test treats both players simultaneously. As a result we can (and will) assume that the player’s strategy is symmetric, and is specified by a permutation-invariant state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in {\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{\psi\in {\mathbb C}^d \otimes {\mathbb C}^d}" /> and a measurement for each question: an observable <img src="https://s0.wp.com/latex.php?latex=%7BW%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W(a)}" class="latex" title="{W(a)}" /> associated to questions of the form <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a)}" class="latex" title="{(W,a)}" />, and a more complicated four-outcome measurement <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BW%5E%7Ba%2Ca%27%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{W^{a,a'}\}}" class="latex" title="{\{W^{a,a'}\}}" /> associated with questions of the form <img src="https://s0.wp.com/latex.php?latex=%7B%28W%2Ca%2Ca%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(W,a,a')}" class="latex" title="{(W,a,a')}" /> (It will not be necessary to go into the details of the formalism for such measurements).</p>
<p>The linearity test described above is exactly identical to the BLR linearity test described earlier, but for the use of the basis label <img src="https://s0.wp.com/latex.php?latex=%7BW%5Cin%5C%7BX%2CZ%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W\in\{X,Z\}}" class="latex" title="{W\in\{X,Z\}}" />. The lemma below is a direct analogue of Lemma <a href="https://mycqstate.wordpress.com/feed/#lemblr-test">1</a>, which extends the analysis to the setting of players sharing entanglement. The lemma was first introduced in a joint <a href="http://ieeexplore.ieee.org/abstract/document/6375302/">paper</a> with Ito, where we used an extension of the linearity test, Babai et al.’s multilinearity test, to show the inclusion of complexity classes NEXP<img src="https://s0.wp.com/latex.php?latex=%7B%5Csubseteq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\subseteq}" class="latex" title="{\subseteq}" />MIP<img src="https://s0.wp.com/latex.php?latex=%7B%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{^*}" class="latex" title="{^*}" />.</p>
<blockquote><p><b>Lemma 7</b> <em><a name="lemcom"></a>Suppose that a family of observables <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7BW%28a%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{W(a)\}}" class="latex" title="{\{W(a)\}}" /> for <img src="https://s0.wp.com/latex.php?latex=%7BW%5Cin%5C%7BX%2CZ%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W\in\{X,Z\}}" class="latex" title="{W\in\{X,Z\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\in\{0,1\}^n}" class="latex" title="{a\in\{0,1\}^n}" />, generates outcomes that succeed in the linearity test with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" />, when applied on a bipartite state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin%7B%5Cmathbb+C%7D%5Ed%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" class="latex" title="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" />. Then the following hold: approximate consistency</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_a+%5C%2C+%5Cpsi%5E%2A+%5Cbig%28X%28a%29+%5Cotimes+X%28a%29%5Cbig%29+%5Cpsi+%5C%2C%3D%5C%2C1-O%28%5Cvarepsilon%29%2C%5Cqquad+%5Cmathop%7B%5Cmathbb+E%7D_b+%5C%2C+%5Cpsi%5E%2A+%5Cbig%28Z%28b%29+%5Cotimes+Z%28b%29+%5Cbig%29%5Cpsi%5C%2C%5Cgeq%5C%2C1-O%28%5Cvarepsilon%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_a \, \psi^* \big(X(a) \otimes X(a)\big) \psi \,=\,1-O(\varepsilon),\qquad \mathop{\mathbb E}_b \, \psi^* \big(Z(b) \otimes Z(b) \big)\psi\,\geq\,1-O(\varepsilon)," class="latex" title="\displaystyle \mathop{\mathbb E}_a \, \psi^* \big(X(a) \otimes X(a)\big) \psi \,=\,1-O(\varepsilon),\qquad \mathop{\mathbb E}_b \, \psi^* \big(Z(b) \otimes Z(b) \big)\psi\,\geq\,1-O(\varepsilon)," /></p>
<p><em>and linearity</em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Ca%27%7D+%5C%2C%5Cbig%5C%7CX%28a%29X%28a%27%29-X%28a%2Ba%27%29%5Cbig%5C%7C_%5Csigma%5E2+%3D+O%28%5Cvarepsilon%29%2C%5Cqquad%5Cmathop%7B%5Cmathbb+E%7D_%7Bb%2Cb%27%7D%5C%2C+%5Cbig%5C%7CZ%28b%29Z%28b%27%29-Z%28b%2Bb%27%29%5Cbig%5C%7C_%5Csigma%5E2+%5C%2C%3D%5C%2C+O%28%7B%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,a'} \,\big\|X(a)X(a')-X(a+a')\big\|_\sigma^2 = O(\varepsilon),\qquad\mathop{\mathbb E}_{b,b'}\, \big\|Z(b)Z(b')-Z(b+b')\big\|_\sigma^2 \,=\, O({\varepsilon})." class="latex" title="\displaystyle \mathop{\mathbb E}_{a,a'} \,\big\|X(a)X(a')-X(a+a')\big\|_\sigma^2 = O(\varepsilon),\qquad\mathop{\mathbb E}_{b,b'}\, \big\|Z(b)Z(b')-Z(b+b')\big\|_\sigma^2 \,=\, O({\varepsilon})." /></p>
</blockquote>
<p>Testing anti-commutation is slightly more involved. We will achieve this by using a two-player game called the Magic Square game. This is a fascinating game, but just as for the linearity test I will treat it superficially and only recall the part of the analysis that is useful for us (see e.g. the <a href="https://arxiv.org/abs/1512.02074">paper</a> by Wu et al. for a description of the game and a proof of Lemma <a href="https://mycqstate.wordpress.com/feed/#lemms">8</a> below).</p>
<blockquote><p><b>Lemma 8 (Magic Square)</b> <em><a name="lemms"></a>The Magic Square game is a two-player game with nine possible questions (with binary answers) for one player and six possible questions (with two-bit answers) for the other player which has the following properties. The distribution on questions in the game is uniform. Two of the questions to the first player are labelled <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> respectively. For any strategy for the players that succeeds in the game with probability at least <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" /> using a bipartite state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin%7B%5Cmathbb+C%7D%5Ed%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" class="latex" title="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" /> and observables <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> for questions <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> respectively, it holds that<a name="eqms-ac"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbig%5C%7C%5Cbig%28+%28XZ%2BZX%29%5Cotimes+I_d+%5Cbig%29%5Cpsi%5Cbig%5C%7C%5E2+%5C%2C%3D%5C%2C+O%5Cbig%28%5Csqrt%7B%5Cvarepsilon%7D%5Cbig%29.+%5C+%5C+%5C+%5C+%5C+%2814%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \big\|\big( (XZ+ZX)\otimes I_d \big)\psi\big\|^2 \,=\, O\big(\sqrt{\varepsilon}\big). \ \ \ \ \ (14)" class="latex" title="\displaystyle \big\|\big( (XZ+ZX)\otimes I_d \big)\psi\big\|^2 \,=\, O\big(\sqrt{\varepsilon}\big). \ \ \ \ \ (14)" /></p>
<p><em><a name="eqms-ac"></a>Moreover, there exists a strategy which succeeds with probability <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> in the game, using <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%3D%5Cphi_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi=\phi_4}" class="latex" title="{\psi=\phi_4}" /> and Pauli observables <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_X+%5Cotimes+I_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_X \otimes I_2}" class="latex" title="{\sigma_X \otimes I_2}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csigma_Z%5Cotimes+I_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sigma_Z\otimes I_2}" class="latex" title="{\sigma_Z\otimes I_2}" /> for questions <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> respectively.</em></p></blockquote>
<p>Based on the Magic Square game we devise the following “anti-commutation test”.</p>
<p><b>Anti-commutation test:</b></p>
<ul>
<li>(a) The referee selects <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b\in\{0,1\}^n}" class="latex" title="{a,b\in\{0,1\}^n}" /> uniformly at random under the condition that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Ccdot+b%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\cdot b=1}" class="latex" title="{a\cdot b=1}" />. He plays the Magic Square game with both players, with the following modifications: if the question to the first player is <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> he sends <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2Ca%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,a)}" class="latex" title="{(X,a)}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%28Z%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(Z,b)}" class="latex" title="{(Z,b)}" /> instead; in all other cases he sends the original label of the question in the Magic Square game together with both strings <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />.</li>
<li>(b) Each player provides answers as in the Magic Square game. The referee accepts if and only if the player’s answers would have been accepted in the game.</li>
</ul>
<p>Using Lemma <a href="https://mycqstate.wordpress.com/feed/#lemms">8</a> it is straightforward to show the following.</p>
<blockquote><p><b>Lemma 9</b> <em><a name="lemac"></a>Suppose a strategy for the players succeeds in the anti-commutation test with probability at least <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" />, when performed on a bipartite state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi+%5Cin+%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{\psi \in {\mathbb C}^d \otimes {\mathbb C}^d}" />. Then the observables <img src="https://s0.wp.com/latex.php?latex=%7BX%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(a)}" class="latex" title="{X(a)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z(b)}" class="latex" title="{Z(b)}" /> applied by the player upon receipt of questions <img src="https://s0.wp.com/latex.php?latex=%7B%28X%2Ca%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(X,a)}" class="latex" title="{(X,a)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%28Z%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(Z,b)}" class="latex" title="{(Z,b)}" /> respectively satisfy<a name="eqac"></a></em></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%3A%5C%2Ca%5Ccdot+b%3D1%7D+%5C%2C%5Cbig%5C%7C+X%28a%29Z%28b%29-%28-1%29%5E%7Ba%5Ccdot+b%7D+Z%28b%29X%28a%29%5Cbig%5C%7C_%5Csigma%5E2%5C%2C%3D%5C%2CO%5Cbig%28%5Csqrt%7B%5Cvarepsilon%7D%5Cbig%29.+%5C+%5C+%5C+%5C+%5C+%2815%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b:\,a\cdot b=1} \,\big\| X(a)Z(b)-(-1)^{a\cdot b} Z(b)X(a)\big\|_\sigma^2\,=\,O\big(\sqrt{\varepsilon}\big). \ \ \ \ \ (15)" class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b:\,a\cdot b=1} \,\big\| X(a)Z(b)-(-1)^{a\cdot b} Z(b)X(a)\big\|_\sigma^2\,=\,O\big(\sqrt{\varepsilon}\big). \ \ \ \ \ (15)" /></p>
<p><em><a name="eqac"></a></em></p></blockquote>
<p><b>3.3. A robust test for high-dimensional entangled states</b></p>
<p>We are ready to state, and prove, our main theorem: a test for high-dimensional entanglement that is “robust”, meaning that success probabilities that are a constant close to the optimal value suffice to certify that the underlying state is within a constant distance from the target state — in this case, a tensor product of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> EPR pairs. Although arguably a direct “quantization” of the BLR result, this is the first test known which achieves constant robustness — all previous <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit tests required success that is inverse polynomially (in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />) close to the optimum in order to provide any meaningful conclusion.</p>
<p><b><img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit Pauli braiding test:</b> With probability <img src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/2}" class="latex" title="{1/2}" /> each,</p>
<ul>
<li>(a) Execute the linearity test.</li>
<li>(b) Execute the anti-commutation test.</li>
</ul>
<blockquote><p><b>Theorem 10</b> <em>Suppose that a family of observables <img src="https://s0.wp.com/latex.php?latex=%7BW%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W(a)}" class="latex" title="{W(a)}" />, for <img src="https://s0.wp.com/latex.php?latex=%7BW%5Cin%5C%7BX%2CZ%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W\in\{X,Z\}}" class="latex" title="{W\in\{X,Z\}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Cin%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\in\{0,1\}^n}" class="latex" title="{a\in\{0,1\}^n}" />, and a state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%5Cin%7B%5Cmathbb+C%7D%5Ed%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" class="latex" title="{\psi\in{\mathbb C}^d\otimes {\mathbb C}^d}" />, generate outcomes that pass the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-qubit Pauli braiding test with probability at least <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7Bd%3D+%281-O%28%5Csqrt%7B%5Cvarepsilon%7D%29%292%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d= (1-O(\sqrt{\varepsilon}))2^n}" class="latex" title="{d= (1-O(\sqrt{\varepsilon}))2^n}" />.</em></p></blockquote>
<p>As should be apparent from the proof it is possible to state a stronger conclusion for the theorem, which includes a characterization of the observables <img src="https://s0.wp.com/latex.php?latex=%7BW%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W(a)}" class="latex" title="{W(a)}" /> and the state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" /> up to local isometries. For simplicity I only recorded the consequence on the dimension of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\psi}" class="latex" title="{\psi}" />.</p>
<p><em>Proof:</em> Using Lemma <a href="https://mycqstate.wordpress.com/feed/#lemcom">7</a> and Lemma <a href="https://mycqstate.wordpress.com/feed/#lemac">9</a>, success with probability <img src="https://s0.wp.com/latex.php?latex=%7B1-%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-\varepsilon}" class="latex" title="{1-\varepsilon}" /> in the test implies that conditions <a href="https://mycqstate.wordpress.com/feed/#eqgh-cons">(9)</a>, <a href="https://mycqstate.wordpress.com/feed/#eqgh-commute">(10)</a> and <a href="https://mycqstate.wordpress.com/feed/#eqgh-ac">(11)</a> in Corollary <a href="https://mycqstate.wordpress.com/feed/#corgh">5</a> are all satisfied, up to error <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7B%5Cvarepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt{\varepsilon})}" class="latex" title="{O(\sqrt{\varepsilon})}" />. (In fact, Lemma <a href="https://mycqstate.wordpress.com/feed/#lemac">9</a> only implies <a href="https://mycqstate.wordpress.com/feed/#eqgh-ac">(11)</a> for strings <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b}" class="latex" title="{a,b}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Ccdot+b%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\cdot b=1}" class="latex" title="{a\cdot b=1}" />. The condition for string such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%5Ccdot+b%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a\cdot b=0}" class="latex" title="{a\cdot b=0}" /> follows from the other conditions.) The conclusion of the corollary is that there exists an isometry <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> such that the observables <img src="https://s0.wp.com/latex.php?latex=%7BX%28a%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X(a)}" class="latex" title="{X(a)}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BZ%28b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z(b)}" class="latex" title="{Z(b)}" /> satisfy</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D%5C%2C+%5Cbig%5C%7C+X%28a%29Z%28b%29+-+V%5E%2Ag%28%5Csigma_X%28a%29%5Csigma_Z%28b%29%29V+%5Cbig%5C%7C_%5Csigma%5E2+%5C%2C%3D%5C%2C+O%28%5Csqrt%7B%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b}\, \big\| X(a)Z(b) - V^*g(\sigma_X(a)\sigma_Z(b))V \big\|_\sigma^2 \,=\, O(\sqrt{\varepsilon})." class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b}\, \big\| X(a)Z(b) - V^*g(\sigma_X(a)\sigma_Z(b))V \big\|_\sigma^2 \,=\, O(\sqrt{\varepsilon})." /></p>
<p>Using again the consistency relations <a href="https://mycqstate.wordpress.com/feed/#eqgh-cons">(9)</a> that follow from part (a) of the test together with the above we get</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathop%7B%5Cmathbb+E%7D_%7Ba%2Cb%7D%5C%2C+%5Cpsi%5E%2A+%28V%5Cotimes+V%29%5E%2A+%5Cbig%28+%5Csigma_X%28a%29%5Csigma_Z%28b%29%5Cotimes+%5Csigma_X%28a%29%5Csigma_Z%28b%29%5Cbig%29%28V%5Cotimes+V%29%5Cpsi+%5C%2C%3D%5C%2C+1-O%28%5Csqrt%7B%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \mathop{\mathbb E}_{a,b}\, \psi^* (V\otimes V)^* \big( \sigma_X(a)\sigma_Z(b)\otimes \sigma_X(a)\sigma_Z(b)\big)(V\otimes V)\psi \,=\, 1-O(\sqrt{\varepsilon})." class="latex" title="\displaystyle \mathop{\mathbb E}_{a,b}\, \psi^* (V\otimes V)^* \big( \sigma_X(a)\sigma_Z(b)\otimes \sigma_X(a)\sigma_Z(b)\big)(V\otimes V)\psi \,=\, 1-O(\sqrt{\varepsilon})." /></p>
<p>Applying Lemma <a href="https://mycqstate.wordpress.com/feed/#lemsr">6</a>, <img src="https://s0.wp.com/latex.php?latex=%7B%28V%5Cotimes+V%29%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(V\otimes V)\psi}" class="latex" title="{(V\otimes V)\psi}" /> has Schmidt rank at least <img src="https://s0.wp.com/latex.php?latex=%7B%281-O%28%5Csqrt%7B%5Cvarepsilon%7D%29%292%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-O(\sqrt{\varepsilon}))2^n}" class="latex" title="{(1-O(\sqrt{\varepsilon}))2^n}" />. But <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V}" class="latex" title="{V}" /> is a local isometry, which cannot increase the Schmidt rank. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2017/06/28/pauli-braiding/"><span class="datestr">at June 28, 2017 03:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2017/06/08/hyperparameter-optimization-and-the-analysis-of-boolean-functions/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2017/06/08/hyperparameter-optimization-and-the-analysis-of-boolean-functions/">Hyperparameter optimization and the analysis of boolean functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">I’ve always admired the beautiful theory on the analysis of boolean functions (see the excellent <a href="http://dl.acm.org/citation.cfm?id=2683783">book</a> of Ryan O’Donnell, as well <a href="http://www.ma.huji.ac.il/~kalai/">Gil Kalai’s lectures</a>). Heck, it used to be my main area of investigation back in the early grad-school days we were studying hardness of approximation, the PCP theorem, and the “new” (back at the time) technology of Fourier analysis for boolean functions.  This technology also gave rise to seminal results on learnability with respect to the uniform distribution. Uniform distribution learning has been widely criticized as unrealistic, and the focus of the theoretical machine learning community has shifted to (mostly) convex, real-valued, agnostic learning in recent years.<p></p>
<p>It is thus particularly exciting to me that some of the amazing work on boolean learning can be used for the very practical problem of Hyperparameter Optimization (HO), which has on the face of it nothing to do with the uniform distribution. <a href="https://arxiv.org/abs/1706.00764">Here is the draft</a>, joint work with <a href="https://www.cs.utexas.edu/~klivans/">Adam Klivans</a> and <a href="http://www.callowbird.com/">Yang Yuan</a>.</p>
<p><a name="more"></a></p>
<p>To describe hyperparameter optimization: many software packages, in particular with the rise of deep learning, come with gazillions of input parameters. An example is the TensorFlow software package for training deep nets. To actually use it the user needs to specify how many layers, which layers are convolutional / full connected, which optimization algorithm to use (stochastic gradient descent, AdaGrad, etc.), whether to add momentum to the optimization or not…. You get the picture – choosing the parameters is by itself an optimization problem!</p>
<p>It is a highly non-convex optimization problem, over mostly discrete (but some continuous) choices. Evaluating the function, i.e. training a deep net over a large dataset with a specific configuration, is very expensive, and you cannot assume any other information about the function such as gradients (that are not even defined for discrete functions).  In other words, sample complexity is of the essence, whereas computation complexity can be relaxed as long as no function evaluations are required.</p>
<p>The automatic choice of hyperparameters has been hyped recently with various names such as “auto tuning”, “auto ML”,  and so forth. For an excellent post on existing approaches and their downsides see these posts <a href="http://www.argmin.net/2016/06/20/hypertuning/">Ben Recht’s blog</a>.</p>
<p>This is where harmonic analysis and compressed sensing can be of help! While in general hyperparameter optimization is computationally hard, we assume a sparse Fourier representation of the objective function. Under this assumption, one can use compressed sensing techniques to recover and optimize the underlying function with low sample complexity.</p>
<p>Surprisingly, using sparse recovery techniques one can even improve the known sample complexity results for well-studied problems such as learning decision trees under the uniform distribution, improving upon classical results by <a href="http://dl.acm.org/citation.cfm?id=174138">Linial, Mansour and Nisan</a>.</p>
<p>Experiments show that the sparsity assumptions in the Fourier domain are justified for certain hyperparameter optimization settings, in particular, when training of deep nets for vision tasks. The harmonic approach (we call the algorithm “Harmonica”) attains a better solution than existing approaches based on multi-armed bandits and Bayesian optimization, and perhaps more importantly, significantly faster. It also beats random search 5X (i.e. random sampling with 5 times as many samples allowed as for our own method, a smart benchmark proposed by <a href="https://people.eecs.berkeley.edu/~kjamieson/hyperband.html">Jamieson</a>).</p>
<p>Those of you interested in code, my collaborator <a href="http://www.callowbird.com/">Yang Yuan</a> has promised to release it on GitHub, so please go ahead and email him <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" alt="😉" style="height: 1em;" class="wp-smiley" /></p>
<p>PS. We’re grateful to Sanjeev Arora for helpful discussions on this topic.</p>
<p>PPS. It has been a long time since my last post, and I still owe the readers an exposition on the data compression approach to unsupervised learning.  In the mean time, you may want to read this <a href="https://arxiv.org/abs/1610.01132">paper with Tengyu Ma</a> in NIPS 2016.</p></div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2017/06/08/hyperparameter-optimization-and-the-analysis-of-boolean-functions/"><span class="datestr">at June 08, 2017 05:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1183">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2017/04/20/unitary-correlation-matrices/">Unitary Correlation Matrices</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today I’d like to sketch a question that’s been pushing me in a lot of different directions over the past few years — some sane, others less so; few fruitful, but all instructive. The question is motivated by the problem of placing upper bounds on the amount of entanglement needed to play a two-player non-local game (near-)optimally. But it can also be stated as a natural mathematical question in itself, so this is how I’ll present it first, and then only briefly discuss some motivation. (I wish I could write I’ll also present results, but these will be quite sparse.) All that is to come is based on discussions with Oded Regev, though all inaccuracies and naïvetés are mine.</p>
<h2>Prelude: Vector Correlation Matrices</h2>
<p>Before jumping to unitary correlation matrices, let’s — rather pedantically — introduce vector correlation matrices. Most of you are already familiar with this simple object: a vector correlation matrix is an <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\times n}" class="latex" title="{n\times n}" /> Hermitian matrix <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> with complex entries such that there exists an integer <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> and unit vectors <img src="https://s0.wp.com/latex.php?latex=%7Bu_1%2C%5Cldots%2Cu_n%5Cin+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u_1,\ldots,u_n\in {\mathbb C}^d}" class="latex" title="{u_1,\ldots,u_n\in {\mathbb C}^d}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BC_%7Bi%2Cj%7D+%3D+%5Clangle+u_i%2Cu_j%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_{i,j} = \langle u_i,u_j\rangle}" class="latex" title="{C_{i,j} = \langle u_i,u_j\rangle}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29%5Cin%5C%7B1%2C%5Cldots%2Cn%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(i,j)\in\{1,\ldots,n\}^2}" class="latex" title="{(i,j)\in\{1,\ldots,n\}^2}" />. In other words: a Gram matrix with diagonal entries equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />.</p>
<p>A natural question is, given a vector correlation matrix <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, what is the minimal dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> in which there exists vectors achieving the specified correlations? Clearly <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\leq n}" class="latex" title="{d\leq n}" />, the dimension of the span of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> vectors; moreover the identity matrix implies that <img src="https://s0.wp.com/latex.php?latex=%7Bd%3Dn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=n}" class="latex" title="{d=n}" /> is sometimes necessary.</p>
<p>If we allow <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-approximations, we can do better: the Johnson-Lindenstrauss lemma implies that <img src="https://s0.wp.com/latex.php?latex=%7Bd%3DO%28%5Cvarepsilon%5E%7B-2%7D%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d=O(\varepsilon^{-2}\log n)}" class="latex" title="{d=O(\varepsilon^{-2}\log n)}" /> is sufficient (and <a href="https://arxiv.org/abs/1609.02094">necessary</a>) to find unit vectors such that <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+u_i%2Cu_j%5Crangle-C_%7Bi%2Cj%7D%7C%5Cleq%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\langle u_i,u_j\rangle-C_{i,j}|\leq\varepsilon}" class="latex" title="{|\langle u_i,u_j\rangle-C_{i,j}|\leq\varepsilon}" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j}" class="latex" title="{i,j}" />. And if we only require the approximation to hold on the average over the choice of <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" />, then no dependence on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> is necessary: <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+O%28%5Cepsilon%5E%7B-2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d = O(\epsilon^{-2})}" class="latex" title="{d = O(\epsilon^{-2})}" /> suffices.</p>
<p>This is all good and well. Now onto the interesting stuff!</p>
<h2>Theme: Unitary Correlation Matrices</h2>
<p>Define a unitary correlation matrix to be an an <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\times n}" class="latex" title="{n\times n}" /> Hermitian matrix <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> with complex entries such that there exists an integer <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> and unitary matrices <img src="https://s0.wp.com/latex.php?latex=%7BU_1%2C%5Cldots%2CU_n%5Cin+%7B%5Cmathbb+C%7D%5E%7Bd%5Ctimes+d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U_1,\ldots,U_n\in {\mathbb C}^{d\times d}}" class="latex" title="{U_1,\ldots,U_n\in {\mathbb C}^{d\times d}}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7BC_%7Bi%2Cj%7D+%3D+d%5E%7B-1%7D%5Ctextrm%7BTr%7D%28U_i+U_j%5E%5Cdagger%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_{i,j} = d^{-1}\textrm{Tr}(U_i U_j^\dagger)}" class="latex" title="{C_{i,j} = d^{-1}\textrm{Tr}(U_i U_j^\dagger)}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29%5Cin%5C%7B1%2C%5Cldots%2Cn%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(i,j)\in\{1,\ldots,n\}^2}" class="latex" title="{(i,j)\in\{1,\ldots,n\}^2}" />. Considering block matrices shows that the set of unitary correlation matrices is convex.</p>
<p>By forgetting the unitary structure of the <img src="https://s0.wp.com/latex.php?latex=%7BU_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U_i}" class="latex" title="{U_i}" /> we see that a unitary correlation matrix is automatically a vector correlation matrix; in particular it is positive semidefinite with all diagonal entries equal to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. While the latter is a characterization of vector correlation matrices, however, as soon as <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Cgeq+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\geq 4}" class="latex" title="{n\geq 4}" /> (and not before) there exists vector correlation matrices that are not unitary correlation matrices. This is not completely trivial to see, and appears in a <a href="https://arxiv.org/abs/0901.0288">paper </a>by Dykema and Juschenko; it is a nice exercise to work out. Now for the main question:</p>
<p>(<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />): Dimension reduction for unitaries. Let <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Cin%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\in{\mathbb N}}" class="latex" title="{n\in{\mathbb N}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt; 0 }" class="latex" title="{\epsilon &gt; 0 }" /> be given. Does there exist an explicit <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%3Dd%27%28n%3B%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'=d'(n;\epsilon)}" class="latex" title="{d'=d'(n;\epsilon)}" /> such that for every <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\times n}" class="latex" title="{n\times n}" /> unitary correlation matrix <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> there are <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" />-dimensional unitaries <img src="https://s0.wp.com/latex.php?latex=%7BV_1%2C%5Cldots%2CV_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1,\ldots,V_n}" class="latex" title="{V_1,\ldots,V_n}" /> such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CBig%7C%5Cfrac%7B1%7D%7Bd%27%7D%5Ctextrm%7BTr%7D%5Cbig%28V_i%5C%2CV_j%5E%5Cdagger%5Cbig%29+-+C_%7Bi%2Cj%7D+%5CBig%7C+%5Cleq+%5Cvarepsilon+%5Cqquad+%5Cforall+%28i%2Cj%29%5Cin%5C%7B1%2C%5Cldots%2Cn%5C%7D%5E2%5C%3B+.&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Big|\frac{1}{d'}\textrm{Tr}\big(V_i\,V_j^\dagger\big) - C_{i,j} \Big| \leq \varepsilon \qquad \forall (i,j)\in\{1,\ldots,n\}^2\; ." class="latex" title="\displaystyle \Big|\frac{1}{d'}\textrm{Tr}\big(V_i\,V_j^\dagger\big) - C_{i,j} \Big| \leq \varepsilon \qquad \forall (i,j)\in\{1,\ldots,n\}^2\; ." /></p>
<p>While the analogue question for vectors is trivial for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon=0}" class="latex" title="{\epsilon=0}" />, and a fundamental result in geometry for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" />, extremely little is known on the question for unitaries. Virtually the only general statement that can be made is that, at least, some bound <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> exists. This follows by a simple compactness argument, but does not yield any meaningful bound on the growth of <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> as a function of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />. In fact no explicit bound, however large, is known to hold in general. Let’s explore the problem a bit.</p>
<h2>Variatio: Equivalent formulations</h2>
<p>A nice feature of question (<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />) is that it is reasonably robust, in the sense that different natural formulations of the question can be shown equivalent, up to simple variations on the precise scaling of <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" />. For example, one can relax the constraint of being unitary to the sole requirement that the matrices have all singular values at most <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. At the opposite end of the spectrum one can consider a more structured problem which considers correlations between projection matrices (so all eigenvalues are <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />). Both these variants can be shown equivalent to the unitary case via some simple reductions.</p>
<p>The one variant which makes a substantial difference is the case of correlation matrices with real entries. A beautiful result of <a href="http://www.math.tau.ac.il/~tsirel/download/qbell87.html">Tsirelson</a> shows that any extremal real correlation matrix can be realized exactly, by Hermitian matrices having all eigenvalues <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B-1%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{-1,1\}}" class="latex" title="{\{-1,1\}}" />,  in dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%27+%3D+2%5E%7B%5Csqrt%7B%5Clfloor+n%2F2%5Crfloor%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d' = 2^{\sqrt{\lfloor n/2\rfloor}}}" class="latex" title="{d' = 2^{\sqrt{\lfloor n/2\rfloor}}}" />, and this bound is tight; relatively precise bounds of the form <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%3D2%5E%7B%5CTheta%28%5Cepsilon%5E%7B-1%7D%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'=2^{\Theta(\epsilon^{-1})}}" class="latex" title="{d'=2^{\Theta(\epsilon^{-1})}}" /> <a href="https://arxiv.org/abs/1609.01652">are known</a> for small enough <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" />. (Note that even though projection matrices are Hermitian, and thus give rise to real correlations, Tsirelson’s result does not imply a positive answer for the case of projections as the dimension-<img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> matrices recovered via Tsirelson’s construction will in general be Hermitian, but not projectors, even when the original matrices were.)</p>
<h2>Interlude: Motivation</h2>
<p><strong>Quantum games. </strong>One can arrive at question (<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />) by asking about the minimal dimension of near-optimal strategies in a quantum two-player game. Experts will immediately see the connection, and I will not elaborate on this. Roughly, the easy observation is that correlations that are achievable by entangled players in a nonlocal game take the form</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clangle+%5Cpsi%7C+A+%5Cotimes+B+%7C%5Cpsi%5Crangle+%3D+%5Ctextrm%7BTr%7D%28AKB%5ETK%5E%5Cdagger%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \langle \psi| A \otimes B |\psi\rangle = \textrm{Tr}(AKB^TK^\dagger)," class="latex" title="\displaystyle \langle \psi| A \otimes B |\psi\rangle = \textrm{Tr}(AKB^TK^\dagger)," /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi\rangle}" class="latex" title="{|\psi\rangle}" /> is a unit vector in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+C%7D%5Ed+%5Cotimes+%7B%5Cmathbb+C%7D%5Ed%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb C}^d \otimes {\mathbb C}^d}" class="latex" title="{{\mathbb C}^d \otimes {\mathbb C}^d}" /> (the entanglement), <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is a complex <img src="https://s0.wp.com/latex.php?latex=%7Bd%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d\times d}" class="latex" title="{d\times d}" /> matrix that can be computed from <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi\rangle}" class="latex" title="{|\psi\rangle}" />, and <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A,B}" class="latex" title="{A,B}" />“observables”, i.e. Hermitian operators that square to identity describing the players’ measurement operators. (A more general formulation considers projections, rather than observables.) In case <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpsi%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\psi\rangle}" class="latex" title="{|\psi\rangle}" /> is the so-called “maximally entangled state”, <img src="https://s0.wp.com/latex.php?latex=%7BK+%3D+d%5E%7B-1%2F2%7D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K = d^{-1/2} I}" class="latex" title="{K = d^{-1/2} I}" /> and we recover precisely an entry from a correlation matrix. (The case of a general state gives rise to a slight variant of question <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cmathfrak%7BD%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\mathfrak{D})}" class="latex" title="{(\mathfrak{D})}" />, to which I am not sure whether it is equivalent or not.)<br />
Arriving at the question from this “physical” angle, it seems like it “ought” to have a reasonable answer: certainly, if one fixes the size of the game, and an approximation error <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" />, then there must exist some dimension that suffices to implement an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-optimal strategy. No such result is known. If anything existing signs seem to point in the negative direction: for instance, Slofstra <a href="https://arxiv.org/abs/1703.08618">very recently</a> showed that there exists a fixed, constant-sized game such that the optimal winning probability of <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> can only be achieved in the limit of infinite dimension (but it does seem to be the case that, for this game, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-optimal strategies can be found in dimension <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctextrm%7Bpoly%7D%28%5Cepsilon%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\textrm{poly}(\epsilon^{-1}}" class="latex" title="{\textrm{poly}(\epsilon^{-1}}" />). Note that this result implies that the set of correlation matrices of projections is not closed.</p>
<p><strong>Connes’ conjecture.</strong> A different, though related, way to arrive at question (<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />) is via the famous “Connes embedding conjecture” in the theory of <img src="https://s0.wp.com/latex.php?latex=%7BC%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C^*}" class="latex" title="{C^*}" /> algebras. <a href="https://www.jstor.org/stable/1971057?seq=1#page_scan_tab_contents">Connes’ embedding conjecture</a> states, rather informally, that any separable <img src="https://s0.wp.com/latex.php?latex=%7BII_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{II_1}" class="latex" title="{II_1}" /> factor (i.e. a von Neumann algebra with trivial center that is infinite-dimensional as a vector space, but has a finite faithful trace) embeds into a suitable ultrapower of the hyperfinite factor <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{R}}" class="latex" title="{\mathcal{R}}" />. Kirchberg <a href="https://eudml.org/doc/144111">showed that</a> the conjecture is equivalent to the following statement.</p>
<p>Theorem. The validity of Connes’ conjecture for some factor <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> is equivalent to the following: For all <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bn%2Ck%5Cin%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n,k\in{\mathbb N}}" class="latex" title="{n,k\in{\mathbb N}}" /> and unitaries <img src="https://s0.wp.com/latex.php?latex=%7BU_1%2C%5Cldots%2CU_n%5Cin+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U_1,\ldots,U_n\in M}" class="latex" title="{U_1,\ldots,U_n\in M}" /> there is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%5Cin%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'\in{\mathbb N}}" class="latex" title="{d'\in{\mathbb N}}" /> and unitaries <img src="https://s0.wp.com/latex.php?latex=%7BV_1%2C%5Cldots%2CV_n%5Cin+M_%7Bd%27%7D%28%7B%5Cmathbb+C%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1,\ldots,V_n\in M_{d'}({\mathbb C})}" class="latex" title="{V_1,\ldots,V_n\in M_{d'}({\mathbb C})}" />, such that</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CBig%7C%5Ctau%5Cbig%28U_i%5C%2CU_j%5E%2A%5Cbig%29-%5Cfrac%7B1%7D%7Bd%7D%5Ctextrm%7BTr%7D%5Cbig%28V_i%5C%2CV_j%5E%5Cdagger%5Cbig%29%5CBig%7C%5Cleq+%5Cepsilon%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle \Big|\tau\big(U_i\,U_j^*\big)-\frac{1}{d}\textrm{Tr}\big(V_i\,V_j^\dagger\big)\Big|\leq \epsilon," class="latex" title="\displaystyle \Big|\tau\big(U_i\,U_j^*\big)-\frac{1}{d}\textrm{Tr}\big(V_i\,V_j^\dagger\big)\Big|\leq \epsilon," /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tau}" class="latex" title="{\tau}" /> is the trace on <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" />.<br />
This formulation is close to question (<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />), except for two important differences: first, we assume that the target correlations are achievable in finite dimension <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />. This makes the problem easier, and would make it trivial if we were not to introduce a second important difference, which is that we ask for explicit bounds on <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" />. As a result I do not know of any formal implication between (<img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7BD%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathfrak{D}}" class="latex" title="{\mathfrak{D}}" />) and Connes’ conjecture, in either direction.</p>
<p><strong>Graph limits. </strong>Finally, for the combinatorialist let me mention an analogous (though, as far I can tell, not directly related) question, formulated by Aldous and Lyons in the context of their study of limits of bounded-degree graphs. The distance between two finite graphs of the same constant degree (but not necessarily the same number of vertices) can be measured via the sampling distance <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />: <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28G%2CG%27%29+%3D+%5Csum_%7Br%3D1%7D%5E%5Cinfty+2%5E%7B-r%7D%5Cdelta_r%28G%2CG%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta(G,G') = \sum_{r=1}^\infty 2^{-r}\delta_r(G,G')}" class="latex" title="{\delta(G,G') = \sum_{r=1}^\infty 2^{-r}\delta_r(G,G')}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_r%28G%2CG%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta_r(G,G')}" class="latex" title="{\delta_r(G,G')}" /> denotes the total variation distance between the distributions on rooted <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />-neighborhoods obtained by sampling a random vertex from <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> (resp. <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G'}" class="latex" title="{G'}" />) and considering the sub-graph induced on all vertices at distance at most <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> from the sampled vertex. With this notion in place, Question 10.1 in Aldous and Lyons’ <a href="https://arxiv.org/abs/math/0603062">paper on unimodular random networks</a> asks the following:</p>
<p>(Aldous-Lyons:) For every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> there is an integer <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> such that for every (finite) graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> there is a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G'}" class="latex" title="{G'}" /> on <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> vertices such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28G%2CG%27%29%5Cleq+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta(G,G')\leq \epsilon}" class="latex" title="{\delta(G,G')\leq \epsilon}" />.</p>
<p>In page 1458 the authors mention that the validity of their conjecture for the special class of Cayley graphs would imply that all finitely generated groups are sofic (very roughly, can be embedded into finite-dimensional permutation groups). Even though we do not know of an example of a group that is not sofic, this would be a very surprising result. In particular, it would imply Connes’s Embedding Conjecture for group von Neumann algebras, since the latter is known to hold for sofic groups.</p>
<h2>Development: Results</h2>
<p>Unfortunately this is going to be one of the shortest, most boring developments in musical history: there is too little to say! I could describe multiple failed attempts. In particular, naïve attempts at dimension reduction, inspired by Johnson-Lindenstrauss or other standard techniques, or incremental “gradient-descent” type of progressive block diagonalization procedures, all seem doomed to fail.</p>
<p>Aside from Tsirelson’s result for real correlation matrices, the one case for which we were able to find a cute proof is the case of permutation correlation matrices, where each <img src="https://s0.wp.com/latex.php?latex=%7BU_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U_i}" class="latex" title="{U_i}" /> is assumed to be a permutation matrix. The fact that permutations are sparse seems to make it easier to operate on them by “shifting entries around”; unitaries have a more rigid structure. The proof uses a simple combinatorial argument, with the heaviest hammer being Hall’s theorem guaranteeing the existence of a perfect matching, which is used to simultaneously re-organize the “<img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />” entries in a subset of the permutation matrices while preserving all correlations. The upper bound on <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> we obtain is of order <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%2F%5Cvarepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n/\varepsilon^2}" class="latex" title="{2^n/\varepsilon^2}" />, which may be the right order.</p>
<p>More is known in terms of negative results, i.e. lower bounds on <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" />. Such bounds abound in the theory of nonlocal games, where they go by the name of “dimension witness”. The best known results I am aware of imply that <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'}" class="latex" title="{d'}" /> should grow at least like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmin%282%5E%7B%5COmega%28%5Cepsilon%5E%7B-1%7D%29%7D%2C2%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\min(2^{\Omega(\epsilon^{-1})},2^n)}" class="latex" title="{\min(2^{\Omega(\epsilon^{-1})},2^n)}" />, which is good for very small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />, and also <img src="https://s0.wp.com/latex.php?latex=%7Bd%27%3D%5COmega%28n%5Ec%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d'=\Omega(n^c)}" class="latex" title="{d'=\Omega(n^c)}" />, which holds for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" /> smaller than a universal constant (the two bounds are obtained from different families of correlations; see <a href="https://arxiv.org/abs/1609.01652">here</a> for the former and <a href="https://arxiv.org/abs/1610.03574">here</a> for the latter). An interesting consequence of the (proof of) the second bound, which appears in joint work with Natarajan, is that even an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-approximation on average (over the entries of C) requires large dimension. This implies that no “oblivious” rounding technique, as in the Johnson-Lindenstrauss lemma, will work: such a technique would guarantee small approximation error on average independently of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />.</p>
<h2>Coda</h2>
<p>There has been a lot of progress recently on lower bounds, stimulated by works on quantum non-local games. This includes a beautiful framework of games for checking “non-commutative” analogues of linear equations over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />, developed by <a href="https://arxiv.org/abs/1209.2729">Cleve and Mittal</a> and <a href="https://arxiv.org/abs/1310.3794">Ji</a>; extensions of the framework to testing finitely presented groups by <a href="https://arxiv.org/abs/1703.08618">Slofstra</a>; a development of approaches based on operator systems by <a href="https://arxiv.org/abs/1503.07207">Paulsen</a> and <a href="https://arxiv.org/abs/1612.02791">co-authors</a>, and many others. But no upper bounds! Get to work: things can’t remain this way.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2017/04/20/unitary-correlation-matrices/"><span class="datestr">at April 20, 2017 09:08 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=1146">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2017/01/16/quid-qpcp/">Quid qPCP?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This blog has already seen three posts on the quantum PCP conjecture: in <a href="https://mycqstate.wordpress.com/2013/02/24/a-quantum-pcp-theorem/">February 2013</a> to highlight several talks at the Simons Institute in Berkeley, in <a href="https://mycqstate.wordpress.com/2013/10/01/quantum-pcp-a-survey/">October 2013</a> to promote a <a href="https://arxiv.org/abs/1309.7495">survey</a> on the topic I wrote with Dorit Aharonov and Itai Arad, and in <a href="https://mycqstate.wordpress.com/2014/10/31/quantum-pcp-conjectures/">October 2014</a> to introduce the two main variants “constraint satisfaction” (CSP) and “multiplayer games” (MIP), of the quantum QCP (qPCP) conjecture. Such a high rate of posting (compared to the average frequency of posts on this blog) might indicate a slight obsession. But you may also notice it’s been…two years! Has no result worthy of note been established since? Certainly not, and although the conjecture still stands strong, there have been a few interesting developments on both variants of the conjecture. In this post I’ll discuss a couple results on the CSP-qPCP. In a follow-up post I’ll describe progress on the MIP-qPCP.</p>
<p>When we wrote the survey three summers ago, the latest word on the CSP-qPCP (see Conjecture 1.3 <a href="https://arxiv.org/pdf/1309.7495v1.pdf">here</a> for a precise formulation) had been given in a <a href="https://arxiv.org/abs/1310.0017">paper</a> by Brandao and Harrow. BH showed, using information-theoretic arguments, that the constraint graphs associated with constant-gap QMA-hard instances of the local Hamiltonian problem had to satisfy “non-expansion” requirements seemingly at odds with the expansion properties of graphs associated with what are often considered the hardest instances of classical CSPs. Intuitively, their argument uses the monogamy of quantum correlations to argue that highly expanding constraint graphs place such strong demands on entanglement that there is always a product state whose energy is not far from the minimum. Although not strictly a no-go result, their theorem indicates that QMA-hard instances must be based on constraint graphs with markedly different spectral properties than those associated with the hardest instances of classical CSP.</p>
<p>For the time being it seems like any proof, or disproof, of the conjecture remains out of reach. Instead of focusing directly on qPCP, it may be more fruitful to develop the objects that are expected to play an important role in the proof, such as (quantum) low-density parity check codes (qLDPC) and (quantum) locally testable codes (qLTC). Two recent works make progress on this front.</p>
<h2><b>The NLETS conjecture</b></h2>
<p>The <a href="https://arxiv.org/abs/1301.1363">no low-energy trivial states (NLTS) conjecture</a> was proposed by Freedman and Hastings as a “complexity-free” analogue of CSP-qPCP. The NLTS conjecture states that there exist local Hamiltonians such that all low-energy (within an additive constant, times the norm of the Hamiltonian, from the minimum) states are “non-trivial”, in the sense that they cannot be generated by a constant-depth quantum circuit applied on a product state. Equivalently, all states that are the output of a constant-depth quantum circuit must have energy a constant above the minimum. NLTS Hamiltonian are good candidates for qPCP as they provide local Hamiltonian for which many obvious classical certificates for the minimal energy of the Hamiltonian (such as the description of a small circuit which generates a low-energy state) are essentially ruled out.</p>
<p>An <a href="https://arxiv.org/abs/1510.02082v2">earlier version</a> of the Eldar-Harrow manuscript claimed a construction of NLTS Hamiltonian, but the paper was recently updated, and the claim retracted. The <a href="https://arxiv.org/abs/1510.02082v3">current manuscript</a> establishes a moderately weaker (though strictly incomparable) result, that the authors call NLETS, for “no low-<em>error</em> trivial states”. The main result of EH is a relatively simple, explicit construction of a family of local Hamiltonians that have no non-trivial “ground state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-impostor”. An <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-impostor is a state that has the same reduced density matrix as a ground state on a fraction <img src="https://s0.wp.com/latex.php?latex=%7B%281-%5Cvarepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(1-\varepsilon)}" class="latex" title="{(1-\varepsilon)}" /> of the qubits, but may differ arbitrarily on the remaining <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" /> fraction. Using that the Hamiltonian is local, impostors necessarily have low energy, but the converse is not true, so that NLETS rules out non-triviality for a more restricted class of states than NLTS. For that restricted class of states, however, the non-triviality established by EH is sronger than required by NLTS: they show that no <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-impostor can even be well-approximated (within inverse-polynomial trace distance) by logarithmic-depth, instead of just constant-depth, quantum circuits.</p>
<p>Let’s see if I can give some basic intuition on their construction; for anything substantial see the <a href="https://arxiv.org/abs/1510.02082">paper</a>, which gives many angles on the result. Consider first first a classical repetition code encoding <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> bit into <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits. This can be made into a locally testable code by enforcing pairwise equality of bits along the edges of a constant-degree expanding graph on vertex set <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{1,\ldots,n\}}" class="latex" title="{\{1,\ldots,n\}}" />. Now allow me a little leap of faith: imagine there existed a magic quantum analogue of this classical repetition code, where equality between pairs of qubits is enforced not only in the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> (computational) basis, but also in the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> (Hadamard) basis. Of course such a thing does not exist: the constraints would force <em>any</em> pair of qubits (linked by the expander) to form an EPR pair, a requirement that strongly violates monogamy. But let’s <em>imagine</em>. Then I claim that we would essentially be done. Why? We need two more observations.</p>
<p>The first key observation made by EH is that any ground state of this imaginary code would have the following property: if you measure all qubits of the state in the same basis, either <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />, then for at least one of the two possible choices the measurement outcomes will be distributed according to a distribution on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit strings that places a large (constant) weight on at least two well-isolated (separated by at least the minimum distance) subsets of the Hamming cube. Note that this does not hold of the classical repetition code: the distribution which all-<img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> codeword is, well, concentrated. But if we were to measure the associated quantum state <img src="https://s0.wp.com/latex.php?latex=%7B%7C0%5Ccdots+0+%5Crangle+%5Csimeq+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28+%7C%2B%5Ccdots%2B%5Crangle%2B%7C-%5Ccdots-%5Crangle%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|0\cdots 0 \rangle \simeq \frac{1}{\sqrt{2}}( |+\cdots+\rangle+|-\cdots-\rangle)}" class="latex" title="{|0\cdots 0 \rangle \simeq \frac{1}{\sqrt{2}}( |+\cdots+\rangle+|-\cdots-\rangle)}" /> in the Hadamard basis, we would get a very spread distribution, with constant mass on two sets that are at distance <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> apart (I realize the equation I wrote is not quite correct! Don’t think too hard about it; obviously my “magical quantum repetition code” does not exist). The reason the distribution obtained in at least one of the two bases must be spread out is due to the uncertainty principle: if the distribution is localized in the <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> basis it must be delocalized in the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> basis, and vice-versa. And the reason it should be concentrated on isolated clumps is that we are measuring a codeword, which, for our magic example, can only lead to outcomes that are supported on the set <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%7C0%5Crangle%5E%7B%5Cotimes+n%7D%2C%7C1%5Crangle%5E%7B%5Cotimes+n%7D%2C%7C%2B%5Crangle%5E%7B%5Cotimes+n%7D%2C%7C-%5Crangle%5E%7B%5Cotimes+n%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{|0\rangle^{\otimes n},|1\rangle^{\otimes n},|+\rangle^{\otimes n},|-\rangle^{\otimes n}\}}" class="latex" title="{\{|0\rangle^{\otimes n},|1\rangle^{\otimes n},|+\rangle^{\otimes n},|-\rangle^{\otimes n}\}}" />.</p>
<p>To conclude we need the second observation, which is that trivial states do <em>not</em> have this property: measuring a trivial state in any product basis will always lead to a highly expanding distribution, which in particular cannot have large mass on well-isolated subsets. This is obviously true for product states, and requires a bit of work to be carried through logarithmically many layers of a quantum circuit; indeed this is where the main technical work of the paper lies.</p>
<p> </p>
<p>So the argument is complete…except for the fact that the required magic quantum repetition code does not exist! Instead, HE find a good make-do by employing a beautiful construction of quantum LDPC codes due to <a href="https://arxiv.org/abs/0903.0566">Tillich and Zemor</a>, the “hypergraph product”. The hypergraph product takes as input any pair of classical linear codes and returns a quantum “product” CSS code whose locality, distance and rate properties can be related to those of the original codes. The toric code can be case as an example of a hypergraph product code; see Section 3 in the <a href="https://arxiv.org/abs/0903.0566">paper</a> for explanations. Unfortunately, the way the distance of the product code scales with other parameters prevents TZ from obtaining good enough qLDPC for the CSP-QPCP; they can “only” obtain codes with constant weight and constant rate, but distance <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt{n})}" class="latex" title="{O(\sqrt{n})}" />.</p>
<p>In the context of NL(E)TS, and even more so qPCP, however, distance may not be the most relevant parameter. EH’s main construction is obtained as the hypergraph product of two expander-based repetition codes, which as a code only has logarithmic distance; nevertheless they are able to show that the robustness derived from the repetition code, together with the logarithmic distance, are enough to separate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-impostors from logarithmic-depth trivial states.</p>
<h2><b>Quantum LDPC &amp; LTC</b></h2>
<p>Quantum low-density parity-check codes (qLDPC) already made a showing in the previous sections. These families of codes are of much broader interest than their possible role in a forthcoming proof of qPCP, and constructions are being actively pursued. For classical codes the situation is largely satisfactory, and there are constructions that simultaneously achieve constant rate and linear distance with constant-weight parity checks. For quantum codes less is known. If we insist on constant-weight stabilizers then the best distance is <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Csqrt%7Bn%7D%5Clog%5E%7B1%2F4%7D+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(\sqrt{n}\log^{1/4} n)}" class="latex" title="{\Omega(\sqrt{n}\log^{1/4} n)}" /> (e.g. <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a414975.pdf">Freedman et al.</a>), a notch above the TZ construction mentioned earlier. The most local construction that achieves linear distance requires stabilizers of weight <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt{n})}" class="latex" title="{O(\sqrt{n})}" /> (e.g. <a href="https://arxiv.org/abs/1311.0885">Bravyi and Hastings</a>).</p>
<p>A recent <a href="https://arxiv.org/abs/1608.05089">paper</a> by Hastings makes progress on constructions of qLDPC – assuming a geometrical conjecture on the volume of certain surfaces defined from lattices in <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\mathbb R}^n}" class="latex" title="{{\mathbb R}^n}" />. Assuming the conjecture, Hastings shows the existence of qLDPC with <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B1-%5Cvarepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{1-\varepsilon}}" class="latex" title="{n^{1-\varepsilon}}" /> distance and logarithmic-weight stabilizers, a marked improvement over the state of the art. Although as discussed earlier even linear-distance, constant-weight, qLDPC would not imply the CSP-qPCP nor NLTS (the resulting Hamiltonian may still have low-energy eigenstates that are not at a small distance from codewords), by analogy with the classical case (and basic intuition!), constructions of such objects should certainly facilitate any attempt at a proof of the conjectures. Moreover, qLDPC suffice for the weaker NLETS introduced by EH, as the latter only makes a statement about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\varepsilon}" class="latex" title="{\varepsilon}" />-impostors, i.e. states that are at a constant distance from codewords. To obtain the stronger implication to NLTS, the proper notion is that of local testability: errors should be detected by a fraction of parity checks proportional to the distance of the error from the closest codeword (and not just <em>some</em> parity check).</p>
<p>Hastings’ construction follows the topological approach to quantum error correcting codes pioneered by Freedman and Kitaev. Although the latter introduced codes whose properties depend on the surface they are embedded in, at best I could tell the formal connection between homology and error correction is made in a comprehensive <a href="https://arxiv.org/abs/quant-ph/0605094">paper</a> by Bombin and Martin-Delgado. The advantage of this approach is that properties of the code, including rate and distance, can be tied to geometric properties of the underlying homology, reducing the construction of good codes to that of manifolds with the right properties.</p>
<p> </p>
<p>In addition to the (conjectural) construction of good qLDPC, almost as an afterthought Hastings provides an unconditional construction of a quantum locally testable code (qLTC), albeit one which encodes two qubits only. Let’s try to visualize this, starting from the helpful warm-up provided by Hastings, a high-dimensional, entangled, locally-testable code…which encodes zero qubit (the code space is one-dimensional). Of course this is trivial, but it’s a warm-up!</p>
<p>The simplest instance to visualize has six physical qubits. To follow the forthcoming paragraphs, take a piece of paper and draw a large tetrahedron. If you didn’t mess up your tetrahedron should have six edges: these are your qubits. Now the parity checks are as follows. Each of the four faces specifies an <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />-stabilizer which acts <img src="https://i2.wp.com/i-want-to-study-engineering.org/figs/tetrahedron_volume.png" alt="Image result for tetrahedron" class=" alignright" />on the three edges forming the face. Each of the four vertices specifies a <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-stabilizer which acts on the three edges that touch the vertex. The resulting eight operators pairwise commute, and they specify a unique (entangled) state in the <img src="https://s0.wp.com/latex.php?latex=%7B2%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^6}" class="latex" title="{2^6}" />-dimensional physical space.</p>
<p>Next we’d like to understand “local” testability. This means that if we fix a set <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" /> of edges, and act on each of them using an <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> error, then the resulting operator should violate (anti-commute) with a fraction of <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-stabilizers that is proportional to the <em>reduced</em> weight of the error, i.e. its distance to the closest operator which commutes with all <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-stabilizers. To see which stabilizers “detect” the error <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" />, we recall that <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> which overlap at an even number of locations commute. Therefore a <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> stabilizer will detect <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" /> if and only if it lies in its <em>boundary</em> <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpartial+O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\partial O}" class="latex" title="{\partial O}" />: the set of vertices which touch an odd number of edges in <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" />. This is our syndrome; it has a certain cardinality. To conclude we need to argue that <img src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O}" class="latex" title="{O}" /> can be modified into a set <img src="https://s0.wp.com/latex.php?latex=%7BO%2BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O+P}" class="latex" title="{O+P}" /> with no boundary, <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpartial%28O%2BP%29%3D%5Cemptyset%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\partial(O+P)=\emptyset}" class="latex" title="{\partial(O+P)=\emptyset}" />, and such that <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> is as small as possible – ideally, it should involve at most as many edges as the size of the boundary <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cpartial+O%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\partial O|}" class="latex" title="{|\partial O|}" />. Here is how Hastings does it: for each vertex in the boundary, introduce an edge that links it to some fixed vertex – say the top-most one in your tetrahedron. Let <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> be the resulting set of edges. Then you can check (on the picture!) that <img src="https://s0.wp.com/latex.php?latex=%7BO%2BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O+P}" class="latex" title="{O+P}" /> is boundary-less. Since we added at most as many edges as vertices in the boundary (if the top-most vertex was part of the boundary it doesn’t contribute any edge), we have proven local testability with respect to <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" /> errors; <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> errors are similar.</p>
<p>This was all in three dimensions. The wonderful thing is that the construction generalizes in a “straightforward” way to <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> dimensions. Consider an <img src="https://s0.wp.com/latex.php?latex=%7B%28n%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(n+1)}" class="latex" title="{(n+1)}" />-element universe <img src="https://s0.wp.com/latex.php?latex=%7BU%3D%5C%7B1%2C%5Cldots%2Cn%2B1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U=\{1,\ldots,n+1\}}" class="latex" title="{U=\{1,\ldots,n+1\}}" />. Qubits are all subsets of <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> of size <img src="https://s0.wp.com/latex.php?latex=%7Bq%3D%28n%2B1%29%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q=(n+1)/2}" class="latex" title="{q=(n+1)/2}" />; there are exponentially many of these. <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-stabilizers are defined for each <img src="https://s0.wp.com/latex.php?latex=%7B%28q-1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q-1)}" class="latex" title="{(q-1)}" />-element subset; each acts on all <img src="https://s0.wp.com/latex.php?latex=%7B%28q%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q+1)}" class="latex" title="{(q+1)}" /> of its <img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q}" class="latex" title="{q}" />-element supersets. Symmetrically, <img src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{X}" class="latex" title="{X}" />-stabilizers are defined for each <img src="https://s0.wp.com/latex.php?latex=%7B%28q%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q+1)}" class="latex" title="{(q+1)}" />-element set; each acts on all <img src="https://s0.wp.com/latex.php?latex=%7B%28q%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q+1)}" class="latex" title="{(q+1)}" /> of its <img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q}" class="latex" title="{q}" />-element subsets. Thus the code is local: each stabilizer has weight <img src="https://s0.wp.com/latex.php?latex=%7B%28q%2B1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q+1)}" class="latex" title="{(q+1)}" />, which is logarithmic in the number of qubits. It remains to check local testability; this follows using precisely the same argument as above (minus the picture…).</p>
<p>This first construction encodes zero qubits. How about getting a couple? Hastings gives a construction achieving this, and remains (poly-logarithmically) locally testable. The idea, very roughly, is to make a toric code by combining together two copies of the code described above. The number of encoded qubits will become non-trivial and local testability will remain. Unfortunately, just as for the toric code, the distance of the result code only scales as <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sqrt{n}}" class="latex" title="{\sqrt{n}}" />. To construct his code Hastings uses a slightly different cellulation than the above-described one. I am not sure precisely why the change is needed, and I defer to the paper for more details. (<a href="https://arxiv.org/abs/1504.00822">Leverrier, Tillich and Zemor</a> had earlier provided a construction, based on the TZ hypergraph product, with linear rate, square root distance, and local testability up to the minimal distance, i.e. for all errors of reduced weight at most <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\sqrt{n})}" class="latex" title="{O(\sqrt{n})}" />.)</p>
<p> </p>
<p>Although the geometric picture takes some effort to grasp, I find these constructions fascinating. Given the Brandao-Harrow objections to using the most “straighforward” expander constructions to achieve CSP-qPCP, or even NLTS, it seems logical to start looking for combinatorial structures that have more subtle properties and lie at the delicate boundary where both robustness (in terms of testability) and entanglement (non-triviality of ground states) can co-exist without challenging monogamy.</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2017/01/16/quid-qpcp/"><span class="datestr">at January 16, 2017 12:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=885">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2016/12/30/qcryptox-post-mortem/">QCryptoX: post-mortem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Our <a href="https://www.edx.org/course/quantum-cryptography-caltechx-delftx-qucryptox">EdX/Delft/Caltech course on quantum cryptography</a> officially ended on Dec. 20th, and is now archived (all material should remain available in the foreseeable future; we will also prepare a self-contained version of the lecture notes). At Caltech, the “flipped classroom” ended a couple weeks earlier; by now all grades are in and the students may even be beginning to recover. <strong>How did it go?</strong></p>
<h2>1. In the classroom</h2>
<p>Fifteen Caltech students, with a roughly equal mix of physics/CS/EE backgrounds, followed the course till the end (we started at ~20). We had a great time, but integration with the online course proved more challenging than I anticipated. Let me say why, in the hope that my experience could be useful to others (including myself, if I repeat the course).</p>
<p>The EdX content was released in 10 weekly updates, on Tuesdays. Since on-campus classes took place Tuesdays and Thursdays, I asked Caltech students to review the material (videos+lecture notes+quizzes) made available online on a given Tuesday by the following Tuesday’s class. I would then be able to structure the class under the assumption that the students had at least some minimal familiarity with the weeks’ concepts. This would allow for a more relaxed, “conversational” mode: I would be able to react to difficulties encountered by the students, and engage them in the exploration of more advanced topics. That was the theory. Some of it worked out, but not quite as well as I had hoped, and this for a variety of reasons:</p>
<ol>
<li><strong>There was a large discrepancy in the students’ level of preparation</strong>. Some had gone through lecture notes in detail, watched all videos, and completed all quizzes. Although some aspects of the week’s material might still puzzle them, they had a good understanding of the basics. But other students had barely pulled up the website, so that they didn’t even really know what topics were covered in a given week. This meant that, if I worked under the assumption that students already had a reasonable grasp of the material, I would loose half the class; whereas if I assumed they had not seen it at all I would put half the class to sleep. As an attempted remedy I enforced some minimal familiarity with the online content by requiring that weekly EdX quizzes be turned in each Tuesday before class. But these quizzes were not hard, and the students could (and did) get away with a very quick scan through the material.</li>
<li>As all students, but, I hear, even more so here, <strong>Caltech undergraduates generally (i) do not show up in class, and (ii) if per chance they happen to land in the right classroom, they certainly won’t participate</strong>. In an attempt to <em>encourage </em>attendance  I made homeworks due right before the Tuesday 10:30am class, the idea being that students would probably turn in homeworks at the last minute, but then they would at least be ready for class. Bad idea: as a result, students ended up spending the night on the homework, dropping it off at 10:29:59… only to skip class so as to catch up on sleep!Slightly over half of the registered students attended any given class, a small group of 8-10 on average. This made it harder to keep participation up. On the whole it still went pretty well, and with a little patience, and insistence, I think I eventually managed to instore a reasonably relaxed atmosphere, where students would naturally raise questions, submit suggestions, etc. But we did not reach the stage of all-out participation I had envisioned.</li>
<li><strong>The material was not easy</strong>. This is partially a result of my inexperience in teaching quantum information; as all bad teachers do I had under-estimated the effort it takes to learn the basis of kets, bras and other “elementary” manipulations, especially when one has but an introductory course in undergraduate linear algebra as background. Given this, I am truly amazed that the 15 students actually survived the class; they had to put in <em>a lot</em> of work. Lucky for me there are bright undergraduates around!We ended the course with projects, on which the students did amazingly well. In groups of 2-3 they read one or more papers in quantum cryptography, all on fairly advanced topics we had not covered in class (such as <a href="https://arxiv.org/abs/quant-ph/9810068">relativistic bit commitment</a>, <a href="https://arxiv.org/abs/1603.09717">quantum homomorphic encryption</a>, <a href="https://arxiv.org/abs/1604.01383">quantum bitcoin</a>, and more!), wrote up a short survey paper outlining some criticisms and ideas they had about what they had read, and gave an invariably excellent course presentation. From my perspective, this was certainly a highlight of the course.</li>
</ol>
<p>Given these observations on what went wrong (or at least sub-optimally), here are a few thoughts on how the course could be improved, mostly for my own benefit (I hope to put some of these to good practice in a year or two!). This should be obvious, but: <strong>the main hurdle in designing a “flipped classroom” is to figure out how to work with the online content</strong>:</p>
<ul>
<li>First there is a scheduling difficulty. Some students complained that by having to go through the weekly videos and lecture notes <em>prior</em> to the discussion of the material in class they simultaneously had to face two weeks’ worth of content at any given time. Scheduling of online material was decided based on other constraints, and turned out to be highly sub-optimal: each week was released on a Tuesday, which was also the first day of class, so that it was unreasonable to ask the students to review the material before that week’s classes….pushing it to the next week, and resulting in the aforementioned overlap. A much better schedule would have been to e.g. release material online on Friday, and then have class on Tuesdays and Thursdays. This would have led to a larger overlap and less schizophrenia.</li>
<li>Then comes the problem of “complementarity”. What can be done in class that does not replicate, but instead enriches, then online material? This is made all the more difficult by the inevitable heterogeneity in the student’s preparation. An effort has to be made to limit this by finding ways to enforce the student’s learning of the material. For instance, each class could be kick-started by a small presentation by one of the students, based on one of the online problems, or even by re-explaining (or, explaining better!) one of the week’s more challenging videos. This should be made in a way that the students find it valuable, both for the presenter and the listeners; I don’t want the outcome to be that no one shows up for class.</li>
<li>Student-led discussions usually work best. They love to expose their ideas to each other, and improve upon them. This forces them to be active, and creative. The best moments in the class where when the discussion really picked up and the students bounced suggestions off each other. The existence of the online material should facilitate this, by giving a common basis on which to base the discussion. My approach this time wasn’t optimal, but based on the experience I think it is possible to do something truly engaging. But it won’t work by itself; one really has to design incentive-based schemes to get the process going.</li>
</ul>
<h2>2. Online</h2>
<p>Success of the online course is rather hard to judge. At the end of the course there were about 8000 officially registered students. Of these, EdX identified ~500 as “active learners” over the last few weeks (dropping from ~1500 over the first few weeks, as is to be expected). I think an active learner is roughly someone who has at least watched some parts of a video, answered a quizz or problem, participated in a forum, etc.<br />
About 100 students pursued an official certificate, which means that they paid ~50$ to have their success in the course officially registered. I couldn’t figure out how many students have actually “passed” the class, but I expect the number to be around 200: most of the certified students plus a few others who didn’t want to pay for the certificate but still turned in most homeworks. This is a fair number for a challenging specialized course, I am pretty happy with it. The high initial enrollment numbers, together with anecdotal evidence from people who got in touch directly, indicate that there certainly is demand for the topic.  The most active students in the course definitely “wanted in”, and we had lots of good questions on the forum. And, many, many typos were fixed!</p>
<p>How satisfied were the students with the course? We ran an “exit survey”, but I don’t have the results yet; I can write about them later (hoping that a significant enough number of students bother to fill in the survey). We also had  pre- and mid-course survey. Some of the more interesting questions had to do with how students learn. In my opinion this is the main challenge in designing a MOOC: how to make it <em>useful</em>? Will the students learn anything by watching videos? Anecdotal evidence (but also serious research, I hear) suggests not. Reading the lecture notes? Maybe, but that requires time and dedication – basically, to be an assiduous learner already. Just as “in-the-classroom” learning, it is the problem-solving that students are brought to engage in that can make a difference. Students like to be challenged; they need to be given an active role. In the mid-course survey many of the positive comments had to do with “Julia lab” assignments that were part of the course, and for which the students had to do some simple coding that let them  experiment with properties of qubits, incompatible measurements, etc. In the pre-course survey students also indicated a marked preference for learning via solving problems rather than by watching videos.</p>
<p>So<strong> a good online MOOC should be one that actively engages the student’s problem-solving skills</strong>. But this is not easy! Much harder than recording a video in front of a tablet &amp; webcam. Even though I was repeatedly told about it before-hand, I learned the lesson the hard way: homework questions have to be vetted <em>very thoroughly.</em> There is no end to a student’s creativity in misinterpreting a statement – let alone 1000 students’. Multiple-choice questions may sound straightforward, but they’re not: one has to be very careful that there is exactly one straight correct answer, while at the same time not making it too obvious which is that answer; when one has a solution in mind it is easy not to realize that other proposed supposedly wrong solutions could in fact be interpreted as correct. The topic of cryptography makes this particularly tricky: we want the students to reason, be creative, devise attacks, but the multiple-choice limits us in this ability. Luckily we had a very dedicated, and creative, team of TAs, both in Delft and In Caltech, and by working together they compiled quite a nice set of problems; I hope they get used and re-used.</p>
<h2>3. Conclusions</h2>
<p>It’s too early (or too late) for conclusions. This was a first, and I hope there’ll be a second. The medium is a challenge, but it’s worth reaching out: we teach elite topics to elite students at elite institutions, but so many more have the drive, interest, and ability to learn the material that it would be irresponsible to leave them out. MOOCs may not be the best way to expand the reach of our work, but it is one way…to be improved!</p>
<p>It was certainly a lot of fun. I owe <strong>a huge thank you to all the students</strong>, in the classroom and online, who suffered through the course. I hope you learned a lot! Second in line were <strong>the TAs</strong>, at Caltech as well as Delft, who did impressive work, coping simultaneously with the heavy online and offline duties. They came up with a great set of resources. Last but not least, behind the scenes, the <strong>video production</strong> and <strong>online learning teams</strong>, from Delt and Caltech, without whose support none of this would have been made possible. Thanks!</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2016/12/30/qcryptox-post-mortem/"><span class="datestr">at December 30, 2016 03:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2016/10/30/unsupervised-learning-ii-the-power-of-improper-convex-relaxations/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/10/30/unsupervised-learning-ii-the-power-of-improper-convex-relaxations/">Unsupervised Learning II: the power of improper convex relaxations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="http://cdn.mathjax.org/mathjax/latest/MathJax.js">http://cdn.mathjax.org/mathjax/latest/MathJax.js</a>   </p>
<div style="text-align: left;" dir="ltr">In this continuation post I’d like to bring out a critical component of learning theory that is essentially missing in today’s approaches for unsupervised learning: improper learning by convex relaxations.<p></p>
<p>Consider the task of learning a sparse linear separator. The sparsity, or $\ell_0$, constraint is non-convex and computationally hard. Here comes the <a href="http://statweb.stanford.edu/~tibs/lasso.html">Lasso</a> – replace the $\ell_0$ constraint with $\ell_1$ convex relaxation — et voila — you’ve enabled polynomial-time solvable convex optimization.</p>
<p><a name="more"></a></p>
<p>Another more timely example: latent variable models for recommender systems, a.k.a. the matrix completion problem. Think of a huge matrix, with one dimension corresponding to users and the other corresponding to media items (e.g. movies as in the Netflix problem). Given a set of entries in the matrix, corresponding to ranking of movies that the users already gave feedback on, the problem is to complete the entire matrix and figure out the preferences of people on movies they haven’t seen.<br />This is of course ill posed without further assumptions. The low-rank assumption intuitively states that people’s preferences are governed by few factors (say genre, director, etc.).  This corresponds to the user-movie matrix having low algebraic rank.</p>
<p>Completing a low-rank matrix is NP-hard in general. However, stemming from the compressed-sensing literature, the statistical-reconstruction approach is to assume additional statistical and structural properties over the user-movie matrix. For example, if this matrix is “incoherent”, and the observations of the entires are obtained via a uniform distribution, then <a href="http://pages.cs.wisc.edu/~brecht/papers/08.Candes.Recht.MatrixCompletion.pdf">this paper by Emmanuel Candes and Ben Recht</a> shows efficient reconstruction is still possible.</p>
<p>But is there an alternative to incoherent assumptions such as incoherence and i.i.d. uniform observations?</p>
<p>A long line of research has taken the “improper learning by convex relaxation approach” and applied it to matrix completion in works such as <a href="http://ttic.uchicago.edu/~nati/Publications/SrebroShraibmanCOLT05.pdf">Srebro and Shraibman</a>, considering convex relaxations to rank such as the trace norm and the max norm. Finally, in  joint work with <a href="https://arxiv.org/abs/1204.0136">Satyen Kale and Shai Shalev-Shwartz</a>, we take this approach to the extreme by not requiring any distribution at all, giving regret bounds in the online convex optimization model (see <a href="http://www.minimizingregret.com/2016/07/more-than-decade-of-online-convex.html">previous post</a>).</p>
<p>By the exposition above one may think that this technique of improper convex relaxations applies only to problems whose hardness comes from “sparsity”.  This is far from the truth, and in the very same <a href="https://arxiv.org/abs/1204.0136">paper</a> referenced above, we show how the technique can be applied to combinatorial problems such as predicting cuts in graphs, and outcomes of matches in tournaments.</p>
<p>In fact, improper learning is such a powerful concept, that problems such that the complexity of problems such as learning DNF formulas has remained open for quite a long time, despite the fact that showing proper learnability was long known to be NP-hard.</p>
<p>On the other hand, improper convex relaxation is not an all powerful magic pill. When designing convex relaxations to learning problems, special care is required so not to increase sample complexity. Consider for example the question of predicting tournaments, as given in this <a href="http://web.eecs.umich.edu/~jabernet/OpenProblemAbernethy.pdf">COLT open problem formulation by Jake Abernethy</a>. The problem, loosely stated, is to iteratively predict the outcome of a match between two competing teams from a league of N teams. The goal is to compete, or make as few mistakes, as the best ranking of teams in hindsight. Since the number of rankings scales as $N!$, the <a href="http://theoryofcomputing.org/articles/v008a006/">multiplicative weights update method</a> can be used to guarantee regret that scales as $\sqrt{T \log N!} = O(\sqrt{T N \log N})$. However, the latter, naively implemented, runs in time proportional to $N!$. Is there an efficient algorithm for the problem attaining similar regret bounds?</p>
<p>A naive improper learning relaxation would treat each pair of competing teams as an instance of binary prediction, for which we have efficient algorithms. The resulting regret bound, however, would scale as the number of pairs of teams over $N$ candidate teams, or as $\sqrt{T N^2}$, essentially removing any meaningful prediction property. What has gone wrong?</p>
<p>By removing the restriction of focus from rankings to pairs of teams, we have enlarged the decision set significantly, and increased the number of examples needed to learn. This is a general and important concern for improper convex relaxations: one needs to relax the problem in such a way that sample complexity (usually measured in terms of Rademacher complexity) doesn’t explode. For the aforementioned problem of predicting tournaments, a convex relaxation that preserves sample complexity up to logarithmic factors is indeed possible, and described in the same <a href="https://arxiv.org/abs/1204.0136">paper</a>.</p>
<p>In the coming posts we’ll describe a framework for unsupervised learning that allows us to use the power of improper convex relaxations.</p>
</div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/10/30/unsupervised-learning-ii-the-power-of-improper-convex-relaxations/"><span class="datestr">at October 30, 2016 01:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=782">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2016/10/24/two-weeks-in/">Two weeks in</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Our <a href="https://www.edx.org/course/quantum-cryptography-caltechx-delftx-qucryptox">course on quantum cryptography</a> will soon enter its third week of instruction (out of ten weeks planned). My parallel “in-class” course at Caltech has already gone through four weeks of lectures. How is it going so far?</p>
<p>There are many possible measures against which to evaluate the experience. An easy one is raw numbers. Online there are a bit over 7,200 students enrolled. But how many are “active”? The statistics tools provided by EdX report 1995 “active” students last week – under what definition of “active”? EdX also reports that 1003 students “watched a video”, and 861 “tried a problem”. What is an active student who neither watched a video nor tried a problem – they clicked on a link? In any case, the proportion seems high; from what I heard a typical experience is that about 2-5% of registered students will complete any given online course. Out of 7,000, this would bring the number of active students by the end of the course at at a couple hundred, a number I would certainly consider a marked success, given the specialized topic and technically challenging material.</p>
<p>At Caltech there are 20 students enrolled in CS/Phys 120. Given the size of our undergraduate population I also consider this to be a rather high number (but the hard drop deadline has not passed yet!). It’s always a pleasure to see our undergraduate’s eagerness to dive into any exciting topic of research that is brought to their attention. I don’t know the enrollment for TU Delft, but they have a large program in quantum information so the numbers are probably at least twice as high.</p>
<p>Numbers are numbers. How about enthusiasm? You saw the word cloud we collected in Week 0. Here is one from Week 2 (“What does “entanglement” evoke in you right now?”; spot the “love” and “love story”; unfortunately only 1% of responses for either!). <img src="https://mycqstate.files.wordpress.com/2016/10/word2.jpg?w=440&amp;h=455" alt="word2" width="440" class="  wp-image-882 alignright" height="455" />Some of the students speak up when prompted for simple feedback such as this, but the vast majority remain otherwise silent, so that involvement is hard to measure. We do have a few rather active participants in the discussion forums, and it’s been a pleasure to read and try to answer their creative questions each day – dear online learners, if you read this, thanks for your extremely valuable comments and feedback, which help make the course better for everyone! It’s amazing how even as we were learning qubits some rather insightful questions, and objections, were raised. It’s clear that people are coming to the course from a huge range of backgrounds, prompting the most unexpected reactions.</p>
<p>A similar challenge arises in the classroom. Students range from the freshmen with no background in quantum information (obviously), nor in quantum mechanics or computer science, to more advanced seniors (who form the bulk of the class) to graduate students in Caltech’s Institute for Quantum Information and Matter (IQIM). How to capture everyone’s attention, interest, imagination? The topic of cryptography helps -there is so much to be fascinated with. I started the course by discussing the problem of <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>, which has the advantage of easily capturing one’s imagination, and for which there is a simple quantum scheme with a clear advantage over classical (cherry on top, the scheme is based on the famous “BB84 states” that will play a major role in the class via their use for quantum key distribution). So newcomers to quantum information could learn about qubits, kets and bras, while others could fend off their impatience by imagining new schemes for public-coin quantum money.</p>
<p>This is not an easy line to thread however, and given the composition of the class I eventually decided to err on the side of caution. Don’t repeat it, but this is my first time even teaching a full class on quantum information, and the basic concepts, not to mention the formalism, can be quite tricky to pick up. So we’re going to take it slow, and we’ll see how far we get. My hope is that the “flipped classroom” format should help needy but motivated students keep afloat by making all the online material available before it is discussed in class. Since the online course has only been going on for a couple weeks I can’t yet report on how well this will work out; my initial impression is that it is not given that the in-class students actually do spend enough time with the online material. I am yet to find the proper way to incentivize this: quizzes? rewards? The best reward should be that they manage to follow the course <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" alt="😉" style="height: 1em;" class="wp-smiley" /></p>
<p>In the coming weeks we’ll start making our way towards quantum key distribution and its analysis. Entanglement, measuring uncertainty, privacy amplification, BB84 and Eckert, and device independence. Quite a program, and it’s certainly nice to attempt it in such pleasant company!</p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2016/10/24/two-weeks-in/"><span class="datestr">at October 24, 2016 04:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2016/10/06/a-mathematical-definition-of-unsupervised-learning/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/10/06/a-mathematical-definition-of-unsupervised-learning/">A mathematical definition of unsupervised learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">Extracting structure from data is considered by many to be the frontier of machine learning. Yet even defining “structure”, or the very goal of learning structure from unlabelled data, is not well understood or rigorously defined.<p></p>
<p>In this post we’ll give a little background to unsupervised learning and argue that, compared to supervised learning, we lack a well-founded theory to tackle even the most basic questions. In the next post, we’ll introduce a new candidate framework.</p>
<p><a name="more"></a></p>
<p>Unsupervised learning is a commonplace component of many undergraduate machine learning courses and books. At the core, the treatment of unsupervised learning is a collection of methods for analyzing data and extracting hidden structure from this data.  Take for example <a href="http://cs229.stanford.edu/schedule.html">John Duchi’s “Machine Learning” course at Stanford</a>. Under “Unsupervised learning”, we have the topics: Clustering, K-means, EM. Mixture of Gaussians, Factor analysis, PCA (Principal components analysis), ICA (Independent components analysis). This is an excellent representation of the current state-of-the-art:</p>
<p></p>
<ul style="text-align: left;">
<li>Clustering and K-means:  by grouping “similar” elements together, usually according to some underlying metric, we can create artificial “labels” for data elements in hope that this rough labelling will be of future use, either in a supervised learning task on the same dataset, or in “understanding” the data. K-means is a widely used algorithm for finding k representative centers of the data, allowing for k “labels”.</li>
<li>Mixture of Gaussians: an exemplary and widely used method stemming from the generative approach to unsupervised learning. This approach stipulates that the data is generated from a distribution, in this case a mixture of normal random variables. There are numerous algorithms for finding the centers of these Gaussians, thereby learning the structure of the data.</li>
<li>Factor analysis, PCA and ICA: the spectral approach to unsupervised learning is perhaps the most widely used, in which the covariance matrix of the data is approximated by the largest few singular vectors. This type of clustering is widely successful in text (word embeddings) and many other forms of data. </li>
</ul>
<p>The above techniques are widely used and successful in practice, and under suitable conditions polynomial-time algorithms are known. The common thread amongst them is finding structure in the data, which turns out for many practical applications to be useful in a variety of supervised learning tasks.</p>
<p>Notable unsupervised learning techniques that are missing from the above list are Restricted Boltzman machines (RBMs), Autoencoders and Dictionary Learning (which was conceived in the context of deep neural networks). The latter techniques attempt to find a succinct representation of the data. Various algorithms and heuristics for RBMs, Autotencoders and Dictionary Learning exist, but these satisfy at least one of the following:</p>
<p>1) come without rigorous performance guarantees.</p>
<p>2) run in exponential time in the worst case.</p>
<p>3) assume strong generative assumptions about the input.</p>
<p>Recent breakthroughs in polynomial time unsupervised learning due to Sanjeev and his group address points (1) &amp; (2) above and require only (3). Independently the same is also achievable by the method of moments, see e.g. <a href="http://arxiv.org/abs/1210.7559">this paper</a>, originally from Sham’s group @ MSR New England, and many more recent advances. What’s the downside of this approach? The Arora-Hazan debate over this point, which the theory-ML students are exposed to in our weekly seminar, is subject for a longer post…</p>
<p>What is missing then? Compare the above syllabus with that of supervised learning, in most undergrad courses and books, the difference in terms of theoretical foundations is stark. For supervised learning  we have Vapnik’s statistical learning theory –  a deep and elegant mathematical theory that classifies exactly the learnable from labeled real-valued examples. Valiant’s computational learning theory adds the crucial element of computation, and over the combined result we have hundreds of scholarly articles describing in detail problems that are learnable efficiently / learnable but inefficiently /  improperly learnable / various other notions.</p>
<p>Is there meaning, in pure mathematical sense such as Vapnik and Valiant instilled into supervised learning, to the unsupervised world?</p>
<p>I like to point out in my ML courses that computational learning theory say something philosophical about science: the concept of “learning”, or a “theory” such as a physical theory of the world, has a precise mathematical meaning independent of humans. While many of us scientists seek a “meaningful” theory in the human sense, there need not be one! It could very well be that a physical theory, for example that of condensed matter, has a “theory” in the Vapnik-Valiant sense, but not one that would be as “meaningful” and “elegant” in the <a href="https://www.ted.com/talks/richard_feynman">Feynman</a> sense. </p>
<p>How can we then, give mathematical meaning to unsupervised learning in a way that:</p>
<ol style="text-align: left;">
<li>Allows natural notions of generalization from examples</li>
<li>Improves future supervised learning tasks regardless of their nature</li>
<li>Allows for natural family of algorithms (hopefully but not necessarily – based on convex optimization)</li>
</ol>
<p>This will be the starting point for our next post…</p>
</div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/10/06/a-mathematical-definition-of-unsupervised-learning/"><span class="datestr">at October 06, 2016 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://mycqstate.wordpress.com/?p=735">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/vidick.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://mycqstate.wordpress.com/2016/09/16/one-week-later/">One week later…</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>…and 736 words of expectation for the class:</p>
<p><img src="https://mycqstate.files.wordpress.com/2016/09/wordcloud.jpg?w=492&amp;h=513" alt="wordcloud" width="492" class="alignnone  wp-image-737" height="513" /></p>
<p>Note the top contender: let’s see if we live up to their expectations!</p>
<p>It’s been a fun first week. We released “Week 0” of the material a month ahead of the official start date, so that those students not yet familiar with the basics of quantum information (what is a qubit?) would have enough time to digest the fundamental notions we’ll be using throughout. (Out of the ~5500 registered students, ~1250 are currently marked my EdX as “active” and 560 have watched at least one video. Keep it coming!)</p>
<p>An unexpected benefit of opening up the platform ahead of time is that it is giving us the time to experiment with (read: debug) some of the tools we plan to use throughout. A first is EdX’s own possibilities for interaction with the students, an example of which is pictured above (“Use the space below to enter a few words that best characterize your expectations for this class”).But we’re also using a couple add-ons:</p>
<p>The first is a system replacing EdX’s discussion forums,called <a href="http://tiny.cc/askalot">Askalot</a>. Cute name – how can you resist. The main benefit of Askalot is that it provides more structure to the discussions, which can be characterized as question/bug report/social discussion/etc, can be up/down-voted, marked as correct/invalidated by the instructors, etc. The students are having fun already, introducing themselves, complaining about bugs in the quizzes, and, of course, about Askalot itself! (Thanks go to Ivan Srba, one of the creators of Askalot, for being extremely responsive and fixing a host of minor bugs overnight – not to mention throwing in the extra feature requested by the students.)</p>
<p>A second is called <a href="http://www.slideshare.net/sameerbhatnagarpolymtl/edm2015presentation-49850960">DALITE</a>. The idea of DALITE is to encourage students to provide an explanation justifying their answer to a question. Indeed, one of the drawbacks of the online platform is the need for automatic grading of assignments, which greatly limits how the student can be engaged in the problem-solving exercise, mostly limited to multiple-choice or simple numeric answers. DALITE (which grew out of, and is still, a serious research project in online learning) introduces a fun twist: the student is asked to type in a “rationale” for her choice. Of course there is no way we could grade such rationales. But here is the idea: once the student has entered her explanation, she is shown the rationale provided by another student (or the instructor) for a different answer, and asked whether she would like to reconsider her decision. The student can choose to change her mind, or stick with her initial choice; she is asked to explain why. It’s really fun to watch the answers provided (“What would happen if quantum mechanics allowed cloning of arbitrary states?”), the change of minds that take place, and the rationale that incentivized said change of mind. (Thanks to Sameer Bhatagnar for helping us set up the tool and explaining its many possibilities, and to Joseph Williams for suggesting its use in the first place.)</p>
<p>We’ll see how these pan out in the longer run. I’m definitely eager to experiment with ways to make the MOOC experience a better learning experience for the students. I’ll let you know how it goes. Suggestions welcome!</p>
<p>PS: 47% students 25 and under, 78% from outside US, is good, but 16% female is not…come on!</p>
<p> </p></div>







<p class="date">
by Thomas <a href="https://mycqstate.wordpress.com/2016/09/16/one-week-later/"><span class="datestr">at September 16, 2016 03:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2016/07/07/more-than-a-decade-of-online-convex-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/07/07/more-than-a-decade-of-online-convex-optimization/">More than a decade of online convex optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">This nostalgic post is written after a <a href="http://www.cs.princeton.edu/~ehazan/tutorial/tutorial.htm">tutorial in ICML 2016</a> as a recollection of a few memories with my friend <a href="http://www.satyenkale.com/">Satyen Kale</a>.<p></p>
<p><a name="more"></a></p>
<p>In ICML 2003 Zinkevich published his paper “<a href="https://www.aaai.org/Papers/ICML/2003/ICML03-120.pdf">Online Convex Programming and Generalized Infinitesimal Gradient Ascent</a>” analyzing the performance of the popular gradient descent method in an online decision-making framework.</p>
<p>The framework addressed in his paper was an iterative game, in which a player chooses a point in a convex decision set, an adversary chooses a cost function, and the player suffers the cost which is the value of the cost function evaluated at the point she chose. The performance metric in this setting is taken from game theory: minimize the <b>regret</b> of the player – which is defined to be the difference of the total cost suffered by the player and that of the best <b>fixed</b> decision in hindsight.</p>
<p>A couple of years later, circa 2004-2005, a group of theory students at Princeton decide to hedge their bets in the research world. At that time, finding an academic position in theoretical computer science was extremely challenging, and looking at other options was a reasonable thing to do. These were the days before the financial meltdown, when a Wall-Street job was the dream of Ivy League graduates.</p>
<p>In our case – hedging our bets meant taking a course in finance at the ORFE department and to look at research problems in finance. We fell upon Tom Cover’s timeless paper “<a href="http://www-isl.stanford.edu/~cover/papers/paper93.pdf">universal portfolios</a>” (I was very fortunate to talk with the great information theorist a few years later in San Diego and him tell about his influence in machine learning).  As good theorists, our first stab at the problem was to obtain a polynomial time algorithm for universal portfolio selection, which we did. Our paper didn’t get accepted to the main theory venues at the time, which turned out for the best in hindsight, pun intended <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" alt="🙂" style="height: 1em;" class="wp-smiley" /> </p>
<p>Cover’s paper on universal portfolios was written in the language of information theory and universal sequences, and applied to wealth which is multiplicatively changing. This was very different than the additive, regret-based and optimization-based paper of Zinkevich.</p>
<p>One of my best memories of all times is the moment in which the connection between optimization and Cover’s method came to mind. It was more than a “guess” at first:  if online gradient descent is effective in online optimization, and if Newton’s method is even better for offline optimization, why can we use Newton’s method in the online world?  Better yet – why can’t we use it for portfolio selection?</p>
<p>It turns out that indeed it can, thereby the <a href="http://cs.princeton.edu/~ehazan/papers/log-journal.pdf">Online Newton Step</a> algorithm came to life, applied to portfolio selection, and presented in COLT 2016 (along with a follow-up paper devoted only to portfolio selection, with <a href="http://rob.schapire.net/">Rob Schapire</a>.  Satyen and me had the nerve to climb up to Rob’s office and waste his time for hours at a time, and Rob was too nice to kick us out…). </p>
<p>The connection between optimization, online learning, and the game theoretic notion of regret has been very fruitful since, giving rise to a multitude of applications, algorithms and settings. To mention a few areas that spawned off:</p>
<ul style="text-align: left;">
<li>Bandit convex optimization – in which the cost value is the only information available to the online player (rather than the entire cost function, or its derivatives). <br />This setting is useful to model a host of limited-observation problems common in online routing and reinforcement learning.</li>
<li>Matrix learning (also called “local learning”) – for capturing problems such as recommendation systems and the matrix completion problem, online gambling and online constraint-satisfaction problems such as online max-cut.</li>
<li>Projection free methods – motivated by the high computational cost of projections of first order methods, the Frank-Wolfe algorithm came into renewed interest in recent years. The <a href="http://icml.cc/2012/papers/292.pdf">online version</a> is particularly useful for problems whose decision set is hard to project upon, but easy to perform linear optimization over. Examples include the spectahedron for various matrix problems, the flow polytope for various graph problems, the cube for submodular optimization, etc.<br /> </li>
<li>Fast first-order methods – the connection of online learning to optimization introduced some new ideas into optimization for machine learning. One of the first examples is the <a href="http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf">Pegasus</a> paper. By now there is a flurry of optimization papers in each and every major ML conference, some incorporate ideas from online convex optimization such as adaptive regularization, introduced in the <a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">AdaGrad</a> paper. </li>
</ul>
<div>There are a multitude of other connections that should be mentioned here, such as the recent literature on adversarial MDPs and online learning, connections to game theory and equilibrium in online games, and many more. For more (partial) information, see our <a href="http://www.cs.princeton.edu/~ehazan/tutorial/tutorial.htm">tutorial webpage</a> and this <a href="http://ocobook.cs.princeton.edu/OCObook.pdf">book draft</a>. </div>
<p></p>
<div>It was a wild ride!  What’s next in store for online learning?  Some exciting new directions in future posts…</div>
<div></div>
<div></div>
<div></div>
</div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/07/07/more-than-a-decade-of-online-convex-optimization/"><span class="datestr">at July 07, 2016 02:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2016/05/26/the-complexity-zoo-and-reductions-in-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/05/26/the-complexity-zoo-and-reductions-in-optimization/">The complexity zoo and reductions in optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">(post by Zeyuan Allen-Zhu and Elad Hazan)<p></p>
<p>The following dilemma is encountered by many of my friends when teaching basic optimization: which variant/proof of gradient descent should one start with? Of course, one needs to decide on which depth of convex analysis one should dive into, and decide on issues such as “should I define strong-convexity?”, “discuss smoothness?”, “Nesterov acceleration?”, etc.</p>
<p><a name="more"></a></p>
<p>This is especially acute for courses that do not deal directly with optimization, which is described as a tool for learning or as a basic building block for other algorithms. Some approaches:</p>
<ul style="text-align: left;">
<li>I teach online gradient descent, in the context of online convex optimization, and then derive the offline counterpart. This is non-standard, but permits an extremely simple derivation and gets to the online learning component first. </li>
<li>Sanjeev Arora teaches basic offline GD for the smooth and strongly-convex case first. </li>
<li>In OR/optimization courses the smooth (but not strongly-convex) case is many times taught first. </li>
</ul>
<p>All of these variants have different proofs whose connections are perhaps not immediate. If one wishes to go into more depth, usually in convex optimization courses one covers the full spectrum of different smoothness/strong-convexity/acceleration/stochasticity regimes, each with a separate analysis (a total of 16 possible configurations!)</p>
<p>This year I’ve tried something different in COS511 @ Princeton, which turns out also to have research significance. We’ve covered basic GD for well-conditioned functions, i.e. smooth and strongly-convex functions, and then <b>extended</b> these result <b>by reduction</b> to all other cases!<br />A (simplified) outline of this teaching strategy is given in chapter 2 of this <a href="http://ocobook.cs.princeton.edu/">book</a>. </p>
<h3></h3>
<h3>Classical Strong-Convexity and Smoothness Reductions</h3>
<p>Given any optimization algorithm A for the well-conditioned case (i.e., strongly convex and smooth case), we can derive an algorithm for smooth but not strongly functions as follows.</p>
<p>Given a non-strongly convex but smooth objective $f$, define a objective by<br />$$ \text{strong-convexity reduction:} \qquad f_1(x) = f(x) +  \epsilon \|x\|^2 $$<br />It is straightforward to see that $f_1$ differs from $f$ by at most $\epsilon$ times a distance factor, and in addition it is $\epsilon$-strongly convex. Thus, one can apply $A$ to minimize $f_1$ and get a solution which is not too far from the optimal solution for $f$ itself.  This simplistic reduction yields an almost optimal rate, up to logarithmic factors.</p>
<p>Similar simplistic assumptions can be derived for (finite-sum forms of) non-smooth by strongly-convex functions (via randomized smoothing or Fenchel duality), and for functions that are neither smooth nor strongly-convex by just applying both reductions simultaneously. Notice that such classes of functions include famous machine learning problems such as SVM, Logistic Regression, SVM, L1-SVM, Lasso, and many others.</p>
<h3></h3>
<h3>Necessity of Reductions</h3>
<div>This is not only a pedagogical question. In fact, very few algorithms apply to the entire spectrum of strong-convexity / smoothness regimes, and thus reductions are very often<i> intrinsically</i> necessary. To name a few examples,</div>
<div>
<ul>
<li>Variance-reduction methods such as SAG, SAGA and SVRG require the objective to be smooth, and do not work for non-smooth problems like SVM. This is because for loss functions such as hinge loss, no unbiased gradient estimator can achieve a variance that approaches to zero</li>
<li>Dual methods such as SDCA or APCG require the objective to be strongly convex, and do not directly apply to non-strongly convex problems. This is because for non-strongly convex objectives such as Lasso, their duals are not even be well-defined.</li>
<li>Primal-dual methods such as SPDC require the objective to be both smooth and SC.</li>
</ul>
</div>
<h3></h3>
<h3>Optimality and Practicality of Reductions</h3>
<div>The folklore strong-convexity and smoothing reductions are suboptimal. Focusing on the strong-convexity reduction for instance:</div>
<div>
<ul>
<li>It incurs a logarithmic factor log(1/ε) in the running time so leading to slower algorithms than direct methods. For instance, if $f(x)$ is smooth but non-strongly convex, gradient descent minimizes it to an $\epsilon$ accuracy in $O(1/\epsilon)$ iterations; if one uses the reduction instead, the complexity becomes $O(\log(1/\epsilon)/\epsilon)$.</li>
<li>More importantly, algorithms based on such reductions become <i><b>biased</b></i>: the original objective value $f(x)$ does not converge to the global minimum, and if the desired accuracy is changed, one has to change the weight of the regularizer $\|x\|^2$ and restart the algorithm. </li>
</ul>
</div>
<p>These theoretical concerns also translate into running time losses and parameter tuning difficulties in practice. For such reasons, researchers usually make efforts on designing unbiased methods instead.</p>
<ul>
<li>For instance, <a href="http://papers.nips.cc/paper/4937-accelerating" target="_blank">SVRG</a> in theory solves the Lasso problem (with smoothness L) in time $O((nd+\frac{Ld}{\epsilon})\log\frac{1}{\epsilon})$ using reduction. Later, direct and unbiased methods for Lasso are introduced, including <a href="http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives" target="_blank">SAGA</a> which has running time $O(\frac{nd+Ld}{\epsilon})$, and <a href="http://arxiv.org/abs/1506.01972" target="_blank">SVRG++</a> which has running time $O(nd \log\frac{1}{\epsilon} + \frac{Ld}{\epsilon})$.</li>
</ul>
<p>One can find academic papers derive various optimization improvements many times for only one of the settings, leaving the other settings desirable. An <b><i>optimal and unbiased</i></b> black-box reduction is thus a tool to extend optimization algorithms from one domain to the rest.</p>
<h3></h3>
<h3>Optimal, Unbiased, and Practical Reductions</h3>
<div>In this <a href="http://arxiv.org/abs/1603.05642">paper</a>, we give optimal and unbiased reductions. For instance, the new reduction, when applied to SVRG, implies the same running time as SVRG++ up to constants, and is unbiased so converges to the global minimum. Perhaps more surprisingly, these new results imply<b><i> new</i></b> theoretical results that were not previously known by direct methods. To name two of such results:</div>
<div>
<ul>
<li>On Lasso, it gives an accelerated running time $O(nd \log\frac{1}{\epsilon} + \frac{\sqrt{nL}d}{\sqrt{\epsilon}})$ where the best known result was not only an biased algorithm but also slower $O( (nd + \frac{\sqrt{nL}d}{\sqrt{\epsilon}}) \log\frac{1}{\epsilon} )$.</li>
<li>On SVM with strong convexity $\sigma$, it gives an accelerated running time $O(nd \log\frac{1}{\epsilon} + \frac{\sqrt{n}d}{\sqrt{\sigma \epsilon}})$ where the best known result was not only an biased algorithm but also slower $O( (nd + \frac{\sqrt{n}d}{\sqrt{\sigma \epsilon}}) \log\frac{1}{\epsilon} )$.</li>
</ul>
<p>These reductions are surprisingly simple. In the language of strong-convexity reduction, the new algorithm starts with a regularizer $\lambda \|x\|^2$ of some large weight $\lambda$, and then keeps halving it throughout the convergence. Here, the time to decrease $\lambda$ can be either decided by theory or by practice (such as by computing duality gap).</p>
<p>A figure to demonstrate the practical performance of our new reduction (red dotted curve) as compared to the classical biased reduction (blue curves, with different regularizer weights) are presented in the figure below.</p></div>
<div style="text-align: center;"><a href="https://minimizingregret.files.wordpress.com/2016/05/7e6fe-sdca-8.png"><img width="320" src="https://minimizingregret.files.wordpress.com/2016/05/7e6fe-sdca-8.png?w=320&amp;h=182" border="0" height="182" /></a><p></p>
<div style="text-align: left;">As a final word – if you were every debating whether to post your paper on ArXiV, yet another example of how quickly it helps research propagate:  only a few weeks after our paper was made available online, Woodworth and Srebro have already made use of our reductions in their new <a href="https://arxiv.org/abs/1605.08003">paper</a>.  </div>
</div>
<p></p></div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/05/26/the-complexity-zoo-and-reductions-in-optimization/"><span class="datestr">at May 26, 2016 01:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://minimizingregret.wordpress.com/2016/03/03/the-two-cultures-of-optimization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/03/03/the-two-cultures-of-optimization/">The two cultures of optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">The standard curriculum in high school math includes elementary functional analysis, and methods for finding the minima, maxima and saddle points of a single dimensional function.  When moving to high dimensions, this becomes beyond the reach of your typical high-school student: mathematical optimization theory spans a multitude of involved techniques in virtually all areas of computer science and mathematics. <p></p>
<p>Iterative methods, in particular, are the most successful algorithms for large-scale optimization and the most widely used in machine learning. Of these, most popular are first-order gradient-based methods due to their very low per-iteration complexity.</p>
<p>However, way before these became prominent, physicists needed to solve large scale optimization problems, since the time of the Manhattan project at Los Alamos. The problems that they faced looked very different, essentially simulation of physical experiments, as were the solutions they developed. The Metropolis algorithm is the basis for randomized optimization methods and Markov Chain Monte Carlo algorithms.</p>
<p>To use the terminology of Breiman’s famous <a href="http://projecteuclid.org/euclid.ss/1009213726">paper</a>, the two cultures of optimization have independently developed to fields of study by themselves. </p>
<p>The readers of this particular blog may be more familiar with the literature on iterative methods. The hallmark of polynomial time methods for mathematical optimization are so called “interior point methods”, based on Newton’s method. Of these, the most efficient and well-studied are “central path following” methods, established in the pioneering work of Nesterov and Nemirovski (see <a href="http://www2.isye.gatech.edu/~nemirovs/Lect_IPM.pdf">this</a> excellent introductory text).</p>
<p>On the other side of the world, the early physicists joined forces with engineers and other scientists to develop highly successful Monte Carlo simulation algorithms. This approach grew out of work in statistical mechanics, and a thorough description is given in <a href="http://minds.jacobs-university.de/sites/default/files/uploads/teaching/share/KirkpatrickSimulatedAnnealing.pdf">this</a> 1983 <i>Science</i> paper by Kirkpatrick, Gelatt, and Vecchi. One of the central questions of this area of research is what happens to particular samples of liquid or solid matter as the temperature of the matter is dropped. As the sample is cooled, the authors ask:</p>
<blockquote class="tr_bq"><p>[…] for example, whether the atoms remain fluid or solidify, and if they solidify, whether they form a crystalline solid or a glass. Ground states and configurations close to them in energy are extremely rare among all the configurations of a macroscopic body, yet they dominate its properties at low temperatures because as T is lowered the Boltzmann distribution collapses into the lowest energy state or states.</p></blockquote>
<p>In this groundbreaking work drawing out the connections between the cooling of matter and the problem of combinatorial optimization, a new algorithmic approach was proposed which we now refer to as <i>Simulated Annealing</i>. In more Computer Science-y terminology, the key idea is to use random walk sampling methods to select a point in the convex set from a distribution which has high entropy (i.e. at a “high temperature”), and then to iteratively modify the distribution to concentrate around the optimum (i.e. sampling at “low temperature”), where we need to perform additional random walk steps after each update to the distribution to guarantee “mixing” (more below). There has been a great deal of work understanding annealing methods, and they are quite popular in practice.</p>
<p>In a recent <a href="http://arxiv.org/abs/1507.02528">paper</a>, we have discovered a close connection between the two methodologies. Roughly speaking, for constrained <b>convex</b> optimization problems, the iterates of Newton’s method lie on the same path as the means of the consecutive distributions of simulated annealing. This is depicted in the picture below. Besides the intriguing connection, this discovery yields algorithmic benefits as well: a faster convex optimization algorithm for the most general input access model, and a resolution of a open problem in interior point methods. </p>
<div style="text-align: center;"><a href="https://minimizingregret.files.wordpress.com/2017/08/70b42-heatpath_with_samples.png"><img width="320" src="https://minimizingregret.files.wordpress.com/2017/08/70b42-heatpath_with_samples.png?w=320&amp;h=265" border="0" height="265" /></a></div>
<p>We continue with a more detailed description of path-following interior point methods, simulated annealing, and finally a discussion of the consequences.</p>
<h3 style="text-align: left;">An overview of the Statistical Mechanics approach to optimization </h3>
<div></div>
<p>The Simulated Annealing approach to optimization is to reduce a given formulation to a sampling problem.  Consider the following distribution over the set $K \subseteq R^n$ for a given function $f: R^n \mapsto R$.<br />\begin{equation}<br />\textstyle<br />  P_{f,t}(x) := \frac{\exp(- f(x)/t )}<br />  { \int_K \exp(-f(x’)/t ) \, dx’ }.<br />\end{equation}<br />This is often referred to as the Boltzmann distribution.  Clearly – as $t$ approaches zero, this distribution approaches a point-mass concentrated on the global minimizer of $f$.</p>
<p>The question is now – how do we sample from the Boltzman distribution? Here comes the fantastic observation of the Los Alamos scientists:  formulate a random walk such that its stationary distribution is exactly the one we wish to sample from. Then simulate the random walk till it mixes – and voila – you have a sample!</p>
<p>The Metropolis algorithm does exactly that – a general recipe for creating Markov Chains whose stationary distribution is whatever we want. The crucial observation is that simulating a random walk is many times significantly easier than reasoning about the distribution itself. Furthermore, it so happens that the mixing time in such chains, although notoriously difficult to analyze rigorously, are very small in practice.</p>
<p>The analogy to physics is as follows – given certain initial conditions for a complex system along with evolution rules, one is interested in system behavior at “steady state”.  Such methods were used to evaluate complex systems during WWII in the infamous Manhattan project.</p>
<p>For optimization, one would use a random walk technique to iteratively sample from ever decreasing temperature parameters $t \mapsto 0$. For $t=0$ sampling amounts to optimization, and thus hard.</p>
<p>The simulated annealing method slowly changes the temperature. For $t = \infty$, the Boltzmann distribution amounts to uniform sampling, and it is reasonable to assume this can be done efficiently. Then, exploiting similarity in consecutive distributions, one can use samples from one temperature (called a “warm start”) to efficiently sample from a cooler one, till a near-zero temperature is reached.  But how to sample from even one Boltzman distributions given a warm start? There is vast literature on random walks and their properties. Popular walks include the “hit and run from a corner”, “ball walk”, and many more.</p>
<p>We henceforth define the <b>Heat Path </b>as the deterministic curve which maps a temperature to the mean of the Boltzmann distribution for that particular temperature. <br />$$ \mu_t = E_{x \sim P_{f,t}} [ x] $$<br />Incredibly, to the best of our knowledge, this fascinating curve was not studied before as a deterministic object in the random walks literature.</p>
<h3 style="text-align: left;">An overview of the Mathematics approach to optimization </h3>
<p>It is of course infeasible to do justice to the large body of work addressing these ideas within the mathematics/ML literature. Let us instead give a brief intro to Interior Point Methods (IPM) – the state of the art in polynomial time methods for convex optimization.</p>
<p>The setting of IPM is that of constrained convex optimization, i.e. minimizing a convex function subject to convex constraints.</p>
<p>The basic idea of IPM is to reduce constrained optimization to unconstrained optimization, similarly to Lagrangian relaxation. The constraints are folded into a single penalty function, called a “barrier function”, which guaranties three properties:</p>
<ol style="text-align: left;">
<li>The barrier approaches infinity near the border of the convex set, thereby ensuring that the optimum is obtained inside the convex set described by the constraints. </li>
<li>The barrier doesn’t distort the objective function too much, so that solving for the new unconstrained problem will give us a close solution to the original formulation.</li>
<li>The barrier function allows for efficient optimization via iterative methods, namely Newton’s method.</li>
</ol>
<p>The hallmark of IPM is the existence of such barrier functions with all three properties, called self-concordant barrier functions, that allow efficient deterministic polynomial-time algorithms for many (but not all) convex optimization formulations. </p>
<p>Iteratively, a “temperature” parameter multiplied by the barrier function is added to the objective, and the resulting unconstrained formulation solved by Newton’s method.  The temperature is reduced, till the barrier effect becomes negligible compared to the original objective, and a solution is obtained.</p>
<p>The curve mapping the temperature parameter to the solution of the unconstrained problem is called the <b>central path</b>. and the overall methodology called “central path following methods”. </p>
<p></p>
<h3 style="text-align: left;">The connection</h3>
<div style="text-align: left;"><span style="font-weight: normal;"><br /></span><span style="font-weight: normal;">In our recent paper, we show that for </span><span style="font-weight: normal;"><b>convex</b></span><span style="font-weight: normal;"> optimization, the </span><span style="font-weight: normal;"><b>heat path</b></span><span style="font-weight: normal;"> and </span><span style="font-weight: normal;"><b>central path</b></span><span style="font-weight: normal;"> for IPM for a particular barrier function (called the <i>entropic barrier</i>, following the terminology of the recent excellent work of <a href="http://arxiv.org/abs/1412.1587">Bubeck and Eldan</a>) are identical! Thus, in some precise sense, the two cultures of optimization have been studied the same object in disguise and using different techniques. </span></div>
<div style="text-align: left;"><span style="font-weight: normal;"><br /></span></div>
<div style="text-align: left;"><span style="font-weight: normal;">Can this observation give any new insight to the design of efficient algorithm? The answer turns out to be affirmative. </span></div>
<div style="text-align: left;"></div>
<div style="text-align: left;">First, we resolve the long standing question in IPM on the existence of an efficiently computable self-concordant barrier for general convex sets. We show that the covariance of the Boltzman distribution is related to the Hessian of a self-concordant barrier (known as the entropic barrier) for any convex set. For this computation, all we need is a membership oracle for the convex set. </div>
<div style="text-align: left;"></div>
<div style="text-align: left;">Second, our observation gives rise to a faster annealing temperature for convex optimization, and a much simpler analysis (building on the seminal work of <a href="http://www.cc.gatech.edu/~vempala/papers/adamanneal.pdf">Kalai and Vempala</a>). This gives a faster polynomial-time algorithm for convex optimization in the membership oracle model – a model in which the access to the convex set is only through an oracle answering questions of the form “is a certain point x inside the set?”. </div>
<div></div>
</div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/03/03/the-two-cultures-of-optimization/"><span class="datestr">at March 03, 2016 05:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://minimizingregret.wordpress.com/2016/03/02/making-second-order-methods-practical-for-machine-learning/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/hazan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://minimizingregret.wordpress.com/2016/03/02/making-second-order-methods-practical-for-machine-learning/">Making second order methods practical for machine learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div style="text-align: left;" dir="ltr">First-order methods such as Gradient Descent, AdaGrad, SVRG, etc. dominate the landscape of optimization for machine learning due to their extremely low per-iteration computational cost. Second order methods have largely been ignored in this context due to their prohibitively large time complexity. As a general rule, any super-linear time operation is prohibitively expensive for large scale data analysis. In our recent <a href="http://arxiv.org/abs/1602.03943">paper</a> we attempt to bridge this divide by exhibiting an efficient linear time second order algorithm for typical <b>(</b>convex<b>)</b> optimization problems arising in machine learning.<p></p>
<p>Previously, second order methods were successfully implemented in linear time for special optimization problems such as maximum flow [see e.g. this <a href="http://arxiv.org/abs/1010.2921">paper</a>] using very different techniques. In the optimization community, Quasi-Newton methods (such as BFGS) have been developed which use first order information to approximate a second order step. Efficient implementations of these methods (such as L-BFGS) have been proposed, mostly without theoretical guarantees.</p>
<p>Concretely, consider the PAC learning model where given $m$ samples $\{(\mathbf{x}_k, y_k)\}_{k\in \{1,\dots, m\}}$ the objective is to produce a hypothesis from a hypothesis class that minimizes the overall error. This optimization problem, known as  Empirical Risk Minimization, can be written as follows:<br />\[\min_{\theta \in \mathbb{R}^d} f(\theta) = \min_{\theta \in \mathbb{R}^d} \left\{\frac{1}{m}\sum\limits_{k=1}^m f_k(\theta) + R(\theta) \right\}.\]<br />In typical examples of interest, such as Logistic Regression, soft-margin Support Vector Machines, etc., we have that each $f_k(\theta)$ is a convex function of the form $\ell(y_k, \theta^{\top}\mathbf{x}_k)$ and $R(\theta)$ is a regularization function. For simplicity, consider the Euclidean regularization given by $R(\theta) = \lambda \|\theta\|^2$.</p>
<p>Gradient Descent is a natural first approach to optimize a function, whereby you repeatedly take steps in the direction opposite of the gradient at the current iterate \[ \mathbf{x}_{t+1} = \mathbf{x}_t – \eta \nabla f(\mathbf{x}_t). \] Under certain conditions on the function and with an appropriate choice of $\eta$, it can be shown this process converges linearly to the true minimizer $\mathbf{x}^*$. If $f$ has condition number $\kappa$, then it can be shown that $O(\kappa \log (1/\epsilon))$ iterations are needed to be $\epsilon$-close to the minimizer, with each iteration taking $O(md)$ time, so the total running time is $O(md \kappa \log(1/ \epsilon))$. Numerous extensions to this basic method have been proposed in the ML literature in recent years, which is a subject for a future blog post.</p>
<p>Given the success of first order methods, a natural extension is to incorporate second order information in deciding what direction to take at each iteration. Newton’s Method does exactly this, where, denoting $\nabla^{-2}f(\mathbf{x}) := [\nabla^2 f(\mathbf{x})]^{-1}$, the update takes the form<br />\[\mathbf{x}_{t+1} = \mathbf{x}_t – \eta \nabla^{-2}f(\mathbf{x}_t) \nabla f(\mathbf{x}_t).\]<br />Although Newton’s Method can converge to the optimum of a quadratic function in a single iteration and can achieve quadratic convergence for general functions (when close enough to the optimum), the per-iteration costs can be prohibitively expensive. Each step requires calculation of the Hessian (which is $O(md^2)$ for functions of the form $f(\mathbf{x})  = \sum\limits_{k=1}^m f_k(\mathbf{x})$) as well as a matrix inversion, which naively takes $O(d^3)$ time.</p>
<p>Drawing inspiration from the success of using stochastic gradient estimates, a natural step forward is to use stochastic second order information in order to estimate the Newton’s direction. One of the key hurdles towards a direct application of this approach is the lack of an immediate unbiased estimator of the inverse Hessian. Indeed, it is not the case that the inverse of an unbiased estimator of the Hessian is an unbiased estimator of the Hessian inverse. In a recent <a href="http://papers.nips.cc/paper/5918-convergence-rates-of-sub-sampled-newton-methods.pdf">paper</a> by Erdogdu and Montanari, they propose an algorithm called NewSamp, which first obtains an estimate of the Hessian by aggregating multiple samples, and then computes the matrix inverse. Unfortunately the above method still suffers due to the prohibitive run-time of the matrix inverse operation. One can take a spectral approximation to speed up inversion, however this comes at the cost of loosing some spectral information and thus loosing some nice theoretical guarantees.</p>
<p>Our method tackles these issues by making use of the Neumann series. Suppose that, for all $\mathbf{x}$, $\|\nabla^2 f(\mathbf{x})\|_2 \leq 1$,  $\nabla^2 f(\mathbf{x}) \succeq 0$. Then, considering the Neumann series of the Hessian matrix, we have that \[\nabla^{-2}f(\mathbf{x}) = \sum\limits_{i=0}^{\infty} \left(I – \nabla^2 f(\mathbf{x})\right)^i.\]<br />The above representation can be used to construct an unbiased estimator in the following way. The key component is an unbiased estimator of a single term $\left(I – \nabla^2 f(\mathbf{x})\right)^i$ in the above summation. This can obtained by taking independent and unbiased samples $\{ \nabla^2 f_{[j]} \}$ and creating the product by multiplying the individual terms. Formally, pick a probability distribution $\{p_i\}_{i \geq 0}$ over the non-negative integers (representing a probability distribution over the individual terms in the series) and sample an index $\hat{i}$. Sample $\hat{i}$ independent and unbiased Hessian samples $\left\{\nabla^2 f_{[j]}(\mathbf{x})\right\}_{j \in \{1, \dots, \hat{i}\}}$ and define the estimator: \[\tilde{\nabla}^{-2} f(\mathbf{x}) = \frac{1}{p_{\hat{i}}} \prod\limits_{j=1}^{\hat{i}} \left(I – \nabla^2 f_{[j]}(\mathbf{x})\right).\]<br />Note that the above estimator is an unbiased estimator which follows from the fact that the samples are independent and unbiased. </p>
<p>We consider the above estimator in our paper and analyze its convergence. The above estimator unfortunately has the disadvantage that it captures one term in the summation and introduces the need for picking a distribution over the terms. However, this issue can be circumvented by making the following observation about a recursive reformulation of the above series:</p>
<p>\[ \nabla^{-2} f = I + \left(I – \nabla^2 f\right)\left( I + \left(I – \nabla^2 f\right) ( \ldots)\right).  \]<br />If we curtail the Taylor series to contain the first $t$ terms (which is equivalent to the recursive depth of $t$ in the above formulation) we can see that an unbiased estimator of the curtailed series can easily be constructed given $t$ unbiased and independent samples as follows:</p>
<p>\[\nabla^{-2} f = I + \left(I – \nabla^2 f_{[t]}\right)\left( I + \left(I – \nabla^2 f_{[t-1]}\right) ( \ldots)\right).\]<br />Note that as $t \rightarrow \infty$ our estimator becomes an unbiased estimator of the inverse. We use the above estimator in place of the true inverse in the Newton Step in the new algorithm, dubbed LiSSA.</p>
<p>The next key observation we make with regards to our estimator is that since we are interested in computing the Newton direction $(\nabla^{-2} f \nabla f)$, we need to repeatedly compute products of the form $(I – \nabla^2 f_{[i]})v$ where $\nabla^2 f_{[i]}$ is a sample of the Hessian and $v$ is a vector. As remarked earlier in common machine learning settings the loss function is usually of the form $\ell_k(y_k, \theta^{\top} \mathbf{x}_k)$ and therefore the hessian is usually of the form $c \mathbf{x}_k\mathbf{x}_k^{\top}$, i.e. it is a rank 1 matrix. It can now be seen that the product $(I – \nabla^2 f_{[i]})v$ can be performed in time $O(d)$, giving us a <b>linear time update step.</b> For full details, we refer the reader to Algorithm 1 in the paper.</p>
<p>We would like to point out an alternative interpretation of our estimator. Consider the second order approximation $g(\mathbf{y})$ to the function $f(\mathbf{x})$ at any point $\mathbf{z}$, where $\mathbf{y} = \mathbf{x} – \mathbf{z}$, given by<br />\[g(\mathbf{y}) = f(\mathbf{z}) + \nabla f(\mathbf{z})^{\top}\mathbf{y} + \frac{1}{2}\mathbf{y}^{\top}\nabla^2 f(\mathbf{z})\mathbf{y}. \]<br />Indeed, a single step of Newton’s method takes us to the minimizer of the quadratic. Let’s consider what a gradient descent step for the above quadratic looks like:<br />\[ \mathbf{x}_{t+1} = (I – \nabla^2 f(\mathbf{z}))\mathbf{x}_t – \nabla f(\mathbf{z}).\]<br />It can now be seen that using $t$ terms of the recursive Taylor series as the inverse of the Hessian corresponds exactly to doing $t$ gradient descent steps on the quadratic approximation. Continuing the analogy we get that our stochastic estimator is a specific form of SGD on the above quadratic. Instead of SGD, one can use any of the improved/accelerated/variance-reduced versions too. Although the above connection is precise we note that it is important that in the expression above, the gradient of $f$ is exact (and not stochastic). Indeed, known lower bounds on stochastic optimization imply that it is impossible to achieve the kind of guarantees we achieve with purely stochastic first and second order information. This is the reason one cannot adapt the standard SGD analysis in our scenario, and it remains an intuitive explanation only.</p>
<p>Before presenting the theorem governing the convergence of LiSSA, we will introduce some useful notation. It is convenient to first assume WLOG that every function $f_k(\mathbf{x})$ has been scaled such that $\|\nabla^2 f_k(\mathbf{x})\| \leq 1.$ We also denote the condition number of $f(\mathbf{x})$ as $\kappa$, and we define $\kappa_{max}$ to be such that<br />\[\lambda_{min}(\nabla^2 f_k(\mathbf{x})) \geq \frac{1}{\kappa_{max}}.\]</p>
<p><b>Theorem:</b>  LiSSA returns a point $\mathbf{x}_t$ such that with probability at least $1 – \delta$, \[f(\mathbf{x}_t) \leq \min\limits_{\mathbf{x}^*} f(\mathbf{x}^*) + \epsilon \] in $O(\log \frac{1}{\epsilon})$ iterations, and total time $ O \left( \left( md + \kappa_{max}^2 \kappa d \right) \log \frac{1}{\epsilon}\right).$</p>
<p>At this point we would like to comment on the running times described above. Our theoretical guarantees are similar to methods such as SVRG and SDCA which achieve running time $O(m + \kappa)d\log(1/\epsilon)$. The additional loss of $\kappa^2_{max}$ in our case is due to the concentration step as we prove high probability bounds on our per-step guarantee. In practice we observe that this concentration step is not required, i.e. applying our estimator only once, performs well as is demonstrated by the graphs below. It is remarkable that the number of <b>iterations </b>is independent of any condition number – this is the significant advantage of second order methods.</p>
<div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://minimizingregret.files.wordpress.com/2016/03/2374e-merged_image.png"><img width="640" src="https://minimizingregret.files.wordpress.com/2016/03/2374e-merged_image.png?w=640&amp;h=424" border="0" height="424" /></a></div>
<p>These preliminary experimental graphs compare LiSSA with standard algorithms such as AdaGrad, Gradient Descent, BFGS and SVRG on a binary Logistic Regression task on three datasets obtained from LibSVM. The metric used in the above graphs is log(Value – Optimum) vs Time Elapsed.</p>
<p>Next we include a comparison between LiSSA and second order methods, vanilla Newton’s Method and NewSamp. We observe that in terms of iterations Newton’s method shows a quadratic decline whereas NewSamp and LiSSA perform similarly in a linear fashion. As is expected, a significant performance advantage for LiSSA is observed when we consider a comparison with respect to time.</p>
<div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://minimizingregret.files.wordpress.com/2016/03/7c7af-mergednewtonimage.png"><img width="640" src="https://minimizingregret.files.wordpress.com/2016/03/7c7af-mergednewtonimage.png?w=640&amp;h=248" border="0" height="248" /></a></div>
<p>To summarize, in this blog post we described a linear time stochastic second order algorithm that achieves linear convergence for typical problems in machine learning while still maintaining run-times theoretically comparable to state-of-the-art first order algorithms. This relies heavily on the special structure of the optimization problem that allows our unbiased hessian estimator to be implemented efficiently, using only vector-vector products. </p></div></div>







<p class="date">
by Elad Hazan <a href="https://minimizingregret.wordpress.com/2016/03/02/making-second-order-methods-practical-for-machine-learning/"><span class="datestr">at March 02, 2016 03:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
